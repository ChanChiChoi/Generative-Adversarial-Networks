{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most codes from https://github.com/hwalsuklee/tensorflow-generative-model-collections/blob/master/CGAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy readingï¼ŒI did not put the \"get_variable\",\"conv2d\" into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, y, y_dim, batch_size, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        \n",
    "        '''concat \"z\" and \"y\" '''\n",
    "        y = tf.reshape(y,shape=[batch_size,1,1,y_dim])\n",
    "        inputs = tf.concat([inputs,y*tf.ones(inputs.shape)],axis=3)\n",
    "        \n",
    "        '''1st: Conv -> lrelu'''\n",
    "        #conv\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        w1 = tf.get_variable('d_wconv1', [4, 4, shape[-1], 64],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('d_bconv1', [64], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.nn.conv2d(inputs, w1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b1), net.get_shape())\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        '''2nd: Conv -> bn -> lrelu'''\n",
    "        #conv\n",
    "        w2 = tf.get_variable('d_wconv2', [4, 4, 64, 128],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable('d_bconv2', [128], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.nn.conv2d(net, w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope='d_bn2')\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b2), net.get_shape())\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        net = tf.reshape(net, [batch_size, -1])\n",
    "        \n",
    "        '''3th: linear -> bn -> lrelu'''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w3 = tf.get_variable(\"d_wlinear3\", [shape[1], 1024], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b3 = tf.get_variable(\"d_blinear3\", [1024], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(net, w3) + b3\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='d_bn3')\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        '''4th: linear '''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w4 = tf.get_variable(\"d_wlinear4\", [shape[1], 1], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b4 = tf.get_variable(\"d_blinear4\", [1], initializer=tf.constant_initializer(0.0))\n",
    "        out_logit = tf.matmul(net, w4) + b4\n",
    "        \n",
    "        '''5th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "        \n",
    "        return out, out_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, y, y_dim, batch_size, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''concat \"z\" and \"y\" '''\n",
    "        z = tf.concat([z,y],axis=1)\n",
    "        \n",
    "        '''1st: linear -> bn -> relu '''\n",
    "        #linear\n",
    "        shape = z.get_shape().as_list()        \n",
    "        w1 = tf.get_variable(\"g_wlinear1\", [shape[1], 1024], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable(\"g_blinear1\", [1024], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(z, w1) + b1\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn1')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net)  \n",
    "        \n",
    "        '''2nd: linear -> bn -> relu '''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w2 = tf.get_variable(\"g_wlinear2\", [shape[1], 128*7*7], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable(\"g_blinear2\", [128*7*7], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(net, w2) + b2\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn2')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net) \n",
    "                     \n",
    "        net = tf.reshape(net, [batch_size, 7, 7, 128])   \n",
    "        \n",
    "        '''3th: deconv -> bn -> relu '''\n",
    "        #deconv\n",
    "        output_shape = [batch_size, 14, 14, 64]\n",
    "        w3 = tf.get_variable('g_wdeconv3', [4, 4, output_shape[-1], net.get_shape()[-1]],\n",
    "                                           initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        net = tf.nn.conv2d_transpose(net, w3, output_shape=output_shape, strides=[1, 2, 2, 1])\n",
    "        b3 = tf.get_variable('g_bdeconv3', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b3), net.get_shape())\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn3')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''4th: deconv -> bn -> sigmoid '''\n",
    "        #deconv\n",
    "        output_shape = [batch_size, 28, 28, 1]\n",
    "        w4 = tf.get_variable('g_wdeconv4', [4, 4, output_shape[-1], net.get_shape()[-1]],\n",
    "                                           initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        net = tf.nn.conv2d_transpose(net, w4, output_shape=output_shape, strides=[1, 2, 2, 1])\n",
    "        b4 = tf.get_variable('g_bdeconv4', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b4), net.get_shape())\n",
    "        #sigmoid\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 62# dimension of noise-vector\n",
    "y_dim = 10# dimension of condition-vector (label)\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size,z_dim], name='z')\n",
    "#label\n",
    "y = tf.placeholder(tf.float32, [batch_size,y_dim], name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "# output of D for real images\n",
    "D_real, D_real_logits = discriminator(inputs,y,y_dim, batch_size, is_training=True, reuse=False)\n",
    "\n",
    "# output of D for fake images\n",
    "G = generator(z,y,y_dim, batch_size, is_training=True, reuse=False)\n",
    "D_fake, D_fake_logits = discriminator(G,y,y_dim, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "# get loss for discriminator\n",
    "d_loss_real = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# get loss for generator\n",
    "g_loss = tf.reduce_mean(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate*5, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "fake_images = generator(z,y,y_dim,batch_size, is_training=False, reuse=True)\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim))\n",
    "test_labels_onehot = np.zeros([batch_size, y_dim],dtype = np.float32)\n",
    "test_labels_onehot[np.arange(batch_size), np.arange(batch_size)%int(np.sqrt(batch_size))] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 1.40009022, g_loss: 0.99826944\n",
      "Step: [1] d_loss: 1.72552574, g_loss: 0.89677227\n",
      "Step: [2] d_loss: 1.40992284, g_loss: 0.90540326\n",
      "Step: [3] d_loss: 1.46819162, g_loss: 0.84609568\n",
      "Step: [4] d_loss: 1.24894619, g_loss: 0.87113923\n",
      "Step: [5] d_loss: 1.24371088, g_loss: 0.87964052\n",
      "Step: [6] d_loss: 1.21443510, g_loss: 0.88207918\n",
      "Step: [7] d_loss: 1.21805739, g_loss: 0.88212138\n",
      "Step: [8] d_loss: 1.17671442, g_loss: 0.90708995\n",
      "Step: [9] d_loss: 1.19989598, g_loss: 0.90570670\n",
      "Step: [10] d_loss: 1.19404364, g_loss: 0.92360544\n",
      "Step: [11] d_loss: 1.21732521, g_loss: 0.93373692\n",
      "Step: [12] d_loss: 1.23231149, g_loss: 0.90987146\n",
      "Step: [13] d_loss: 1.21831501, g_loss: 0.90658522\n",
      "Step: [14] d_loss: 1.24069071, g_loss: 0.88136482\n",
      "Step: [15] d_loss: 1.18991911, g_loss: 0.86105031\n",
      "Step: [16] d_loss: 1.19786644, g_loss: 0.85212272\n",
      "Step: [17] d_loss: 1.17470574, g_loss: 0.86985350\n",
      "Step: [18] d_loss: 1.19833338, g_loss: 0.84412229\n",
      "Step: [19] d_loss: 1.19530249, g_loss: 0.86246091\n",
      "Step: [20] d_loss: 1.21521282, g_loss: 0.86384666\n",
      "Step: [21] d_loss: 1.26371861, g_loss: 0.92155731\n",
      "Step: [22] d_loss: 1.25627518, g_loss: 0.84316534\n",
      "Step: [23] d_loss: 1.20836198, g_loss: 0.85412359\n",
      "Step: [24] d_loss: 1.25883222, g_loss: 0.86204684\n",
      "Step: [25] d_loss: 1.28156638, g_loss: 0.91261041\n",
      "Step: [26] d_loss: 1.35517013, g_loss: 0.90462387\n",
      "Step: [27] d_loss: 1.29598892, g_loss: 0.84176373\n",
      "Step: [28] d_loss: 1.27273369, g_loss: 0.83652008\n",
      "Step: [29] d_loss: 1.27159572, g_loss: 0.83785486\n",
      "Step: [30] d_loss: 1.24970257, g_loss: 0.84031498\n",
      "Step: [31] d_loss: 1.25028324, g_loss: 0.83674866\n",
      "Step: [32] d_loss: 1.28380454, g_loss: 0.83278048\n",
      "Step: [33] d_loss: 1.32647395, g_loss: 0.84656358\n",
      "Step: [34] d_loss: 1.30548811, g_loss: 0.83902001\n",
      "Step: [35] d_loss: 1.30561972, g_loss: 0.83049893\n",
      "Step: [36] d_loss: 1.28696787, g_loss: 0.82024068\n",
      "Step: [37] d_loss: 1.29227746, g_loss: 0.81905574\n",
      "Step: [38] d_loss: 1.27471399, g_loss: 0.81965935\n",
      "Step: [39] d_loss: 1.29212928, g_loss: 0.79961818\n",
      "Step: [40] d_loss: 1.29539633, g_loss: 0.81273812\n",
      "Step: [41] d_loss: 1.30884337, g_loss: 0.83309138\n",
      "Step: [42] d_loss: 1.34034121, g_loss: 0.79815590\n",
      "Step: [43] d_loss: 1.32429028, g_loss: 0.80215430\n",
      "Step: [44] d_loss: 1.31770170, g_loss: 0.79242849\n",
      "Step: [45] d_loss: 1.30315506, g_loss: 0.80232996\n",
      "Step: [46] d_loss: 1.30588233, g_loss: 0.79366940\n",
      "Step: [47] d_loss: 1.30522454, g_loss: 0.76212609\n",
      "Step: [48] d_loss: 1.29912317, g_loss: 0.81292963\n",
      "Step: [49] d_loss: 1.32334495, g_loss: 0.79681414\n",
      "Step: [50] d_loss: 1.32550049, g_loss: 0.80198717\n",
      "Step: [51] d_loss: 1.31265116, g_loss: 0.77897489\n",
      "Step: [52] d_loss: 1.29818594, g_loss: 0.78952980\n",
      "Step: [53] d_loss: 1.29141092, g_loss: 0.79655343\n",
      "Step: [54] d_loss: 1.30079174, g_loss: 0.77963591\n",
      "Step: [55] d_loss: 1.31163454, g_loss: 0.76754856\n",
      "Step: [56] d_loss: 1.33193302, g_loss: 0.76995313\n",
      "Step: [57] d_loss: 1.31968212, g_loss: 0.77963102\n",
      "Step: [58] d_loss: 1.34158731, g_loss: 0.77172005\n",
      "Step: [59] d_loss: 1.35331821, g_loss: 0.80917150\n",
      "Step: [60] d_loss: 1.35710144, g_loss: 0.76709914\n",
      "Step: [61] d_loss: 1.33998322, g_loss: 0.77879202\n",
      "Step: [62] d_loss: 1.35849798, g_loss: 0.77343607\n",
      "Step: [63] d_loss: 1.34393740, g_loss: 0.77355289\n",
      "Step: [64] d_loss: 1.34187639, g_loss: 0.75785607\n",
      "Step: [65] d_loss: 1.35216713, g_loss: 0.76486790\n",
      "Step: [66] d_loss: 1.33207464, g_loss: 0.77358842\n",
      "Step: [67] d_loss: 1.34419394, g_loss: 0.78044462\n",
      "Step: [68] d_loss: 1.33961987, g_loss: 0.79518706\n",
      "Step: [69] d_loss: 1.34500444, g_loss: 0.77478838\n",
      "Step: [70] d_loss: 1.33699667, g_loss: 0.76519424\n",
      "Step: [71] d_loss: 1.32893753, g_loss: 0.75074995\n",
      "Step: [72] d_loss: 1.33884621, g_loss: 0.76201832\n",
      "Step: [73] d_loss: 1.34043145, g_loss: 0.77583742\n",
      "Step: [74] d_loss: 1.35150695, g_loss: 0.75631213\n",
      "Step: [75] d_loss: 1.33563495, g_loss: 0.76825058\n",
      "Step: [76] d_loss: 1.34574437, g_loss: 0.75463736\n",
      "Step: [77] d_loss: 1.36782801, g_loss: 0.79661179\n",
      "Step: [78] d_loss: 1.40222526, g_loss: 0.80216527\n",
      "Step: [79] d_loss: 1.38734484, g_loss: 0.76370406\n",
      "Step: [80] d_loss: 1.36833751, g_loss: 0.73272812\n",
      "Step: [81] d_loss: 1.35642207, g_loss: 0.71940553\n",
      "Step: [82] d_loss: 1.36647987, g_loss: 0.73888165\n",
      "Step: [83] d_loss: 1.33624780, g_loss: 0.73260194\n",
      "Step: [84] d_loss: 1.34608865, g_loss: 0.75317848\n",
      "Step: [85] d_loss: 1.36955738, g_loss: 0.76729053\n",
      "Step: [86] d_loss: 1.35671592, g_loss: 0.75912893\n",
      "Step: [87] d_loss: 1.35890853, g_loss: 0.75725901\n",
      "Step: [88] d_loss: 1.36622334, g_loss: 0.75277483\n",
      "Step: [89] d_loss: 1.37662280, g_loss: 0.76901299\n",
      "Step: [90] d_loss: 1.38545895, g_loss: 0.74682903\n",
      "Step: [91] d_loss: 1.38871622, g_loss: 0.75349343\n",
      "Step: [92] d_loss: 1.35830021, g_loss: 0.74296737\n",
      "Step: [93] d_loss: 1.38980091, g_loss: 0.75409043\n",
      "Step: [94] d_loss: 1.38872766, g_loss: 0.75009739\n",
      "Step: [95] d_loss: 1.37282729, g_loss: 0.76176620\n",
      "Step: [96] d_loss: 1.38687110, g_loss: 0.75330245\n",
      "Step: [97] d_loss: 1.36122799, g_loss: 0.74054724\n",
      "Step: [98] d_loss: 1.34556878, g_loss: 0.74881196\n",
      "Step: [99] d_loss: 1.35716057, g_loss: 0.73780948\n",
      "Step: [100] d_loss: 1.33741033, g_loss: 0.74435782\n",
      "Step: [101] d_loss: 1.34147632, g_loss: 0.75341797\n",
      "Step: [102] d_loss: 1.34444070, g_loss: 0.73671293\n",
      "Step: [103] d_loss: 1.34770572, g_loss: 0.74240935\n",
      "Step: [104] d_loss: 1.36280799, g_loss: 0.74152219\n",
      "Step: [105] d_loss: 1.34320772, g_loss: 0.74674517\n",
      "Step: [106] d_loss: 1.36902201, g_loss: 0.74557799\n",
      "Step: [107] d_loss: 1.34913635, g_loss: 0.75104368\n",
      "Step: [108] d_loss: 1.35169172, g_loss: 0.74270672\n",
      "Step: [109] d_loss: 1.35759485, g_loss: 0.73756224\n",
      "Step: [110] d_loss: 1.35701621, g_loss: 0.73242629\n",
      "Step: [111] d_loss: 1.35548687, g_loss: 0.75890934\n",
      "Step: [112] d_loss: 1.36048758, g_loss: 0.74967158\n",
      "Step: [113] d_loss: 1.35130203, g_loss: 0.74624521\n",
      "Step: [114] d_loss: 1.35547733, g_loss: 0.76173103\n",
      "Step: [115] d_loss: 1.34836388, g_loss: 0.76310188\n",
      "Step: [116] d_loss: 1.34155118, g_loss: 0.74217081\n",
      "Step: [117] d_loss: 1.35203004, g_loss: 0.73728156\n",
      "Step: [118] d_loss: 1.34594846, g_loss: 0.72668040\n",
      "Step: [119] d_loss: 1.35545158, g_loss: 0.72484106\n",
      "Step: [120] d_loss: 1.36712432, g_loss: 0.75255549\n",
      "Step: [121] d_loss: 1.37470448, g_loss: 0.75316411\n",
      "Step: [122] d_loss: 1.36354995, g_loss: 0.74774379\n",
      "Step: [123] d_loss: 1.35880256, g_loss: 0.74009264\n",
      "Step: [124] d_loss: 1.34411263, g_loss: 0.74644411\n",
      "Step: [125] d_loss: 1.34922886, g_loss: 0.74750859\n",
      "Step: [126] d_loss: 1.33560371, g_loss: 0.75281066\n",
      "Step: [127] d_loss: 1.36363721, g_loss: 0.72639048\n",
      "Step: [128] d_loss: 1.35264766, g_loss: 0.73424643\n",
      "Step: [129] d_loss: 1.34322143, g_loss: 0.73681819\n",
      "Step: [130] d_loss: 1.35835719, g_loss: 0.74192441\n",
      "Step: [131] d_loss: 1.35574841, g_loss: 0.74101144\n",
      "Step: [132] d_loss: 1.37386310, g_loss: 0.73078102\n",
      "Step: [133] d_loss: 1.36852455, g_loss: 0.73622704\n",
      "Step: [134] d_loss: 1.37008047, g_loss: 0.75087029\n",
      "Step: [135] d_loss: 1.37518024, g_loss: 0.74651289\n",
      "Step: [136] d_loss: 1.36837459, g_loss: 0.75732434\n",
      "Step: [137] d_loss: 1.36777616, g_loss: 0.73251182\n",
      "Step: [138] d_loss: 1.36128533, g_loss: 0.74287635\n",
      "Step: [139] d_loss: 1.35063076, g_loss: 0.73725533\n",
      "Step: [140] d_loss: 1.35286415, g_loss: 0.73919004\n",
      "Step: [141] d_loss: 1.36334085, g_loss: 0.73495770\n",
      "Step: [142] d_loss: 1.35705245, g_loss: 0.72630614\n",
      "Step: [143] d_loss: 1.35060668, g_loss: 0.74230224\n",
      "Step: [144] d_loss: 1.35395491, g_loss: 0.75184309\n",
      "Step: [145] d_loss: 1.35855985, g_loss: 0.75082767\n",
      "Step: [146] d_loss: 1.36645353, g_loss: 0.73818350\n",
      "Step: [147] d_loss: 1.36347699, g_loss: 0.75969636\n",
      "Step: [148] d_loss: 1.37299049, g_loss: 0.72629380\n",
      "Step: [149] d_loss: 1.36247158, g_loss: 0.74183655\n",
      "Step: [150] d_loss: 1.37523246, g_loss: 0.74685949\n",
      "Step: [151] d_loss: 1.37437332, g_loss: 0.74672234\n",
      "Step: [152] d_loss: 1.36667621, g_loss: 0.75983703\n",
      "Step: [153] d_loss: 1.38971329, g_loss: 0.75363255\n",
      "Step: [154] d_loss: 1.36891758, g_loss: 0.74713439\n",
      "Step: [155] d_loss: 1.38048434, g_loss: 0.74541426\n",
      "Step: [156] d_loss: 1.36471093, g_loss: 0.74256206\n",
      "Step: [157] d_loss: 1.36895597, g_loss: 0.71728969\n",
      "Step: [158] d_loss: 1.35747898, g_loss: 0.73014605\n",
      "Step: [159] d_loss: 1.35459065, g_loss: 0.73840237\n",
      "Step: [160] d_loss: 1.36749554, g_loss: 0.74293011\n",
      "Step: [161] d_loss: 1.35536981, g_loss: 0.74442530\n",
      "Step: [162] d_loss: 1.36244631, g_loss: 0.73719621\n",
      "Step: [163] d_loss: 1.36500812, g_loss: 0.74158394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [164] d_loss: 1.36629438, g_loss: 0.75276393\n",
      "Step: [165] d_loss: 1.38012505, g_loss: 0.74062490\n",
      "Step: [166] d_loss: 1.36098433, g_loss: 0.72897208\n",
      "Step: [167] d_loss: 1.35254931, g_loss: 0.73139477\n",
      "Step: [168] d_loss: 1.37290132, g_loss: 0.72484374\n",
      "Step: [169] d_loss: 1.35688424, g_loss: 0.73914874\n",
      "Step: [170] d_loss: 1.36999559, g_loss: 0.73009014\n",
      "Step: [171] d_loss: 1.35513902, g_loss: 0.74348950\n",
      "Step: [172] d_loss: 1.37044191, g_loss: 0.72724813\n",
      "Step: [173] d_loss: 1.35233355, g_loss: 0.74685997\n",
      "Step: [174] d_loss: 1.36758804, g_loss: 0.72448313\n",
      "Step: [175] d_loss: 1.36089635, g_loss: 0.75275671\n",
      "Step: [176] d_loss: 1.36715269, g_loss: 0.75579458\n",
      "Step: [177] d_loss: 1.37264085, g_loss: 0.74844599\n",
      "Step: [178] d_loss: 1.36631012, g_loss: 0.73894095\n",
      "Step: [179] d_loss: 1.36499226, g_loss: 0.75739211\n",
      "Step: [180] d_loss: 1.37179232, g_loss: 0.73850590\n",
      "Step: [181] d_loss: 1.37264574, g_loss: 0.75780225\n",
      "Step: [182] d_loss: 1.37444925, g_loss: 0.73419988\n",
      "Step: [183] d_loss: 1.36276937, g_loss: 0.74258202\n",
      "Step: [184] d_loss: 1.36472249, g_loss: 0.73949027\n",
      "Step: [185] d_loss: 1.35942149, g_loss: 0.73768508\n",
      "Step: [186] d_loss: 1.34864950, g_loss: 0.74389005\n",
      "Step: [187] d_loss: 1.35739541, g_loss: 0.74975479\n",
      "Step: [188] d_loss: 1.36512351, g_loss: 0.75624055\n",
      "Step: [189] d_loss: 1.37562251, g_loss: 0.74233389\n",
      "Step: [190] d_loss: 1.36865604, g_loss: 0.72248530\n",
      "Step: [191] d_loss: 1.38153160, g_loss: 0.73794854\n",
      "Step: [192] d_loss: 1.36483240, g_loss: 0.73744488\n",
      "Step: [193] d_loss: 1.37470484, g_loss: 0.73151898\n",
      "Step: [194] d_loss: 1.36449289, g_loss: 0.73545355\n",
      "Step: [195] d_loss: 1.37214160, g_loss: 0.73612469\n",
      "Step: [196] d_loss: 1.35834956, g_loss: 0.73641706\n",
      "Step: [197] d_loss: 1.36868858, g_loss: 0.73093259\n",
      "Step: [198] d_loss: 1.36899030, g_loss: 0.72969502\n",
      "Step: [199] d_loss: 1.34896266, g_loss: 0.74638528\n",
      "Step: [200] d_loss: 1.36252022, g_loss: 0.72554135\n",
      "Step: [201] d_loss: 1.34633493, g_loss: 0.73515671\n",
      "Step: [202] d_loss: 1.35182917, g_loss: 0.74815631\n",
      "Step: [203] d_loss: 1.36854303, g_loss: 0.73864865\n",
      "Step: [204] d_loss: 1.36780715, g_loss: 0.74320012\n",
      "Step: [205] d_loss: 1.36568570, g_loss: 0.73329985\n",
      "Step: [206] d_loss: 1.36134613, g_loss: 0.72954553\n",
      "Step: [207] d_loss: 1.36602402, g_loss: 0.73416179\n",
      "Step: [208] d_loss: 1.36029625, g_loss: 0.74081552\n",
      "Step: [209] d_loss: 1.37165165, g_loss: 0.73498225\n",
      "Step: [210] d_loss: 1.37296200, g_loss: 0.73942113\n",
      "Step: [211] d_loss: 1.39073491, g_loss: 0.72679770\n",
      "Step: [212] d_loss: 1.37344730, g_loss: 0.74811435\n",
      "Step: [213] d_loss: 1.38191402, g_loss: 0.75723147\n",
      "Step: [214] d_loss: 1.38378930, g_loss: 0.75938237\n",
      "Step: [215] d_loss: 1.37941134, g_loss: 0.74895996\n",
      "Step: [216] d_loss: 1.36474693, g_loss: 0.75061011\n",
      "Step: [217] d_loss: 1.37638152, g_loss: 0.75108874\n",
      "Step: [218] d_loss: 1.36826992, g_loss: 0.76103050\n",
      "Step: [219] d_loss: 1.36782670, g_loss: 0.75316292\n",
      "Step: [220] d_loss: 1.36631036, g_loss: 0.75249559\n",
      "Step: [221] d_loss: 1.39154363, g_loss: 0.74610579\n",
      "Step: [222] d_loss: 1.38192630, g_loss: 0.74156630\n",
      "Step: [223] d_loss: 1.38424420, g_loss: 0.73616743\n",
      "Step: [224] d_loss: 1.36992049, g_loss: 0.75109065\n",
      "Step: [225] d_loss: 1.37059736, g_loss: 0.74193668\n",
      "Step: [226] d_loss: 1.38007963, g_loss: 0.73142290\n",
      "Step: [227] d_loss: 1.37080467, g_loss: 0.73301274\n",
      "Step: [228] d_loss: 1.37175727, g_loss: 0.73152113\n",
      "Step: [229] d_loss: 1.36149848, g_loss: 0.73964643\n",
      "Step: [230] d_loss: 1.37041450, g_loss: 0.72445643\n",
      "Step: [231] d_loss: 1.37581825, g_loss: 0.73175049\n",
      "Step: [232] d_loss: 1.36715722, g_loss: 0.73064244\n",
      "Step: [233] d_loss: 1.36735773, g_loss: 0.73533440\n",
      "Step: [234] d_loss: 1.35780203, g_loss: 0.73637950\n",
      "Step: [235] d_loss: 1.36591649, g_loss: 0.73723674\n",
      "Step: [236] d_loss: 1.36174226, g_loss: 0.73492229\n",
      "Step: [237] d_loss: 1.36172688, g_loss: 0.72843361\n",
      "Step: [238] d_loss: 1.36970198, g_loss: 0.72746944\n",
      "Step: [239] d_loss: 1.35141110, g_loss: 0.72855633\n",
      "Step: [240] d_loss: 1.36207902, g_loss: 0.73511863\n",
      "Step: [241] d_loss: 1.36456752, g_loss: 0.73367220\n",
      "Step: [242] d_loss: 1.36888814, g_loss: 0.72652960\n",
      "Step: [243] d_loss: 1.37018895, g_loss: 0.73647022\n",
      "Step: [244] d_loss: 1.38344955, g_loss: 0.72796345\n",
      "Step: [245] d_loss: 1.37339139, g_loss: 0.73039639\n",
      "Step: [246] d_loss: 1.38278615, g_loss: 0.74552763\n",
      "Step: [247] d_loss: 1.38817406, g_loss: 0.74320877\n",
      "Step: [248] d_loss: 1.38249445, g_loss: 0.73740512\n",
      "Step: [249] d_loss: 1.37010765, g_loss: 0.74450105\n",
      "Step: [250] d_loss: 1.36899495, g_loss: 0.74211454\n",
      "Step: [251] d_loss: 1.35310650, g_loss: 0.74770385\n",
      "Step: [252] d_loss: 1.35975742, g_loss: 0.75243056\n",
      "Step: [253] d_loss: 1.38414645, g_loss: 0.73614269\n",
      "Step: [254] d_loss: 1.37682164, g_loss: 0.73932123\n",
      "Step: [255] d_loss: 1.36547124, g_loss: 0.75257665\n",
      "Step: [256] d_loss: 1.36097014, g_loss: 0.74408185\n",
      "Step: [257] d_loss: 1.36635387, g_loss: 0.73319066\n",
      "Step: [258] d_loss: 1.36869526, g_loss: 0.73350430\n",
      "Step: [259] d_loss: 1.35515904, g_loss: 0.74997950\n",
      "Step: [260] d_loss: 1.36811256, g_loss: 0.74503052\n",
      "Step: [261] d_loss: 1.36959374, g_loss: 0.73277575\n",
      "Step: [262] d_loss: 1.37913036, g_loss: 0.74196494\n",
      "Step: [263] d_loss: 1.36311316, g_loss: 0.74972701\n",
      "Step: [264] d_loss: 1.37345529, g_loss: 0.75176823\n",
      "Step: [265] d_loss: 1.38228679, g_loss: 0.74058986\n",
      "Step: [266] d_loss: 1.37721181, g_loss: 0.75363910\n",
      "Step: [267] d_loss: 1.37812901, g_loss: 0.73973989\n",
      "Step: [268] d_loss: 1.37003875, g_loss: 0.73067224\n",
      "Step: [269] d_loss: 1.37949610, g_loss: 0.74770308\n",
      "Step: [270] d_loss: 1.37144136, g_loss: 0.74431813\n",
      "Step: [271] d_loss: 1.37436783, g_loss: 0.74010420\n",
      "Step: [272] d_loss: 1.38472462, g_loss: 0.72812223\n",
      "Step: [273] d_loss: 1.37194347, g_loss: 0.74025500\n",
      "Step: [274] d_loss: 1.38336074, g_loss: 0.73082972\n",
      "Step: [275] d_loss: 1.38618720, g_loss: 0.73729992\n",
      "Step: [276] d_loss: 1.37685931, g_loss: 0.72920722\n",
      "Step: [277] d_loss: 1.36736917, g_loss: 0.73320699\n",
      "Step: [278] d_loss: 1.37962842, g_loss: 0.73167551\n",
      "Step: [279] d_loss: 1.37043273, g_loss: 0.73961854\n",
      "Step: [280] d_loss: 1.36854672, g_loss: 0.73389888\n",
      "Step: [281] d_loss: 1.36621881, g_loss: 0.74238485\n",
      "Step: [282] d_loss: 1.37772870, g_loss: 0.73564351\n",
      "Step: [283] d_loss: 1.36499393, g_loss: 0.74078250\n",
      "Step: [284] d_loss: 1.36542737, g_loss: 0.74547470\n",
      "Step: [285] d_loss: 1.38671732, g_loss: 0.72918731\n",
      "Step: [286] d_loss: 1.37322009, g_loss: 0.72906661\n",
      "Step: [287] d_loss: 1.38053548, g_loss: 0.73259485\n",
      "Step: [288] d_loss: 1.37215877, g_loss: 0.72502506\n",
      "Step: [289] d_loss: 1.37679613, g_loss: 0.73381412\n",
      "Step: [290] d_loss: 1.38679230, g_loss: 0.73522449\n",
      "Step: [291] d_loss: 1.37466407, g_loss: 0.73575258\n",
      "Step: [292] d_loss: 1.37408268, g_loss: 0.72710562\n",
      "Step: [293] d_loss: 1.36607790, g_loss: 0.74071455\n",
      "Step: [294] d_loss: 1.35851097, g_loss: 0.73566854\n",
      "Step: [295] d_loss: 1.36380982, g_loss: 0.73591948\n",
      "Step: [296] d_loss: 1.37101007, g_loss: 0.73848891\n",
      "Step: [297] d_loss: 1.38108742, g_loss: 0.72947961\n",
      "Step: [298] d_loss: 1.36892724, g_loss: 0.73383784\n",
      "Step: [299] d_loss: 1.38268018, g_loss: 0.72646773\n",
      "Step: [300] d_loss: 1.37044907, g_loss: 0.73616207\n",
      "Step: [301] d_loss: 1.38613510, g_loss: 0.74295831\n",
      "Step: [302] d_loss: 1.37152207, g_loss: 0.73960221\n",
      "Step: [303] d_loss: 1.36546552, g_loss: 0.73367500\n",
      "Step: [304] d_loss: 1.35722113, g_loss: 0.74506783\n",
      "Step: [305] d_loss: 1.36031628, g_loss: 0.73391664\n",
      "Step: [306] d_loss: 1.36795378, g_loss: 0.73992050\n",
      "Step: [307] d_loss: 1.37872148, g_loss: 0.74011362\n",
      "Step: [308] d_loss: 1.38359880, g_loss: 0.73861897\n",
      "Step: [309] d_loss: 1.36840284, g_loss: 0.73946768\n",
      "Step: [310] d_loss: 1.36540627, g_loss: 0.73297232\n",
      "Step: [311] d_loss: 1.37920415, g_loss: 0.74106634\n",
      "Step: [312] d_loss: 1.37499630, g_loss: 0.74107522\n",
      "Step: [313] d_loss: 1.37293565, g_loss: 0.73338038\n",
      "Step: [314] d_loss: 1.37711775, g_loss: 0.72992218\n",
      "Step: [315] d_loss: 1.37049913, g_loss: 0.72742933\n",
      "Step: [316] d_loss: 1.37618661, g_loss: 0.72368169\n",
      "Step: [317] d_loss: 1.35904241, g_loss: 0.74425715\n",
      "Step: [318] d_loss: 1.36662924, g_loss: 0.73168641\n",
      "Step: [319] d_loss: 1.37698197, g_loss: 0.72898388\n",
      "Step: [320] d_loss: 1.37449908, g_loss: 0.73510492\n",
      "Step: [321] d_loss: 1.37200248, g_loss: 0.73382676\n",
      "Step: [322] d_loss: 1.38368976, g_loss: 0.73017222\n",
      "Step: [323] d_loss: 1.38796711, g_loss: 0.72003645\n",
      "Step: [324] d_loss: 1.38780105, g_loss: 0.73084581\n",
      "Step: [325] d_loss: 1.37466836, g_loss: 0.73255181\n",
      "Step: [326] d_loss: 1.37799299, g_loss: 0.73712289\n",
      "Step: [327] d_loss: 1.38502121, g_loss: 0.72075140\n",
      "Step: [328] d_loss: 1.37021601, g_loss: 0.72204995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [329] d_loss: 1.36869562, g_loss: 0.73489213\n",
      "Step: [330] d_loss: 1.36171722, g_loss: 0.73814321\n",
      "Step: [331] d_loss: 1.36433578, g_loss: 0.74838614\n",
      "Step: [332] d_loss: 1.37132585, g_loss: 0.74629909\n",
      "Step: [333] d_loss: 1.37917280, g_loss: 0.72513688\n",
      "Step: [334] d_loss: 1.37947607, g_loss: 0.73729455\n",
      "Step: [335] d_loss: 1.39709735, g_loss: 0.73894697\n",
      "Step: [336] d_loss: 1.38324237, g_loss: 0.73179907\n",
      "Step: [337] d_loss: 1.39046454, g_loss: 0.74035692\n",
      "Step: [338] d_loss: 1.37454796, g_loss: 0.74118274\n",
      "Step: [339] d_loss: 1.37718201, g_loss: 0.73850775\n",
      "Step: [340] d_loss: 1.37023020, g_loss: 0.74608618\n",
      "Step: [341] d_loss: 1.37266147, g_loss: 0.74053222\n",
      "Step: [342] d_loss: 1.37613666, g_loss: 0.73386502\n",
      "Step: [343] d_loss: 1.36692262, g_loss: 0.74643618\n",
      "Step: [344] d_loss: 1.37761676, g_loss: 0.73722291\n",
      "Step: [345] d_loss: 1.37727261, g_loss: 0.74311143\n",
      "Step: [346] d_loss: 1.38194942, g_loss: 0.74472809\n",
      "Step: [347] d_loss: 1.37961614, g_loss: 0.74147046\n",
      "Step: [348] d_loss: 1.37178874, g_loss: 0.73493481\n",
      "Step: [349] d_loss: 1.36325109, g_loss: 0.73564714\n",
      "Step: [350] d_loss: 1.37288666, g_loss: 0.73903883\n",
      "Step: [351] d_loss: 1.36714411, g_loss: 0.74561119\n",
      "Step: [352] d_loss: 1.37670636, g_loss: 0.73661160\n",
      "Step: [353] d_loss: 1.37053323, g_loss: 0.73147500\n",
      "Step: [354] d_loss: 1.37643409, g_loss: 0.73236877\n",
      "Step: [355] d_loss: 1.38240409, g_loss: 0.72291827\n",
      "Step: [356] d_loss: 1.36659908, g_loss: 0.72918922\n",
      "Step: [357] d_loss: 1.37878358, g_loss: 0.72633302\n",
      "Step: [358] d_loss: 1.37735486, g_loss: 0.72820008\n",
      "Step: [359] d_loss: 1.38332033, g_loss: 0.73016632\n",
      "Step: [360] d_loss: 1.38516426, g_loss: 0.72859251\n",
      "Step: [361] d_loss: 1.37597632, g_loss: 0.73578221\n",
      "Step: [362] d_loss: 1.37870657, g_loss: 0.73259616\n",
      "Step: [363] d_loss: 1.36620927, g_loss: 0.74313867\n",
      "Step: [364] d_loss: 1.37032366, g_loss: 0.75113225\n",
      "Step: [365] d_loss: 1.37718654, g_loss: 0.74657249\n",
      "Step: [366] d_loss: 1.37557435, g_loss: 0.73505390\n",
      "Step: [367] d_loss: 1.38840604, g_loss: 0.72603011\n",
      "Step: [368] d_loss: 1.38683558, g_loss: 0.73197401\n",
      "Step: [369] d_loss: 1.36976111, g_loss: 0.73495978\n",
      "Step: [370] d_loss: 1.37846720, g_loss: 0.72942626\n",
      "Step: [371] d_loss: 1.37703824, g_loss: 0.74760509\n",
      "Step: [372] d_loss: 1.37262058, g_loss: 0.73362315\n",
      "Step: [373] d_loss: 1.37235141, g_loss: 0.73660702\n",
      "Step: [374] d_loss: 1.37920594, g_loss: 0.73618448\n",
      "Step: [375] d_loss: 1.37208736, g_loss: 0.74197739\n",
      "Step: [376] d_loss: 1.37860811, g_loss: 0.72688138\n",
      "Step: [377] d_loss: 1.38621426, g_loss: 0.73479831\n",
      "Step: [378] d_loss: 1.38088799, g_loss: 0.73316908\n",
      "Step: [379] d_loss: 1.37786841, g_loss: 0.73137152\n",
      "Step: [380] d_loss: 1.37120581, g_loss: 0.73233223\n",
      "Step: [381] d_loss: 1.37854028, g_loss: 0.72827947\n",
      "Step: [382] d_loss: 1.37099922, g_loss: 0.73837185\n",
      "Step: [383] d_loss: 1.36453032, g_loss: 0.73476279\n",
      "Step: [384] d_loss: 1.39254558, g_loss: 0.73271573\n",
      "Step: [385] d_loss: 1.39194036, g_loss: 0.74559796\n",
      "Step: [386] d_loss: 1.38278794, g_loss: 0.75058162\n",
      "Step: [387] d_loss: 1.38081098, g_loss: 0.75179446\n",
      "Step: [388] d_loss: 1.38858211, g_loss: 0.73721027\n",
      "Step: [389] d_loss: 1.37604785, g_loss: 0.72739756\n",
      "Step: [390] d_loss: 1.36584926, g_loss: 0.73497236\n",
      "Step: [391] d_loss: 1.35497200, g_loss: 0.74183577\n",
      "Step: [392] d_loss: 1.36620831, g_loss: 0.74150956\n",
      "Step: [393] d_loss: 1.37979996, g_loss: 0.72757387\n",
      "Step: [394] d_loss: 1.36543000, g_loss: 0.73458576\n",
      "Step: [395] d_loss: 1.36859787, g_loss: 0.74585533\n",
      "Step: [396] d_loss: 1.38634920, g_loss: 0.74075902\n",
      "Step: [397] d_loss: 1.37414873, g_loss: 0.73607975\n",
      "Step: [398] d_loss: 1.36263919, g_loss: 0.73355973\n",
      "Step: [399] d_loss: 1.36859822, g_loss: 0.73130286\n",
      "Step: [400] d_loss: 1.38068104, g_loss: 0.72523445\n",
      "Step: [401] d_loss: 1.37964559, g_loss: 0.72084296\n",
      "Step: [402] d_loss: 1.37551165, g_loss: 0.73105061\n",
      "Step: [403] d_loss: 1.38493645, g_loss: 0.72491068\n",
      "Step: [404] d_loss: 1.39279079, g_loss: 0.72953796\n",
      "Step: [405] d_loss: 1.39121985, g_loss: 0.72929096\n",
      "Step: [406] d_loss: 1.38045001, g_loss: 0.73188579\n",
      "Step: [407] d_loss: 1.37386012, g_loss: 0.73755759\n",
      "Step: [408] d_loss: 1.38083577, g_loss: 0.72626936\n",
      "Step: [409] d_loss: 1.37894976, g_loss: 0.74396360\n",
      "Step: [410] d_loss: 1.38543642, g_loss: 0.72519553\n",
      "Step: [411] d_loss: 1.37572575, g_loss: 0.73510426\n",
      "Step: [412] d_loss: 1.36541677, g_loss: 0.74400675\n",
      "Step: [413] d_loss: 1.37462139, g_loss: 0.73289204\n",
      "Step: [414] d_loss: 1.36302090, g_loss: 0.73344952\n",
      "Step: [415] d_loss: 1.37230468, g_loss: 0.73670375\n",
      "Step: [416] d_loss: 1.38362062, g_loss: 0.73315263\n",
      "Step: [417] d_loss: 1.37623644, g_loss: 0.73461294\n",
      "Step: [418] d_loss: 1.37462616, g_loss: 0.73148239\n",
      "Step: [419] d_loss: 1.36950636, g_loss: 0.73842937\n",
      "Step: [420] d_loss: 1.36051393, g_loss: 0.74009997\n",
      "Step: [421] d_loss: 1.38036895, g_loss: 0.73260057\n",
      "Step: [422] d_loss: 1.37065887, g_loss: 0.73541391\n",
      "Step: [423] d_loss: 1.37559080, g_loss: 0.73100781\n",
      "Step: [424] d_loss: 1.39467967, g_loss: 0.73428982\n",
      "Step: [425] d_loss: 1.39230371, g_loss: 0.72800118\n",
      "Step: [426] d_loss: 1.38954616, g_loss: 0.73421156\n",
      "Step: [427] d_loss: 1.38309526, g_loss: 0.74667698\n",
      "Step: [428] d_loss: 1.38880348, g_loss: 0.73965770\n",
      "Step: [429] d_loss: 1.38673329, g_loss: 0.72729635\n",
      "Step: [430] d_loss: 1.37635016, g_loss: 0.73740184\n",
      "Step: [431] d_loss: 1.39942718, g_loss: 0.72755790\n",
      "Step: [432] d_loss: 1.36805606, g_loss: 0.73597157\n",
      "Step: [433] d_loss: 1.36559212, g_loss: 0.73245317\n",
      "Step: [434] d_loss: 1.37752402, g_loss: 0.73956788\n",
      "Step: [435] d_loss: 1.39845347, g_loss: 0.73102927\n",
      "Step: [436] d_loss: 1.37923431, g_loss: 0.74080420\n",
      "Step: [437] d_loss: 1.39170718, g_loss: 0.73022002\n",
      "Step: [438] d_loss: 1.37384725, g_loss: 0.73912990\n",
      "Step: [439] d_loss: 1.38516808, g_loss: 0.72513807\n",
      "Step: [440] d_loss: 1.36239457, g_loss: 0.74710023\n",
      "Step: [441] d_loss: 1.36655998, g_loss: 0.74386787\n",
      "Step: [442] d_loss: 1.36277640, g_loss: 0.73178315\n",
      "Step: [443] d_loss: 1.37486839, g_loss: 0.74082923\n",
      "Step: [444] d_loss: 1.37992358, g_loss: 0.73716795\n",
      "Step: [445] d_loss: 1.38641632, g_loss: 0.72529233\n",
      "Step: [446] d_loss: 1.38206482, g_loss: 0.73009324\n",
      "Step: [447] d_loss: 1.37449443, g_loss: 0.73553854\n",
      "Step: [448] d_loss: 1.39047742, g_loss: 0.73017514\n",
      "Step: [449] d_loss: 1.38896728, g_loss: 0.74721569\n",
      "Step: [450] d_loss: 1.40224457, g_loss: 0.74040127\n",
      "Step: [451] d_loss: 1.39969039, g_loss: 0.73045862\n",
      "Step: [452] d_loss: 1.39105678, g_loss: 0.73425412\n",
      "Step: [453] d_loss: 1.38393128, g_loss: 0.74239731\n",
      "Step: [454] d_loss: 1.37178719, g_loss: 0.73853910\n",
      "Step: [455] d_loss: 1.37647486, g_loss: 0.74830842\n",
      "Step: [456] d_loss: 1.38210773, g_loss: 0.73561811\n",
      "Step: [457] d_loss: 1.36769152, g_loss: 0.73807782\n",
      "Step: [458] d_loss: 1.35518765, g_loss: 0.74438846\n",
      "Step: [459] d_loss: 1.38799787, g_loss: 0.72940660\n",
      "Step: [460] d_loss: 1.37560177, g_loss: 0.74214828\n",
      "Step: [461] d_loss: 1.38235545, g_loss: 0.74266678\n",
      "Step: [462] d_loss: 1.39864707, g_loss: 0.72875059\n",
      "Step: [463] d_loss: 1.41013575, g_loss: 0.72733796\n",
      "Step: [464] d_loss: 1.39128733, g_loss: 0.72581899\n",
      "Step: [465] d_loss: 1.38253796, g_loss: 0.74572217\n",
      "Step: [466] d_loss: 1.37977707, g_loss: 0.72790700\n",
      "Step: [467] d_loss: 1.37419975, g_loss: 0.73966956\n",
      "Step: [468] d_loss: 1.37337649, g_loss: 0.73395979\n",
      "Step: [469] d_loss: 1.36770201, g_loss: 0.73294437\n",
      "Step: [470] d_loss: 1.37034297, g_loss: 0.74510169\n",
      "Step: [471] d_loss: 1.37011552, g_loss: 0.73965204\n",
      "Step: [472] d_loss: 1.37831795, g_loss: 0.73254091\n",
      "Step: [473] d_loss: 1.38694394, g_loss: 0.72744900\n",
      "Step: [474] d_loss: 1.37221241, g_loss: 0.73433757\n",
      "Step: [475] d_loss: 1.37472606, g_loss: 0.73253036\n",
      "Step: [476] d_loss: 1.36057246, g_loss: 0.74540699\n",
      "Step: [477] d_loss: 1.37721717, g_loss: 0.72783136\n",
      "Step: [478] d_loss: 1.38178277, g_loss: 0.71895862\n",
      "Step: [479] d_loss: 1.37592101, g_loss: 0.72709727\n",
      "Step: [480] d_loss: 1.38481522, g_loss: 0.72992742\n",
      "Step: [481] d_loss: 1.39569664, g_loss: 0.72599828\n",
      "Step: [482] d_loss: 1.37605977, g_loss: 0.72743887\n",
      "Step: [483] d_loss: 1.38497353, g_loss: 0.72798109\n",
      "Step: [484] d_loss: 1.36702025, g_loss: 0.74498272\n",
      "Step: [485] d_loss: 1.37468290, g_loss: 0.73687732\n",
      "Step: [486] d_loss: 1.37745571, g_loss: 0.74113822\n",
      "Step: [487] d_loss: 1.37164736, g_loss: 0.73256022\n",
      "Step: [488] d_loss: 1.36639106, g_loss: 0.74282151\n",
      "Step: [489] d_loss: 1.35840607, g_loss: 0.74024636\n",
      "Step: [490] d_loss: 1.36808944, g_loss: 0.73688775\n",
      "Step: [491] d_loss: 1.37671173, g_loss: 0.73726094\n",
      "Step: [492] d_loss: 1.37988007, g_loss: 0.74346888\n",
      "Step: [493] d_loss: 1.37632287, g_loss: 0.73332250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [494] d_loss: 1.36731350, g_loss: 0.73770511\n",
      "Step: [495] d_loss: 1.36282158, g_loss: 0.73427439\n",
      "Step: [496] d_loss: 1.36673927, g_loss: 0.73700643\n",
      "Step: [497] d_loss: 1.39362431, g_loss: 0.73428661\n",
      "Step: [498] d_loss: 1.38390994, g_loss: 0.74055564\n",
      "Step: [499] d_loss: 1.37060535, g_loss: 0.74619883\n",
      "Step: [500] d_loss: 1.38117397, g_loss: 0.73554075\n",
      "Step: [501] d_loss: 1.38509202, g_loss: 0.72610235\n",
      "Step: [502] d_loss: 1.38347852, g_loss: 0.72458255\n",
      "Step: [503] d_loss: 1.37994659, g_loss: 0.72944450\n",
      "Step: [504] d_loss: 1.36194921, g_loss: 0.73943472\n",
      "Step: [505] d_loss: 1.37511837, g_loss: 0.73374486\n",
      "Step: [506] d_loss: 1.36967707, g_loss: 0.73337454\n",
      "Step: [507] d_loss: 1.36203051, g_loss: 0.73332125\n",
      "Step: [508] d_loss: 1.38010120, g_loss: 0.72979718\n",
      "Step: [509] d_loss: 1.39214694, g_loss: 0.74214524\n",
      "Step: [510] d_loss: 1.37631035, g_loss: 0.74509859\n",
      "Step: [511] d_loss: 1.38630176, g_loss: 0.72970295\n",
      "Step: [512] d_loss: 1.37038577, g_loss: 0.73742807\n",
      "Step: [513] d_loss: 1.38394308, g_loss: 0.73760110\n",
      "Step: [514] d_loss: 1.37410462, g_loss: 0.73826486\n",
      "Step: [515] d_loss: 1.37247658, g_loss: 0.73825377\n",
      "Step: [516] d_loss: 1.36061597, g_loss: 0.74249083\n",
      "Step: [517] d_loss: 1.36871696, g_loss: 0.74226546\n",
      "Step: [518] d_loss: 1.37375474, g_loss: 0.73284853\n",
      "Step: [519] d_loss: 1.37383509, g_loss: 0.72894716\n",
      "Step: [520] d_loss: 1.37203479, g_loss: 0.73051530\n",
      "Step: [521] d_loss: 1.36984098, g_loss: 0.74129128\n",
      "Step: [522] d_loss: 1.36213231, g_loss: 0.74339539\n",
      "Step: [523] d_loss: 1.37259603, g_loss: 0.73316050\n",
      "Step: [524] d_loss: 1.36985147, g_loss: 0.74470091\n",
      "Step: [525] d_loss: 1.36636460, g_loss: 0.73753715\n",
      "Step: [526] d_loss: 1.37433314, g_loss: 0.72787189\n",
      "Step: [527] d_loss: 1.36675715, g_loss: 0.72672200\n",
      "Step: [528] d_loss: 1.38337314, g_loss: 0.72683275\n",
      "Step: [529] d_loss: 1.39049375, g_loss: 0.72477829\n",
      "Step: [530] d_loss: 1.38181496, g_loss: 0.73087811\n",
      "Step: [531] d_loss: 1.37938023, g_loss: 0.73301917\n",
      "Step: [532] d_loss: 1.36450529, g_loss: 0.73571420\n",
      "Step: [533] d_loss: 1.38650870, g_loss: 0.73624617\n",
      "Step: [534] d_loss: 1.37193608, g_loss: 0.73600644\n",
      "Step: [535] d_loss: 1.37531662, g_loss: 0.73231107\n",
      "Step: [536] d_loss: 1.38607788, g_loss: 0.72393852\n",
      "Step: [537] d_loss: 1.36094391, g_loss: 0.73919177\n",
      "Step: [538] d_loss: 1.37225676, g_loss: 0.73372042\n",
      "Step: [539] d_loss: 1.37560451, g_loss: 0.73221385\n",
      "Step: [540] d_loss: 1.37293983, g_loss: 0.73790050\n",
      "Step: [541] d_loss: 1.36958098, g_loss: 0.73728764\n",
      "Step: [542] d_loss: 1.36958003, g_loss: 0.73392045\n",
      "Step: [543] d_loss: 1.36662114, g_loss: 0.73359287\n",
      "Step: [544] d_loss: 1.36846459, g_loss: 0.73443270\n",
      "Step: [545] d_loss: 1.37522399, g_loss: 0.72899431\n",
      "Step: [546] d_loss: 1.38734198, g_loss: 0.72092879\n",
      "Step: [547] d_loss: 1.36370552, g_loss: 0.73432183\n",
      "Step: [548] d_loss: 1.36461473, g_loss: 0.73192048\n",
      "Step: [549] d_loss: 1.36305928, g_loss: 0.73580194\n",
      "Step: [550] d_loss: 1.37520540, g_loss: 0.72683942\n",
      "Step: [551] d_loss: 1.38258529, g_loss: 0.73299801\n",
      "Step: [552] d_loss: 1.36636877, g_loss: 0.73495078\n",
      "Step: [553] d_loss: 1.36853600, g_loss: 0.72554612\n",
      "Step: [554] d_loss: 1.37539494, g_loss: 0.73240840\n",
      "Step: [555] d_loss: 1.37148023, g_loss: 0.72277123\n",
      "Step: [556] d_loss: 1.38612556, g_loss: 0.72466803\n",
      "Step: [557] d_loss: 1.36620474, g_loss: 0.73377311\n",
      "Step: [558] d_loss: 1.36632693, g_loss: 0.73343956\n",
      "Step: [559] d_loss: 1.38087177, g_loss: 0.73642552\n",
      "Step: [560] d_loss: 1.37430954, g_loss: 0.72716224\n",
      "Step: [561] d_loss: 1.37460184, g_loss: 0.73241949\n",
      "Step: [562] d_loss: 1.36821985, g_loss: 0.74045455\n",
      "Step: [563] d_loss: 1.38170052, g_loss: 0.73377097\n",
      "Step: [564] d_loss: 1.38139057, g_loss: 0.73471576\n",
      "Step: [565] d_loss: 1.37608361, g_loss: 0.72456503\n",
      "Step: [566] d_loss: 1.36134386, g_loss: 0.74807775\n",
      "Step: [567] d_loss: 1.37168741, g_loss: 0.73792827\n",
      "Step: [568] d_loss: 1.37078166, g_loss: 0.73006105\n",
      "Step: [569] d_loss: 1.36462915, g_loss: 0.74071217\n",
      "Step: [570] d_loss: 1.35382009, g_loss: 0.74394804\n",
      "Step: [571] d_loss: 1.37725735, g_loss: 0.73118657\n",
      "Step: [572] d_loss: 1.37218332, g_loss: 0.73931170\n",
      "Step: [573] d_loss: 1.36198282, g_loss: 0.74371636\n",
      "Step: [574] d_loss: 1.37885737, g_loss: 0.73064005\n",
      "Step: [575] d_loss: 1.38079751, g_loss: 0.73173505\n",
      "Step: [576] d_loss: 1.37456775, g_loss: 0.73256886\n",
      "Step: [577] d_loss: 1.36059225, g_loss: 0.73613763\n",
      "Step: [578] d_loss: 1.36349988, g_loss: 0.73707759\n",
      "Step: [579] d_loss: 1.37871909, g_loss: 0.73182476\n",
      "Step: [580] d_loss: 1.38637245, g_loss: 0.71905780\n",
      "Step: [581] d_loss: 1.37835550, g_loss: 0.72780734\n",
      "Step: [582] d_loss: 1.39220107, g_loss: 0.73377407\n",
      "Step: [583] d_loss: 1.36709821, g_loss: 0.74759179\n",
      "Step: [584] d_loss: 1.37758720, g_loss: 0.73831189\n",
      "Step: [585] d_loss: 1.39162087, g_loss: 0.73415148\n",
      "Step: [586] d_loss: 1.37631357, g_loss: 0.73022544\n",
      "Step: [587] d_loss: 1.36216211, g_loss: 0.73870766\n",
      "Step: [588] d_loss: 1.37421298, g_loss: 0.72778869\n",
      "Step: [589] d_loss: 1.37820506, g_loss: 0.73828626\n",
      "Step: [590] d_loss: 1.36830235, g_loss: 0.74227762\n",
      "Step: [591] d_loss: 1.35646462, g_loss: 0.74743819\n",
      "Step: [592] d_loss: 1.36094964, g_loss: 0.74634558\n",
      "Step: [593] d_loss: 1.37263930, g_loss: 0.73747706\n",
      "Step: [594] d_loss: 1.37078047, g_loss: 0.73314714\n",
      "Step: [595] d_loss: 1.36397910, g_loss: 0.74732172\n",
      "Step: [596] d_loss: 1.36927330, g_loss: 0.73957348\n",
      "Step: [597] d_loss: 1.37025511, g_loss: 0.72988999\n",
      "Step: [598] d_loss: 1.36347175, g_loss: 0.73608470\n",
      "Step: [599] d_loss: 1.39993215, g_loss: 0.73103106\n",
      "Step: [600] d_loss: 1.39777887, g_loss: 0.72522992\n",
      "Step: [601] d_loss: 1.38339388, g_loss: 0.72755897\n",
      "Step: [602] d_loss: 1.38469315, g_loss: 0.72045279\n",
      "Step: [603] d_loss: 1.38674521, g_loss: 0.71783483\n",
      "Step: [604] d_loss: 1.38615882, g_loss: 0.72479713\n",
      "Step: [605] d_loss: 1.38035178, g_loss: 0.72708136\n",
      "Step: [606] d_loss: 1.37090993, g_loss: 0.73432803\n",
      "Step: [607] d_loss: 1.36925209, g_loss: 0.73250550\n",
      "Step: [608] d_loss: 1.37681305, g_loss: 0.73812407\n",
      "Step: [609] d_loss: 1.36253881, g_loss: 0.74051493\n",
      "Step: [610] d_loss: 1.37888503, g_loss: 0.73319137\n",
      "Step: [611] d_loss: 1.37482798, g_loss: 0.73397189\n",
      "Step: [612] d_loss: 1.37044358, g_loss: 0.73707092\n",
      "Step: [613] d_loss: 1.38135076, g_loss: 0.74059075\n",
      "Step: [614] d_loss: 1.37960505, g_loss: 0.72896636\n",
      "Step: [615] d_loss: 1.38643920, g_loss: 0.72628403\n",
      "Step: [616] d_loss: 1.36294508, g_loss: 0.73736560\n",
      "Step: [617] d_loss: 1.35926604, g_loss: 0.73667401\n",
      "Step: [618] d_loss: 1.36766052, g_loss: 0.73995829\n",
      "Step: [619] d_loss: 1.38283825, g_loss: 0.73249209\n",
      "Step: [620] d_loss: 1.38064790, g_loss: 0.73247898\n",
      "Step: [621] d_loss: 1.37407839, g_loss: 0.73217022\n",
      "Step: [622] d_loss: 1.38798451, g_loss: 0.73004985\n",
      "Step: [623] d_loss: 1.37077379, g_loss: 0.73865330\n",
      "Step: [624] d_loss: 1.37947679, g_loss: 0.73975116\n",
      "Step: [625] d_loss: 1.38464618, g_loss: 0.73545384\n",
      "Step: [626] d_loss: 1.38049483, g_loss: 0.73347807\n",
      "Step: [627] d_loss: 1.37390029, g_loss: 0.74061191\n",
      "Step: [628] d_loss: 1.38175225, g_loss: 0.73046613\n",
      "Step: [629] d_loss: 1.38428187, g_loss: 0.73249859\n",
      "Step: [630] d_loss: 1.37003613, g_loss: 0.73307371\n",
      "Step: [631] d_loss: 1.37495399, g_loss: 0.73246729\n",
      "Step: [632] d_loss: 1.36447930, g_loss: 0.74320877\n",
      "Step: [633] d_loss: 1.35543847, g_loss: 0.74507987\n",
      "Step: [634] d_loss: 1.36932969, g_loss: 0.73422611\n",
      "Step: [635] d_loss: 1.35882378, g_loss: 0.73511910\n",
      "Step: [636] d_loss: 1.38409400, g_loss: 0.72860122\n",
      "Step: [637] d_loss: 1.36084867, g_loss: 0.74175513\n",
      "Step: [638] d_loss: 1.37858200, g_loss: 0.71940446\n",
      "Step: [639] d_loss: 1.36783147, g_loss: 0.73534310\n",
      "Step: [640] d_loss: 1.36730576, g_loss: 0.74513090\n",
      "Step: [641] d_loss: 1.35793793, g_loss: 0.73848057\n",
      "Step: [642] d_loss: 1.36986721, g_loss: 0.74321783\n",
      "Step: [643] d_loss: 1.37179387, g_loss: 0.73685014\n",
      "Step: [644] d_loss: 1.37710774, g_loss: 0.72085822\n",
      "Step: [645] d_loss: 1.38118327, g_loss: 0.73322809\n",
      "Step: [646] d_loss: 1.36840701, g_loss: 0.73838472\n",
      "Step: [647] d_loss: 1.38110375, g_loss: 0.73735821\n",
      "Step: [648] d_loss: 1.37832987, g_loss: 0.73624074\n",
      "Step: [649] d_loss: 1.37370646, g_loss: 0.73450041\n",
      "Step: [650] d_loss: 1.37666702, g_loss: 0.73760962\n",
      "Step: [651] d_loss: 1.37143564, g_loss: 0.73566377\n",
      "Step: [652] d_loss: 1.36134756, g_loss: 0.74198645\n",
      "Step: [653] d_loss: 1.37073970, g_loss: 0.74000418\n",
      "Step: [654] d_loss: 1.36483097, g_loss: 0.74300373\n",
      "Step: [655] d_loss: 1.36036921, g_loss: 0.74484611\n",
      "Step: [656] d_loss: 1.34702730, g_loss: 0.74234241\n",
      "Step: [657] d_loss: 1.34332645, g_loss: 0.74722439\n",
      "Step: [658] d_loss: 1.34644270, g_loss: 0.74154449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [659] d_loss: 1.36191142, g_loss: 0.74152279\n",
      "Step: [660] d_loss: 1.37355161, g_loss: 0.73949313\n",
      "Step: [661] d_loss: 1.37365937, g_loss: 0.73133105\n",
      "Step: [662] d_loss: 1.38137257, g_loss: 0.71456140\n",
      "Step: [663] d_loss: 1.37168276, g_loss: 0.72430110\n",
      "Step: [664] d_loss: 1.38675523, g_loss: 0.72747904\n",
      "Step: [665] d_loss: 1.39509892, g_loss: 0.72898579\n",
      "Step: [666] d_loss: 1.37925029, g_loss: 0.73456383\n",
      "Step: [667] d_loss: 1.36448836, g_loss: 0.74542522\n",
      "Step: [668] d_loss: 1.38926625, g_loss: 0.73370039\n",
      "Step: [669] d_loss: 1.37876630, g_loss: 0.74944520\n",
      "Step: [670] d_loss: 1.37493539, g_loss: 0.73906958\n",
      "Step: [671] d_loss: 1.37104797, g_loss: 0.74836522\n",
      "Step: [672] d_loss: 1.38939571, g_loss: 0.73932040\n",
      "Step: [673] d_loss: 1.38541591, g_loss: 0.74052501\n",
      "Step: [674] d_loss: 1.39761853, g_loss: 0.75083411\n",
      "Step: [675] d_loss: 1.38892579, g_loss: 0.73605883\n",
      "Step: [676] d_loss: 1.36834025, g_loss: 0.74231935\n",
      "Step: [677] d_loss: 1.37556338, g_loss: 0.72770417\n",
      "Step: [678] d_loss: 1.37018192, g_loss: 0.75546622\n",
      "Step: [679] d_loss: 1.37366772, g_loss: 0.74131012\n",
      "Step: [680] d_loss: 1.36037517, g_loss: 0.74663520\n",
      "Step: [681] d_loss: 1.37470675, g_loss: 0.73568332\n",
      "Step: [682] d_loss: 1.35530341, g_loss: 0.74816185\n",
      "Step: [683] d_loss: 1.36825418, g_loss: 0.74563861\n",
      "Step: [684] d_loss: 1.39233172, g_loss: 0.73032844\n",
      "Step: [685] d_loss: 1.39822757, g_loss: 0.72286433\n",
      "Step: [686] d_loss: 1.37537611, g_loss: 0.72989905\n",
      "Step: [687] d_loss: 1.37805223, g_loss: 0.73862123\n",
      "Step: [688] d_loss: 1.38331294, g_loss: 0.72910589\n",
      "Step: [689] d_loss: 1.38893378, g_loss: 0.72658777\n",
      "Step: [690] d_loss: 1.39001131, g_loss: 0.72934395\n",
      "Step: [691] d_loss: 1.38258338, g_loss: 0.73416567\n",
      "Step: [692] d_loss: 1.37734079, g_loss: 0.73669207\n",
      "Step: [693] d_loss: 1.37249768, g_loss: 0.74497187\n",
      "Step: [694] d_loss: 1.37838316, g_loss: 0.74210393\n",
      "Step: [695] d_loss: 1.36739373, g_loss: 0.73584199\n",
      "Step: [696] d_loss: 1.36066580, g_loss: 0.74179256\n",
      "Step: [697] d_loss: 1.37291694, g_loss: 0.73509616\n",
      "Step: [698] d_loss: 1.36494470, g_loss: 0.73872346\n",
      "Step: [699] d_loss: 1.36510515, g_loss: 0.73542166\n",
      "Step: [700] d_loss: 1.37044072, g_loss: 0.72782350\n",
      "Step: [701] d_loss: 1.36167216, g_loss: 0.73369646\n",
      "Step: [702] d_loss: 1.36330211, g_loss: 0.73736531\n",
      "Step: [703] d_loss: 1.38554978, g_loss: 0.72387898\n",
      "Step: [704] d_loss: 1.36677384, g_loss: 0.72927427\n",
      "Step: [705] d_loss: 1.38031983, g_loss: 0.73395014\n",
      "Step: [706] d_loss: 1.37227297, g_loss: 0.73925906\n",
      "Step: [707] d_loss: 1.39056349, g_loss: 0.72460961\n",
      "Step: [708] d_loss: 1.38319945, g_loss: 0.73532927\n",
      "Step: [709] d_loss: 1.38372195, g_loss: 0.73052299\n",
      "Step: [710] d_loss: 1.37833834, g_loss: 0.74319416\n",
      "Step: [711] d_loss: 1.38065267, g_loss: 0.72375071\n",
      "Step: [712] d_loss: 1.38223422, g_loss: 0.73234797\n",
      "Step: [713] d_loss: 1.36481977, g_loss: 0.73642147\n",
      "Step: [714] d_loss: 1.38299870, g_loss: 0.72750860\n",
      "Step: [715] d_loss: 1.38236189, g_loss: 0.73461765\n",
      "Step: [716] d_loss: 1.38824284, g_loss: 0.73260659\n",
      "Step: [717] d_loss: 1.36967361, g_loss: 0.73799336\n",
      "Step: [718] d_loss: 1.37098801, g_loss: 0.73388898\n",
      "Step: [719] d_loss: 1.38323176, g_loss: 0.72532135\n",
      "Step: [720] d_loss: 1.37986565, g_loss: 0.73948741\n",
      "Step: [721] d_loss: 1.37167692, g_loss: 0.73807597\n",
      "Step: [722] d_loss: 1.35946846, g_loss: 0.74971449\n",
      "Step: [723] d_loss: 1.38281298, g_loss: 0.74345803\n",
      "Step: [724] d_loss: 1.40114951, g_loss: 0.72593999\n",
      "Step: [725] d_loss: 1.37043095, g_loss: 0.73637605\n",
      "Step: [726] d_loss: 1.39164150, g_loss: 0.72040272\n",
      "Step: [727] d_loss: 1.39019287, g_loss: 0.71588182\n",
      "Step: [728] d_loss: 1.38389802, g_loss: 0.72719848\n",
      "Step: [729] d_loss: 1.36320257, g_loss: 0.73572087\n",
      "Step: [730] d_loss: 1.38653421, g_loss: 0.73122656\n",
      "Step: [731] d_loss: 1.37008929, g_loss: 0.74205679\n",
      "Step: [732] d_loss: 1.38234484, g_loss: 0.72653049\n",
      "Step: [733] d_loss: 1.37706256, g_loss: 0.72476530\n",
      "Step: [734] d_loss: 1.36243892, g_loss: 0.74466759\n",
      "Step: [735] d_loss: 1.37703145, g_loss: 0.73242384\n",
      "Step: [736] d_loss: 1.35706019, g_loss: 0.74471647\n",
      "Step: [737] d_loss: 1.37211716, g_loss: 0.72782612\n",
      "Step: [738] d_loss: 1.37422645, g_loss: 0.73050201\n",
      "Step: [739] d_loss: 1.38385713, g_loss: 0.73170877\n",
      "Step: [740] d_loss: 1.38334858, g_loss: 0.72882986\n",
      "Step: [741] d_loss: 1.35930228, g_loss: 0.74954677\n",
      "Step: [742] d_loss: 1.37980437, g_loss: 0.73391509\n",
      "Step: [743] d_loss: 1.36846852, g_loss: 0.73800385\n",
      "Step: [744] d_loss: 1.37766910, g_loss: 0.73431158\n",
      "Step: [745] d_loss: 1.37981021, g_loss: 0.73108089\n",
      "Step: [746] d_loss: 1.36384320, g_loss: 0.74276841\n",
      "Step: [747] d_loss: 1.36901593, g_loss: 0.72951806\n",
      "Step: [748] d_loss: 1.37408113, g_loss: 0.72645903\n",
      "Step: [749] d_loss: 1.38512492, g_loss: 0.73615408\n",
      "Step: [750] d_loss: 1.37535334, g_loss: 0.73384827\n",
      "Step: [751] d_loss: 1.38842714, g_loss: 0.73140216\n",
      "Step: [752] d_loss: 1.38499308, g_loss: 0.72087693\n",
      "Step: [753] d_loss: 1.37383413, g_loss: 0.73561001\n",
      "Step: [754] d_loss: 1.36721778, g_loss: 0.73772079\n",
      "Step: [755] d_loss: 1.36939216, g_loss: 0.72851849\n",
      "Step: [756] d_loss: 1.39012432, g_loss: 0.72688615\n",
      "Step: [757] d_loss: 1.37390876, g_loss: 0.73658764\n",
      "Step: [758] d_loss: 1.38148689, g_loss: 0.73022544\n",
      "Step: [759] d_loss: 1.37215006, g_loss: 0.73058760\n",
      "Step: [760] d_loss: 1.37935770, g_loss: 0.72301829\n",
      "Step: [761] d_loss: 1.36394048, g_loss: 0.73821175\n",
      "Step: [762] d_loss: 1.37785923, g_loss: 0.73014414\n",
      "Step: [763] d_loss: 1.37065268, g_loss: 0.72714305\n",
      "Step: [764] d_loss: 1.37285233, g_loss: 0.73372138\n",
      "Step: [765] d_loss: 1.37704420, g_loss: 0.73683274\n",
      "Step: [766] d_loss: 1.38265395, g_loss: 0.72628152\n",
      "Step: [767] d_loss: 1.38358021, g_loss: 0.72589171\n",
      "Step: [768] d_loss: 1.38047457, g_loss: 0.73950589\n",
      "Step: [769] d_loss: 1.36096644, g_loss: 0.74258101\n",
      "Step: [770] d_loss: 1.37557316, g_loss: 0.73719889\n",
      "Step: [771] d_loss: 1.37078786, g_loss: 0.74743378\n",
      "Step: [772] d_loss: 1.37607813, g_loss: 0.74098849\n",
      "Step: [773] d_loss: 1.37747812, g_loss: 0.74198508\n",
      "Step: [774] d_loss: 1.36179566, g_loss: 0.73951834\n",
      "Step: [775] d_loss: 1.36698341, g_loss: 0.73013520\n",
      "Step: [776] d_loss: 1.37582469, g_loss: 0.71933544\n",
      "Step: [777] d_loss: 1.37236428, g_loss: 0.72956562\n",
      "Step: [778] d_loss: 1.37552667, g_loss: 0.73076153\n",
      "Step: [779] d_loss: 1.37137985, g_loss: 0.73490226\n",
      "Step: [780] d_loss: 1.37415361, g_loss: 0.73428571\n",
      "Step: [781] d_loss: 1.37976897, g_loss: 0.72546929\n",
      "Step: [782] d_loss: 1.38220406, g_loss: 0.71696717\n",
      "Step: [783] d_loss: 1.37968957, g_loss: 0.72854584\n",
      "Step: [784] d_loss: 1.37598515, g_loss: 0.73462611\n",
      "Step: [785] d_loss: 1.38242006, g_loss: 0.72827625\n",
      "Step: [786] d_loss: 1.39074147, g_loss: 0.72326505\n",
      "Step: [787] d_loss: 1.38705730, g_loss: 0.72994256\n",
      "Step: [788] d_loss: 1.36844146, g_loss: 0.74319047\n",
      "Step: [789] d_loss: 1.38596833, g_loss: 0.73224533\n",
      "Step: [790] d_loss: 1.38357198, g_loss: 0.73105294\n",
      "Step: [791] d_loss: 1.36560690, g_loss: 0.73032671\n",
      "Step: [792] d_loss: 1.37101197, g_loss: 0.74014533\n",
      "Step: [793] d_loss: 1.36138344, g_loss: 0.74580872\n",
      "Step: [794] d_loss: 1.37664843, g_loss: 0.73680520\n",
      "Step: [795] d_loss: 1.37625182, g_loss: 0.73603928\n",
      "Step: [796] d_loss: 1.37624621, g_loss: 0.74402606\n",
      "Step: [797] d_loss: 1.37558913, g_loss: 0.74504817\n",
      "Step: [798] d_loss: 1.36705363, g_loss: 0.73412031\n",
      "Step: [799] d_loss: 1.36898136, g_loss: 0.73644996\n",
      "Step: [800] d_loss: 1.36781192, g_loss: 0.73268223\n",
      "Step: [801] d_loss: 1.37937510, g_loss: 0.72650921\n",
      "Step: [802] d_loss: 1.38743758, g_loss: 0.72387010\n",
      "Step: [803] d_loss: 1.38470423, g_loss: 0.72702545\n",
      "Step: [804] d_loss: 1.39021778, g_loss: 0.73202914\n",
      "Step: [805] d_loss: 1.36161244, g_loss: 0.74120516\n",
      "Step: [806] d_loss: 1.37653387, g_loss: 0.73078865\n",
      "Step: [807] d_loss: 1.37397540, g_loss: 0.72344834\n",
      "Step: [808] d_loss: 1.37850380, g_loss: 0.72129238\n",
      "Step: [809] d_loss: 1.39102888, g_loss: 0.72439551\n",
      "Step: [810] d_loss: 1.38471663, g_loss: 0.72550404\n",
      "Step: [811] d_loss: 1.38036871, g_loss: 0.73363400\n",
      "Step: [812] d_loss: 1.36896300, g_loss: 0.73337281\n",
      "Step: [813] d_loss: 1.37783766, g_loss: 0.73068261\n",
      "Step: [814] d_loss: 1.38456845, g_loss: 0.72698468\n",
      "Step: [815] d_loss: 1.36808181, g_loss: 0.74037743\n",
      "Step: [816] d_loss: 1.38930643, g_loss: 0.72904044\n",
      "Step: [817] d_loss: 1.36964464, g_loss: 0.73686087\n",
      "Step: [818] d_loss: 1.37668693, g_loss: 0.73472470\n",
      "Step: [819] d_loss: 1.38193989, g_loss: 0.73752856\n",
      "Step: [820] d_loss: 1.36332452, g_loss: 0.75025034\n",
      "Step: [821] d_loss: 1.37458944, g_loss: 0.73419416\n",
      "Step: [822] d_loss: 1.37839806, g_loss: 0.72845364\n",
      "Step: [823] d_loss: 1.36990905, g_loss: 0.73290932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [824] d_loss: 1.37859106, g_loss: 0.73425865\n",
      "Step: [825] d_loss: 1.36897314, g_loss: 0.73384076\n",
      "Step: [826] d_loss: 1.38585174, g_loss: 0.72315574\n",
      "Step: [827] d_loss: 1.36988783, g_loss: 0.73515600\n",
      "Step: [828] d_loss: 1.35870397, g_loss: 0.73587489\n",
      "Step: [829] d_loss: 1.38871884, g_loss: 0.72236943\n",
      "Step: [830] d_loss: 1.38650048, g_loss: 0.72512156\n",
      "Step: [831] d_loss: 1.36494410, g_loss: 0.73684680\n",
      "Step: [832] d_loss: 1.37948871, g_loss: 0.72480607\n",
      "Step: [833] d_loss: 1.37378836, g_loss: 0.72839904\n",
      "Step: [834] d_loss: 1.37609601, g_loss: 0.72381616\n",
      "Step: [835] d_loss: 1.38364565, g_loss: 0.73765004\n",
      "Step: [836] d_loss: 1.38275862, g_loss: 0.71831036\n",
      "Step: [837] d_loss: 1.38245308, g_loss: 0.72837007\n",
      "Step: [838] d_loss: 1.38053441, g_loss: 0.72593021\n",
      "Step: [839] d_loss: 1.37964272, g_loss: 0.73132086\n",
      "Step: [840] d_loss: 1.37496877, g_loss: 0.73272783\n",
      "Step: [841] d_loss: 1.37921536, g_loss: 0.72405255\n",
      "Step: [842] d_loss: 1.37486553, g_loss: 0.73910511\n",
      "Step: [843] d_loss: 1.38355970, g_loss: 0.73952526\n",
      "Step: [844] d_loss: 1.39023256, g_loss: 0.74304968\n",
      "Step: [845] d_loss: 1.40048230, g_loss: 0.75083339\n",
      "Step: [846] d_loss: 1.39738154, g_loss: 0.73760176\n",
      "Step: [847] d_loss: 1.38902104, g_loss: 0.73415530\n",
      "Step: [848] d_loss: 1.39624715, g_loss: 0.73180079\n",
      "Step: [849] d_loss: 1.37630630, g_loss: 0.73526251\n",
      "Step: [850] d_loss: 1.37927520, g_loss: 0.73489720\n",
      "Step: [851] d_loss: 1.37133658, g_loss: 0.73889124\n",
      "Step: [852] d_loss: 1.36583030, g_loss: 0.73883921\n",
      "Step: [853] d_loss: 1.38473403, g_loss: 0.73024344\n",
      "Step: [854] d_loss: 1.36435723, g_loss: 0.74078566\n",
      "Step: [855] d_loss: 1.37323225, g_loss: 0.73304689\n",
      "Step: [856] d_loss: 1.37359834, g_loss: 0.72141552\n",
      "Step: [857] d_loss: 1.36049223, g_loss: 0.73898423\n",
      "Step: [858] d_loss: 1.38629222, g_loss: 0.72553301\n",
      "Step: [859] d_loss: 1.37093270, g_loss: 0.72737294\n",
      "Step: [860] d_loss: 1.37498081, g_loss: 0.73058325\n",
      "Step: [861] d_loss: 1.38146365, g_loss: 0.72731018\n",
      "Step: [862] d_loss: 1.37242508, g_loss: 0.73697931\n",
      "Step: [863] d_loss: 1.37533975, g_loss: 0.73962212\n",
      "Step: [864] d_loss: 1.38446963, g_loss: 0.73676300\n",
      "Step: [865] d_loss: 1.36736488, g_loss: 0.73773962\n",
      "Step: [866] d_loss: 1.37450886, g_loss: 0.74335420\n",
      "Step: [867] d_loss: 1.37512267, g_loss: 0.74052954\n",
      "Step: [868] d_loss: 1.38103390, g_loss: 0.73162186\n",
      "Step: [869] d_loss: 1.37660599, g_loss: 0.73379457\n",
      "Step: [870] d_loss: 1.36904645, g_loss: 0.73424077\n",
      "Step: [871] d_loss: 1.36274850, g_loss: 0.73833382\n",
      "Step: [872] d_loss: 1.35861087, g_loss: 0.74620092\n",
      "Step: [873] d_loss: 1.37613678, g_loss: 0.73330927\n",
      "Step: [874] d_loss: 1.38252556, g_loss: 0.72803497\n",
      "Step: [875] d_loss: 1.38594294, g_loss: 0.72886455\n",
      "Step: [876] d_loss: 1.38914990, g_loss: 0.73213971\n",
      "Step: [877] d_loss: 1.38083172, g_loss: 0.73081309\n",
      "Step: [878] d_loss: 1.37013388, g_loss: 0.74015939\n",
      "Step: [879] d_loss: 1.39343226, g_loss: 0.72116530\n",
      "Step: [880] d_loss: 1.39711761, g_loss: 0.73371589\n",
      "Step: [881] d_loss: 1.38360357, g_loss: 0.73886287\n",
      "Step: [882] d_loss: 1.36586559, g_loss: 0.74431884\n",
      "Step: [883] d_loss: 1.37607515, g_loss: 0.72607607\n",
      "Step: [884] d_loss: 1.37913775, g_loss: 0.73099250\n",
      "Step: [885] d_loss: 1.35685349, g_loss: 0.74164283\n",
      "Step: [886] d_loss: 1.37029219, g_loss: 0.72368312\n",
      "Step: [887] d_loss: 1.36696696, g_loss: 0.74033475\n",
      "Step: [888] d_loss: 1.37296629, g_loss: 0.73176700\n",
      "Step: [889] d_loss: 1.36500311, g_loss: 0.73096520\n",
      "Step: [890] d_loss: 1.36196303, g_loss: 0.73352540\n",
      "Step: [891] d_loss: 1.35757995, g_loss: 0.73822069\n",
      "Step: [892] d_loss: 1.36368275, g_loss: 0.73292434\n",
      "Step: [893] d_loss: 1.35040855, g_loss: 0.73843169\n",
      "Step: [894] d_loss: 1.38213515, g_loss: 0.72619927\n",
      "Step: [895] d_loss: 1.36630201, g_loss: 0.72794580\n",
      "Step: [896] d_loss: 1.37014270, g_loss: 0.73320901\n",
      "Step: [897] d_loss: 1.37140751, g_loss: 0.72825509\n",
      "Step: [898] d_loss: 1.38132715, g_loss: 0.72714496\n",
      "Step: [899] d_loss: 1.37512779, g_loss: 0.72985494\n",
      "Step: [900] d_loss: 1.38507962, g_loss: 0.72829270\n",
      "Step: [901] d_loss: 1.37254179, g_loss: 0.73037952\n",
      "Step: [902] d_loss: 1.38174796, g_loss: 0.72730279\n",
      "Step: [903] d_loss: 1.38238215, g_loss: 0.73522413\n",
      "Step: [904] d_loss: 1.38139486, g_loss: 0.72666425\n",
      "Step: [905] d_loss: 1.37427425, g_loss: 0.73453903\n",
      "Step: [906] d_loss: 1.36500478, g_loss: 0.73847985\n",
      "Step: [907] d_loss: 1.37512589, g_loss: 0.73219740\n",
      "Step: [908] d_loss: 1.35733700, g_loss: 0.74232268\n",
      "Step: [909] d_loss: 1.35668898, g_loss: 0.74489200\n",
      "Step: [910] d_loss: 1.36657715, g_loss: 0.74094141\n",
      "Step: [911] d_loss: 1.38927221, g_loss: 0.73297340\n",
      "Step: [912] d_loss: 1.37035584, g_loss: 0.73264593\n",
      "Step: [913] d_loss: 1.37522376, g_loss: 0.73085672\n",
      "Step: [914] d_loss: 1.37774324, g_loss: 0.73799253\n",
      "Step: [915] d_loss: 1.39046597, g_loss: 0.73658854\n",
      "Step: [916] d_loss: 1.38036084, g_loss: 0.73626941\n",
      "Step: [917] d_loss: 1.37091470, g_loss: 0.74313116\n",
      "Step: [918] d_loss: 1.36322784, g_loss: 0.73509967\n",
      "Step: [919] d_loss: 1.37446249, g_loss: 0.72806072\n",
      "Step: [920] d_loss: 1.39163089, g_loss: 0.72937894\n",
      "Step: [921] d_loss: 1.37372828, g_loss: 0.72945869\n",
      "Step: [922] d_loss: 1.37759447, g_loss: 0.73625922\n",
      "Step: [923] d_loss: 1.37565780, g_loss: 0.73563087\n",
      "Step: [924] d_loss: 1.38339829, g_loss: 0.74445665\n",
      "Step: [925] d_loss: 1.38862610, g_loss: 0.73026669\n",
      "Step: [926] d_loss: 1.37277305, g_loss: 0.73279738\n",
      "Step: [927] d_loss: 1.36355996, g_loss: 0.73693252\n",
      "Step: [928] d_loss: 1.36558187, g_loss: 0.73944604\n",
      "Step: [929] d_loss: 1.37769878, g_loss: 0.72921163\n",
      "Step: [930] d_loss: 1.36750913, g_loss: 0.74208528\n",
      "Step: [931] d_loss: 1.37677407, g_loss: 0.73951030\n",
      "Step: [932] d_loss: 1.38813937, g_loss: 0.72180474\n",
      "Step: [933] d_loss: 1.37865329, g_loss: 0.73468202\n",
      "Step: [934] d_loss: 1.38138700, g_loss: 0.72826278\n",
      "Step: [935] d_loss: 1.38145447, g_loss: 0.74038506\n",
      "Step: [936] d_loss: 1.39130604, g_loss: 0.72266263\n",
      "Step: [937] d_loss: 1.37687647, g_loss: 0.73345488\n",
      "Step: [938] d_loss: 1.37822938, g_loss: 0.74037546\n",
      "Step: [939] d_loss: 1.38088083, g_loss: 0.73465788\n",
      "Step: [940] d_loss: 1.36231279, g_loss: 0.72700107\n",
      "Step: [941] d_loss: 1.36704433, g_loss: 0.74135274\n",
      "Step: [942] d_loss: 1.36416507, g_loss: 0.74044287\n",
      "Step: [943] d_loss: 1.38009167, g_loss: 0.72601074\n",
      "Step: [944] d_loss: 1.36180019, g_loss: 0.73858547\n",
      "Step: [945] d_loss: 1.37280953, g_loss: 0.73611605\n",
      "Step: [946] d_loss: 1.36686707, g_loss: 0.73782563\n",
      "Step: [947] d_loss: 1.36524296, g_loss: 0.73346794\n",
      "Step: [948] d_loss: 1.37225401, g_loss: 0.73032224\n",
      "Step: [949] d_loss: 1.36826587, g_loss: 0.73631233\n",
      "Step: [950] d_loss: 1.36398566, g_loss: 0.73525667\n",
      "Step: [951] d_loss: 1.36335850, g_loss: 0.73587918\n",
      "Step: [952] d_loss: 1.37325346, g_loss: 0.74021333\n",
      "Step: [953] d_loss: 1.40192103, g_loss: 0.73551321\n",
      "Step: [954] d_loss: 1.40193105, g_loss: 0.73396116\n",
      "Step: [955] d_loss: 1.37403107, g_loss: 0.73019701\n",
      "Step: [956] d_loss: 1.39079762, g_loss: 0.73499465\n",
      "Step: [957] d_loss: 1.37768149, g_loss: 0.73395401\n",
      "Step: [958] d_loss: 1.37556458, g_loss: 0.73583573\n",
      "Step: [959] d_loss: 1.36663878, g_loss: 0.73584789\n",
      "Step: [960] d_loss: 1.36823678, g_loss: 0.73534381\n",
      "Step: [961] d_loss: 1.38063478, g_loss: 0.72724730\n",
      "Step: [962] d_loss: 1.37348151, g_loss: 0.73204756\n",
      "Step: [963] d_loss: 1.39472997, g_loss: 0.73391891\n",
      "Step: [964] d_loss: 1.37302279, g_loss: 0.74072683\n",
      "Step: [965] d_loss: 1.37108779, g_loss: 0.74376380\n",
      "Step: [966] d_loss: 1.36706662, g_loss: 0.73690873\n",
      "Step: [967] d_loss: 1.35691619, g_loss: 0.74063408\n",
      "Step: [968] d_loss: 1.37382650, g_loss: 0.73231459\n",
      "Step: [969] d_loss: 1.37313485, g_loss: 0.73361170\n",
      "Step: [970] d_loss: 1.36933088, g_loss: 0.73593175\n",
      "Step: [971] d_loss: 1.37743521, g_loss: 0.73146057\n",
      "Step: [972] d_loss: 1.36713243, g_loss: 0.73351866\n",
      "Step: [973] d_loss: 1.39150584, g_loss: 0.72690296\n",
      "Step: [974] d_loss: 1.37992978, g_loss: 0.72950125\n",
      "Step: [975] d_loss: 1.38237321, g_loss: 0.72453332\n",
      "Step: [976] d_loss: 1.37374496, g_loss: 0.72499204\n",
      "Step: [977] d_loss: 1.35891414, g_loss: 0.73416889\n",
      "Step: [978] d_loss: 1.37893414, g_loss: 0.73412681\n",
      "Step: [979] d_loss: 1.39462495, g_loss: 0.72407222\n",
      "Step: [980] d_loss: 1.37534809, g_loss: 0.73199254\n",
      "Step: [981] d_loss: 1.38727844, g_loss: 0.72866243\n",
      "Step: [982] d_loss: 1.38651204, g_loss: 0.72671145\n",
      "Step: [983] d_loss: 1.39162040, g_loss: 0.72151053\n",
      "Step: [984] d_loss: 1.39379859, g_loss: 0.73182052\n",
      "Step: [985] d_loss: 1.41587675, g_loss: 0.74154919\n",
      "Step: [986] d_loss: 1.40475559, g_loss: 0.75702220\n",
      "Step: [987] d_loss: 1.40176368, g_loss: 0.73542279\n",
      "Step: [988] d_loss: 1.38516140, g_loss: 0.72758812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [989] d_loss: 1.38613856, g_loss: 0.74550045\n",
      "Step: [990] d_loss: 1.39916956, g_loss: 0.71523333\n",
      "Step: [991] d_loss: 1.37328625, g_loss: 0.73054236\n",
      "Step: [992] d_loss: 1.37854767, g_loss: 0.72394979\n",
      "Step: [993] d_loss: 1.37956345, g_loss: 0.73021805\n",
      "Step: [994] d_loss: 1.37655735, g_loss: 0.73378086\n",
      "Step: [995] d_loss: 1.39229369, g_loss: 0.73246282\n",
      "Step: [996] d_loss: 1.37072968, g_loss: 0.73250240\n",
      "Step: [997] d_loss: 1.36846805, g_loss: 0.73075390\n",
      "Step: [998] d_loss: 1.37508702, g_loss: 0.73377931\n",
      "Step: [999] d_loss: 1.37726951, g_loss: 0.73046643\n",
      "Step: [1000] d_loss: 1.38439333, g_loss: 0.71645278\n",
      "Step: [1001] d_loss: 1.37307382, g_loss: 0.73911750\n",
      "Step: [1002] d_loss: 1.37979949, g_loss: 0.73448014\n",
      "Step: [1003] d_loss: 1.38139367, g_loss: 0.72398812\n",
      "Step: [1004] d_loss: 1.37569094, g_loss: 0.71999812\n",
      "Step: [1005] d_loss: 1.38868046, g_loss: 0.72222102\n",
      "Step: [1006] d_loss: 1.38584065, g_loss: 0.72788262\n",
      "Step: [1007] d_loss: 1.37297797, g_loss: 0.73762333\n",
      "Step: [1008] d_loss: 1.39236009, g_loss: 0.72656697\n",
      "Step: [1009] d_loss: 1.35958064, g_loss: 0.74234962\n",
      "Step: [1010] d_loss: 1.38490427, g_loss: 0.72806543\n",
      "Step: [1011] d_loss: 1.37324619, g_loss: 0.73113883\n",
      "Step: [1012] d_loss: 1.37805343, g_loss: 0.72729087\n",
      "Step: [1013] d_loss: 1.35056365, g_loss: 0.74475610\n",
      "Step: [1014] d_loss: 1.36474073, g_loss: 0.73665494\n",
      "Step: [1015] d_loss: 1.38378954, g_loss: 0.72530901\n",
      "Step: [1016] d_loss: 1.36219275, g_loss: 0.72878313\n",
      "Step: [1017] d_loss: 1.36342895, g_loss: 0.72982299\n",
      "Step: [1018] d_loss: 1.38226008, g_loss: 0.73034531\n",
      "Step: [1019] d_loss: 1.37612998, g_loss: 0.73066181\n",
      "Step: [1020] d_loss: 1.36371350, g_loss: 0.73187649\n",
      "Step: [1021] d_loss: 1.37642360, g_loss: 0.72133911\n",
      "Step: [1022] d_loss: 1.37118864, g_loss: 0.72864807\n",
      "Step: [1023] d_loss: 1.38954973, g_loss: 0.72580850\n",
      "Step: [1024] d_loss: 1.37029147, g_loss: 0.72580451\n",
      "Step: [1025] d_loss: 1.38847375, g_loss: 0.72052789\n",
      "Step: [1026] d_loss: 1.37978506, g_loss: 0.73281443\n",
      "Step: [1027] d_loss: 1.36256170, g_loss: 0.73103201\n",
      "Step: [1028] d_loss: 1.37718225, g_loss: 0.72539079\n",
      "Step: [1029] d_loss: 1.37993693, g_loss: 0.73165977\n",
      "Step: [1030] d_loss: 1.35885525, g_loss: 0.73383987\n",
      "Step: [1031] d_loss: 1.37479806, g_loss: 0.73372269\n",
      "Step: [1032] d_loss: 1.37735224, g_loss: 0.72699666\n",
      "Step: [1033] d_loss: 1.38078737, g_loss: 0.73884374\n",
      "Step: [1034] d_loss: 1.38424277, g_loss: 0.72284770\n",
      "Step: [1035] d_loss: 1.37944531, g_loss: 0.72907400\n",
      "Step: [1036] d_loss: 1.39092731, g_loss: 0.72964674\n",
      "Step: [1037] d_loss: 1.37140048, g_loss: 0.73215270\n",
      "Step: [1038] d_loss: 1.37945902, g_loss: 0.72609007\n",
      "Step: [1039] d_loss: 1.36759079, g_loss: 0.72821468\n",
      "Step: [1040] d_loss: 1.37404144, g_loss: 0.73195708\n",
      "Step: [1041] d_loss: 1.36962664, g_loss: 0.73009419\n",
      "Step: [1042] d_loss: 1.37222648, g_loss: 0.73394835\n",
      "Step: [1043] d_loss: 1.36720502, g_loss: 0.73824650\n",
      "Step: [1044] d_loss: 1.35476851, g_loss: 0.73523581\n",
      "Step: [1045] d_loss: 1.37028027, g_loss: 0.73515207\n",
      "Step: [1046] d_loss: 1.36474454, g_loss: 0.74086583\n",
      "Step: [1047] d_loss: 1.35946321, g_loss: 0.73774260\n",
      "Step: [1048] d_loss: 1.38198924, g_loss: 0.72538137\n",
      "Step: [1049] d_loss: 1.36202598, g_loss: 0.74119735\n",
      "Step: [1050] d_loss: 1.36773849, g_loss: 0.74476963\n",
      "Step: [1051] d_loss: 1.36913788, g_loss: 0.73579407\n",
      "Step: [1052] d_loss: 1.38601160, g_loss: 0.71863306\n",
      "Step: [1053] d_loss: 1.37987506, g_loss: 0.73063612\n",
      "Step: [1054] d_loss: 1.38400614, g_loss: 0.72857106\n",
      "Step: [1055] d_loss: 1.38470054, g_loss: 0.73453200\n",
      "Step: [1056] d_loss: 1.37429273, g_loss: 0.73510480\n",
      "Step: [1057] d_loss: 1.38859272, g_loss: 0.72246188\n",
      "Step: [1058] d_loss: 1.39451385, g_loss: 0.72748303\n",
      "Step: [1059] d_loss: 1.37515986, g_loss: 0.73502707\n",
      "Step: [1060] d_loss: 1.36565447, g_loss: 0.72967166\n",
      "Step: [1061] d_loss: 1.39047933, g_loss: 0.72026885\n",
      "Step: [1062] d_loss: 1.39017773, g_loss: 0.72208935\n",
      "Step: [1063] d_loss: 1.38455045, g_loss: 0.72427011\n",
      "Step: [1064] d_loss: 1.37849128, g_loss: 0.74218839\n",
      "Step: [1065] d_loss: 1.37697721, g_loss: 0.72832757\n",
      "Step: [1066] d_loss: 1.37510514, g_loss: 0.72236514\n",
      "Step: [1067] d_loss: 1.39028215, g_loss: 0.72632152\n",
      "Step: [1068] d_loss: 1.37905741, g_loss: 0.72924066\n",
      "Step: [1069] d_loss: 1.38455772, g_loss: 0.73201478\n",
      "Step: [1070] d_loss: 1.36616755, g_loss: 0.74579012\n",
      "Step: [1071] d_loss: 1.37881899, g_loss: 0.73636591\n",
      "Step: [1072] d_loss: 1.36589217, g_loss: 0.73476374\n",
      "Step: [1073] d_loss: 1.37187648, g_loss: 0.74001694\n",
      "Step: [1074] d_loss: 1.37913179, g_loss: 0.73640573\n",
      "Step: [1075] d_loss: 1.36507154, g_loss: 0.74247968\n",
      "Step: [1076] d_loss: 1.36297631, g_loss: 0.73629701\n",
      "Step: [1077] d_loss: 1.36796880, g_loss: 0.73948109\n",
      "Step: [1078] d_loss: 1.35216057, g_loss: 0.74348140\n",
      "Step: [1079] d_loss: 1.37144005, g_loss: 0.73103356\n",
      "Step: [1080] d_loss: 1.36733842, g_loss: 0.74042153\n",
      "Step: [1081] d_loss: 1.38068318, g_loss: 0.73153198\n",
      "Step: [1082] d_loss: 1.37129092, g_loss: 0.73686934\n",
      "Step: [1083] d_loss: 1.38162565, g_loss: 0.72974575\n",
      "Step: [1084] d_loss: 1.36782408, g_loss: 0.73498857\n",
      "Step: [1085] d_loss: 1.38683617, g_loss: 0.72640675\n",
      "Step: [1086] d_loss: 1.37752795, g_loss: 0.73118007\n",
      "Step: [1087] d_loss: 1.38847613, g_loss: 0.73528141\n",
      "Step: [1088] d_loss: 1.37507963, g_loss: 0.72786421\n",
      "Step: [1089] d_loss: 1.37899995, g_loss: 0.73069108\n",
      "Step: [1090] d_loss: 1.38082719, g_loss: 0.73516148\n",
      "Step: [1091] d_loss: 1.36931562, g_loss: 0.73281693\n",
      "Step: [1092] d_loss: 1.38043702, g_loss: 0.73811042\n",
      "Step: [1093] d_loss: 1.39147174, g_loss: 0.72614920\n",
      "Step: [1094] d_loss: 1.38057852, g_loss: 0.72682476\n",
      "Step: [1095] d_loss: 1.38058519, g_loss: 0.72496295\n",
      "Step: [1096] d_loss: 1.38416791, g_loss: 0.72723031\n",
      "Step: [1097] d_loss: 1.36514378, g_loss: 0.73610771\n",
      "Step: [1098] d_loss: 1.37214065, g_loss: 0.73206449\n",
      "Step: [1099] d_loss: 1.37445819, g_loss: 0.73899126\n",
      "Step: [1100] d_loss: 1.38183808, g_loss: 0.74099314\n",
      "Step: [1101] d_loss: 1.37189555, g_loss: 0.73419499\n",
      "Step: [1102] d_loss: 1.37020731, g_loss: 0.73061132\n",
      "Step: [1103] d_loss: 1.38726783, g_loss: 0.72444010\n",
      "Step: [1104] d_loss: 1.38035774, g_loss: 0.73373997\n",
      "Step: [1105] d_loss: 1.39227903, g_loss: 0.73543829\n",
      "Step: [1106] d_loss: 1.39155293, g_loss: 0.73723102\n",
      "Step: [1107] d_loss: 1.36883187, g_loss: 0.73600584\n",
      "Step: [1108] d_loss: 1.38680387, g_loss: 0.72133452\n",
      "Step: [1109] d_loss: 1.37855411, g_loss: 0.73377430\n",
      "Step: [1110] d_loss: 1.37524986, g_loss: 0.73090976\n",
      "Step: [1111] d_loss: 1.37779343, g_loss: 0.73550165\n",
      "Step: [1112] d_loss: 1.36383390, g_loss: 0.73565394\n",
      "Step: [1113] d_loss: 1.37114453, g_loss: 0.73019278\n",
      "Step: [1114] d_loss: 1.38142717, g_loss: 0.73014909\n",
      "Step: [1115] d_loss: 1.37008214, g_loss: 0.73863423\n",
      "Step: [1116] d_loss: 1.37965906, g_loss: 0.73355699\n",
      "Step: [1117] d_loss: 1.37855601, g_loss: 0.72728932\n",
      "Step: [1118] d_loss: 1.38402271, g_loss: 0.74033642\n",
      "Step: [1119] d_loss: 1.38048494, g_loss: 0.73516089\n",
      "Step: [1120] d_loss: 1.36758232, g_loss: 0.73883337\n",
      "Step: [1121] d_loss: 1.38175845, g_loss: 0.72869611\n",
      "Step: [1122] d_loss: 1.38340473, g_loss: 0.72323167\n",
      "Step: [1123] d_loss: 1.37894034, g_loss: 0.72792530\n",
      "Step: [1124] d_loss: 1.38794601, g_loss: 0.72701073\n",
      "Step: [1125] d_loss: 1.37590480, g_loss: 0.72158223\n",
      "Step: [1126] d_loss: 1.38198400, g_loss: 0.72708285\n",
      "Step: [1127] d_loss: 1.38146186, g_loss: 0.72619027\n",
      "Step: [1128] d_loss: 1.37940431, g_loss: 0.72575605\n",
      "Step: [1129] d_loss: 1.37648368, g_loss: 0.73688364\n",
      "Step: [1130] d_loss: 1.38515878, g_loss: 0.73090369\n",
      "Step: [1131] d_loss: 1.38946366, g_loss: 0.72484314\n",
      "Step: [1132] d_loss: 1.37064624, g_loss: 0.73971188\n",
      "Step: [1133] d_loss: 1.36821771, g_loss: 0.73371613\n",
      "Step: [1134] d_loss: 1.37580204, g_loss: 0.72829008\n",
      "Step: [1135] d_loss: 1.38339782, g_loss: 0.72765040\n",
      "Step: [1136] d_loss: 1.37160492, g_loss: 0.72820508\n",
      "Step: [1137] d_loss: 1.36683726, g_loss: 0.74377733\n",
      "Step: [1138] d_loss: 1.38039458, g_loss: 0.72436154\n",
      "Step: [1139] d_loss: 1.37602639, g_loss: 0.73006821\n",
      "Step: [1140] d_loss: 1.36512423, g_loss: 0.73751760\n",
      "Step: [1141] d_loss: 1.37689245, g_loss: 0.73427844\n",
      "Step: [1142] d_loss: 1.37207878, g_loss: 0.73537540\n",
      "Step: [1143] d_loss: 1.39939690, g_loss: 0.71856666\n",
      "Step: [1144] d_loss: 1.36720896, g_loss: 0.74473393\n",
      "Step: [1145] d_loss: 1.38695097, g_loss: 0.72534740\n",
      "Step: [1146] d_loss: 1.38088942, g_loss: 0.73072243\n",
      "Step: [1147] d_loss: 1.38579011, g_loss: 0.73201239\n",
      "Step: [1148] d_loss: 1.36079836, g_loss: 0.74305552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1149] d_loss: 1.37320459, g_loss: 0.74170512\n",
      "Step: [1150] d_loss: 1.37515521, g_loss: 0.72832823\n",
      "Step: [1151] d_loss: 1.37115777, g_loss: 0.74134552\n",
      "Step: [1152] d_loss: 1.38245702, g_loss: 0.72757220\n",
      "Step: [1153] d_loss: 1.38030148, g_loss: 0.73598337\n",
      "Step: [1154] d_loss: 1.36899638, g_loss: 0.73013914\n",
      "Step: [1155] d_loss: 1.36719656, g_loss: 0.73376691\n",
      "Step: [1156] d_loss: 1.38614893, g_loss: 0.72606277\n",
      "Step: [1157] d_loss: 1.38094616, g_loss: 0.72882950\n",
      "Step: [1158] d_loss: 1.38977182, g_loss: 0.72504193\n",
      "Step: [1159] d_loss: 1.37425601, g_loss: 0.73334157\n",
      "Step: [1160] d_loss: 1.36230016, g_loss: 0.73044866\n",
      "Step: [1161] d_loss: 1.37916780, g_loss: 0.73900318\n",
      "Step: [1162] d_loss: 1.37985611, g_loss: 0.72764933\n",
      "Step: [1163] d_loss: 1.38723552, g_loss: 0.72487032\n",
      "Step: [1164] d_loss: 1.37053275, g_loss: 0.74166358\n",
      "Step: [1165] d_loss: 1.37742543, g_loss: 0.73730397\n",
      "Step: [1166] d_loss: 1.37191844, g_loss: 0.73297191\n",
      "Step: [1167] d_loss: 1.37509632, g_loss: 0.73815906\n",
      "Step: [1168] d_loss: 1.37843800, g_loss: 0.73640233\n",
      "Step: [1169] d_loss: 1.39071751, g_loss: 0.72046411\n",
      "Step: [1170] d_loss: 1.37468135, g_loss: 0.73076272\n",
      "Step: [1171] d_loss: 1.38867760, g_loss: 0.72307396\n",
      "Step: [1172] d_loss: 1.39082658, g_loss: 0.72452021\n",
      "Step: [1173] d_loss: 1.36695957, g_loss: 0.73096085\n",
      "Step: [1174] d_loss: 1.38233972, g_loss: 0.72984111\n",
      "Step: [1175] d_loss: 1.37057447, g_loss: 0.73351216\n",
      "Step: [1176] d_loss: 1.38223553, g_loss: 0.73207104\n",
      "Step: [1177] d_loss: 1.37680483, g_loss: 0.72381318\n",
      "Step: [1178] d_loss: 1.37024105, g_loss: 0.73918718\n",
      "Step: [1179] d_loss: 1.38241088, g_loss: 0.72968668\n",
      "Step: [1180] d_loss: 1.37136996, g_loss: 0.73549914\n",
      "Step: [1181] d_loss: 1.36550665, g_loss: 0.73032343\n",
      "Step: [1182] d_loss: 1.36382401, g_loss: 0.73846579\n",
      "Step: [1183] d_loss: 1.37025034, g_loss: 0.72583532\n",
      "Step: [1184] d_loss: 1.39159656, g_loss: 0.72259402\n",
      "Step: [1185] d_loss: 1.38238454, g_loss: 0.72289264\n",
      "Step: [1186] d_loss: 1.40305018, g_loss: 0.71902919\n",
      "Step: [1187] d_loss: 1.39115644, g_loss: 0.71295917\n",
      "Step: [1188] d_loss: 1.38578081, g_loss: 0.72204292\n",
      "Step: [1189] d_loss: 1.37650168, g_loss: 0.72797823\n",
      "Step: [1190] d_loss: 1.38065910, g_loss: 0.73001242\n",
      "Step: [1191] d_loss: 1.38374901, g_loss: 0.72637659\n",
      "Step: [1192] d_loss: 1.39279795, g_loss: 0.72411412\n",
      "Step: [1193] d_loss: 1.37008858, g_loss: 0.73358834\n",
      "Step: [1194] d_loss: 1.38328242, g_loss: 0.72477531\n",
      "Step: [1195] d_loss: 1.37453890, g_loss: 0.73501045\n",
      "Step: [1196] d_loss: 1.36402869, g_loss: 0.73691481\n",
      "Step: [1197] d_loss: 1.36591208, g_loss: 0.73690861\n",
      "Step: [1198] d_loss: 1.36852527, g_loss: 0.73724639\n",
      "Step: [1199] d_loss: 1.36653304, g_loss: 0.74101669\n",
      "Step: [1200] d_loss: 1.37064970, g_loss: 0.73687363\n",
      "Step: [1201] d_loss: 1.36568201, g_loss: 0.73998201\n",
      "Step: [1202] d_loss: 1.37456143, g_loss: 0.73015869\n",
      "Step: [1203] d_loss: 1.37723112, g_loss: 0.73640215\n",
      "Step: [1204] d_loss: 1.39088321, g_loss: 0.73002762\n",
      "Step: [1205] d_loss: 1.38161385, g_loss: 0.73641151\n",
      "Step: [1206] d_loss: 1.36933970, g_loss: 0.73956323\n",
      "Step: [1207] d_loss: 1.37816703, g_loss: 0.72740442\n",
      "Step: [1208] d_loss: 1.39074373, g_loss: 0.71628356\n",
      "Step: [1209] d_loss: 1.37861836, g_loss: 0.73126423\n",
      "Step: [1210] d_loss: 1.38057745, g_loss: 0.72828376\n",
      "Step: [1211] d_loss: 1.37520540, g_loss: 0.72930998\n",
      "Step: [1212] d_loss: 1.37763405, g_loss: 0.73002571\n",
      "Step: [1213] d_loss: 1.38621819, g_loss: 0.73527944\n",
      "Step: [1214] d_loss: 1.38828635, g_loss: 0.72169012\n",
      "Step: [1215] d_loss: 1.37826145, g_loss: 0.73245221\n",
      "Step: [1216] d_loss: 1.37700808, g_loss: 0.74458325\n",
      "Step: [1217] d_loss: 1.38630557, g_loss: 0.73092663\n",
      "Step: [1218] d_loss: 1.36579168, g_loss: 0.74125814\n",
      "Step: [1219] d_loss: 1.37911725, g_loss: 0.72849852\n",
      "Step: [1220] d_loss: 1.37809134, g_loss: 0.73604643\n",
      "Step: [1221] d_loss: 1.35921741, g_loss: 0.73571277\n",
      "Step: [1222] d_loss: 1.39831018, g_loss: 0.71105003\n",
      "Step: [1223] d_loss: 1.37847567, g_loss: 0.72823757\n",
      "Step: [1224] d_loss: 1.38703895, g_loss: 0.73182112\n",
      "Step: [1225] d_loss: 1.37828803, g_loss: 0.71951234\n",
      "Step: [1226] d_loss: 1.37951517, g_loss: 0.73136652\n",
      "Step: [1227] d_loss: 1.38665283, g_loss: 0.72775918\n",
      "Step: [1228] d_loss: 1.39119232, g_loss: 0.72617126\n",
      "Step: [1229] d_loss: 1.38282585, g_loss: 0.72920656\n",
      "Step: [1230] d_loss: 1.37991476, g_loss: 0.73498189\n",
      "Step: [1231] d_loss: 1.38670182, g_loss: 0.72609401\n",
      "Step: [1232] d_loss: 1.39671469, g_loss: 0.72997284\n",
      "Step: [1233] d_loss: 1.38493526, g_loss: 0.73679000\n",
      "Step: [1234] d_loss: 1.38495970, g_loss: 0.72904366\n",
      "Step: [1235] d_loss: 1.39377499, g_loss: 0.72341931\n",
      "Step: [1236] d_loss: 1.36264861, g_loss: 0.73290062\n",
      "Step: [1237] d_loss: 1.38342047, g_loss: 0.72482729\n",
      "Step: [1238] d_loss: 1.40058815, g_loss: 0.73412788\n",
      "Step: [1239] d_loss: 1.37223589, g_loss: 0.74111700\n",
      "Step: [1240] d_loss: 1.39849448, g_loss: 0.73340952\n",
      "Step: [1241] d_loss: 1.36289966, g_loss: 0.74885654\n",
      "Step: [1242] d_loss: 1.37913394, g_loss: 0.73052478\n",
      "Step: [1243] d_loss: 1.38801467, g_loss: 0.72859097\n",
      "Step: [1244] d_loss: 1.37831700, g_loss: 0.73974669\n",
      "Step: [1245] d_loss: 1.37978601, g_loss: 0.73181099\n",
      "Step: [1246] d_loss: 1.37799132, g_loss: 0.74353421\n",
      "Step: [1247] d_loss: 1.38978946, g_loss: 0.73497838\n",
      "Step: [1248] d_loss: 1.39485085, g_loss: 0.73962104\n",
      "Step: [1249] d_loss: 1.38920474, g_loss: 0.72999966\n",
      "Step: [1250] d_loss: 1.39575326, g_loss: 0.72205591\n",
      "Step: [1251] d_loss: 1.37949252, g_loss: 0.72723043\n",
      "Step: [1252] d_loss: 1.38267004, g_loss: 0.71661627\n",
      "Step: [1253] d_loss: 1.38188303, g_loss: 0.72646081\n",
      "Step: [1254] d_loss: 1.38312626, g_loss: 0.72308278\n",
      "Step: [1255] d_loss: 1.39272320, g_loss: 0.72890043\n",
      "Step: [1256] d_loss: 1.39271688, g_loss: 0.72356021\n",
      "Step: [1257] d_loss: 1.38647294, g_loss: 0.72876000\n",
      "Step: [1258] d_loss: 1.37530565, g_loss: 0.72512078\n",
      "Step: [1259] d_loss: 1.39237916, g_loss: 0.72022080\n",
      "Step: [1260] d_loss: 1.37588501, g_loss: 0.72754550\n",
      "Step: [1261] d_loss: 1.37241292, g_loss: 0.72514069\n",
      "Step: [1262] d_loss: 1.36631858, g_loss: 0.72960711\n",
      "Step: [1263] d_loss: 1.36696362, g_loss: 0.72941494\n",
      "Step: [1264] d_loss: 1.39011550, g_loss: 0.72664952\n",
      "Step: [1265] d_loss: 1.39171934, g_loss: 0.71990865\n",
      "Step: [1266] d_loss: 1.38261068, g_loss: 0.73055279\n",
      "Step: [1267] d_loss: 1.39314699, g_loss: 0.72000378\n",
      "Step: [1268] d_loss: 1.38418770, g_loss: 0.73005497\n",
      "Step: [1269] d_loss: 1.37007368, g_loss: 0.73878276\n",
      "Step: [1270] d_loss: 1.37660122, g_loss: 0.74204540\n",
      "Step: [1271] d_loss: 1.37550414, g_loss: 0.73563492\n",
      "Step: [1272] d_loss: 1.37610376, g_loss: 0.74539375\n",
      "Step: [1273] d_loss: 1.38234556, g_loss: 0.72978878\n",
      "Step: [1274] d_loss: 1.38835871, g_loss: 0.72237885\n",
      "Step: [1275] d_loss: 1.37489283, g_loss: 0.73261029\n",
      "Step: [1276] d_loss: 1.37598538, g_loss: 0.73035181\n",
      "Step: [1277] d_loss: 1.38387370, g_loss: 0.72810745\n",
      "Step: [1278] d_loss: 1.38001895, g_loss: 0.72715068\n",
      "Step: [1279] d_loss: 1.38593090, g_loss: 0.71563709\n",
      "Step: [1280] d_loss: 1.37988496, g_loss: 0.73016578\n",
      "Step: [1281] d_loss: 1.36878276, g_loss: 0.72497308\n",
      "Step: [1282] d_loss: 1.38661993, g_loss: 0.72459984\n",
      "Step: [1283] d_loss: 1.39070976, g_loss: 0.70873749\n",
      "Step: [1284] d_loss: 1.39408362, g_loss: 0.71749127\n",
      "Step: [1285] d_loss: 1.39358711, g_loss: 0.71586347\n",
      "Step: [1286] d_loss: 1.39039469, g_loss: 0.71341276\n",
      "Step: [1287] d_loss: 1.37000835, g_loss: 0.73178101\n",
      "Step: [1288] d_loss: 1.38640761, g_loss: 0.72124326\n",
      "Step: [1289] d_loss: 1.37600899, g_loss: 0.72384584\n",
      "Step: [1290] d_loss: 1.38038516, g_loss: 0.72903585\n",
      "Step: [1291] d_loss: 1.38705730, g_loss: 0.71723169\n",
      "Step: [1292] d_loss: 1.37685275, g_loss: 0.72656006\n",
      "Step: [1293] d_loss: 1.36590469, g_loss: 0.73420930\n",
      "Step: [1294] d_loss: 1.37804449, g_loss: 0.73019886\n",
      "Step: [1295] d_loss: 1.37753916, g_loss: 0.72833568\n",
      "Step: [1296] d_loss: 1.37045717, g_loss: 0.72485667\n",
      "Step: [1297] d_loss: 1.38834476, g_loss: 0.71644664\n",
      "Step: [1298] d_loss: 1.37419081, g_loss: 0.72746098\n",
      "Step: [1299] d_loss: 1.37199461, g_loss: 0.74396706\n",
      "Step: [1300] d_loss: 1.37725735, g_loss: 0.73131198\n",
      "Step: [1301] d_loss: 1.38992667, g_loss: 0.73399192\n",
      "Step: [1302] d_loss: 1.38398230, g_loss: 0.73626626\n",
      "Step: [1303] d_loss: 1.38255048, g_loss: 0.73362041\n",
      "Step: [1304] d_loss: 1.39236546, g_loss: 0.72636318\n",
      "Step: [1305] d_loss: 1.38603973, g_loss: 0.73490977\n",
      "Step: [1306] d_loss: 1.38634062, g_loss: 0.73118532\n",
      "Step: [1307] d_loss: 1.37676609, g_loss: 0.73887825\n",
      "Step: [1308] d_loss: 1.38930118, g_loss: 0.73484892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1309] d_loss: 1.36074555, g_loss: 0.73212212\n",
      "Step: [1310] d_loss: 1.38252687, g_loss: 0.72469848\n",
      "Step: [1311] d_loss: 1.37398696, g_loss: 0.73027593\n",
      "Step: [1312] d_loss: 1.38003480, g_loss: 0.73911667\n",
      "Step: [1313] d_loss: 1.37717772, g_loss: 0.73764074\n",
      "Step: [1314] d_loss: 1.38323593, g_loss: 0.72601247\n",
      "Step: [1315] d_loss: 1.36785173, g_loss: 0.72991645\n",
      "Step: [1316] d_loss: 1.37951326, g_loss: 0.72993839\n",
      "Step: [1317] d_loss: 1.38479698, g_loss: 0.72989362\n",
      "Step: [1318] d_loss: 1.38343561, g_loss: 0.72327721\n",
      "Step: [1319] d_loss: 1.37428343, g_loss: 0.71794033\n",
      "Step: [1320] d_loss: 1.36850047, g_loss: 0.73817086\n",
      "Step: [1321] d_loss: 1.37322414, g_loss: 0.72676414\n",
      "Step: [1322] d_loss: 1.38090050, g_loss: 0.72914070\n",
      "Step: [1323] d_loss: 1.38096642, g_loss: 0.73680586\n",
      "Step: [1324] d_loss: 1.37853372, g_loss: 0.72599453\n",
      "Step: [1325] d_loss: 1.37778032, g_loss: 0.73247641\n",
      "Step: [1326] d_loss: 1.39748955, g_loss: 0.71557355\n",
      "Step: [1327] d_loss: 1.36942387, g_loss: 0.72312140\n",
      "Step: [1328] d_loss: 1.38229024, g_loss: 0.71778738\n",
      "Step: [1329] d_loss: 1.38100600, g_loss: 0.72615409\n",
      "Step: [1330] d_loss: 1.37768984, g_loss: 0.73096257\n",
      "Step: [1331] d_loss: 1.38694549, g_loss: 0.73231423\n",
      "Step: [1332] d_loss: 1.39122415, g_loss: 0.72287875\n",
      "Step: [1333] d_loss: 1.36377215, g_loss: 0.73655552\n",
      "Step: [1334] d_loss: 1.38289726, g_loss: 0.73080826\n",
      "Step: [1335] d_loss: 1.38792717, g_loss: 0.72458297\n",
      "Step: [1336] d_loss: 1.39714861, g_loss: 0.71959460\n",
      "Step: [1337] d_loss: 1.37528169, g_loss: 0.73621500\n",
      "Step: [1338] d_loss: 1.38828993, g_loss: 0.72992402\n",
      "Step: [1339] d_loss: 1.38485503, g_loss: 0.72259343\n",
      "Step: [1340] d_loss: 1.36232305, g_loss: 0.73920846\n",
      "Step: [1341] d_loss: 1.36837685, g_loss: 0.74024117\n",
      "Step: [1342] d_loss: 1.37447906, g_loss: 0.74174899\n",
      "Step: [1343] d_loss: 1.36489081, g_loss: 0.72719818\n",
      "Step: [1344] d_loss: 1.38007438, g_loss: 0.72635370\n",
      "Step: [1345] d_loss: 1.36806083, g_loss: 0.72940248\n",
      "Step: [1346] d_loss: 1.36948800, g_loss: 0.72990108\n",
      "Step: [1347] d_loss: 1.37512231, g_loss: 0.74027789\n",
      "Step: [1348] d_loss: 1.38049459, g_loss: 0.73047376\n",
      "Step: [1349] d_loss: 1.38045740, g_loss: 0.72094434\n",
      "Step: [1350] d_loss: 1.38635874, g_loss: 0.72927296\n",
      "Step: [1351] d_loss: 1.38803029, g_loss: 0.72694421\n",
      "Step: [1352] d_loss: 1.37870121, g_loss: 0.72570670\n",
      "Step: [1353] d_loss: 1.38731933, g_loss: 0.72144192\n",
      "Step: [1354] d_loss: 1.36674356, g_loss: 0.73688459\n",
      "Step: [1355] d_loss: 1.38735402, g_loss: 0.72483909\n",
      "Step: [1356] d_loss: 1.38833737, g_loss: 0.71901733\n",
      "Step: [1357] d_loss: 1.38293219, g_loss: 0.72393942\n",
      "Step: [1358] d_loss: 1.38145375, g_loss: 0.73574644\n",
      "Step: [1359] d_loss: 1.38239658, g_loss: 0.73045230\n",
      "Step: [1360] d_loss: 1.38918173, g_loss: 0.71327275\n",
      "Step: [1361] d_loss: 1.38760352, g_loss: 0.71666586\n",
      "Step: [1362] d_loss: 1.38207269, g_loss: 0.72577536\n",
      "Step: [1363] d_loss: 1.38887191, g_loss: 0.73676467\n",
      "Step: [1364] d_loss: 1.39116025, g_loss: 0.75265068\n",
      "Step: [1365] d_loss: 1.40582848, g_loss: 0.72473681\n",
      "Step: [1366] d_loss: 1.39145255, g_loss: 0.73567206\n",
      "Step: [1367] d_loss: 1.37991464, g_loss: 0.73902774\n",
      "Step: [1368] d_loss: 1.39114130, g_loss: 0.72020280\n",
      "Step: [1369] d_loss: 1.38253021, g_loss: 0.72135079\n",
      "Step: [1370] d_loss: 1.38536859, g_loss: 0.71677178\n",
      "Step: [1371] d_loss: 1.38813782, g_loss: 0.71505868\n",
      "Step: [1372] d_loss: 1.37979817, g_loss: 0.72531295\n",
      "Step: [1373] d_loss: 1.38634801, g_loss: 0.73441207\n",
      "Step: [1374] d_loss: 1.37942719, g_loss: 0.72974837\n",
      "Step: [1375] d_loss: 1.39025331, g_loss: 0.72654629\n",
      "Step: [1376] d_loss: 1.38115048, g_loss: 0.72698605\n",
      "Step: [1377] d_loss: 1.37568200, g_loss: 0.72542989\n",
      "Step: [1378] d_loss: 1.36148667, g_loss: 0.74107844\n",
      "Step: [1379] d_loss: 1.37591004, g_loss: 0.72796500\n",
      "Step: [1380] d_loss: 1.37601984, g_loss: 0.73581553\n",
      "Step: [1381] d_loss: 1.38252747, g_loss: 0.72613275\n",
      "Step: [1382] d_loss: 1.36709547, g_loss: 0.73622334\n",
      "Step: [1383] d_loss: 1.37788868, g_loss: 0.72971469\n",
      "Step: [1384] d_loss: 1.36106634, g_loss: 0.73516721\n",
      "Step: [1385] d_loss: 1.35921681, g_loss: 0.74168181\n",
      "Step: [1386] d_loss: 1.37654495, g_loss: 0.73024523\n",
      "Step: [1387] d_loss: 1.37837505, g_loss: 0.72456312\n",
      "Step: [1388] d_loss: 1.38696814, g_loss: 0.73381901\n",
      "Step: [1389] d_loss: 1.38867903, g_loss: 0.72805601\n",
      "Step: [1390] d_loss: 1.38295949, g_loss: 0.72078377\n",
      "Step: [1391] d_loss: 1.39448881, g_loss: 0.71713471\n",
      "Step: [1392] d_loss: 1.39723206, g_loss: 0.72434455\n",
      "Step: [1393] d_loss: 1.36403525, g_loss: 0.73520601\n",
      "Step: [1394] d_loss: 1.39095461, g_loss: 0.73648852\n",
      "Step: [1395] d_loss: 1.39635742, g_loss: 0.72195327\n",
      "Step: [1396] d_loss: 1.38703811, g_loss: 0.71953702\n",
      "Step: [1397] d_loss: 1.37910843, g_loss: 0.73871511\n",
      "Step: [1398] d_loss: 1.39981580, g_loss: 0.71952718\n",
      "Step: [1399] d_loss: 1.37557578, g_loss: 0.72537845\n",
      "Step: [1400] d_loss: 1.38226485, g_loss: 0.72640687\n",
      "Step: [1401] d_loss: 1.38574314, g_loss: 0.73204124\n",
      "Step: [1402] d_loss: 1.36755872, g_loss: 0.73485082\n",
      "Step: [1403] d_loss: 1.37986100, g_loss: 0.73380744\n",
      "Step: [1404] d_loss: 1.38631272, g_loss: 0.72041011\n",
      "Step: [1405] d_loss: 1.38879979, g_loss: 0.73266923\n",
      "Step: [1406] d_loss: 1.37896919, g_loss: 0.73597610\n",
      "Step: [1407] d_loss: 1.38091755, g_loss: 0.73218334\n",
      "Step: [1408] d_loss: 1.38435268, g_loss: 0.72837543\n",
      "Step: [1409] d_loss: 1.37699938, g_loss: 0.73388737\n",
      "Step: [1410] d_loss: 1.39093709, g_loss: 0.72476512\n",
      "Step: [1411] d_loss: 1.38909626, g_loss: 0.71856916\n",
      "Step: [1412] d_loss: 1.37648797, g_loss: 0.73207361\n",
      "Step: [1413] d_loss: 1.37873638, g_loss: 0.72927147\n",
      "Step: [1414] d_loss: 1.37742507, g_loss: 0.72778839\n",
      "Step: [1415] d_loss: 1.37760603, g_loss: 0.72888768\n",
      "Step: [1416] d_loss: 1.37472701, g_loss: 0.73283154\n",
      "Step: [1417] d_loss: 1.37576771, g_loss: 0.72712535\n",
      "Step: [1418] d_loss: 1.38067436, g_loss: 0.72939360\n",
      "Step: [1419] d_loss: 1.39097548, g_loss: 0.73006910\n",
      "Step: [1420] d_loss: 1.37899041, g_loss: 0.72780681\n",
      "Step: [1421] d_loss: 1.35920763, g_loss: 0.73834109\n",
      "Step: [1422] d_loss: 1.37334406, g_loss: 0.73451155\n",
      "Step: [1423] d_loss: 1.36381078, g_loss: 0.73438555\n",
      "Step: [1424] d_loss: 1.37853003, g_loss: 0.73811007\n",
      "Step: [1425] d_loss: 1.37475717, g_loss: 0.72730577\n",
      "Step: [1426] d_loss: 1.37375903, g_loss: 0.73911273\n",
      "Step: [1427] d_loss: 1.39423132, g_loss: 0.72152257\n",
      "Step: [1428] d_loss: 1.37951326, g_loss: 0.72376251\n",
      "Step: [1429] d_loss: 1.38374317, g_loss: 0.72860587\n",
      "Step: [1430] d_loss: 1.37156320, g_loss: 0.73995841\n",
      "Step: [1431] d_loss: 1.38398778, g_loss: 0.73405075\n",
      "Step: [1432] d_loss: 1.37396693, g_loss: 0.73625171\n",
      "Step: [1433] d_loss: 1.38557100, g_loss: 0.72620434\n",
      "Step: [1434] d_loss: 1.38800168, g_loss: 0.72851723\n",
      "Step: [1435] d_loss: 1.37702084, g_loss: 0.72575921\n",
      "Step: [1436] d_loss: 1.38752675, g_loss: 0.72173083\n",
      "Step: [1437] d_loss: 1.38388526, g_loss: 0.72802508\n",
      "Step: [1438] d_loss: 1.37052047, g_loss: 0.73231363\n",
      "Step: [1439] d_loss: 1.39157236, g_loss: 0.72028774\n",
      "Step: [1440] d_loss: 1.39449954, g_loss: 0.72615361\n",
      "Step: [1441] d_loss: 1.38573742, g_loss: 0.73256683\n",
      "Step: [1442] d_loss: 1.36984718, g_loss: 0.72697693\n",
      "Step: [1443] d_loss: 1.38396263, g_loss: 0.72490621\n",
      "Step: [1444] d_loss: 1.38216925, g_loss: 0.73025072\n",
      "Step: [1445] d_loss: 1.38666534, g_loss: 0.72048044\n",
      "Step: [1446] d_loss: 1.38335013, g_loss: 0.72420382\n",
      "Step: [1447] d_loss: 1.37759924, g_loss: 0.73042178\n",
      "Step: [1448] d_loss: 1.36319470, g_loss: 0.73566139\n",
      "Step: [1449] d_loss: 1.37541878, g_loss: 0.73960245\n",
      "Step: [1450] d_loss: 1.38580132, g_loss: 0.72803795\n",
      "Step: [1451] d_loss: 1.38599443, g_loss: 0.73107326\n",
      "Step: [1452] d_loss: 1.36064947, g_loss: 0.73615384\n",
      "Step: [1453] d_loss: 1.37644529, g_loss: 0.72121859\n",
      "Step: [1454] d_loss: 1.38466644, g_loss: 0.72558975\n",
      "Step: [1455] d_loss: 1.36510789, g_loss: 0.74138379\n",
      "Step: [1456] d_loss: 1.37095845, g_loss: 0.72770524\n",
      "Step: [1457] d_loss: 1.38356745, g_loss: 0.72464252\n",
      "Step: [1458] d_loss: 1.38743794, g_loss: 0.72437555\n",
      "Step: [1459] d_loss: 1.39139020, g_loss: 0.72360551\n",
      "Step: [1460] d_loss: 1.37322021, g_loss: 0.72923917\n",
      "Step: [1461] d_loss: 1.37631130, g_loss: 0.72854984\n",
      "Step: [1462] d_loss: 1.38566530, g_loss: 0.73411745\n",
      "Step: [1463] d_loss: 1.37479103, g_loss: 0.73243546\n",
      "Step: [1464] d_loss: 1.36231959, g_loss: 0.73392665\n",
      "Step: [1465] d_loss: 1.38657165, g_loss: 0.73436892\n",
      "Step: [1466] d_loss: 1.39083695, g_loss: 0.72008520\n",
      "Step: [1467] d_loss: 1.38116288, g_loss: 0.71358544\n",
      "Step: [1468] d_loss: 1.38755584, g_loss: 0.72944152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1469] d_loss: 1.39144516, g_loss: 0.72307485\n",
      "Step: [1470] d_loss: 1.40445960, g_loss: 0.72117281\n",
      "Step: [1471] d_loss: 1.38173866, g_loss: 0.72738326\n",
      "Step: [1472] d_loss: 1.38166738, g_loss: 0.72930241\n",
      "Step: [1473] d_loss: 1.38260365, g_loss: 0.72754657\n",
      "Step: [1474] d_loss: 1.38792777, g_loss: 0.71886146\n",
      "Step: [1475] d_loss: 1.37679052, g_loss: 0.72532111\n",
      "Step: [1476] d_loss: 1.38705337, g_loss: 0.73023540\n",
      "Step: [1477] d_loss: 1.38503718, g_loss: 0.72921431\n",
      "Step: [1478] d_loss: 1.36605918, g_loss: 0.73409635\n",
      "Step: [1479] d_loss: 1.37399936, g_loss: 0.72724336\n",
      "Step: [1480] d_loss: 1.37887800, g_loss: 0.72548139\n",
      "Step: [1481] d_loss: 1.38136518, g_loss: 0.72796941\n",
      "Step: [1482] d_loss: 1.36729121, g_loss: 0.73404819\n",
      "Step: [1483] d_loss: 1.37920547, g_loss: 0.72845632\n",
      "Step: [1484] d_loss: 1.36408949, g_loss: 0.73050326\n",
      "Step: [1485] d_loss: 1.36092472, g_loss: 0.73426425\n",
      "Step: [1486] d_loss: 1.37219477, g_loss: 0.72519398\n",
      "Step: [1487] d_loss: 1.37411022, g_loss: 0.73091447\n",
      "Step: [1488] d_loss: 1.38851190, g_loss: 0.72443819\n",
      "Step: [1489] d_loss: 1.38069808, g_loss: 0.72288334\n",
      "Step: [1490] d_loss: 1.37281108, g_loss: 0.73256016\n",
      "Step: [1491] d_loss: 1.38610864, g_loss: 0.71960926\n",
      "Step: [1492] d_loss: 1.40055728, g_loss: 0.72211587\n",
      "Step: [1493] d_loss: 1.38258958, g_loss: 0.72193527\n",
      "Step: [1494] d_loss: 1.37880492, g_loss: 0.73213768\n",
      "Step: [1495] d_loss: 1.38644481, g_loss: 0.72077972\n",
      "Step: [1496] d_loss: 1.38241982, g_loss: 0.72996783\n",
      "Step: [1497] d_loss: 1.37737775, g_loss: 0.72831386\n",
      "Step: [1498] d_loss: 1.37821531, g_loss: 0.72766161\n",
      "Step: [1499] d_loss: 1.37872314, g_loss: 0.73065484\n",
      "Step: [1500] d_loss: 1.36732888, g_loss: 0.73862839\n",
      "Step: [1501] d_loss: 1.35961413, g_loss: 0.74164045\n",
      "Step: [1502] d_loss: 1.36714649, g_loss: 0.73216259\n",
      "Step: [1503] d_loss: 1.38560104, g_loss: 0.72179317\n",
      "Step: [1504] d_loss: 1.37546325, g_loss: 0.74078137\n",
      "Step: [1505] d_loss: 1.37594497, g_loss: 0.72575516\n",
      "Step: [1506] d_loss: 1.37302732, g_loss: 0.73375869\n",
      "Step: [1507] d_loss: 1.37992191, g_loss: 0.72424012\n",
      "Step: [1508] d_loss: 1.37756252, g_loss: 0.73375231\n",
      "Step: [1509] d_loss: 1.39837933, g_loss: 0.71641767\n",
      "Step: [1510] d_loss: 1.38593316, g_loss: 0.72961253\n",
      "Step: [1511] d_loss: 1.37462640, g_loss: 0.73047411\n",
      "Step: [1512] d_loss: 1.38565576, g_loss: 0.72382665\n",
      "Step: [1513] d_loss: 1.36706781, g_loss: 0.71966082\n",
      "Step: [1514] d_loss: 1.37810981, g_loss: 0.73626405\n",
      "Step: [1515] d_loss: 1.38620627, g_loss: 0.72636271\n",
      "Step: [1516] d_loss: 1.37917233, g_loss: 0.73631263\n",
      "Step: [1517] d_loss: 1.38109088, g_loss: 0.73041546\n",
      "Step: [1518] d_loss: 1.39076102, g_loss: 0.72060865\n",
      "Step: [1519] d_loss: 1.38258910, g_loss: 0.72908831\n",
      "Step: [1520] d_loss: 1.40139890, g_loss: 0.71705008\n",
      "Step: [1521] d_loss: 1.37808061, g_loss: 0.72259861\n",
      "Step: [1522] d_loss: 1.37183559, g_loss: 0.72937673\n",
      "Step: [1523] d_loss: 1.38394189, g_loss: 0.72604883\n",
      "Step: [1524] d_loss: 1.37478209, g_loss: 0.72462404\n",
      "Step: [1525] d_loss: 1.37581789, g_loss: 0.73198080\n",
      "Step: [1526] d_loss: 1.36969149, g_loss: 0.72848821\n",
      "Step: [1527] d_loss: 1.35454798, g_loss: 0.73536134\n",
      "Step: [1528] d_loss: 1.38628244, g_loss: 0.72434068\n",
      "Step: [1529] d_loss: 1.38430893, g_loss: 0.71858186\n",
      "Step: [1530] d_loss: 1.38978791, g_loss: 0.71931541\n",
      "Step: [1531] d_loss: 1.38976002, g_loss: 0.72361058\n",
      "Step: [1532] d_loss: 1.38742828, g_loss: 0.71828389\n",
      "Step: [1533] d_loss: 1.38225818, g_loss: 0.71957922\n",
      "Step: [1534] d_loss: 1.39008462, g_loss: 0.72681105\n",
      "Step: [1535] d_loss: 1.38332963, g_loss: 0.72707856\n",
      "Step: [1536] d_loss: 1.38745940, g_loss: 0.72912771\n",
      "Step: [1537] d_loss: 1.38298750, g_loss: 0.73134053\n",
      "Step: [1538] d_loss: 1.37680626, g_loss: 0.73854935\n",
      "Step: [1539] d_loss: 1.39037108, g_loss: 0.73354304\n",
      "Step: [1540] d_loss: 1.39459181, g_loss: 0.72803915\n",
      "Step: [1541] d_loss: 1.40539896, g_loss: 0.71132469\n",
      "Step: [1542] d_loss: 1.38321710, g_loss: 0.72058630\n",
      "Step: [1543] d_loss: 1.37791228, g_loss: 0.72597843\n",
      "Step: [1544] d_loss: 1.37234116, g_loss: 0.73308265\n",
      "Step: [1545] d_loss: 1.37829232, g_loss: 0.72861987\n",
      "Step: [1546] d_loss: 1.37660563, g_loss: 0.72999072\n",
      "Step: [1547] d_loss: 1.38479865, g_loss: 0.71737850\n",
      "Step: [1548] d_loss: 1.37871826, g_loss: 0.72623944\n",
      "Step: [1549] d_loss: 1.37217283, g_loss: 0.72998530\n",
      "Step: [1550] d_loss: 1.38228154, g_loss: 0.72571540\n",
      "Step: [1551] d_loss: 1.38457370, g_loss: 0.72510958\n",
      "Step: [1552] d_loss: 1.37564051, g_loss: 0.72935420\n",
      "Step: [1553] d_loss: 1.36505163, g_loss: 0.73605120\n",
      "Step: [1554] d_loss: 1.36604977, g_loss: 0.73641574\n",
      "Step: [1555] d_loss: 1.37820363, g_loss: 0.72280967\n",
      "Step: [1556] d_loss: 1.38562274, g_loss: 0.73365986\n",
      "Step: [1557] d_loss: 1.37290621, g_loss: 0.73070407\n",
      "Step: [1558] d_loss: 1.36791992, g_loss: 0.72093737\n",
      "Step: [1559] d_loss: 1.37614512, g_loss: 0.73148131\n",
      "Step: [1560] d_loss: 1.38235223, g_loss: 0.72086692\n",
      "Step: [1561] d_loss: 1.38741350, g_loss: 0.72053367\n",
      "Step: [1562] d_loss: 1.38157296, g_loss: 0.72870839\n",
      "Step: [1563] d_loss: 1.36553979, g_loss: 0.73278040\n",
      "Step: [1564] d_loss: 1.37525749, g_loss: 0.72723150\n",
      "Step: [1565] d_loss: 1.39145744, g_loss: 0.72431427\n",
      "Step: [1566] d_loss: 1.37121665, g_loss: 0.74160433\n",
      "Step: [1567] d_loss: 1.36312628, g_loss: 0.74007010\n",
      "Step: [1568] d_loss: 1.39616299, g_loss: 0.71407914\n",
      "Step: [1569] d_loss: 1.38032722, g_loss: 0.72159398\n",
      "Step: [1570] d_loss: 1.37570143, g_loss: 0.72831070\n",
      "Step: [1571] d_loss: 1.36956787, g_loss: 0.72884297\n",
      "Step: [1572] d_loss: 1.39889979, g_loss: 0.71185052\n",
      "Step: [1573] d_loss: 1.38345385, g_loss: 0.72712624\n",
      "Step: [1574] d_loss: 1.38821101, g_loss: 0.72137278\n",
      "Step: [1575] d_loss: 1.37664843, g_loss: 0.72785008\n",
      "Step: [1576] d_loss: 1.37663436, g_loss: 0.73238599\n",
      "Step: [1577] d_loss: 1.38075638, g_loss: 0.72833252\n",
      "Step: [1578] d_loss: 1.39015055, g_loss: 0.72072887\n",
      "Step: [1579] d_loss: 1.38363695, g_loss: 0.72293520\n",
      "Step: [1580] d_loss: 1.36721146, g_loss: 0.73111272\n",
      "Step: [1581] d_loss: 1.36457920, g_loss: 0.73664266\n",
      "Step: [1582] d_loss: 1.37857902, g_loss: 0.72524750\n",
      "Step: [1583] d_loss: 1.36599481, g_loss: 0.72218096\n",
      "Step: [1584] d_loss: 1.38274479, g_loss: 0.72826499\n",
      "Step: [1585] d_loss: 1.38186038, g_loss: 0.71584707\n",
      "Step: [1586] d_loss: 1.37612915, g_loss: 0.73301655\n",
      "Step: [1587] d_loss: 1.37564051, g_loss: 0.72591972\n",
      "Step: [1588] d_loss: 1.36256361, g_loss: 0.74239904\n",
      "Step: [1589] d_loss: 1.39502478, g_loss: 0.72099340\n",
      "Step: [1590] d_loss: 1.37855792, g_loss: 0.73199880\n",
      "Step: [1591] d_loss: 1.38512003, g_loss: 0.72497797\n",
      "Step: [1592] d_loss: 1.37827802, g_loss: 0.72376454\n",
      "Step: [1593] d_loss: 1.38587666, g_loss: 0.72192681\n",
      "Step: [1594] d_loss: 1.37725151, g_loss: 0.72645354\n",
      "Step: [1595] d_loss: 1.40214944, g_loss: 0.71334040\n",
      "Step: [1596] d_loss: 1.38863111, g_loss: 0.72481364\n",
      "Step: [1597] d_loss: 1.37489867, g_loss: 0.72327137\n",
      "Step: [1598] d_loss: 1.39409673, g_loss: 0.72326732\n",
      "Step: [1599] d_loss: 1.38033295, g_loss: 0.72506034\n",
      "Step: [1600] d_loss: 1.38378894, g_loss: 0.72147357\n",
      "Step: [1601] d_loss: 1.38465011, g_loss: 0.72573352\n",
      "Step: [1602] d_loss: 1.38972688, g_loss: 0.72806537\n",
      "Step: [1603] d_loss: 1.38283730, g_loss: 0.72585905\n",
      "Step: [1604] d_loss: 1.37175536, g_loss: 0.72547078\n",
      "Step: [1605] d_loss: 1.37625945, g_loss: 0.73310345\n",
      "Step: [1606] d_loss: 1.38559079, g_loss: 0.72749186\n",
      "Step: [1607] d_loss: 1.37105334, g_loss: 0.73234290\n",
      "Step: [1608] d_loss: 1.37851357, g_loss: 0.73297220\n",
      "Step: [1609] d_loss: 1.36703384, g_loss: 0.73249781\n",
      "Step: [1610] d_loss: 1.37187839, g_loss: 0.73225284\n",
      "Step: [1611] d_loss: 1.38005495, g_loss: 0.73069596\n",
      "Step: [1612] d_loss: 1.38605356, g_loss: 0.73376286\n",
      "Step: [1613] d_loss: 1.37209964, g_loss: 0.73537177\n",
      "Step: [1614] d_loss: 1.38917983, g_loss: 0.71921194\n",
      "Step: [1615] d_loss: 1.37961268, g_loss: 0.72151476\n",
      "Step: [1616] d_loss: 1.38136840, g_loss: 0.73383695\n",
      "Step: [1617] d_loss: 1.36813116, g_loss: 0.72970355\n",
      "Step: [1618] d_loss: 1.37308943, g_loss: 0.72799289\n",
      "Step: [1619] d_loss: 1.36318564, g_loss: 0.73356533\n",
      "Step: [1620] d_loss: 1.35414267, g_loss: 0.74397886\n",
      "Step: [1621] d_loss: 1.36708736, g_loss: 0.73663360\n",
      "Step: [1622] d_loss: 1.38207662, g_loss: 0.73266000\n",
      "Step: [1623] d_loss: 1.40195823, g_loss: 0.72125864\n",
      "Step: [1624] d_loss: 1.39439666, g_loss: 0.72316897\n",
      "Step: [1625] d_loss: 1.35771513, g_loss: 0.73705983\n",
      "Step: [1626] d_loss: 1.38587081, g_loss: 0.71959269\n",
      "Step: [1627] d_loss: 1.38522947, g_loss: 0.72303641\n",
      "Step: [1628] d_loss: 1.38978267, g_loss: 0.72975039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1629] d_loss: 1.39590454, g_loss: 0.72437096\n",
      "Step: [1630] d_loss: 1.39257240, g_loss: 0.73352778\n",
      "Step: [1631] d_loss: 1.37350154, g_loss: 0.73039442\n",
      "Step: [1632] d_loss: 1.39579022, g_loss: 0.73095644\n",
      "Step: [1633] d_loss: 1.35984683, g_loss: 0.73410618\n",
      "Step: [1634] d_loss: 1.37601197, g_loss: 0.72880983\n",
      "Step: [1635] d_loss: 1.38097262, g_loss: 0.72452098\n",
      "Step: [1636] d_loss: 1.38168025, g_loss: 0.73277622\n",
      "Step: [1637] d_loss: 1.36836278, g_loss: 0.73268580\n",
      "Step: [1638] d_loss: 1.39373171, g_loss: 0.72382063\n",
      "Step: [1639] d_loss: 1.38309979, g_loss: 0.72785282\n",
      "Step: [1640] d_loss: 1.38120723, g_loss: 0.73499525\n",
      "Step: [1641] d_loss: 1.37981868, g_loss: 0.73916769\n",
      "Step: [1642] d_loss: 1.38704491, g_loss: 0.72417259\n",
      "Step: [1643] d_loss: 1.36899877, g_loss: 0.74705815\n",
      "Step: [1644] d_loss: 1.38149798, g_loss: 0.72321868\n",
      "Step: [1645] d_loss: 1.37749302, g_loss: 0.73319405\n",
      "Step: [1646] d_loss: 1.38504517, g_loss: 0.72729099\n",
      "Step: [1647] d_loss: 1.38310552, g_loss: 0.72257447\n",
      "Step: [1648] d_loss: 1.38530147, g_loss: 0.72571206\n",
      "Step: [1649] d_loss: 1.38816094, g_loss: 0.72123110\n",
      "Step: [1650] d_loss: 1.38516402, g_loss: 0.72594309\n",
      "Step: [1651] d_loss: 1.38410211, g_loss: 0.73228776\n",
      "Step: [1652] d_loss: 1.38716209, g_loss: 0.73149955\n",
      "Step: [1653] d_loss: 1.38981557, g_loss: 0.72108054\n",
      "Step: [1654] d_loss: 1.37940133, g_loss: 0.73331439\n",
      "Step: [1655] d_loss: 1.37269449, g_loss: 0.73041606\n",
      "Step: [1656] d_loss: 1.37638617, g_loss: 0.72981048\n",
      "Step: [1657] d_loss: 1.38552964, g_loss: 0.73073906\n",
      "Step: [1658] d_loss: 1.38261616, g_loss: 0.72642589\n",
      "Step: [1659] d_loss: 1.37007701, g_loss: 0.73426938\n",
      "Step: [1660] d_loss: 1.38582373, g_loss: 0.72628373\n",
      "Step: [1661] d_loss: 1.38152814, g_loss: 0.73274350\n",
      "Step: [1662] d_loss: 1.37260056, g_loss: 0.72848493\n",
      "Step: [1663] d_loss: 1.39061093, g_loss: 0.71907818\n",
      "Step: [1664] d_loss: 1.37190568, g_loss: 0.72910243\n",
      "Step: [1665] d_loss: 1.38656974, g_loss: 0.72459060\n",
      "Step: [1666] d_loss: 1.36596537, g_loss: 0.73774672\n",
      "Step: [1667] d_loss: 1.38423300, g_loss: 0.71925879\n",
      "Step: [1668] d_loss: 1.38071108, g_loss: 0.72404015\n",
      "Step: [1669] d_loss: 1.37852740, g_loss: 0.72553688\n",
      "Step: [1670] d_loss: 1.37430036, g_loss: 0.72595906\n",
      "Step: [1671] d_loss: 1.38787627, g_loss: 0.72447598\n",
      "Step: [1672] d_loss: 1.37711930, g_loss: 0.73524857\n",
      "Step: [1673] d_loss: 1.37752461, g_loss: 0.73837775\n",
      "Step: [1674] d_loss: 1.39804935, g_loss: 0.72612715\n",
      "Step: [1675] d_loss: 1.39472890, g_loss: 0.72169018\n",
      "Step: [1676] d_loss: 1.39918542, g_loss: 0.71369636\n",
      "Step: [1677] d_loss: 1.38301921, g_loss: 0.72371018\n",
      "Step: [1678] d_loss: 1.39321494, g_loss: 0.74239838\n",
      "Step: [1679] d_loss: 1.40138292, g_loss: 0.73103690\n",
      "Step: [1680] d_loss: 1.39449954, g_loss: 0.72696471\n",
      "Step: [1681] d_loss: 1.38395762, g_loss: 0.73518938\n",
      "Step: [1682] d_loss: 1.38110685, g_loss: 0.72710371\n",
      "Step: [1683] d_loss: 1.38902879, g_loss: 0.71673763\n",
      "Step: [1684] d_loss: 1.38236451, g_loss: 0.73110378\n",
      "Step: [1685] d_loss: 1.36983299, g_loss: 0.73091221\n",
      "Step: [1686] d_loss: 1.39313495, g_loss: 0.72126949\n",
      "Step: [1687] d_loss: 1.38260651, g_loss: 0.72187322\n",
      "Step: [1688] d_loss: 1.38354778, g_loss: 0.72166938\n",
      "Step: [1689] d_loss: 1.36450434, g_loss: 0.74028718\n",
      "Step: [1690] d_loss: 1.38496804, g_loss: 0.72888947\n",
      "Step: [1691] d_loss: 1.37322843, g_loss: 0.73093987\n",
      "Step: [1692] d_loss: 1.39005733, g_loss: 0.71871692\n",
      "Step: [1693] d_loss: 1.36584377, g_loss: 0.73557317\n",
      "Step: [1694] d_loss: 1.37840354, g_loss: 0.73730624\n",
      "Step: [1695] d_loss: 1.38314939, g_loss: 0.71695113\n",
      "Step: [1696] d_loss: 1.37489855, g_loss: 0.72805184\n",
      "Step: [1697] d_loss: 1.39067578, g_loss: 0.72243053\n",
      "Step: [1698] d_loss: 1.38763070, g_loss: 0.73351741\n",
      "Step: [1699] d_loss: 1.38512826, g_loss: 0.72137898\n",
      "Step: [1700] d_loss: 1.38274074, g_loss: 0.72037518\n",
      "Step: [1701] d_loss: 1.39172792, g_loss: 0.72232687\n",
      "Step: [1702] d_loss: 1.37227893, g_loss: 0.73226261\n",
      "Step: [1703] d_loss: 1.37919843, g_loss: 0.73654437\n",
      "Step: [1704] d_loss: 1.38440919, g_loss: 0.72274268\n",
      "Step: [1705] d_loss: 1.37083483, g_loss: 0.72649127\n",
      "Step: [1706] d_loss: 1.37797058, g_loss: 0.71725726\n",
      "Step: [1707] d_loss: 1.36968255, g_loss: 0.72840232\n",
      "Step: [1708] d_loss: 1.36372542, g_loss: 0.72867578\n",
      "Step: [1709] d_loss: 1.37656903, g_loss: 0.73370516\n",
      "Step: [1710] d_loss: 1.37881315, g_loss: 0.72529268\n",
      "Step: [1711] d_loss: 1.36731219, g_loss: 0.73671770\n",
      "Step: [1712] d_loss: 1.37081134, g_loss: 0.73125362\n",
      "Step: [1713] d_loss: 1.37801552, g_loss: 0.72639227\n",
      "Step: [1714] d_loss: 1.37514496, g_loss: 0.72025526\n",
      "Step: [1715] d_loss: 1.38818192, g_loss: 0.71931887\n",
      "Step: [1716] d_loss: 1.37808180, g_loss: 0.72271359\n",
      "Step: [1717] d_loss: 1.38911796, g_loss: 0.72698569\n",
      "Step: [1718] d_loss: 1.39059782, g_loss: 0.71792758\n",
      "Step: [1719] d_loss: 1.38556194, g_loss: 0.71996480\n",
      "Step: [1720] d_loss: 1.37731194, g_loss: 0.73559231\n",
      "Step: [1721] d_loss: 1.38803577, g_loss: 0.73003507\n",
      "Step: [1722] d_loss: 1.39166892, g_loss: 0.72391880\n",
      "Step: [1723] d_loss: 1.38928199, g_loss: 0.71946973\n",
      "Step: [1724] d_loss: 1.38029981, g_loss: 0.72547567\n",
      "Step: [1725] d_loss: 1.38584459, g_loss: 0.72321451\n",
      "Step: [1726] d_loss: 1.37557495, g_loss: 0.73165631\n",
      "Step: [1727] d_loss: 1.38140488, g_loss: 0.72020257\n",
      "Step: [1728] d_loss: 1.36822748, g_loss: 0.72223926\n",
      "Step: [1729] d_loss: 1.37390256, g_loss: 0.72691107\n",
      "Step: [1730] d_loss: 1.37971616, g_loss: 0.73320293\n",
      "Step: [1731] d_loss: 1.39623868, g_loss: 0.71708155\n",
      "Step: [1732] d_loss: 1.38264394, g_loss: 0.72565389\n",
      "Step: [1733] d_loss: 1.37806082, g_loss: 0.72028899\n",
      "Step: [1734] d_loss: 1.37740028, g_loss: 0.73224175\n",
      "Step: [1735] d_loss: 1.38778257, g_loss: 0.71697485\n",
      "Step: [1736] d_loss: 1.37903857, g_loss: 0.72746420\n",
      "Step: [1737] d_loss: 1.36729419, g_loss: 0.73744202\n",
      "Step: [1738] d_loss: 1.37816811, g_loss: 0.73078603\n",
      "Step: [1739] d_loss: 1.37246418, g_loss: 0.73309076\n",
      "Step: [1740] d_loss: 1.37800193, g_loss: 0.72834921\n",
      "Step: [1741] d_loss: 1.38551307, g_loss: 0.71665859\n",
      "Step: [1742] d_loss: 1.38750339, g_loss: 0.71634257\n",
      "Step: [1743] d_loss: 1.38503885, g_loss: 0.72753751\n",
      "Step: [1744] d_loss: 1.38621581, g_loss: 0.73602676\n",
      "Step: [1745] d_loss: 1.39332712, g_loss: 0.72550666\n",
      "Step: [1746] d_loss: 1.37293577, g_loss: 0.71852994\n",
      "Step: [1747] d_loss: 1.38942039, g_loss: 0.71520448\n",
      "Step: [1748] d_loss: 1.37674344, g_loss: 0.73684531\n",
      "Step: [1749] d_loss: 1.38478720, g_loss: 0.71850181\n",
      "Step: [1750] d_loss: 1.38648343, g_loss: 0.71588469\n",
      "Step: [1751] d_loss: 1.38375974, g_loss: 0.72169018\n",
      "Step: [1752] d_loss: 1.38419044, g_loss: 0.72563946\n",
      "Step: [1753] d_loss: 1.37912607, g_loss: 0.73328477\n",
      "Step: [1754] d_loss: 1.38775790, g_loss: 0.72596824\n",
      "Step: [1755] d_loss: 1.37126601, g_loss: 0.73051637\n",
      "Step: [1756] d_loss: 1.38251483, g_loss: 0.71622753\n",
      "Step: [1757] d_loss: 1.36881244, g_loss: 0.73167938\n",
      "Step: [1758] d_loss: 1.36711359, g_loss: 0.73746049\n",
      "Step: [1759] d_loss: 1.36437595, g_loss: 0.73538804\n",
      "Step: [1760] d_loss: 1.37911725, g_loss: 0.72655648\n",
      "Step: [1761] d_loss: 1.37509966, g_loss: 0.73167390\n",
      "Step: [1762] d_loss: 1.36170769, g_loss: 0.73668957\n",
      "Step: [1763] d_loss: 1.37793541, g_loss: 0.72256011\n",
      "Step: [1764] d_loss: 1.37863016, g_loss: 0.72271734\n",
      "Step: [1765] d_loss: 1.38709641, g_loss: 0.72048700\n",
      "Step: [1766] d_loss: 1.37684369, g_loss: 0.72364259\n",
      "Step: [1767] d_loss: 1.38246346, g_loss: 0.72120404\n",
      "Step: [1768] d_loss: 1.38774180, g_loss: 0.72423077\n",
      "Step: [1769] d_loss: 1.38597393, g_loss: 0.72016764\n",
      "Step: [1770] d_loss: 1.39000010, g_loss: 0.72426748\n",
      "Step: [1771] d_loss: 1.38723540, g_loss: 0.71835667\n",
      "Step: [1772] d_loss: 1.38741684, g_loss: 0.72567034\n",
      "Step: [1773] d_loss: 1.37404072, g_loss: 0.72788048\n",
      "Step: [1774] d_loss: 1.37188172, g_loss: 0.71982247\n",
      "Step: [1775] d_loss: 1.37139726, g_loss: 0.72971416\n",
      "Step: [1776] d_loss: 1.38260126, g_loss: 0.72376925\n",
      "Step: [1777] d_loss: 1.37178802, g_loss: 0.72802371\n",
      "Step: [1778] d_loss: 1.38868165, g_loss: 0.71607149\n",
      "Step: [1779] d_loss: 1.37264204, g_loss: 0.72639638\n",
      "Step: [1780] d_loss: 1.38162351, g_loss: 0.71947908\n",
      "Step: [1781] d_loss: 1.38572121, g_loss: 0.72665411\n",
      "Step: [1782] d_loss: 1.37894833, g_loss: 0.72647429\n",
      "Step: [1783] d_loss: 1.37575996, g_loss: 0.72820795\n",
      "Step: [1784] d_loss: 1.38217819, g_loss: 0.71996433\n",
      "Step: [1785] d_loss: 1.38463628, g_loss: 0.72119296\n",
      "Step: [1786] d_loss: 1.36639512, g_loss: 0.73320794\n",
      "Step: [1787] d_loss: 1.37627804, g_loss: 0.72670066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1788] d_loss: 1.37607253, g_loss: 0.72648430\n",
      "Step: [1789] d_loss: 1.37616014, g_loss: 0.72902858\n",
      "Step: [1790] d_loss: 1.38272238, g_loss: 0.72313249\n",
      "Step: [1791] d_loss: 1.36731386, g_loss: 0.73555529\n",
      "Step: [1792] d_loss: 1.37892091, g_loss: 0.73190737\n",
      "Step: [1793] d_loss: 1.35553622, g_loss: 0.74172807\n",
      "Step: [1794] d_loss: 1.37198138, g_loss: 0.73042446\n",
      "Step: [1795] d_loss: 1.38172519, g_loss: 0.72126877\n",
      "Step: [1796] d_loss: 1.37976301, g_loss: 0.72260582\n",
      "Step: [1797] d_loss: 1.37484884, g_loss: 0.72340131\n",
      "Step: [1798] d_loss: 1.37466013, g_loss: 0.72513878\n",
      "Step: [1799] d_loss: 1.39511549, g_loss: 0.71993691\n",
      "Step: [1800] d_loss: 1.41353059, g_loss: 0.72125715\n",
      "Step: [1801] d_loss: 1.38925648, g_loss: 0.72827399\n",
      "Step: [1802] d_loss: 1.39278340, g_loss: 0.71780205\n",
      "Step: [1803] d_loss: 1.37188494, g_loss: 0.72890931\n",
      "Step: [1804] d_loss: 1.36807323, g_loss: 0.73520029\n",
      "Step: [1805] d_loss: 1.39595509, g_loss: 0.72557950\n",
      "Step: [1806] d_loss: 1.40782905, g_loss: 0.72149342\n",
      "Step: [1807] d_loss: 1.38852715, g_loss: 0.72517586\n",
      "Step: [1808] d_loss: 1.37869668, g_loss: 0.72664523\n",
      "Step: [1809] d_loss: 1.38506734, g_loss: 0.72235864\n",
      "Step: [1810] d_loss: 1.37917638, g_loss: 0.72376728\n",
      "Step: [1811] d_loss: 1.37845802, g_loss: 0.73109865\n",
      "Step: [1812] d_loss: 1.36840463, g_loss: 0.73008847\n",
      "Step: [1813] d_loss: 1.37366557, g_loss: 0.72888356\n",
      "Step: [1814] d_loss: 1.37755132, g_loss: 0.73194706\n",
      "Step: [1815] d_loss: 1.36652148, g_loss: 0.74220896\n",
      "Step: [1816] d_loss: 1.36129642, g_loss: 0.73001838\n",
      "Step: [1817] d_loss: 1.37991714, g_loss: 0.72666991\n",
      "Step: [1818] d_loss: 1.39018536, g_loss: 0.72669268\n",
      "Step: [1819] d_loss: 1.39205968, g_loss: 0.72519571\n",
      "Step: [1820] d_loss: 1.39131200, g_loss: 0.72475529\n",
      "Step: [1821] d_loss: 1.38766778, g_loss: 0.72082710\n",
      "Step: [1822] d_loss: 1.37425649, g_loss: 0.72525769\n",
      "Step: [1823] d_loss: 1.36112428, g_loss: 0.73752725\n",
      "Step: [1824] d_loss: 1.37711048, g_loss: 0.72558200\n",
      "Step: [1825] d_loss: 1.37954998, g_loss: 0.72499406\n",
      "Step: [1826] d_loss: 1.38024735, g_loss: 0.72690678\n",
      "Step: [1827] d_loss: 1.38373995, g_loss: 0.72415680\n",
      "Step: [1828] d_loss: 1.36860538, g_loss: 0.73402560\n",
      "Step: [1829] d_loss: 1.36414909, g_loss: 0.73480082\n",
      "Step: [1830] d_loss: 1.36507058, g_loss: 0.72580487\n",
      "Step: [1831] d_loss: 1.37767398, g_loss: 0.73004329\n",
      "Step: [1832] d_loss: 1.35532022, g_loss: 0.73691952\n",
      "Step: [1833] d_loss: 1.37178683, g_loss: 0.72906595\n",
      "Step: [1834] d_loss: 1.36955333, g_loss: 0.73878038\n",
      "Step: [1835] d_loss: 1.39030874, g_loss: 0.73516107\n",
      "Step: [1836] d_loss: 1.36937845, g_loss: 0.71811557\n",
      "Step: [1837] d_loss: 1.39593613, g_loss: 0.72123915\n",
      "Step: [1838] d_loss: 1.37644005, g_loss: 0.73281813\n",
      "Step: [1839] d_loss: 1.37969565, g_loss: 0.72586846\n",
      "Step: [1840] d_loss: 1.39590406, g_loss: 0.72453272\n",
      "Step: [1841] d_loss: 1.39917803, g_loss: 0.72302783\n",
      "Step: [1842] d_loss: 1.39556980, g_loss: 0.71934360\n",
      "Step: [1843] d_loss: 1.38876319, g_loss: 0.72729325\n",
      "Step: [1844] d_loss: 1.40193534, g_loss: 0.71456766\n",
      "Step: [1845] d_loss: 1.38225389, g_loss: 0.72216552\n",
      "Step: [1846] d_loss: 1.37440109, g_loss: 0.73110211\n",
      "Step: [1847] d_loss: 1.38371134, g_loss: 0.73046440\n",
      "Step: [1848] d_loss: 1.39010406, g_loss: 0.73245835\n",
      "Step: [1849] d_loss: 1.38313532, g_loss: 0.72406906\n",
      "Step: [1850] d_loss: 1.40059435, g_loss: 0.72171324\n",
      "Step: [1851] d_loss: 1.38300264, g_loss: 0.73090827\n",
      "Step: [1852] d_loss: 1.36349273, g_loss: 0.72860599\n",
      "Step: [1853] d_loss: 1.38129222, g_loss: 0.72106344\n",
      "Step: [1854] d_loss: 1.38890660, g_loss: 0.71922910\n",
      "Step: [1855] d_loss: 1.38162398, g_loss: 0.72383994\n",
      "Step: [1856] d_loss: 1.38452315, g_loss: 0.72632462\n",
      "Step: [1857] d_loss: 1.39322960, g_loss: 0.72024721\n",
      "Step: [1858] d_loss: 1.37838292, g_loss: 0.72678560\n",
      "Step: [1859] d_loss: 1.37163460, g_loss: 0.73133218\n",
      "Step: [1860] d_loss: 1.37822819, g_loss: 0.72620845\n",
      "Step: [1861] d_loss: 1.38360691, g_loss: 0.72361094\n",
      "Step: [1862] d_loss: 1.38421619, g_loss: 0.72153223\n",
      "Step: [1863] d_loss: 1.37472963, g_loss: 0.72630799\n",
      "Step: [1864] d_loss: 1.39733052, g_loss: 0.72309005\n",
      "Step: [1865] d_loss: 1.37216198, g_loss: 0.72853976\n",
      "Step: [1866] d_loss: 1.39458370, g_loss: 0.71553028\n",
      "Step: [1867] d_loss: 1.39096570, g_loss: 0.72225225\n",
      "Step: [1868] d_loss: 1.39112282, g_loss: 0.72462624\n",
      "Step: [1869] d_loss: 1.39479780, g_loss: 0.72767097\n",
      "Step: [1870] d_loss: 1.38652372, g_loss: 0.72154605\n",
      "Step: [1871] d_loss: 1.39575994, g_loss: 0.71220565\n",
      "Step: [1872] d_loss: 1.39492345, g_loss: 0.71938246\n",
      "Step: [1873] d_loss: 1.37901378, g_loss: 0.73128796\n",
      "Step: [1874] d_loss: 1.38552618, g_loss: 0.71763229\n",
      "Step: [1875] d_loss: 1.36742592, g_loss: 0.73178494\n",
      "Step: [1876] d_loss: 1.37988734, g_loss: 0.72379094\n",
      "Step: [1877] d_loss: 1.37290215, g_loss: 0.72471499\n",
      "Step: [1878] d_loss: 1.36948752, g_loss: 0.72934210\n",
      "Step: [1879] d_loss: 1.37090182, g_loss: 0.72832519\n",
      "Step: [1880] d_loss: 1.38189554, g_loss: 0.73210096\n",
      "Step: [1881] d_loss: 1.38532639, g_loss: 0.72903711\n",
      "Step: [1882] d_loss: 1.38055503, g_loss: 0.71927345\n",
      "Step: [1883] d_loss: 1.38605547, g_loss: 0.72640979\n",
      "Step: [1884] d_loss: 1.37772179, g_loss: 0.72603285\n",
      "Step: [1885] d_loss: 1.38092649, g_loss: 0.72132695\n",
      "Step: [1886] d_loss: 1.37430501, g_loss: 0.73231363\n",
      "Step: [1887] d_loss: 1.39258122, g_loss: 0.72797126\n",
      "Step: [1888] d_loss: 1.41084683, g_loss: 0.71286517\n",
      "Step: [1889] d_loss: 1.37901556, g_loss: 0.72165453\n",
      "Step: [1890] d_loss: 1.38339067, g_loss: 0.72371042\n",
      "Step: [1891] d_loss: 1.38695061, g_loss: 0.73389995\n",
      "Step: [1892] d_loss: 1.39903688, g_loss: 0.71904755\n",
      "Step: [1893] d_loss: 1.38078523, g_loss: 0.72717524\n",
      "Step: [1894] d_loss: 1.38086498, g_loss: 0.72535217\n",
      "Step: [1895] d_loss: 1.39277124, g_loss: 0.71805477\n",
      "Step: [1896] d_loss: 1.38216341, g_loss: 0.72536397\n",
      "Step: [1897] d_loss: 1.38875699, g_loss: 0.71718740\n",
      "Step: [1898] d_loss: 1.38152444, g_loss: 0.72290325\n",
      "Step: [1899] d_loss: 1.37759078, g_loss: 0.73104346\n",
      "Step: [1900] d_loss: 1.39237094, g_loss: 0.72564733\n",
      "Step: [1901] d_loss: 1.36561489, g_loss: 0.73234582\n",
      "Step: [1902] d_loss: 1.36521959, g_loss: 0.73271251\n",
      "Step: [1903] d_loss: 1.36050463, g_loss: 0.73512548\n",
      "Step: [1904] d_loss: 1.36485076, g_loss: 0.73178160\n",
      "Step: [1905] d_loss: 1.37292600, g_loss: 0.72586215\n",
      "Step: [1906] d_loss: 1.37541771, g_loss: 0.73097515\n",
      "Step: [1907] d_loss: 1.35573292, g_loss: 0.74637914\n",
      "Step: [1908] d_loss: 1.36422026, g_loss: 0.73109710\n",
      "Step: [1909] d_loss: 1.38544786, g_loss: 0.72556877\n",
      "Step: [1910] d_loss: 1.36833763, g_loss: 0.73943144\n",
      "Step: [1911] d_loss: 1.40985608, g_loss: 0.72639602\n",
      "Step: [1912] d_loss: 1.40766454, g_loss: 0.72600776\n",
      "Step: [1913] d_loss: 1.39625931, g_loss: 0.71732771\n",
      "Step: [1914] d_loss: 1.38566184, g_loss: 0.71487433\n",
      "Step: [1915] d_loss: 1.38030887, g_loss: 0.72705513\n",
      "Step: [1916] d_loss: 1.39011002, g_loss: 0.72141242\n",
      "Step: [1917] d_loss: 1.36794722, g_loss: 0.72434723\n",
      "Step: [1918] d_loss: 1.38227439, g_loss: 0.72344971\n",
      "Step: [1919] d_loss: 1.37956738, g_loss: 0.73111027\n",
      "Step: [1920] d_loss: 1.37139595, g_loss: 0.72428250\n",
      "Step: [1921] d_loss: 1.38721061, g_loss: 0.72376215\n",
      "Step: [1922] d_loss: 1.36890841, g_loss: 0.72537923\n",
      "Step: [1923] d_loss: 1.40013242, g_loss: 0.70880306\n",
      "Step: [1924] d_loss: 1.38780332, g_loss: 0.71714509\n",
      "Step: [1925] d_loss: 1.38925099, g_loss: 0.72165865\n",
      "Step: [1926] d_loss: 1.37788808, g_loss: 0.72196281\n",
      "Step: [1927] d_loss: 1.38119805, g_loss: 0.71915883\n",
      "Step: [1928] d_loss: 1.38637483, g_loss: 0.72462273\n",
      "Step: [1929] d_loss: 1.39548922, g_loss: 0.72013551\n",
      "Step: [1930] d_loss: 1.37510681, g_loss: 0.73004609\n",
      "Step: [1931] d_loss: 1.37530637, g_loss: 0.73083860\n",
      "Step: [1932] d_loss: 1.37135029, g_loss: 0.73012424\n",
      "Step: [1933] d_loss: 1.37912595, g_loss: 0.72842813\n",
      "Step: [1934] d_loss: 1.37002254, g_loss: 0.72528142\n",
      "Step: [1935] d_loss: 1.39155996, g_loss: 0.72827917\n",
      "Step: [1936] d_loss: 1.36378789, g_loss: 0.72795147\n",
      "Step: [1937] d_loss: 1.38143802, g_loss: 0.72597998\n",
      "Step: [1938] d_loss: 1.38791108, g_loss: 0.71536070\n",
      "Step: [1939] d_loss: 1.36836660, g_loss: 0.73399383\n",
      "Step: [1940] d_loss: 1.37329435, g_loss: 0.73422480\n",
      "Step: [1941] d_loss: 1.38195062, g_loss: 0.72194195\n",
      "Step: [1942] d_loss: 1.37646639, g_loss: 0.72604078\n",
      "Step: [1943] d_loss: 1.38408756, g_loss: 0.72976512\n",
      "Step: [1944] d_loss: 1.37126911, g_loss: 0.73641860\n",
      "Step: [1945] d_loss: 1.37110662, g_loss: 0.72676706\n",
      "Step: [1946] d_loss: 1.37844133, g_loss: 0.73409379\n",
      "Step: [1947] d_loss: 1.36986756, g_loss: 0.73325205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1948] d_loss: 1.37405539, g_loss: 0.73206317\n",
      "Step: [1949] d_loss: 1.37761092, g_loss: 0.72819924\n",
      "Step: [1950] d_loss: 1.36702728, g_loss: 0.74051726\n",
      "Step: [1951] d_loss: 1.37009597, g_loss: 0.72633278\n",
      "Step: [1952] d_loss: 1.37777638, g_loss: 0.71872526\n",
      "Step: [1953] d_loss: 1.38431549, g_loss: 0.71984816\n",
      "Step: [1954] d_loss: 1.38641286, g_loss: 0.72278154\n",
      "Step: [1955] d_loss: 1.38187873, g_loss: 0.72298157\n",
      "Step: [1956] d_loss: 1.38270688, g_loss: 0.72921222\n",
      "Step: [1957] d_loss: 1.38000000, g_loss: 0.73231030\n",
      "Step: [1958] d_loss: 1.38561511, g_loss: 0.72294050\n",
      "Step: [1959] d_loss: 1.39219570, g_loss: 0.71563172\n",
      "Step: [1960] d_loss: 1.38437867, g_loss: 0.72051764\n",
      "Step: [1961] d_loss: 1.38840771, g_loss: 0.71579504\n",
      "Step: [1962] d_loss: 1.38689327, g_loss: 0.71718228\n",
      "Step: [1963] d_loss: 1.36730886, g_loss: 0.73302889\n",
      "Step: [1964] d_loss: 1.37130713, g_loss: 0.73208600\n",
      "Step: [1965] d_loss: 1.39537287, g_loss: 0.71442676\n",
      "Step: [1966] d_loss: 1.39721203, g_loss: 0.71422076\n",
      "Step: [1967] d_loss: 1.38305140, g_loss: 0.73084146\n",
      "Step: [1968] d_loss: 1.38270998, g_loss: 0.71981037\n",
      "Step: [1969] d_loss: 1.37181127, g_loss: 0.72937757\n",
      "Step: [1970] d_loss: 1.38501000, g_loss: 0.72909451\n",
      "Step: [1971] d_loss: 1.38458526, g_loss: 0.72991514\n",
      "Step: [1972] d_loss: 1.39459395, g_loss: 0.72232980\n",
      "Step: [1973] d_loss: 1.37728858, g_loss: 0.72187114\n",
      "Step: [1974] d_loss: 1.37030125, g_loss: 0.73245090\n",
      "Step: [1975] d_loss: 1.36641717, g_loss: 0.72939801\n",
      "Step: [1976] d_loss: 1.36679006, g_loss: 0.72990859\n",
      "Step: [1977] d_loss: 1.36968005, g_loss: 0.73420632\n",
      "Step: [1978] d_loss: 1.39579988, g_loss: 0.71804065\n",
      "Step: [1979] d_loss: 1.38205755, g_loss: 0.73052788\n",
      "Step: [1980] d_loss: 1.39941680, g_loss: 0.71566296\n",
      "Step: [1981] d_loss: 1.38133097, g_loss: 0.72390902\n",
      "Step: [1982] d_loss: 1.37943649, g_loss: 0.72853339\n",
      "Step: [1983] d_loss: 1.38639593, g_loss: 0.71982467\n",
      "Step: [1984] d_loss: 1.37256098, g_loss: 0.72611839\n",
      "Step: [1985] d_loss: 1.35958481, g_loss: 0.72981954\n",
      "Step: [1986] d_loss: 1.37274480, g_loss: 0.72080004\n",
      "Step: [1987] d_loss: 1.36104965, g_loss: 0.73319536\n",
      "Step: [1988] d_loss: 1.37848437, g_loss: 0.73762339\n",
      "Step: [1989] d_loss: 1.36518145, g_loss: 0.74599057\n",
      "Step: [1990] d_loss: 1.37123287, g_loss: 0.73227441\n",
      "Step: [1991] d_loss: 1.39045429, g_loss: 0.72498286\n",
      "Step: [1992] d_loss: 1.37124085, g_loss: 0.72532797\n",
      "Step: [1993] d_loss: 1.38717294, g_loss: 0.72642446\n",
      "Step: [1994] d_loss: 1.41298771, g_loss: 0.74367994\n",
      "Step: [1995] d_loss: 1.39996362, g_loss: 0.73050022\n",
      "Step: [1996] d_loss: 1.39511299, g_loss: 0.72203767\n",
      "Step: [1997] d_loss: 1.39132094, g_loss: 0.72697878\n",
      "Step: [1998] d_loss: 1.37271333, g_loss: 0.72034323\n",
      "Step: [1999] d_loss: 1.37648964, g_loss: 0.73018825\n",
      "Step: [2000] d_loss: 1.37970090, g_loss: 0.72700506\n",
      "Step: [2001] d_loss: 1.36816812, g_loss: 0.72989941\n",
      "Step: [2002] d_loss: 1.39377046, g_loss: 0.71361697\n",
      "Step: [2003] d_loss: 1.38681030, g_loss: 0.73851871\n",
      "Step: [2004] d_loss: 1.38263679, g_loss: 0.72966427\n",
      "Step: [2005] d_loss: 1.38285208, g_loss: 0.73013556\n",
      "Step: [2006] d_loss: 1.37351525, g_loss: 0.73015440\n",
      "Step: [2007] d_loss: 1.37544298, g_loss: 0.72624844\n",
      "Step: [2008] d_loss: 1.37197256, g_loss: 0.73281318\n",
      "Step: [2009] d_loss: 1.37418735, g_loss: 0.72391820\n",
      "Step: [2010] d_loss: 1.36162627, g_loss: 0.73276854\n",
      "Step: [2011] d_loss: 1.38925731, g_loss: 0.71693867\n",
      "Step: [2012] d_loss: 1.38023186, g_loss: 0.72821510\n",
      "Step: [2013] d_loss: 1.38503218, g_loss: 0.72525889\n",
      "Step: [2014] d_loss: 1.38627434, g_loss: 0.71975482\n",
      "Step: [2015] d_loss: 1.37460220, g_loss: 0.72522169\n",
      "Step: [2016] d_loss: 1.38861966, g_loss: 0.72414517\n",
      "Step: [2017] d_loss: 1.38694322, g_loss: 0.72061896\n",
      "Step: [2018] d_loss: 1.38207316, g_loss: 0.72509742\n",
      "Step: [2019] d_loss: 1.36700201, g_loss: 0.72461146\n",
      "Step: [2020] d_loss: 1.39003670, g_loss: 0.71252960\n",
      "Step: [2021] d_loss: 1.38188791, g_loss: 0.72066325\n",
      "Step: [2022] d_loss: 1.39233136, g_loss: 0.71666753\n",
      "Step: [2023] d_loss: 1.37834108, g_loss: 0.73043466\n",
      "Step: [2024] d_loss: 1.38484979, g_loss: 0.73107684\n",
      "Step: [2025] d_loss: 1.38349009, g_loss: 0.71260214\n",
      "Step: [2026] d_loss: 1.36873829, g_loss: 0.73178852\n",
      "Step: [2027] d_loss: 1.37275028, g_loss: 0.73354006\n",
      "Step: [2028] d_loss: 1.37959957, g_loss: 0.72210574\n",
      "Step: [2029] d_loss: 1.36833954, g_loss: 0.73077977\n",
      "Step: [2030] d_loss: 1.38402152, g_loss: 0.71833920\n",
      "Step: [2031] d_loss: 1.36942458, g_loss: 0.73448402\n",
      "Step: [2032] d_loss: 1.37725461, g_loss: 0.71667004\n",
      "Step: [2033] d_loss: 1.39354849, g_loss: 0.71188289\n",
      "Step: [2034] d_loss: 1.37768769, g_loss: 0.72617668\n",
      "Step: [2035] d_loss: 1.38380766, g_loss: 0.72528905\n",
      "Step: [2036] d_loss: 1.37759876, g_loss: 0.72699261\n",
      "Step: [2037] d_loss: 1.38650799, g_loss: 0.72030485\n",
      "Step: [2038] d_loss: 1.37868345, g_loss: 0.71995384\n",
      "Step: [2039] d_loss: 1.38558197, g_loss: 0.72102940\n",
      "Step: [2040] d_loss: 1.39039588, g_loss: 0.72083890\n",
      "Step: [2041] d_loss: 1.38968945, g_loss: 0.72081685\n",
      "Step: [2042] d_loss: 1.37670159, g_loss: 0.72599661\n",
      "Step: [2043] d_loss: 1.39172995, g_loss: 0.72194046\n",
      "Step: [2044] d_loss: 1.37848532, g_loss: 0.72683024\n",
      "Step: [2045] d_loss: 1.36829388, g_loss: 0.72984940\n",
      "Step: [2046] d_loss: 1.36917782, g_loss: 0.72170413\n",
      "Step: [2047] d_loss: 1.38448596, g_loss: 0.71914029\n",
      "Step: [2048] d_loss: 1.36506116, g_loss: 0.72857082\n",
      "Step: [2049] d_loss: 1.39268827, g_loss: 0.72749633\n",
      "Step: [2050] d_loss: 1.36113143, g_loss: 0.73668349\n",
      "Step: [2051] d_loss: 1.38499188, g_loss: 0.71326101\n",
      "Step: [2052] d_loss: 1.36909974, g_loss: 0.72730684\n",
      "Step: [2053] d_loss: 1.37903750, g_loss: 0.72067523\n",
      "Step: [2054] d_loss: 1.37303448, g_loss: 0.72767961\n",
      "Step: [2055] d_loss: 1.38778210, g_loss: 0.72039580\n",
      "Step: [2056] d_loss: 1.38998842, g_loss: 0.71841824\n",
      "Step: [2057] d_loss: 1.38663566, g_loss: 0.72002172\n",
      "Step: [2058] d_loss: 1.38879561, g_loss: 0.72757840\n",
      "Step: [2059] d_loss: 1.38671279, g_loss: 0.72399914\n",
      "Step: [2060] d_loss: 1.38201404, g_loss: 0.73079669\n",
      "Step: [2061] d_loss: 1.39273214, g_loss: 0.71961445\n",
      "Step: [2062] d_loss: 1.38431001, g_loss: 0.72314608\n",
      "Step: [2063] d_loss: 1.38401747, g_loss: 0.72147161\n",
      "Step: [2064] d_loss: 1.38801897, g_loss: 0.72095537\n",
      "Step: [2065] d_loss: 1.39482212, g_loss: 0.72518110\n",
      "Step: [2066] d_loss: 1.37639594, g_loss: 0.73062706\n",
      "Step: [2067] d_loss: 1.37697816, g_loss: 0.72481346\n",
      "Step: [2068] d_loss: 1.38563836, g_loss: 0.72817612\n",
      "Step: [2069] d_loss: 1.38772106, g_loss: 0.72034776\n",
      "Step: [2070] d_loss: 1.37654805, g_loss: 0.72625256\n",
      "Step: [2071] d_loss: 1.36770964, g_loss: 0.72767168\n",
      "Step: [2072] d_loss: 1.37479806, g_loss: 0.72960895\n",
      "Step: [2073] d_loss: 1.39023626, g_loss: 0.72500002\n",
      "Step: [2074] d_loss: 1.38063407, g_loss: 0.73320603\n",
      "Step: [2075] d_loss: 1.36387622, g_loss: 0.72818315\n",
      "Step: [2076] d_loss: 1.39822972, g_loss: 0.71569937\n",
      "Step: [2077] d_loss: 1.38641095, g_loss: 0.72568673\n",
      "Step: [2078] d_loss: 1.37357688, g_loss: 0.72517318\n",
      "Step: [2079] d_loss: 1.38157535, g_loss: 0.71705985\n",
      "Step: [2080] d_loss: 1.36422658, g_loss: 0.73513901\n",
      "Step: [2081] d_loss: 1.37410295, g_loss: 0.73842996\n",
      "Step: [2082] d_loss: 1.38538897, g_loss: 0.72385764\n",
      "Step: [2083] d_loss: 1.37739480, g_loss: 0.73281199\n",
      "Step: [2084] d_loss: 1.38236201, g_loss: 0.72267592\n",
      "Step: [2085] d_loss: 1.38161159, g_loss: 0.71516085\n",
      "Step: [2086] d_loss: 1.38335013, g_loss: 0.72216517\n",
      "Step: [2087] d_loss: 1.37587249, g_loss: 0.72971714\n",
      "Step: [2088] d_loss: 1.39132619, g_loss: 0.71985275\n",
      "Step: [2089] d_loss: 1.37063801, g_loss: 0.73620474\n",
      "Step: [2090] d_loss: 1.39629066, g_loss: 0.71220970\n",
      "Step: [2091] d_loss: 1.37481439, g_loss: 0.72425151\n",
      "Step: [2092] d_loss: 1.36562979, g_loss: 0.74177331\n",
      "Step: [2093] d_loss: 1.39016080, g_loss: 0.71493238\n",
      "Step: [2094] d_loss: 1.36192703, g_loss: 0.73779583\n",
      "Step: [2095] d_loss: 1.38469470, g_loss: 0.71980345\n",
      "Step: [2096] d_loss: 1.36682367, g_loss: 0.73625904\n",
      "Step: [2097] d_loss: 1.38695335, g_loss: 0.72641701\n",
      "Step: [2098] d_loss: 1.38754249, g_loss: 0.72097743\n",
      "Step: [2099] d_loss: 1.38675463, g_loss: 0.72161317\n",
      "Step: [2100] d_loss: 1.37695789, g_loss: 0.72323006\n",
      "Step: [2101] d_loss: 1.37914348, g_loss: 0.73100209\n",
      "Step: [2102] d_loss: 1.38175595, g_loss: 0.73173767\n",
      "Step: [2103] d_loss: 1.38768435, g_loss: 0.72211927\n",
      "Step: [2104] d_loss: 1.38157344, g_loss: 0.72343957\n",
      "Step: [2105] d_loss: 1.37772226, g_loss: 0.72188067\n",
      "Step: [2106] d_loss: 1.38978755, g_loss: 0.72445387\n",
      "Step: [2107] d_loss: 1.38151741, g_loss: 0.72337651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2108] d_loss: 1.39116991, g_loss: 0.71807355\n",
      "Step: [2109] d_loss: 1.38754559, g_loss: 0.71659130\n",
      "Step: [2110] d_loss: 1.38432002, g_loss: 0.72220206\n",
      "Step: [2111] d_loss: 1.37799513, g_loss: 0.72933257\n",
      "Step: [2112] d_loss: 1.37974381, g_loss: 0.72842366\n",
      "Step: [2113] d_loss: 1.39621496, g_loss: 0.72145510\n",
      "Step: [2114] d_loss: 1.38271046, g_loss: 0.72825503\n",
      "Step: [2115] d_loss: 1.37860537, g_loss: 0.72150779\n",
      "Step: [2116] d_loss: 1.37856770, g_loss: 0.71884584\n",
      "Step: [2117] d_loss: 1.38016999, g_loss: 0.72641361\n",
      "Step: [2118] d_loss: 1.36951184, g_loss: 0.73881423\n",
      "Step: [2119] d_loss: 1.36813653, g_loss: 0.73408562\n",
      "Step: [2120] d_loss: 1.36732638, g_loss: 0.72634524\n",
      "Step: [2121] d_loss: 1.37501228, g_loss: 0.72438383\n",
      "Step: [2122] d_loss: 1.36538005, g_loss: 0.72963804\n",
      "Step: [2123] d_loss: 1.36773038, g_loss: 0.73231626\n",
      "Step: [2124] d_loss: 1.37863970, g_loss: 0.72524726\n",
      "Step: [2125] d_loss: 1.38240075, g_loss: 0.72709459\n",
      "Step: [2126] d_loss: 1.39824748, g_loss: 0.72184706\n",
      "Step: [2127] d_loss: 1.37281847, g_loss: 0.73022699\n",
      "Step: [2128] d_loss: 1.39320707, g_loss: 0.71284509\n",
      "Step: [2129] d_loss: 1.38865638, g_loss: 0.71888852\n",
      "Step: [2130] d_loss: 1.38263607, g_loss: 0.72562915\n",
      "Step: [2131] d_loss: 1.38007510, g_loss: 0.72772443\n",
      "Step: [2132] d_loss: 1.37442446, g_loss: 0.72489834\n",
      "Step: [2133] d_loss: 1.39871216, g_loss: 0.71647424\n",
      "Step: [2134] d_loss: 1.38801432, g_loss: 0.72432941\n",
      "Step: [2135] d_loss: 1.39224219, g_loss: 0.72215414\n",
      "Step: [2136] d_loss: 1.39552963, g_loss: 0.71652281\n",
      "Step: [2137] d_loss: 1.39001918, g_loss: 0.72622120\n",
      "Step: [2138] d_loss: 1.37292767, g_loss: 0.72491640\n",
      "Step: [2139] d_loss: 1.38498807, g_loss: 0.72468686\n",
      "Step: [2140] d_loss: 1.37503135, g_loss: 0.73094749\n",
      "Step: [2141] d_loss: 1.38241041, g_loss: 0.72638977\n",
      "Step: [2142] d_loss: 1.37801600, g_loss: 0.72770637\n",
      "Step: [2143] d_loss: 1.39048839, g_loss: 0.71793425\n",
      "Step: [2144] d_loss: 1.38102865, g_loss: 0.72718704\n",
      "Step: [2145] d_loss: 1.38827443, g_loss: 0.71589100\n",
      "Step: [2146] d_loss: 1.37937045, g_loss: 0.72952193\n",
      "Step: [2147] d_loss: 1.36992049, g_loss: 0.73314500\n",
      "Step: [2148] d_loss: 1.38166332, g_loss: 0.72459495\n",
      "Step: [2149] d_loss: 1.37583017, g_loss: 0.72310781\n",
      "Step: [2150] d_loss: 1.38166356, g_loss: 0.72398758\n",
      "Step: [2151] d_loss: 1.37527311, g_loss: 0.72763824\n",
      "Step: [2152] d_loss: 1.37776768, g_loss: 0.72463369\n",
      "Step: [2153] d_loss: 1.37601209, g_loss: 0.73125803\n",
      "Step: [2154] d_loss: 1.37645578, g_loss: 0.72795838\n",
      "Step: [2155] d_loss: 1.37060761, g_loss: 0.72350192\n",
      "Step: [2156] d_loss: 1.36803031, g_loss: 0.73361063\n",
      "Step: [2157] d_loss: 1.36513841, g_loss: 0.72611010\n",
      "Step: [2158] d_loss: 1.39748549, g_loss: 0.71930414\n",
      "Step: [2159] d_loss: 1.38554931, g_loss: 0.72301120\n",
      "Step: [2160] d_loss: 1.38408566, g_loss: 0.73310745\n",
      "Step: [2161] d_loss: 1.38994694, g_loss: 0.72708398\n",
      "Step: [2162] d_loss: 1.39546323, g_loss: 0.71783340\n",
      "Step: [2163] d_loss: 1.38361990, g_loss: 0.72115296\n",
      "Step: [2164] d_loss: 1.38303590, g_loss: 0.73009568\n",
      "Step: [2165] d_loss: 1.38504791, g_loss: 0.72839111\n",
      "Step: [2166] d_loss: 1.37029624, g_loss: 0.72631526\n",
      "Step: [2167] d_loss: 1.39028049, g_loss: 0.71287316\n",
      "Step: [2168] d_loss: 1.39202929, g_loss: 0.71031898\n",
      "Step: [2169] d_loss: 1.39805305, g_loss: 0.71461004\n",
      "Step: [2170] d_loss: 1.39489543, g_loss: 0.72127199\n",
      "Step: [2171] d_loss: 1.38450074, g_loss: 0.73638105\n",
      "Step: [2172] d_loss: 1.37968850, g_loss: 0.73500276\n",
      "Step: [2173] d_loss: 1.39566410, g_loss: 0.71896625\n",
      "Step: [2174] d_loss: 1.38578796, g_loss: 0.72184336\n",
      "Step: [2175] d_loss: 1.39661086, g_loss: 0.71509850\n",
      "Step: [2176] d_loss: 1.37935042, g_loss: 0.72541511\n",
      "Step: [2177] d_loss: 1.38672519, g_loss: 0.72056603\n",
      "Step: [2178] d_loss: 1.38677311, g_loss: 0.72237659\n",
      "Step: [2179] d_loss: 1.38363791, g_loss: 0.72242349\n",
      "Step: [2180] d_loss: 1.38370895, g_loss: 0.71872008\n",
      "Step: [2181] d_loss: 1.37895191, g_loss: 0.72239190\n",
      "Step: [2182] d_loss: 1.39198768, g_loss: 0.71661282\n",
      "Step: [2183] d_loss: 1.38095689, g_loss: 0.72669077\n",
      "Step: [2184] d_loss: 1.37632394, g_loss: 0.73467672\n",
      "Step: [2185] d_loss: 1.37797964, g_loss: 0.73490107\n",
      "Step: [2186] d_loss: 1.37390924, g_loss: 0.72906905\n",
      "Step: [2187] d_loss: 1.37344408, g_loss: 0.72956526\n",
      "Step: [2188] d_loss: 1.36870265, g_loss: 0.72538781\n",
      "Step: [2189] d_loss: 1.38112116, g_loss: 0.72324407\n",
      "Step: [2190] d_loss: 1.35880947, g_loss: 0.73908603\n",
      "Step: [2191] d_loss: 1.37098575, g_loss: 0.73000640\n",
      "Step: [2192] d_loss: 1.38669586, g_loss: 0.71775723\n",
      "Step: [2193] d_loss: 1.38367617, g_loss: 0.71882385\n",
      "Step: [2194] d_loss: 1.37848186, g_loss: 0.72695327\n",
      "Step: [2195] d_loss: 1.36453521, g_loss: 0.73334694\n",
      "Step: [2196] d_loss: 1.38829684, g_loss: 0.73055339\n",
      "Step: [2197] d_loss: 1.38031721, g_loss: 0.74331176\n",
      "Step: [2198] d_loss: 1.40413296, g_loss: 0.71607530\n",
      "Step: [2199] d_loss: 1.38626981, g_loss: 0.73089784\n",
      "Step: [2200] d_loss: 1.40055037, g_loss: 0.71745008\n",
      "Step: [2201] d_loss: 1.39032435, g_loss: 0.71476704\n",
      "Step: [2202] d_loss: 1.40812755, g_loss: 0.70972657\n",
      "Step: [2203] d_loss: 1.39359021, g_loss: 0.72565830\n",
      "Step: [2204] d_loss: 1.39664316, g_loss: 0.72754335\n",
      "Step: [2205] d_loss: 1.39596879, g_loss: 0.72182256\n",
      "Step: [2206] d_loss: 1.39483547, g_loss: 0.71970439\n",
      "Step: [2207] d_loss: 1.39460397, g_loss: 0.72128338\n",
      "Step: [2208] d_loss: 1.38897395, g_loss: 0.72239220\n",
      "Step: [2209] d_loss: 1.38223886, g_loss: 0.72724319\n",
      "Step: [2210] d_loss: 1.38598895, g_loss: 0.72645438\n",
      "Step: [2211] d_loss: 1.38126802, g_loss: 0.72246337\n",
      "Step: [2212] d_loss: 1.39440203, g_loss: 0.70982480\n",
      "Step: [2213] d_loss: 1.38402224, g_loss: 0.72328717\n",
      "Step: [2214] d_loss: 1.37818050, g_loss: 0.72097921\n",
      "Step: [2215] d_loss: 1.36802363, g_loss: 0.73334551\n",
      "Step: [2216] d_loss: 1.37332845, g_loss: 0.73174393\n",
      "Step: [2217] d_loss: 1.38309383, g_loss: 0.72640097\n",
      "Step: [2218] d_loss: 1.37964547, g_loss: 0.72953379\n",
      "Step: [2219] d_loss: 1.38573539, g_loss: 0.72010970\n",
      "Step: [2220] d_loss: 1.37490320, g_loss: 0.73285639\n",
      "Step: [2221] d_loss: 1.38477278, g_loss: 0.72031999\n",
      "Step: [2222] d_loss: 1.37792730, g_loss: 0.72442305\n",
      "Step: [2223] d_loss: 1.37986970, g_loss: 0.72185540\n",
      "Step: [2224] d_loss: 1.37280607, g_loss: 0.72594237\n",
      "Step: [2225] d_loss: 1.38311696, g_loss: 0.72194576\n",
      "Step: [2226] d_loss: 1.39182019, g_loss: 0.71282732\n",
      "Step: [2227] d_loss: 1.38312721, g_loss: 0.72721380\n",
      "Step: [2228] d_loss: 1.39147496, g_loss: 0.72079909\n",
      "Step: [2229] d_loss: 1.39478958, g_loss: 0.71573377\n",
      "Step: [2230] d_loss: 1.38609993, g_loss: 0.71966225\n",
      "Step: [2231] d_loss: 1.38833344, g_loss: 0.72146147\n",
      "Step: [2232] d_loss: 1.38486362, g_loss: 0.72645330\n",
      "Step: [2233] d_loss: 1.39159584, g_loss: 0.72940862\n",
      "Step: [2234] d_loss: 1.39709496, g_loss: 0.71966141\n",
      "Step: [2235] d_loss: 1.38981199, g_loss: 0.71848643\n",
      "Step: [2236] d_loss: 1.39153910, g_loss: 0.72820485\n",
      "Step: [2237] d_loss: 1.39214039, g_loss: 0.71973729\n",
      "Step: [2238] d_loss: 1.38793135, g_loss: 0.72215801\n",
      "Step: [2239] d_loss: 1.36423612, g_loss: 0.73273259\n",
      "Step: [2240] d_loss: 1.38538802, g_loss: 0.72771728\n",
      "Step: [2241] d_loss: 1.38280320, g_loss: 0.71451831\n",
      "Step: [2242] d_loss: 1.38574696, g_loss: 0.72075975\n",
      "Step: [2243] d_loss: 1.37240505, g_loss: 0.72153020\n",
      "Step: [2244] d_loss: 1.39028525, g_loss: 0.71672148\n",
      "Step: [2245] d_loss: 1.38681650, g_loss: 0.71775651\n",
      "Step: [2246] d_loss: 1.38514018, g_loss: 0.71889746\n",
      "Step: [2247] d_loss: 1.37593400, g_loss: 0.71489942\n",
      "Step: [2248] d_loss: 1.38742292, g_loss: 0.71729445\n",
      "Step: [2249] d_loss: 1.40126920, g_loss: 0.71070349\n",
      "Step: [2250] d_loss: 1.37938464, g_loss: 0.71996403\n",
      "Step: [2251] d_loss: 1.37016129, g_loss: 0.72091794\n",
      "Step: [2252] d_loss: 1.36300611, g_loss: 0.72249365\n",
      "Step: [2253] d_loss: 1.38139558, g_loss: 0.71114069\n",
      "Step: [2254] d_loss: 1.38648391, g_loss: 0.72076964\n",
      "Step: [2255] d_loss: 1.37743235, g_loss: 0.72137916\n",
      "Step: [2256] d_loss: 1.37033534, g_loss: 0.72375256\n",
      "Step: [2257] d_loss: 1.38911390, g_loss: 0.72448707\n",
      "Step: [2258] d_loss: 1.38746357, g_loss: 0.71754658\n",
      "Step: [2259] d_loss: 1.37520683, g_loss: 0.72441649\n",
      "Step: [2260] d_loss: 1.36465406, g_loss: 0.72622710\n",
      "Step: [2261] d_loss: 1.38399935, g_loss: 0.72821498\n",
      "Step: [2262] d_loss: 1.38498652, g_loss: 0.72216541\n",
      "Step: [2263] d_loss: 1.39402378, g_loss: 0.72228241\n",
      "Step: [2264] d_loss: 1.41005754, g_loss: 0.71656275\n",
      "Step: [2265] d_loss: 1.41038740, g_loss: 0.70675349\n",
      "Step: [2266] d_loss: 1.37931132, g_loss: 0.72264016\n",
      "Step: [2267] d_loss: 1.38665819, g_loss: 0.71939707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2268] d_loss: 1.39322221, g_loss: 0.72341025\n",
      "Step: [2269] d_loss: 1.38473153, g_loss: 0.72789216\n",
      "Step: [2270] d_loss: 1.39531171, g_loss: 0.71126759\n",
      "Step: [2271] d_loss: 1.39346564, g_loss: 0.70926327\n",
      "Step: [2272] d_loss: 1.38009822, g_loss: 0.72288084\n",
      "Step: [2273] d_loss: 1.39260948, g_loss: 0.71908569\n",
      "Step: [2274] d_loss: 1.37919664, g_loss: 0.72133148\n",
      "Step: [2275] d_loss: 1.38851953, g_loss: 0.72320330\n",
      "Step: [2276] d_loss: 1.37332797, g_loss: 0.72373539\n",
      "Step: [2277] d_loss: 1.36692405, g_loss: 0.72754145\n",
      "Step: [2278] d_loss: 1.37179184, g_loss: 0.72593379\n",
      "Step: [2279] d_loss: 1.36751366, g_loss: 0.72795343\n",
      "Step: [2280] d_loss: 1.37412620, g_loss: 0.72383857\n",
      "Step: [2281] d_loss: 1.36677158, g_loss: 0.72397864\n",
      "Step: [2282] d_loss: 1.37706602, g_loss: 0.72165948\n",
      "Step: [2283] d_loss: 1.36991477, g_loss: 0.72601509\n",
      "Step: [2284] d_loss: 1.38798857, g_loss: 0.72732735\n",
      "Step: [2285] d_loss: 1.38363838, g_loss: 0.72092938\n",
      "Step: [2286] d_loss: 1.38034391, g_loss: 0.72037566\n",
      "Step: [2287] d_loss: 1.36216390, g_loss: 0.72984397\n",
      "Step: [2288] d_loss: 1.38578129, g_loss: 0.72115773\n",
      "Step: [2289] d_loss: 1.37726057, g_loss: 0.72420174\n",
      "Step: [2290] d_loss: 1.38091159, g_loss: 0.73161292\n",
      "Step: [2291] d_loss: 1.38634515, g_loss: 0.72463346\n",
      "Step: [2292] d_loss: 1.39004159, g_loss: 0.72093409\n",
      "Step: [2293] d_loss: 1.37789488, g_loss: 0.72716135\n",
      "Step: [2294] d_loss: 1.39367199, g_loss: 0.71824217\n",
      "Step: [2295] d_loss: 1.41048932, g_loss: 0.70552605\n",
      "Step: [2296] d_loss: 1.39730406, g_loss: 0.71227813\n",
      "Step: [2297] d_loss: 1.37052393, g_loss: 0.73246622\n",
      "Step: [2298] d_loss: 1.38586164, g_loss: 0.73099786\n",
      "Step: [2299] d_loss: 1.39185071, g_loss: 0.73368347\n",
      "Step: [2300] d_loss: 1.38465238, g_loss: 0.70982635\n",
      "Step: [2301] d_loss: 1.37992775, g_loss: 0.71831024\n",
      "Step: [2302] d_loss: 1.37674785, g_loss: 0.72013736\n",
      "Step: [2303] d_loss: 1.37846363, g_loss: 0.71396267\n",
      "Step: [2304] d_loss: 1.38739860, g_loss: 0.72154707\n",
      "Step: [2305] d_loss: 1.37647033, g_loss: 0.72447586\n",
      "Step: [2306] d_loss: 1.37800527, g_loss: 0.72279543\n",
      "Step: [2307] d_loss: 1.38659263, g_loss: 0.71544516\n",
      "Step: [2308] d_loss: 1.39440823, g_loss: 0.72133720\n",
      "Step: [2309] d_loss: 1.37606263, g_loss: 0.71940434\n",
      "Step: [2310] d_loss: 1.38276768, g_loss: 0.72644472\n",
      "Step: [2311] d_loss: 1.40175104, g_loss: 0.71826136\n",
      "Step: [2312] d_loss: 1.38907278, g_loss: 0.72329909\n",
      "Step: [2313] d_loss: 1.39201212, g_loss: 0.71680975\n",
      "Step: [2314] d_loss: 1.39809048, g_loss: 0.71307838\n",
      "Step: [2315] d_loss: 1.38259959, g_loss: 0.70949328\n",
      "Step: [2316] d_loss: 1.37312961, g_loss: 0.73212695\n",
      "Step: [2317] d_loss: 1.39839220, g_loss: 0.71943957\n",
      "Step: [2318] d_loss: 1.37502503, g_loss: 0.73310924\n",
      "Step: [2319] d_loss: 1.39690614, g_loss: 0.71211439\n",
      "Step: [2320] d_loss: 1.38661063, g_loss: 0.71275949\n",
      "Step: [2321] d_loss: 1.38058925, g_loss: 0.72049201\n",
      "Step: [2322] d_loss: 1.38577437, g_loss: 0.71306521\n",
      "Step: [2323] d_loss: 1.37416291, g_loss: 0.72472298\n",
      "Step: [2324] d_loss: 1.38840699, g_loss: 0.71983486\n",
      "Step: [2325] d_loss: 1.37531376, g_loss: 0.72757673\n",
      "Step: [2326] d_loss: 1.36686420, g_loss: 0.73286825\n",
      "Step: [2327] d_loss: 1.38162971, g_loss: 0.72259498\n",
      "Step: [2328] d_loss: 1.36457050, g_loss: 0.72285461\n",
      "Step: [2329] d_loss: 1.37974787, g_loss: 0.72320896\n",
      "Step: [2330] d_loss: 1.38680172, g_loss: 0.71719861\n",
      "Step: [2331] d_loss: 1.37400126, g_loss: 0.72313035\n",
      "Step: [2332] d_loss: 1.38997436, g_loss: 0.71165287\n",
      "Step: [2333] d_loss: 1.39075434, g_loss: 0.70993531\n",
      "Step: [2334] d_loss: 1.37896347, g_loss: 0.71703804\n",
      "Step: [2335] d_loss: 1.40482187, g_loss: 0.71379858\n",
      "Step: [2336] d_loss: 1.37588358, g_loss: 0.72441053\n",
      "Step: [2337] d_loss: 1.38434696, g_loss: 0.71789753\n",
      "Step: [2338] d_loss: 1.37917733, g_loss: 0.72289264\n",
      "Step: [2339] d_loss: 1.37665677, g_loss: 0.72544014\n",
      "Step: [2340] d_loss: 1.37774491, g_loss: 0.71937782\n",
      "Step: [2341] d_loss: 1.37576115, g_loss: 0.72048640\n",
      "Step: [2342] d_loss: 1.38606644, g_loss: 0.72699142\n",
      "Step: [2343] d_loss: 1.39241862, g_loss: 0.72229838\n",
      "Step: [2344] d_loss: 1.39628112, g_loss: 0.71115756\n",
      "Step: [2345] d_loss: 1.38513184, g_loss: 0.71928847\n",
      "Step: [2346] d_loss: 1.39357972, g_loss: 0.73638088\n",
      "Step: [2347] d_loss: 1.40629268, g_loss: 0.74537253\n",
      "Step: [2348] d_loss: 1.42181122, g_loss: 0.74880052\n",
      "Step: [2349] d_loss: 1.42176163, g_loss: 0.74909335\n",
      "Step: [2350] d_loss: 1.41201925, g_loss: 0.72915810\n",
      "Step: [2351] d_loss: 1.39061737, g_loss: 0.72277838\n",
      "Step: [2352] d_loss: 1.38186109, g_loss: 0.73033172\n",
      "Step: [2353] d_loss: 1.39586663, g_loss: 0.71540487\n",
      "Step: [2354] d_loss: 1.39130092, g_loss: 0.71490794\n",
      "Step: [2355] d_loss: 1.38477778, g_loss: 0.71970552\n",
      "Step: [2356] d_loss: 1.39157438, g_loss: 0.71353519\n",
      "Step: [2357] d_loss: 1.37700582, g_loss: 0.71728510\n",
      "Step: [2358] d_loss: 1.39004445, g_loss: 0.71706331\n",
      "Step: [2359] d_loss: 1.39492202, g_loss: 0.71690518\n",
      "Step: [2360] d_loss: 1.39073825, g_loss: 0.70267618\n",
      "Step: [2361] d_loss: 1.38889146, g_loss: 0.71476972\n",
      "Step: [2362] d_loss: 1.37193274, g_loss: 0.72296047\n",
      "Step: [2363] d_loss: 1.38253212, g_loss: 0.71591616\n",
      "Step: [2364] d_loss: 1.37715483, g_loss: 0.72500753\n",
      "Step: [2365] d_loss: 1.37055707, g_loss: 0.72110951\n",
      "Step: [2366] d_loss: 1.38679087, g_loss: 0.71720487\n",
      "Step: [2367] d_loss: 1.36333585, g_loss: 0.72686309\n",
      "Step: [2368] d_loss: 1.38865876, g_loss: 0.71856266\n",
      "Step: [2369] d_loss: 1.38005710, g_loss: 0.72116143\n",
      "Step: [2370] d_loss: 1.38612962, g_loss: 0.71616954\n",
      "Step: [2371] d_loss: 1.38239288, g_loss: 0.71714592\n",
      "Step: [2372] d_loss: 1.39713275, g_loss: 0.71425170\n",
      "Step: [2373] d_loss: 1.38374615, g_loss: 0.71532482\n",
      "Step: [2374] d_loss: 1.36847067, g_loss: 0.72611362\n",
      "Step: [2375] d_loss: 1.38115966, g_loss: 0.71784329\n",
      "Step: [2376] d_loss: 1.38808072, g_loss: 0.71636474\n",
      "Step: [2377] d_loss: 1.38833034, g_loss: 0.71462858\n",
      "Step: [2378] d_loss: 1.38828897, g_loss: 0.70806909\n",
      "Step: [2379] d_loss: 1.36986291, g_loss: 0.72416878\n",
      "Step: [2380] d_loss: 1.38031983, g_loss: 0.72802055\n",
      "Step: [2381] d_loss: 1.40300655, g_loss: 0.71785808\n",
      "Step: [2382] d_loss: 1.39497626, g_loss: 0.71366161\n",
      "Step: [2383] d_loss: 1.39285016, g_loss: 0.71680701\n",
      "Step: [2384] d_loss: 1.38523531, g_loss: 0.71839643\n",
      "Step: [2385] d_loss: 1.37378073, g_loss: 0.71709335\n",
      "Step: [2386] d_loss: 1.37878156, g_loss: 0.72251821\n",
      "Step: [2387] d_loss: 1.37725282, g_loss: 0.71974206\n",
      "Step: [2388] d_loss: 1.39167309, g_loss: 0.71718872\n",
      "Step: [2389] d_loss: 1.38997650, g_loss: 0.72289622\n",
      "Step: [2390] d_loss: 1.37165737, g_loss: 0.72099745\n",
      "Step: [2391] d_loss: 1.38676202, g_loss: 0.71751809\n",
      "Step: [2392] d_loss: 1.38108802, g_loss: 0.72082371\n",
      "Step: [2393] d_loss: 1.37909198, g_loss: 0.71738684\n",
      "Step: [2394] d_loss: 1.38345480, g_loss: 0.71366453\n",
      "Step: [2395] d_loss: 1.39574504, g_loss: 0.70960152\n",
      "Step: [2396] d_loss: 1.38996077, g_loss: 0.71691000\n",
      "Step: [2397] d_loss: 1.39087629, g_loss: 0.72177780\n",
      "Step: [2398] d_loss: 1.39129949, g_loss: 0.71931982\n",
      "Step: [2399] d_loss: 1.38812482, g_loss: 0.71701324\n",
      "Step: [2400] d_loss: 1.38318253, g_loss: 0.71367246\n",
      "Step: [2401] d_loss: 1.39677644, g_loss: 0.71508145\n",
      "Step: [2402] d_loss: 1.38487530, g_loss: 0.71549606\n",
      "Step: [2403] d_loss: 1.38096893, g_loss: 0.72045255\n",
      "Step: [2404] d_loss: 1.39242566, g_loss: 0.72025019\n",
      "Step: [2405] d_loss: 1.38899696, g_loss: 0.71938145\n",
      "Step: [2406] d_loss: 1.38591647, g_loss: 0.70966017\n",
      "Step: [2407] d_loss: 1.37405849, g_loss: 0.71941900\n",
      "Step: [2408] d_loss: 1.37924910, g_loss: 0.71511006\n",
      "Step: [2409] d_loss: 1.39097977, g_loss: 0.71628940\n",
      "Step: [2410] d_loss: 1.37251878, g_loss: 0.72289354\n",
      "Step: [2411] d_loss: 1.36169505, g_loss: 0.73012358\n",
      "Step: [2412] d_loss: 1.37861145, g_loss: 0.71743786\n",
      "Step: [2413] d_loss: 1.37896359, g_loss: 0.71743071\n",
      "Step: [2414] d_loss: 1.39030266, g_loss: 0.70887434\n",
      "Step: [2415] d_loss: 1.37488484, g_loss: 0.72903168\n",
      "Step: [2416] d_loss: 1.37882757, g_loss: 0.72305936\n",
      "Step: [2417] d_loss: 1.37849021, g_loss: 0.71625781\n",
      "Step: [2418] d_loss: 1.38979352, g_loss: 0.72267216\n",
      "Step: [2419] d_loss: 1.37356162, g_loss: 0.72168756\n",
      "Step: [2420] d_loss: 1.38295126, g_loss: 0.71453631\n",
      "Step: [2421] d_loss: 1.38286138, g_loss: 0.71558493\n",
      "Step: [2422] d_loss: 1.37775385, g_loss: 0.71317571\n",
      "Step: [2423] d_loss: 1.40225768, g_loss: 0.70966673\n",
      "Step: [2424] d_loss: 1.37594652, g_loss: 0.72253430\n",
      "Step: [2425] d_loss: 1.37936783, g_loss: 0.71315914\n",
      "Step: [2426] d_loss: 1.39007425, g_loss: 0.70997196\n",
      "Step: [2427] d_loss: 1.38620448, g_loss: 0.71907997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2428] d_loss: 1.38543034, g_loss: 0.71775997\n",
      "Step: [2429] d_loss: 1.38262177, g_loss: 0.71995449\n",
      "Step: [2430] d_loss: 1.39428735, g_loss: 0.70969296\n",
      "Step: [2431] d_loss: 1.39276564, g_loss: 0.71874321\n",
      "Step: [2432] d_loss: 1.37944520, g_loss: 0.73402339\n",
      "Step: [2433] d_loss: 1.39573073, g_loss: 0.72642696\n",
      "Step: [2434] d_loss: 1.38618636, g_loss: 0.71391690\n",
      "Step: [2435] d_loss: 1.37675726, g_loss: 0.72454298\n",
      "Step: [2436] d_loss: 1.37539363, g_loss: 0.72524649\n",
      "Step: [2437] d_loss: 1.36913252, g_loss: 0.72757483\n",
      "Step: [2438] d_loss: 1.38196564, g_loss: 0.72033328\n",
      "Step: [2439] d_loss: 1.36631942, g_loss: 0.73934740\n",
      "Step: [2440] d_loss: 1.37937498, g_loss: 0.71907341\n",
      "Step: [2441] d_loss: 1.38540697, g_loss: 0.72472882\n",
      "Step: [2442] d_loss: 1.38702679, g_loss: 0.71962553\n",
      "Step: [2443] d_loss: 1.39974713, g_loss: 0.71054167\n",
      "Step: [2444] d_loss: 1.39237905, g_loss: 0.71070874\n",
      "Step: [2445] d_loss: 1.37352848, g_loss: 0.71770000\n",
      "Step: [2446] d_loss: 1.38917518, g_loss: 0.71373516\n",
      "Step: [2447] d_loss: 1.38699389, g_loss: 0.71483326\n",
      "Step: [2448] d_loss: 1.38237333, g_loss: 0.70889878\n",
      "Step: [2449] d_loss: 1.38886166, g_loss: 0.71784949\n",
      "Step: [2450] d_loss: 1.39269090, g_loss: 0.71185225\n",
      "Step: [2451] d_loss: 1.37916279, g_loss: 0.71920007\n",
      "Step: [2452] d_loss: 1.39413428, g_loss: 0.71322751\n",
      "Step: [2453] d_loss: 1.38505316, g_loss: 0.71542573\n",
      "Step: [2454] d_loss: 1.38291574, g_loss: 0.71410990\n",
      "Step: [2455] d_loss: 1.38996553, g_loss: 0.70866060\n",
      "Step: [2456] d_loss: 1.39714456, g_loss: 0.71494734\n",
      "Step: [2457] d_loss: 1.39132941, g_loss: 0.71348584\n",
      "Step: [2458] d_loss: 1.38238573, g_loss: 0.72150540\n",
      "Step: [2459] d_loss: 1.38917303, g_loss: 0.71853209\n",
      "Step: [2460] d_loss: 1.39906847, g_loss: 0.71169382\n",
      "Step: [2461] d_loss: 1.37610936, g_loss: 0.71407688\n",
      "Step: [2462] d_loss: 1.38133192, g_loss: 0.72009885\n",
      "Step: [2463] d_loss: 1.37677574, g_loss: 0.72016144\n",
      "Step: [2464] d_loss: 1.38910115, g_loss: 0.71795815\n",
      "Step: [2465] d_loss: 1.37990177, g_loss: 0.71380973\n",
      "Step: [2466] d_loss: 1.39891481, g_loss: 0.70801753\n",
      "Step: [2467] d_loss: 1.38498414, g_loss: 0.71573234\n",
      "Step: [2468] d_loss: 1.37811744, g_loss: 0.71360326\n",
      "Step: [2469] d_loss: 1.37505186, g_loss: 0.71967012\n",
      "Step: [2470] d_loss: 1.38708138, g_loss: 0.71681964\n",
      "Step: [2471] d_loss: 1.38112903, g_loss: 0.71699309\n",
      "Step: [2472] d_loss: 1.38473022, g_loss: 0.71598792\n",
      "Step: [2473] d_loss: 1.37707770, g_loss: 0.71201146\n",
      "Step: [2474] d_loss: 1.37922263, g_loss: 0.71602285\n",
      "Step: [2475] d_loss: 1.37786269, g_loss: 0.71980023\n",
      "Step: [2476] d_loss: 1.38641822, g_loss: 0.71762192\n",
      "Step: [2477] d_loss: 1.38861585, g_loss: 0.71406454\n",
      "Step: [2478] d_loss: 1.38389921, g_loss: 0.71999300\n",
      "Step: [2479] d_loss: 1.37251759, g_loss: 0.71841860\n",
      "Step: [2480] d_loss: 1.38033569, g_loss: 0.72038633\n",
      "Step: [2481] d_loss: 1.37734199, g_loss: 0.72540039\n",
      "Step: [2482] d_loss: 1.38050556, g_loss: 0.72146732\n",
      "Step: [2483] d_loss: 1.38283908, g_loss: 0.72677529\n",
      "Step: [2484] d_loss: 1.38531029, g_loss: 0.72771770\n",
      "Step: [2485] d_loss: 1.39030778, g_loss: 0.71283174\n",
      "Step: [2486] d_loss: 1.39568830, g_loss: 0.70668221\n",
      "Step: [2487] d_loss: 1.38022828, g_loss: 0.72348958\n",
      "Step: [2488] d_loss: 1.37873077, g_loss: 0.71642077\n",
      "Step: [2489] d_loss: 1.39992547, g_loss: 0.71038616\n",
      "Step: [2490] d_loss: 1.38633275, g_loss: 0.72672874\n",
      "Step: [2491] d_loss: 1.38766408, g_loss: 0.73159516\n",
      "Step: [2492] d_loss: 1.39323115, g_loss: 0.71341485\n",
      "Step: [2493] d_loss: 1.38399649, g_loss: 0.71530801\n",
      "Step: [2494] d_loss: 1.37473750, g_loss: 0.72643232\n",
      "Step: [2495] d_loss: 1.37151778, g_loss: 0.72454774\n",
      "Step: [2496] d_loss: 1.38732564, g_loss: 0.72337759\n",
      "Step: [2497] d_loss: 1.38355029, g_loss: 0.72164077\n",
      "Step: [2498] d_loss: 1.38285446, g_loss: 0.71518290\n",
      "Step: [2499] d_loss: 1.38304710, g_loss: 0.72195208\n",
      "Step: [2500] d_loss: 1.38995552, g_loss: 0.70944178\n",
      "Step: [2501] d_loss: 1.38679624, g_loss: 0.71651852\n",
      "Step: [2502] d_loss: 1.39519167, g_loss: 0.71357942\n",
      "Step: [2503] d_loss: 1.37979889, g_loss: 0.71995264\n",
      "Step: [2504] d_loss: 1.38397098, g_loss: 0.71912777\n",
      "Step: [2505] d_loss: 1.39531374, g_loss: 0.70670778\n",
      "Step: [2506] d_loss: 1.38121974, g_loss: 0.72138345\n",
      "Step: [2507] d_loss: 1.38390481, g_loss: 0.71921581\n",
      "Step: [2508] d_loss: 1.37748945, g_loss: 0.71584117\n",
      "Step: [2509] d_loss: 1.37447453, g_loss: 0.71878493\n",
      "Step: [2510] d_loss: 1.37451291, g_loss: 0.72306633\n",
      "Step: [2511] d_loss: 1.36519063, g_loss: 0.71981794\n",
      "Step: [2512] d_loss: 1.36822391, g_loss: 0.72040540\n",
      "Step: [2513] d_loss: 1.38078594, g_loss: 0.72898966\n",
      "Step: [2514] d_loss: 1.39361620, g_loss: 0.71515483\n",
      "Step: [2515] d_loss: 1.39463019, g_loss: 0.71282881\n",
      "Step: [2516] d_loss: 1.37313724, g_loss: 0.72155291\n",
      "Step: [2517] d_loss: 1.39768898, g_loss: 0.71939749\n",
      "Step: [2518] d_loss: 1.39094281, g_loss: 0.72664642\n",
      "Step: [2519] d_loss: 1.39890480, g_loss: 0.71223849\n",
      "Step: [2520] d_loss: 1.37912560, g_loss: 0.71999180\n",
      "Step: [2521] d_loss: 1.38985097, g_loss: 0.71617568\n",
      "Step: [2522] d_loss: 1.38559699, g_loss: 0.71978879\n",
      "Step: [2523] d_loss: 1.38676298, g_loss: 0.71489120\n",
      "Step: [2524] d_loss: 1.37540567, g_loss: 0.72334039\n",
      "Step: [2525] d_loss: 1.37442565, g_loss: 0.71578836\n",
      "Step: [2526] d_loss: 1.37970018, g_loss: 0.72065610\n",
      "Step: [2527] d_loss: 1.37577045, g_loss: 0.71298242\n",
      "Step: [2528] d_loss: 1.38078380, g_loss: 0.72820860\n",
      "Step: [2529] d_loss: 1.39203644, g_loss: 0.71741545\n",
      "Step: [2530] d_loss: 1.38295007, g_loss: 0.72683764\n",
      "Step: [2531] d_loss: 1.39253235, g_loss: 0.72235084\n",
      "Step: [2532] d_loss: 1.39408433, g_loss: 0.71903801\n",
      "Step: [2533] d_loss: 1.38712287, g_loss: 0.71928787\n",
      "Step: [2534] d_loss: 1.39831042, g_loss: 0.71895498\n",
      "Step: [2535] d_loss: 1.40038943, g_loss: 0.71759033\n",
      "Step: [2536] d_loss: 1.38027167, g_loss: 0.72177196\n",
      "Step: [2537] d_loss: 1.38708138, g_loss: 0.73535711\n",
      "Step: [2538] d_loss: 1.38250458, g_loss: 0.72904396\n",
      "Step: [2539] d_loss: 1.39990950, g_loss: 0.71795511\n",
      "Step: [2540] d_loss: 1.37950361, g_loss: 0.71211571\n",
      "Step: [2541] d_loss: 1.39012516, g_loss: 0.71123344\n",
      "Step: [2542] d_loss: 1.37991905, g_loss: 0.72223604\n",
      "Step: [2543] d_loss: 1.39706993, g_loss: 0.71415418\n",
      "Step: [2544] d_loss: 1.39499521, g_loss: 0.71601772\n",
      "Step: [2545] d_loss: 1.37112987, g_loss: 0.72106373\n",
      "Step: [2546] d_loss: 1.37315726, g_loss: 0.72062832\n",
      "Step: [2547] d_loss: 1.37852573, g_loss: 0.72054124\n",
      "Step: [2548] d_loss: 1.37507439, g_loss: 0.72619164\n",
      "Step: [2549] d_loss: 1.36866570, g_loss: 0.72035325\n",
      "Step: [2550] d_loss: 1.38187766, g_loss: 0.72168845\n",
      "Step: [2551] d_loss: 1.38097596, g_loss: 0.71163714\n",
      "Step: [2552] d_loss: 1.38396668, g_loss: 0.71629560\n",
      "Step: [2553] d_loss: 1.36020494, g_loss: 0.72724390\n",
      "Step: [2554] d_loss: 1.37121332, g_loss: 0.72776884\n",
      "Step: [2555] d_loss: 1.39014769, g_loss: 0.71761441\n",
      "Step: [2556] d_loss: 1.39017534, g_loss: 0.71311426\n",
      "Step: [2557] d_loss: 1.38108397, g_loss: 0.72055918\n",
      "Step: [2558] d_loss: 1.38076961, g_loss: 0.72133648\n",
      "Step: [2559] d_loss: 1.38755167, g_loss: 0.71815413\n",
      "Step: [2560] d_loss: 1.39482152, g_loss: 0.71115923\n",
      "Step: [2561] d_loss: 1.36964321, g_loss: 0.73138726\n",
      "Step: [2562] d_loss: 1.38209033, g_loss: 0.72610158\n",
      "Step: [2563] d_loss: 1.37740397, g_loss: 0.71933115\n",
      "Step: [2564] d_loss: 1.37781501, g_loss: 0.72238994\n",
      "Step: [2565] d_loss: 1.37590575, g_loss: 0.72529089\n",
      "Step: [2566] d_loss: 1.37624002, g_loss: 0.71879148\n",
      "Step: [2567] d_loss: 1.37771320, g_loss: 0.71776992\n",
      "Step: [2568] d_loss: 1.38079238, g_loss: 0.72524488\n",
      "Step: [2569] d_loss: 1.37137210, g_loss: 0.72264314\n",
      "Step: [2570] d_loss: 1.38358116, g_loss: 0.71125156\n",
      "Step: [2571] d_loss: 1.38037562, g_loss: 0.71965694\n",
      "Step: [2572] d_loss: 1.37291133, g_loss: 0.72646499\n",
      "Step: [2573] d_loss: 1.37623703, g_loss: 0.72077215\n",
      "Step: [2574] d_loss: 1.38125825, g_loss: 0.71349537\n",
      "Step: [2575] d_loss: 1.37772095, g_loss: 0.72771287\n",
      "Step: [2576] d_loss: 1.38576841, g_loss: 0.71799242\n",
      "Step: [2577] d_loss: 1.37185240, g_loss: 0.71968257\n",
      "Step: [2578] d_loss: 1.37730408, g_loss: 0.72449958\n",
      "Step: [2579] d_loss: 1.38608098, g_loss: 0.71773845\n",
      "Step: [2580] d_loss: 1.38205791, g_loss: 0.71827555\n",
      "Step: [2581] d_loss: 1.38984966, g_loss: 0.70598567\n",
      "Step: [2582] d_loss: 1.36807156, g_loss: 0.72470635\n",
      "Step: [2583] d_loss: 1.37341833, g_loss: 0.72742009\n",
      "Step: [2584] d_loss: 1.37529838, g_loss: 0.72269189\n",
      "Step: [2585] d_loss: 1.38084459, g_loss: 0.72221696\n",
      "Step: [2586] d_loss: 1.38463295, g_loss: 0.71912134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2587] d_loss: 1.37461269, g_loss: 0.71734035\n",
      "Step: [2588] d_loss: 1.38188589, g_loss: 0.72353584\n",
      "Step: [2589] d_loss: 1.38827026, g_loss: 0.71027243\n",
      "Step: [2590] d_loss: 1.36634398, g_loss: 0.73469430\n",
      "Step: [2591] d_loss: 1.39270926, g_loss: 0.71009815\n",
      "Step: [2592] d_loss: 1.37119818, g_loss: 0.72472107\n",
      "Step: [2593] d_loss: 1.38087714, g_loss: 0.71700209\n",
      "Step: [2594] d_loss: 1.38734066, g_loss: 0.71564710\n",
      "Step: [2595] d_loss: 1.39383292, g_loss: 0.71669829\n",
      "Step: [2596] d_loss: 1.37227130, g_loss: 0.72916174\n",
      "Step: [2597] d_loss: 1.37013602, g_loss: 0.72614306\n",
      "Step: [2598] d_loss: 1.37834692, g_loss: 0.72083443\n",
      "Step: [2599] d_loss: 1.39210916, g_loss: 0.71709818\n",
      "Step: [2600] d_loss: 1.37706304, g_loss: 0.72314090\n",
      "Step: [2601] d_loss: 1.38502264, g_loss: 0.72142363\n",
      "Step: [2602] d_loss: 1.37520599, g_loss: 0.72783780\n",
      "Step: [2603] d_loss: 1.38092732, g_loss: 0.71664518\n",
      "Step: [2604] d_loss: 1.39791846, g_loss: 0.70878458\n",
      "Step: [2605] d_loss: 1.38154960, g_loss: 0.71732807\n",
      "Step: [2606] d_loss: 1.37003779, g_loss: 0.71323299\n",
      "Step: [2607] d_loss: 1.37495971, g_loss: 0.72327805\n",
      "Step: [2608] d_loss: 1.38060808, g_loss: 0.72027862\n",
      "Step: [2609] d_loss: 1.38617051, g_loss: 0.71990329\n",
      "Step: [2610] d_loss: 1.38138676, g_loss: 0.72962451\n",
      "Step: [2611] d_loss: 1.39195740, g_loss: 0.71812499\n",
      "Step: [2612] d_loss: 1.37956500, g_loss: 0.71965605\n",
      "Step: [2613] d_loss: 1.37841153, g_loss: 0.72180259\n",
      "Step: [2614] d_loss: 1.37678134, g_loss: 0.72142667\n",
      "Step: [2615] d_loss: 1.38081646, g_loss: 0.71693927\n",
      "Step: [2616] d_loss: 1.37154222, g_loss: 0.72509193\n",
      "Step: [2617] d_loss: 1.37464237, g_loss: 0.72255647\n",
      "Step: [2618] d_loss: 1.37938368, g_loss: 0.72748542\n",
      "Step: [2619] d_loss: 1.36392581, g_loss: 0.72542036\n",
      "Step: [2620] d_loss: 1.36456013, g_loss: 0.73193264\n",
      "Step: [2621] d_loss: 1.37876284, g_loss: 0.72485858\n",
      "Step: [2622] d_loss: 1.39338565, g_loss: 0.72205621\n",
      "Step: [2623] d_loss: 1.37927401, g_loss: 0.72853917\n",
      "Step: [2624] d_loss: 1.39248276, g_loss: 0.72825247\n",
      "Step: [2625] d_loss: 1.38569987, g_loss: 0.71699321\n",
      "Step: [2626] d_loss: 1.37169099, g_loss: 0.72741747\n",
      "Step: [2627] d_loss: 1.38665581, g_loss: 0.71373600\n",
      "Step: [2628] d_loss: 1.37597358, g_loss: 0.71913135\n",
      "Step: [2629] d_loss: 1.37519372, g_loss: 0.72506022\n",
      "Step: [2630] d_loss: 1.37169409, g_loss: 0.72329760\n",
      "Step: [2631] d_loss: 1.37208414, g_loss: 0.72684705\n",
      "Step: [2632] d_loss: 1.38632751, g_loss: 0.71297503\n",
      "Step: [2633] d_loss: 1.37703681, g_loss: 0.72283304\n",
      "Step: [2634] d_loss: 1.38340652, g_loss: 0.70985901\n",
      "Step: [2635] d_loss: 1.39342391, g_loss: 0.71513736\n",
      "Step: [2636] d_loss: 1.40119362, g_loss: 0.71280831\n",
      "Step: [2637] d_loss: 1.39827073, g_loss: 0.71596181\n",
      "Step: [2638] d_loss: 1.38682365, g_loss: 0.72000188\n",
      "Step: [2639] d_loss: 1.37869155, g_loss: 0.71950841\n",
      "Step: [2640] d_loss: 1.38511825, g_loss: 0.71639758\n",
      "Step: [2641] d_loss: 1.38338113, g_loss: 0.71251965\n",
      "Step: [2642] d_loss: 1.37487006, g_loss: 0.72431684\n",
      "Step: [2643] d_loss: 1.39559412, g_loss: 0.71857226\n",
      "Step: [2644] d_loss: 1.38262153, g_loss: 0.71703196\n",
      "Step: [2645] d_loss: 1.38486743, g_loss: 0.72736108\n",
      "Step: [2646] d_loss: 1.37752914, g_loss: 0.72543204\n",
      "Step: [2647] d_loss: 1.37891698, g_loss: 0.72527045\n",
      "Step: [2648] d_loss: 1.39291334, g_loss: 0.71557176\n",
      "Step: [2649] d_loss: 1.38057256, g_loss: 0.72502548\n",
      "Step: [2650] d_loss: 1.38919854, g_loss: 0.72231078\n",
      "Step: [2651] d_loss: 1.38264298, g_loss: 0.72346795\n",
      "Step: [2652] d_loss: 1.38275838, g_loss: 0.71688116\n",
      "Step: [2653] d_loss: 1.37404728, g_loss: 0.71803403\n",
      "Step: [2654] d_loss: 1.39025140, g_loss: 0.71373171\n",
      "Step: [2655] d_loss: 1.37413669, g_loss: 0.72551066\n",
      "Step: [2656] d_loss: 1.37575197, g_loss: 0.72575784\n",
      "Step: [2657] d_loss: 1.36390328, g_loss: 0.73238146\n",
      "Step: [2658] d_loss: 1.37892485, g_loss: 0.72901022\n",
      "Step: [2659] d_loss: 1.38155055, g_loss: 0.72138131\n",
      "Step: [2660] d_loss: 1.38495648, g_loss: 0.71610951\n",
      "Step: [2661] d_loss: 1.37620544, g_loss: 0.71674573\n",
      "Step: [2662] d_loss: 1.38804197, g_loss: 0.72014618\n",
      "Step: [2663] d_loss: 1.37883830, g_loss: 0.72107971\n",
      "Step: [2664] d_loss: 1.37230909, g_loss: 0.72917980\n",
      "Step: [2665] d_loss: 1.38441634, g_loss: 0.72491068\n",
      "Step: [2666] d_loss: 1.38128674, g_loss: 0.72191167\n",
      "Step: [2667] d_loss: 1.36481678, g_loss: 0.73201716\n",
      "Step: [2668] d_loss: 1.37358177, g_loss: 0.73194039\n",
      "Step: [2669] d_loss: 1.39491773, g_loss: 0.71559179\n",
      "Step: [2670] d_loss: 1.38381791, g_loss: 0.71978140\n",
      "Step: [2671] d_loss: 1.38643491, g_loss: 0.71789390\n",
      "Step: [2672] d_loss: 1.37558341, g_loss: 0.72288048\n",
      "Step: [2673] d_loss: 1.38174200, g_loss: 0.72083431\n",
      "Step: [2674] d_loss: 1.39048553, g_loss: 0.72133815\n",
      "Step: [2675] d_loss: 1.40586591, g_loss: 0.73013377\n",
      "Step: [2676] d_loss: 1.41448140, g_loss: 0.71934813\n",
      "Step: [2677] d_loss: 1.40137839, g_loss: 0.71991348\n",
      "Step: [2678] d_loss: 1.38347435, g_loss: 0.71435618\n",
      "Step: [2679] d_loss: 1.37813628, g_loss: 0.71690500\n",
      "Step: [2680] d_loss: 1.38642740, g_loss: 0.71793795\n",
      "Step: [2681] d_loss: 1.38387775, g_loss: 0.72120464\n",
      "Step: [2682] d_loss: 1.39055479, g_loss: 0.71108246\n",
      "Step: [2683] d_loss: 1.38463199, g_loss: 0.71506178\n",
      "Step: [2684] d_loss: 1.37516236, g_loss: 0.71618581\n",
      "Step: [2685] d_loss: 1.38445234, g_loss: 0.71584433\n",
      "Step: [2686] d_loss: 1.38239503, g_loss: 0.71869361\n",
      "Step: [2687] d_loss: 1.36709976, g_loss: 0.72803849\n",
      "Step: [2688] d_loss: 1.36977148, g_loss: 0.72100794\n",
      "Step: [2689] d_loss: 1.37484407, g_loss: 0.72702050\n",
      "Step: [2690] d_loss: 1.37151718, g_loss: 0.72146511\n",
      "Step: [2691] d_loss: 1.38761711, g_loss: 0.71845210\n",
      "Step: [2692] d_loss: 1.37851322, g_loss: 0.71582198\n",
      "Step: [2693] d_loss: 1.38583696, g_loss: 0.71474105\n",
      "Step: [2694] d_loss: 1.38019872, g_loss: 0.72634935\n",
      "Step: [2695] d_loss: 1.37954605, g_loss: 0.72099340\n",
      "Step: [2696] d_loss: 1.38901472, g_loss: 0.71930760\n",
      "Step: [2697] d_loss: 1.37351847, g_loss: 0.72128522\n",
      "Step: [2698] d_loss: 1.37877595, g_loss: 0.71610254\n",
      "Step: [2699] d_loss: 1.38232410, g_loss: 0.71788293\n",
      "Step: [2700] d_loss: 1.38808572, g_loss: 0.71364117\n",
      "Step: [2701] d_loss: 1.37799478, g_loss: 0.72124982\n",
      "Step: [2702] d_loss: 1.38536608, g_loss: 0.71881032\n",
      "Step: [2703] d_loss: 1.36644685, g_loss: 0.72659373\n",
      "Step: [2704] d_loss: 1.37934256, g_loss: 0.72693557\n",
      "Step: [2705] d_loss: 1.37768388, g_loss: 0.72034711\n",
      "Step: [2706] d_loss: 1.38744712, g_loss: 0.71041381\n",
      "Step: [2707] d_loss: 1.37255812, g_loss: 0.72002840\n",
      "Step: [2708] d_loss: 1.37672102, g_loss: 0.72622889\n",
      "Step: [2709] d_loss: 1.39086461, g_loss: 0.72204781\n",
      "Step: [2710] d_loss: 1.38503122, g_loss: 0.71899009\n",
      "Step: [2711] d_loss: 1.38202262, g_loss: 0.71912408\n",
      "Step: [2712] d_loss: 1.37139702, g_loss: 0.72364628\n",
      "Step: [2713] d_loss: 1.37276709, g_loss: 0.72195399\n",
      "Step: [2714] d_loss: 1.37332869, g_loss: 0.72393322\n",
      "Step: [2715] d_loss: 1.37618208, g_loss: 0.71726614\n",
      "Step: [2716] d_loss: 1.38138270, g_loss: 0.72578037\n",
      "Step: [2717] d_loss: 1.37963390, g_loss: 0.71598661\n",
      "Step: [2718] d_loss: 1.38111055, g_loss: 0.71669734\n",
      "Step: [2719] d_loss: 1.38624239, g_loss: 0.72203708\n",
      "Step: [2720] d_loss: 1.38690925, g_loss: 0.71059895\n",
      "Step: [2721] d_loss: 1.38074827, g_loss: 0.72170329\n",
      "Step: [2722] d_loss: 1.37580514, g_loss: 0.72044063\n",
      "Step: [2723] d_loss: 1.38122416, g_loss: 0.72191811\n",
      "Step: [2724] d_loss: 1.37764323, g_loss: 0.71943808\n",
      "Step: [2725] d_loss: 1.39658070, g_loss: 0.71544027\n",
      "Step: [2726] d_loss: 1.37967384, g_loss: 0.71587729\n",
      "Step: [2727] d_loss: 1.37368107, g_loss: 0.72458088\n",
      "Step: [2728] d_loss: 1.37612391, g_loss: 0.71585464\n",
      "Step: [2729] d_loss: 1.37623501, g_loss: 0.72766209\n",
      "Step: [2730] d_loss: 1.38714635, g_loss: 0.72098327\n",
      "Step: [2731] d_loss: 1.37334287, g_loss: 0.72468412\n",
      "Step: [2732] d_loss: 1.38438046, g_loss: 0.71930701\n",
      "Step: [2733] d_loss: 1.38909435, g_loss: 0.72036636\n",
      "Step: [2734] d_loss: 1.39325750, g_loss: 0.71243072\n",
      "Step: [2735] d_loss: 1.37523150, g_loss: 0.71556181\n",
      "Step: [2736] d_loss: 1.37451434, g_loss: 0.71988368\n",
      "Step: [2737] d_loss: 1.37327886, g_loss: 0.72349060\n",
      "Step: [2738] d_loss: 1.36618042, g_loss: 0.72616553\n",
      "Step: [2739] d_loss: 1.36949599, g_loss: 0.72487444\n",
      "Step: [2740] d_loss: 1.39315581, g_loss: 0.71977633\n",
      "Step: [2741] d_loss: 1.37554073, g_loss: 0.72091115\n",
      "Step: [2742] d_loss: 1.37078738, g_loss: 0.71920490\n",
      "Step: [2743] d_loss: 1.38376951, g_loss: 0.71629953\n",
      "Step: [2744] d_loss: 1.38478065, g_loss: 0.72817540\n",
      "Step: [2745] d_loss: 1.36788106, g_loss: 0.72594249\n",
      "Step: [2746] d_loss: 1.38239121, g_loss: 0.72334111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2747] d_loss: 1.37406397, g_loss: 0.72202981\n",
      "Step: [2748] d_loss: 1.37366319, g_loss: 0.72849327\n",
      "Step: [2749] d_loss: 1.37897062, g_loss: 0.71884787\n",
      "Step: [2750] d_loss: 1.37276864, g_loss: 0.72567213\n",
      "Step: [2751] d_loss: 1.38513100, g_loss: 0.72107470\n",
      "Step: [2752] d_loss: 1.38496506, g_loss: 0.71286595\n",
      "Step: [2753] d_loss: 1.38319898, g_loss: 0.73151404\n",
      "Step: [2754] d_loss: 1.37782502, g_loss: 0.72208059\n",
      "Step: [2755] d_loss: 1.37694168, g_loss: 0.72517741\n",
      "Step: [2756] d_loss: 1.37490952, g_loss: 0.72134495\n",
      "Step: [2757] d_loss: 1.38700819, g_loss: 0.72511417\n",
      "Step: [2758] d_loss: 1.38226366, g_loss: 0.72265589\n",
      "Step: [2759] d_loss: 1.38240600, g_loss: 0.72383344\n",
      "Step: [2760] d_loss: 1.38161874, g_loss: 0.73104155\n",
      "Step: [2761] d_loss: 1.39268351, g_loss: 0.71498388\n",
      "Step: [2762] d_loss: 1.40377069, g_loss: 0.71848297\n",
      "Step: [2763] d_loss: 1.38149762, g_loss: 0.72394454\n",
      "Step: [2764] d_loss: 1.39816356, g_loss: 0.71398950\n",
      "Step: [2765] d_loss: 1.37697911, g_loss: 0.71514654\n",
      "Step: [2766] d_loss: 1.37051821, g_loss: 0.72235203\n",
      "Step: [2767] d_loss: 1.37833405, g_loss: 0.72402430\n",
      "Step: [2768] d_loss: 1.35663819, g_loss: 0.73628920\n",
      "Step: [2769] d_loss: 1.37869954, g_loss: 0.72699857\n",
      "Step: [2770] d_loss: 1.38097429, g_loss: 0.71969813\n",
      "Step: [2771] d_loss: 1.38918281, g_loss: 0.71266806\n",
      "Step: [2772] d_loss: 1.37546206, g_loss: 0.72568876\n",
      "Step: [2773] d_loss: 1.39327800, g_loss: 0.72339231\n",
      "Step: [2774] d_loss: 1.37814665, g_loss: 0.71968555\n",
      "Step: [2775] d_loss: 1.38113928, g_loss: 0.71791804\n",
      "Step: [2776] d_loss: 1.38082862, g_loss: 0.72467649\n",
      "Step: [2777] d_loss: 1.39237332, g_loss: 0.71208262\n",
      "Step: [2778] d_loss: 1.37800527, g_loss: 0.72004271\n",
      "Step: [2779] d_loss: 1.39132667, g_loss: 0.71700835\n",
      "Step: [2780] d_loss: 1.38163638, g_loss: 0.72388834\n",
      "Step: [2781] d_loss: 1.38611054, g_loss: 0.72039050\n",
      "Step: [2782] d_loss: 1.38679242, g_loss: 0.71532983\n",
      "Step: [2783] d_loss: 1.37327611, g_loss: 0.72220123\n",
      "Step: [2784] d_loss: 1.37952316, g_loss: 0.71979052\n",
      "Step: [2785] d_loss: 1.38146651, g_loss: 0.72271603\n",
      "Step: [2786] d_loss: 1.38566256, g_loss: 0.70687616\n",
      "Step: [2787] d_loss: 1.37329173, g_loss: 0.72362816\n",
      "Step: [2788] d_loss: 1.37151170, g_loss: 0.72757542\n",
      "Step: [2789] d_loss: 1.38481712, g_loss: 0.71809578\n",
      "Step: [2790] d_loss: 1.38229418, g_loss: 0.71736372\n",
      "Step: [2791] d_loss: 1.37267756, g_loss: 0.72608763\n",
      "Step: [2792] d_loss: 1.36653709, g_loss: 0.72605842\n",
      "Step: [2793] d_loss: 1.38399243, g_loss: 0.71650255\n",
      "Step: [2794] d_loss: 1.37297273, g_loss: 0.72160375\n",
      "Step: [2795] d_loss: 1.37555981, g_loss: 0.73693836\n",
      "Step: [2796] d_loss: 1.37434471, g_loss: 0.73032522\n",
      "Step: [2797] d_loss: 1.39238119, g_loss: 0.71944487\n",
      "Step: [2798] d_loss: 1.38102174, g_loss: 0.71523619\n",
      "Step: [2799] d_loss: 1.38482165, g_loss: 0.71881151\n",
      "Step: [2800] d_loss: 1.37764287, g_loss: 0.71465671\n",
      "Step: [2801] d_loss: 1.38506997, g_loss: 0.71687722\n",
      "Step: [2802] d_loss: 1.38906133, g_loss: 0.71738005\n",
      "Step: [2803] d_loss: 1.36419606, g_loss: 0.72787285\n",
      "Step: [2804] d_loss: 1.39365816, g_loss: 0.71267021\n",
      "Step: [2805] d_loss: 1.38260007, g_loss: 0.72191167\n",
      "Step: [2806] d_loss: 1.39569163, g_loss: 0.70836902\n",
      "Step: [2807] d_loss: 1.37795186, g_loss: 0.71931791\n",
      "Step: [2808] d_loss: 1.37998176, g_loss: 0.71318275\n",
      "Step: [2809] d_loss: 1.38368726, g_loss: 0.71744633\n",
      "Step: [2810] d_loss: 1.37964749, g_loss: 0.72696173\n",
      "Step: [2811] d_loss: 1.36456943, g_loss: 0.73187107\n",
      "Step: [2812] d_loss: 1.37494481, g_loss: 0.72352475\n",
      "Step: [2813] d_loss: 1.38446021, g_loss: 0.72286981\n",
      "Step: [2814] d_loss: 1.38147628, g_loss: 0.71918505\n",
      "Step: [2815] d_loss: 1.38485444, g_loss: 0.71365470\n",
      "Step: [2816] d_loss: 1.37471771, g_loss: 0.72984833\n",
      "Step: [2817] d_loss: 1.37979043, g_loss: 0.72409940\n",
      "Step: [2818] d_loss: 1.36461949, g_loss: 0.72919053\n",
      "Step: [2819] d_loss: 1.35776019, g_loss: 0.73915821\n",
      "Step: [2820] d_loss: 1.36013877, g_loss: 0.73329806\n",
      "Step: [2821] d_loss: 1.37188792, g_loss: 0.72228611\n",
      "Step: [2822] d_loss: 1.37312508, g_loss: 0.72694576\n",
      "Step: [2823] d_loss: 1.36562216, g_loss: 0.73064470\n",
      "Step: [2824] d_loss: 1.36885858, g_loss: 0.72933364\n",
      "Step: [2825] d_loss: 1.38212967, g_loss: 0.72023594\n",
      "Step: [2826] d_loss: 1.39841557, g_loss: 0.71439600\n",
      "Step: [2827] d_loss: 1.38982749, g_loss: 0.72380275\n",
      "Step: [2828] d_loss: 1.37342238, g_loss: 0.73727858\n",
      "Step: [2829] d_loss: 1.37289810, g_loss: 0.72473574\n",
      "Step: [2830] d_loss: 1.38968933, g_loss: 0.71432757\n",
      "Step: [2831] d_loss: 1.38277388, g_loss: 0.72054136\n",
      "Step: [2832] d_loss: 1.38073039, g_loss: 0.72621512\n",
      "Step: [2833] d_loss: 1.38258922, g_loss: 0.71880841\n",
      "Step: [2834] d_loss: 1.38185811, g_loss: 0.71998262\n",
      "Step: [2835] d_loss: 1.37816083, g_loss: 0.72256702\n",
      "Step: [2836] d_loss: 1.38435912, g_loss: 0.72317219\n",
      "Step: [2837] d_loss: 1.39815092, g_loss: 0.71419752\n",
      "Step: [2838] d_loss: 1.37981164, g_loss: 0.72105467\n",
      "Step: [2839] d_loss: 1.37925017, g_loss: 0.72590137\n",
      "Step: [2840] d_loss: 1.37833500, g_loss: 0.72906077\n",
      "Step: [2841] d_loss: 1.37975371, g_loss: 0.72130048\n",
      "Step: [2842] d_loss: 1.36873126, g_loss: 0.72523606\n",
      "Step: [2843] d_loss: 1.39008069, g_loss: 0.71818143\n",
      "Step: [2844] d_loss: 1.38020039, g_loss: 0.72633928\n",
      "Step: [2845] d_loss: 1.36519027, g_loss: 0.72905570\n",
      "Step: [2846] d_loss: 1.38532329, g_loss: 0.72365236\n",
      "Step: [2847] d_loss: 1.37100768, g_loss: 0.72350538\n",
      "Step: [2848] d_loss: 1.37933815, g_loss: 0.72633630\n",
      "Step: [2849] d_loss: 1.39157081, g_loss: 0.71525359\n",
      "Step: [2850] d_loss: 1.39214253, g_loss: 0.71286392\n",
      "Step: [2851] d_loss: 1.38263488, g_loss: 0.72061908\n",
      "Step: [2852] d_loss: 1.37830925, g_loss: 0.72653610\n",
      "Step: [2853] d_loss: 1.38723564, g_loss: 0.72158408\n",
      "Step: [2854] d_loss: 1.39074957, g_loss: 0.71906161\n",
      "Step: [2855] d_loss: 1.39173222, g_loss: 0.73102701\n",
      "Step: [2856] d_loss: 1.40965641, g_loss: 0.73281324\n",
      "Step: [2857] d_loss: 1.40870559, g_loss: 0.72670579\n",
      "Step: [2858] d_loss: 1.39111710, g_loss: 0.71863776\n",
      "Step: [2859] d_loss: 1.38256192, g_loss: 0.72240412\n",
      "Step: [2860] d_loss: 1.39360285, g_loss: 0.71621543\n",
      "Step: [2861] d_loss: 1.38020062, g_loss: 0.72293878\n",
      "Step: [2862] d_loss: 1.37598372, g_loss: 0.71932018\n",
      "Step: [2863] d_loss: 1.37326097, g_loss: 0.72601956\n",
      "Step: [2864] d_loss: 1.37481797, g_loss: 0.72541976\n",
      "Step: [2865] d_loss: 1.38535857, g_loss: 0.71292508\n",
      "Step: [2866] d_loss: 1.36986363, g_loss: 0.72270554\n",
      "Step: [2867] d_loss: 1.38744521, g_loss: 0.71923995\n",
      "Step: [2868] d_loss: 1.39188600, g_loss: 0.73785603\n",
      "Step: [2869] d_loss: 1.39584935, g_loss: 0.71495956\n",
      "Step: [2870] d_loss: 1.37548554, g_loss: 0.72425437\n",
      "Step: [2871] d_loss: 1.38911748, g_loss: 0.71896118\n",
      "Step: [2872] d_loss: 1.38365793, g_loss: 0.71486735\n",
      "Step: [2873] d_loss: 1.38095176, g_loss: 0.70968479\n",
      "Step: [2874] d_loss: 1.37168264, g_loss: 0.72871399\n",
      "Step: [2875] d_loss: 1.38137865, g_loss: 0.72277176\n",
      "Step: [2876] d_loss: 1.40137804, g_loss: 0.71221042\n",
      "Step: [2877] d_loss: 1.39155638, g_loss: 0.71456695\n",
      "Step: [2878] d_loss: 1.37816167, g_loss: 0.71810818\n",
      "Step: [2879] d_loss: 1.38172841, g_loss: 0.72250128\n",
      "Step: [2880] d_loss: 1.37283206, g_loss: 0.72076797\n",
      "Step: [2881] d_loss: 1.38185620, g_loss: 0.71846712\n",
      "Step: [2882] d_loss: 1.39022040, g_loss: 0.72160441\n",
      "Step: [2883] d_loss: 1.39485848, g_loss: 0.71515352\n",
      "Step: [2884] d_loss: 1.38641310, g_loss: 0.72335267\n",
      "Step: [2885] d_loss: 1.39436650, g_loss: 0.71992403\n",
      "Step: [2886] d_loss: 1.38015032, g_loss: 0.72908157\n",
      "Step: [2887] d_loss: 1.38187253, g_loss: 0.71689069\n",
      "Step: [2888] d_loss: 1.37409639, g_loss: 0.71907657\n",
      "Step: [2889] d_loss: 1.38964665, g_loss: 0.71463490\n",
      "Step: [2890] d_loss: 1.36804020, g_loss: 0.72473961\n",
      "Step: [2891] d_loss: 1.38795435, g_loss: 0.71461916\n",
      "Step: [2892] d_loss: 1.38304853, g_loss: 0.71928751\n",
      "Step: [2893] d_loss: 1.38974166, g_loss: 0.72284275\n",
      "Step: [2894] d_loss: 1.39369273, g_loss: 0.71726048\n",
      "Step: [2895] d_loss: 1.36271214, g_loss: 0.73043752\n",
      "Step: [2896] d_loss: 1.38503039, g_loss: 0.71746647\n",
      "Step: [2897] d_loss: 1.37757015, g_loss: 0.72180367\n",
      "Step: [2898] d_loss: 1.37063742, g_loss: 0.72497010\n",
      "Step: [2899] d_loss: 1.37713087, g_loss: 0.72374463\n",
      "Step: [2900] d_loss: 1.37631130, g_loss: 0.72337192\n",
      "Step: [2901] d_loss: 1.38641548, g_loss: 0.72186697\n",
      "Step: [2902] d_loss: 1.38085234, g_loss: 0.72430450\n",
      "Step: [2903] d_loss: 1.38366580, g_loss: 0.71776295\n",
      "Step: [2904] d_loss: 1.38145280, g_loss: 0.71644962\n",
      "Step: [2905] d_loss: 1.37419629, g_loss: 0.71495265\n",
      "Step: [2906] d_loss: 1.37445521, g_loss: 0.72920781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2907] d_loss: 1.37319958, g_loss: 0.73431218\n",
      "Step: [2908] d_loss: 1.39609265, g_loss: 0.72111726\n",
      "Step: [2909] d_loss: 1.37941575, g_loss: 0.71637124\n",
      "Step: [2910] d_loss: 1.37626636, g_loss: 0.72143555\n",
      "Step: [2911] d_loss: 1.39207602, g_loss: 0.71649092\n",
      "Step: [2912] d_loss: 1.38740611, g_loss: 0.70861578\n",
      "Step: [2913] d_loss: 1.38774204, g_loss: 0.72423208\n",
      "Step: [2914] d_loss: 1.38402414, g_loss: 0.71793616\n",
      "Step: [2915] d_loss: 1.38031399, g_loss: 0.72783637\n",
      "Step: [2916] d_loss: 1.38249981, g_loss: 0.71303761\n",
      "Step: [2917] d_loss: 1.38497627, g_loss: 0.71005821\n",
      "Step: [2918] d_loss: 1.37775683, g_loss: 0.72647381\n",
      "Step: [2919] d_loss: 1.36463308, g_loss: 0.72843421\n",
      "Step: [2920] d_loss: 1.38447213, g_loss: 0.72001398\n",
      "Step: [2921] d_loss: 1.36477029, g_loss: 0.72818506\n",
      "Step: [2922] d_loss: 1.38455629, g_loss: 0.71064997\n",
      "Step: [2923] d_loss: 1.37506199, g_loss: 0.72970986\n",
      "Step: [2924] d_loss: 1.37378085, g_loss: 0.73187417\n",
      "Step: [2925] d_loss: 1.37796891, g_loss: 0.72180808\n",
      "Step: [2926] d_loss: 1.37400758, g_loss: 0.72604066\n",
      "Step: [2927] d_loss: 1.37628174, g_loss: 0.71989214\n",
      "Step: [2928] d_loss: 1.36871243, g_loss: 0.71554285\n",
      "Step: [2929] d_loss: 1.37382483, g_loss: 0.72545332\n",
      "Step: [2930] d_loss: 1.37179637, g_loss: 0.72358775\n",
      "Step: [2931] d_loss: 1.38488078, g_loss: 0.72079009\n",
      "Step: [2932] d_loss: 1.39703298, g_loss: 0.71859312\n",
      "Step: [2933] d_loss: 1.38438678, g_loss: 0.72878587\n",
      "Step: [2934] d_loss: 1.38341355, g_loss: 0.71951306\n",
      "Step: [2935] d_loss: 1.39989591, g_loss: 0.70088023\n",
      "Step: [2936] d_loss: 1.39030647, g_loss: 0.72007781\n",
      "Step: [2937] d_loss: 1.37987256, g_loss: 0.72235358\n",
      "Step: [2938] d_loss: 1.38322246, g_loss: 0.71836793\n",
      "Step: [2939] d_loss: 1.38651156, g_loss: 0.72372442\n",
      "Step: [2940] d_loss: 1.38897097, g_loss: 0.70820856\n",
      "Step: [2941] d_loss: 1.40343785, g_loss: 0.71746582\n",
      "Step: [2942] d_loss: 1.38464034, g_loss: 0.71227956\n",
      "Step: [2943] d_loss: 1.37421870, g_loss: 0.72649330\n",
      "Step: [2944] d_loss: 1.38509595, g_loss: 0.71725833\n",
      "Step: [2945] d_loss: 1.38818598, g_loss: 0.71095645\n",
      "Step: [2946] d_loss: 1.38847017, g_loss: 0.71592283\n",
      "Step: [2947] d_loss: 1.38573217, g_loss: 0.72509587\n",
      "Step: [2948] d_loss: 1.39306211, g_loss: 0.71734929\n",
      "Step: [2949] d_loss: 1.38773012, g_loss: 0.71264458\n",
      "Step: [2950] d_loss: 1.37695551, g_loss: 0.72524214\n",
      "Step: [2951] d_loss: 1.36718225, g_loss: 0.71531594\n",
      "Step: [2952] d_loss: 1.38129556, g_loss: 0.71551681\n",
      "Step: [2953] d_loss: 1.38948607, g_loss: 0.71879995\n",
      "Step: [2954] d_loss: 1.39522815, g_loss: 0.72023332\n",
      "Step: [2955] d_loss: 1.37100434, g_loss: 0.72594947\n",
      "Step: [2956] d_loss: 1.38633907, g_loss: 0.72338909\n",
      "Step: [2957] d_loss: 1.38458562, g_loss: 0.71743155\n",
      "Step: [2958] d_loss: 1.37682176, g_loss: 0.71862781\n",
      "Step: [2959] d_loss: 1.38436878, g_loss: 0.71933389\n",
      "Step: [2960] d_loss: 1.36979771, g_loss: 0.72895682\n",
      "Step: [2961] d_loss: 1.38710546, g_loss: 0.71909982\n",
      "Step: [2962] d_loss: 1.39054775, g_loss: 0.71713340\n",
      "Step: [2963] d_loss: 1.38475060, g_loss: 0.72816110\n",
      "Step: [2964] d_loss: 1.37580419, g_loss: 0.72238702\n",
      "Step: [2965] d_loss: 1.39214468, g_loss: 0.71446788\n",
      "Step: [2966] d_loss: 1.38209295, g_loss: 0.71381575\n",
      "Step: [2967] d_loss: 1.38331914, g_loss: 0.71552002\n",
      "Step: [2968] d_loss: 1.37236834, g_loss: 0.72006238\n",
      "Step: [2969] d_loss: 1.38658667, g_loss: 0.71517336\n",
      "Step: [2970] d_loss: 1.37761283, g_loss: 0.71920484\n",
      "Step: [2971] d_loss: 1.37941444, g_loss: 0.71913683\n",
      "Step: [2972] d_loss: 1.38331723, g_loss: 0.71409190\n",
      "Step: [2973] d_loss: 1.36776733, g_loss: 0.72675359\n",
      "Step: [2974] d_loss: 1.38005662, g_loss: 0.71636033\n",
      "Step: [2975] d_loss: 1.37649250, g_loss: 0.72591960\n",
      "Step: [2976] d_loss: 1.36203170, g_loss: 0.73364341\n",
      "Step: [2977] d_loss: 1.37691164, g_loss: 0.73003942\n",
      "Step: [2978] d_loss: 1.37570989, g_loss: 0.72401971\n",
      "Step: [2979] d_loss: 1.38383985, g_loss: 0.71839637\n",
      "Step: [2980] d_loss: 1.39012551, g_loss: 0.71257114\n",
      "Step: [2981] d_loss: 1.38357437, g_loss: 0.72528934\n",
      "Step: [2982] d_loss: 1.38610065, g_loss: 0.72532147\n",
      "Step: [2983] d_loss: 1.37786007, g_loss: 0.72331226\n",
      "Step: [2984] d_loss: 1.37571239, g_loss: 0.72019732\n",
      "Step: [2985] d_loss: 1.36684084, g_loss: 0.72872782\n",
      "Step: [2986] d_loss: 1.38541842, g_loss: 0.71188879\n",
      "Step: [2987] d_loss: 1.38143373, g_loss: 0.72048867\n",
      "Step: [2988] d_loss: 1.36990595, g_loss: 0.72601235\n",
      "Step: [2989] d_loss: 1.38499355, g_loss: 0.72191966\n",
      "Step: [2990] d_loss: 1.38299215, g_loss: 0.71879816\n",
      "Step: [2991] d_loss: 1.39743114, g_loss: 0.71633106\n",
      "Step: [2992] d_loss: 1.38264024, g_loss: 0.72223461\n",
      "Step: [2993] d_loss: 1.37018406, g_loss: 0.72705513\n",
      "Step: [2994] d_loss: 1.37631059, g_loss: 0.72304124\n",
      "Step: [2995] d_loss: 1.38914347, g_loss: 0.72472608\n",
      "Step: [2996] d_loss: 1.37704635, g_loss: 0.72763199\n",
      "Step: [2997] d_loss: 1.37947607, g_loss: 0.71456027\n",
      "Step: [2998] d_loss: 1.38454068, g_loss: 0.72050226\n",
      "Step: [2999] d_loss: 1.37396693, g_loss: 0.72472763\n",
      "Step: [3000] d_loss: 1.39641607, g_loss: 0.70956600\n",
      "Step: [3001] d_loss: 1.38408422, g_loss: 0.72060633\n",
      "Step: [3002] d_loss: 1.37828779, g_loss: 0.72324800\n",
      "Step: [3003] d_loss: 1.36906826, g_loss: 0.72395241\n",
      "Step: [3004] d_loss: 1.38988400, g_loss: 0.71167088\n",
      "Step: [3005] d_loss: 1.38169587, g_loss: 0.72039425\n",
      "Step: [3006] d_loss: 1.38487887, g_loss: 0.71948045\n",
      "Step: [3007] d_loss: 1.36636186, g_loss: 0.72843874\n",
      "Step: [3008] d_loss: 1.38015485, g_loss: 0.71446484\n",
      "Step: [3009] d_loss: 1.37362623, g_loss: 0.72429156\n",
      "Step: [3010] d_loss: 1.38648081, g_loss: 0.71858168\n",
      "Step: [3011] d_loss: 1.38810611, g_loss: 0.72415006\n",
      "Step: [3012] d_loss: 1.37425542, g_loss: 0.72497213\n",
      "Step: [3013] d_loss: 1.38325846, g_loss: 0.72413760\n",
      "Step: [3014] d_loss: 1.38612294, g_loss: 0.72036576\n",
      "Step: [3015] d_loss: 1.37159109, g_loss: 0.72494447\n",
      "Step: [3016] d_loss: 1.37164044, g_loss: 0.72555459\n",
      "Step: [3017] d_loss: 1.38631964, g_loss: 0.71876299\n",
      "Step: [3018] d_loss: 1.38886428, g_loss: 0.71453536\n",
      "Step: [3019] d_loss: 1.38989031, g_loss: 0.70923686\n",
      "Step: [3020] d_loss: 1.37231851, g_loss: 0.72766012\n",
      "Step: [3021] d_loss: 1.39919949, g_loss: 0.71162891\n",
      "Step: [3022] d_loss: 1.37487221, g_loss: 0.71888530\n",
      "Step: [3023] d_loss: 1.38585544, g_loss: 0.71643800\n",
      "Step: [3024] d_loss: 1.38421464, g_loss: 0.72266507\n",
      "Step: [3025] d_loss: 1.39542103, g_loss: 0.71235335\n",
      "Step: [3026] d_loss: 1.36989570, g_loss: 0.72316182\n",
      "Step: [3027] d_loss: 1.39032447, g_loss: 0.71207619\n",
      "Step: [3028] d_loss: 1.39245772, g_loss: 0.72581255\n",
      "Step: [3029] d_loss: 1.39314330, g_loss: 0.72385478\n",
      "Step: [3030] d_loss: 1.38979602, g_loss: 0.72295296\n",
      "Step: [3031] d_loss: 1.37668800, g_loss: 0.72056323\n",
      "Step: [3032] d_loss: 1.36543632, g_loss: 0.72479308\n",
      "Step: [3033] d_loss: 1.39161825, g_loss: 0.71177125\n",
      "Step: [3034] d_loss: 1.38071251, g_loss: 0.71731603\n",
      "Step: [3035] d_loss: 1.38211608, g_loss: 0.71644294\n",
      "Step: [3036] d_loss: 1.38881886, g_loss: 0.71747243\n",
      "Step: [3037] d_loss: 1.38850689, g_loss: 0.71410441\n",
      "Step: [3038] d_loss: 1.37642074, g_loss: 0.71707749\n",
      "Step: [3039] d_loss: 1.38462186, g_loss: 0.71632624\n",
      "Step: [3040] d_loss: 1.37307489, g_loss: 0.71801722\n",
      "Step: [3041] d_loss: 1.38227701, g_loss: 0.71524322\n",
      "Step: [3042] d_loss: 1.37085629, g_loss: 0.72437602\n",
      "Step: [3043] d_loss: 1.39762998, g_loss: 0.71521175\n",
      "Step: [3044] d_loss: 1.39802313, g_loss: 0.72112280\n",
      "Step: [3045] d_loss: 1.38746321, g_loss: 0.71892440\n",
      "Step: [3046] d_loss: 1.38376355, g_loss: 0.72252083\n",
      "Step: [3047] d_loss: 1.38502491, g_loss: 0.71958864\n",
      "Step: [3048] d_loss: 1.38483191, g_loss: 0.71215504\n",
      "Step: [3049] d_loss: 1.38242650, g_loss: 0.72224325\n",
      "Step: [3050] d_loss: 1.37699068, g_loss: 0.72899151\n",
      "Step: [3051] d_loss: 1.37840557, g_loss: 0.71867943\n",
      "Step: [3052] d_loss: 1.36950660, g_loss: 0.72324705\n",
      "Step: [3053] d_loss: 1.38372278, g_loss: 0.72476345\n",
      "Step: [3054] d_loss: 1.38148499, g_loss: 0.72046942\n",
      "Step: [3055] d_loss: 1.38176560, g_loss: 0.71785736\n",
      "Step: [3056] d_loss: 1.36971617, g_loss: 0.72477430\n",
      "Step: [3057] d_loss: 1.38365436, g_loss: 0.72333860\n",
      "Step: [3058] d_loss: 1.39409006, g_loss: 0.72354949\n",
      "Step: [3059] d_loss: 1.38924623, g_loss: 0.70613259\n",
      "Step: [3060] d_loss: 1.37060213, g_loss: 0.72137761\n",
      "Step: [3061] d_loss: 1.39476502, g_loss: 0.72300571\n",
      "Step: [3062] d_loss: 1.40637124, g_loss: 0.70831668\n",
      "Step: [3063] d_loss: 1.38862634, g_loss: 0.71660691\n",
      "Step: [3064] d_loss: 1.38822794, g_loss: 0.71684992\n",
      "Step: [3065] d_loss: 1.38868213, g_loss: 0.71330607\n",
      "Step: [3066] d_loss: 1.40054703, g_loss: 0.70439392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3067] d_loss: 1.38173532, g_loss: 0.71682274\n",
      "Step: [3068] d_loss: 1.39314926, g_loss: 0.71097034\n",
      "Step: [3069] d_loss: 1.39068890, g_loss: 0.71205521\n",
      "Step: [3070] d_loss: 1.39526439, g_loss: 0.70780540\n",
      "Step: [3071] d_loss: 1.36477733, g_loss: 0.72015250\n",
      "Step: [3072] d_loss: 1.37577343, g_loss: 0.72034168\n",
      "Step: [3073] d_loss: 1.39292693, g_loss: 0.71468365\n",
      "Step: [3074] d_loss: 1.38448715, g_loss: 0.71548849\n",
      "Step: [3075] d_loss: 1.38131571, g_loss: 0.71292061\n",
      "Step: [3076] d_loss: 1.36308670, g_loss: 0.72531188\n",
      "Step: [3077] d_loss: 1.37680173, g_loss: 0.71635771\n",
      "Step: [3078] d_loss: 1.38696313, g_loss: 0.71730816\n",
      "Step: [3079] d_loss: 1.37066376, g_loss: 0.72777253\n",
      "Step: [3080] d_loss: 1.37878227, g_loss: 0.72528273\n",
      "Step: [3081] d_loss: 1.37974048, g_loss: 0.72158521\n",
      "Step: [3082] d_loss: 1.39094234, g_loss: 0.70125103\n",
      "Step: [3083] d_loss: 1.38189304, g_loss: 0.71392578\n",
      "Step: [3084] d_loss: 1.37091720, g_loss: 0.71767431\n",
      "Step: [3085] d_loss: 1.39180613, g_loss: 0.72147983\n",
      "Step: [3086] d_loss: 1.38227630, g_loss: 0.71727282\n",
      "Step: [3087] d_loss: 1.38491988, g_loss: 0.71402395\n",
      "Step: [3088] d_loss: 1.37923801, g_loss: 0.71598715\n",
      "Step: [3089] d_loss: 1.37977791, g_loss: 0.71122509\n",
      "Step: [3090] d_loss: 1.36965907, g_loss: 0.72502100\n",
      "Step: [3091] d_loss: 1.38057101, g_loss: 0.72921085\n",
      "Step: [3092] d_loss: 1.39540279, g_loss: 0.72945756\n",
      "Step: [3093] d_loss: 1.42003632, g_loss: 0.70833325\n",
      "Step: [3094] d_loss: 1.38705254, g_loss: 0.72731018\n",
      "Step: [3095] d_loss: 1.39095831, g_loss: 0.71665508\n",
      "Step: [3096] d_loss: 1.37962770, g_loss: 0.71687967\n",
      "Step: [3097] d_loss: 1.39157343, g_loss: 0.71420282\n",
      "Step: [3098] d_loss: 1.38893342, g_loss: 0.71632981\n",
      "Step: [3099] d_loss: 1.39761055, g_loss: 0.71251208\n",
      "Step: [3100] d_loss: 1.38198626, g_loss: 0.71661437\n",
      "Step: [3101] d_loss: 1.38373148, g_loss: 0.71668547\n",
      "Step: [3102] d_loss: 1.38822913, g_loss: 0.70637643\n",
      "Step: [3103] d_loss: 1.38369775, g_loss: 0.71634859\n",
      "Step: [3104] d_loss: 1.37923801, g_loss: 0.71574736\n",
      "Step: [3105] d_loss: 1.38804328, g_loss: 0.71211898\n",
      "Step: [3106] d_loss: 1.37897992, g_loss: 0.72308153\n",
      "Step: [3107] d_loss: 1.38185859, g_loss: 0.72917736\n",
      "Step: [3108] d_loss: 1.37827635, g_loss: 0.72260857\n",
      "Step: [3109] d_loss: 1.38132811, g_loss: 0.71845311\n",
      "Step: [3110] d_loss: 1.38211894, g_loss: 0.71615279\n",
      "Step: [3111] d_loss: 1.37622476, g_loss: 0.71814585\n",
      "Step: [3112] d_loss: 1.38731003, g_loss: 0.71112877\n",
      "Step: [3113] d_loss: 1.38676178, g_loss: 0.71763754\n",
      "Step: [3114] d_loss: 1.37334943, g_loss: 0.71925873\n",
      "Step: [3115] d_loss: 1.37509286, g_loss: 0.72078085\n",
      "Step: [3116] d_loss: 1.37849271, g_loss: 0.71632755\n",
      "Step: [3117] d_loss: 1.38761020, g_loss: 0.71202850\n",
      "Step: [3118] d_loss: 1.38125646, g_loss: 0.70836210\n",
      "Step: [3119] d_loss: 1.37606633, g_loss: 0.71566188\n",
      "Step: [3120] d_loss: 1.38158596, g_loss: 0.71709204\n",
      "Step: [3121] d_loss: 1.37705803, g_loss: 0.72095621\n",
      "Step: [3122] d_loss: 1.38298893, g_loss: 0.71407735\n",
      "Step: [3123] d_loss: 1.38815510, g_loss: 0.71767914\n",
      "Step: [3124] d_loss: 1.38546634, g_loss: 0.71494520\n",
      "Step: [3125] d_loss: 1.37233639, g_loss: 0.71382713\n",
      "Step: [3126] d_loss: 1.38461637, g_loss: 0.71276331\n",
      "Step: [3127] d_loss: 1.38465858, g_loss: 0.71953273\n",
      "Step: [3128] d_loss: 1.38087559, g_loss: 0.71514881\n",
      "Step: [3129] d_loss: 1.38352847, g_loss: 0.70844823\n",
      "Step: [3130] d_loss: 1.39862490, g_loss: 0.70337117\n",
      "Step: [3131] d_loss: 1.39328909, g_loss: 0.70145249\n",
      "Step: [3132] d_loss: 1.39195633, g_loss: 0.71329659\n",
      "Step: [3133] d_loss: 1.39568973, g_loss: 0.70863158\n",
      "Step: [3134] d_loss: 1.38538456, g_loss: 0.71202970\n",
      "Step: [3135] d_loss: 1.39120817, g_loss: 0.70990735\n",
      "Step: [3136] d_loss: 1.38607407, g_loss: 0.70948696\n",
      "Step: [3137] d_loss: 1.39072132, g_loss: 0.71463108\n",
      "Step: [3138] d_loss: 1.39024353, g_loss: 0.71170151\n",
      "Step: [3139] d_loss: 1.37712193, g_loss: 0.71534961\n",
      "Step: [3140] d_loss: 1.38542259, g_loss: 0.71326053\n",
      "Step: [3141] d_loss: 1.37827086, g_loss: 0.71035409\n",
      "Step: [3142] d_loss: 1.38432634, g_loss: 0.71717942\n",
      "Step: [3143] d_loss: 1.37839532, g_loss: 0.71561027\n",
      "Step: [3144] d_loss: 1.38182902, g_loss: 0.71974874\n",
      "Step: [3145] d_loss: 1.38327646, g_loss: 0.71392822\n",
      "Step: [3146] d_loss: 1.38250780, g_loss: 0.72993094\n",
      "Step: [3147] d_loss: 1.38195729, g_loss: 0.71516955\n",
      "Step: [3148] d_loss: 1.38361466, g_loss: 0.71439737\n",
      "Step: [3149] d_loss: 1.37373698, g_loss: 0.72008413\n",
      "Step: [3150] d_loss: 1.37174845, g_loss: 0.72208166\n",
      "Step: [3151] d_loss: 1.38947082, g_loss: 0.71834564\n",
      "Step: [3152] d_loss: 1.39213824, g_loss: 0.71487093\n",
      "Step: [3153] d_loss: 1.37677932, g_loss: 0.71521890\n",
      "Step: [3154] d_loss: 1.38063836, g_loss: 0.71908009\n",
      "Step: [3155] d_loss: 1.38647223, g_loss: 0.71062183\n",
      "Step: [3156] d_loss: 1.38275015, g_loss: 0.71497923\n",
      "Step: [3157] d_loss: 1.38993752, g_loss: 0.71743625\n",
      "Step: [3158] d_loss: 1.37428212, g_loss: 0.71639085\n",
      "Step: [3159] d_loss: 1.38472939, g_loss: 0.71466070\n",
      "Step: [3160] d_loss: 1.38686824, g_loss: 0.70800686\n",
      "Step: [3161] d_loss: 1.38780284, g_loss: 0.71002173\n",
      "Step: [3162] d_loss: 1.38953614, g_loss: 0.72183871\n",
      "Step: [3163] d_loss: 1.38418281, g_loss: 0.71359992\n",
      "Step: [3164] d_loss: 1.38513112, g_loss: 0.72230899\n",
      "Step: [3165] d_loss: 1.38476062, g_loss: 0.71701771\n",
      "Step: [3166] d_loss: 1.38689744, g_loss: 0.71178567\n",
      "Step: [3167] d_loss: 1.38172925, g_loss: 0.71550834\n",
      "Step: [3168] d_loss: 1.37671852, g_loss: 0.71354210\n",
      "Step: [3169] d_loss: 1.39538288, g_loss: 0.71140760\n",
      "Step: [3170] d_loss: 1.38141799, g_loss: 0.71939945\n",
      "Step: [3171] d_loss: 1.36651051, g_loss: 0.72341299\n",
      "Step: [3172] d_loss: 1.37952244, g_loss: 0.71706212\n",
      "Step: [3173] d_loss: 1.38313901, g_loss: 0.72157526\n",
      "Step: [3174] d_loss: 1.39191759, g_loss: 0.72577864\n",
      "Step: [3175] d_loss: 1.39886332, g_loss: 0.70955276\n",
      "Step: [3176] d_loss: 1.37867427, g_loss: 0.71406102\n",
      "Step: [3177] d_loss: 1.37678933, g_loss: 0.71821165\n",
      "Step: [3178] d_loss: 1.37361574, g_loss: 0.72243845\n",
      "Step: [3179] d_loss: 1.36852658, g_loss: 0.72277534\n",
      "Step: [3180] d_loss: 1.38150311, g_loss: 0.71366858\n",
      "Step: [3181] d_loss: 1.37936532, g_loss: 0.71580315\n",
      "Step: [3182] d_loss: 1.37456274, g_loss: 0.72104847\n",
      "Step: [3183] d_loss: 1.36266732, g_loss: 0.72452694\n",
      "Step: [3184] d_loss: 1.36395741, g_loss: 0.72166538\n",
      "Step: [3185] d_loss: 1.38302088, g_loss: 0.71083462\n",
      "Step: [3186] d_loss: 1.38438177, g_loss: 0.71655786\n",
      "Step: [3187] d_loss: 1.38689065, g_loss: 0.70922458\n",
      "Step: [3188] d_loss: 1.39747632, g_loss: 0.70064914\n",
      "Step: [3189] d_loss: 1.39089715, g_loss: 0.71554005\n",
      "Step: [3190] d_loss: 1.38612628, g_loss: 0.71976888\n",
      "Step: [3191] d_loss: 1.39132452, g_loss: 0.70396209\n",
      "Step: [3192] d_loss: 1.36902881, g_loss: 0.72605860\n",
      "Step: [3193] d_loss: 1.39916325, g_loss: 0.70792723\n",
      "Step: [3194] d_loss: 1.38585281, g_loss: 0.71578103\n",
      "Step: [3195] d_loss: 1.38860798, g_loss: 0.71312547\n",
      "Step: [3196] d_loss: 1.38864756, g_loss: 0.70768607\n",
      "Step: [3197] d_loss: 1.37448788, g_loss: 0.71643496\n",
      "Step: [3198] d_loss: 1.38191211, g_loss: 0.71580595\n",
      "Step: [3199] d_loss: 1.38141525, g_loss: 0.71949697\n",
      "Step: [3200] d_loss: 1.38526821, g_loss: 0.70782220\n",
      "Step: [3201] d_loss: 1.39149773, g_loss: 0.71187282\n",
      "Step: [3202] d_loss: 1.38309550, g_loss: 0.70872712\n",
      "Step: [3203] d_loss: 1.37993562, g_loss: 0.71582216\n",
      "Step: [3204] d_loss: 1.38438129, g_loss: 0.71346265\n",
      "Step: [3205] d_loss: 1.39665842, g_loss: 0.71820700\n",
      "Step: [3206] d_loss: 1.40295076, g_loss: 0.70410812\n",
      "Step: [3207] d_loss: 1.39366508, g_loss: 0.70671773\n",
      "Step: [3208] d_loss: 1.37947881, g_loss: 0.71372342\n",
      "Step: [3209] d_loss: 1.37503147, g_loss: 0.71875906\n",
      "Step: [3210] d_loss: 1.39008713, g_loss: 0.71265316\n",
      "Step: [3211] d_loss: 1.38984954, g_loss: 0.71517009\n",
      "Step: [3212] d_loss: 1.38116002, g_loss: 0.71325517\n",
      "Step: [3213] d_loss: 1.37785435, g_loss: 0.71783394\n",
      "Step: [3214] d_loss: 1.38682365, g_loss: 0.71555185\n",
      "Step: [3215] d_loss: 1.38632154, g_loss: 0.71299326\n",
      "Step: [3216] d_loss: 1.38237214, g_loss: 0.71492928\n",
      "Step: [3217] d_loss: 1.38635945, g_loss: 0.71258318\n",
      "Step: [3218] d_loss: 1.39089751, g_loss: 0.71255803\n",
      "Step: [3219] d_loss: 1.37345958, g_loss: 0.71638715\n",
      "Step: [3220] d_loss: 1.39128327, g_loss: 0.70668095\n",
      "Step: [3221] d_loss: 1.37042141, g_loss: 0.72463638\n",
      "Step: [3222] d_loss: 1.38167953, g_loss: 0.71969384\n",
      "Step: [3223] d_loss: 1.36995423, g_loss: 0.71985352\n",
      "Step: [3224] d_loss: 1.37956047, g_loss: 0.71689665\n",
      "Step: [3225] d_loss: 1.37532878, g_loss: 0.72379696\n",
      "Step: [3226] d_loss: 1.38053536, g_loss: 0.72141325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3227] d_loss: 1.37855220, g_loss: 0.71632963\n",
      "Step: [3228] d_loss: 1.37877440, g_loss: 0.71635294\n",
      "Step: [3229] d_loss: 1.36839724, g_loss: 0.72077304\n",
      "Step: [3230] d_loss: 1.38612938, g_loss: 0.71205491\n",
      "Step: [3231] d_loss: 1.38117194, g_loss: 0.71991789\n",
      "Step: [3232] d_loss: 1.38864589, g_loss: 0.71665215\n",
      "Step: [3233] d_loss: 1.38796842, g_loss: 0.71976346\n",
      "Step: [3234] d_loss: 1.39776289, g_loss: 0.71148086\n",
      "Step: [3235] d_loss: 1.37610960, g_loss: 0.72136796\n",
      "Step: [3236] d_loss: 1.38043213, g_loss: 0.72110981\n",
      "Step: [3237] d_loss: 1.38822484, g_loss: 0.71897852\n",
      "Step: [3238] d_loss: 1.36494565, g_loss: 0.72945684\n",
      "Step: [3239] d_loss: 1.38347113, g_loss: 0.71920723\n",
      "Step: [3240] d_loss: 1.38167763, g_loss: 0.71587902\n",
      "Step: [3241] d_loss: 1.37152505, g_loss: 0.71998918\n",
      "Step: [3242] d_loss: 1.38507128, g_loss: 0.72108418\n",
      "Step: [3243] d_loss: 1.38013804, g_loss: 0.72420377\n",
      "Step: [3244] d_loss: 1.38106394, g_loss: 0.72416341\n",
      "Step: [3245] d_loss: 1.37938428, g_loss: 0.72585356\n",
      "Step: [3246] d_loss: 1.38315487, g_loss: 0.71523947\n",
      "Step: [3247] d_loss: 1.37913799, g_loss: 0.72719359\n",
      "Step: [3248] d_loss: 1.38743138, g_loss: 0.71393585\n",
      "Step: [3249] d_loss: 1.37543130, g_loss: 0.72562200\n",
      "Step: [3250] d_loss: 1.37823498, g_loss: 0.72161281\n",
      "Step: [3251] d_loss: 1.36771166, g_loss: 0.72311157\n",
      "Step: [3252] d_loss: 1.37083089, g_loss: 0.71842319\n",
      "Step: [3253] d_loss: 1.36774802, g_loss: 0.71819127\n",
      "Step: [3254] d_loss: 1.39182413, g_loss: 0.71273470\n",
      "Step: [3255] d_loss: 1.38588107, g_loss: 0.71707582\n",
      "Step: [3256] d_loss: 1.38593900, g_loss: 0.71849030\n",
      "Step: [3257] d_loss: 1.38560772, g_loss: 0.70947945\n",
      "Step: [3258] d_loss: 1.38988614, g_loss: 0.70668226\n",
      "Step: [3259] d_loss: 1.38348901, g_loss: 0.71462506\n",
      "Step: [3260] d_loss: 1.36693251, g_loss: 0.72420371\n",
      "Step: [3261] d_loss: 1.37691748, g_loss: 0.71975315\n",
      "Step: [3262] d_loss: 1.38529742, g_loss: 0.71003193\n",
      "Step: [3263] d_loss: 1.36673975, g_loss: 0.71979141\n",
      "Step: [3264] d_loss: 1.38152146, g_loss: 0.71635687\n",
      "Step: [3265] d_loss: 1.37955022, g_loss: 0.72429526\n",
      "Step: [3266] d_loss: 1.38399315, g_loss: 0.71597910\n",
      "Step: [3267] d_loss: 1.38820517, g_loss: 0.71523106\n",
      "Step: [3268] d_loss: 1.38588810, g_loss: 0.71712756\n",
      "Step: [3269] d_loss: 1.37945724, g_loss: 0.71615374\n",
      "Step: [3270] d_loss: 1.39329755, g_loss: 0.70665622\n",
      "Step: [3271] d_loss: 1.38780499, g_loss: 0.71703368\n",
      "Step: [3272] d_loss: 1.39168310, g_loss: 0.72494888\n",
      "Step: [3273] d_loss: 1.39409542, g_loss: 0.70606774\n",
      "Step: [3274] d_loss: 1.37860155, g_loss: 0.71389842\n",
      "Step: [3275] d_loss: 1.36717689, g_loss: 0.72099280\n",
      "Step: [3276] d_loss: 1.38121963, g_loss: 0.71978462\n",
      "Step: [3277] d_loss: 1.38730884, g_loss: 0.71813142\n",
      "Step: [3278] d_loss: 1.38067794, g_loss: 0.71926439\n",
      "Step: [3279] d_loss: 1.38161910, g_loss: 0.71797717\n",
      "Step: [3280] d_loss: 1.39529777, g_loss: 0.70741552\n",
      "Step: [3281] d_loss: 1.39128351, g_loss: 0.71839553\n",
      "Step: [3282] d_loss: 1.38576484, g_loss: 0.71212280\n",
      "Step: [3283] d_loss: 1.39271665, g_loss: 0.71205866\n",
      "Step: [3284] d_loss: 1.38807201, g_loss: 0.72091997\n",
      "Step: [3285] d_loss: 1.38153458, g_loss: 0.71641409\n",
      "Step: [3286] d_loss: 1.37869608, g_loss: 0.71257561\n",
      "Step: [3287] d_loss: 1.38560295, g_loss: 0.72080451\n",
      "Step: [3288] d_loss: 1.38566637, g_loss: 0.72139466\n",
      "Step: [3289] d_loss: 1.38829124, g_loss: 0.71879125\n",
      "Step: [3290] d_loss: 1.38735008, g_loss: 0.71691525\n",
      "Step: [3291] d_loss: 1.38633394, g_loss: 0.72103000\n",
      "Step: [3292] d_loss: 1.37343872, g_loss: 0.71886253\n",
      "Step: [3293] d_loss: 1.38165641, g_loss: 0.72085059\n",
      "Step: [3294] d_loss: 1.37819302, g_loss: 0.71361768\n",
      "Step: [3295] d_loss: 1.38507056, g_loss: 0.72693717\n",
      "Step: [3296] d_loss: 1.38216674, g_loss: 0.72501183\n",
      "Step: [3297] d_loss: 1.38007855, g_loss: 0.72859025\n",
      "Step: [3298] d_loss: 1.38558531, g_loss: 0.72690427\n",
      "Step: [3299] d_loss: 1.37776649, g_loss: 0.72103798\n",
      "Step: [3300] d_loss: 1.38015699, g_loss: 0.72204596\n",
      "Step: [3301] d_loss: 1.38399792, g_loss: 0.72123289\n",
      "Step: [3302] d_loss: 1.37712741, g_loss: 0.71881235\n",
      "Step: [3303] d_loss: 1.37854171, g_loss: 0.71194673\n",
      "Step: [3304] d_loss: 1.37884045, g_loss: 0.72109258\n",
      "Step: [3305] d_loss: 1.37752008, g_loss: 0.72032201\n",
      "Step: [3306] d_loss: 1.36726177, g_loss: 0.72475976\n",
      "Step: [3307] d_loss: 1.38258243, g_loss: 0.72156525\n",
      "Step: [3308] d_loss: 1.37841320, g_loss: 0.72212911\n",
      "Step: [3309] d_loss: 1.38214064, g_loss: 0.72115684\n",
      "Step: [3310] d_loss: 1.38134825, g_loss: 0.71738529\n",
      "Step: [3311] d_loss: 1.39088559, g_loss: 0.71009505\n",
      "Step: [3312] d_loss: 1.37649679, g_loss: 0.71883631\n",
      "Step: [3313] d_loss: 1.39742947, g_loss: 0.70895892\n",
      "Step: [3314] d_loss: 1.37210619, g_loss: 0.72916406\n",
      "Step: [3315] d_loss: 1.37121129, g_loss: 0.72892058\n",
      "Step: [3316] d_loss: 1.39056468, g_loss: 0.71797955\n",
      "Step: [3317] d_loss: 1.38888645, g_loss: 0.71765405\n",
      "Step: [3318] d_loss: 1.39734566, g_loss: 0.71634358\n",
      "Step: [3319] d_loss: 1.38758063, g_loss: 0.71800756\n",
      "Step: [3320] d_loss: 1.39534783, g_loss: 0.71168900\n",
      "Step: [3321] d_loss: 1.38957143, g_loss: 0.71373045\n",
      "Step: [3322] d_loss: 1.38186932, g_loss: 0.71935320\n",
      "Step: [3323] d_loss: 1.38986015, g_loss: 0.72776932\n",
      "Step: [3324] d_loss: 1.39436340, g_loss: 0.71856481\n",
      "Step: [3325] d_loss: 1.38140333, g_loss: 0.71832705\n",
      "Step: [3326] d_loss: 1.38660967, g_loss: 0.72249663\n",
      "Step: [3327] d_loss: 1.38158846, g_loss: 0.72006929\n",
      "Step: [3328] d_loss: 1.38040090, g_loss: 0.73671031\n",
      "Step: [3329] d_loss: 1.37086964, g_loss: 0.71791816\n",
      "Step: [3330] d_loss: 1.38398528, g_loss: 0.72149205\n",
      "Step: [3331] d_loss: 1.37942612, g_loss: 0.72437197\n",
      "Step: [3332] d_loss: 1.37932241, g_loss: 0.71944618\n",
      "Step: [3333] d_loss: 1.38413620, g_loss: 0.72209352\n",
      "Step: [3334] d_loss: 1.38678634, g_loss: 0.71252918\n",
      "Step: [3335] d_loss: 1.38793719, g_loss: 0.72620404\n",
      "Step: [3336] d_loss: 1.38524663, g_loss: 0.72439724\n",
      "Step: [3337] d_loss: 1.38813090, g_loss: 0.71538138\n",
      "Step: [3338] d_loss: 1.38009989, g_loss: 0.72142637\n",
      "Step: [3339] d_loss: 1.37785125, g_loss: 0.71635389\n",
      "Step: [3340] d_loss: 1.37169456, g_loss: 0.71891594\n",
      "Step: [3341] d_loss: 1.39012182, g_loss: 0.71739018\n",
      "Step: [3342] d_loss: 1.38419437, g_loss: 0.71474296\n",
      "Step: [3343] d_loss: 1.38928783, g_loss: 0.71654743\n",
      "Step: [3344] d_loss: 1.38354576, g_loss: 0.71798706\n",
      "Step: [3345] d_loss: 1.39023423, g_loss: 0.71825433\n",
      "Step: [3346] d_loss: 1.39433932, g_loss: 0.70817941\n",
      "Step: [3347] d_loss: 1.39719152, g_loss: 0.71181035\n",
      "Step: [3348] d_loss: 1.38273478, g_loss: 0.71941924\n",
      "Step: [3349] d_loss: 1.38787913, g_loss: 0.71839273\n",
      "Step: [3350] d_loss: 1.37192094, g_loss: 0.72652698\n",
      "Step: [3351] d_loss: 1.37917721, g_loss: 0.71902400\n",
      "Step: [3352] d_loss: 1.38022757, g_loss: 0.72406936\n",
      "Step: [3353] d_loss: 1.37858438, g_loss: 0.72324699\n",
      "Step: [3354] d_loss: 1.37585640, g_loss: 0.72184128\n",
      "Step: [3355] d_loss: 1.39281678, g_loss: 0.71109879\n",
      "Step: [3356] d_loss: 1.38604283, g_loss: 0.71492028\n",
      "Step: [3357] d_loss: 1.38663924, g_loss: 0.71618640\n",
      "Step: [3358] d_loss: 1.38095188, g_loss: 0.72268003\n",
      "Step: [3359] d_loss: 1.38857102, g_loss: 0.71744263\n",
      "Step: [3360] d_loss: 1.39034784, g_loss: 0.71653003\n",
      "Step: [3361] d_loss: 1.37882769, g_loss: 0.71814370\n",
      "Step: [3362] d_loss: 1.39489269, g_loss: 0.70785069\n",
      "Step: [3363] d_loss: 1.39001632, g_loss: 0.72368848\n",
      "Step: [3364] d_loss: 1.38072610, g_loss: 0.72126836\n",
      "Step: [3365] d_loss: 1.38448727, g_loss: 0.71650398\n",
      "Step: [3366] d_loss: 1.38929975, g_loss: 0.71317858\n",
      "Step: [3367] d_loss: 1.38936079, g_loss: 0.70702469\n",
      "Step: [3368] d_loss: 1.38272250, g_loss: 0.72006500\n",
      "Step: [3369] d_loss: 1.37757051, g_loss: 0.72302866\n",
      "Step: [3370] d_loss: 1.39264226, g_loss: 0.70943522\n",
      "Step: [3371] d_loss: 1.37664557, g_loss: 0.72582674\n",
      "Step: [3372] d_loss: 1.37465179, g_loss: 0.71615160\n",
      "Step: [3373] d_loss: 1.37783980, g_loss: 0.71171701\n",
      "Step: [3374] d_loss: 1.37985206, g_loss: 0.71728897\n",
      "Step: [3375] d_loss: 1.37369037, g_loss: 0.71929616\n",
      "Step: [3376] d_loss: 1.37461603, g_loss: 0.71807516\n",
      "Step: [3377] d_loss: 1.37177134, g_loss: 0.72881496\n",
      "Step: [3378] d_loss: 1.39381862, g_loss: 0.70965225\n",
      "Step: [3379] d_loss: 1.38126540, g_loss: 0.71268928\n",
      "Step: [3380] d_loss: 1.38628316, g_loss: 0.71482563\n",
      "Step: [3381] d_loss: 1.38752925, g_loss: 0.72298014\n",
      "Step: [3382] d_loss: 1.37246764, g_loss: 0.71665549\n",
      "Step: [3383] d_loss: 1.38553500, g_loss: 0.70923400\n",
      "Step: [3384] d_loss: 1.37942863, g_loss: 0.70885003\n",
      "Step: [3385] d_loss: 1.36851859, g_loss: 0.73019671\n",
      "Step: [3386] d_loss: 1.37924898, g_loss: 0.72205687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3387] d_loss: 1.39062643, g_loss: 0.71790278\n",
      "Step: [3388] d_loss: 1.39315975, g_loss: 0.72094965\n",
      "Step: [3389] d_loss: 1.39815140, g_loss: 0.71327823\n",
      "Step: [3390] d_loss: 1.39715111, g_loss: 0.70842594\n",
      "Step: [3391] d_loss: 1.38689876, g_loss: 0.71743727\n",
      "Step: [3392] d_loss: 1.38989019, g_loss: 0.71724921\n",
      "Step: [3393] d_loss: 1.38137615, g_loss: 0.72024786\n",
      "Step: [3394] d_loss: 1.37984109, g_loss: 0.71530026\n",
      "Step: [3395] d_loss: 1.37892604, g_loss: 0.72251701\n",
      "Step: [3396] d_loss: 1.38198757, g_loss: 0.71393520\n",
      "Step: [3397] d_loss: 1.38450122, g_loss: 0.71775085\n",
      "Step: [3398] d_loss: 1.37869394, g_loss: 0.71905851\n",
      "Step: [3399] d_loss: 1.37953579, g_loss: 0.71509993\n",
      "Step: [3400] d_loss: 1.39637482, g_loss: 0.70738739\n",
      "Step: [3401] d_loss: 1.37860298, g_loss: 0.72094172\n",
      "Step: [3402] d_loss: 1.38662636, g_loss: 0.71030676\n",
      "Step: [3403] d_loss: 1.38339496, g_loss: 0.71716481\n",
      "Step: [3404] d_loss: 1.38478065, g_loss: 0.71977961\n",
      "Step: [3405] d_loss: 1.39586902, g_loss: 0.70894694\n",
      "Step: [3406] d_loss: 1.38210356, g_loss: 0.71633816\n",
      "Step: [3407] d_loss: 1.40047288, g_loss: 0.71415561\n",
      "Step: [3408] d_loss: 1.39073360, g_loss: 0.71915257\n",
      "Step: [3409] d_loss: 1.40568864, g_loss: 0.70250165\n",
      "Step: [3410] d_loss: 1.39019513, g_loss: 0.70356464\n",
      "Step: [3411] d_loss: 1.40392232, g_loss: 0.70510495\n",
      "Step: [3412] d_loss: 1.38763988, g_loss: 0.71378016\n",
      "Step: [3413] d_loss: 1.39548230, g_loss: 0.70729542\n",
      "Step: [3414] d_loss: 1.39585662, g_loss: 0.71127820\n",
      "Step: [3415] d_loss: 1.37867010, g_loss: 0.72006929\n",
      "Step: [3416] d_loss: 1.37802720, g_loss: 0.71756792\n",
      "Step: [3417] d_loss: 1.38949656, g_loss: 0.71701765\n",
      "Step: [3418] d_loss: 1.38296771, g_loss: 0.71818656\n",
      "Step: [3419] d_loss: 1.38015032, g_loss: 0.71890479\n",
      "Step: [3420] d_loss: 1.36975718, g_loss: 0.72961986\n",
      "Step: [3421] d_loss: 1.38773572, g_loss: 0.71745551\n",
      "Step: [3422] d_loss: 1.37463093, g_loss: 0.72375345\n",
      "Step: [3423] d_loss: 1.37023807, g_loss: 0.72590858\n",
      "Step: [3424] d_loss: 1.38539600, g_loss: 0.71427149\n",
      "Step: [3425] d_loss: 1.37991452, g_loss: 0.71721613\n",
      "Step: [3426] d_loss: 1.38292062, g_loss: 0.71277767\n",
      "Step: [3427] d_loss: 1.37853622, g_loss: 0.72206110\n",
      "Step: [3428] d_loss: 1.38097906, g_loss: 0.72180790\n",
      "Step: [3429] d_loss: 1.37860966, g_loss: 0.71746087\n",
      "Step: [3430] d_loss: 1.38877058, g_loss: 0.72243869\n",
      "Step: [3431] d_loss: 1.38899612, g_loss: 0.71462697\n",
      "Step: [3432] d_loss: 1.38922548, g_loss: 0.70254850\n",
      "Step: [3433] d_loss: 1.40083194, g_loss: 0.70256603\n",
      "Step: [3434] d_loss: 1.39272559, g_loss: 0.71944243\n",
      "Step: [3435] d_loss: 1.38682759, g_loss: 0.71138245\n",
      "Step: [3436] d_loss: 1.39157152, g_loss: 0.71186966\n",
      "Step: [3437] d_loss: 1.37224686, g_loss: 0.72449911\n",
      "Step: [3438] d_loss: 1.38991094, g_loss: 0.71701336\n",
      "Step: [3439] d_loss: 1.38440895, g_loss: 0.71665418\n",
      "Step: [3440] d_loss: 1.37505603, g_loss: 0.72165453\n",
      "Step: [3441] d_loss: 1.36783206, g_loss: 0.72682369\n",
      "Step: [3442] d_loss: 1.38373709, g_loss: 0.71174014\n",
      "Step: [3443] d_loss: 1.37864006, g_loss: 0.72885662\n",
      "Step: [3444] d_loss: 1.38396287, g_loss: 0.72176731\n",
      "Step: [3445] d_loss: 1.39267576, g_loss: 0.71009576\n",
      "Step: [3446] d_loss: 1.37635732, g_loss: 0.72412765\n",
      "Step: [3447] d_loss: 1.37678695, g_loss: 0.72720176\n",
      "Step: [3448] d_loss: 1.36572385, g_loss: 0.71143210\n",
      "Step: [3449] d_loss: 1.37308645, g_loss: 0.72064483\n",
      "Step: [3450] d_loss: 1.37092686, g_loss: 0.71318793\n",
      "Step: [3451] d_loss: 1.37700725, g_loss: 0.71626723\n",
      "Step: [3452] d_loss: 1.39091516, g_loss: 0.71393418\n",
      "Step: [3453] d_loss: 1.38827372, g_loss: 0.71695983\n",
      "Step: [3454] d_loss: 1.38850152, g_loss: 0.71144974\n",
      "Step: [3455] d_loss: 1.38754964, g_loss: 0.71811211\n",
      "Step: [3456] d_loss: 1.37861371, g_loss: 0.71285409\n",
      "Step: [3457] d_loss: 1.37669206, g_loss: 0.71826082\n",
      "Step: [3458] d_loss: 1.39677429, g_loss: 0.71094918\n",
      "Step: [3459] d_loss: 1.40297222, g_loss: 0.71087980\n",
      "Step: [3460] d_loss: 1.39035678, g_loss: 0.71013546\n",
      "Step: [3461] d_loss: 1.39348817, g_loss: 0.70929658\n",
      "Step: [3462] d_loss: 1.38959002, g_loss: 0.71451348\n",
      "Step: [3463] d_loss: 1.38430357, g_loss: 0.71918988\n",
      "Step: [3464] d_loss: 1.40619087, g_loss: 0.71206760\n",
      "Step: [3465] d_loss: 1.38229811, g_loss: 0.72241533\n",
      "Step: [3466] d_loss: 1.38650107, g_loss: 0.71249247\n",
      "Step: [3467] d_loss: 1.37576342, g_loss: 0.71699440\n",
      "Step: [3468] d_loss: 1.38509834, g_loss: 0.71478057\n",
      "Step: [3469] d_loss: 1.36854386, g_loss: 0.71892226\n",
      "Step: [3470] d_loss: 1.38237357, g_loss: 0.72014534\n",
      "Step: [3471] d_loss: 1.38184261, g_loss: 0.72130138\n",
      "Step: [3472] d_loss: 1.38551402, g_loss: 0.71887457\n",
      "Step: [3473] d_loss: 1.38660026, g_loss: 0.71221161\n",
      "Step: [3474] d_loss: 1.37796187, g_loss: 0.72260535\n",
      "Step: [3475] d_loss: 1.38424933, g_loss: 0.72119528\n",
      "Step: [3476] d_loss: 1.39436376, g_loss: 0.71945649\n",
      "Step: [3477] d_loss: 1.40227079, g_loss: 0.71624184\n",
      "Step: [3478] d_loss: 1.38617873, g_loss: 0.71875602\n",
      "Step: [3479] d_loss: 1.38168526, g_loss: 0.72109514\n",
      "Step: [3480] d_loss: 1.38440967, g_loss: 0.72150326\n",
      "Step: [3481] d_loss: 1.36660504, g_loss: 0.73346061\n",
      "Step: [3482] d_loss: 1.37449980, g_loss: 0.72954452\n",
      "Step: [3483] d_loss: 1.38612223, g_loss: 0.72072697\n",
      "Step: [3484] d_loss: 1.40120447, g_loss: 0.71388793\n",
      "Step: [3485] d_loss: 1.38791203, g_loss: 0.71611190\n",
      "Step: [3486] d_loss: 1.38670111, g_loss: 0.72314703\n",
      "Step: [3487] d_loss: 1.38654637, g_loss: 0.71937191\n",
      "Step: [3488] d_loss: 1.37954211, g_loss: 0.72097278\n",
      "Step: [3489] d_loss: 1.39181328, g_loss: 0.71254390\n",
      "Step: [3490] d_loss: 1.38586664, g_loss: 0.71514988\n",
      "Step: [3491] d_loss: 1.37805879, g_loss: 0.71574712\n",
      "Step: [3492] d_loss: 1.38667774, g_loss: 0.71710402\n",
      "Step: [3493] d_loss: 1.39603031, g_loss: 0.71644366\n",
      "Step: [3494] d_loss: 1.37590837, g_loss: 0.71385682\n",
      "Step: [3495] d_loss: 1.38424349, g_loss: 0.71762794\n",
      "Step: [3496] d_loss: 1.38206363, g_loss: 0.71537590\n",
      "Step: [3497] d_loss: 1.38164902, g_loss: 0.71332848\n",
      "Step: [3498] d_loss: 1.38201261, g_loss: 0.71962851\n",
      "Step: [3499] d_loss: 1.38333654, g_loss: 0.72519690\n",
      "Step: [3500] d_loss: 1.37482285, g_loss: 0.71708524\n",
      "Step: [3501] d_loss: 1.39199412, g_loss: 0.70598066\n",
      "Step: [3502] d_loss: 1.37794423, g_loss: 0.72369331\n",
      "Step: [3503] d_loss: 1.38141274, g_loss: 0.72247398\n",
      "Step: [3504] d_loss: 1.37349272, g_loss: 0.72033429\n",
      "Step: [3505] d_loss: 1.38396978, g_loss: 0.71457481\n",
      "Step: [3506] d_loss: 1.37890780, g_loss: 0.71483505\n",
      "Step: [3507] d_loss: 1.36670709, g_loss: 0.71171170\n",
      "Step: [3508] d_loss: 1.37700653, g_loss: 0.72325969\n",
      "Step: [3509] d_loss: 1.39418149, g_loss: 0.71715677\n",
      "Step: [3510] d_loss: 1.36145544, g_loss: 0.72839880\n",
      "Step: [3511] d_loss: 1.38912547, g_loss: 0.71235836\n",
      "Step: [3512] d_loss: 1.38975263, g_loss: 0.70342249\n",
      "Step: [3513] d_loss: 1.38705826, g_loss: 0.70709026\n",
      "Step: [3514] d_loss: 1.38817763, g_loss: 0.70681751\n",
      "Step: [3515] d_loss: 1.39109504, g_loss: 0.71241355\n",
      "Step: [3516] d_loss: 1.38329065, g_loss: 0.71966028\n",
      "Step: [3517] d_loss: 1.38869262, g_loss: 0.71175092\n",
      "Step: [3518] d_loss: 1.37162995, g_loss: 0.71990407\n",
      "Step: [3519] d_loss: 1.40142250, g_loss: 0.70920396\n",
      "Step: [3520] d_loss: 1.38409090, g_loss: 0.70947504\n",
      "Step: [3521] d_loss: 1.38336396, g_loss: 0.71761227\n",
      "Step: [3522] d_loss: 1.40061772, g_loss: 0.71036935\n",
      "Step: [3523] d_loss: 1.38033175, g_loss: 0.70922667\n",
      "Step: [3524] d_loss: 1.38272774, g_loss: 0.71417046\n",
      "Step: [3525] d_loss: 1.38706064, g_loss: 0.71166205\n",
      "Step: [3526] d_loss: 1.39140654, g_loss: 0.71443409\n",
      "Step: [3527] d_loss: 1.37823451, g_loss: 0.72737634\n",
      "Step: [3528] d_loss: 1.38346004, g_loss: 0.72215575\n",
      "Step: [3529] d_loss: 1.38606501, g_loss: 0.71124583\n",
      "Step: [3530] d_loss: 1.38585949, g_loss: 0.71272910\n",
      "Step: [3531] d_loss: 1.37394905, g_loss: 0.71640146\n",
      "Step: [3532] d_loss: 1.38200772, g_loss: 0.71221983\n",
      "Step: [3533] d_loss: 1.38127291, g_loss: 0.71278536\n",
      "Step: [3534] d_loss: 1.39958203, g_loss: 0.70692205\n",
      "Step: [3535] d_loss: 1.39395869, g_loss: 0.71189606\n",
      "Step: [3536] d_loss: 1.37678623, g_loss: 0.71577626\n",
      "Step: [3537] d_loss: 1.38220942, g_loss: 0.71788710\n",
      "Step: [3538] d_loss: 1.37482238, g_loss: 0.71456790\n",
      "Step: [3539] d_loss: 1.38899088, g_loss: 0.70962381\n",
      "Step: [3540] d_loss: 1.38564825, g_loss: 0.71324211\n",
      "Step: [3541] d_loss: 1.38321912, g_loss: 0.71219695\n",
      "Step: [3542] d_loss: 1.38238096, g_loss: 0.71071732\n",
      "Step: [3543] d_loss: 1.37168825, g_loss: 0.72135043\n",
      "Step: [3544] d_loss: 1.38386071, g_loss: 0.71875459\n",
      "Step: [3545] d_loss: 1.38028598, g_loss: 0.71043533\n",
      "Step: [3546] d_loss: 1.38959634, g_loss: 0.70682937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3547] d_loss: 1.38779163, g_loss: 0.71674371\n",
      "Step: [3548] d_loss: 1.37884307, g_loss: 0.72267747\n",
      "Step: [3549] d_loss: 1.38887525, g_loss: 0.71973336\n",
      "Step: [3550] d_loss: 1.39343584, g_loss: 0.71159214\n",
      "Step: [3551] d_loss: 1.38672423, g_loss: 0.71250522\n",
      "Step: [3552] d_loss: 1.38023353, g_loss: 0.73007298\n",
      "Step: [3553] d_loss: 1.41788816, g_loss: 0.72700173\n",
      "Step: [3554] d_loss: 1.41263545, g_loss: 0.73189288\n",
      "Step: [3555] d_loss: 1.41261256, g_loss: 0.72006619\n",
      "Step: [3556] d_loss: 1.39231408, g_loss: 0.70828617\n",
      "Step: [3557] d_loss: 1.38427436, g_loss: 0.72064602\n",
      "Step: [3558] d_loss: 1.38119721, g_loss: 0.70998621\n",
      "Step: [3559] d_loss: 1.39079356, g_loss: 0.71847880\n",
      "Step: [3560] d_loss: 1.37993777, g_loss: 0.71009231\n",
      "Step: [3561] d_loss: 1.37161756, g_loss: 0.71618497\n",
      "Step: [3562] d_loss: 1.35862184, g_loss: 0.72734398\n",
      "Step: [3563] d_loss: 1.37424898, g_loss: 0.72037339\n",
      "Step: [3564] d_loss: 1.38554132, g_loss: 0.71152145\n",
      "Step: [3565] d_loss: 1.38439274, g_loss: 0.71590531\n",
      "Step: [3566] d_loss: 1.37325954, g_loss: 0.72208679\n",
      "Step: [3567] d_loss: 1.37791324, g_loss: 0.71481967\n",
      "Step: [3568] d_loss: 1.38187122, g_loss: 0.71632802\n",
      "Step: [3569] d_loss: 1.38592100, g_loss: 0.71393514\n",
      "Step: [3570] d_loss: 1.38118768, g_loss: 0.71202624\n",
      "Step: [3571] d_loss: 1.38798285, g_loss: 0.71713722\n",
      "Step: [3572] d_loss: 1.37107348, g_loss: 0.71733069\n",
      "Step: [3573] d_loss: 1.38644838, g_loss: 0.70378983\n",
      "Step: [3574] d_loss: 1.39622772, g_loss: 0.71101803\n",
      "Step: [3575] d_loss: 1.38533771, g_loss: 0.71990597\n",
      "Step: [3576] d_loss: 1.38651872, g_loss: 0.71291339\n",
      "Step: [3577] d_loss: 1.38826251, g_loss: 0.71487594\n",
      "Step: [3578] d_loss: 1.39015841, g_loss: 0.71261001\n",
      "Step: [3579] d_loss: 1.38930738, g_loss: 0.72693431\n",
      "Step: [3580] d_loss: 1.39033604, g_loss: 0.71325743\n",
      "Step: [3581] d_loss: 1.38500571, g_loss: 0.71534908\n",
      "Step: [3582] d_loss: 1.39384818, g_loss: 0.70560944\n",
      "Step: [3583] d_loss: 1.39090419, g_loss: 0.71098447\n",
      "Step: [3584] d_loss: 1.37745273, g_loss: 0.72440600\n",
      "Step: [3585] d_loss: 1.37610734, g_loss: 0.71604323\n",
      "Step: [3586] d_loss: 1.38354111, g_loss: 0.71939516\n",
      "Step: [3587] d_loss: 1.39702034, g_loss: 0.71613812\n",
      "Step: [3588] d_loss: 1.39068186, g_loss: 0.70723474\n",
      "Step: [3589] d_loss: 1.37550902, g_loss: 0.72493184\n",
      "Step: [3590] d_loss: 1.37361705, g_loss: 0.71821368\n",
      "Step: [3591] d_loss: 1.38449800, g_loss: 0.71425802\n",
      "Step: [3592] d_loss: 1.38021135, g_loss: 0.71209848\n",
      "Step: [3593] d_loss: 1.39320600, g_loss: 0.70475268\n",
      "Step: [3594] d_loss: 1.37857842, g_loss: 0.72364593\n",
      "Step: [3595] d_loss: 1.37488985, g_loss: 0.71977878\n",
      "Step: [3596] d_loss: 1.38290477, g_loss: 0.71957111\n",
      "Step: [3597] d_loss: 1.37140119, g_loss: 0.72035241\n",
      "Step: [3598] d_loss: 1.37772357, g_loss: 0.71417302\n",
      "Step: [3599] d_loss: 1.39017105, g_loss: 0.70744050\n",
      "Step: [3600] d_loss: 1.38870203, g_loss: 0.71489090\n",
      "Step: [3601] d_loss: 1.38097310, g_loss: 0.71737373\n",
      "Step: [3602] d_loss: 1.38395381, g_loss: 0.72139907\n",
      "Step: [3603] d_loss: 1.38057923, g_loss: 0.71935010\n",
      "Step: [3604] d_loss: 1.38098586, g_loss: 0.71945155\n",
      "Step: [3605] d_loss: 1.37753201, g_loss: 0.71713203\n",
      "Step: [3606] d_loss: 1.38915133, g_loss: 0.70856023\n",
      "Step: [3607] d_loss: 1.39089406, g_loss: 0.70814008\n",
      "Step: [3608] d_loss: 1.38358843, g_loss: 0.70864761\n",
      "Step: [3609] d_loss: 1.38834262, g_loss: 0.71084487\n",
      "Step: [3610] d_loss: 1.38731205, g_loss: 0.71335912\n",
      "Step: [3611] d_loss: 1.38537943, g_loss: 0.71603382\n",
      "Step: [3612] d_loss: 1.38045335, g_loss: 0.71681517\n",
      "Step: [3613] d_loss: 1.38848948, g_loss: 0.70967609\n",
      "Step: [3614] d_loss: 1.38517010, g_loss: 0.71452183\n",
      "Step: [3615] d_loss: 1.37789035, g_loss: 0.72091722\n",
      "Step: [3616] d_loss: 1.39332342, g_loss: 0.70963669\n",
      "Step: [3617] d_loss: 1.38498545, g_loss: 0.71810961\n",
      "Step: [3618] d_loss: 1.38393736, g_loss: 0.71229011\n",
      "Step: [3619] d_loss: 1.37335467, g_loss: 0.71471071\n",
      "Step: [3620] d_loss: 1.38431633, g_loss: 0.71996963\n",
      "Step: [3621] d_loss: 1.38378000, g_loss: 0.71480381\n",
      "Step: [3622] d_loss: 1.38520586, g_loss: 0.71599156\n",
      "Step: [3623] d_loss: 1.38790894, g_loss: 0.71424377\n",
      "Step: [3624] d_loss: 1.38187134, g_loss: 0.71188140\n",
      "Step: [3625] d_loss: 1.37158132, g_loss: 0.72756052\n",
      "Step: [3626] d_loss: 1.39641011, g_loss: 0.73709518\n",
      "Step: [3627] d_loss: 1.40350854, g_loss: 0.72774607\n",
      "Step: [3628] d_loss: 1.40279222, g_loss: 0.70906806\n",
      "Step: [3629] d_loss: 1.38930750, g_loss: 0.71524733\n",
      "Step: [3630] d_loss: 1.38118458, g_loss: 0.71975231\n",
      "Step: [3631] d_loss: 1.39793921, g_loss: 0.71540731\n",
      "Step: [3632] d_loss: 1.38138628, g_loss: 0.71320629\n",
      "Step: [3633] d_loss: 1.39591622, g_loss: 0.71466601\n",
      "Step: [3634] d_loss: 1.38443232, g_loss: 0.71891654\n",
      "Step: [3635] d_loss: 1.38112736, g_loss: 0.72864234\n",
      "Step: [3636] d_loss: 1.37296379, g_loss: 0.72177207\n",
      "Step: [3637] d_loss: 1.37818134, g_loss: 0.72324651\n",
      "Step: [3638] d_loss: 1.38389087, g_loss: 0.71258724\n",
      "Step: [3639] d_loss: 1.37520123, g_loss: 0.72133696\n",
      "Step: [3640] d_loss: 1.36904740, g_loss: 0.71999460\n",
      "Step: [3641] d_loss: 1.38211191, g_loss: 0.71019208\n",
      "Step: [3642] d_loss: 1.35140121, g_loss: 0.73531270\n",
      "Step: [3643] d_loss: 1.36233950, g_loss: 0.72201127\n",
      "Step: [3644] d_loss: 1.37231934, g_loss: 0.72422659\n",
      "Step: [3645] d_loss: 1.37245619, g_loss: 0.71748984\n",
      "Step: [3646] d_loss: 1.37941182, g_loss: 0.70904166\n",
      "Step: [3647] d_loss: 1.38928759, g_loss: 0.70339006\n",
      "Step: [3648] d_loss: 1.38510668, g_loss: 0.71501213\n",
      "Step: [3649] d_loss: 1.37760139, g_loss: 0.71708786\n",
      "Step: [3650] d_loss: 1.39929152, g_loss: 0.71018386\n",
      "Step: [3651] d_loss: 1.39444697, g_loss: 0.71090674\n",
      "Step: [3652] d_loss: 1.38442767, g_loss: 0.71419865\n",
      "Step: [3653] d_loss: 1.36705089, g_loss: 0.72466457\n",
      "Step: [3654] d_loss: 1.37945783, g_loss: 0.71404850\n",
      "Step: [3655] d_loss: 1.38544631, g_loss: 0.71095490\n",
      "Step: [3656] d_loss: 1.37415338, g_loss: 0.72219026\n",
      "Step: [3657] d_loss: 1.37993324, g_loss: 0.71873528\n",
      "Step: [3658] d_loss: 1.36987591, g_loss: 0.72007447\n",
      "Step: [3659] d_loss: 1.38102674, g_loss: 0.71516573\n",
      "Step: [3660] d_loss: 1.37253737, g_loss: 0.72044402\n",
      "Step: [3661] d_loss: 1.39862537, g_loss: 0.71317661\n",
      "Step: [3662] d_loss: 1.38238788, g_loss: 0.70747840\n",
      "Step: [3663] d_loss: 1.38150334, g_loss: 0.71657687\n",
      "Step: [3664] d_loss: 1.38147783, g_loss: 0.71759164\n",
      "Step: [3665] d_loss: 1.37456071, g_loss: 0.72197837\n",
      "Step: [3666] d_loss: 1.37624216, g_loss: 0.72184765\n",
      "Step: [3667] d_loss: 1.39275157, g_loss: 0.70512545\n",
      "Step: [3668] d_loss: 1.37368846, g_loss: 0.71637762\n",
      "Step: [3669] d_loss: 1.38945007, g_loss: 0.71319461\n",
      "Step: [3670] d_loss: 1.38867998, g_loss: 0.70764589\n",
      "Step: [3671] d_loss: 1.39711630, g_loss: 0.71205819\n",
      "Step: [3672] d_loss: 1.38875806, g_loss: 0.71801865\n",
      "Step: [3673] d_loss: 1.38945222, g_loss: 0.71226817\n",
      "Step: [3674] d_loss: 1.38247359, g_loss: 0.71706069\n",
      "Step: [3675] d_loss: 1.37779391, g_loss: 0.71864974\n",
      "Step: [3676] d_loss: 1.38002896, g_loss: 0.72496760\n",
      "Step: [3677] d_loss: 1.38393211, g_loss: 0.71555042\n",
      "Step: [3678] d_loss: 1.37989569, g_loss: 0.71380889\n",
      "Step: [3679] d_loss: 1.38264847, g_loss: 0.72424066\n",
      "Step: [3680] d_loss: 1.37509060, g_loss: 0.71660566\n",
      "Step: [3681] d_loss: 1.38249123, g_loss: 0.71878564\n",
      "Step: [3682] d_loss: 1.37324715, g_loss: 0.71807170\n",
      "Step: [3683] d_loss: 1.37321615, g_loss: 0.71916789\n",
      "Step: [3684] d_loss: 1.37528932, g_loss: 0.72589171\n",
      "Step: [3685] d_loss: 1.38702011, g_loss: 0.71463168\n",
      "Step: [3686] d_loss: 1.36804056, g_loss: 0.72126830\n",
      "Step: [3687] d_loss: 1.37737322, g_loss: 0.72593153\n",
      "Step: [3688] d_loss: 1.36596107, g_loss: 0.72575647\n",
      "Step: [3689] d_loss: 1.37834537, g_loss: 0.71865070\n",
      "Step: [3690] d_loss: 1.37017667, g_loss: 0.72369403\n",
      "Step: [3691] d_loss: 1.39046645, g_loss: 0.71300960\n",
      "Step: [3692] d_loss: 1.38622367, g_loss: 0.71091241\n",
      "Step: [3693] d_loss: 1.38644123, g_loss: 0.71327734\n",
      "Step: [3694] d_loss: 1.39689660, g_loss: 0.71496868\n",
      "Step: [3695] d_loss: 1.39108181, g_loss: 0.71328491\n",
      "Step: [3696] d_loss: 1.38451362, g_loss: 0.71222591\n",
      "Step: [3697] d_loss: 1.39051604, g_loss: 0.71641362\n",
      "Step: [3698] d_loss: 1.37565827, g_loss: 0.72012496\n",
      "Step: [3699] d_loss: 1.39055622, g_loss: 0.70898187\n",
      "Step: [3700] d_loss: 1.39432549, g_loss: 0.72077376\n",
      "Step: [3701] d_loss: 1.38615906, g_loss: 0.71545267\n",
      "Step: [3702] d_loss: 1.38994253, g_loss: 0.72111303\n",
      "Step: [3703] d_loss: 1.37573791, g_loss: 0.71440852\n",
      "Step: [3704] d_loss: 1.39507866, g_loss: 0.71609420\n",
      "Step: [3705] d_loss: 1.37925470, g_loss: 0.71458852\n",
      "Step: [3706] d_loss: 1.38967180, g_loss: 0.71933365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3707] d_loss: 1.38395870, g_loss: 0.71895397\n",
      "Step: [3708] d_loss: 1.36782277, g_loss: 0.71814746\n",
      "Step: [3709] d_loss: 1.37652469, g_loss: 0.71568477\n",
      "Step: [3710] d_loss: 1.37061501, g_loss: 0.72235703\n",
      "Step: [3711] d_loss: 1.38753057, g_loss: 0.71356940\n",
      "Step: [3712] d_loss: 1.38532329, g_loss: 0.71611273\n",
      "Step: [3713] d_loss: 1.38023496, g_loss: 0.71334642\n",
      "Step: [3714] d_loss: 1.37891424, g_loss: 0.71952873\n",
      "Step: [3715] d_loss: 1.38784599, g_loss: 0.71559602\n",
      "Step: [3716] d_loss: 1.38184047, g_loss: 0.71113169\n",
      "Step: [3717] d_loss: 1.38487804, g_loss: 0.71131361\n",
      "Step: [3718] d_loss: 1.38322949, g_loss: 0.71412051\n",
      "Step: [3719] d_loss: 1.39172363, g_loss: 0.70484811\n",
      "Step: [3720] d_loss: 1.37779689, g_loss: 0.71769011\n",
      "Step: [3721] d_loss: 1.37518728, g_loss: 0.71678841\n",
      "Step: [3722] d_loss: 1.39023972, g_loss: 0.72320330\n",
      "Step: [3723] d_loss: 1.37768793, g_loss: 0.71195012\n",
      "Step: [3724] d_loss: 1.38352299, g_loss: 0.71700960\n",
      "Step: [3725] d_loss: 1.38706088, g_loss: 0.71383119\n",
      "Step: [3726] d_loss: 1.39028478, g_loss: 0.71555936\n",
      "Step: [3727] d_loss: 1.38468027, g_loss: 0.71035391\n",
      "Step: [3728] d_loss: 1.38330352, g_loss: 0.71117008\n",
      "Step: [3729] d_loss: 1.37790775, g_loss: 0.71573293\n",
      "Step: [3730] d_loss: 1.37129307, g_loss: 0.72141790\n",
      "Step: [3731] d_loss: 1.37449896, g_loss: 0.72593403\n",
      "Step: [3732] d_loss: 1.37054527, g_loss: 0.72705901\n",
      "Step: [3733] d_loss: 1.38589501, g_loss: 0.71419907\n",
      "Step: [3734] d_loss: 1.38601160, g_loss: 0.70975816\n",
      "Step: [3735] d_loss: 1.37527633, g_loss: 0.72278351\n",
      "Step: [3736] d_loss: 1.38021421, g_loss: 0.71912897\n",
      "Step: [3737] d_loss: 1.37529063, g_loss: 0.71861112\n",
      "Step: [3738] d_loss: 1.37815690, g_loss: 0.71941555\n",
      "Step: [3739] d_loss: 1.38587511, g_loss: 0.72192967\n",
      "Step: [3740] d_loss: 1.38191855, g_loss: 0.71316528\n",
      "Step: [3741] d_loss: 1.38154602, g_loss: 0.71969736\n",
      "Step: [3742] d_loss: 1.38484168, g_loss: 0.71541619\n",
      "Step: [3743] d_loss: 1.37329698, g_loss: 0.72033560\n",
      "Step: [3744] d_loss: 1.38090074, g_loss: 0.71625555\n",
      "Step: [3745] d_loss: 1.38472569, g_loss: 0.71517205\n",
      "Step: [3746] d_loss: 1.38216782, g_loss: 0.71050549\n",
      "Step: [3747] d_loss: 1.36723661, g_loss: 0.72264993\n",
      "Step: [3748] d_loss: 1.37456012, g_loss: 0.72555590\n",
      "Step: [3749] d_loss: 1.39712083, g_loss: 0.70614475\n",
      "Step: [3750] d_loss: 1.38466001, g_loss: 0.71748924\n",
      "Step: [3751] d_loss: 1.38981664, g_loss: 0.71411526\n",
      "Step: [3752] d_loss: 1.38906121, g_loss: 0.71234882\n",
      "Step: [3753] d_loss: 1.38835430, g_loss: 0.71517003\n",
      "Step: [3754] d_loss: 1.39242625, g_loss: 0.71451414\n",
      "Step: [3755] d_loss: 1.40334356, g_loss: 0.70896077\n",
      "Step: [3756] d_loss: 1.37686801, g_loss: 0.72933781\n",
      "Step: [3757] d_loss: 1.37179565, g_loss: 0.71947956\n",
      "Step: [3758] d_loss: 1.38365626, g_loss: 0.71801221\n",
      "Step: [3759] d_loss: 1.37746835, g_loss: 0.71409500\n",
      "Step: [3760] d_loss: 1.37991190, g_loss: 0.71059179\n",
      "Step: [3761] d_loss: 1.37439394, g_loss: 0.72969294\n",
      "Step: [3762] d_loss: 1.38501024, g_loss: 0.71162117\n",
      "Step: [3763] d_loss: 1.39034235, g_loss: 0.70770442\n",
      "Step: [3764] d_loss: 1.38655579, g_loss: 0.70898211\n",
      "Step: [3765] d_loss: 1.39946115, g_loss: 0.71250278\n",
      "Step: [3766] d_loss: 1.38768399, g_loss: 0.71595711\n",
      "Step: [3767] d_loss: 1.38183308, g_loss: 0.72325957\n",
      "Step: [3768] d_loss: 1.38344800, g_loss: 0.71849626\n",
      "Step: [3769] d_loss: 1.38527775, g_loss: 0.71354353\n",
      "Step: [3770] d_loss: 1.39102507, g_loss: 0.71137822\n",
      "Step: [3771] d_loss: 1.39008439, g_loss: 0.71466255\n",
      "Step: [3772] d_loss: 1.39094877, g_loss: 0.71406376\n",
      "Step: [3773] d_loss: 1.39578807, g_loss: 0.71228313\n",
      "Step: [3774] d_loss: 1.39084637, g_loss: 0.71331495\n",
      "Step: [3775] d_loss: 1.37684453, g_loss: 0.71932292\n",
      "Step: [3776] d_loss: 1.37721395, g_loss: 0.71902168\n",
      "Step: [3777] d_loss: 1.39023757, g_loss: 0.71735871\n",
      "Step: [3778] d_loss: 1.38337278, g_loss: 0.71631867\n",
      "Step: [3779] d_loss: 1.38618636, g_loss: 0.71486330\n",
      "Step: [3780] d_loss: 1.37097311, g_loss: 0.71811271\n",
      "Step: [3781] d_loss: 1.37646377, g_loss: 0.72377419\n",
      "Step: [3782] d_loss: 1.40232778, g_loss: 0.71301568\n",
      "Step: [3783] d_loss: 1.39207351, g_loss: 0.72861147\n",
      "Step: [3784] d_loss: 1.39048636, g_loss: 0.71435642\n",
      "Step: [3785] d_loss: 1.37251747, g_loss: 0.71709913\n",
      "Step: [3786] d_loss: 1.37225735, g_loss: 0.72422582\n",
      "Step: [3787] d_loss: 1.38033104, g_loss: 0.71919161\n",
      "Step: [3788] d_loss: 1.37889147, g_loss: 0.71883178\n",
      "Step: [3789] d_loss: 1.38590145, g_loss: 0.71853995\n",
      "Step: [3790] d_loss: 1.38120770, g_loss: 0.72192323\n",
      "Step: [3791] d_loss: 1.38994813, g_loss: 0.71068859\n",
      "Step: [3792] d_loss: 1.37722909, g_loss: 0.72010636\n",
      "Step: [3793] d_loss: 1.39327621, g_loss: 0.71510810\n",
      "Step: [3794] d_loss: 1.38403690, g_loss: 0.71541500\n",
      "Step: [3795] d_loss: 1.38665628, g_loss: 0.71696079\n",
      "Step: [3796] d_loss: 1.38101625, g_loss: 0.71542132\n",
      "Step: [3797] d_loss: 1.38755393, g_loss: 0.70799649\n",
      "Step: [3798] d_loss: 1.37274814, g_loss: 0.72291100\n",
      "Step: [3799] d_loss: 1.37977540, g_loss: 0.71701849\n",
      "Step: [3800] d_loss: 1.37875462, g_loss: 0.71759367\n",
      "Step: [3801] d_loss: 1.37823486, g_loss: 0.71735716\n",
      "Step: [3802] d_loss: 1.37530708, g_loss: 0.71704280\n",
      "Step: [3803] d_loss: 1.38170481, g_loss: 0.71967518\n",
      "Step: [3804] d_loss: 1.37932551, g_loss: 0.72186810\n",
      "Step: [3805] d_loss: 1.38482666, g_loss: 0.70893121\n",
      "Step: [3806] d_loss: 1.38451195, g_loss: 0.71795475\n",
      "Step: [3807] d_loss: 1.39307463, g_loss: 0.70983195\n",
      "Step: [3808] d_loss: 1.38677204, g_loss: 0.71119940\n",
      "Step: [3809] d_loss: 1.39118648, g_loss: 0.71312702\n",
      "Step: [3810] d_loss: 1.37741613, g_loss: 0.71934026\n",
      "Step: [3811] d_loss: 1.38550663, g_loss: 0.71714181\n",
      "Step: [3812] d_loss: 1.38362074, g_loss: 0.71318966\n",
      "Step: [3813] d_loss: 1.38602710, g_loss: 0.71799505\n",
      "Step: [3814] d_loss: 1.39331663, g_loss: 0.71009707\n",
      "Step: [3815] d_loss: 1.37907732, g_loss: 0.72573352\n",
      "Step: [3816] d_loss: 1.37659991, g_loss: 0.73050094\n",
      "Step: [3817] d_loss: 1.39409328, g_loss: 0.71928215\n",
      "Step: [3818] d_loss: 1.37005913, g_loss: 0.72280455\n",
      "Step: [3819] d_loss: 1.39336586, g_loss: 0.70808637\n",
      "Step: [3820] d_loss: 1.38018847, g_loss: 0.71158385\n",
      "Step: [3821] d_loss: 1.37090731, g_loss: 0.71774614\n",
      "Step: [3822] d_loss: 1.38981688, g_loss: 0.72037375\n",
      "Step: [3823] d_loss: 1.39155793, g_loss: 0.70894432\n",
      "Step: [3824] d_loss: 1.40389466, g_loss: 0.70463240\n",
      "Step: [3825] d_loss: 1.39163506, g_loss: 0.71199995\n",
      "Step: [3826] d_loss: 1.37883115, g_loss: 0.71891320\n",
      "Step: [3827] d_loss: 1.39227402, g_loss: 0.70518088\n",
      "Step: [3828] d_loss: 1.38238406, g_loss: 0.71814448\n",
      "Step: [3829] d_loss: 1.38190365, g_loss: 0.71671224\n",
      "Step: [3830] d_loss: 1.38560057, g_loss: 0.72016388\n",
      "Step: [3831] d_loss: 1.38602626, g_loss: 0.71726960\n",
      "Step: [3832] d_loss: 1.39155889, g_loss: 0.72360647\n",
      "Step: [3833] d_loss: 1.40004086, g_loss: 0.71594465\n",
      "Step: [3834] d_loss: 1.38381398, g_loss: 0.71737891\n",
      "Step: [3835] d_loss: 1.38412404, g_loss: 0.71989286\n",
      "Step: [3836] d_loss: 1.39205587, g_loss: 0.70854187\n",
      "Step: [3837] d_loss: 1.38487267, g_loss: 0.71047533\n",
      "Step: [3838] d_loss: 1.37784874, g_loss: 0.71374547\n",
      "Step: [3839] d_loss: 1.38161242, g_loss: 0.72014821\n",
      "Step: [3840] d_loss: 1.37612700, g_loss: 0.72115493\n",
      "Step: [3841] d_loss: 1.39209366, g_loss: 0.71734321\n",
      "Step: [3842] d_loss: 1.38370705, g_loss: 0.71785975\n",
      "Step: [3843] d_loss: 1.39296269, g_loss: 0.70412064\n",
      "Step: [3844] d_loss: 1.38385165, g_loss: 0.70700336\n",
      "Step: [3845] d_loss: 1.38953829, g_loss: 0.71382600\n",
      "Step: [3846] d_loss: 1.38556182, g_loss: 0.70981562\n",
      "Step: [3847] d_loss: 1.38240361, g_loss: 0.71924484\n",
      "Step: [3848] d_loss: 1.39116800, g_loss: 0.71375620\n",
      "Step: [3849] d_loss: 1.36980915, g_loss: 0.71834403\n",
      "Step: [3850] d_loss: 1.38081884, g_loss: 0.71612978\n",
      "Step: [3851] d_loss: 1.38604832, g_loss: 0.71350360\n",
      "Step: [3852] d_loss: 1.38720393, g_loss: 0.71181440\n",
      "Step: [3853] d_loss: 1.40160751, g_loss: 0.71197945\n",
      "Step: [3854] d_loss: 1.39151812, g_loss: 0.71952701\n",
      "Step: [3855] d_loss: 1.39332891, g_loss: 0.72616333\n",
      "Step: [3856] d_loss: 1.39403725, g_loss: 0.71359980\n",
      "Step: [3857] d_loss: 1.38497663, g_loss: 0.71025139\n",
      "Step: [3858] d_loss: 1.39064622, g_loss: 0.70995331\n",
      "Step: [3859] d_loss: 1.38414931, g_loss: 0.71018577\n",
      "Step: [3860] d_loss: 1.37867188, g_loss: 0.71134484\n",
      "Step: [3861] d_loss: 1.39427614, g_loss: 0.70205790\n",
      "Step: [3862] d_loss: 1.37554383, g_loss: 0.71901453\n",
      "Step: [3863] d_loss: 1.38765264, g_loss: 0.71084571\n",
      "Step: [3864] d_loss: 1.38416266, g_loss: 0.71236062\n",
      "Step: [3865] d_loss: 1.36302733, g_loss: 0.71639025\n",
      "Step: [3866] d_loss: 1.37663555, g_loss: 0.71449375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3867] d_loss: 1.38532424, g_loss: 0.71271610\n",
      "Step: [3868] d_loss: 1.40171313, g_loss: 0.71599793\n",
      "Step: [3869] d_loss: 1.38119698, g_loss: 0.70599294\n",
      "Step: [3870] d_loss: 1.39522314, g_loss: 0.70003945\n",
      "Step: [3871] d_loss: 1.38360524, g_loss: 0.71033597\n",
      "Step: [3872] d_loss: 1.39507294, g_loss: 0.70862746\n",
      "Step: [3873] d_loss: 1.39655316, g_loss: 0.71488380\n",
      "Step: [3874] d_loss: 1.37789536, g_loss: 0.72140902\n",
      "Step: [3875] d_loss: 1.39165962, g_loss: 0.71383107\n",
      "Step: [3876] d_loss: 1.40092516, g_loss: 0.70821297\n",
      "Step: [3877] d_loss: 1.39690030, g_loss: 0.70233452\n",
      "Step: [3878] d_loss: 1.40109575, g_loss: 0.71029001\n",
      "Step: [3879] d_loss: 1.37661242, g_loss: 0.70473433\n",
      "Step: [3880] d_loss: 1.37642360, g_loss: 0.71466482\n",
      "Step: [3881] d_loss: 1.38849688, g_loss: 0.71385741\n",
      "Step: [3882] d_loss: 1.39078748, g_loss: 0.71672428\n",
      "Step: [3883] d_loss: 1.38868201, g_loss: 0.70731682\n",
      "Step: [3884] d_loss: 1.38139176, g_loss: 0.71215069\n",
      "Step: [3885] d_loss: 1.39275980, g_loss: 0.71403944\n",
      "Step: [3886] d_loss: 1.38683033, g_loss: 0.71427858\n",
      "Step: [3887] d_loss: 1.38129914, g_loss: 0.71346474\n",
      "Step: [3888] d_loss: 1.38280582, g_loss: 0.71362913\n",
      "Step: [3889] d_loss: 1.37778533, g_loss: 0.70771128\n",
      "Step: [3890] d_loss: 1.37529898, g_loss: 0.71996850\n",
      "Step: [3891] d_loss: 1.38019156, g_loss: 0.71729898\n",
      "Step: [3892] d_loss: 1.37855339, g_loss: 0.71259236\n",
      "Step: [3893] d_loss: 1.38418007, g_loss: 0.72012872\n",
      "Step: [3894] d_loss: 1.38236153, g_loss: 0.71582586\n",
      "Step: [3895] d_loss: 1.38004613, g_loss: 0.72200644\n",
      "Step: [3896] d_loss: 1.38383496, g_loss: 0.72115290\n",
      "Step: [3897] d_loss: 1.38422298, g_loss: 0.71230483\n",
      "Step: [3898] d_loss: 1.38884926, g_loss: 0.71292049\n",
      "Step: [3899] d_loss: 1.39286566, g_loss: 0.70854115\n",
      "Step: [3900] d_loss: 1.38203418, g_loss: 0.71704268\n",
      "Step: [3901] d_loss: 1.38029361, g_loss: 0.72322661\n",
      "Step: [3902] d_loss: 1.37959683, g_loss: 0.72321081\n",
      "Step: [3903] d_loss: 1.38604903, g_loss: 0.71161878\n",
      "Step: [3904] d_loss: 1.37503839, g_loss: 0.72126257\n",
      "Step: [3905] d_loss: 1.38030696, g_loss: 0.71925706\n",
      "Step: [3906] d_loss: 1.38536978, g_loss: 0.70903414\n",
      "Step: [3907] d_loss: 1.37848377, g_loss: 0.71958011\n",
      "Step: [3908] d_loss: 1.36570287, g_loss: 0.71937281\n",
      "Step: [3909] d_loss: 1.38248610, g_loss: 0.71885711\n",
      "Step: [3910] d_loss: 1.38243949, g_loss: 0.71015888\n",
      "Step: [3911] d_loss: 1.38625991, g_loss: 0.70996785\n",
      "Step: [3912] d_loss: 1.38590395, g_loss: 0.71270823\n",
      "Step: [3913] d_loss: 1.37679839, g_loss: 0.71370351\n",
      "Step: [3914] d_loss: 1.39216268, g_loss: 0.70717359\n",
      "Step: [3915] d_loss: 1.38316154, g_loss: 0.71412730\n",
      "Step: [3916] d_loss: 1.38599586, g_loss: 0.71060765\n",
      "Step: [3917] d_loss: 1.38600659, g_loss: 0.70982480\n",
      "Step: [3918] d_loss: 1.39030242, g_loss: 0.70857352\n",
      "Step: [3919] d_loss: 1.37486982, g_loss: 0.71778661\n",
      "Step: [3920] d_loss: 1.39091372, g_loss: 0.71308804\n",
      "Step: [3921] d_loss: 1.38760614, g_loss: 0.71451753\n",
      "Step: [3922] d_loss: 1.38703609, g_loss: 0.70682740\n",
      "Step: [3923] d_loss: 1.38042831, g_loss: 0.71204019\n",
      "Step: [3924] d_loss: 1.37353718, g_loss: 0.71569061\n",
      "Step: [3925] d_loss: 1.39363253, g_loss: 0.70771372\n",
      "Step: [3926] d_loss: 1.38761544, g_loss: 0.71180022\n",
      "Step: [3927] d_loss: 1.37792838, g_loss: 0.71097469\n",
      "Step: [3928] d_loss: 1.37770009, g_loss: 0.70339286\n",
      "Step: [3929] d_loss: 1.39535511, g_loss: 0.71193874\n",
      "Step: [3930] d_loss: 1.39962137, g_loss: 0.71211940\n",
      "Step: [3931] d_loss: 1.38734376, g_loss: 0.71265310\n",
      "Step: [3932] d_loss: 1.38296461, g_loss: 0.70953536\n",
      "Step: [3933] d_loss: 1.39014411, g_loss: 0.71460491\n",
      "Step: [3934] d_loss: 1.38457584, g_loss: 0.72004282\n",
      "Step: [3935] d_loss: 1.37053490, g_loss: 0.71979165\n",
      "Step: [3936] d_loss: 1.39198732, g_loss: 0.71077847\n",
      "Step: [3937] d_loss: 1.38070536, g_loss: 0.71399271\n",
      "Step: [3938] d_loss: 1.38304830, g_loss: 0.71277261\n",
      "Step: [3939] d_loss: 1.37735474, g_loss: 0.72376037\n",
      "Step: [3940] d_loss: 1.37405801, g_loss: 0.72049546\n",
      "Step: [3941] d_loss: 1.37960291, g_loss: 0.70263362\n",
      "Step: [3942] d_loss: 1.37283492, g_loss: 0.71261442\n",
      "Step: [3943] d_loss: 1.37796235, g_loss: 0.71072674\n",
      "Step: [3944] d_loss: 1.37215447, g_loss: 0.72049367\n",
      "Step: [3945] d_loss: 1.38924170, g_loss: 0.71054357\n",
      "Step: [3946] d_loss: 1.38549638, g_loss: 0.72158849\n",
      "Step: [3947] d_loss: 1.39160442, g_loss: 0.72108966\n",
      "Step: [3948] d_loss: 1.38733959, g_loss: 0.71215135\n",
      "Step: [3949] d_loss: 1.38069022, g_loss: 0.71707642\n",
      "Step: [3950] d_loss: 1.37568974, g_loss: 0.71258873\n",
      "Step: [3951] d_loss: 1.39511561, g_loss: 0.70951664\n",
      "Step: [3952] d_loss: 1.37709463, g_loss: 0.71259689\n",
      "Step: [3953] d_loss: 1.37769985, g_loss: 0.71704876\n",
      "Step: [3954] d_loss: 1.37893903, g_loss: 0.70946664\n",
      "Step: [3955] d_loss: 1.37940788, g_loss: 0.71142256\n",
      "Step: [3956] d_loss: 1.39463973, g_loss: 0.70356560\n",
      "Step: [3957] d_loss: 1.38982892, g_loss: 0.71089268\n",
      "Step: [3958] d_loss: 1.39418805, g_loss: 0.71079504\n",
      "Step: [3959] d_loss: 1.38153446, g_loss: 0.71227020\n",
      "Step: [3960] d_loss: 1.38972783, g_loss: 0.71204191\n",
      "Step: [3961] d_loss: 1.39405727, g_loss: 0.70616400\n",
      "Step: [3962] d_loss: 1.38868833, g_loss: 0.71275395\n",
      "Step: [3963] d_loss: 1.38597465, g_loss: 0.71739429\n",
      "Step: [3964] d_loss: 1.38094258, g_loss: 0.71672583\n",
      "Step: [3965] d_loss: 1.38759685, g_loss: 0.72429001\n",
      "Step: [3966] d_loss: 1.40179908, g_loss: 0.70700938\n",
      "Step: [3967] d_loss: 1.38433385, g_loss: 0.71011639\n",
      "Step: [3968] d_loss: 1.39185274, g_loss: 0.71718270\n",
      "Step: [3969] d_loss: 1.38308096, g_loss: 0.71175468\n",
      "Step: [3970] d_loss: 1.37108481, g_loss: 0.72436082\n",
      "Step: [3971] d_loss: 1.38062656, g_loss: 0.71086186\n",
      "Step: [3972] d_loss: 1.38560247, g_loss: 0.71528566\n",
      "Step: [3973] d_loss: 1.39181566, g_loss: 0.71015203\n",
      "Step: [3974] d_loss: 1.38533950, g_loss: 0.72433960\n",
      "Step: [3975] d_loss: 1.40008712, g_loss: 0.73990744\n",
      "Step: [3976] d_loss: 1.43509483, g_loss: 0.73751140\n",
      "Step: [3977] d_loss: 1.42223191, g_loss: 0.73415363\n",
      "Step: [3978] d_loss: 1.40573239, g_loss: 0.72176647\n",
      "Step: [3979] d_loss: 1.38234234, g_loss: 0.72081375\n",
      "Step: [3980] d_loss: 1.38893890, g_loss: 0.71471941\n",
      "Step: [3981] d_loss: 1.38056397, g_loss: 0.71313161\n",
      "Step: [3982] d_loss: 1.37758446, g_loss: 0.71642101\n",
      "Step: [3983] d_loss: 1.39047050, g_loss: 0.71291089\n",
      "Step: [3984] d_loss: 1.37868452, g_loss: 0.71673411\n",
      "Step: [3985] d_loss: 1.38864732, g_loss: 0.71591437\n",
      "Step: [3986] d_loss: 1.38780165, g_loss: 0.70672745\n",
      "Step: [3987] d_loss: 1.39168096, g_loss: 0.70606577\n",
      "Step: [3988] d_loss: 1.37801254, g_loss: 0.70705700\n",
      "Step: [3989] d_loss: 1.37783027, g_loss: 0.71529937\n",
      "Step: [3990] d_loss: 1.38060772, g_loss: 0.71606874\n",
      "Step: [3991] d_loss: 1.37600350, g_loss: 0.72127891\n",
      "Step: [3992] d_loss: 1.38931751, g_loss: 0.70974952\n",
      "Step: [3993] d_loss: 1.38904858, g_loss: 0.70665002\n",
      "Step: [3994] d_loss: 1.38661087, g_loss: 0.71018642\n",
      "Step: [3995] d_loss: 1.39245284, g_loss: 0.70571530\n",
      "Step: [3996] d_loss: 1.37657022, g_loss: 0.71574938\n",
      "Step: [3997] d_loss: 1.38181818, g_loss: 0.71154940\n",
      "Step: [3998] d_loss: 1.38800073, g_loss: 0.71251702\n",
      "Step: [3999] d_loss: 1.37763417, g_loss: 0.71678895\n",
      "Step: [4000] d_loss: 1.38064194, g_loss: 0.71457386\n",
      "Step: [4001] d_loss: 1.37862110, g_loss: 0.71075654\n",
      "Step: [4002] d_loss: 1.38743389, g_loss: 0.70737267\n",
      "Step: [4003] d_loss: 1.39228642, g_loss: 0.70547414\n",
      "Step: [4004] d_loss: 1.37426448, g_loss: 0.71494174\n",
      "Step: [4005] d_loss: 1.38159800, g_loss: 0.71113932\n",
      "Step: [4006] d_loss: 1.38347554, g_loss: 0.70853460\n",
      "Step: [4007] d_loss: 1.39079618, g_loss: 0.70742023\n",
      "Step: [4008] d_loss: 1.38063848, g_loss: 0.71323264\n",
      "Step: [4009] d_loss: 1.38229465, g_loss: 0.70993423\n",
      "Step: [4010] d_loss: 1.39610887, g_loss: 0.70721304\n",
      "Step: [4011] d_loss: 1.37439895, g_loss: 0.71507633\n",
      "Step: [4012] d_loss: 1.38593221, g_loss: 0.70582008\n",
      "Step: [4013] d_loss: 1.38598228, g_loss: 0.70634288\n",
      "Step: [4014] d_loss: 1.37814546, g_loss: 0.71659684\n",
      "Step: [4015] d_loss: 1.38317132, g_loss: 0.70748627\n",
      "Step: [4016] d_loss: 1.37606859, g_loss: 0.71250284\n",
      "Step: [4017] d_loss: 1.37654471, g_loss: 0.71519345\n",
      "Step: [4018] d_loss: 1.37822628, g_loss: 0.71336335\n",
      "Step: [4019] d_loss: 1.37759542, g_loss: 0.71925855\n",
      "Step: [4020] d_loss: 1.38544667, g_loss: 0.70609850\n",
      "Step: [4021] d_loss: 1.38771260, g_loss: 0.70866144\n",
      "Step: [4022] d_loss: 1.37339282, g_loss: 0.71528757\n",
      "Step: [4023] d_loss: 1.38590288, g_loss: 0.71538818\n",
      "Step: [4024] d_loss: 1.38577855, g_loss: 0.70309603\n",
      "Step: [4025] d_loss: 1.36883307, g_loss: 0.71735108\n",
      "Step: [4026] d_loss: 1.38742900, g_loss: 0.71187937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4027] d_loss: 1.39173508, g_loss: 0.70466769\n",
      "Step: [4028] d_loss: 1.38463795, g_loss: 0.71271169\n",
      "Step: [4029] d_loss: 1.39014530, g_loss: 0.70887613\n",
      "Step: [4030] d_loss: 1.37566054, g_loss: 0.71264291\n",
      "Step: [4031] d_loss: 1.38890505, g_loss: 0.71245420\n",
      "Step: [4032] d_loss: 1.38278246, g_loss: 0.70627403\n",
      "Step: [4033] d_loss: 1.37056541, g_loss: 0.71520019\n",
      "Step: [4034] d_loss: 1.36959469, g_loss: 0.71482676\n",
      "Step: [4035] d_loss: 1.39370227, g_loss: 0.70488089\n",
      "Step: [4036] d_loss: 1.38991058, g_loss: 0.71075761\n",
      "Step: [4037] d_loss: 1.38827050, g_loss: 0.70870095\n",
      "Step: [4038] d_loss: 1.38497984, g_loss: 0.71281964\n",
      "Step: [4039] d_loss: 1.38237190, g_loss: 0.71998429\n",
      "Step: [4040] d_loss: 1.39405584, g_loss: 0.70003766\n",
      "Step: [4041] d_loss: 1.39748502, g_loss: 0.70308560\n",
      "Step: [4042] d_loss: 1.38291025, g_loss: 0.71882176\n",
      "Step: [4043] d_loss: 1.38604224, g_loss: 0.71398765\n",
      "Step: [4044] d_loss: 1.39040089, g_loss: 0.70576370\n",
      "Step: [4045] d_loss: 1.37706733, g_loss: 0.71068192\n",
      "Step: [4046] d_loss: 1.39413321, g_loss: 0.70403451\n",
      "Step: [4047] d_loss: 1.38414240, g_loss: 0.71263695\n",
      "Step: [4048] d_loss: 1.37800789, g_loss: 0.71763879\n",
      "Step: [4049] d_loss: 1.38517737, g_loss: 0.71196091\n",
      "Step: [4050] d_loss: 1.38453579, g_loss: 0.71475101\n",
      "Step: [4051] d_loss: 1.39307725, g_loss: 0.71343040\n",
      "Step: [4052] d_loss: 1.37361956, g_loss: 0.72457027\n",
      "Step: [4053] d_loss: 1.38794720, g_loss: 0.70607799\n",
      "Step: [4054] d_loss: 1.39033043, g_loss: 0.70906740\n",
      "Step: [4055] d_loss: 1.38665104, g_loss: 0.71252227\n",
      "Step: [4056] d_loss: 1.39386976, g_loss: 0.70595717\n",
      "Step: [4057] d_loss: 1.37691760, g_loss: 0.71405774\n",
      "Step: [4058] d_loss: 1.38909984, g_loss: 0.70510793\n",
      "Step: [4059] d_loss: 1.39420891, g_loss: 0.70776308\n",
      "Step: [4060] d_loss: 1.38858843, g_loss: 0.70306802\n",
      "Step: [4061] d_loss: 1.38412666, g_loss: 0.71158379\n",
      "Step: [4062] d_loss: 1.37675643, g_loss: 0.71454316\n",
      "Step: [4063] d_loss: 1.37934542, g_loss: 0.71706140\n",
      "Step: [4064] d_loss: 1.38177466, g_loss: 0.71563095\n",
      "Step: [4065] d_loss: 1.38786530, g_loss: 0.71125859\n",
      "Step: [4066] d_loss: 1.38001800, g_loss: 0.71775889\n",
      "Step: [4067] d_loss: 1.38558722, g_loss: 0.71447825\n",
      "Step: [4068] d_loss: 1.38669634, g_loss: 0.71005297\n",
      "Step: [4069] d_loss: 1.38108182, g_loss: 0.71744585\n",
      "Step: [4070] d_loss: 1.38201070, g_loss: 0.71162957\n",
      "Step: [4071] d_loss: 1.38492537, g_loss: 0.71615386\n",
      "Step: [4072] d_loss: 1.37742901, g_loss: 0.72421372\n",
      "Step: [4073] d_loss: 1.38554764, g_loss: 0.71152818\n",
      "Step: [4074] d_loss: 1.38271952, g_loss: 0.71611685\n",
      "Step: [4075] d_loss: 1.39372373, g_loss: 0.70891804\n",
      "Step: [4076] d_loss: 1.38331938, g_loss: 0.71925789\n",
      "Step: [4077] d_loss: 1.38070512, g_loss: 0.71230227\n",
      "Step: [4078] d_loss: 1.37763357, g_loss: 0.72143912\n",
      "Step: [4079] d_loss: 1.38291883, g_loss: 0.71513826\n",
      "Step: [4080] d_loss: 1.38136446, g_loss: 0.71675205\n",
      "Step: [4081] d_loss: 1.38169611, g_loss: 0.70878458\n",
      "Step: [4082] d_loss: 1.37370324, g_loss: 0.71538603\n",
      "Step: [4083] d_loss: 1.38402915, g_loss: 0.71714777\n",
      "Step: [4084] d_loss: 1.37968242, g_loss: 0.72439224\n",
      "Step: [4085] d_loss: 1.39531446, g_loss: 0.70677608\n",
      "Step: [4086] d_loss: 1.37381256, g_loss: 0.71198285\n",
      "Step: [4087] d_loss: 1.38894987, g_loss: 0.71511471\n",
      "Step: [4088] d_loss: 1.39723444, g_loss: 0.70428592\n",
      "Step: [4089] d_loss: 1.38695061, g_loss: 0.70733875\n",
      "Step: [4090] d_loss: 1.37420297, g_loss: 0.71519041\n",
      "Step: [4091] d_loss: 1.39700866, g_loss: 0.70529211\n",
      "Step: [4092] d_loss: 1.38460588, g_loss: 0.70579815\n",
      "Step: [4093] d_loss: 1.38161469, g_loss: 0.71047074\n",
      "Step: [4094] d_loss: 1.38940334, g_loss: 0.71134561\n",
      "Step: [4095] d_loss: 1.38367343, g_loss: 0.71545732\n",
      "Step: [4096] d_loss: 1.38017106, g_loss: 0.70241416\n",
      "Step: [4097] d_loss: 1.38268590, g_loss: 0.71723104\n",
      "Step: [4098] d_loss: 1.38209748, g_loss: 0.71717179\n",
      "Step: [4099] d_loss: 1.39927137, g_loss: 0.70911551\n",
      "Step: [4100] d_loss: 1.39015651, g_loss: 0.71931636\n",
      "Step: [4101] d_loss: 1.39848602, g_loss: 0.70159471\n",
      "Step: [4102] d_loss: 1.38469565, g_loss: 0.71708632\n",
      "Step: [4103] d_loss: 1.38310242, g_loss: 0.71771252\n",
      "Step: [4104] d_loss: 1.38652062, g_loss: 0.72072637\n",
      "Step: [4105] d_loss: 1.39652717, g_loss: 0.70876920\n",
      "Step: [4106] d_loss: 1.38552499, g_loss: 0.71299380\n",
      "Step: [4107] d_loss: 1.38526893, g_loss: 0.70940328\n",
      "Step: [4108] d_loss: 1.37927127, g_loss: 0.70866740\n",
      "Step: [4109] d_loss: 1.38502240, g_loss: 0.71152937\n",
      "Step: [4110] d_loss: 1.38040042, g_loss: 0.71702647\n",
      "Step: [4111] d_loss: 1.39058256, g_loss: 0.70925182\n",
      "Step: [4112] d_loss: 1.38203168, g_loss: 0.71813411\n",
      "Step: [4113] d_loss: 1.39008939, g_loss: 0.72498304\n",
      "Step: [4114] d_loss: 1.39248323, g_loss: 0.71903360\n",
      "Step: [4115] d_loss: 1.36917055, g_loss: 0.71960962\n",
      "Step: [4116] d_loss: 1.36385381, g_loss: 0.72207433\n",
      "Step: [4117] d_loss: 1.37953019, g_loss: 0.71348715\n",
      "Step: [4118] d_loss: 1.38113725, g_loss: 0.71585166\n",
      "Step: [4119] d_loss: 1.37499166, g_loss: 0.71565235\n",
      "Step: [4120] d_loss: 1.38593054, g_loss: 0.70757312\n",
      "Step: [4121] d_loss: 1.38363266, g_loss: 0.70980126\n",
      "Step: [4122] d_loss: 1.37979555, g_loss: 0.71747792\n",
      "Step: [4123] d_loss: 1.37298620, g_loss: 0.71664560\n",
      "Step: [4124] d_loss: 1.38627279, g_loss: 0.70513839\n",
      "Step: [4125] d_loss: 1.38727987, g_loss: 0.70731235\n",
      "Step: [4126] d_loss: 1.38058126, g_loss: 0.70937020\n",
      "Step: [4127] d_loss: 1.37475634, g_loss: 0.70774305\n",
      "Step: [4128] d_loss: 1.37936330, g_loss: 0.72119290\n",
      "Step: [4129] d_loss: 1.38580537, g_loss: 0.71493149\n",
      "Step: [4130] d_loss: 1.40508413, g_loss: 0.70373291\n",
      "Step: [4131] d_loss: 1.38692904, g_loss: 0.70815814\n",
      "Step: [4132] d_loss: 1.37123358, g_loss: 0.72109777\n",
      "Step: [4133] d_loss: 1.38939786, g_loss: 0.70622468\n",
      "Step: [4134] d_loss: 1.39363205, g_loss: 0.70997643\n",
      "Step: [4135] d_loss: 1.36781287, g_loss: 0.71706074\n",
      "Step: [4136] d_loss: 1.38598311, g_loss: 0.71164852\n",
      "Step: [4137] d_loss: 1.38236070, g_loss: 0.71141887\n",
      "Step: [4138] d_loss: 1.38393807, g_loss: 0.70569509\n",
      "Step: [4139] d_loss: 1.37752318, g_loss: 0.71041375\n",
      "Step: [4140] d_loss: 1.38455307, g_loss: 0.70940238\n",
      "Step: [4141] d_loss: 1.38752866, g_loss: 0.70925337\n",
      "Step: [4142] d_loss: 1.39241624, g_loss: 0.70429170\n",
      "Step: [4143] d_loss: 1.38438010, g_loss: 0.71286148\n",
      "Step: [4144] d_loss: 1.38064027, g_loss: 0.72028863\n",
      "Step: [4145] d_loss: 1.38633847, g_loss: 0.70823395\n",
      "Step: [4146] d_loss: 1.38115108, g_loss: 0.70607179\n",
      "Step: [4147] d_loss: 1.38566470, g_loss: 0.71801704\n",
      "Step: [4148] d_loss: 1.39339089, g_loss: 0.70788223\n",
      "Step: [4149] d_loss: 1.39762545, g_loss: 0.69537050\n",
      "Step: [4150] d_loss: 1.37954092, g_loss: 0.70923305\n",
      "Step: [4151] d_loss: 1.37911999, g_loss: 0.70303112\n",
      "Step: [4152] d_loss: 1.37744284, g_loss: 0.71016967\n",
      "Step: [4153] d_loss: 1.39034748, g_loss: 0.71189290\n",
      "Step: [4154] d_loss: 1.36940384, g_loss: 0.71786273\n",
      "Step: [4155] d_loss: 1.36986196, g_loss: 0.71809900\n",
      "Step: [4156] d_loss: 1.38198996, g_loss: 0.71421611\n",
      "Step: [4157] d_loss: 1.38034296, g_loss: 0.71434748\n",
      "Step: [4158] d_loss: 1.38175786, g_loss: 0.71405441\n",
      "Step: [4159] d_loss: 1.38626432, g_loss: 0.71041554\n",
      "Step: [4160] d_loss: 1.38365579, g_loss: 0.71091521\n",
      "Step: [4161] d_loss: 1.38903451, g_loss: 0.70936459\n",
      "Step: [4162] d_loss: 1.39475846, g_loss: 0.70378321\n",
      "Step: [4163] d_loss: 1.38458943, g_loss: 0.72026408\n",
      "Step: [4164] d_loss: 1.37579298, g_loss: 0.72117126\n",
      "Step: [4165] d_loss: 1.38472915, g_loss: 0.71884567\n",
      "Step: [4166] d_loss: 1.38122964, g_loss: 0.71606576\n",
      "Step: [4167] d_loss: 1.38491309, g_loss: 0.71042955\n",
      "Step: [4168] d_loss: 1.38301921, g_loss: 0.71484494\n",
      "Step: [4169] d_loss: 1.38641620, g_loss: 0.70987707\n",
      "Step: [4170] d_loss: 1.39294291, g_loss: 0.70882308\n",
      "Step: [4171] d_loss: 1.38160753, g_loss: 0.71651328\n",
      "Step: [4172] d_loss: 1.38606453, g_loss: 0.71136451\n",
      "Step: [4173] d_loss: 1.38370013, g_loss: 0.71042961\n",
      "Step: [4174] d_loss: 1.37777400, g_loss: 0.71034396\n",
      "Step: [4175] d_loss: 1.36531878, g_loss: 0.71558046\n",
      "Step: [4176] d_loss: 1.38970613, g_loss: 0.71148002\n",
      "Step: [4177] d_loss: 1.38110852, g_loss: 0.70982957\n",
      "Step: [4178] d_loss: 1.38969588, g_loss: 0.70846981\n",
      "Step: [4179] d_loss: 1.38193488, g_loss: 0.71325254\n",
      "Step: [4180] d_loss: 1.39095592, g_loss: 0.70982587\n",
      "Step: [4181] d_loss: 1.37623799, g_loss: 0.71416521\n",
      "Step: [4182] d_loss: 1.37909293, g_loss: 0.71246445\n",
      "Step: [4183] d_loss: 1.38255501, g_loss: 0.70838135\n",
      "Step: [4184] d_loss: 1.38176024, g_loss: 0.71281946\n",
      "Step: [4185] d_loss: 1.38416505, g_loss: 0.70628214\n",
      "Step: [4186] d_loss: 1.37910855, g_loss: 0.71392947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4187] d_loss: 1.38939786, g_loss: 0.70521486\n",
      "Step: [4188] d_loss: 1.39446783, g_loss: 0.70049739\n",
      "Step: [4189] d_loss: 1.37929797, g_loss: 0.70803249\n",
      "Step: [4190] d_loss: 1.37975001, g_loss: 0.70939839\n",
      "Step: [4191] d_loss: 1.38611090, g_loss: 0.71228468\n",
      "Step: [4192] d_loss: 1.38557351, g_loss: 0.70978987\n",
      "Step: [4193] d_loss: 1.38312221, g_loss: 0.70994908\n",
      "Step: [4194] d_loss: 1.37198496, g_loss: 0.71304721\n",
      "Step: [4195] d_loss: 1.37693644, g_loss: 0.71797371\n",
      "Step: [4196] d_loss: 1.37896872, g_loss: 0.71149373\n",
      "Step: [4197] d_loss: 1.38252211, g_loss: 0.70951718\n",
      "Step: [4198] d_loss: 1.39131081, g_loss: 0.71124953\n",
      "Step: [4199] d_loss: 1.39266872, g_loss: 0.70856184\n",
      "Step: [4200] d_loss: 1.38934469, g_loss: 0.70402759\n",
      "Step: [4201] d_loss: 1.38649392, g_loss: 0.70827049\n",
      "Step: [4202] d_loss: 1.39366210, g_loss: 0.70660204\n",
      "Step: [4203] d_loss: 1.38717747, g_loss: 0.71074164\n",
      "Step: [4204] d_loss: 1.38787401, g_loss: 0.70932460\n",
      "Step: [4205] d_loss: 1.37851572, g_loss: 0.71959358\n",
      "Step: [4206] d_loss: 1.38676095, g_loss: 0.71642292\n",
      "Step: [4207] d_loss: 1.38780165, g_loss: 0.70508349\n",
      "Step: [4208] d_loss: 1.38061547, g_loss: 0.70853156\n",
      "Step: [4209] d_loss: 1.36991549, g_loss: 0.72120982\n",
      "Step: [4210] d_loss: 1.38328230, g_loss: 0.71370995\n",
      "Step: [4211] d_loss: 1.37549400, g_loss: 0.70996642\n",
      "Step: [4212] d_loss: 1.38097143, g_loss: 0.71467805\n",
      "Step: [4213] d_loss: 1.38401306, g_loss: 0.71339136\n",
      "Step: [4214] d_loss: 1.38115394, g_loss: 0.70651472\n",
      "Step: [4215] d_loss: 1.39186704, g_loss: 0.70953834\n",
      "Step: [4216] d_loss: 1.38762844, g_loss: 0.70732838\n",
      "Step: [4217] d_loss: 1.38747466, g_loss: 0.70540267\n",
      "Step: [4218] d_loss: 1.38114226, g_loss: 0.72089326\n",
      "Step: [4219] d_loss: 1.38586974, g_loss: 0.71001601\n",
      "Step: [4220] d_loss: 1.38289070, g_loss: 0.71708059\n",
      "Step: [4221] d_loss: 1.38416767, g_loss: 0.70676816\n",
      "Step: [4222] d_loss: 1.38561726, g_loss: 0.70323604\n",
      "Step: [4223] d_loss: 1.39063489, g_loss: 0.70441508\n",
      "Step: [4224] d_loss: 1.37640429, g_loss: 0.71593624\n",
      "Step: [4225] d_loss: 1.37582040, g_loss: 0.71432692\n",
      "Step: [4226] d_loss: 1.37966204, g_loss: 0.71820098\n",
      "Step: [4227] d_loss: 1.38300931, g_loss: 0.71009159\n",
      "Step: [4228] d_loss: 1.37435174, g_loss: 0.70655209\n",
      "Step: [4229] d_loss: 1.38338828, g_loss: 0.71406519\n",
      "Step: [4230] d_loss: 1.38058901, g_loss: 0.70995295\n",
      "Step: [4231] d_loss: 1.38197410, g_loss: 0.71836823\n",
      "Step: [4232] d_loss: 1.38383985, g_loss: 0.71081996\n",
      "Step: [4233] d_loss: 1.39371061, g_loss: 0.69986409\n",
      "Step: [4234] d_loss: 1.37903285, g_loss: 0.70824915\n",
      "Step: [4235] d_loss: 1.37579560, g_loss: 0.71310127\n",
      "Step: [4236] d_loss: 1.39490461, g_loss: 0.70948505\n",
      "Step: [4237] d_loss: 1.38138604, g_loss: 0.71112895\n",
      "Step: [4238] d_loss: 1.38121545, g_loss: 0.71355933\n",
      "Step: [4239] d_loss: 1.38795364, g_loss: 0.70843768\n",
      "Step: [4240] d_loss: 1.40795016, g_loss: 0.70839381\n",
      "Step: [4241] d_loss: 1.38616824, g_loss: 0.71175289\n",
      "Step: [4242] d_loss: 1.39181232, g_loss: 0.71021783\n",
      "Step: [4243] d_loss: 1.38698971, g_loss: 0.70554429\n",
      "Step: [4244] d_loss: 1.38479400, g_loss: 0.70822585\n",
      "Step: [4245] d_loss: 1.38667130, g_loss: 0.71195066\n",
      "Step: [4246] d_loss: 1.38468671, g_loss: 0.70734012\n",
      "Step: [4247] d_loss: 1.37935984, g_loss: 0.71777922\n",
      "Step: [4248] d_loss: 1.38867235, g_loss: 0.71734869\n",
      "Step: [4249] d_loss: 1.38777626, g_loss: 0.71251762\n",
      "Step: [4250] d_loss: 1.37245739, g_loss: 0.71440691\n",
      "Step: [4251] d_loss: 1.38129520, g_loss: 0.71194190\n",
      "Step: [4252] d_loss: 1.37300158, g_loss: 0.71578622\n",
      "Step: [4253] d_loss: 1.37467813, g_loss: 0.71541083\n",
      "Step: [4254] d_loss: 1.39334345, g_loss: 0.70241117\n",
      "Step: [4255] d_loss: 1.39077902, g_loss: 0.70760471\n",
      "Step: [4256] d_loss: 1.38776159, g_loss: 0.71043187\n",
      "Step: [4257] d_loss: 1.39185488, g_loss: 0.71102220\n",
      "Step: [4258] d_loss: 1.40341091, g_loss: 0.69845772\n",
      "Step: [4259] d_loss: 1.37967718, g_loss: 0.70876694\n",
      "Step: [4260] d_loss: 1.38916755, g_loss: 0.70937908\n",
      "Step: [4261] d_loss: 1.38358092, g_loss: 0.72046208\n",
      "Step: [4262] d_loss: 1.39221847, g_loss: 0.70623291\n",
      "Step: [4263] d_loss: 1.39403355, g_loss: 0.70717818\n",
      "Step: [4264] d_loss: 1.38992107, g_loss: 0.70461273\n",
      "Step: [4265] d_loss: 1.38942266, g_loss: 0.71067256\n",
      "Step: [4266] d_loss: 1.38056839, g_loss: 0.72249109\n",
      "Step: [4267] d_loss: 1.38994110, g_loss: 0.71215111\n",
      "Step: [4268] d_loss: 1.38049901, g_loss: 0.70414579\n",
      "Step: [4269] d_loss: 1.37109852, g_loss: 0.71420658\n",
      "Step: [4270] d_loss: 1.39731061, g_loss: 0.70816654\n",
      "Step: [4271] d_loss: 1.37428951, g_loss: 0.71436489\n",
      "Step: [4272] d_loss: 1.39359558, g_loss: 0.70313466\n",
      "Step: [4273] d_loss: 1.38408506, g_loss: 0.71168232\n",
      "Step: [4274] d_loss: 1.38391304, g_loss: 0.70673233\n",
      "Step: [4275] d_loss: 1.39199376, g_loss: 0.71085203\n",
      "Step: [4276] d_loss: 1.38159215, g_loss: 0.71505165\n",
      "Step: [4277] d_loss: 1.38540578, g_loss: 0.71883792\n",
      "Step: [4278] d_loss: 1.38684893, g_loss: 0.71339047\n",
      "Step: [4279] d_loss: 1.38328290, g_loss: 0.70867538\n",
      "Step: [4280] d_loss: 1.39383554, g_loss: 0.69735581\n",
      "Step: [4281] d_loss: 1.38398480, g_loss: 0.70589966\n",
      "Step: [4282] d_loss: 1.37389314, g_loss: 0.71870708\n",
      "Step: [4283] d_loss: 1.38319254, g_loss: 0.71538979\n",
      "Step: [4284] d_loss: 1.39260697, g_loss: 0.70782757\n",
      "Step: [4285] d_loss: 1.38142610, g_loss: 0.71249902\n",
      "Step: [4286] d_loss: 1.38971257, g_loss: 0.70729887\n",
      "Step: [4287] d_loss: 1.38518047, g_loss: 0.71529973\n",
      "Step: [4288] d_loss: 1.38054562, g_loss: 0.71787727\n",
      "Step: [4289] d_loss: 1.37525010, g_loss: 0.71655691\n",
      "Step: [4290] d_loss: 1.38391197, g_loss: 0.71118307\n",
      "Step: [4291] d_loss: 1.39012241, g_loss: 0.70370191\n",
      "Step: [4292] d_loss: 1.37727916, g_loss: 0.71012634\n",
      "Step: [4293] d_loss: 1.38137817, g_loss: 0.70793855\n",
      "Step: [4294] d_loss: 1.38249958, g_loss: 0.71462470\n",
      "Step: [4295] d_loss: 1.36835861, g_loss: 0.71949291\n",
      "Step: [4296] d_loss: 1.39361668, g_loss: 0.70769405\n",
      "Step: [4297] d_loss: 1.37923717, g_loss: 0.71093416\n",
      "Step: [4298] d_loss: 1.37776327, g_loss: 0.70756567\n",
      "Step: [4299] d_loss: 1.38707447, g_loss: 0.70456660\n",
      "Step: [4300] d_loss: 1.37824345, g_loss: 0.71703839\n",
      "Step: [4301] d_loss: 1.37262464, g_loss: 0.71975660\n",
      "Step: [4302] d_loss: 1.38370395, g_loss: 0.70923662\n",
      "Step: [4303] d_loss: 1.37797403, g_loss: 0.71820527\n",
      "Step: [4304] d_loss: 1.37232542, g_loss: 0.71779436\n",
      "Step: [4305] d_loss: 1.38495874, g_loss: 0.70203328\n",
      "Step: [4306] d_loss: 1.38852000, g_loss: 0.70354402\n",
      "Step: [4307] d_loss: 1.38441741, g_loss: 0.70949763\n",
      "Step: [4308] d_loss: 1.39144266, g_loss: 0.71032554\n",
      "Step: [4309] d_loss: 1.38835263, g_loss: 0.71618462\n",
      "Step: [4310] d_loss: 1.38871431, g_loss: 0.70082664\n",
      "Step: [4311] d_loss: 1.38647032, g_loss: 0.71503294\n",
      "Step: [4312] d_loss: 1.39138293, g_loss: 0.70067608\n",
      "Step: [4313] d_loss: 1.37609410, g_loss: 0.70933580\n",
      "Step: [4314] d_loss: 1.38965547, g_loss: 0.70503855\n",
      "Step: [4315] d_loss: 1.38200867, g_loss: 0.71493840\n",
      "Step: [4316] d_loss: 1.37820399, g_loss: 0.71587408\n",
      "Step: [4317] d_loss: 1.37724185, g_loss: 0.71002328\n",
      "Step: [4318] d_loss: 1.36952031, g_loss: 0.72048056\n",
      "Step: [4319] d_loss: 1.36966312, g_loss: 0.71810192\n",
      "Step: [4320] d_loss: 1.39274120, g_loss: 0.70536321\n",
      "Step: [4321] d_loss: 1.38572383, g_loss: 0.71177483\n",
      "Step: [4322] d_loss: 1.38263679, g_loss: 0.71389192\n",
      "Step: [4323] d_loss: 1.37383413, g_loss: 0.71410787\n",
      "Step: [4324] d_loss: 1.37051368, g_loss: 0.71864438\n",
      "Step: [4325] d_loss: 1.38746274, g_loss: 0.71579248\n",
      "Step: [4326] d_loss: 1.38153958, g_loss: 0.71344835\n",
      "Step: [4327] d_loss: 1.39517593, g_loss: 0.70839190\n",
      "Step: [4328] d_loss: 1.38006902, g_loss: 0.71434808\n",
      "Step: [4329] d_loss: 1.38022041, g_loss: 0.71429223\n",
      "Step: [4330] d_loss: 1.38288224, g_loss: 0.71375692\n",
      "Step: [4331] d_loss: 1.39158750, g_loss: 0.70377433\n",
      "Step: [4332] d_loss: 1.37914729, g_loss: 0.70880842\n",
      "Step: [4333] d_loss: 1.38352680, g_loss: 0.71591246\n",
      "Step: [4334] d_loss: 1.38370204, g_loss: 0.70461679\n",
      "Step: [4335] d_loss: 1.37933266, g_loss: 0.71780741\n",
      "Step: [4336] d_loss: 1.39632750, g_loss: 0.70626599\n",
      "Step: [4337] d_loss: 1.38107455, g_loss: 0.71049440\n",
      "Step: [4338] d_loss: 1.38284743, g_loss: 0.71591133\n",
      "Step: [4339] d_loss: 1.37804699, g_loss: 0.71274447\n",
      "Step: [4340] d_loss: 1.39200413, g_loss: 0.70313752\n",
      "Step: [4341] d_loss: 1.38508606, g_loss: 0.71031249\n",
      "Step: [4342] d_loss: 1.38262153, g_loss: 0.71637273\n",
      "Step: [4343] d_loss: 1.38447750, g_loss: 0.71077979\n",
      "Step: [4344] d_loss: 1.38214374, g_loss: 0.71033847\n",
      "Step: [4345] d_loss: 1.38230753, g_loss: 0.70866632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4346] d_loss: 1.38030660, g_loss: 0.71306098\n",
      "Step: [4347] d_loss: 1.37919688, g_loss: 0.71592891\n",
      "Step: [4348] d_loss: 1.38042545, g_loss: 0.71327144\n",
      "Step: [4349] d_loss: 1.37380421, g_loss: 0.71621156\n",
      "Step: [4350] d_loss: 1.38910329, g_loss: 0.70925474\n",
      "Step: [4351] d_loss: 1.39329004, g_loss: 0.71461260\n",
      "Step: [4352] d_loss: 1.39643574, g_loss: 0.70723438\n",
      "Step: [4353] d_loss: 1.39129269, g_loss: 0.70593524\n",
      "Step: [4354] d_loss: 1.39105058, g_loss: 0.71216387\n",
      "Step: [4355] d_loss: 1.38013434, g_loss: 0.71987402\n",
      "Step: [4356] d_loss: 1.38453507, g_loss: 0.71868348\n",
      "Step: [4357] d_loss: 1.38635182, g_loss: 0.71195173\n",
      "Step: [4358] d_loss: 1.38824892, g_loss: 0.70793802\n",
      "Step: [4359] d_loss: 1.38905787, g_loss: 0.70981681\n",
      "Step: [4360] d_loss: 1.38464224, g_loss: 0.71185690\n",
      "Step: [4361] d_loss: 1.37390041, g_loss: 0.72092652\n",
      "Step: [4362] d_loss: 1.38412976, g_loss: 0.71218812\n",
      "Step: [4363] d_loss: 1.37664855, g_loss: 0.71588349\n",
      "Step: [4364] d_loss: 1.37162054, g_loss: 0.71510643\n",
      "Step: [4365] d_loss: 1.39312935, g_loss: 0.71079743\n",
      "Step: [4366] d_loss: 1.39078546, g_loss: 0.70422673\n",
      "Step: [4367] d_loss: 1.37616920, g_loss: 0.71851885\n",
      "Step: [4368] d_loss: 1.38414979, g_loss: 0.70546198\n",
      "Step: [4369] d_loss: 1.38238132, g_loss: 0.71097732\n",
      "Step: [4370] d_loss: 1.37590265, g_loss: 0.71277046\n",
      "Step: [4371] d_loss: 1.37933028, g_loss: 0.72075725\n",
      "Step: [4372] d_loss: 1.37977958, g_loss: 0.71377897\n",
      "Step: [4373] d_loss: 1.38485122, g_loss: 0.71588254\n",
      "Step: [4374] d_loss: 1.37954974, g_loss: 0.71551788\n",
      "Step: [4375] d_loss: 1.38094163, g_loss: 0.71676648\n",
      "Step: [4376] d_loss: 1.38486528, g_loss: 0.71327889\n",
      "Step: [4377] d_loss: 1.39497149, g_loss: 0.71361774\n",
      "Step: [4378] d_loss: 1.39287972, g_loss: 0.71079612\n",
      "Step: [4379] d_loss: 1.38746166, g_loss: 0.71538436\n",
      "Step: [4380] d_loss: 1.37918139, g_loss: 0.71140629\n",
      "Step: [4381] d_loss: 1.37882805, g_loss: 0.71600085\n",
      "Step: [4382] d_loss: 1.36921227, g_loss: 0.71736062\n",
      "Step: [4383] d_loss: 1.38203263, g_loss: 0.70993453\n",
      "Step: [4384] d_loss: 1.37956035, g_loss: 0.70635843\n",
      "Step: [4385] d_loss: 1.38179517, g_loss: 0.71195936\n",
      "Step: [4386] d_loss: 1.37765956, g_loss: 0.71222800\n",
      "Step: [4387] d_loss: 1.38331413, g_loss: 0.71429229\n",
      "Step: [4388] d_loss: 1.38029349, g_loss: 0.71301216\n",
      "Step: [4389] d_loss: 1.38424969, g_loss: 0.71248138\n",
      "Step: [4390] d_loss: 1.37257123, g_loss: 0.70781887\n",
      "Step: [4391] d_loss: 1.39309490, g_loss: 0.71413004\n",
      "Step: [4392] d_loss: 1.38901424, g_loss: 0.71489620\n",
      "Step: [4393] d_loss: 1.40515780, g_loss: 0.71704769\n",
      "Step: [4394] d_loss: 1.38499725, g_loss: 0.71939147\n",
      "Step: [4395] d_loss: 1.39288568, g_loss: 0.70319307\n",
      "Step: [4396] d_loss: 1.39559674, g_loss: 0.70848703\n",
      "Step: [4397] d_loss: 1.38321519, g_loss: 0.70502275\n",
      "Step: [4398] d_loss: 1.38066077, g_loss: 0.71527028\n",
      "Step: [4399] d_loss: 1.37772799, g_loss: 0.71647608\n",
      "Step: [4400] d_loss: 1.38465083, g_loss: 0.71342039\n",
      "Step: [4401] d_loss: 1.37681293, g_loss: 0.71408141\n",
      "Step: [4402] d_loss: 1.38880301, g_loss: 0.71748495\n",
      "Step: [4403] d_loss: 1.36718678, g_loss: 0.72116876\n",
      "Step: [4404] d_loss: 1.38902473, g_loss: 0.71489787\n",
      "Step: [4405] d_loss: 1.38298357, g_loss: 0.71501529\n",
      "Step: [4406] d_loss: 1.39334428, g_loss: 0.70847034\n",
      "Step: [4407] d_loss: 1.38357973, g_loss: 0.70482874\n",
      "Step: [4408] d_loss: 1.38082612, g_loss: 0.70261550\n",
      "Step: [4409] d_loss: 1.39074302, g_loss: 0.71813655\n",
      "Step: [4410] d_loss: 1.39772940, g_loss: 0.71252763\n",
      "Step: [4411] d_loss: 1.38763344, g_loss: 0.71195704\n",
      "Step: [4412] d_loss: 1.38464630, g_loss: 0.71158463\n",
      "Step: [4413] d_loss: 1.38292432, g_loss: 0.70494783\n",
      "Step: [4414] d_loss: 1.38555765, g_loss: 0.70915043\n",
      "Step: [4415] d_loss: 1.39697480, g_loss: 0.70510298\n",
      "Step: [4416] d_loss: 1.37893343, g_loss: 0.71168256\n",
      "Step: [4417] d_loss: 1.37544537, g_loss: 0.71364176\n",
      "Step: [4418] d_loss: 1.37482715, g_loss: 0.71596932\n",
      "Step: [4419] d_loss: 1.39428282, g_loss: 0.70427704\n",
      "Step: [4420] d_loss: 1.38623810, g_loss: 0.70657468\n",
      "Step: [4421] d_loss: 1.38521338, g_loss: 0.70454037\n",
      "Step: [4422] d_loss: 1.38374567, g_loss: 0.70587456\n",
      "Step: [4423] d_loss: 1.37950993, g_loss: 0.71801424\n",
      "Step: [4424] d_loss: 1.38050056, g_loss: 0.71302223\n",
      "Step: [4425] d_loss: 1.38480139, g_loss: 0.71153855\n",
      "Step: [4426] d_loss: 1.38112175, g_loss: 0.70997632\n",
      "Step: [4427] d_loss: 1.38784075, g_loss: 0.71085662\n",
      "Step: [4428] d_loss: 1.37206519, g_loss: 0.72259068\n",
      "Step: [4429] d_loss: 1.38239598, g_loss: 0.71718776\n",
      "Step: [4430] d_loss: 1.38967133, g_loss: 0.71519572\n",
      "Step: [4431] d_loss: 1.38067794, g_loss: 0.70841134\n",
      "Step: [4432] d_loss: 1.38189769, g_loss: 0.72389781\n",
      "Step: [4433] d_loss: 1.39474797, g_loss: 0.70738846\n",
      "Step: [4434] d_loss: 1.38284898, g_loss: 0.71721929\n",
      "Step: [4435] d_loss: 1.39078784, g_loss: 0.71240795\n",
      "Step: [4436] d_loss: 1.38542938, g_loss: 0.72176182\n",
      "Step: [4437] d_loss: 1.38984108, g_loss: 0.71864653\n",
      "Step: [4438] d_loss: 1.39508843, g_loss: 0.71206832\n",
      "Step: [4439] d_loss: 1.38457274, g_loss: 0.71196997\n",
      "Step: [4440] d_loss: 1.38992739, g_loss: 0.70193255\n",
      "Step: [4441] d_loss: 1.40022314, g_loss: 0.70394111\n",
      "Step: [4442] d_loss: 1.39052463, g_loss: 0.70982146\n",
      "Step: [4443] d_loss: 1.39663446, g_loss: 0.70709229\n",
      "Step: [4444] d_loss: 1.38589907, g_loss: 0.71421683\n",
      "Step: [4445] d_loss: 1.38136911, g_loss: 0.71926969\n",
      "Step: [4446] d_loss: 1.37754703, g_loss: 0.70693719\n",
      "Step: [4447] d_loss: 1.38750911, g_loss: 0.70459747\n",
      "Step: [4448] d_loss: 1.38689923, g_loss: 0.71675932\n",
      "Step: [4449] d_loss: 1.38031197, g_loss: 0.70900595\n",
      "Step: [4450] d_loss: 1.37524915, g_loss: 0.71246791\n",
      "Step: [4451] d_loss: 1.38873029, g_loss: 0.71161890\n",
      "Step: [4452] d_loss: 1.38100433, g_loss: 0.70917547\n",
      "Step: [4453] d_loss: 1.38545561, g_loss: 0.70859993\n",
      "Step: [4454] d_loss: 1.37732148, g_loss: 0.71502531\n",
      "Step: [4455] d_loss: 1.38486159, g_loss: 0.70986521\n",
      "Step: [4456] d_loss: 1.38372827, g_loss: 0.70729733\n",
      "Step: [4457] d_loss: 1.37880802, g_loss: 0.70942092\n",
      "Step: [4458] d_loss: 1.38233137, g_loss: 0.70968521\n",
      "Step: [4459] d_loss: 1.39378500, g_loss: 0.70477748\n",
      "Step: [4460] d_loss: 1.37995172, g_loss: 0.70540613\n",
      "Step: [4461] d_loss: 1.38656282, g_loss: 0.70655769\n",
      "Step: [4462] d_loss: 1.38054967, g_loss: 0.70948261\n",
      "Step: [4463] d_loss: 1.37495244, g_loss: 0.71653783\n",
      "Step: [4464] d_loss: 1.38578367, g_loss: 0.70900565\n",
      "Step: [4465] d_loss: 1.38599885, g_loss: 0.71180785\n",
      "Step: [4466] d_loss: 1.38879573, g_loss: 0.69994855\n",
      "Step: [4467] d_loss: 1.38055634, g_loss: 0.71047765\n",
      "Step: [4468] d_loss: 1.38676989, g_loss: 0.70807248\n",
      "Step: [4469] d_loss: 1.37206888, g_loss: 0.71731722\n",
      "Step: [4470] d_loss: 1.38114953, g_loss: 0.71390998\n",
      "Step: [4471] d_loss: 1.38270497, g_loss: 0.71978581\n",
      "Step: [4472] d_loss: 1.39521432, g_loss: 0.70623887\n",
      "Step: [4473] d_loss: 1.38072157, g_loss: 0.71223909\n",
      "Step: [4474] d_loss: 1.38696933, g_loss: 0.71295947\n",
      "Step: [4475] d_loss: 1.38991332, g_loss: 0.71116304\n",
      "Step: [4476] d_loss: 1.39561951, g_loss: 0.70749307\n",
      "Step: [4477] d_loss: 1.38033235, g_loss: 0.71398008\n",
      "Step: [4478] d_loss: 1.38404191, g_loss: 0.70271564\n",
      "Step: [4479] d_loss: 1.36427140, g_loss: 0.71617544\n",
      "Step: [4480] d_loss: 1.38087904, g_loss: 0.71552628\n",
      "Step: [4481] d_loss: 1.38522482, g_loss: 0.70671368\n",
      "Step: [4482] d_loss: 1.39008713, g_loss: 0.70715296\n",
      "Step: [4483] d_loss: 1.38394427, g_loss: 0.70712596\n",
      "Step: [4484] d_loss: 1.39925718, g_loss: 0.70319545\n",
      "Step: [4485] d_loss: 1.39097965, g_loss: 0.70871639\n",
      "Step: [4486] d_loss: 1.39767277, g_loss: 0.69969922\n",
      "Step: [4487] d_loss: 1.40192735, g_loss: 0.70603573\n",
      "Step: [4488] d_loss: 1.39717591, g_loss: 0.71305668\n",
      "Step: [4489] d_loss: 1.39100206, g_loss: 0.70357746\n",
      "Step: [4490] d_loss: 1.39912677, g_loss: 0.70399570\n",
      "Step: [4491] d_loss: 1.39210570, g_loss: 0.70456654\n",
      "Step: [4492] d_loss: 1.38403440, g_loss: 0.70406908\n",
      "Step: [4493] d_loss: 1.39777565, g_loss: 0.70424348\n",
      "Step: [4494] d_loss: 1.39054275, g_loss: 0.70765269\n",
      "Step: [4495] d_loss: 1.38518929, g_loss: 0.70622867\n",
      "Step: [4496] d_loss: 1.39472294, g_loss: 0.69935358\n",
      "Step: [4497] d_loss: 1.38865519, g_loss: 0.71020198\n",
      "Step: [4498] d_loss: 1.39304924, g_loss: 0.70827401\n",
      "Step: [4499] d_loss: 1.39304519, g_loss: 0.70037258\n",
      "Step: [4500] d_loss: 1.38843322, g_loss: 0.71424031\n",
      "Step: [4501] d_loss: 1.39494824, g_loss: 0.70670360\n",
      "Step: [4502] d_loss: 1.38730502, g_loss: 0.70968199\n",
      "Step: [4503] d_loss: 1.39010954, g_loss: 0.70599973\n",
      "Step: [4504] d_loss: 1.37901533, g_loss: 0.70946020\n",
      "Step: [4505] d_loss: 1.37765169, g_loss: 0.71765035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4506] d_loss: 1.37893653, g_loss: 0.70963383\n",
      "Step: [4507] d_loss: 1.38003135, g_loss: 0.71718585\n",
      "Step: [4508] d_loss: 1.38349092, g_loss: 0.71330482\n",
      "Step: [4509] d_loss: 1.38077486, g_loss: 0.71376681\n",
      "Step: [4510] d_loss: 1.37098956, g_loss: 0.71570420\n",
      "Step: [4511] d_loss: 1.38159895, g_loss: 0.71093106\n",
      "Step: [4512] d_loss: 1.37903428, g_loss: 0.71658301\n",
      "Step: [4513] d_loss: 1.38524783, g_loss: 0.71364683\n",
      "Step: [4514] d_loss: 1.39598942, g_loss: 0.70628142\n",
      "Step: [4515] d_loss: 1.38771510, g_loss: 0.71176410\n",
      "Step: [4516] d_loss: 1.38655210, g_loss: 0.70645720\n",
      "Step: [4517] d_loss: 1.37777758, g_loss: 0.71656179\n",
      "Step: [4518] d_loss: 1.38952827, g_loss: 0.71720213\n",
      "Step: [4519] d_loss: 1.37730217, g_loss: 0.71700597\n",
      "Step: [4520] d_loss: 1.38210332, g_loss: 0.71479243\n",
      "Step: [4521] d_loss: 1.38274992, g_loss: 0.70892262\n",
      "Step: [4522] d_loss: 1.39633715, g_loss: 0.70933080\n",
      "Step: [4523] d_loss: 1.38340282, g_loss: 0.70982671\n",
      "Step: [4524] d_loss: 1.39316559, g_loss: 0.69799924\n",
      "Step: [4525] d_loss: 1.39292860, g_loss: 0.70848852\n",
      "Step: [4526] d_loss: 1.38065994, g_loss: 0.71039057\n",
      "Step: [4527] d_loss: 1.37096858, g_loss: 0.71460998\n",
      "Step: [4528] d_loss: 1.37508643, g_loss: 0.71579796\n",
      "Step: [4529] d_loss: 1.36942744, g_loss: 0.71458524\n",
      "Step: [4530] d_loss: 1.37134945, g_loss: 0.72115767\n",
      "Step: [4531] d_loss: 1.37555242, g_loss: 0.70976543\n",
      "Step: [4532] d_loss: 1.38235319, g_loss: 0.71617013\n",
      "Step: [4533] d_loss: 1.39249396, g_loss: 0.70254654\n",
      "Step: [4534] d_loss: 1.37305832, g_loss: 0.71084189\n",
      "Step: [4535] d_loss: 1.39193845, g_loss: 0.70836246\n",
      "Step: [4536] d_loss: 1.39588201, g_loss: 0.70724773\n",
      "Step: [4537] d_loss: 1.38946342, g_loss: 0.70745271\n",
      "Step: [4538] d_loss: 1.38451183, g_loss: 0.70584333\n",
      "Step: [4539] d_loss: 1.38415360, g_loss: 0.70838201\n",
      "Step: [4540] d_loss: 1.38027787, g_loss: 0.71221977\n",
      "Step: [4541] d_loss: 1.39735866, g_loss: 0.69861877\n",
      "Step: [4542] d_loss: 1.39700246, g_loss: 0.70828217\n",
      "Step: [4543] d_loss: 1.39581275, g_loss: 0.69655502\n",
      "Step: [4544] d_loss: 1.38676238, g_loss: 0.70459229\n",
      "Step: [4545] d_loss: 1.39679956, g_loss: 0.70140016\n",
      "Step: [4546] d_loss: 1.38995731, g_loss: 0.70430893\n",
      "Step: [4547] d_loss: 1.37556815, g_loss: 0.71037728\n",
      "Step: [4548] d_loss: 1.38281453, g_loss: 0.71341705\n",
      "Step: [4549] d_loss: 1.38525975, g_loss: 0.70976824\n",
      "Step: [4550] d_loss: 1.38580155, g_loss: 0.70832622\n",
      "Step: [4551] d_loss: 1.38863277, g_loss: 0.71860003\n",
      "Step: [4552] d_loss: 1.40973043, g_loss: 0.71958077\n",
      "Step: [4553] d_loss: 1.41651714, g_loss: 0.72233188\n",
      "Step: [4554] d_loss: 1.40421033, g_loss: 0.71182919\n",
      "Step: [4555] d_loss: 1.39514172, g_loss: 0.70400351\n",
      "Step: [4556] d_loss: 1.39486122, g_loss: 0.70926487\n",
      "Step: [4557] d_loss: 1.38260758, g_loss: 0.70620203\n",
      "Step: [4558] d_loss: 1.38331962, g_loss: 0.71355867\n",
      "Step: [4559] d_loss: 1.37725461, g_loss: 0.70727336\n",
      "Step: [4560] d_loss: 1.37897682, g_loss: 0.71485883\n",
      "Step: [4561] d_loss: 1.39216304, g_loss: 0.70638913\n",
      "Step: [4562] d_loss: 1.37980926, g_loss: 0.70966029\n",
      "Step: [4563] d_loss: 1.38651896, g_loss: 0.70162761\n",
      "Step: [4564] d_loss: 1.38380742, g_loss: 0.70843792\n",
      "Step: [4565] d_loss: 1.38150764, g_loss: 0.70962578\n",
      "Step: [4566] d_loss: 1.38796043, g_loss: 0.70805222\n",
      "Step: [4567] d_loss: 1.38478327, g_loss: 0.70761430\n",
      "Step: [4568] d_loss: 1.40088809, g_loss: 0.69950354\n",
      "Step: [4569] d_loss: 1.38421440, g_loss: 0.70298320\n",
      "Step: [4570] d_loss: 1.38639879, g_loss: 0.70954603\n",
      "Step: [4571] d_loss: 1.37646222, g_loss: 0.71165591\n",
      "Step: [4572] d_loss: 1.39008462, g_loss: 0.70155817\n",
      "Step: [4573] d_loss: 1.38547778, g_loss: 0.71334231\n",
      "Step: [4574] d_loss: 1.38554490, g_loss: 0.71380699\n",
      "Step: [4575] d_loss: 1.37349486, g_loss: 0.71318090\n",
      "Step: [4576] d_loss: 1.38862050, g_loss: 0.70619559\n",
      "Step: [4577] d_loss: 1.39418495, g_loss: 0.70104098\n",
      "Step: [4578] d_loss: 1.38923466, g_loss: 0.70397210\n",
      "Step: [4579] d_loss: 1.38800192, g_loss: 0.70423698\n",
      "Step: [4580] d_loss: 1.37327492, g_loss: 0.71526682\n",
      "Step: [4581] d_loss: 1.38961697, g_loss: 0.71014196\n",
      "Step: [4582] d_loss: 1.38211703, g_loss: 0.70726883\n",
      "Step: [4583] d_loss: 1.38645697, g_loss: 0.70744610\n",
      "Step: [4584] d_loss: 1.39013064, g_loss: 0.70940793\n",
      "Step: [4585] d_loss: 1.37965965, g_loss: 0.71003515\n",
      "Step: [4586] d_loss: 1.38984263, g_loss: 0.71074474\n",
      "Step: [4587] d_loss: 1.38158989, g_loss: 0.71381235\n",
      "Step: [4588] d_loss: 1.38764334, g_loss: 0.70984483\n",
      "Step: [4589] d_loss: 1.38267708, g_loss: 0.71164578\n",
      "Step: [4590] d_loss: 1.39640474, g_loss: 0.70258719\n",
      "Step: [4591] d_loss: 1.39178133, g_loss: 0.70457876\n",
      "Step: [4592] d_loss: 1.38497186, g_loss: 0.71057808\n",
      "Step: [4593] d_loss: 1.39659250, g_loss: 0.70118511\n",
      "Step: [4594] d_loss: 1.39217710, g_loss: 0.70607412\n",
      "Step: [4595] d_loss: 1.37842333, g_loss: 0.71690524\n",
      "Step: [4596] d_loss: 1.39140201, g_loss: 0.71570641\n",
      "Step: [4597] d_loss: 1.38819122, g_loss: 0.71272153\n",
      "Step: [4598] d_loss: 1.38951457, g_loss: 0.70899296\n",
      "Step: [4599] d_loss: 1.39319515, g_loss: 0.69971919\n",
      "Step: [4600] d_loss: 1.38233757, g_loss: 0.71001595\n",
      "Step: [4601] d_loss: 1.38463068, g_loss: 0.70499575\n",
      "Step: [4602] d_loss: 1.39260960, g_loss: 0.70604622\n",
      "Step: [4603] d_loss: 1.38228273, g_loss: 0.71256793\n",
      "Step: [4604] d_loss: 1.38629854, g_loss: 0.70991337\n",
      "Step: [4605] d_loss: 1.36793876, g_loss: 0.72418785\n",
      "Step: [4606] d_loss: 1.38495755, g_loss: 0.71429133\n",
      "Step: [4607] d_loss: 1.37984860, g_loss: 0.70173562\n",
      "Step: [4608] d_loss: 1.37273276, g_loss: 0.71569639\n",
      "Step: [4609] d_loss: 1.38373256, g_loss: 0.70409596\n",
      "Step: [4610] d_loss: 1.38670933, g_loss: 0.70010185\n",
      "Step: [4611] d_loss: 1.38822126, g_loss: 0.70437533\n",
      "Step: [4612] d_loss: 1.38417864, g_loss: 0.71352851\n",
      "Step: [4613] d_loss: 1.38196993, g_loss: 0.71560514\n",
      "Step: [4614] d_loss: 1.38717651, g_loss: 0.70729601\n",
      "Step: [4615] d_loss: 1.38086510, g_loss: 0.70680618\n",
      "Step: [4616] d_loss: 1.37988830, g_loss: 0.70933336\n",
      "Step: [4617] d_loss: 1.40119624, g_loss: 0.69971752\n",
      "Step: [4618] d_loss: 1.38905013, g_loss: 0.70326126\n",
      "Step: [4619] d_loss: 1.38134742, g_loss: 0.70618808\n",
      "Step: [4620] d_loss: 1.38626766, g_loss: 0.70695984\n",
      "Step: [4621] d_loss: 1.39795566, g_loss: 0.69321841\n",
      "Step: [4622] d_loss: 1.39365649, g_loss: 0.70664954\n",
      "Step: [4623] d_loss: 1.39441752, g_loss: 0.70578361\n",
      "Step: [4624] d_loss: 1.38615656, g_loss: 0.70817554\n",
      "Step: [4625] d_loss: 1.39045739, g_loss: 0.70468402\n",
      "Step: [4626] d_loss: 1.39524484, g_loss: 0.71280813\n",
      "Step: [4627] d_loss: 1.39127445, g_loss: 0.70669615\n",
      "Step: [4628] d_loss: 1.39042020, g_loss: 0.70103359\n",
      "Step: [4629] d_loss: 1.39148891, g_loss: 0.70396346\n",
      "Step: [4630] d_loss: 1.38386965, g_loss: 0.70859814\n",
      "Step: [4631] d_loss: 1.38384950, g_loss: 0.70609665\n",
      "Step: [4632] d_loss: 1.37762499, g_loss: 0.71514273\n",
      "Step: [4633] d_loss: 1.37787473, g_loss: 0.70479035\n",
      "Step: [4634] d_loss: 1.37650955, g_loss: 0.71036494\n",
      "Step: [4635] d_loss: 1.38256657, g_loss: 0.70516676\n",
      "Step: [4636] d_loss: 1.38198757, g_loss: 0.70912701\n",
      "Step: [4637] d_loss: 1.38276589, g_loss: 0.70808613\n",
      "Step: [4638] d_loss: 1.37336659, g_loss: 0.70817542\n",
      "Step: [4639] d_loss: 1.39325106, g_loss: 0.70340300\n",
      "Step: [4640] d_loss: 1.38667846, g_loss: 0.70466232\n",
      "Step: [4641] d_loss: 1.36806273, g_loss: 0.71976006\n",
      "Step: [4642] d_loss: 1.39727283, g_loss: 0.69859695\n",
      "Step: [4643] d_loss: 1.39034605, g_loss: 0.70383531\n",
      "Step: [4644] d_loss: 1.38570416, g_loss: 0.70702183\n",
      "Step: [4645] d_loss: 1.37991250, g_loss: 0.70869279\n",
      "Step: [4646] d_loss: 1.37595773, g_loss: 0.71232724\n",
      "Step: [4647] d_loss: 1.39069319, g_loss: 0.70282722\n",
      "Step: [4648] d_loss: 1.39627624, g_loss: 0.69801354\n",
      "Step: [4649] d_loss: 1.38969135, g_loss: 0.70220053\n",
      "Step: [4650] d_loss: 1.37975574, g_loss: 0.71057045\n",
      "Step: [4651] d_loss: 1.38512766, g_loss: 0.70787108\n",
      "Step: [4652] d_loss: 1.38514757, g_loss: 0.71133041\n",
      "Step: [4653] d_loss: 1.37770820, g_loss: 0.70757151\n",
      "Step: [4654] d_loss: 1.39290476, g_loss: 0.70362830\n",
      "Step: [4655] d_loss: 1.38257623, g_loss: 0.70717216\n",
      "Step: [4656] d_loss: 1.38013399, g_loss: 0.70450664\n",
      "Step: [4657] d_loss: 1.38158655, g_loss: 0.70376897\n",
      "Step: [4658] d_loss: 1.37190485, g_loss: 0.72489530\n",
      "Step: [4659] d_loss: 1.37707531, g_loss: 0.71113008\n",
      "Step: [4660] d_loss: 1.38430357, g_loss: 0.71111280\n",
      "Step: [4661] d_loss: 1.38354373, g_loss: 0.69994032\n",
      "Step: [4662] d_loss: 1.39067817, g_loss: 0.70416123\n",
      "Step: [4663] d_loss: 1.38939106, g_loss: 0.70723182\n",
      "Step: [4664] d_loss: 1.38614166, g_loss: 0.71119332\n",
      "Step: [4665] d_loss: 1.38920212, g_loss: 0.71402299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4666] d_loss: 1.39748573, g_loss: 0.69904912\n",
      "Step: [4667] d_loss: 1.39499974, g_loss: 0.70081031\n",
      "Step: [4668] d_loss: 1.39648485, g_loss: 0.70580459\n",
      "Step: [4669] d_loss: 1.38852596, g_loss: 0.70178318\n",
      "Step: [4670] d_loss: 1.38688612, g_loss: 0.70559192\n",
      "Step: [4671] d_loss: 1.38909483, g_loss: 0.70332301\n",
      "Step: [4672] d_loss: 1.38475752, g_loss: 0.70292509\n",
      "Step: [4673] d_loss: 1.39061213, g_loss: 0.70114887\n",
      "Step: [4674] d_loss: 1.39058900, g_loss: 0.70940095\n",
      "Step: [4675] d_loss: 1.38445210, g_loss: 0.70565283\n",
      "Step: [4676] d_loss: 1.37883043, g_loss: 0.70804489\n",
      "Step: [4677] d_loss: 1.37799776, g_loss: 0.70853949\n",
      "Step: [4678] d_loss: 1.38497138, g_loss: 0.70718485\n",
      "Step: [4679] d_loss: 1.38426709, g_loss: 0.70242196\n",
      "Step: [4680] d_loss: 1.38866174, g_loss: 0.70182711\n",
      "Step: [4681] d_loss: 1.38477349, g_loss: 0.70421571\n",
      "Step: [4682] d_loss: 1.38455176, g_loss: 0.71099794\n",
      "Step: [4683] d_loss: 1.38752139, g_loss: 0.70559204\n",
      "Step: [4684] d_loss: 1.38922668, g_loss: 0.71107769\n",
      "Step: [4685] d_loss: 1.38678062, g_loss: 0.70953566\n",
      "Step: [4686] d_loss: 1.38724983, g_loss: 0.70212543\n",
      "Step: [4687] d_loss: 1.39091372, g_loss: 0.69592351\n",
      "Step: [4688] d_loss: 1.38659692, g_loss: 0.70205325\n",
      "Step: [4689] d_loss: 1.38504291, g_loss: 0.70812893\n",
      "Step: [4690] d_loss: 1.39539623, g_loss: 0.70600903\n",
      "Step: [4691] d_loss: 1.37928295, g_loss: 0.71051466\n",
      "Step: [4692] d_loss: 1.38642383, g_loss: 0.70687878\n",
      "Step: [4693] d_loss: 1.39412308, g_loss: 0.70443881\n",
      "Step: [4694] d_loss: 1.38682604, g_loss: 0.71422708\n",
      "Step: [4695] d_loss: 1.39948225, g_loss: 0.71139222\n",
      "Step: [4696] d_loss: 1.39028466, g_loss: 0.70772856\n",
      "Step: [4697] d_loss: 1.38031054, g_loss: 0.70547616\n",
      "Step: [4698] d_loss: 1.38890624, g_loss: 0.70571828\n",
      "Step: [4699] d_loss: 1.38825989, g_loss: 0.70517063\n",
      "Step: [4700] d_loss: 1.38416696, g_loss: 0.71266586\n",
      "Step: [4701] d_loss: 1.38639641, g_loss: 0.69861603\n",
      "Step: [4702] d_loss: 1.38765132, g_loss: 0.70585465\n",
      "Step: [4703] d_loss: 1.39072764, g_loss: 0.70039845\n",
      "Step: [4704] d_loss: 1.38591480, g_loss: 0.70006752\n",
      "Step: [4705] d_loss: 1.38683414, g_loss: 0.70318997\n",
      "Step: [4706] d_loss: 1.39506149, g_loss: 0.69876432\n",
      "Step: [4707] d_loss: 1.39253187, g_loss: 0.70203733\n",
      "Step: [4708] d_loss: 1.38579297, g_loss: 0.70512301\n",
      "Step: [4709] d_loss: 1.38810563, g_loss: 0.70134318\n",
      "Step: [4710] d_loss: 1.38910270, g_loss: 0.70005369\n",
      "Step: [4711] d_loss: 1.38392663, g_loss: 0.69266093\n",
      "Step: [4712] d_loss: 1.38922811, g_loss: 0.69983697\n",
      "Step: [4713] d_loss: 1.39478302, g_loss: 0.69931483\n",
      "Step: [4714] d_loss: 1.38606167, g_loss: 0.70390427\n",
      "Step: [4715] d_loss: 1.38605464, g_loss: 0.70662552\n",
      "Step: [4716] d_loss: 1.38205123, g_loss: 0.70770884\n",
      "Step: [4717] d_loss: 1.38800406, g_loss: 0.69792885\n",
      "Step: [4718] d_loss: 1.38496184, g_loss: 0.70219123\n",
      "Step: [4719] d_loss: 1.38052368, g_loss: 0.71096408\n",
      "Step: [4720] d_loss: 1.38387251, g_loss: 0.70539039\n",
      "Step: [4721] d_loss: 1.38350582, g_loss: 0.70491034\n",
      "Step: [4722] d_loss: 1.38328767, g_loss: 0.70663214\n",
      "Step: [4723] d_loss: 1.38298368, g_loss: 0.70696634\n",
      "Step: [4724] d_loss: 1.39483452, g_loss: 0.70460069\n",
      "Step: [4725] d_loss: 1.38217878, g_loss: 0.70746219\n",
      "Step: [4726] d_loss: 1.38742435, g_loss: 0.71202767\n",
      "Step: [4727] d_loss: 1.39013600, g_loss: 0.70932209\n",
      "Step: [4728] d_loss: 1.38814020, g_loss: 0.70595193\n",
      "Step: [4729] d_loss: 1.38432217, g_loss: 0.70641065\n",
      "Step: [4730] d_loss: 1.38971925, g_loss: 0.71176672\n",
      "Step: [4731] d_loss: 1.40698111, g_loss: 0.70011628\n",
      "Step: [4732] d_loss: 1.39635825, g_loss: 0.70385242\n",
      "Step: [4733] d_loss: 1.38357615, g_loss: 0.70763481\n",
      "Step: [4734] d_loss: 1.38125432, g_loss: 0.70938963\n",
      "Step: [4735] d_loss: 1.37490284, g_loss: 0.71255493\n",
      "Step: [4736] d_loss: 1.38266087, g_loss: 0.70557129\n",
      "Step: [4737] d_loss: 1.39024973, g_loss: 0.70025480\n",
      "Step: [4738] d_loss: 1.37447214, g_loss: 0.71248347\n",
      "Step: [4739] d_loss: 1.37973976, g_loss: 0.70522416\n",
      "Step: [4740] d_loss: 1.36709607, g_loss: 0.71491957\n",
      "Step: [4741] d_loss: 1.37421370, g_loss: 0.70392144\n",
      "Step: [4742] d_loss: 1.37894583, g_loss: 0.71162283\n",
      "Step: [4743] d_loss: 1.38670206, g_loss: 0.70225215\n",
      "Step: [4744] d_loss: 1.38738477, g_loss: 0.69549245\n",
      "Step: [4745] d_loss: 1.38494575, g_loss: 0.70206136\n",
      "Step: [4746] d_loss: 1.38323140, g_loss: 0.69635886\n",
      "Step: [4747] d_loss: 1.38621473, g_loss: 0.70588148\n",
      "Step: [4748] d_loss: 1.38933372, g_loss: 0.70040667\n",
      "Step: [4749] d_loss: 1.39987445, g_loss: 0.70040220\n",
      "Step: [4750] d_loss: 1.38427711, g_loss: 0.69925475\n",
      "Step: [4751] d_loss: 1.38512206, g_loss: 0.70243555\n",
      "Step: [4752] d_loss: 1.38530111, g_loss: 0.70275462\n",
      "Step: [4753] d_loss: 1.38500285, g_loss: 0.70541221\n",
      "Step: [4754] d_loss: 1.39335573, g_loss: 0.69852710\n",
      "Step: [4755] d_loss: 1.38999319, g_loss: 0.70020044\n",
      "Step: [4756] d_loss: 1.39352155, g_loss: 0.70300931\n",
      "Step: [4757] d_loss: 1.40003061, g_loss: 0.69771653\n",
      "Step: [4758] d_loss: 1.39623165, g_loss: 0.69844019\n",
      "Step: [4759] d_loss: 1.38242292, g_loss: 0.70291233\n",
      "Step: [4760] d_loss: 1.39402723, g_loss: 0.70116055\n",
      "Step: [4761] d_loss: 1.38152385, g_loss: 0.70237964\n",
      "Step: [4762] d_loss: 1.38690519, g_loss: 0.70650125\n",
      "Step: [4763] d_loss: 1.37957668, g_loss: 0.70790124\n",
      "Step: [4764] d_loss: 1.38854337, g_loss: 0.70160997\n",
      "Step: [4765] d_loss: 1.39118481, g_loss: 0.69672525\n",
      "Step: [4766] d_loss: 1.38305449, g_loss: 0.70914036\n",
      "Step: [4767] d_loss: 1.39939415, g_loss: 0.69821823\n",
      "Step: [4768] d_loss: 1.37855077, g_loss: 0.71095383\n",
      "Step: [4769] d_loss: 1.38398635, g_loss: 0.70389456\n",
      "Step: [4770] d_loss: 1.38456964, g_loss: 0.70354503\n",
      "Step: [4771] d_loss: 1.38314211, g_loss: 0.70047241\n",
      "Step: [4772] d_loss: 1.39241743, g_loss: 0.70795256\n",
      "Step: [4773] d_loss: 1.38593459, g_loss: 0.70759159\n",
      "Step: [4774] d_loss: 1.38130140, g_loss: 0.71015561\n",
      "Step: [4775] d_loss: 1.37865043, g_loss: 0.70508838\n",
      "Step: [4776] d_loss: 1.38105965, g_loss: 0.70663035\n",
      "Step: [4777] d_loss: 1.38117194, g_loss: 0.70915699\n",
      "Step: [4778] d_loss: 1.38308334, g_loss: 0.70477581\n",
      "Step: [4779] d_loss: 1.38001585, g_loss: 0.70808047\n",
      "Step: [4780] d_loss: 1.38370693, g_loss: 0.70310628\n",
      "Step: [4781] d_loss: 1.38528287, g_loss: 0.70672143\n",
      "Step: [4782] d_loss: 1.39524317, g_loss: 0.70209163\n",
      "Step: [4783] d_loss: 1.39631748, g_loss: 0.71060634\n",
      "Step: [4784] d_loss: 1.38658428, g_loss: 0.70441711\n",
      "Step: [4785] d_loss: 1.38077974, g_loss: 0.70106983\n",
      "Step: [4786] d_loss: 1.37930369, g_loss: 0.70410222\n",
      "Step: [4787] d_loss: 1.38162923, g_loss: 0.70758545\n",
      "Step: [4788] d_loss: 1.39017022, g_loss: 0.69808960\n",
      "Step: [4789] d_loss: 1.38619792, g_loss: 0.70047927\n",
      "Step: [4790] d_loss: 1.37621331, g_loss: 0.70616913\n",
      "Step: [4791] d_loss: 1.37642848, g_loss: 0.70586336\n",
      "Step: [4792] d_loss: 1.39144027, g_loss: 0.70326328\n",
      "Step: [4793] d_loss: 1.38720226, g_loss: 0.70255905\n",
      "Step: [4794] d_loss: 1.39106822, g_loss: 0.70074224\n",
      "Step: [4795] d_loss: 1.38847280, g_loss: 0.69463229\n",
      "Step: [4796] d_loss: 1.37802231, g_loss: 0.71221411\n",
      "Step: [4797] d_loss: 1.37705374, g_loss: 0.70403695\n",
      "Step: [4798] d_loss: 1.38088179, g_loss: 0.70046514\n",
      "Step: [4799] d_loss: 1.38245201, g_loss: 0.70851666\n",
      "Step: [4800] d_loss: 1.38496578, g_loss: 0.71241266\n",
      "Step: [4801] d_loss: 1.38887882, g_loss: 0.71031171\n",
      "Step: [4802] d_loss: 1.39129710, g_loss: 0.71029782\n",
      "Step: [4803] d_loss: 1.40478814, g_loss: 0.70974737\n",
      "Step: [4804] d_loss: 1.40938842, g_loss: 0.71057534\n",
      "Step: [4805] d_loss: 1.39481890, g_loss: 0.70735085\n",
      "Step: [4806] d_loss: 1.38813543, g_loss: 0.70054102\n",
      "Step: [4807] d_loss: 1.39944291, g_loss: 0.69756591\n",
      "Step: [4808] d_loss: 1.37988091, g_loss: 0.70522255\n",
      "Step: [4809] d_loss: 1.39059114, g_loss: 0.69982928\n",
      "Step: [4810] d_loss: 1.40253520, g_loss: 0.69415569\n",
      "Step: [4811] d_loss: 1.38648462, g_loss: 0.70570791\n",
      "Step: [4812] d_loss: 1.39116311, g_loss: 0.70335150\n",
      "Step: [4813] d_loss: 1.39452899, g_loss: 0.70399058\n",
      "Step: [4814] d_loss: 1.38709998, g_loss: 0.70560777\n",
      "Step: [4815] d_loss: 1.39777660, g_loss: 0.70088965\n",
      "Step: [4816] d_loss: 1.38730836, g_loss: 0.69232577\n",
      "Step: [4817] d_loss: 1.38201618, g_loss: 0.70704663\n",
      "Step: [4818] d_loss: 1.38373494, g_loss: 0.70006031\n",
      "Step: [4819] d_loss: 1.37738752, g_loss: 0.71167266\n",
      "Step: [4820] d_loss: 1.37785912, g_loss: 0.71266973\n",
      "Step: [4821] d_loss: 1.37761199, g_loss: 0.72022688\n",
      "Step: [4822] d_loss: 1.38482666, g_loss: 0.70501554\n",
      "Step: [4823] d_loss: 1.38004518, g_loss: 0.70215768\n",
      "Step: [4824] d_loss: 1.37566137, g_loss: 0.71099699\n",
      "Step: [4825] d_loss: 1.38176966, g_loss: 0.71780300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4826] d_loss: 1.38623190, g_loss: 0.70982778\n",
      "Step: [4827] d_loss: 1.37474585, g_loss: 0.70711958\n",
      "Step: [4828] d_loss: 1.37639165, g_loss: 0.70649135\n",
      "Step: [4829] d_loss: 1.39033771, g_loss: 0.69840908\n",
      "Step: [4830] d_loss: 1.38326883, g_loss: 0.70722735\n",
      "Step: [4831] d_loss: 1.38321328, g_loss: 0.70698154\n",
      "Step: [4832] d_loss: 1.38136649, g_loss: 0.71295255\n",
      "Step: [4833] d_loss: 1.39977252, g_loss: 0.70306963\n",
      "Step: [4834] d_loss: 1.38580680, g_loss: 0.70024562\n",
      "Step: [4835] d_loss: 1.38837361, g_loss: 0.70618439\n",
      "Step: [4836] d_loss: 1.39041483, g_loss: 0.70108926\n",
      "Step: [4837] d_loss: 1.39390910, g_loss: 0.69651836\n",
      "Step: [4838] d_loss: 1.39337969, g_loss: 0.70056069\n",
      "Step: [4839] d_loss: 1.38595283, g_loss: 0.70369369\n",
      "Step: [4840] d_loss: 1.38575315, g_loss: 0.70167685\n",
      "Step: [4841] d_loss: 1.40047646, g_loss: 0.70171678\n",
      "Step: [4842] d_loss: 1.38140512, g_loss: 0.71127701\n",
      "Step: [4843] d_loss: 1.38915563, g_loss: 0.69968009\n",
      "Step: [4844] d_loss: 1.39823782, g_loss: 0.70496869\n",
      "Step: [4845] d_loss: 1.39014900, g_loss: 0.70890546\n",
      "Step: [4846] d_loss: 1.38695633, g_loss: 0.70649070\n",
      "Step: [4847] d_loss: 1.37892210, g_loss: 0.71007824\n",
      "Step: [4848] d_loss: 1.38821697, g_loss: 0.70236969\n",
      "Step: [4849] d_loss: 1.38394570, g_loss: 0.70747364\n",
      "Step: [4850] d_loss: 1.38804638, g_loss: 0.70384300\n",
      "Step: [4851] d_loss: 1.38314939, g_loss: 0.70643473\n",
      "Step: [4852] d_loss: 1.38667917, g_loss: 0.71066332\n",
      "Step: [4853] d_loss: 1.38610601, g_loss: 0.70678985\n",
      "Step: [4854] d_loss: 1.39531875, g_loss: 0.71096432\n",
      "Step: [4855] d_loss: 1.38596320, g_loss: 0.70670772\n",
      "Step: [4856] d_loss: 1.38147354, g_loss: 0.70104039\n",
      "Step: [4857] d_loss: 1.38464642, g_loss: 0.70598376\n",
      "Step: [4858] d_loss: 1.37811816, g_loss: 0.71104968\n",
      "Step: [4859] d_loss: 1.39699793, g_loss: 0.70128179\n",
      "Step: [4860] d_loss: 1.38017511, g_loss: 0.71587682\n",
      "Step: [4861] d_loss: 1.38468719, g_loss: 0.70686936\n",
      "Step: [4862] d_loss: 1.38773143, g_loss: 0.70359939\n",
      "Step: [4863] d_loss: 1.37744737, g_loss: 0.70629144\n",
      "Step: [4864] d_loss: 1.38593173, g_loss: 0.70730710\n",
      "Step: [4865] d_loss: 1.37866318, g_loss: 0.71268564\n",
      "Step: [4866] d_loss: 1.38935733, g_loss: 0.69808441\n",
      "Step: [4867] d_loss: 1.37601876, g_loss: 0.71079409\n",
      "Step: [4868] d_loss: 1.39482975, g_loss: 0.69934094\n",
      "Step: [4869] d_loss: 1.38045228, g_loss: 0.70636332\n",
      "Step: [4870] d_loss: 1.38039446, g_loss: 0.70214891\n",
      "Step: [4871] d_loss: 1.39043999, g_loss: 0.70159006\n",
      "Step: [4872] d_loss: 1.38630962, g_loss: 0.70582420\n",
      "Step: [4873] d_loss: 1.38824332, g_loss: 0.71041322\n",
      "Step: [4874] d_loss: 1.38608313, g_loss: 0.70097697\n",
      "Step: [4875] d_loss: 1.39523888, g_loss: 0.70432758\n",
      "Step: [4876] d_loss: 1.39551544, g_loss: 0.70651853\n",
      "Step: [4877] d_loss: 1.39446068, g_loss: 0.70970690\n",
      "Step: [4878] d_loss: 1.37999463, g_loss: 0.70663399\n",
      "Step: [4879] d_loss: 1.38021600, g_loss: 0.70639074\n",
      "Step: [4880] d_loss: 1.38092208, g_loss: 0.70467854\n",
      "Step: [4881] d_loss: 1.38742399, g_loss: 0.70406103\n",
      "Step: [4882] d_loss: 1.38355207, g_loss: 0.70169485\n",
      "Step: [4883] d_loss: 1.38874078, g_loss: 0.70127898\n",
      "Step: [4884] d_loss: 1.38781130, g_loss: 0.70772845\n",
      "Step: [4885] d_loss: 1.38858581, g_loss: 0.70151788\n",
      "Step: [4886] d_loss: 1.38384664, g_loss: 0.71022952\n",
      "Step: [4887] d_loss: 1.38654923, g_loss: 0.70257097\n",
      "Step: [4888] d_loss: 1.38295066, g_loss: 0.70345223\n",
      "Step: [4889] d_loss: 1.37570047, g_loss: 0.70633221\n",
      "Step: [4890] d_loss: 1.38353240, g_loss: 0.71045369\n",
      "Step: [4891] d_loss: 1.38407350, g_loss: 0.70587832\n",
      "Step: [4892] d_loss: 1.39455080, g_loss: 0.69459343\n",
      "Step: [4893] d_loss: 1.38398194, g_loss: 0.70942998\n",
      "Step: [4894] d_loss: 1.38512588, g_loss: 0.70890450\n",
      "Step: [4895] d_loss: 1.38479614, g_loss: 0.70063543\n",
      "Step: [4896] d_loss: 1.39954996, g_loss: 0.70095170\n",
      "Step: [4897] d_loss: 1.38465929, g_loss: 0.70439839\n",
      "Step: [4898] d_loss: 1.37647665, g_loss: 0.70649838\n",
      "Step: [4899] d_loss: 1.38328576, g_loss: 0.70250905\n",
      "Step: [4900] d_loss: 1.38488245, g_loss: 0.70631629\n",
      "Step: [4901] d_loss: 1.38219070, g_loss: 0.70510238\n",
      "Step: [4902] d_loss: 1.39165902, g_loss: 0.69778907\n",
      "Step: [4903] d_loss: 1.37756646, g_loss: 0.71130675\n",
      "Step: [4904] d_loss: 1.39797258, g_loss: 0.69869530\n",
      "Step: [4905] d_loss: 1.38241959, g_loss: 0.71128589\n",
      "Step: [4906] d_loss: 1.39621437, g_loss: 0.69697511\n",
      "Step: [4907] d_loss: 1.39772129, g_loss: 0.69818389\n",
      "Step: [4908] d_loss: 1.38758886, g_loss: 0.70335126\n",
      "Step: [4909] d_loss: 1.38982606, g_loss: 0.70704913\n",
      "Step: [4910] d_loss: 1.38705969, g_loss: 0.70120680\n",
      "Step: [4911] d_loss: 1.38204944, g_loss: 0.70968592\n",
      "Step: [4912] d_loss: 1.38261843, g_loss: 0.70788324\n",
      "Step: [4913] d_loss: 1.38324988, g_loss: 0.70471722\n",
      "Step: [4914] d_loss: 1.38369298, g_loss: 0.70987391\n",
      "Step: [4915] d_loss: 1.38579655, g_loss: 0.71334952\n",
      "Step: [4916] d_loss: 1.37768698, g_loss: 0.70867217\n",
      "Step: [4917] d_loss: 1.37626970, g_loss: 0.70737469\n",
      "Step: [4918] d_loss: 1.38576770, g_loss: 0.71134996\n",
      "Step: [4919] d_loss: 1.38360250, g_loss: 0.71191192\n",
      "Step: [4920] d_loss: 1.38423467, g_loss: 0.70381039\n",
      "Step: [4921] d_loss: 1.39242983, g_loss: 0.69816858\n",
      "Step: [4922] d_loss: 1.37805474, g_loss: 0.70771492\n",
      "Step: [4923] d_loss: 1.38965797, g_loss: 0.70313287\n",
      "Step: [4924] d_loss: 1.37798285, g_loss: 0.69966793\n",
      "Step: [4925] d_loss: 1.39376605, g_loss: 0.69559550\n",
      "Step: [4926] d_loss: 1.39274526, g_loss: 0.70093274\n",
      "Step: [4927] d_loss: 1.39275479, g_loss: 0.70186937\n",
      "Step: [4928] d_loss: 1.37908959, g_loss: 0.70171636\n",
      "Step: [4929] d_loss: 1.39002681, g_loss: 0.70100379\n",
      "Step: [4930] d_loss: 1.38384306, g_loss: 0.70364177\n",
      "Step: [4931] d_loss: 1.38448310, g_loss: 0.70616007\n",
      "Step: [4932] d_loss: 1.39049911, g_loss: 0.69652998\n",
      "Step: [4933] d_loss: 1.38868153, g_loss: 0.70374006\n",
      "Step: [4934] d_loss: 1.39103496, g_loss: 0.70593286\n",
      "Step: [4935] d_loss: 1.38547444, g_loss: 0.70820022\n",
      "Step: [4936] d_loss: 1.37910223, g_loss: 0.70061272\n",
      "Step: [4937] d_loss: 1.38949049, g_loss: 0.69823104\n",
      "Step: [4938] d_loss: 1.37978160, g_loss: 0.70641923\n",
      "Step: [4939] d_loss: 1.39282799, g_loss: 0.69991016\n",
      "Step: [4940] d_loss: 1.38162994, g_loss: 0.70140672\n",
      "Step: [4941] d_loss: 1.38581276, g_loss: 0.70071584\n",
      "Step: [4942] d_loss: 1.37981784, g_loss: 0.70876348\n",
      "Step: [4943] d_loss: 1.37697387, g_loss: 0.70698315\n",
      "Step: [4944] d_loss: 1.38449216, g_loss: 0.70516157\n",
      "Step: [4945] d_loss: 1.38697922, g_loss: 0.70879441\n",
      "Step: [4946] d_loss: 1.38165665, g_loss: 0.70544624\n",
      "Step: [4947] d_loss: 1.37783980, g_loss: 0.71194351\n",
      "Step: [4948] d_loss: 1.38893116, g_loss: 0.71174037\n",
      "Step: [4949] d_loss: 1.39365304, g_loss: 0.71505255\n",
      "Step: [4950] d_loss: 1.38567412, g_loss: 0.71357954\n",
      "Step: [4951] d_loss: 1.38303971, g_loss: 0.70377994\n",
      "Step: [4952] d_loss: 1.38665605, g_loss: 0.70484042\n",
      "Step: [4953] d_loss: 1.38592672, g_loss: 0.70137703\n",
      "Step: [4954] d_loss: 1.38145876, g_loss: 0.69737691\n",
      "Step: [4955] d_loss: 1.39192164, g_loss: 0.70068932\n",
      "Step: [4956] d_loss: 1.38729525, g_loss: 0.70975280\n",
      "Step: [4957] d_loss: 1.39568138, g_loss: 0.70602310\n",
      "Step: [4958] d_loss: 1.39336026, g_loss: 0.70164084\n",
      "Step: [4959] d_loss: 1.38294244, g_loss: 0.70457643\n",
      "Step: [4960] d_loss: 1.38188624, g_loss: 0.70155418\n",
      "Step: [4961] d_loss: 1.37758422, g_loss: 0.71240222\n",
      "Step: [4962] d_loss: 1.40076113, g_loss: 0.70333475\n",
      "Step: [4963] d_loss: 1.38449073, g_loss: 0.71296620\n",
      "Step: [4964] d_loss: 1.38998008, g_loss: 0.69905484\n",
      "Step: [4965] d_loss: 1.38604474, g_loss: 0.70419860\n",
      "Step: [4966] d_loss: 1.38111639, g_loss: 0.71589565\n",
      "Step: [4967] d_loss: 1.38416660, g_loss: 0.70251125\n",
      "Step: [4968] d_loss: 1.38504434, g_loss: 0.71195722\n",
      "Step: [4969] d_loss: 1.38862526, g_loss: 0.70027977\n",
      "Step: [4970] d_loss: 1.37609601, g_loss: 0.71000779\n",
      "Step: [4971] d_loss: 1.38743782, g_loss: 0.69100249\n",
      "Step: [4972] d_loss: 1.38635325, g_loss: 0.70085537\n",
      "Step: [4973] d_loss: 1.38303518, g_loss: 0.70935684\n",
      "Step: [4974] d_loss: 1.38642526, g_loss: 0.70364493\n",
      "Step: [4975] d_loss: 1.38773775, g_loss: 0.70315951\n",
      "Step: [4976] d_loss: 1.37815511, g_loss: 0.70638478\n",
      "Step: [4977] d_loss: 1.38281274, g_loss: 0.70215178\n",
      "Step: [4978] d_loss: 1.39351022, g_loss: 0.69762111\n",
      "Step: [4979] d_loss: 1.39413917, g_loss: 0.69615149\n",
      "Step: [4980] d_loss: 1.38358068, g_loss: 0.70173603\n",
      "Step: [4981] d_loss: 1.37922430, g_loss: 0.70303428\n",
      "Step: [4982] d_loss: 1.38653898, g_loss: 0.70438313\n",
      "Step: [4983] d_loss: 1.38431036, g_loss: 0.70300132\n",
      "Step: [4984] d_loss: 1.38552964, g_loss: 0.69826096\n",
      "Step: [4985] d_loss: 1.39486706, g_loss: 0.70336950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4986] d_loss: 1.37715268, g_loss: 0.70302331\n",
      "Step: [4987] d_loss: 1.38201308, g_loss: 0.70394576\n",
      "Step: [4988] d_loss: 1.38336670, g_loss: 0.70753181\n",
      "Step: [4989] d_loss: 1.38921273, g_loss: 0.70262837\n",
      "Step: [4990] d_loss: 1.38225007, g_loss: 0.70644057\n",
      "Step: [4991] d_loss: 1.37807465, g_loss: 0.71366858\n",
      "Step: [4992] d_loss: 1.39362729, g_loss: 0.69870502\n",
      "Step: [4993] d_loss: 1.38483214, g_loss: 0.70666575\n",
      "Step: [4994] d_loss: 1.38715863, g_loss: 0.70237970\n",
      "Step: [4995] d_loss: 1.37880397, g_loss: 0.71136695\n",
      "Step: [4996] d_loss: 1.38382125, g_loss: 0.70442283\n",
      "Step: [4997] d_loss: 1.38805342, g_loss: 0.70955837\n",
      "Step: [4998] d_loss: 1.37606168, g_loss: 0.71888077\n",
      "Step: [4999] d_loss: 1.38683224, g_loss: 0.70504320\n",
      "Step: [5000] d_loss: 1.38934350, g_loss: 0.70486814\n",
      "Step: [5001] d_loss: 1.39481449, g_loss: 0.70627642\n",
      "Step: [5002] d_loss: 1.37677121, g_loss: 0.71000618\n",
      "Step: [5003] d_loss: 1.39764166, g_loss: 0.69926941\n",
      "Step: [5004] d_loss: 1.38946056, g_loss: 0.70796454\n",
      "Step: [5005] d_loss: 1.38320994, g_loss: 0.70683146\n",
      "Step: [5006] d_loss: 1.38662863, g_loss: 0.70411325\n",
      "Step: [5007] d_loss: 1.38646102, g_loss: 0.70882803\n",
      "Step: [5008] d_loss: 1.37907887, g_loss: 0.70740461\n",
      "Step: [5009] d_loss: 1.39248443, g_loss: 0.69850218\n",
      "Step: [5010] d_loss: 1.37710297, g_loss: 0.70638216\n",
      "Step: [5011] d_loss: 1.38985646, g_loss: 0.70221072\n",
      "Step: [5012] d_loss: 1.39491010, g_loss: 0.70196056\n",
      "Step: [5013] d_loss: 1.39645147, g_loss: 0.69998026\n",
      "Step: [5014] d_loss: 1.38067889, g_loss: 0.70143759\n",
      "Step: [5015] d_loss: 1.38518548, g_loss: 0.70379812\n",
      "Step: [5016] d_loss: 1.39003944, g_loss: 0.69757271\n",
      "Step: [5017] d_loss: 1.39105701, g_loss: 0.70206362\n",
      "Step: [5018] d_loss: 1.38653135, g_loss: 0.70319355\n",
      "Step: [5019] d_loss: 1.38234997, g_loss: 0.70633185\n",
      "Step: [5020] d_loss: 1.39419460, g_loss: 0.69244373\n",
      "Step: [5021] d_loss: 1.38881707, g_loss: 0.70187408\n",
      "Step: [5022] d_loss: 1.38193762, g_loss: 0.70803308\n",
      "Step: [5023] d_loss: 1.37969983, g_loss: 0.70767879\n",
      "Step: [5024] d_loss: 1.38889325, g_loss: 0.70055437\n",
      "Step: [5025] d_loss: 1.38167775, g_loss: 0.70160455\n",
      "Step: [5026] d_loss: 1.37955856, g_loss: 0.70523965\n",
      "Step: [5027] d_loss: 1.37627292, g_loss: 0.71213734\n",
      "Step: [5028] d_loss: 1.38964522, g_loss: 0.70081186\n",
      "Step: [5029] d_loss: 1.40334892, g_loss: 0.69869667\n",
      "Step: [5030] d_loss: 1.38015676, g_loss: 0.71525818\n",
      "Step: [5031] d_loss: 1.38639152, g_loss: 0.70556891\n",
      "Step: [5032] d_loss: 1.38320267, g_loss: 0.70875174\n",
      "Step: [5033] d_loss: 1.38241470, g_loss: 0.70498276\n",
      "Step: [5034] d_loss: 1.39270878, g_loss: 0.70048368\n",
      "Step: [5035] d_loss: 1.38623416, g_loss: 0.70743704\n",
      "Step: [5036] d_loss: 1.37992275, g_loss: 0.70389742\n",
      "Step: [5037] d_loss: 1.37888157, g_loss: 0.70193577\n",
      "Step: [5038] d_loss: 1.38914108, g_loss: 0.70451224\n",
      "Step: [5039] d_loss: 1.37535715, g_loss: 0.71335196\n",
      "Step: [5040] d_loss: 1.38151360, g_loss: 0.70743197\n",
      "Step: [5041] d_loss: 1.38730502, g_loss: 0.70947820\n",
      "Step: [5042] d_loss: 1.38590050, g_loss: 0.71182066\n",
      "Step: [5043] d_loss: 1.37641430, g_loss: 0.71171844\n",
      "Step: [5044] d_loss: 1.39085460, g_loss: 0.70839900\n",
      "Step: [5045] d_loss: 1.37362599, g_loss: 0.71385133\n",
      "Step: [5046] d_loss: 1.37705839, g_loss: 0.71117800\n",
      "Step: [5047] d_loss: 1.38468838, g_loss: 0.71353006\n",
      "Step: [5048] d_loss: 1.37832952, g_loss: 0.71891487\n",
      "Step: [5049] d_loss: 1.38699520, g_loss: 0.71049738\n",
      "Step: [5050] d_loss: 1.38460326, g_loss: 0.70668554\n",
      "Step: [5051] d_loss: 1.38861656, g_loss: 0.70605755\n",
      "Step: [5052] d_loss: 1.38650358, g_loss: 0.70053458\n",
      "Step: [5053] d_loss: 1.38545752, g_loss: 0.70207274\n",
      "Step: [5054] d_loss: 1.38954830, g_loss: 0.70038867\n",
      "Step: [5055] d_loss: 1.39173174, g_loss: 0.70632368\n",
      "Step: [5056] d_loss: 1.40235281, g_loss: 0.69732600\n",
      "Step: [5057] d_loss: 1.37995934, g_loss: 0.70760268\n",
      "Step: [5058] d_loss: 1.39817405, g_loss: 0.69761068\n",
      "Step: [5059] d_loss: 1.38579798, g_loss: 0.70332360\n",
      "Step: [5060] d_loss: 1.38473547, g_loss: 0.69953841\n",
      "Step: [5061] d_loss: 1.38731599, g_loss: 0.69857430\n",
      "Step: [5062] d_loss: 1.38995743, g_loss: 0.70266694\n",
      "Step: [5063] d_loss: 1.37949824, g_loss: 0.70784652\n",
      "Step: [5064] d_loss: 1.38114357, g_loss: 0.70280153\n",
      "Step: [5065] d_loss: 1.38326108, g_loss: 0.70410126\n",
      "Step: [5066] d_loss: 1.38191950, g_loss: 0.70718849\n",
      "Step: [5067] d_loss: 1.38692904, g_loss: 0.69833684\n",
      "Step: [5068] d_loss: 1.37944376, g_loss: 0.70893478\n",
      "Step: [5069] d_loss: 1.39424896, g_loss: 0.70228779\n",
      "Step: [5070] d_loss: 1.37331486, g_loss: 0.71373951\n",
      "Step: [5071] d_loss: 1.38119483, g_loss: 0.70392841\n",
      "Step: [5072] d_loss: 1.38601041, g_loss: 0.71167523\n",
      "Step: [5073] d_loss: 1.39031792, g_loss: 0.70318574\n",
      "Step: [5074] d_loss: 1.38245153, g_loss: 0.70439631\n",
      "Step: [5075] d_loss: 1.38855624, g_loss: 0.70686829\n",
      "Step: [5076] d_loss: 1.38872516, g_loss: 0.70801908\n",
      "Step: [5077] d_loss: 1.39075279, g_loss: 0.70596290\n",
      "Step: [5078] d_loss: 1.39350843, g_loss: 0.70632398\n",
      "Step: [5079] d_loss: 1.39208221, g_loss: 0.69528925\n",
      "Step: [5080] d_loss: 1.39343667, g_loss: 0.69629562\n",
      "Step: [5081] d_loss: 1.39056504, g_loss: 0.70067173\n",
      "Step: [5082] d_loss: 1.38302052, g_loss: 0.71004736\n",
      "Step: [5083] d_loss: 1.38322377, g_loss: 0.70498729\n",
      "Step: [5084] d_loss: 1.37945330, g_loss: 0.70744002\n",
      "Step: [5085] d_loss: 1.39320207, g_loss: 0.70641971\n",
      "Step: [5086] d_loss: 1.39592004, g_loss: 0.70148617\n",
      "Step: [5087] d_loss: 1.37569773, g_loss: 0.70904672\n",
      "Step: [5088] d_loss: 1.39607072, g_loss: 0.70511448\n",
      "Step: [5089] d_loss: 1.38976359, g_loss: 0.70435464\n",
      "Step: [5090] d_loss: 1.39205837, g_loss: 0.70285386\n",
      "Step: [5091] d_loss: 1.38069010, g_loss: 0.71117932\n",
      "Step: [5092] d_loss: 1.39483953, g_loss: 0.69907993\n",
      "Step: [5093] d_loss: 1.39224625, g_loss: 0.70534849\n",
      "Step: [5094] d_loss: 1.38461018, g_loss: 0.70957214\n",
      "Step: [5095] d_loss: 1.39229143, g_loss: 0.70273763\n",
      "Step: [5096] d_loss: 1.39072609, g_loss: 0.70147121\n",
      "Step: [5097] d_loss: 1.39228344, g_loss: 0.70208997\n",
      "Step: [5098] d_loss: 1.38279366, g_loss: 0.70176399\n",
      "Step: [5099] d_loss: 1.37586784, g_loss: 0.70581889\n",
      "Step: [5100] d_loss: 1.39039278, g_loss: 0.70132577\n",
      "Step: [5101] d_loss: 1.39319992, g_loss: 0.70923793\n",
      "Step: [5102] d_loss: 1.38516545, g_loss: 0.70789480\n",
      "Step: [5103] d_loss: 1.38167381, g_loss: 0.70244157\n",
      "Step: [5104] d_loss: 1.39554119, g_loss: 0.69101375\n",
      "Step: [5105] d_loss: 1.38534796, g_loss: 0.70468044\n",
      "Step: [5106] d_loss: 1.38359404, g_loss: 0.70672190\n",
      "Step: [5107] d_loss: 1.38869262, g_loss: 0.71055555\n",
      "Step: [5108] d_loss: 1.39010382, g_loss: 0.69475329\n",
      "Step: [5109] d_loss: 1.38634956, g_loss: 0.69636357\n",
      "Step: [5110] d_loss: 1.38387692, g_loss: 0.69805270\n",
      "Step: [5111] d_loss: 1.38644886, g_loss: 0.70077246\n",
      "Step: [5112] d_loss: 1.39408875, g_loss: 0.69802308\n",
      "Step: [5113] d_loss: 1.38731766, g_loss: 0.70012516\n",
      "Step: [5114] d_loss: 1.38007307, g_loss: 0.70693332\n",
      "Step: [5115] d_loss: 1.38874650, g_loss: 0.70523727\n",
      "Step: [5116] d_loss: 1.39730823, g_loss: 0.69471598\n",
      "Step: [5117] d_loss: 1.39088380, g_loss: 0.69986832\n",
      "Step: [5118] d_loss: 1.38742578, g_loss: 0.69726241\n",
      "Step: [5119] d_loss: 1.38866091, g_loss: 0.69502842\n",
      "Step: [5120] d_loss: 1.38647115, g_loss: 0.70195556\n",
      "Step: [5121] d_loss: 1.38244557, g_loss: 0.70739830\n",
      "Step: [5122] d_loss: 1.39213753, g_loss: 0.69698018\n",
      "Step: [5123] d_loss: 1.38307607, g_loss: 0.70684552\n",
      "Step: [5124] d_loss: 1.38466883, g_loss: 0.70135862\n",
      "Step: [5125] d_loss: 1.39184153, g_loss: 0.70299417\n",
      "Step: [5126] d_loss: 1.38021421, g_loss: 0.71101111\n",
      "Step: [5127] d_loss: 1.38567924, g_loss: 0.70438087\n",
      "Step: [5128] d_loss: 1.38349676, g_loss: 0.71059614\n",
      "Step: [5129] d_loss: 1.37942696, g_loss: 0.70347667\n",
      "Step: [5130] d_loss: 1.39184070, g_loss: 0.70227528\n",
      "Step: [5131] d_loss: 1.38906014, g_loss: 0.70397711\n",
      "Step: [5132] d_loss: 1.38518739, g_loss: 0.70480943\n",
      "Step: [5133] d_loss: 1.38279259, g_loss: 0.70743287\n",
      "Step: [5134] d_loss: 1.37662411, g_loss: 0.70768815\n",
      "Step: [5135] d_loss: 1.38215804, g_loss: 0.70768696\n",
      "Step: [5136] d_loss: 1.38111424, g_loss: 0.70347810\n",
      "Step: [5137] d_loss: 1.38409734, g_loss: 0.70459056\n",
      "Step: [5138] d_loss: 1.39025545, g_loss: 0.70007694\n",
      "Step: [5139] d_loss: 1.39260745, g_loss: 0.69893515\n",
      "Step: [5140] d_loss: 1.38150311, g_loss: 0.70236760\n",
      "Step: [5141] d_loss: 1.38033128, g_loss: 0.70737243\n",
      "Step: [5142] d_loss: 1.38958883, g_loss: 0.69966924\n",
      "Step: [5143] d_loss: 1.38189888, g_loss: 0.70198226\n",
      "Step: [5144] d_loss: 1.38594627, g_loss: 0.69876015\n",
      "Step: [5145] d_loss: 1.39217520, g_loss: 0.70029312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5146] d_loss: 1.38015795, g_loss: 0.70065188\n",
      "Step: [5147] d_loss: 1.38097429, g_loss: 0.70761007\n",
      "Step: [5148] d_loss: 1.38378453, g_loss: 0.70145929\n",
      "Step: [5149] d_loss: 1.38475955, g_loss: 0.70566857\n",
      "Step: [5150] d_loss: 1.38219619, g_loss: 0.69811606\n",
      "Step: [5151] d_loss: 1.38656664, g_loss: 0.69947684\n",
      "Step: [5152] d_loss: 1.38583159, g_loss: 0.70828319\n",
      "Step: [5153] d_loss: 1.38262677, g_loss: 0.70367515\n",
      "Step: [5154] d_loss: 1.39063191, g_loss: 0.70148194\n",
      "Step: [5155] d_loss: 1.38140273, g_loss: 0.70317590\n",
      "Step: [5156] d_loss: 1.36779213, g_loss: 0.70972407\n",
      "Step: [5157] d_loss: 1.38651633, g_loss: 0.69820905\n",
      "Step: [5158] d_loss: 1.38468051, g_loss: 0.70053709\n",
      "Step: [5159] d_loss: 1.38549232, g_loss: 0.70218623\n",
      "Step: [5160] d_loss: 1.38464248, g_loss: 0.69985449\n",
      "Step: [5161] d_loss: 1.38815796, g_loss: 0.69919777\n",
      "Step: [5162] d_loss: 1.39560437, g_loss: 0.69176942\n",
      "Step: [5163] d_loss: 1.39311099, g_loss: 0.69837165\n",
      "Step: [5164] d_loss: 1.37714601, g_loss: 0.70887005\n",
      "Step: [5165] d_loss: 1.38068438, g_loss: 0.69991612\n",
      "Step: [5166] d_loss: 1.38807249, g_loss: 0.70353138\n",
      "Step: [5167] d_loss: 1.39510608, g_loss: 0.69861913\n",
      "Step: [5168] d_loss: 1.38630068, g_loss: 0.70203424\n",
      "Step: [5169] d_loss: 1.39160776, g_loss: 0.69950360\n",
      "Step: [5170] d_loss: 1.38489377, g_loss: 0.70055246\n",
      "Step: [5171] d_loss: 1.38280845, g_loss: 0.70173985\n",
      "Step: [5172] d_loss: 1.38487136, g_loss: 0.70427620\n",
      "Step: [5173] d_loss: 1.39107203, g_loss: 0.69986033\n",
      "Step: [5174] d_loss: 1.38725638, g_loss: 0.70070833\n",
      "Step: [5175] d_loss: 1.38551664, g_loss: 0.70529133\n",
      "Step: [5176] d_loss: 1.39007664, g_loss: 0.70068467\n",
      "Step: [5177] d_loss: 1.39002848, g_loss: 0.70112264\n",
      "Step: [5178] d_loss: 1.38185894, g_loss: 0.70574439\n",
      "Step: [5179] d_loss: 1.37927854, g_loss: 0.70418394\n",
      "Step: [5180] d_loss: 1.38496935, g_loss: 0.70154023\n",
      "Step: [5181] d_loss: 1.38146412, g_loss: 0.70711839\n",
      "Step: [5182] d_loss: 1.37941027, g_loss: 0.70658678\n",
      "Step: [5183] d_loss: 1.37813663, g_loss: 0.70765376\n",
      "Step: [5184] d_loss: 1.38073564, g_loss: 0.69951087\n",
      "Step: [5185] d_loss: 1.38044882, g_loss: 0.70334655\n",
      "Step: [5186] d_loss: 1.37665415, g_loss: 0.70804703\n",
      "Step: [5187] d_loss: 1.38041747, g_loss: 0.70521051\n",
      "Step: [5188] d_loss: 1.38574433, g_loss: 0.70139980\n",
      "Step: [5189] d_loss: 1.39317846, g_loss: 0.69631338\n",
      "Step: [5190] d_loss: 1.38505185, g_loss: 0.70238423\n",
      "Step: [5191] d_loss: 1.38655639, g_loss: 0.70219564\n",
      "Step: [5192] d_loss: 1.38708055, g_loss: 0.70355397\n",
      "Step: [5193] d_loss: 1.38404560, g_loss: 0.70269489\n",
      "Step: [5194] d_loss: 1.38987553, g_loss: 0.69789791\n",
      "Step: [5195] d_loss: 1.37963700, g_loss: 0.69975972\n",
      "Step: [5196] d_loss: 1.38346910, g_loss: 0.70575857\n",
      "Step: [5197] d_loss: 1.38532722, g_loss: 0.70487845\n",
      "Step: [5198] d_loss: 1.39008021, g_loss: 0.69565707\n",
      "Step: [5199] d_loss: 1.38295460, g_loss: 0.70144594\n",
      "Step: [5200] d_loss: 1.38883257, g_loss: 0.69905889\n",
      "Step: [5201] d_loss: 1.39064634, g_loss: 0.69538271\n",
      "Step: [5202] d_loss: 1.38549829, g_loss: 0.69904470\n",
      "Step: [5203] d_loss: 1.38373423, g_loss: 0.70415014\n",
      "Step: [5204] d_loss: 1.39180708, g_loss: 0.70230907\n",
      "Step: [5205] d_loss: 1.38169050, g_loss: 0.70020473\n",
      "Step: [5206] d_loss: 1.38852024, g_loss: 0.70287967\n",
      "Step: [5207] d_loss: 1.38681674, g_loss: 0.70321560\n",
      "Step: [5208] d_loss: 1.38919497, g_loss: 0.69789839\n",
      "Step: [5209] d_loss: 1.38208938, g_loss: 0.70965338\n",
      "Step: [5210] d_loss: 1.38715374, g_loss: 0.69394803\n",
      "Step: [5211] d_loss: 1.38778043, g_loss: 0.70550823\n",
      "Step: [5212] d_loss: 1.39036632, g_loss: 0.69868767\n",
      "Step: [5213] d_loss: 1.38870907, g_loss: 0.69672763\n",
      "Step: [5214] d_loss: 1.38762879, g_loss: 0.70270127\n",
      "Step: [5215] d_loss: 1.38168192, g_loss: 0.70909703\n",
      "Step: [5216] d_loss: 1.38113785, g_loss: 0.70028496\n",
      "Step: [5217] d_loss: 1.37678730, g_loss: 0.70342797\n",
      "Step: [5218] d_loss: 1.38644660, g_loss: 0.70250463\n",
      "Step: [5219] d_loss: 1.37190700, g_loss: 0.71207428\n",
      "Step: [5220] d_loss: 1.38658679, g_loss: 0.70147181\n",
      "Step: [5221] d_loss: 1.39177394, g_loss: 0.70649952\n",
      "Step: [5222] d_loss: 1.38429642, g_loss: 0.70405352\n",
      "Step: [5223] d_loss: 1.38501549, g_loss: 0.70658171\n",
      "Step: [5224] d_loss: 1.38379955, g_loss: 0.69726872\n",
      "Step: [5225] d_loss: 1.38298225, g_loss: 0.70203328\n",
      "Step: [5226] d_loss: 1.39131117, g_loss: 0.69863671\n",
      "Step: [5227] d_loss: 1.39043093, g_loss: 0.70135105\n",
      "Step: [5228] d_loss: 1.39040518, g_loss: 0.70513874\n",
      "Step: [5229] d_loss: 1.38005900, g_loss: 0.70698607\n",
      "Step: [5230] d_loss: 1.39305305, g_loss: 0.69495279\n",
      "Step: [5231] d_loss: 1.38794422, g_loss: 0.70352954\n",
      "Step: [5232] d_loss: 1.38023710, g_loss: 0.70205122\n",
      "Step: [5233] d_loss: 1.38399458, g_loss: 0.70639998\n",
      "Step: [5234] d_loss: 1.38449717, g_loss: 0.70217359\n",
      "Step: [5235] d_loss: 1.38154817, g_loss: 0.70397675\n",
      "Step: [5236] d_loss: 1.38437104, g_loss: 0.70088291\n",
      "Step: [5237] d_loss: 1.38725567, g_loss: 0.70013261\n",
      "Step: [5238] d_loss: 1.38708019, g_loss: 0.69862187\n",
      "Step: [5239] d_loss: 1.38144362, g_loss: 0.69702983\n",
      "Step: [5240] d_loss: 1.38560426, g_loss: 0.70408964\n",
      "Step: [5241] d_loss: 1.39060414, g_loss: 0.70133293\n",
      "Step: [5242] d_loss: 1.38216162, g_loss: 0.70157284\n",
      "Step: [5243] d_loss: 1.38570023, g_loss: 0.70287627\n",
      "Step: [5244] d_loss: 1.38754869, g_loss: 0.69342470\n",
      "Step: [5245] d_loss: 1.39040291, g_loss: 0.69893885\n",
      "Step: [5246] d_loss: 1.38168955, g_loss: 0.70132369\n",
      "Step: [5247] d_loss: 1.38253427, g_loss: 0.69980133\n",
      "Step: [5248] d_loss: 1.39439082, g_loss: 0.69285220\n",
      "Step: [5249] d_loss: 1.38666928, g_loss: 0.70228207\n",
      "Step: [5250] d_loss: 1.38588381, g_loss: 0.70538515\n",
      "Step: [5251] d_loss: 1.38619840, g_loss: 0.70081890\n",
      "Step: [5252] d_loss: 1.38541889, g_loss: 0.69890070\n",
      "Step: [5253] d_loss: 1.37443161, g_loss: 0.70482469\n",
      "Step: [5254] d_loss: 1.38477457, g_loss: 0.70135629\n",
      "Step: [5255] d_loss: 1.37788510, g_loss: 0.70564401\n",
      "Step: [5256] d_loss: 1.39161265, g_loss: 0.70168912\n",
      "Step: [5257] d_loss: 1.38443756, g_loss: 0.69961214\n",
      "Step: [5258] d_loss: 1.38553011, g_loss: 0.70050681\n",
      "Step: [5259] d_loss: 1.38003993, g_loss: 0.70263422\n",
      "Step: [5260] d_loss: 1.38710833, g_loss: 0.69984031\n",
      "Step: [5261] d_loss: 1.39268303, g_loss: 0.69873428\n",
      "Step: [5262] d_loss: 1.38585973, g_loss: 0.70700717\n",
      "Step: [5263] d_loss: 1.37785017, g_loss: 0.70687121\n",
      "Step: [5264] d_loss: 1.38927281, g_loss: 0.70082289\n",
      "Step: [5265] d_loss: 1.38421941, g_loss: 0.70406795\n",
      "Step: [5266] d_loss: 1.38743734, g_loss: 0.69981378\n",
      "Step: [5267] d_loss: 1.37761700, g_loss: 0.71348858\n",
      "Step: [5268] d_loss: 1.38987446, g_loss: 0.70139217\n",
      "Step: [5269] d_loss: 1.38937044, g_loss: 0.69827199\n",
      "Step: [5270] d_loss: 1.38890457, g_loss: 0.70208901\n",
      "Step: [5271] d_loss: 1.38312376, g_loss: 0.70154345\n",
      "Step: [5272] d_loss: 1.38189173, g_loss: 0.70180291\n",
      "Step: [5273] d_loss: 1.38897729, g_loss: 0.70177007\n",
      "Step: [5274] d_loss: 1.38146019, g_loss: 0.70556808\n",
      "Step: [5275] d_loss: 1.37307048, g_loss: 0.70961910\n",
      "Step: [5276] d_loss: 1.38133717, g_loss: 0.70825243\n",
      "Step: [5277] d_loss: 1.38504529, g_loss: 0.70268351\n",
      "Step: [5278] d_loss: 1.37951469, g_loss: 0.70669377\n",
      "Step: [5279] d_loss: 1.37788618, g_loss: 0.70442796\n",
      "Step: [5280] d_loss: 1.38706911, g_loss: 0.70296454\n",
      "Step: [5281] d_loss: 1.38884044, g_loss: 0.70586073\n",
      "Step: [5282] d_loss: 1.39512515, g_loss: 0.69977665\n",
      "Step: [5283] d_loss: 1.39245033, g_loss: 0.69959146\n",
      "Step: [5284] d_loss: 1.39420664, g_loss: 0.69383931\n",
      "Step: [5285] d_loss: 1.38683939, g_loss: 0.70095730\n",
      "Step: [5286] d_loss: 1.38153088, g_loss: 0.70179069\n",
      "Step: [5287] d_loss: 1.37871170, g_loss: 0.71054935\n",
      "Step: [5288] d_loss: 1.38939095, g_loss: 0.70523441\n",
      "Step: [5289] d_loss: 1.39038634, g_loss: 0.69706130\n",
      "Step: [5290] d_loss: 1.39192498, g_loss: 0.70027786\n",
      "Step: [5291] d_loss: 1.39248824, g_loss: 0.69104958\n",
      "Step: [5292] d_loss: 1.38781595, g_loss: 0.70317930\n",
      "Step: [5293] d_loss: 1.39082778, g_loss: 0.70402545\n",
      "Step: [5294] d_loss: 1.38513601, g_loss: 0.70457333\n",
      "Step: [5295] d_loss: 1.38436019, g_loss: 0.70684111\n",
      "Step: [5296] d_loss: 1.38242459, g_loss: 0.70075482\n",
      "Step: [5297] d_loss: 1.38199425, g_loss: 0.70636523\n",
      "Step: [5298] d_loss: 1.38997638, g_loss: 0.69769090\n",
      "Step: [5299] d_loss: 1.37820375, g_loss: 0.70368123\n",
      "Step: [5300] d_loss: 1.38925815, g_loss: 0.70455414\n",
      "Step: [5301] d_loss: 1.38169169, g_loss: 0.70657772\n",
      "Step: [5302] d_loss: 1.38004708, g_loss: 0.70188713\n",
      "Step: [5303] d_loss: 1.38448238, g_loss: 0.70409185\n",
      "Step: [5304] d_loss: 1.38216293, g_loss: 0.70231789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5305] d_loss: 1.37712884, g_loss: 0.70448673\n",
      "Step: [5306] d_loss: 1.39177299, g_loss: 0.70064789\n",
      "Step: [5307] d_loss: 1.38569617, g_loss: 0.70498526\n",
      "Step: [5308] d_loss: 1.39070141, g_loss: 0.70765162\n",
      "Step: [5309] d_loss: 1.38099658, g_loss: 0.70757473\n",
      "Step: [5310] d_loss: 1.38590240, g_loss: 0.70122623\n",
      "Step: [5311] d_loss: 1.38647258, g_loss: 0.69978702\n",
      "Step: [5312] d_loss: 1.38879633, g_loss: 0.69841611\n",
      "Step: [5313] d_loss: 1.38083220, g_loss: 0.70269710\n",
      "Step: [5314] d_loss: 1.37730372, g_loss: 0.70768809\n",
      "Step: [5315] d_loss: 1.38578761, g_loss: 0.70145202\n",
      "Step: [5316] d_loss: 1.38761485, g_loss: 0.70295554\n",
      "Step: [5317] d_loss: 1.38730145, g_loss: 0.70771343\n",
      "Step: [5318] d_loss: 1.38028479, g_loss: 0.70311999\n",
      "Step: [5319] d_loss: 1.38838327, g_loss: 0.69719875\n",
      "Step: [5320] d_loss: 1.38402057, g_loss: 0.70151961\n",
      "Step: [5321] d_loss: 1.38653874, g_loss: 0.70103419\n",
      "Step: [5322] d_loss: 1.38585365, g_loss: 0.70414281\n",
      "Step: [5323] d_loss: 1.38492250, g_loss: 0.70103633\n",
      "Step: [5324] d_loss: 1.38262415, g_loss: 0.70055342\n",
      "Step: [5325] d_loss: 1.38325357, g_loss: 0.70631582\n",
      "Step: [5326] d_loss: 1.37894058, g_loss: 0.70532191\n",
      "Step: [5327] d_loss: 1.38881135, g_loss: 0.69907522\n",
      "Step: [5328] d_loss: 1.39232087, g_loss: 0.69943261\n",
      "Step: [5329] d_loss: 1.38121033, g_loss: 0.70320106\n",
      "Step: [5330] d_loss: 1.38685179, g_loss: 0.69987345\n",
      "Step: [5331] d_loss: 1.38427532, g_loss: 0.70088375\n",
      "Step: [5332] d_loss: 1.38853884, g_loss: 0.69549084\n",
      "Step: [5333] d_loss: 1.38587224, g_loss: 0.70013213\n",
      "Step: [5334] d_loss: 1.38621867, g_loss: 0.70280826\n",
      "Step: [5335] d_loss: 1.38421857, g_loss: 0.70274842\n",
      "Step: [5336] d_loss: 1.38128638, g_loss: 0.70401037\n",
      "Step: [5337] d_loss: 1.39732003, g_loss: 0.69426763\n",
      "Step: [5338] d_loss: 1.38244545, g_loss: 0.70415080\n",
      "Step: [5339] d_loss: 1.39016163, g_loss: 0.70282090\n",
      "Step: [5340] d_loss: 1.38079143, g_loss: 0.70096141\n",
      "Step: [5341] d_loss: 1.38187850, g_loss: 0.70408785\n",
      "Step: [5342] d_loss: 1.38739431, g_loss: 0.70220947\n",
      "Step: [5343] d_loss: 1.37351704, g_loss: 0.70793629\n",
      "Step: [5344] d_loss: 1.38966465, g_loss: 0.70035982\n",
      "Step: [5345] d_loss: 1.40005827, g_loss: 0.69449067\n",
      "Step: [5346] d_loss: 1.38049710, g_loss: 0.70435417\n",
      "Step: [5347] d_loss: 1.38877916, g_loss: 0.70452619\n",
      "Step: [5348] d_loss: 1.38339937, g_loss: 0.70417231\n",
      "Step: [5349] d_loss: 1.38601542, g_loss: 0.70624626\n",
      "Step: [5350] d_loss: 1.38412213, g_loss: 0.71167910\n",
      "Step: [5351] d_loss: 1.39508820, g_loss: 0.69950968\n",
      "Step: [5352] d_loss: 1.38643372, g_loss: 0.70341015\n",
      "Step: [5353] d_loss: 1.37992263, g_loss: 0.70124739\n",
      "Step: [5354] d_loss: 1.38164473, g_loss: 0.70166332\n",
      "Step: [5355] d_loss: 1.38649511, g_loss: 0.70327342\n",
      "Step: [5356] d_loss: 1.37880230, g_loss: 0.70503616\n",
      "Step: [5357] d_loss: 1.38050532, g_loss: 0.70449483\n",
      "Step: [5358] d_loss: 1.38722301, g_loss: 0.70567036\n",
      "Step: [5359] d_loss: 1.39157271, g_loss: 0.70042169\n",
      "Step: [5360] d_loss: 1.37928796, g_loss: 0.70816755\n",
      "Step: [5361] d_loss: 1.38855147, g_loss: 0.70010853\n",
      "Step: [5362] d_loss: 1.38446343, g_loss: 0.69869012\n",
      "Step: [5363] d_loss: 1.37955856, g_loss: 0.70559329\n",
      "Step: [5364] d_loss: 1.38387764, g_loss: 0.70580417\n",
      "Step: [5365] d_loss: 1.38956857, g_loss: 0.70293117\n",
      "Step: [5366] d_loss: 1.38687611, g_loss: 0.70344818\n",
      "Step: [5367] d_loss: 1.39246082, g_loss: 0.69994265\n",
      "Step: [5368] d_loss: 1.38154101, g_loss: 0.69888067\n",
      "Step: [5369] d_loss: 1.38113236, g_loss: 0.70919156\n",
      "Step: [5370] d_loss: 1.38448930, g_loss: 0.70648736\n",
      "Step: [5371] d_loss: 1.38357329, g_loss: 0.70530045\n",
      "Step: [5372] d_loss: 1.38161778, g_loss: 0.70958018\n",
      "Step: [5373] d_loss: 1.38369834, g_loss: 0.70298976\n",
      "Step: [5374] d_loss: 1.39449024, g_loss: 0.70267457\n",
      "Step: [5375] d_loss: 1.38247132, g_loss: 0.70644301\n",
      "Step: [5376] d_loss: 1.38106084, g_loss: 0.70502651\n",
      "Step: [5377] d_loss: 1.39175212, g_loss: 0.69983935\n",
      "Step: [5378] d_loss: 1.38612533, g_loss: 0.70326412\n",
      "Step: [5379] d_loss: 1.39510500, g_loss: 0.70467395\n",
      "Step: [5380] d_loss: 1.38717246, g_loss: 0.70341015\n",
      "Step: [5381] d_loss: 1.38868868, g_loss: 0.70316517\n",
      "Step: [5382] d_loss: 1.38785267, g_loss: 0.69813895\n",
      "Step: [5383] d_loss: 1.38421583, g_loss: 0.70123518\n",
      "Step: [5384] d_loss: 1.39197588, g_loss: 0.70495450\n",
      "Step: [5385] d_loss: 1.39564967, g_loss: 0.70415068\n",
      "Step: [5386] d_loss: 1.38646090, g_loss: 0.70376414\n",
      "Step: [5387] d_loss: 1.39208841, g_loss: 0.69600868\n",
      "Step: [5388] d_loss: 1.39310837, g_loss: 0.69941968\n",
      "Step: [5389] d_loss: 1.39346552, g_loss: 0.69865733\n",
      "Step: [5390] d_loss: 1.38986051, g_loss: 0.69756907\n",
      "Step: [5391] d_loss: 1.38697386, g_loss: 0.69633216\n",
      "Step: [5392] d_loss: 1.38269043, g_loss: 0.70814145\n",
      "Step: [5393] d_loss: 1.39040089, g_loss: 0.69781905\n",
      "Step: [5394] d_loss: 1.37646890, g_loss: 0.70773315\n",
      "Step: [5395] d_loss: 1.37950444, g_loss: 0.70490813\n",
      "Step: [5396] d_loss: 1.38212323, g_loss: 0.70406312\n",
      "Step: [5397] d_loss: 1.38421750, g_loss: 0.70664304\n",
      "Step: [5398] d_loss: 1.38711977, g_loss: 0.70161986\n",
      "Step: [5399] d_loss: 1.38492119, g_loss: 0.69955879\n",
      "Step: [5400] d_loss: 1.39209938, g_loss: 0.69741261\n",
      "Step: [5401] d_loss: 1.38937891, g_loss: 0.69987082\n",
      "Step: [5402] d_loss: 1.38373554, g_loss: 0.70220637\n",
      "Step: [5403] d_loss: 1.38931799, g_loss: 0.71155041\n",
      "Step: [5404] d_loss: 1.39846826, g_loss: 0.70648402\n",
      "Step: [5405] d_loss: 1.39561236, g_loss: 0.69966513\n",
      "Step: [5406] d_loss: 1.38635957, g_loss: 0.70025349\n",
      "Step: [5407] d_loss: 1.38990092, g_loss: 0.70248443\n",
      "Step: [5408] d_loss: 1.38805747, g_loss: 0.69983166\n",
      "Step: [5409] d_loss: 1.38012362, g_loss: 0.69876170\n",
      "Step: [5410] d_loss: 1.39134753, g_loss: 0.69923913\n",
      "Step: [5411] d_loss: 1.39145482, g_loss: 0.70199585\n",
      "Step: [5412] d_loss: 1.37984610, g_loss: 0.70408547\n",
      "Step: [5413] d_loss: 1.38579035, g_loss: 0.70174813\n",
      "Step: [5414] d_loss: 1.38390923, g_loss: 0.70100033\n",
      "Step: [5415] d_loss: 1.38986719, g_loss: 0.69652367\n",
      "Step: [5416] d_loss: 1.37745881, g_loss: 0.70344484\n",
      "Step: [5417] d_loss: 1.38416672, g_loss: 0.70246613\n",
      "Step: [5418] d_loss: 1.38545656, g_loss: 0.69926012\n",
      "Step: [5419] d_loss: 1.38864136, g_loss: 0.70046353\n",
      "Step: [5420] d_loss: 1.39497578, g_loss: 0.70096427\n",
      "Step: [5421] d_loss: 1.39249086, g_loss: 0.69337022\n",
      "Step: [5422] d_loss: 1.38527024, g_loss: 0.70096684\n",
      "Step: [5423] d_loss: 1.38812709, g_loss: 0.70096850\n",
      "Step: [5424] d_loss: 1.39480126, g_loss: 0.69453907\n",
      "Step: [5425] d_loss: 1.39568210, g_loss: 0.69039714\n",
      "Step: [5426] d_loss: 1.39222288, g_loss: 0.69624293\n",
      "Step: [5427] d_loss: 1.38766766, g_loss: 0.70309836\n",
      "Step: [5428] d_loss: 1.38708901, g_loss: 0.70167083\n",
      "Step: [5429] d_loss: 1.38778090, g_loss: 0.70383012\n",
      "Step: [5430] d_loss: 1.38640213, g_loss: 0.69874942\n",
      "Step: [5431] d_loss: 1.38395870, g_loss: 0.70119417\n",
      "Step: [5432] d_loss: 1.38344967, g_loss: 0.70257938\n",
      "Step: [5433] d_loss: 1.38202250, g_loss: 0.70441306\n",
      "Step: [5434] d_loss: 1.38655961, g_loss: 0.70098734\n",
      "Step: [5435] d_loss: 1.37742686, g_loss: 0.70571268\n",
      "Step: [5436] d_loss: 1.38412201, g_loss: 0.70072752\n",
      "Step: [5437] d_loss: 1.38797319, g_loss: 0.69510591\n",
      "Step: [5438] d_loss: 1.38884175, g_loss: 0.69792020\n",
      "Step: [5439] d_loss: 1.38463354, g_loss: 0.70437193\n",
      "Step: [5440] d_loss: 1.38011742, g_loss: 0.70614946\n",
      "Step: [5441] d_loss: 1.38363504, g_loss: 0.70751607\n",
      "Step: [5442] d_loss: 1.38365161, g_loss: 0.70785493\n",
      "Step: [5443] d_loss: 1.38191962, g_loss: 0.70145661\n",
      "Step: [5444] d_loss: 1.39048469, g_loss: 0.70162147\n",
      "Step: [5445] d_loss: 1.38210392, g_loss: 0.70159787\n",
      "Step: [5446] d_loss: 1.38204050, g_loss: 0.70660734\n",
      "Step: [5447] d_loss: 1.38389039, g_loss: 0.70068711\n",
      "Step: [5448] d_loss: 1.39010000, g_loss: 0.69819105\n",
      "Step: [5449] d_loss: 1.38940859, g_loss: 0.70107788\n",
      "Step: [5450] d_loss: 1.39492881, g_loss: 0.69218922\n",
      "Step: [5451] d_loss: 1.38367009, g_loss: 0.70016587\n",
      "Step: [5452] d_loss: 1.38759661, g_loss: 0.70266163\n",
      "Step: [5453] d_loss: 1.39530778, g_loss: 0.70076460\n",
      "Step: [5454] d_loss: 1.39133739, g_loss: 0.69576669\n",
      "Step: [5455] d_loss: 1.38963342, g_loss: 0.69719774\n",
      "Step: [5456] d_loss: 1.39401817, g_loss: 0.69641650\n",
      "Step: [5457] d_loss: 1.38592911, g_loss: 0.70246547\n",
      "Step: [5458] d_loss: 1.37867057, g_loss: 0.70332599\n",
      "Step: [5459] d_loss: 1.38275337, g_loss: 0.70325327\n",
      "Step: [5460] d_loss: 1.39212561, g_loss: 0.69439781\n",
      "Step: [5461] d_loss: 1.38677645, g_loss: 0.69881701\n",
      "Step: [5462] d_loss: 1.38439441, g_loss: 0.69706774\n",
      "Step: [5463] d_loss: 1.38545942, g_loss: 0.70013773\n",
      "Step: [5464] d_loss: 1.38229012, g_loss: 0.70374173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5465] d_loss: 1.38639998, g_loss: 0.69987756\n",
      "Step: [5466] d_loss: 1.38682711, g_loss: 0.70099455\n",
      "Step: [5467] d_loss: 1.38986015, g_loss: 0.69738603\n",
      "Step: [5468] d_loss: 1.38170624, g_loss: 0.70289767\n",
      "Step: [5469] d_loss: 1.38196027, g_loss: 0.70424908\n",
      "Step: [5470] d_loss: 1.38229442, g_loss: 0.69969523\n",
      "Step: [5471] d_loss: 1.38371873, g_loss: 0.70387733\n",
      "Step: [5472] d_loss: 1.38036680, g_loss: 0.70908320\n",
      "Step: [5473] d_loss: 1.38115728, g_loss: 0.70342135\n",
      "Step: [5474] d_loss: 1.38959455, g_loss: 0.69623566\n",
      "Step: [5475] d_loss: 1.38653541, g_loss: 0.69948584\n",
      "Step: [5476] d_loss: 1.38128185, g_loss: 0.70273900\n",
      "Step: [5477] d_loss: 1.38503170, g_loss: 0.70356429\n",
      "Step: [5478] d_loss: 1.38732851, g_loss: 0.70200741\n",
      "Step: [5479] d_loss: 1.39250660, g_loss: 0.70286965\n",
      "Step: [5480] d_loss: 1.38981867, g_loss: 0.70226502\n",
      "Step: [5481] d_loss: 1.38899088, g_loss: 0.70841587\n",
      "Step: [5482] d_loss: 1.39145923, g_loss: 0.69772190\n",
      "Step: [5483] d_loss: 1.38763165, g_loss: 0.70492530\n",
      "Step: [5484] d_loss: 1.38526940, g_loss: 0.70372832\n",
      "Step: [5485] d_loss: 1.38727295, g_loss: 0.70208126\n",
      "Step: [5486] d_loss: 1.39100170, g_loss: 0.69949812\n",
      "Step: [5487] d_loss: 1.37740493, g_loss: 0.70838130\n",
      "Step: [5488] d_loss: 1.38281560, g_loss: 0.70198965\n",
      "Step: [5489] d_loss: 1.38028955, g_loss: 0.70600748\n",
      "Step: [5490] d_loss: 1.38158143, g_loss: 0.70516765\n",
      "Step: [5491] d_loss: 1.38391697, g_loss: 0.69644487\n",
      "Step: [5492] d_loss: 1.39432025, g_loss: 0.70366788\n",
      "Step: [5493] d_loss: 1.38862824, g_loss: 0.69832510\n",
      "Step: [5494] d_loss: 1.38802266, g_loss: 0.69741905\n",
      "Step: [5495] d_loss: 1.39121509, g_loss: 0.69935548\n",
      "Step: [5496] d_loss: 1.38784111, g_loss: 0.69654059\n",
      "Step: [5497] d_loss: 1.38794184, g_loss: 0.69905043\n",
      "Step: [5498] d_loss: 1.38694572, g_loss: 0.70177698\n",
      "Step: [5499] d_loss: 1.38867617, g_loss: 0.70521271\n",
      "Step: [5500] d_loss: 1.38393593, g_loss: 0.69989860\n",
      "Step: [5501] d_loss: 1.38789141, g_loss: 0.69536185\n",
      "Step: [5502] d_loss: 1.39043748, g_loss: 0.69323516\n",
      "Step: [5503] d_loss: 1.38950753, g_loss: 0.70376420\n",
      "Step: [5504] d_loss: 1.38657951, g_loss: 0.70187855\n",
      "Step: [5505] d_loss: 1.38726997, g_loss: 0.69711190\n",
      "Step: [5506] d_loss: 1.39486444, g_loss: 0.69708729\n",
      "Step: [5507] d_loss: 1.39617229, g_loss: 0.69601929\n",
      "Step: [5508] d_loss: 1.38125885, g_loss: 0.70325178\n",
      "Step: [5509] d_loss: 1.38936424, g_loss: 0.70018196\n",
      "Step: [5510] d_loss: 1.38823545, g_loss: 0.70001167\n",
      "Step: [5511] d_loss: 1.37843060, g_loss: 0.69958150\n",
      "Step: [5512] d_loss: 1.37671781, g_loss: 0.70597577\n",
      "Step: [5513] d_loss: 1.38683748, g_loss: 0.69722414\n",
      "Step: [5514] d_loss: 1.38083780, g_loss: 0.70941609\n",
      "Step: [5515] d_loss: 1.38620508, g_loss: 0.69492191\n",
      "Step: [5516] d_loss: 1.38254809, g_loss: 0.70505840\n",
      "Step: [5517] d_loss: 1.38425553, g_loss: 0.70466173\n",
      "Step: [5518] d_loss: 1.39251244, g_loss: 0.70124620\n",
      "Step: [5519] d_loss: 1.39118695, g_loss: 0.69695175\n",
      "Step: [5520] d_loss: 1.38523984, g_loss: 0.70137370\n",
      "Step: [5521] d_loss: 1.39176691, g_loss: 0.70119649\n",
      "Step: [5522] d_loss: 1.37712669, g_loss: 0.70716453\n",
      "Step: [5523] d_loss: 1.39110279, g_loss: 0.69560540\n",
      "Step: [5524] d_loss: 1.38553524, g_loss: 0.70196122\n",
      "Step: [5525] d_loss: 1.38372219, g_loss: 0.70197809\n",
      "Step: [5526] d_loss: 1.39268303, g_loss: 0.70300555\n",
      "Step: [5527] d_loss: 1.38945293, g_loss: 0.69390261\n",
      "Step: [5528] d_loss: 1.38526690, g_loss: 0.70147347\n",
      "Step: [5529] d_loss: 1.38687944, g_loss: 0.70545071\n",
      "Step: [5530] d_loss: 1.39658546, g_loss: 0.69453669\n",
      "Step: [5531] d_loss: 1.37891197, g_loss: 0.71028328\n",
      "Step: [5532] d_loss: 1.39064860, g_loss: 0.70013666\n",
      "Step: [5533] d_loss: 1.38913012, g_loss: 0.69947535\n",
      "Step: [5534] d_loss: 1.37916899, g_loss: 0.70384014\n",
      "Step: [5535] d_loss: 1.39242768, g_loss: 0.70128751\n",
      "Step: [5536] d_loss: 1.38984263, g_loss: 0.70054102\n",
      "Step: [5537] d_loss: 1.38725734, g_loss: 0.69936568\n",
      "Step: [5538] d_loss: 1.39130378, g_loss: 0.69515115\n",
      "Step: [5539] d_loss: 1.38618684, g_loss: 0.69726181\n",
      "Step: [5540] d_loss: 1.39474523, g_loss: 0.70111835\n",
      "Step: [5541] d_loss: 1.38458335, g_loss: 0.70983535\n",
      "Step: [5542] d_loss: 1.38338017, g_loss: 0.70287979\n",
      "Step: [5543] d_loss: 1.38620830, g_loss: 0.69553161\n",
      "Step: [5544] d_loss: 1.37971377, g_loss: 0.69957560\n",
      "Step: [5545] d_loss: 1.38357091, g_loss: 0.70466757\n",
      "Step: [5546] d_loss: 1.38535666, g_loss: 0.69845939\n",
      "Step: [5547] d_loss: 1.38298178, g_loss: 0.69994938\n",
      "Step: [5548] d_loss: 1.39002681, g_loss: 0.70070481\n",
      "Step: [5549] d_loss: 1.38972247, g_loss: 0.69851935\n",
      "Step: [5550] d_loss: 1.38705254, g_loss: 0.70241332\n",
      "Step: [5551] d_loss: 1.38731122, g_loss: 0.69780940\n",
      "Step: [5552] d_loss: 1.37663150, g_loss: 0.70758235\n",
      "Step: [5553] d_loss: 1.38601530, g_loss: 0.70308977\n",
      "Step: [5554] d_loss: 1.39130116, g_loss: 0.69365484\n",
      "Step: [5555] d_loss: 1.38930476, g_loss: 0.69212937\n",
      "Step: [5556] d_loss: 1.39238703, g_loss: 0.69470513\n",
      "Step: [5557] d_loss: 1.38203907, g_loss: 0.69705832\n",
      "Step: [5558] d_loss: 1.38840246, g_loss: 0.70123780\n",
      "Step: [5559] d_loss: 1.38981175, g_loss: 0.70020342\n",
      "Step: [5560] d_loss: 1.38881516, g_loss: 0.69417781\n",
      "Step: [5561] d_loss: 1.38848007, g_loss: 0.69800413\n",
      "Step: [5562] d_loss: 1.39005125, g_loss: 0.69406843\n",
      "Step: [5563] d_loss: 1.38620329, g_loss: 0.70005000\n",
      "Step: [5564] d_loss: 1.38221645, g_loss: 0.69771087\n",
      "Step: [5565] d_loss: 1.38201785, g_loss: 0.70154727\n",
      "Step: [5566] d_loss: 1.38487029, g_loss: 0.70016384\n",
      "Step: [5567] d_loss: 1.38683343, g_loss: 0.69983447\n",
      "Step: [5568] d_loss: 1.38495243, g_loss: 0.69826388\n",
      "Step: [5569] d_loss: 1.38502669, g_loss: 0.69866413\n",
      "Step: [5570] d_loss: 1.38816237, g_loss: 0.69453830\n",
      "Step: [5571] d_loss: 1.38911664, g_loss: 0.69696653\n",
      "Step: [5572] d_loss: 1.38590300, g_loss: 0.70064461\n",
      "Step: [5573] d_loss: 1.38689506, g_loss: 0.70139349\n",
      "Step: [5574] d_loss: 1.38594866, g_loss: 0.69913399\n",
      "Step: [5575] d_loss: 1.37937057, g_loss: 0.70266479\n",
      "Step: [5576] d_loss: 1.38514423, g_loss: 0.70290542\n",
      "Step: [5577] d_loss: 1.39125693, g_loss: 0.69482362\n",
      "Step: [5578] d_loss: 1.38703728, g_loss: 0.69972146\n",
      "Step: [5579] d_loss: 1.38453150, g_loss: 0.69545984\n",
      "Step: [5580] d_loss: 1.38940191, g_loss: 0.69978344\n",
      "Step: [5581] d_loss: 1.39163208, g_loss: 0.69847810\n",
      "Step: [5582] d_loss: 1.38380551, g_loss: 0.70120943\n",
      "Step: [5583] d_loss: 1.38926256, g_loss: 0.69718248\n",
      "Step: [5584] d_loss: 1.38451695, g_loss: 0.69697559\n",
      "Step: [5585] d_loss: 1.38894296, g_loss: 0.69675756\n",
      "Step: [5586] d_loss: 1.38817072, g_loss: 0.70134890\n",
      "Step: [5587] d_loss: 1.38285756, g_loss: 0.70471114\n",
      "Step: [5588] d_loss: 1.38330925, g_loss: 0.70435852\n",
      "Step: [5589] d_loss: 1.38892281, g_loss: 0.70322728\n",
      "Step: [5590] d_loss: 1.38158655, g_loss: 0.70105052\n",
      "Step: [5591] d_loss: 1.38130760, g_loss: 0.69962335\n",
      "Step: [5592] d_loss: 1.38880539, g_loss: 0.69771969\n",
      "Step: [5593] d_loss: 1.38482785, g_loss: 0.69905746\n",
      "Step: [5594] d_loss: 1.38373554, g_loss: 0.69963431\n",
      "Step: [5595] d_loss: 1.39459205, g_loss: 0.69755054\n",
      "Step: [5596] d_loss: 1.38298678, g_loss: 0.70940411\n",
      "Step: [5597] d_loss: 1.38886964, g_loss: 0.70541197\n",
      "Step: [5598] d_loss: 1.39148724, g_loss: 0.70155555\n",
      "Step: [5599] d_loss: 1.38719535, g_loss: 0.69243753\n",
      "Step: [5600] d_loss: 1.39201021, g_loss: 0.69282532\n",
      "Step: [5601] d_loss: 1.38292372, g_loss: 0.69825399\n",
      "Step: [5602] d_loss: 1.38889766, g_loss: 0.70023346\n",
      "Step: [5603] d_loss: 1.38684607, g_loss: 0.69628131\n",
      "Step: [5604] d_loss: 1.38543987, g_loss: 0.69970965\n",
      "Step: [5605] d_loss: 1.38177967, g_loss: 0.70174170\n",
      "Step: [5606] d_loss: 1.39031410, g_loss: 0.69477797\n",
      "Step: [5607] d_loss: 1.38130784, g_loss: 0.69865561\n",
      "Step: [5608] d_loss: 1.39045572, g_loss: 0.69930148\n",
      "Step: [5609] d_loss: 1.39189982, g_loss: 0.69525528\n",
      "Step: [5610] d_loss: 1.37917197, g_loss: 0.70543039\n",
      "Step: [5611] d_loss: 1.38378572, g_loss: 0.69686788\n",
      "Step: [5612] d_loss: 1.38899207, g_loss: 0.69625306\n",
      "Step: [5613] d_loss: 1.39369988, g_loss: 0.69779217\n",
      "Step: [5614] d_loss: 1.38616896, g_loss: 0.69964874\n",
      "Step: [5615] d_loss: 1.39120865, g_loss: 0.70412588\n",
      "Step: [5616] d_loss: 1.38968503, g_loss: 0.69760394\n",
      "Step: [5617] d_loss: 1.37951851, g_loss: 0.70175791\n",
      "Step: [5618] d_loss: 1.38607347, g_loss: 0.69797254\n",
      "Step: [5619] d_loss: 1.38729382, g_loss: 0.69427335\n",
      "Step: [5620] d_loss: 1.38751984, g_loss: 0.69393933\n",
      "Step: [5621] d_loss: 1.38563251, g_loss: 0.69748366\n",
      "Step: [5622] d_loss: 1.38521528, g_loss: 0.69997883\n",
      "Step: [5623] d_loss: 1.39108431, g_loss: 0.69647098\n",
      "Step: [5624] d_loss: 1.37809050, g_loss: 0.70087039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5625] d_loss: 1.38945580, g_loss: 0.69641650\n",
      "Step: [5626] d_loss: 1.38407815, g_loss: 0.70284075\n",
      "Step: [5627] d_loss: 1.38775086, g_loss: 0.69862258\n",
      "Step: [5628] d_loss: 1.38915336, g_loss: 0.70023727\n",
      "Step: [5629] d_loss: 1.39048529, g_loss: 0.69490063\n",
      "Step: [5630] d_loss: 1.39319825, g_loss: 0.69309050\n",
      "Step: [5631] d_loss: 1.38533139, g_loss: 0.70157754\n",
      "Step: [5632] d_loss: 1.38556480, g_loss: 0.69813442\n",
      "Step: [5633] d_loss: 1.38055801, g_loss: 0.70078450\n",
      "Step: [5634] d_loss: 1.39122391, g_loss: 0.69523883\n",
      "Step: [5635] d_loss: 1.38349378, g_loss: 0.69840419\n",
      "Step: [5636] d_loss: 1.38423753, g_loss: 0.69740999\n",
      "Step: [5637] d_loss: 1.38524735, g_loss: 0.69909537\n",
      "Step: [5638] d_loss: 1.38330936, g_loss: 0.70238435\n",
      "Step: [5639] d_loss: 1.38279188, g_loss: 0.69737518\n",
      "Step: [5640] d_loss: 1.38461947, g_loss: 0.70334494\n",
      "Step: [5641] d_loss: 1.38349569, g_loss: 0.70423484\n",
      "Step: [5642] d_loss: 1.38374424, g_loss: 0.70121014\n",
      "Step: [5643] d_loss: 1.38149071, g_loss: 0.70191675\n",
      "Step: [5644] d_loss: 1.37841344, g_loss: 0.70302421\n",
      "Step: [5645] d_loss: 1.38315034, g_loss: 0.69763899\n",
      "Step: [5646] d_loss: 1.38086438, g_loss: 0.70425832\n",
      "Step: [5647] d_loss: 1.37743711, g_loss: 0.70652074\n",
      "Step: [5648] d_loss: 1.38406229, g_loss: 0.70105892\n",
      "Step: [5649] d_loss: 1.38033056, g_loss: 0.70062298\n",
      "Step: [5650] d_loss: 1.38117528, g_loss: 0.70053422\n",
      "Step: [5651] d_loss: 1.37440073, g_loss: 0.70134556\n",
      "Step: [5652] d_loss: 1.38687623, g_loss: 0.69948399\n",
      "Step: [5653] d_loss: 1.37273717, g_loss: 0.71030414\n",
      "Step: [5654] d_loss: 1.39423144, g_loss: 0.69554949\n",
      "Step: [5655] d_loss: 1.39577150, g_loss: 0.69268262\n",
      "Step: [5656] d_loss: 1.38452387, g_loss: 0.69302309\n",
      "Step: [5657] d_loss: 1.38489699, g_loss: 0.70036912\n",
      "Step: [5658] d_loss: 1.38427770, g_loss: 0.70025009\n",
      "Step: [5659] d_loss: 1.39161563, g_loss: 0.69852662\n",
      "Step: [5660] d_loss: 1.38891160, g_loss: 0.69371903\n",
      "Step: [5661] d_loss: 1.38815808, g_loss: 0.69645828\n",
      "Step: [5662] d_loss: 1.38791406, g_loss: 0.70303208\n",
      "Step: [5663] d_loss: 1.39348841, g_loss: 0.69162261\n",
      "Step: [5664] d_loss: 1.39116085, g_loss: 0.69713092\n",
      "Step: [5665] d_loss: 1.38991570, g_loss: 0.70153719\n",
      "Step: [5666] d_loss: 1.39197528, g_loss: 0.68861085\n",
      "Step: [5667] d_loss: 1.38757563, g_loss: 0.69489336\n",
      "Step: [5668] d_loss: 1.39044809, g_loss: 0.69983071\n",
      "Step: [5669] d_loss: 1.38783383, g_loss: 0.69779664\n",
      "Step: [5670] d_loss: 1.38620877, g_loss: 0.70093268\n",
      "Step: [5671] d_loss: 1.38734388, g_loss: 0.69929212\n",
      "Step: [5672] d_loss: 1.38784027, g_loss: 0.69991988\n",
      "Step: [5673] d_loss: 1.38912201, g_loss: 0.69719106\n",
      "Step: [5674] d_loss: 1.38471830, g_loss: 0.70142233\n",
      "Step: [5675] d_loss: 1.38484740, g_loss: 0.69769824\n",
      "Step: [5676] d_loss: 1.38576317, g_loss: 0.70002460\n",
      "Step: [5677] d_loss: 1.38865709, g_loss: 0.69832742\n",
      "Step: [5678] d_loss: 1.38360858, g_loss: 0.69872224\n",
      "Step: [5679] d_loss: 1.38199711, g_loss: 0.69910288\n",
      "Step: [5680] d_loss: 1.38778114, g_loss: 0.69775331\n",
      "Step: [5681] d_loss: 1.38386893, g_loss: 0.70442140\n",
      "Step: [5682] d_loss: 1.39192390, g_loss: 0.69883347\n",
      "Step: [5683] d_loss: 1.39024019, g_loss: 0.69929552\n",
      "Step: [5684] d_loss: 1.38331509, g_loss: 0.69733989\n",
      "Step: [5685] d_loss: 1.39618337, g_loss: 0.69770849\n",
      "Step: [5686] d_loss: 1.38480890, g_loss: 0.69657552\n",
      "Step: [5687] d_loss: 1.38642776, g_loss: 0.69824386\n",
      "Step: [5688] d_loss: 1.38373828, g_loss: 0.70100117\n",
      "Step: [5689] d_loss: 1.38517320, g_loss: 0.70622492\n",
      "Step: [5690] d_loss: 1.38373840, g_loss: 0.70142519\n",
      "Step: [5691] d_loss: 1.39008498, g_loss: 0.69681859\n",
      "Step: [5692] d_loss: 1.38321710, g_loss: 0.69605982\n",
      "Step: [5693] d_loss: 1.38975585, g_loss: 0.69792324\n",
      "Step: [5694] d_loss: 1.38392949, g_loss: 0.70222354\n",
      "Step: [5695] d_loss: 1.38871479, g_loss: 0.70122701\n",
      "Step: [5696] d_loss: 1.38601720, g_loss: 0.70277822\n",
      "Step: [5697] d_loss: 1.38425231, g_loss: 0.69635534\n",
      "Step: [5698] d_loss: 1.38347685, g_loss: 0.69912285\n",
      "Step: [5699] d_loss: 1.38597941, g_loss: 0.69351071\n",
      "Step: [5700] d_loss: 1.38316333, g_loss: 0.69724685\n",
      "Step: [5701] d_loss: 1.38019276, g_loss: 0.70224917\n",
      "Step: [5702] d_loss: 1.37637496, g_loss: 0.70399928\n",
      "Step: [5703] d_loss: 1.38431156, g_loss: 0.70462030\n",
      "Step: [5704] d_loss: 1.38760102, g_loss: 0.69793159\n",
      "Step: [5705] d_loss: 1.38126945, g_loss: 0.69272876\n",
      "Step: [5706] d_loss: 1.39111161, g_loss: 0.69584829\n",
      "Step: [5707] d_loss: 1.39052701, g_loss: 0.69719207\n",
      "Step: [5708] d_loss: 1.39142513, g_loss: 0.69882822\n",
      "Step: [5709] d_loss: 1.38012445, g_loss: 0.69981277\n",
      "Step: [5710] d_loss: 1.38156188, g_loss: 0.70060647\n",
      "Step: [5711] d_loss: 1.39291537, g_loss: 0.69773209\n",
      "Step: [5712] d_loss: 1.38775361, g_loss: 0.69206214\n",
      "Step: [5713] d_loss: 1.38018918, g_loss: 0.70006132\n",
      "Step: [5714] d_loss: 1.38997173, g_loss: 0.69877911\n",
      "Step: [5715] d_loss: 1.39064968, g_loss: 0.69815445\n",
      "Step: [5716] d_loss: 1.38226426, g_loss: 0.69925332\n",
      "Step: [5717] d_loss: 1.38210082, g_loss: 0.69767475\n",
      "Step: [5718] d_loss: 1.38454056, g_loss: 0.69791228\n",
      "Step: [5719] d_loss: 1.38650572, g_loss: 0.69441283\n",
      "Step: [5720] d_loss: 1.38897133, g_loss: 0.70051849\n",
      "Step: [5721] d_loss: 1.38584006, g_loss: 0.69891691\n",
      "Step: [5722] d_loss: 1.39200044, g_loss: 0.69282472\n",
      "Step: [5723] d_loss: 1.39138639, g_loss: 0.69585526\n",
      "Step: [5724] d_loss: 1.38930964, g_loss: 0.69778007\n",
      "Step: [5725] d_loss: 1.39139974, g_loss: 0.69706941\n",
      "Step: [5726] d_loss: 1.38046598, g_loss: 0.69923967\n",
      "Step: [5727] d_loss: 1.39489269, g_loss: 0.69352353\n",
      "Step: [5728] d_loss: 1.38989031, g_loss: 0.70392787\n",
      "Step: [5729] d_loss: 1.39459705, g_loss: 0.70079774\n",
      "Step: [5730] d_loss: 1.38447618, g_loss: 0.69770104\n",
      "Step: [5731] d_loss: 1.39643490, g_loss: 0.70283234\n",
      "Step: [5732] d_loss: 1.38767540, g_loss: 0.70102537\n",
      "Step: [5733] d_loss: 1.38498008, g_loss: 0.69694996\n",
      "Step: [5734] d_loss: 1.38829136, g_loss: 0.69608873\n",
      "Step: [5735] d_loss: 1.38772750, g_loss: 0.69526649\n",
      "Step: [5736] d_loss: 1.38653946, g_loss: 0.70003921\n",
      "Step: [5737] d_loss: 1.38636947, g_loss: 0.69976223\n",
      "Step: [5738] d_loss: 1.38321900, g_loss: 0.69902325\n",
      "Step: [5739] d_loss: 1.38466167, g_loss: 0.69925302\n",
      "Step: [5740] d_loss: 1.38864827, g_loss: 0.69337869\n",
      "Step: [5741] d_loss: 1.38399482, g_loss: 0.70227444\n",
      "Step: [5742] d_loss: 1.38790846, g_loss: 0.69638956\n",
      "Step: [5743] d_loss: 1.38477814, g_loss: 0.69763619\n",
      "Step: [5744] d_loss: 1.38646317, g_loss: 0.69692671\n",
      "Step: [5745] d_loss: 1.38915348, g_loss: 0.69587123\n",
      "Step: [5746] d_loss: 1.38818729, g_loss: 0.69760263\n",
      "Step: [5747] d_loss: 1.39474106, g_loss: 0.69729853\n",
      "Step: [5748] d_loss: 1.39970374, g_loss: 0.69963026\n",
      "Step: [5749] d_loss: 1.39099324, g_loss: 0.69299656\n",
      "Step: [5750] d_loss: 1.38872266, g_loss: 0.69665492\n",
      "Step: [5751] d_loss: 1.38811803, g_loss: 0.69543970\n",
      "Step: [5752] d_loss: 1.38611007, g_loss: 0.69590002\n",
      "Step: [5753] d_loss: 1.38339972, g_loss: 0.70468324\n",
      "Step: [5754] d_loss: 1.38215423, g_loss: 0.70161581\n",
      "Step: [5755] d_loss: 1.38895631, g_loss: 0.69603217\n",
      "Step: [5756] d_loss: 1.38714933, g_loss: 0.69681883\n",
      "Step: [5757] d_loss: 1.38129449, g_loss: 0.69322747\n",
      "Step: [5758] d_loss: 1.38737595, g_loss: 0.69366777\n",
      "Step: [5759] d_loss: 1.38286173, g_loss: 0.69619846\n",
      "Step: [5760] d_loss: 1.38808894, g_loss: 0.70067137\n",
      "Step: [5761] d_loss: 1.37758565, g_loss: 0.70240688\n",
      "Step: [5762] d_loss: 1.38507938, g_loss: 0.69970536\n",
      "Step: [5763] d_loss: 1.38359666, g_loss: 0.69810265\n",
      "Step: [5764] d_loss: 1.38674712, g_loss: 0.69818223\n",
      "Step: [5765] d_loss: 1.38480258, g_loss: 0.69780445\n",
      "Step: [5766] d_loss: 1.38593674, g_loss: 0.70158267\n",
      "Step: [5767] d_loss: 1.38438106, g_loss: 0.69951057\n",
      "Step: [5768] d_loss: 1.38050473, g_loss: 0.69726646\n",
      "Step: [5769] d_loss: 1.38256359, g_loss: 0.70015788\n",
      "Step: [5770] d_loss: 1.39532375, g_loss: 0.68917716\n",
      "Step: [5771] d_loss: 1.38885307, g_loss: 0.69951451\n",
      "Step: [5772] d_loss: 1.39431834, g_loss: 0.69007385\n",
      "Step: [5773] d_loss: 1.38716102, g_loss: 0.70112151\n",
      "Step: [5774] d_loss: 1.38740277, g_loss: 0.69887805\n",
      "Step: [5775] d_loss: 1.39194846, g_loss: 0.69456911\n",
      "Step: [5776] d_loss: 1.38375247, g_loss: 0.70143622\n",
      "Step: [5777] d_loss: 1.38587248, g_loss: 0.69752580\n",
      "Step: [5778] d_loss: 1.38405335, g_loss: 0.69960099\n",
      "Step: [5779] d_loss: 1.38706505, g_loss: 0.69680613\n",
      "Step: [5780] d_loss: 1.38825297, g_loss: 0.69751918\n",
      "Step: [5781] d_loss: 1.39012623, g_loss: 0.69567454\n",
      "Step: [5782] d_loss: 1.38987470, g_loss: 0.69738543\n",
      "Step: [5783] d_loss: 1.38453674, g_loss: 0.69733083\n",
      "Step: [5784] d_loss: 1.38294816, g_loss: 0.70131862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5785] d_loss: 1.38125336, g_loss: 0.70113802\n",
      "Step: [5786] d_loss: 1.38266921, g_loss: 0.69885254\n",
      "Step: [5787] d_loss: 1.38656425, g_loss: 0.69736212\n",
      "Step: [5788] d_loss: 1.38102782, g_loss: 0.69635260\n",
      "Step: [5789] d_loss: 1.38142645, g_loss: 0.70592260\n",
      "Step: [5790] d_loss: 1.38453126, g_loss: 0.69786203\n",
      "Step: [5791] d_loss: 1.38270378, g_loss: 0.69818884\n",
      "Step: [5792] d_loss: 1.38229275, g_loss: 0.69704616\n",
      "Step: [5793] d_loss: 1.38624883, g_loss: 0.69517195\n",
      "Step: [5794] d_loss: 1.39219534, g_loss: 0.69317532\n",
      "Step: [5795] d_loss: 1.38507557, g_loss: 0.69471133\n",
      "Step: [5796] d_loss: 1.38345218, g_loss: 0.70091152\n",
      "Step: [5797] d_loss: 1.38687646, g_loss: 0.69576710\n",
      "Step: [5798] d_loss: 1.38567686, g_loss: 0.69715142\n",
      "Step: [5799] d_loss: 1.38817251, g_loss: 0.69640940\n",
      "Step: [5800] d_loss: 1.38929713, g_loss: 0.69348645\n",
      "Step: [5801] d_loss: 1.38990247, g_loss: 0.69522154\n",
      "Step: [5802] d_loss: 1.38879156, g_loss: 0.69836438\n",
      "Step: [5803] d_loss: 1.38982248, g_loss: 0.69753426\n",
      "Step: [5804] d_loss: 1.38920164, g_loss: 0.69740498\n",
      "Step: [5805] d_loss: 1.39094269, g_loss: 0.69778633\n",
      "Step: [5806] d_loss: 1.38447988, g_loss: 0.70015025\n",
      "Step: [5807] d_loss: 1.38840055, g_loss: 0.69397056\n",
      "Step: [5808] d_loss: 1.38767314, g_loss: 0.69407320\n",
      "Step: [5809] d_loss: 1.38739347, g_loss: 0.69819343\n",
      "Step: [5810] d_loss: 1.38970447, g_loss: 0.69363189\n",
      "Step: [5811] d_loss: 1.38486516, g_loss: 0.70009327\n",
      "Step: [5812] d_loss: 1.38740051, g_loss: 0.69842386\n",
      "Step: [5813] d_loss: 1.38735056, g_loss: 0.69863522\n",
      "Step: [5814] d_loss: 1.38570404, g_loss: 0.69804287\n",
      "Step: [5815] d_loss: 1.38530898, g_loss: 0.69076860\n",
      "Step: [5816] d_loss: 1.38650441, g_loss: 0.70004404\n",
      "Step: [5817] d_loss: 1.38663423, g_loss: 0.70033205\n",
      "Step: [5818] d_loss: 1.38276410, g_loss: 0.70098621\n",
      "Step: [5819] d_loss: 1.38476682, g_loss: 0.70014721\n",
      "Step: [5820] d_loss: 1.38670540, g_loss: 0.69620144\n",
      "Step: [5821] d_loss: 1.38055682, g_loss: 0.69678485\n",
      "Step: [5822] d_loss: 1.38310957, g_loss: 0.69891596\n",
      "Step: [5823] d_loss: 1.38419211, g_loss: 0.70066518\n",
      "Step: [5824] d_loss: 1.38276339, g_loss: 0.69653207\n",
      "Step: [5825] d_loss: 1.38176250, g_loss: 0.69821632\n",
      "Step: [5826] d_loss: 1.38664114, g_loss: 0.69727170\n",
      "Step: [5827] d_loss: 1.38297629, g_loss: 0.69977558\n",
      "Step: [5828] d_loss: 1.38371277, g_loss: 0.70062053\n",
      "Step: [5829] d_loss: 1.38272166, g_loss: 0.69943887\n",
      "Step: [5830] d_loss: 1.38646591, g_loss: 0.70018113\n",
      "Step: [5831] d_loss: 1.39092386, g_loss: 0.69694436\n",
      "Step: [5832] d_loss: 1.38694024, g_loss: 0.69515252\n",
      "Step: [5833] d_loss: 1.38423228, g_loss: 0.69824362\n",
      "Step: [5834] d_loss: 1.39019382, g_loss: 0.69538981\n",
      "Step: [5835] d_loss: 1.38566446, g_loss: 0.69894141\n",
      "Step: [5836] d_loss: 1.39149642, g_loss: 0.69297308\n",
      "Step: [5837] d_loss: 1.38551462, g_loss: 0.69738007\n",
      "Step: [5838] d_loss: 1.39163661, g_loss: 0.69805408\n",
      "Step: [5839] d_loss: 1.38726258, g_loss: 0.69629550\n",
      "Step: [5840] d_loss: 1.38243818, g_loss: 0.69972742\n",
      "Step: [5841] d_loss: 1.39305019, g_loss: 0.69424582\n",
      "Step: [5842] d_loss: 1.38763428, g_loss: 0.69500184\n",
      "Step: [5843] d_loss: 1.38390386, g_loss: 0.70111132\n",
      "Step: [5844] d_loss: 1.39160466, g_loss: 0.69807136\n",
      "Step: [5845] d_loss: 1.38738680, g_loss: 0.70526540\n",
      "Step: [5846] d_loss: 1.38955164, g_loss: 0.69540262\n",
      "Step: [5847] d_loss: 1.38335896, g_loss: 0.69707799\n",
      "Step: [5848] d_loss: 1.38414192, g_loss: 0.69951886\n",
      "Step: [5849] d_loss: 1.38586998, g_loss: 0.69911772\n",
      "Step: [5850] d_loss: 1.38543427, g_loss: 0.69902050\n",
      "Step: [5851] d_loss: 1.38253880, g_loss: 0.70063514\n",
      "Step: [5852] d_loss: 1.38466620, g_loss: 0.69724274\n",
      "Step: [5853] d_loss: 1.38579750, g_loss: 0.69681013\n",
      "Step: [5854] d_loss: 1.38391280, g_loss: 0.69719481\n",
      "Step: [5855] d_loss: 1.38335550, g_loss: 0.69852859\n",
      "Step: [5856] d_loss: 1.38804150, g_loss: 0.70219576\n",
      "Step: [5857] d_loss: 1.38777566, g_loss: 0.69787908\n",
      "Step: [5858] d_loss: 1.38894117, g_loss: 0.69911236\n",
      "Step: [5859] d_loss: 1.38724470, g_loss: 0.69724780\n",
      "Step: [5860] d_loss: 1.38554525, g_loss: 0.69657207\n",
      "Step: [5861] d_loss: 1.38734221, g_loss: 0.69332731\n",
      "Step: [5862] d_loss: 1.38762176, g_loss: 0.69569170\n",
      "Step: [5863] d_loss: 1.38448191, g_loss: 0.69928014\n",
      "Step: [5864] d_loss: 1.38864613, g_loss: 0.69478369\n",
      "Step: [5865] d_loss: 1.38436961, g_loss: 0.69989944\n",
      "Step: [5866] d_loss: 1.38766360, g_loss: 0.69513893\n",
      "Step: [5867] d_loss: 1.38371670, g_loss: 0.69801557\n",
      "Step: [5868] d_loss: 1.38894117, g_loss: 0.69603693\n",
      "Step: [5869] d_loss: 1.38942671, g_loss: 0.69586480\n",
      "Step: [5870] d_loss: 1.38507855, g_loss: 0.69763994\n",
      "Step: [5871] d_loss: 1.38531268, g_loss: 0.70071352\n",
      "Step: [5872] d_loss: 1.38577414, g_loss: 0.69872200\n",
      "Step: [5873] d_loss: 1.38370585, g_loss: 0.70201945\n",
      "Step: [5874] d_loss: 1.38833117, g_loss: 0.69575155\n",
      "Step: [5875] d_loss: 1.38698459, g_loss: 0.69587982\n",
      "Step: [5876] d_loss: 1.38154435, g_loss: 0.69515049\n",
      "Step: [5877] d_loss: 1.38297439, g_loss: 0.69568110\n",
      "Step: [5878] d_loss: 1.38724780, g_loss: 0.69799799\n",
      "Step: [5879] d_loss: 1.38339484, g_loss: 0.69800639\n",
      "Step: [5880] d_loss: 1.38373935, g_loss: 0.69681668\n",
      "Step: [5881] d_loss: 1.38460481, g_loss: 0.69517565\n",
      "Step: [5882] d_loss: 1.38978386, g_loss: 0.69177371\n",
      "Step: [5883] d_loss: 1.38850248, g_loss: 0.69782126\n",
      "Step: [5884] d_loss: 1.38568413, g_loss: 0.69533420\n",
      "Step: [5885] d_loss: 1.38376570, g_loss: 0.70294678\n",
      "Step: [5886] d_loss: 1.38950682, g_loss: 0.69726539\n",
      "Step: [5887] d_loss: 1.38366973, g_loss: 0.69498199\n",
      "Step: [5888] d_loss: 1.39074814, g_loss: 0.69356418\n",
      "Step: [5889] d_loss: 1.38994288, g_loss: 0.69783103\n",
      "Step: [5890] d_loss: 1.38681471, g_loss: 0.70369679\n",
      "Step: [5891] d_loss: 1.39536929, g_loss: 0.69711816\n",
      "Step: [5892] d_loss: 1.38864899, g_loss: 0.70346522\n",
      "Step: [5893] d_loss: 1.39017308, g_loss: 0.69767666\n",
      "Step: [5894] d_loss: 1.39107752, g_loss: 0.69757986\n",
      "Step: [5895] d_loss: 1.38551188, g_loss: 0.69641292\n",
      "Step: [5896] d_loss: 1.38920617, g_loss: 0.69760716\n",
      "Step: [5897] d_loss: 1.38429737, g_loss: 0.69746745\n",
      "Step: [5898] d_loss: 1.38723540, g_loss: 0.69847393\n",
      "Step: [5899] d_loss: 1.38629651, g_loss: 0.69868767\n",
      "Step: [5900] d_loss: 1.38659799, g_loss: 0.69928837\n",
      "Step: [5901] d_loss: 1.38415134, g_loss: 0.69645309\n",
      "Step: [5902] d_loss: 1.38617444, g_loss: 0.69699985\n",
      "Step: [5903] d_loss: 1.38799655, g_loss: 0.69929922\n",
      "Step: [5904] d_loss: 1.38560832, g_loss: 0.70383853\n",
      "Step: [5905] d_loss: 1.38783360, g_loss: 0.69811654\n",
      "Step: [5906] d_loss: 1.38822114, g_loss: 0.69769609\n",
      "Step: [5907] d_loss: 1.38707018, g_loss: 0.69387352\n",
      "Step: [5908] d_loss: 1.38922429, g_loss: 0.69822025\n",
      "Step: [5909] d_loss: 1.38378406, g_loss: 0.69859785\n",
      "Step: [5910] d_loss: 1.38933778, g_loss: 0.68864727\n",
      "Step: [5911] d_loss: 1.38412881, g_loss: 0.70056081\n",
      "Step: [5912] d_loss: 1.38641977, g_loss: 0.69672507\n",
      "Step: [5913] d_loss: 1.38062167, g_loss: 0.70579523\n",
      "Step: [5914] d_loss: 1.38718832, g_loss: 0.69646412\n",
      "Step: [5915] d_loss: 1.38706756, g_loss: 0.69470453\n",
      "Step: [5916] d_loss: 1.38201714, g_loss: 0.69608194\n",
      "Step: [5917] d_loss: 1.38718522, g_loss: 0.70193124\n",
      "Step: [5918] d_loss: 1.38669586, g_loss: 0.70376021\n",
      "Step: [5919] d_loss: 1.37976527, g_loss: 0.70450377\n",
      "Step: [5920] d_loss: 1.38245916, g_loss: 0.70178312\n",
      "Step: [5921] d_loss: 1.38595974, g_loss: 0.69413650\n",
      "Step: [5922] d_loss: 1.38637447, g_loss: 0.69848025\n",
      "Step: [5923] d_loss: 1.38639486, g_loss: 0.69721603\n",
      "Step: [5924] d_loss: 1.38809109, g_loss: 0.69500393\n",
      "Step: [5925] d_loss: 1.38291907, g_loss: 0.70115590\n",
      "Step: [5926] d_loss: 1.39041376, g_loss: 0.69354606\n",
      "Step: [5927] d_loss: 1.38702631, g_loss: 0.69761646\n",
      "Step: [5928] d_loss: 1.38792872, g_loss: 0.70191491\n",
      "Step: [5929] d_loss: 1.38694191, g_loss: 0.70496690\n",
      "Step: [5930] d_loss: 1.39144063, g_loss: 0.69712794\n",
      "Step: [5931] d_loss: 1.38238060, g_loss: 0.69481933\n",
      "Step: [5932] d_loss: 1.38755488, g_loss: 0.69395900\n",
      "Step: [5933] d_loss: 1.38887095, g_loss: 0.69480884\n",
      "Step: [5934] d_loss: 1.38862085, g_loss: 0.69843042\n",
      "Step: [5935] d_loss: 1.38464952, g_loss: 0.70267200\n",
      "Step: [5936] d_loss: 1.38775837, g_loss: 0.69650382\n",
      "Step: [5937] d_loss: 1.38767636, g_loss: 0.69925439\n",
      "Step: [5938] d_loss: 1.38975525, g_loss: 0.69665682\n",
      "Step: [5939] d_loss: 1.38205338, g_loss: 0.69855922\n",
      "Step: [5940] d_loss: 1.38274980, g_loss: 0.70162141\n",
      "Step: [5941] d_loss: 1.38219535, g_loss: 0.69516271\n",
      "Step: [5942] d_loss: 1.38544452, g_loss: 0.69852215\n",
      "Step: [5943] d_loss: 1.38971758, g_loss: 0.69027501\n",
      "Step: [5944] d_loss: 1.38375580, g_loss: 0.70144707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5945] d_loss: 1.38392997, g_loss: 0.69993579\n",
      "Step: [5946] d_loss: 1.37929022, g_loss: 0.70448470\n",
      "Step: [5947] d_loss: 1.39277220, g_loss: 0.69536865\n",
      "Step: [5948] d_loss: 1.38478792, g_loss: 0.70258397\n",
      "Step: [5949] d_loss: 1.38558161, g_loss: 0.69591367\n",
      "Step: [5950] d_loss: 1.38494360, g_loss: 0.69608581\n",
      "Step: [5951] d_loss: 1.38765395, g_loss: 0.70134592\n",
      "Step: [5952] d_loss: 1.38457251, g_loss: 0.69737649\n",
      "Step: [5953] d_loss: 1.38717866, g_loss: 0.69326621\n",
      "Step: [5954] d_loss: 1.38633215, g_loss: 0.69586068\n",
      "Step: [5955] d_loss: 1.38796282, g_loss: 0.69820571\n",
      "Step: [5956] d_loss: 1.38682473, g_loss: 0.70473433\n",
      "Step: [5957] d_loss: 1.38474607, g_loss: 0.70277762\n",
      "Step: [5958] d_loss: 1.38576090, g_loss: 0.69971883\n",
      "Step: [5959] d_loss: 1.38688266, g_loss: 0.69791782\n",
      "Step: [5960] d_loss: 1.38768125, g_loss: 0.69406605\n",
      "Step: [5961] d_loss: 1.38177407, g_loss: 0.69559962\n",
      "Step: [5962] d_loss: 1.39039803, g_loss: 0.69532788\n",
      "Step: [5963] d_loss: 1.38819528, g_loss: 0.69755334\n",
      "Step: [5964] d_loss: 1.38358426, g_loss: 0.70112932\n",
      "Step: [5965] d_loss: 1.38939452, g_loss: 0.69624299\n",
      "Step: [5966] d_loss: 1.38439441, g_loss: 0.69664860\n",
      "Step: [5967] d_loss: 1.39212036, g_loss: 0.69262713\n",
      "Step: [5968] d_loss: 1.38492393, g_loss: 0.69821632\n",
      "Step: [5969] d_loss: 1.38725340, g_loss: 0.69774044\n",
      "Step: [5970] d_loss: 1.38748944, g_loss: 0.69490123\n",
      "Step: [5971] d_loss: 1.38812685, g_loss: 0.69347614\n",
      "Step: [5972] d_loss: 1.38540697, g_loss: 0.69773185\n",
      "Step: [5973] d_loss: 1.38488281, g_loss: 0.69689536\n",
      "Step: [5974] d_loss: 1.38359952, g_loss: 0.69861418\n",
      "Step: [5975] d_loss: 1.38483262, g_loss: 0.69540232\n",
      "Step: [5976] d_loss: 1.38409925, g_loss: 0.69474506\n",
      "Step: [5977] d_loss: 1.38879824, g_loss: 0.69689918\n",
      "Step: [5978] d_loss: 1.39165092, g_loss: 0.69630343\n",
      "Step: [5979] d_loss: 1.38816738, g_loss: 0.69751763\n",
      "Step: [5980] d_loss: 1.38435841, g_loss: 0.69876021\n",
      "Step: [5981] d_loss: 1.38633108, g_loss: 0.69669056\n",
      "Step: [5982] d_loss: 1.38250673, g_loss: 0.69892234\n",
      "Step: [5983] d_loss: 1.38121986, g_loss: 0.70050734\n",
      "Step: [5984] d_loss: 1.38447642, g_loss: 0.69763410\n",
      "Step: [5985] d_loss: 1.38386631, g_loss: 0.69937003\n",
      "Step: [5986] d_loss: 1.38114619, g_loss: 0.69946086\n",
      "Step: [5987] d_loss: 1.38377619, g_loss: 0.69880074\n",
      "Step: [5988] d_loss: 1.38508844, g_loss: 0.69598722\n",
      "Step: [5989] d_loss: 1.38416946, g_loss: 0.69755858\n",
      "Step: [5990] d_loss: 1.38267708, g_loss: 0.69857323\n",
      "Step: [5991] d_loss: 1.38126087, g_loss: 0.69886857\n",
      "Step: [5992] d_loss: 1.38149881, g_loss: 0.69981635\n",
      "Step: [5993] d_loss: 1.38597608, g_loss: 0.70353949\n",
      "Step: [5994] d_loss: 1.38058996, g_loss: 0.70119357\n",
      "Step: [5995] d_loss: 1.38197565, g_loss: 0.69540751\n",
      "Step: [5996] d_loss: 1.38536239, g_loss: 0.69565153\n",
      "Step: [5997] d_loss: 1.38795960, g_loss: 0.69931763\n",
      "Step: [5998] d_loss: 1.38839960, g_loss: 0.69771051\n",
      "Step: [5999] d_loss: 1.38771117, g_loss: 0.70289981\n",
      "Step: [6000] d_loss: 1.38592660, g_loss: 0.69597131\n",
      "Step: [6001] d_loss: 1.39063323, g_loss: 0.69857717\n",
      "Step: [6002] d_loss: 1.38979745, g_loss: 0.70124197\n",
      "Step: [6003] d_loss: 1.38604128, g_loss: 0.70431721\n",
      "Step: [6004] d_loss: 1.39270425, g_loss: 0.69631898\n",
      "Step: [6005] d_loss: 1.38936257, g_loss: 0.69623029\n",
      "Step: [6006] d_loss: 1.38893127, g_loss: 0.69528604\n",
      "Step: [6007] d_loss: 1.39021897, g_loss: 0.69820541\n",
      "Step: [6008] d_loss: 1.38227701, g_loss: 0.69676256\n",
      "Step: [6009] d_loss: 1.38643062, g_loss: 0.69774354\n",
      "Step: [6010] d_loss: 1.38614988, g_loss: 0.70262706\n",
      "Step: [6011] d_loss: 1.38422024, g_loss: 0.70381123\n",
      "Step: [6012] d_loss: 1.38868344, g_loss: 0.69538695\n",
      "Step: [6013] d_loss: 1.38223732, g_loss: 0.69846106\n",
      "Step: [6014] d_loss: 1.39085126, g_loss: 0.69712341\n",
      "Step: [6015] d_loss: 1.38526356, g_loss: 0.69873154\n",
      "Step: [6016] d_loss: 1.38639998, g_loss: 0.69487810\n",
      "Step: [6017] d_loss: 1.38414478, g_loss: 0.69955754\n",
      "Step: [6018] d_loss: 1.38891065, g_loss: 0.69490814\n",
      "Step: [6019] d_loss: 1.38415456, g_loss: 0.70229948\n",
      "Step: [6020] d_loss: 1.38684535, g_loss: 0.69891959\n",
      "Step: [6021] d_loss: 1.38809800, g_loss: 0.70151389\n",
      "Step: [6022] d_loss: 1.39076054, g_loss: 0.69608068\n",
      "Step: [6023] d_loss: 1.38662291, g_loss: 0.69596922\n",
      "Step: [6024] d_loss: 1.38621724, g_loss: 0.69394851\n",
      "Step: [6025] d_loss: 1.38342023, g_loss: 0.69912398\n",
      "Step: [6026] d_loss: 1.38606334, g_loss: 0.69626200\n",
      "Step: [6027] d_loss: 1.39091992, g_loss: 0.69535422\n",
      "Step: [6028] d_loss: 1.38777041, g_loss: 0.69487572\n",
      "Step: [6029] d_loss: 1.38332975, g_loss: 0.69922692\n",
      "Step: [6030] d_loss: 1.38596725, g_loss: 0.70060658\n",
      "Step: [6031] d_loss: 1.38729978, g_loss: 0.69584394\n",
      "Step: [6032] d_loss: 1.38542676, g_loss: 0.69245708\n",
      "Step: [6033] d_loss: 1.38672149, g_loss: 0.69656783\n",
      "Step: [6034] d_loss: 1.38383555, g_loss: 0.69548118\n",
      "Step: [6035] d_loss: 1.38486314, g_loss: 0.69633710\n",
      "Step: [6036] d_loss: 1.38800669, g_loss: 0.69582254\n",
      "Step: [6037] d_loss: 1.38555527, g_loss: 0.70046967\n",
      "Step: [6038] d_loss: 1.39316201, g_loss: 0.69737560\n",
      "Step: [6039] d_loss: 1.38646185, g_loss: 0.69749337\n",
      "Step: [6040] d_loss: 1.38937616, g_loss: 0.69850057\n",
      "Step: [6041] d_loss: 1.38769639, g_loss: 0.69443882\n",
      "Step: [6042] d_loss: 1.39009643, g_loss: 0.69151187\n",
      "Step: [6043] d_loss: 1.38673925, g_loss: 0.69592154\n",
      "Step: [6044] d_loss: 1.38407016, g_loss: 0.69915986\n",
      "Step: [6045] d_loss: 1.38456583, g_loss: 0.69558185\n",
      "Step: [6046] d_loss: 1.38808286, g_loss: 0.69401348\n",
      "Step: [6047] d_loss: 1.38787150, g_loss: 0.69726992\n",
      "Step: [6048] d_loss: 1.39138281, g_loss: 0.69495440\n",
      "Step: [6049] d_loss: 1.38499236, g_loss: 0.69376981\n",
      "Step: [6050] d_loss: 1.38786077, g_loss: 0.69538605\n",
      "Step: [6051] d_loss: 1.38814259, g_loss: 0.69751751\n",
      "Step: [6052] d_loss: 1.38565385, g_loss: 0.69629723\n",
      "Step: [6053] d_loss: 1.38882124, g_loss: 0.69498068\n",
      "Step: [6054] d_loss: 1.38786495, g_loss: 0.69832361\n",
      "Step: [6055] d_loss: 1.38793588, g_loss: 0.69516993\n",
      "Step: [6056] d_loss: 1.38664520, g_loss: 0.69613004\n",
      "Step: [6057] d_loss: 1.38426650, g_loss: 0.69735014\n",
      "Step: [6058] d_loss: 1.38739467, g_loss: 0.69632661\n",
      "Step: [6059] d_loss: 1.38626552, g_loss: 0.70041037\n",
      "Step: [6060] d_loss: 1.38803387, g_loss: 0.69903839\n",
      "Step: [6061] d_loss: 1.38738394, g_loss: 0.69682992\n",
      "Step: [6062] d_loss: 1.38788629, g_loss: 0.69457936\n",
      "Step: [6063] d_loss: 1.38680506, g_loss: 0.69426620\n",
      "Step: [6064] d_loss: 1.38655794, g_loss: 0.69606268\n",
      "Step: [6065] d_loss: 1.38812304, g_loss: 0.69819832\n",
      "Step: [6066] d_loss: 1.38775659, g_loss: 0.69965529\n",
      "Step: [6067] d_loss: 1.38578916, g_loss: 0.70148903\n",
      "Step: [6068] d_loss: 1.38420963, g_loss: 0.69946778\n",
      "Step: [6069] d_loss: 1.38420093, g_loss: 0.69853067\n",
      "Step: [6070] d_loss: 1.38452077, g_loss: 0.69624346\n",
      "Step: [6071] d_loss: 1.38437653, g_loss: 0.69489515\n",
      "Step: [6072] d_loss: 1.38541794, g_loss: 0.69660163\n",
      "Step: [6073] d_loss: 1.38716412, g_loss: 0.69492954\n",
      "Step: [6074] d_loss: 1.38203239, g_loss: 0.70161408\n",
      "Step: [6075] d_loss: 1.38324237, g_loss: 0.69850791\n",
      "Step: [6076] d_loss: 1.38464880, g_loss: 0.69935673\n",
      "Step: [6077] d_loss: 1.38659191, g_loss: 0.69517255\n",
      "Step: [6078] d_loss: 1.39118862, g_loss: 0.69234675\n",
      "Step: [6079] d_loss: 1.38485336, g_loss: 0.69718200\n",
      "Step: [6080] d_loss: 1.38783026, g_loss: 0.69637346\n",
      "Step: [6081] d_loss: 1.38905358, g_loss: 0.69499660\n",
      "Step: [6082] d_loss: 1.38684154, g_loss: 0.69403642\n",
      "Step: [6083] d_loss: 1.38871896, g_loss: 0.69394106\n",
      "Step: [6084] d_loss: 1.38520253, g_loss: 0.69660568\n",
      "Step: [6085] d_loss: 1.38078427, g_loss: 0.70003545\n",
      "Step: [6086] d_loss: 1.38644493, g_loss: 0.69593132\n",
      "Step: [6087] d_loss: 1.38407302, g_loss: 0.69694752\n",
      "Step: [6088] d_loss: 1.38830876, g_loss: 0.69390541\n",
      "Step: [6089] d_loss: 1.38527703, g_loss: 0.69394577\n",
      "Step: [6090] d_loss: 1.38504958, g_loss: 0.69779909\n",
      "Step: [6091] d_loss: 1.38649690, g_loss: 0.69732630\n",
      "Step: [6092] d_loss: 1.38745272, g_loss: 0.69631720\n",
      "Step: [6093] d_loss: 1.38698912, g_loss: 0.69264638\n",
      "Step: [6094] d_loss: 1.38692188, g_loss: 0.69406849\n",
      "Step: [6095] d_loss: 1.38753819, g_loss: 0.69705153\n",
      "Step: [6096] d_loss: 1.38785744, g_loss: 0.69254327\n",
      "Step: [6097] d_loss: 1.38844216, g_loss: 0.69709909\n",
      "Step: [6098] d_loss: 1.38898325, g_loss: 0.69767642\n",
      "Step: [6099] d_loss: 1.38615644, g_loss: 0.69611973\n",
      "Step: [6100] d_loss: 1.38724232, g_loss: 0.69852751\n",
      "Step: [6101] d_loss: 1.39016354, g_loss: 0.69709265\n",
      "Step: [6102] d_loss: 1.38794208, g_loss: 0.69723988\n",
      "Step: [6103] d_loss: 1.38550353, g_loss: 0.69658685\n",
      "Step: [6104] d_loss: 1.38755679, g_loss: 0.69560826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6105] d_loss: 1.38673317, g_loss: 0.70029300\n",
      "Step: [6106] d_loss: 1.38915610, g_loss: 0.69290590\n",
      "Step: [6107] d_loss: 1.38639593, g_loss: 0.69406426\n",
      "Step: [6108] d_loss: 1.38688684, g_loss: 0.69461250\n",
      "Step: [6109] d_loss: 1.38593733, g_loss: 0.69355261\n",
      "Step: [6110] d_loss: 1.38823152, g_loss: 0.69739640\n",
      "Step: [6111] d_loss: 1.38558543, g_loss: 0.69813740\n",
      "Step: [6112] d_loss: 1.38562334, g_loss: 0.69564509\n",
      "Step: [6113] d_loss: 1.38251662, g_loss: 0.70119870\n",
      "Step: [6114] d_loss: 1.38680840, g_loss: 0.69602096\n",
      "Step: [6115] d_loss: 1.38591611, g_loss: 0.69553769\n",
      "Step: [6116] d_loss: 1.38735962, g_loss: 0.69848353\n",
      "Step: [6117] d_loss: 1.38554645, g_loss: 0.69497329\n",
      "Step: [6118] d_loss: 1.38225567, g_loss: 0.69596595\n",
      "Step: [6119] d_loss: 1.38466883, g_loss: 0.69621760\n",
      "Step: [6120] d_loss: 1.38531220, g_loss: 0.69618785\n",
      "Step: [6121] d_loss: 1.38326883, g_loss: 0.70098686\n",
      "Step: [6122] d_loss: 1.38261628, g_loss: 0.69962919\n",
      "Step: [6123] d_loss: 1.38702369, g_loss: 0.70054233\n",
      "Step: [6124] d_loss: 1.38585138, g_loss: 0.70358312\n",
      "Step: [6125] d_loss: 1.38247442, g_loss: 0.69974887\n",
      "Step: [6126] d_loss: 1.38739276, g_loss: 0.69161785\n",
      "Step: [6127] d_loss: 1.38615203, g_loss: 0.69561231\n",
      "Step: [6128] d_loss: 1.38556373, g_loss: 0.69770563\n",
      "Step: [6129] d_loss: 1.38227832, g_loss: 0.69649708\n",
      "Step: [6130] d_loss: 1.38569832, g_loss: 0.69714171\n",
      "Step: [6131] d_loss: 1.38223457, g_loss: 0.70041835\n",
      "Step: [6132] d_loss: 1.38510966, g_loss: 0.69664133\n",
      "Step: [6133] d_loss: 1.38526249, g_loss: 0.69888878\n",
      "Step: [6134] d_loss: 1.38507068, g_loss: 0.69471359\n",
      "Step: [6135] d_loss: 1.38847756, g_loss: 0.69416958\n",
      "Step: [6136] d_loss: 1.38297367, g_loss: 0.69618732\n",
      "Step: [6137] d_loss: 1.38473511, g_loss: 0.69560540\n",
      "Step: [6138] d_loss: 1.38268578, g_loss: 0.69793433\n",
      "Step: [6139] d_loss: 1.38622344, g_loss: 0.69714099\n",
      "Step: [6140] d_loss: 1.38953197, g_loss: 0.69568908\n",
      "Step: [6141] d_loss: 1.38278878, g_loss: 0.69495094\n",
      "Step: [6142] d_loss: 1.38369632, g_loss: 0.69791508\n",
      "Step: [6143] d_loss: 1.38514590, g_loss: 0.69820619\n",
      "Step: [6144] d_loss: 1.37954640, g_loss: 0.70027518\n",
      "Step: [6145] d_loss: 1.38819408, g_loss: 0.69677478\n",
      "Step: [6146] d_loss: 1.38365817, g_loss: 0.69419670\n",
      "Step: [6147] d_loss: 1.38336205, g_loss: 0.70072174\n",
      "Step: [6148] d_loss: 1.38603485, g_loss: 0.69934410\n",
      "Step: [6149] d_loss: 1.38890624, g_loss: 0.69598079\n",
      "Step: [6150] d_loss: 1.38846672, g_loss: 0.69761848\n",
      "Step: [6151] d_loss: 1.38578272, g_loss: 0.69365251\n",
      "Step: [6152] d_loss: 1.38650811, g_loss: 0.69752383\n",
      "Step: [6153] d_loss: 1.38501954, g_loss: 0.69469023\n",
      "Step: [6154] d_loss: 1.38339996, g_loss: 0.69947487\n",
      "Step: [6155] d_loss: 1.38862979, g_loss: 0.69874215\n",
      "Step: [6156] d_loss: 1.38359976, g_loss: 0.69887823\n",
      "Step: [6157] d_loss: 1.38684785, g_loss: 0.69796503\n",
      "Step: [6158] d_loss: 1.38480878, g_loss: 0.69550180\n",
      "Step: [6159] d_loss: 1.38833332, g_loss: 0.69838035\n",
      "Step: [6160] d_loss: 1.38529301, g_loss: 0.69957942\n",
      "Step: [6161] d_loss: 1.38577485, g_loss: 0.69726747\n",
      "Step: [6162] d_loss: 1.38279927, g_loss: 0.69616914\n",
      "Step: [6163] d_loss: 1.38224375, g_loss: 0.69900888\n",
      "Step: [6164] d_loss: 1.38455009, g_loss: 0.70182765\n",
      "Step: [6165] d_loss: 1.38697946, g_loss: 0.69878745\n",
      "Step: [6166] d_loss: 1.38750052, g_loss: 0.69644809\n",
      "Step: [6167] d_loss: 1.38500714, g_loss: 0.69843620\n",
      "Step: [6168] d_loss: 1.38846314, g_loss: 0.70031577\n",
      "Step: [6169] d_loss: 1.38010573, g_loss: 0.70923746\n",
      "Step: [6170] d_loss: 1.38711798, g_loss: 0.69730258\n",
      "Step: [6171] d_loss: 1.38574708, g_loss: 0.69898957\n",
      "Step: [6172] d_loss: 1.38253140, g_loss: 0.70601696\n",
      "Step: [6173] d_loss: 1.38817382, g_loss: 0.69968301\n",
      "Step: [6174] d_loss: 1.38980138, g_loss: 0.70020258\n",
      "Step: [6175] d_loss: 1.38165951, g_loss: 0.69997907\n",
      "Step: [6176] d_loss: 1.38289261, g_loss: 0.70094967\n",
      "Step: [6177] d_loss: 1.38197017, g_loss: 0.69970191\n",
      "Step: [6178] d_loss: 1.38888168, g_loss: 0.69634342\n",
      "Step: [6179] d_loss: 1.38542151, g_loss: 0.69933999\n",
      "Step: [6180] d_loss: 1.38173711, g_loss: 0.69672447\n",
      "Step: [6181] d_loss: 1.37977910, g_loss: 0.69982016\n",
      "Step: [6182] d_loss: 1.38347208, g_loss: 0.69665205\n",
      "Step: [6183] d_loss: 1.39111781, g_loss: 0.69527555\n",
      "Step: [6184] d_loss: 1.38558197, g_loss: 0.69861794\n",
      "Step: [6185] d_loss: 1.39385796, g_loss: 0.69005895\n",
      "Step: [6186] d_loss: 1.38904405, g_loss: 0.69557655\n",
      "Step: [6187] d_loss: 1.38414967, g_loss: 0.69729644\n",
      "Step: [6188] d_loss: 1.39015567, g_loss: 0.69548291\n",
      "Step: [6189] d_loss: 1.38648903, g_loss: 0.70019710\n",
      "Step: [6190] d_loss: 1.39044452, g_loss: 0.69805360\n",
      "Step: [6191] d_loss: 1.38446450, g_loss: 0.69784248\n",
      "Step: [6192] d_loss: 1.38331103, g_loss: 0.69922608\n",
      "Step: [6193] d_loss: 1.39000344, g_loss: 0.69384462\n",
      "Step: [6194] d_loss: 1.38987708, g_loss: 0.69603252\n",
      "Step: [6195] d_loss: 1.38456476, g_loss: 0.69617563\n",
      "Step: [6196] d_loss: 1.38948607, g_loss: 0.69678462\n",
      "Step: [6197] d_loss: 1.38176906, g_loss: 0.69922620\n",
      "Step: [6198] d_loss: 1.38596368, g_loss: 0.69361222\n",
      "Step: [6199] d_loss: 1.38618779, g_loss: 0.69761443\n",
      "Step: [6200] d_loss: 1.38558817, g_loss: 0.69793892\n",
      "Step: [6201] d_loss: 1.38798022, g_loss: 0.70147836\n",
      "Step: [6202] d_loss: 1.38992620, g_loss: 0.69759655\n",
      "Step: [6203] d_loss: 1.38684440, g_loss: 0.69629025\n",
      "Step: [6204] d_loss: 1.37997484, g_loss: 0.69896317\n",
      "Step: [6205] d_loss: 1.37874556, g_loss: 0.70277584\n",
      "Step: [6206] d_loss: 1.38316369, g_loss: 0.69748521\n",
      "Step: [6207] d_loss: 1.39593208, g_loss: 0.68906981\n",
      "Step: [6208] d_loss: 1.38426089, g_loss: 0.70171517\n",
      "Step: [6209] d_loss: 1.38976300, g_loss: 0.70223606\n",
      "Step: [6210] d_loss: 1.39185345, g_loss: 0.70106989\n",
      "Step: [6211] d_loss: 1.38676322, g_loss: 0.70296299\n",
      "Step: [6212] d_loss: 1.39313555, g_loss: 0.69079697\n",
      "Step: [6213] d_loss: 1.38955343, g_loss: 0.69451857\n",
      "Step: [6214] d_loss: 1.38630891, g_loss: 0.69443190\n",
      "Step: [6215] d_loss: 1.38659930, g_loss: 0.69565171\n",
      "Step: [6216] d_loss: 1.38361359, g_loss: 0.70415270\n",
      "Step: [6217] d_loss: 1.39090931, g_loss: 0.70225447\n",
      "Step: [6218] d_loss: 1.38588881, g_loss: 0.70365721\n",
      "Step: [6219] d_loss: 1.38876891, g_loss: 0.69571042\n",
      "Step: [6220] d_loss: 1.38697398, g_loss: 0.69536150\n",
      "Step: [6221] d_loss: 1.38444734, g_loss: 0.69459587\n",
      "Step: [6222] d_loss: 1.38634503, g_loss: 0.69444484\n",
      "Step: [6223] d_loss: 1.38518298, g_loss: 0.70505989\n",
      "Step: [6224] d_loss: 1.38729668, g_loss: 0.69974691\n",
      "Step: [6225] d_loss: 1.38994038, g_loss: 0.70061684\n",
      "Step: [6226] d_loss: 1.38507628, g_loss: 0.69536138\n",
      "Step: [6227] d_loss: 1.38386726, g_loss: 0.69401324\n",
      "Step: [6228] d_loss: 1.38699698, g_loss: 0.69660771\n",
      "Step: [6229] d_loss: 1.38433981, g_loss: 0.69972515\n",
      "Step: [6230] d_loss: 1.38446736, g_loss: 0.70345664\n",
      "Step: [6231] d_loss: 1.38269591, g_loss: 0.69795495\n",
      "Step: [6232] d_loss: 1.38351035, g_loss: 0.69904482\n",
      "Step: [6233] d_loss: 1.38523006, g_loss: 0.69202453\n",
      "Step: [6234] d_loss: 1.38500214, g_loss: 0.69555521\n",
      "Step: [6235] d_loss: 1.38594079, g_loss: 0.70055878\n",
      "Step: [6236] d_loss: 1.39304638, g_loss: 0.69708616\n",
      "Step: [6237] d_loss: 1.38691521, g_loss: 0.69879317\n",
      "Step: [6238] d_loss: 1.38849926, g_loss: 0.69280624\n",
      "Step: [6239] d_loss: 1.39063072, g_loss: 0.69071519\n",
      "Step: [6240] d_loss: 1.38596535, g_loss: 0.69908047\n",
      "Step: [6241] d_loss: 1.38808298, g_loss: 0.69776380\n",
      "Step: [6242] d_loss: 1.38636982, g_loss: 0.69839072\n",
      "Step: [6243] d_loss: 1.38651896, g_loss: 0.69489866\n",
      "Step: [6244] d_loss: 1.38852191, g_loss: 0.69501948\n",
      "Step: [6245] d_loss: 1.38979936, g_loss: 0.69300079\n",
      "Step: [6246] d_loss: 1.38625407, g_loss: 0.70074725\n",
      "Step: [6247] d_loss: 1.38373744, g_loss: 0.69824886\n",
      "Step: [6248] d_loss: 1.39009976, g_loss: 0.69675219\n",
      "Step: [6249] d_loss: 1.38473833, g_loss: 0.69752276\n",
      "Step: [6250] d_loss: 1.38802171, g_loss: 0.69212615\n",
      "Step: [6251] d_loss: 1.39228439, g_loss: 0.69413346\n",
      "Step: [6252] d_loss: 1.38880622, g_loss: 0.69349408\n",
      "Step: [6253] d_loss: 1.38706672, g_loss: 0.69935906\n",
      "Step: [6254] d_loss: 1.38191783, g_loss: 0.69603908\n",
      "Step: [6255] d_loss: 1.38144350, g_loss: 0.69651586\n",
      "Step: [6256] d_loss: 1.38383198, g_loss: 0.69634759\n",
      "Step: [6257] d_loss: 1.38338661, g_loss: 0.69819933\n",
      "Step: [6258] d_loss: 1.38226962, g_loss: 0.70274448\n",
      "Step: [6259] d_loss: 1.38180602, g_loss: 0.69357926\n",
      "Step: [6260] d_loss: 1.38111401, g_loss: 0.70217013\n",
      "Step: [6261] d_loss: 1.38739347, g_loss: 0.70304120\n",
      "Step: [6262] d_loss: 1.37995195, g_loss: 0.69972271\n",
      "Step: [6263] d_loss: 1.38323665, g_loss: 0.69394249\n",
      "Step: [6264] d_loss: 1.38592386, g_loss: 0.69887900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6265] d_loss: 1.38691831, g_loss: 0.69891495\n",
      "Step: [6266] d_loss: 1.38238525, g_loss: 0.69856048\n",
      "Step: [6267] d_loss: 1.38477337, g_loss: 0.69743097\n",
      "Step: [6268] d_loss: 1.38824856, g_loss: 0.69830954\n",
      "Step: [6269] d_loss: 1.38774562, g_loss: 0.69946128\n",
      "Step: [6270] d_loss: 1.39070892, g_loss: 0.69619250\n",
      "Step: [6271] d_loss: 1.38720155, g_loss: 0.69917470\n",
      "Step: [6272] d_loss: 1.38767755, g_loss: 0.69860941\n",
      "Step: [6273] d_loss: 1.38434196, g_loss: 0.69805866\n",
      "Step: [6274] d_loss: 1.38814092, g_loss: 0.69602656\n",
      "Step: [6275] d_loss: 1.38837481, g_loss: 0.69341862\n",
      "Step: [6276] d_loss: 1.38590133, g_loss: 0.69362736\n",
      "Step: [6277] d_loss: 1.38499236, g_loss: 0.70253491\n",
      "Step: [6278] d_loss: 1.38753581, g_loss: 0.70186388\n",
      "Step: [6279] d_loss: 1.38763988, g_loss: 0.70010591\n",
      "Step: [6280] d_loss: 1.38448203, g_loss: 0.70043671\n",
      "Step: [6281] d_loss: 1.38260746, g_loss: 0.70084018\n",
      "Step: [6282] d_loss: 1.38105774, g_loss: 0.69671816\n",
      "Step: [6283] d_loss: 1.38708997, g_loss: 0.69318813\n",
      "Step: [6284] d_loss: 1.38947821, g_loss: 0.69723296\n",
      "Step: [6285] d_loss: 1.38796544, g_loss: 0.69788712\n",
      "Step: [6286] d_loss: 1.38620770, g_loss: 0.69860244\n",
      "Step: [6287] d_loss: 1.38833928, g_loss: 0.69831777\n",
      "Step: [6288] d_loss: 1.38598073, g_loss: 0.69301182\n",
      "Step: [6289] d_loss: 1.38849425, g_loss: 0.69446594\n",
      "Step: [6290] d_loss: 1.38810849, g_loss: 0.70058787\n",
      "Step: [6291] d_loss: 1.38590181, g_loss: 0.69964230\n",
      "Step: [6292] d_loss: 1.38352907, g_loss: 0.69636726\n",
      "Step: [6293] d_loss: 1.38361788, g_loss: 0.69876432\n",
      "Step: [6294] d_loss: 1.38787127, g_loss: 0.69427502\n",
      "Step: [6295] d_loss: 1.38415027, g_loss: 0.69597077\n",
      "Step: [6296] d_loss: 1.38784659, g_loss: 0.69425666\n",
      "Step: [6297] d_loss: 1.38765335, g_loss: 0.69462043\n",
      "Step: [6298] d_loss: 1.38531828, g_loss: 0.70184261\n",
      "Step: [6299] d_loss: 1.38635540, g_loss: 0.69736350\n",
      "Step: [6300] d_loss: 1.38630307, g_loss: 0.69546461\n",
      "Step: [6301] d_loss: 1.38726676, g_loss: 0.69958508\n",
      "Step: [6302] d_loss: 1.38754988, g_loss: 0.69578099\n",
      "Step: [6303] d_loss: 1.38392591, g_loss: 0.69752932\n",
      "Step: [6304] d_loss: 1.38715351, g_loss: 0.69424939\n",
      "Step: [6305] d_loss: 1.38347864, g_loss: 0.69463849\n",
      "Step: [6306] d_loss: 1.38521469, g_loss: 0.69785774\n",
      "Step: [6307] d_loss: 1.38872278, g_loss: 0.69584489\n",
      "Step: [6308] d_loss: 1.38621080, g_loss: 0.69973201\n",
      "Step: [6309] d_loss: 1.38155138, g_loss: 0.69987434\n",
      "Step: [6310] d_loss: 1.38449216, g_loss: 0.69918430\n",
      "Step: [6311] d_loss: 1.38526106, g_loss: 0.69790339\n",
      "Step: [6312] d_loss: 1.38575315, g_loss: 0.69465721\n",
      "Step: [6313] d_loss: 1.38442349, g_loss: 0.69719970\n",
      "Step: [6314] d_loss: 1.38334846, g_loss: 0.69811565\n",
      "Step: [6315] d_loss: 1.38129115, g_loss: 0.69869196\n",
      "Step: [6316] d_loss: 1.38346636, g_loss: 0.69699836\n",
      "Step: [6317] d_loss: 1.39018226, g_loss: 0.69981778\n",
      "Step: [6318] d_loss: 1.38490975, g_loss: 0.69962728\n",
      "Step: [6319] d_loss: 1.38695121, g_loss: 0.69528872\n",
      "Step: [6320] d_loss: 1.38777995, g_loss: 0.69643939\n",
      "Step: [6321] d_loss: 1.38615966, g_loss: 0.69306654\n",
      "Step: [6322] d_loss: 1.38663280, g_loss: 0.69447172\n",
      "Step: [6323] d_loss: 1.38800502, g_loss: 0.69639480\n",
      "Step: [6324] d_loss: 1.38985765, g_loss: 0.69676602\n",
      "Step: [6325] d_loss: 1.38315094, g_loss: 0.69751859\n",
      "Step: [6326] d_loss: 1.38768137, g_loss: 0.69589996\n",
      "Step: [6327] d_loss: 1.38802195, g_loss: 0.69685662\n",
      "Step: [6328] d_loss: 1.38620365, g_loss: 0.69983208\n",
      "Step: [6329] d_loss: 1.39152169, g_loss: 0.69562095\n",
      "Step: [6330] d_loss: 1.39002573, g_loss: 0.69559491\n",
      "Step: [6331] d_loss: 1.38756478, g_loss: 0.69701922\n",
      "Step: [6332] d_loss: 1.38476503, g_loss: 0.69735801\n",
      "Step: [6333] d_loss: 1.38680792, g_loss: 0.69650286\n",
      "Step: [6334] d_loss: 1.38908887, g_loss: 0.69449693\n",
      "Step: [6335] d_loss: 1.38891959, g_loss: 0.69565088\n",
      "Step: [6336] d_loss: 1.38439989, g_loss: 0.69652885\n",
      "Step: [6337] d_loss: 1.38521051, g_loss: 0.69936109\n",
      "Step: [6338] d_loss: 1.38643551, g_loss: 0.69674140\n",
      "Step: [6339] d_loss: 1.38686848, g_loss: 0.69483900\n",
      "Step: [6340] d_loss: 1.38641047, g_loss: 0.69238228\n",
      "Step: [6341] d_loss: 1.38340604, g_loss: 0.69705498\n",
      "Step: [6342] d_loss: 1.38243520, g_loss: 0.69935602\n",
      "Step: [6343] d_loss: 1.38665283, g_loss: 0.69686842\n",
      "Step: [6344] d_loss: 1.38559461, g_loss: 0.69548887\n",
      "Step: [6345] d_loss: 1.38418424, g_loss: 0.69945610\n",
      "Step: [6346] d_loss: 1.38300180, g_loss: 0.69576865\n",
      "Step: [6347] d_loss: 1.38665199, g_loss: 0.69739687\n",
      "Step: [6348] d_loss: 1.38408685, g_loss: 0.69945008\n",
      "Step: [6349] d_loss: 1.38909948, g_loss: 0.69428384\n",
      "Step: [6350] d_loss: 1.38539267, g_loss: 0.69425058\n",
      "Step: [6351] d_loss: 1.38591313, g_loss: 0.69427091\n",
      "Step: [6352] d_loss: 1.38537133, g_loss: 0.69039571\n",
      "Step: [6353] d_loss: 1.38059425, g_loss: 0.69919664\n",
      "Step: [6354] d_loss: 1.38568616, g_loss: 0.69629240\n",
      "Step: [6355] d_loss: 1.38854408, g_loss: 0.69673306\n",
      "Step: [6356] d_loss: 1.38560557, g_loss: 0.69535023\n",
      "Step: [6357] d_loss: 1.38534331, g_loss: 0.69448996\n",
      "Step: [6358] d_loss: 1.38527918, g_loss: 0.69481456\n",
      "Step: [6359] d_loss: 1.38666606, g_loss: 0.69538450\n",
      "Step: [6360] d_loss: 1.38808572, g_loss: 0.69602758\n",
      "Step: [6361] d_loss: 1.38873839, g_loss: 0.69490445\n",
      "Step: [6362] d_loss: 1.38689399, g_loss: 0.69700480\n",
      "Step: [6363] d_loss: 1.38612247, g_loss: 0.69869900\n",
      "Step: [6364] d_loss: 1.39046860, g_loss: 0.69303608\n",
      "Step: [6365] d_loss: 1.38509583, g_loss: 0.69677281\n",
      "Step: [6366] d_loss: 1.38510752, g_loss: 0.69862390\n",
      "Step: [6367] d_loss: 1.38850856, g_loss: 0.69482303\n",
      "Step: [6368] d_loss: 1.39014339, g_loss: 0.69299215\n",
      "Step: [6369] d_loss: 1.38773608, g_loss: 0.69586945\n",
      "Step: [6370] d_loss: 1.38463998, g_loss: 0.70041585\n",
      "Step: [6371] d_loss: 1.38265896, g_loss: 0.69749367\n",
      "Step: [6372] d_loss: 1.38888228, g_loss: 0.69326544\n",
      "Step: [6373] d_loss: 1.38588428, g_loss: 0.69528639\n",
      "Step: [6374] d_loss: 1.38619196, g_loss: 0.69836909\n",
      "Step: [6375] d_loss: 1.38444018, g_loss: 0.69532132\n",
      "Step: [6376] d_loss: 1.38503885, g_loss: 0.69890672\n",
      "Step: [6377] d_loss: 1.38416803, g_loss: 0.69747275\n",
      "Step: [6378] d_loss: 1.38346577, g_loss: 0.69838464\n",
      "Step: [6379] d_loss: 1.38802719, g_loss: 0.69273072\n",
      "Step: [6380] d_loss: 1.38794947, g_loss: 0.69545710\n",
      "Step: [6381] d_loss: 1.38489878, g_loss: 0.69550866\n",
      "Step: [6382] d_loss: 1.38663328, g_loss: 0.69560289\n",
      "Step: [6383] d_loss: 1.38496947, g_loss: 0.69720268\n",
      "Step: [6384] d_loss: 1.38828838, g_loss: 0.69570804\n",
      "Step: [6385] d_loss: 1.38617241, g_loss: 0.69660872\n",
      "Step: [6386] d_loss: 1.38794911, g_loss: 0.69620168\n",
      "Step: [6387] d_loss: 1.38293386, g_loss: 0.69704342\n",
      "Step: [6388] d_loss: 1.38730311, g_loss: 0.69561535\n",
      "Step: [6389] d_loss: 1.38603723, g_loss: 0.69454992\n",
      "Step: [6390] d_loss: 1.38509834, g_loss: 0.69636726\n",
      "Step: [6391] d_loss: 1.38269544, g_loss: 0.69879830\n",
      "Step: [6392] d_loss: 1.38946342, g_loss: 0.69677341\n",
      "Step: [6393] d_loss: 1.38227558, g_loss: 0.70103061\n",
      "Step: [6394] d_loss: 1.38828850, g_loss: 0.69806665\n",
      "Step: [6395] d_loss: 1.38262546, g_loss: 0.69749629\n",
      "Step: [6396] d_loss: 1.38571644, g_loss: 0.69542807\n",
      "Step: [6397] d_loss: 1.38533616, g_loss: 0.69586396\n",
      "Step: [6398] d_loss: 1.38472199, g_loss: 0.69560015\n",
      "Step: [6399] d_loss: 1.38299727, g_loss: 0.69846773\n",
      "Step: [6400] d_loss: 1.38319325, g_loss: 0.69810796\n",
      "Step: [6401] d_loss: 1.38257778, g_loss: 0.70140541\n",
      "Step: [6402] d_loss: 1.38962734, g_loss: 0.69292313\n",
      "Step: [6403] d_loss: 1.38238358, g_loss: 0.69682801\n",
      "Step: [6404] d_loss: 1.38309479, g_loss: 0.69745231\n",
      "Step: [6405] d_loss: 1.38795769, g_loss: 0.69632840\n",
      "Step: [6406] d_loss: 1.38650048, g_loss: 0.69626403\n",
      "Step: [6407] d_loss: 1.38647187, g_loss: 0.69707274\n",
      "Step: [6408] d_loss: 1.38814533, g_loss: 0.69269282\n",
      "Step: [6409] d_loss: 1.38215804, g_loss: 0.69900060\n",
      "Step: [6410] d_loss: 1.38685858, g_loss: 0.70041746\n",
      "Step: [6411] d_loss: 1.38848758, g_loss: 0.69176567\n",
      "Step: [6412] d_loss: 1.38348961, g_loss: 0.69648033\n",
      "Step: [6413] d_loss: 1.38761830, g_loss: 0.69619548\n",
      "Step: [6414] d_loss: 1.38490748, g_loss: 0.69572806\n",
      "Step: [6415] d_loss: 1.38413405, g_loss: 0.69915676\n",
      "Step: [6416] d_loss: 1.38798690, g_loss: 0.69774526\n",
      "Step: [6417] d_loss: 1.38540924, g_loss: 0.69660449\n",
      "Step: [6418] d_loss: 1.38792467, g_loss: 0.69483399\n",
      "Step: [6419] d_loss: 1.39360750, g_loss: 0.69452703\n",
      "Step: [6420] d_loss: 1.39061689, g_loss: 0.69196498\n",
      "Step: [6421] d_loss: 1.38861001, g_loss: 0.69309944\n",
      "Step: [6422] d_loss: 1.38505769, g_loss: 0.69822031\n",
      "Step: [6423] d_loss: 1.38495374, g_loss: 0.69631898\n",
      "Step: [6424] d_loss: 1.38475275, g_loss: 0.69674009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6425] d_loss: 1.38855529, g_loss: 0.69586754\n",
      "Step: [6426] d_loss: 1.38356304, g_loss: 0.69526309\n",
      "Step: [6427] d_loss: 1.38204670, g_loss: 0.69754398\n",
      "Step: [6428] d_loss: 1.38875782, g_loss: 0.69646972\n",
      "Step: [6429] d_loss: 1.38510036, g_loss: 0.69653422\n",
      "Step: [6430] d_loss: 1.38654232, g_loss: 0.69503826\n",
      "Step: [6431] d_loss: 1.38795280, g_loss: 0.69352889\n",
      "Step: [6432] d_loss: 1.38827550, g_loss: 0.69199079\n",
      "Step: [6433] d_loss: 1.38774765, g_loss: 0.69776976\n",
      "Step: [6434] d_loss: 1.38888144, g_loss: 0.69694525\n",
      "Step: [6435] d_loss: 1.38936877, g_loss: 0.69000858\n",
      "Step: [6436] d_loss: 1.38643956, g_loss: 0.70198619\n",
      "Step: [6437] d_loss: 1.38737130, g_loss: 0.70029181\n",
      "Step: [6438] d_loss: 1.38718295, g_loss: 0.70177126\n",
      "Step: [6439] d_loss: 1.38750172, g_loss: 0.69874519\n",
      "Step: [6440] d_loss: 1.38573742, g_loss: 0.69725639\n",
      "Step: [6441] d_loss: 1.38259423, g_loss: 0.69857460\n",
      "Step: [6442] d_loss: 1.39050710, g_loss: 0.69721359\n",
      "Step: [6443] d_loss: 1.38737357, g_loss: 0.70038080\n",
      "Step: [6444] d_loss: 1.38466084, g_loss: 0.70092487\n",
      "Step: [6445] d_loss: 1.38952506, g_loss: 0.69256520\n",
      "Step: [6446] d_loss: 1.38267303, g_loss: 0.70134246\n",
      "Step: [6447] d_loss: 1.38432539, g_loss: 0.70102268\n",
      "Step: [6448] d_loss: 1.38821793, g_loss: 0.69678968\n",
      "Step: [6449] d_loss: 1.38640380, g_loss: 0.69765109\n",
      "Step: [6450] d_loss: 1.38321257, g_loss: 0.69514787\n",
      "Step: [6451] d_loss: 1.38557386, g_loss: 0.69437677\n",
      "Step: [6452] d_loss: 1.38449299, g_loss: 0.69659781\n",
      "Step: [6453] d_loss: 1.38665533, g_loss: 0.69860566\n",
      "Step: [6454] d_loss: 1.38728070, g_loss: 0.69376183\n",
      "Step: [6455] d_loss: 1.38758445, g_loss: 0.69358855\n",
      "Step: [6456] d_loss: 1.38623071, g_loss: 0.69782865\n",
      "Step: [6457] d_loss: 1.38712025, g_loss: 0.69814730\n",
      "Step: [6458] d_loss: 1.38213420, g_loss: 0.70111912\n",
      "Step: [6459] d_loss: 1.38736057, g_loss: 0.69656599\n",
      "Step: [6460] d_loss: 1.38707459, g_loss: 0.69839358\n",
      "Step: [6461] d_loss: 1.38821054, g_loss: 0.69450736\n",
      "Step: [6462] d_loss: 1.38629341, g_loss: 0.69597435\n",
      "Step: [6463] d_loss: 1.38333774, g_loss: 0.69647872\n",
      "Step: [6464] d_loss: 1.38933384, g_loss: 0.69090521\n",
      "Step: [6465] d_loss: 1.38719416, g_loss: 0.69696760\n",
      "Step: [6466] d_loss: 1.38644540, g_loss: 0.69499582\n",
      "Step: [6467] d_loss: 1.38821948, g_loss: 0.69475651\n",
      "Step: [6468] d_loss: 1.38517845, g_loss: 0.69606274\n",
      "Step: [6469] d_loss: 1.38556790, g_loss: 0.69380474\n",
      "Step: [6470] d_loss: 1.38415849, g_loss: 0.69728208\n",
      "Step: [6471] d_loss: 1.38608861, g_loss: 0.69616675\n",
      "Step: [6472] d_loss: 1.38838887, g_loss: 0.69543695\n",
      "Step: [6473] d_loss: 1.38625002, g_loss: 0.70066273\n",
      "Step: [6474] d_loss: 1.38708758, g_loss: 0.69556844\n",
      "Step: [6475] d_loss: 1.38626766, g_loss: 0.69729310\n",
      "Step: [6476] d_loss: 1.38708603, g_loss: 0.69447517\n",
      "Step: [6477] d_loss: 1.38733292, g_loss: 0.69544262\n",
      "Step: [6478] d_loss: 1.38618302, g_loss: 0.69561648\n",
      "Step: [6479] d_loss: 1.38925934, g_loss: 0.69800901\n",
      "Step: [6480] d_loss: 1.38322735, g_loss: 0.69844711\n",
      "Step: [6481] d_loss: 1.38836277, g_loss: 0.69314259\n",
      "Step: [6482] d_loss: 1.38628650, g_loss: 0.69113880\n",
      "Step: [6483] d_loss: 1.38710678, g_loss: 0.69489568\n",
      "Step: [6484] d_loss: 1.38684952, g_loss: 0.69625914\n",
      "Step: [6485] d_loss: 1.38589263, g_loss: 0.69621915\n",
      "Step: [6486] d_loss: 1.38423419, g_loss: 0.69824958\n",
      "Step: [6487] d_loss: 1.38428175, g_loss: 0.69785821\n",
      "Step: [6488] d_loss: 1.38516521, g_loss: 0.69575024\n",
      "Step: [6489] d_loss: 1.38725495, g_loss: 0.69560981\n",
      "Step: [6490] d_loss: 1.38566053, g_loss: 0.69607592\n",
      "Step: [6491] d_loss: 1.38898993, g_loss: 0.69509536\n",
      "Step: [6492] d_loss: 1.38930583, g_loss: 0.69582158\n",
      "Step: [6493] d_loss: 1.38635898, g_loss: 0.69957215\n",
      "Step: [6494] d_loss: 1.38534427, g_loss: 0.69762212\n",
      "Step: [6495] d_loss: 1.38417184, g_loss: 0.69533676\n",
      "Step: [6496] d_loss: 1.38431478, g_loss: 0.69741702\n",
      "Step: [6497] d_loss: 1.38675857, g_loss: 0.69443059\n",
      "Step: [6498] d_loss: 1.38411713, g_loss: 0.69909716\n",
      "Step: [6499] d_loss: 1.38259006, g_loss: 0.69906187\n",
      "Step: [6500] d_loss: 1.38653791, g_loss: 0.69502848\n",
      "Step: [6501] d_loss: 1.38539600, g_loss: 0.69576448\n",
      "Step: [6502] d_loss: 1.38655496, g_loss: 0.69695169\n",
      "Step: [6503] d_loss: 1.38744807, g_loss: 0.69603086\n",
      "Step: [6504] d_loss: 1.38766074, g_loss: 0.69139391\n",
      "Step: [6505] d_loss: 1.38799310, g_loss: 0.69508767\n",
      "Step: [6506] d_loss: 1.38679123, g_loss: 0.69554496\n",
      "Step: [6507] d_loss: 1.38601255, g_loss: 0.69808823\n",
      "Step: [6508] d_loss: 1.38644814, g_loss: 0.69696862\n",
      "Step: [6509] d_loss: 1.38435102, g_loss: 0.69582748\n",
      "Step: [6510] d_loss: 1.38531864, g_loss: 0.69463062\n",
      "Step: [6511] d_loss: 1.38501048, g_loss: 0.69704050\n",
      "Step: [6512] d_loss: 1.38583517, g_loss: 0.69585842\n",
      "Step: [6513] d_loss: 1.38435853, g_loss: 0.69710207\n",
      "Step: [6514] d_loss: 1.38533342, g_loss: 0.69707489\n",
      "Step: [6515] d_loss: 1.38159943, g_loss: 0.70271885\n",
      "Step: [6516] d_loss: 1.38575101, g_loss: 0.69916278\n",
      "Step: [6517] d_loss: 1.39024639, g_loss: 0.69594383\n",
      "Step: [6518] d_loss: 1.38460469, g_loss: 0.69631600\n",
      "Step: [6519] d_loss: 1.38432527, g_loss: 0.69508159\n",
      "Step: [6520] d_loss: 1.38562393, g_loss: 0.69493341\n",
      "Step: [6521] d_loss: 1.38530874, g_loss: 0.69380194\n",
      "Step: [6522] d_loss: 1.38838208, g_loss: 0.69788611\n",
      "Step: [6523] d_loss: 1.38521194, g_loss: 0.69438612\n",
      "Step: [6524] d_loss: 1.38585281, g_loss: 0.69494033\n",
      "Step: [6525] d_loss: 1.38375449, g_loss: 0.69542027\n",
      "Step: [6526] d_loss: 1.38437963, g_loss: 0.69809419\n",
      "Step: [6527] d_loss: 1.38313878, g_loss: 0.69641292\n",
      "Step: [6528] d_loss: 1.38744080, g_loss: 0.69689488\n",
      "Step: [6529] d_loss: 1.38749433, g_loss: 0.69552886\n",
      "Step: [6530] d_loss: 1.38941455, g_loss: 0.69455314\n",
      "Step: [6531] d_loss: 1.38957596, g_loss: 0.69476974\n",
      "Step: [6532] d_loss: 1.38595974, g_loss: 0.69598961\n",
      "Step: [6533] d_loss: 1.38749909, g_loss: 0.69095767\n",
      "Step: [6534] d_loss: 1.38724875, g_loss: 0.69487667\n",
      "Step: [6535] d_loss: 1.38490129, g_loss: 0.69818342\n",
      "Step: [6536] d_loss: 1.38535476, g_loss: 0.69569218\n",
      "Step: [6537] d_loss: 1.38530362, g_loss: 0.69473267\n",
      "Step: [6538] d_loss: 1.39103699, g_loss: 0.69767767\n",
      "Step: [6539] d_loss: 1.38786793, g_loss: 0.69239497\n",
      "Step: [6540] d_loss: 1.38258302, g_loss: 0.69803250\n",
      "Step: [6541] d_loss: 1.38971925, g_loss: 0.69454539\n",
      "Step: [6542] d_loss: 1.38765967, g_loss: 0.69519114\n",
      "Step: [6543] d_loss: 1.38781273, g_loss: 0.69481432\n",
      "Step: [6544] d_loss: 1.38455772, g_loss: 0.69804072\n",
      "Step: [6545] d_loss: 1.38479447, g_loss: 0.69554937\n",
      "Step: [6546] d_loss: 1.38458574, g_loss: 0.69562268\n",
      "Step: [6547] d_loss: 1.38235116, g_loss: 0.69883376\n",
      "Step: [6548] d_loss: 1.38601589, g_loss: 0.69611108\n",
      "Step: [6549] d_loss: 1.38587296, g_loss: 0.69592381\n",
      "Step: [6550] d_loss: 1.38640964, g_loss: 0.69720811\n",
      "Step: [6551] d_loss: 1.38290691, g_loss: 0.69551206\n",
      "Step: [6552] d_loss: 1.38598669, g_loss: 0.69770992\n",
      "Step: [6553] d_loss: 1.38736248, g_loss: 0.69920027\n",
      "Step: [6554] d_loss: 1.38573349, g_loss: 0.69927526\n",
      "Step: [6555] d_loss: 1.38915563, g_loss: 0.70037997\n",
      "Step: [6556] d_loss: 1.38827801, g_loss: 0.69525027\n",
      "Step: [6557] d_loss: 1.38511896, g_loss: 0.69623804\n",
      "Step: [6558] d_loss: 1.38764119, g_loss: 0.69399536\n",
      "Step: [6559] d_loss: 1.38350439, g_loss: 0.69497126\n",
      "Step: [6560] d_loss: 1.38456953, g_loss: 0.69784498\n",
      "Step: [6561] d_loss: 1.38518119, g_loss: 0.69808435\n",
      "Step: [6562] d_loss: 1.38249576, g_loss: 0.69757903\n",
      "Step: [6563] d_loss: 1.38463819, g_loss: 0.69425511\n",
      "Step: [6564] d_loss: 1.38392377, g_loss: 0.69656861\n",
      "Step: [6565] d_loss: 1.38740611, g_loss: 0.69362128\n",
      "Step: [6566] d_loss: 1.38846898, g_loss: 0.70172077\n",
      "Step: [6567] d_loss: 1.39017630, g_loss: 0.69851315\n",
      "Step: [6568] d_loss: 1.38721704, g_loss: 0.69743276\n",
      "Step: [6569] d_loss: 1.38422394, g_loss: 0.69412971\n",
      "Step: [6570] d_loss: 1.38825727, g_loss: 0.69662577\n",
      "Step: [6571] d_loss: 1.38676763, g_loss: 0.69796997\n",
      "Step: [6572] d_loss: 1.38217795, g_loss: 0.69885540\n",
      "Step: [6573] d_loss: 1.38303757, g_loss: 0.69392276\n",
      "Step: [6574] d_loss: 1.38322425, g_loss: 0.69994569\n",
      "Step: [6575] d_loss: 1.38644099, g_loss: 0.69423497\n",
      "Step: [6576] d_loss: 1.38926959, g_loss: 0.69416630\n",
      "Step: [6577] d_loss: 1.38508403, g_loss: 0.69619882\n",
      "Step: [6578] d_loss: 1.38658273, g_loss: 0.69492072\n",
      "Step: [6579] d_loss: 1.38756311, g_loss: 0.69657582\n",
      "Step: [6580] d_loss: 1.38575125, g_loss: 0.69917738\n",
      "Step: [6581] d_loss: 1.38909006, g_loss: 0.69541895\n",
      "Step: [6582] d_loss: 1.38803279, g_loss: 0.69416809\n",
      "Step: [6583] d_loss: 1.38585806, g_loss: 0.69301063\n",
      "Step: [6584] d_loss: 1.38603389, g_loss: 0.69566381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6585] d_loss: 1.38808405, g_loss: 0.69823062\n",
      "Step: [6586] d_loss: 1.38455033, g_loss: 0.69694716\n",
      "Step: [6587] d_loss: 1.38816106, g_loss: 0.69547260\n",
      "Step: [6588] d_loss: 1.38771927, g_loss: 0.69413370\n",
      "Step: [6589] d_loss: 1.38566411, g_loss: 0.69701862\n",
      "Step: [6590] d_loss: 1.38764215, g_loss: 0.69454241\n",
      "Step: [6591] d_loss: 1.38430011, g_loss: 0.69611359\n",
      "Step: [6592] d_loss: 1.38339996, g_loss: 0.69592738\n",
      "Step: [6593] d_loss: 1.38581002, g_loss: 0.69719183\n",
      "Step: [6594] d_loss: 1.38637114, g_loss: 0.69644380\n",
      "Step: [6595] d_loss: 1.38807464, g_loss: 0.69825065\n",
      "Step: [6596] d_loss: 1.38461733, g_loss: 0.69904971\n",
      "Step: [6597] d_loss: 1.38378823, g_loss: 0.69685173\n",
      "Step: [6598] d_loss: 1.38581157, g_loss: 0.69721144\n",
      "Step: [6599] d_loss: 1.38157868, g_loss: 0.69951624\n",
      "Step: [6600] d_loss: 1.38644755, g_loss: 0.69732428\n",
      "Step: [6601] d_loss: 1.38618672, g_loss: 0.69855309\n",
      "Step: [6602] d_loss: 1.38589692, g_loss: 0.69965261\n",
      "Step: [6603] d_loss: 1.38431752, g_loss: 0.69449985\n",
      "Step: [6604] d_loss: 1.38729858, g_loss: 0.69433278\n",
      "Step: [6605] d_loss: 1.38607061, g_loss: 0.69791174\n",
      "Step: [6606] d_loss: 1.38504291, g_loss: 0.70017874\n",
      "Step: [6607] d_loss: 1.38333499, g_loss: 0.69982487\n",
      "Step: [6608] d_loss: 1.38808060, g_loss: 0.69595897\n",
      "Step: [6609] d_loss: 1.38625419, g_loss: 0.69587910\n",
      "Step: [6610] d_loss: 1.38536870, g_loss: 0.69569480\n",
      "Step: [6611] d_loss: 1.38583064, g_loss: 0.69466996\n",
      "Step: [6612] d_loss: 1.38968980, g_loss: 0.69555509\n",
      "Step: [6613] d_loss: 1.38782656, g_loss: 0.69573861\n",
      "Step: [6614] d_loss: 1.39077806, g_loss: 0.69561779\n",
      "Step: [6615] d_loss: 1.39258039, g_loss: 0.69332910\n",
      "Step: [6616] d_loss: 1.38959026, g_loss: 0.69604403\n",
      "Step: [6617] d_loss: 1.38512254, g_loss: 0.69736028\n",
      "Step: [6618] d_loss: 1.38674748, g_loss: 0.69649667\n",
      "Step: [6619] d_loss: 1.38946545, g_loss: 0.69044244\n",
      "Step: [6620] d_loss: 1.38556850, g_loss: 0.69596821\n",
      "Step: [6621] d_loss: 1.38431251, g_loss: 0.70083547\n",
      "Step: [6622] d_loss: 1.38322878, g_loss: 0.69907343\n",
      "Step: [6623] d_loss: 1.38799977, g_loss: 0.70280063\n",
      "Step: [6624] d_loss: 1.38281250, g_loss: 0.69813895\n",
      "Step: [6625] d_loss: 1.38543534, g_loss: 0.69813025\n",
      "Step: [6626] d_loss: 1.38639331, g_loss: 0.69854671\n",
      "Step: [6627] d_loss: 1.38466692, g_loss: 0.69739920\n",
      "Step: [6628] d_loss: 1.38392210, g_loss: 0.69824052\n",
      "Step: [6629] d_loss: 1.38724530, g_loss: 0.69439125\n",
      "Step: [6630] d_loss: 1.38610709, g_loss: 0.69768035\n",
      "Step: [6631] d_loss: 1.38970232, g_loss: 0.69364452\n",
      "Step: [6632] d_loss: 1.38663721, g_loss: 0.69525713\n",
      "Step: [6633] d_loss: 1.38391376, g_loss: 0.69710040\n",
      "Step: [6634] d_loss: 1.39158082, g_loss: 0.69586039\n",
      "Step: [6635] d_loss: 1.38885033, g_loss: 0.69651634\n",
      "Step: [6636] d_loss: 1.38852108, g_loss: 0.70003569\n",
      "Step: [6637] d_loss: 1.38628805, g_loss: 0.69797808\n",
      "Step: [6638] d_loss: 1.38804877, g_loss: 0.69295216\n",
      "Step: [6639] d_loss: 1.38752043, g_loss: 0.69363099\n",
      "Step: [6640] d_loss: 1.38766789, g_loss: 0.69302523\n",
      "Step: [6641] d_loss: 1.38714981, g_loss: 0.69317067\n",
      "Step: [6642] d_loss: 1.38497305, g_loss: 0.69702387\n",
      "Step: [6643] d_loss: 1.38767171, g_loss: 0.69681406\n",
      "Step: [6644] d_loss: 1.38637328, g_loss: 0.69652337\n",
      "Step: [6645] d_loss: 1.38540840, g_loss: 0.69295716\n",
      "Step: [6646] d_loss: 1.38522029, g_loss: 0.69542408\n",
      "Step: [6647] d_loss: 1.38447535, g_loss: 0.69496161\n",
      "Step: [6648] d_loss: 1.38802719, g_loss: 0.69870859\n",
      "Step: [6649] d_loss: 1.38769627, g_loss: 0.69615561\n",
      "Step: [6650] d_loss: 1.38433385, g_loss: 0.69864428\n",
      "Step: [6651] d_loss: 1.38356352, g_loss: 0.69986206\n",
      "Step: [6652] d_loss: 1.38664603, g_loss: 0.69604117\n",
      "Step: [6653] d_loss: 1.38372934, g_loss: 0.69405985\n",
      "Step: [6654] d_loss: 1.38641858, g_loss: 0.69397640\n",
      "Step: [6655] d_loss: 1.38589203, g_loss: 0.69870400\n",
      "Step: [6656] d_loss: 1.38615179, g_loss: 0.70060337\n",
      "Step: [6657] d_loss: 1.38324440, g_loss: 0.69833499\n",
      "Step: [6658] d_loss: 1.38624001, g_loss: 0.69161105\n",
      "Step: [6659] d_loss: 1.38522792, g_loss: 0.70314598\n",
      "Step: [6660] d_loss: 1.38937759, g_loss: 0.69439864\n",
      "Step: [6661] d_loss: 1.38908494, g_loss: 0.70091403\n",
      "Step: [6662] d_loss: 1.38377261, g_loss: 0.70006418\n",
      "Step: [6663] d_loss: 1.38957143, g_loss: 0.69572353\n",
      "Step: [6664] d_loss: 1.38583374, g_loss: 0.70283258\n",
      "Step: [6665] d_loss: 1.39143884, g_loss: 0.68955910\n",
      "Step: [6666] d_loss: 1.38765764, g_loss: 0.69598114\n",
      "Step: [6667] d_loss: 1.38677061, g_loss: 0.70018983\n",
      "Step: [6668] d_loss: 1.38672709, g_loss: 0.70081061\n",
      "Step: [6669] d_loss: 1.38698006, g_loss: 0.69664037\n",
      "Step: [6670] d_loss: 1.38150668, g_loss: 0.70079732\n",
      "Step: [6671] d_loss: 1.38801289, g_loss: 0.69623840\n",
      "Step: [6672] d_loss: 1.38620710, g_loss: 0.69763881\n",
      "Step: [6673] d_loss: 1.38440895, g_loss: 0.69712120\n",
      "Step: [6674] d_loss: 1.39124584, g_loss: 0.69674104\n",
      "Step: [6675] d_loss: 1.38770342, g_loss: 0.69305640\n",
      "Step: [6676] d_loss: 1.38521516, g_loss: 0.69880545\n",
      "Step: [6677] d_loss: 1.38771057, g_loss: 0.69839466\n",
      "Step: [6678] d_loss: 1.38892007, g_loss: 0.69854945\n",
      "Step: [6679] d_loss: 1.38471282, g_loss: 0.69590044\n",
      "Step: [6680] d_loss: 1.38536859, g_loss: 0.69610244\n",
      "Step: [6681] d_loss: 1.38437366, g_loss: 0.69805253\n",
      "Step: [6682] d_loss: 1.38741875, g_loss: 0.69691056\n",
      "Step: [6683] d_loss: 1.38579881, g_loss: 0.69620121\n",
      "Step: [6684] d_loss: 1.38491583, g_loss: 0.69508624\n",
      "Step: [6685] d_loss: 1.39157939, g_loss: 0.69232261\n",
      "Step: [6686] d_loss: 1.38943911, g_loss: 0.69456464\n",
      "Step: [6687] d_loss: 1.39052820, g_loss: 0.69150770\n",
      "Step: [6688] d_loss: 1.38960528, g_loss: 0.69545346\n",
      "Step: [6689] d_loss: 1.38845682, g_loss: 0.69580168\n",
      "Step: [6690] d_loss: 1.38666511, g_loss: 0.69935393\n",
      "Step: [6691] d_loss: 1.38954067, g_loss: 0.69520998\n",
      "Step: [6692] d_loss: 1.38853276, g_loss: 0.69342268\n",
      "Step: [6693] d_loss: 1.38513970, g_loss: 0.69233483\n",
      "Step: [6694] d_loss: 1.38500798, g_loss: 0.69664866\n",
      "Step: [6695] d_loss: 1.38543379, g_loss: 0.69866991\n",
      "Step: [6696] d_loss: 1.38615680, g_loss: 0.69543147\n",
      "Step: [6697] d_loss: 1.38502693, g_loss: 0.69752896\n",
      "Step: [6698] d_loss: 1.38416326, g_loss: 0.69565469\n",
      "Step: [6699] d_loss: 1.38421082, g_loss: 0.69624090\n",
      "Step: [6700] d_loss: 1.37868047, g_loss: 0.70150208\n",
      "Step: [6701] d_loss: 1.38265848, g_loss: 0.69813937\n",
      "Step: [6702] d_loss: 1.38466620, g_loss: 0.69968367\n",
      "Step: [6703] d_loss: 1.38415098, g_loss: 0.69560033\n",
      "Step: [6704] d_loss: 1.38427913, g_loss: 0.70116776\n",
      "Step: [6705] d_loss: 1.38643539, g_loss: 0.69381052\n",
      "Step: [6706] d_loss: 1.38377714, g_loss: 0.69924426\n",
      "Step: [6707] d_loss: 1.38684320, g_loss: 0.69520366\n",
      "Step: [6708] d_loss: 1.38380909, g_loss: 0.69400644\n",
      "Step: [6709] d_loss: 1.38757360, g_loss: 0.70108640\n",
      "Step: [6710] d_loss: 1.38249111, g_loss: 0.70089191\n",
      "Step: [6711] d_loss: 1.38506413, g_loss: 0.69476831\n",
      "Step: [6712] d_loss: 1.38799393, g_loss: 0.69167948\n",
      "Step: [6713] d_loss: 1.39133060, g_loss: 0.69347835\n",
      "Step: [6714] d_loss: 1.39013791, g_loss: 0.69985425\n",
      "Step: [6715] d_loss: 1.38893664, g_loss: 0.70265043\n",
      "Step: [6716] d_loss: 1.39390719, g_loss: 0.70325464\n",
      "Step: [6717] d_loss: 1.39058745, g_loss: 0.69425821\n",
      "Step: [6718] d_loss: 1.38569355, g_loss: 0.69511515\n",
      "Step: [6719] d_loss: 1.38868713, g_loss: 0.69654381\n",
      "Step: [6720] d_loss: 1.38618636, g_loss: 0.69772184\n",
      "Step: [6721] d_loss: 1.38590932, g_loss: 0.70229864\n",
      "Step: [6722] d_loss: 1.38609707, g_loss: 0.69776815\n",
      "Step: [6723] d_loss: 1.38341594, g_loss: 0.69974345\n",
      "Step: [6724] d_loss: 1.38720012, g_loss: 0.69654584\n",
      "Step: [6725] d_loss: 1.38443732, g_loss: 0.69885081\n",
      "Step: [6726] d_loss: 1.38605809, g_loss: 0.69499445\n",
      "Step: [6727] d_loss: 1.38386381, g_loss: 0.69829977\n",
      "Step: [6728] d_loss: 1.38419664, g_loss: 0.69897169\n",
      "Step: [6729] d_loss: 1.38554227, g_loss: 0.69657135\n",
      "Step: [6730] d_loss: 1.38654542, g_loss: 0.69971466\n",
      "Step: [6731] d_loss: 1.38641751, g_loss: 0.69898725\n",
      "Step: [6732] d_loss: 1.38610554, g_loss: 0.69456381\n",
      "Step: [6733] d_loss: 1.39136982, g_loss: 0.69667673\n",
      "Step: [6734] d_loss: 1.38882458, g_loss: 0.69059801\n",
      "Step: [6735] d_loss: 1.38641572, g_loss: 0.69550753\n",
      "Step: [6736] d_loss: 1.38582253, g_loss: 0.69491851\n",
      "Step: [6737] d_loss: 1.38723159, g_loss: 0.69955385\n",
      "Step: [6738] d_loss: 1.38519740, g_loss: 0.69813871\n",
      "Step: [6739] d_loss: 1.38059258, g_loss: 0.69969594\n",
      "Step: [6740] d_loss: 1.38923752, g_loss: 0.69195652\n",
      "Step: [6741] d_loss: 1.38637543, g_loss: 0.69532835\n",
      "Step: [6742] d_loss: 1.38763642, g_loss: 0.69365084\n",
      "Step: [6743] d_loss: 1.38281441, g_loss: 0.69574529\n",
      "Step: [6744] d_loss: 1.38468266, g_loss: 0.69656920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6745] d_loss: 1.38591766, g_loss: 0.69704509\n",
      "Step: [6746] d_loss: 1.38522720, g_loss: 0.69729048\n",
      "Step: [6747] d_loss: 1.38692033, g_loss: 0.69801229\n",
      "Step: [6748] d_loss: 1.38679326, g_loss: 0.69327819\n",
      "Step: [6749] d_loss: 1.38205802, g_loss: 0.70168352\n",
      "Step: [6750] d_loss: 1.39402509, g_loss: 0.69278598\n",
      "Step: [6751] d_loss: 1.38050127, g_loss: 0.69888783\n",
      "Step: [6752] d_loss: 1.38608837, g_loss: 0.69997072\n",
      "Step: [6753] d_loss: 1.38611615, g_loss: 0.69674325\n",
      "Step: [6754] d_loss: 1.38383341, g_loss: 0.69858789\n",
      "Step: [6755] d_loss: 1.38556421, g_loss: 0.69303644\n",
      "Step: [6756] d_loss: 1.38808680, g_loss: 0.69797575\n",
      "Step: [6757] d_loss: 1.38775420, g_loss: 0.69758093\n",
      "Step: [6758] d_loss: 1.38631308, g_loss: 0.69294810\n",
      "Step: [6759] d_loss: 1.38268340, g_loss: 0.69717741\n",
      "Step: [6760] d_loss: 1.38735163, g_loss: 0.69553316\n",
      "Step: [6761] d_loss: 1.38800621, g_loss: 0.70255542\n",
      "Step: [6762] d_loss: 1.38982213, g_loss: 0.69649571\n",
      "Step: [6763] d_loss: 1.38335490, g_loss: 0.70014346\n",
      "Step: [6764] d_loss: 1.38400877, g_loss: 0.69813204\n",
      "Step: [6765] d_loss: 1.38525152, g_loss: 0.69623673\n",
      "Step: [6766] d_loss: 1.38867366, g_loss: 0.69813931\n",
      "Step: [6767] d_loss: 1.38911366, g_loss: 0.69396883\n",
      "Step: [6768] d_loss: 1.39047241, g_loss: 0.69574225\n",
      "Step: [6769] d_loss: 1.38961470, g_loss: 0.69807124\n",
      "Step: [6770] d_loss: 1.38669300, g_loss: 0.69751501\n",
      "Step: [6771] d_loss: 1.38468170, g_loss: 0.69690251\n",
      "Step: [6772] d_loss: 1.38606000, g_loss: 0.69693768\n",
      "Step: [6773] d_loss: 1.38767052, g_loss: 0.69434428\n",
      "Step: [6774] d_loss: 1.38537979, g_loss: 0.69858605\n",
      "Step: [6775] d_loss: 1.38531828, g_loss: 0.69836342\n",
      "Step: [6776] d_loss: 1.38572705, g_loss: 0.69734305\n",
      "Step: [6777] d_loss: 1.38703108, g_loss: 0.69389009\n",
      "Step: [6778] d_loss: 1.38177562, g_loss: 0.69955099\n",
      "Step: [6779] d_loss: 1.38295126, g_loss: 0.69857371\n",
      "Step: [6780] d_loss: 1.38663757, g_loss: 0.70070803\n",
      "Step: [6781] d_loss: 1.38978517, g_loss: 0.69600379\n",
      "Step: [6782] d_loss: 1.38851047, g_loss: 0.70391202\n",
      "Step: [6783] d_loss: 1.38706326, g_loss: 0.69548547\n",
      "Step: [6784] d_loss: 1.38999081, g_loss: 0.70049495\n",
      "Step: [6785] d_loss: 1.38454747, g_loss: 0.70253992\n",
      "Step: [6786] d_loss: 1.38232183, g_loss: 0.70145977\n",
      "Step: [6787] d_loss: 1.38389981, g_loss: 0.69699627\n",
      "Step: [6788] d_loss: 1.39053118, g_loss: 0.69302511\n",
      "Step: [6789] d_loss: 1.38648629, g_loss: 0.70020914\n",
      "Step: [6790] d_loss: 1.38454533, g_loss: 0.70011181\n",
      "Step: [6791] d_loss: 1.38265920, g_loss: 0.69801080\n",
      "Step: [6792] d_loss: 1.38925433, g_loss: 0.69669884\n",
      "Step: [6793] d_loss: 1.38918960, g_loss: 0.69604546\n",
      "Step: [6794] d_loss: 1.38278818, g_loss: 0.69644070\n",
      "Step: [6795] d_loss: 1.38795400, g_loss: 0.69026262\n",
      "Step: [6796] d_loss: 1.38909531, g_loss: 0.69325495\n",
      "Step: [6797] d_loss: 1.38778806, g_loss: 0.69717300\n",
      "Step: [6798] d_loss: 1.38389015, g_loss: 0.70178473\n",
      "Step: [6799] d_loss: 1.38583207, g_loss: 0.69677806\n",
      "Step: [6800] d_loss: 1.38605690, g_loss: 0.69531268\n",
      "Step: [6801] d_loss: 1.39108133, g_loss: 0.69421440\n",
      "Step: [6802] d_loss: 1.38294685, g_loss: 0.69948721\n",
      "Step: [6803] d_loss: 1.38563156, g_loss: 0.69471467\n",
      "Step: [6804] d_loss: 1.38787544, g_loss: 0.69423711\n",
      "Step: [6805] d_loss: 1.38981104, g_loss: 0.69838309\n",
      "Step: [6806] d_loss: 1.38502550, g_loss: 0.69952434\n",
      "Step: [6807] d_loss: 1.38237393, g_loss: 0.69668114\n",
      "Step: [6808] d_loss: 1.38784516, g_loss: 0.69577777\n",
      "Step: [6809] d_loss: 1.38691521, g_loss: 0.69927144\n",
      "Step: [6810] d_loss: 1.38440251, g_loss: 0.69935119\n",
      "Step: [6811] d_loss: 1.38342011, g_loss: 0.69937909\n",
      "Step: [6812] d_loss: 1.38416076, g_loss: 0.69681692\n",
      "Step: [6813] d_loss: 1.38812578, g_loss: 0.69563007\n",
      "Step: [6814] d_loss: 1.38627315, g_loss: 0.69951153\n",
      "Step: [6815] d_loss: 1.38319540, g_loss: 0.69818711\n",
      "Step: [6816] d_loss: 1.38247609, g_loss: 0.69980812\n",
      "Step: [6817] d_loss: 1.38817751, g_loss: 0.69853443\n",
      "Step: [6818] d_loss: 1.38618767, g_loss: 0.69575548\n",
      "Step: [6819] d_loss: 1.38378668, g_loss: 0.69843018\n",
      "Step: [6820] d_loss: 1.38613725, g_loss: 0.69737411\n",
      "Step: [6821] d_loss: 1.38288355, g_loss: 0.69743615\n",
      "Step: [6822] d_loss: 1.38550889, g_loss: 0.69969153\n",
      "Step: [6823] d_loss: 1.38237953, g_loss: 0.70216525\n",
      "Step: [6824] d_loss: 1.38274109, g_loss: 0.69868731\n",
      "Step: [6825] d_loss: 1.38259935, g_loss: 0.69659781\n",
      "Step: [6826] d_loss: 1.38625884, g_loss: 0.69359958\n",
      "Step: [6827] d_loss: 1.38049936, g_loss: 0.69973278\n",
      "Step: [6828] d_loss: 1.38752925, g_loss: 0.69539595\n",
      "Step: [6829] d_loss: 1.38231206, g_loss: 0.70169294\n",
      "Step: [6830] d_loss: 1.39163017, g_loss: 0.69392031\n",
      "Step: [6831] d_loss: 1.38750052, g_loss: 0.69860107\n",
      "Step: [6832] d_loss: 1.39149702, g_loss: 0.69399416\n",
      "Step: [6833] d_loss: 1.38905632, g_loss: 0.69723070\n",
      "Step: [6834] d_loss: 1.39103770, g_loss: 0.69171071\n",
      "Step: [6835] d_loss: 1.38860202, g_loss: 0.69759333\n",
      "Step: [6836] d_loss: 1.38969755, g_loss: 0.70026934\n",
      "Step: [6837] d_loss: 1.38874984, g_loss: 0.69689202\n",
      "Step: [6838] d_loss: 1.38509178, g_loss: 0.69699264\n",
      "Step: [6839] d_loss: 1.38637447, g_loss: 0.69453025\n",
      "Step: [6840] d_loss: 1.38631845, g_loss: 0.69961357\n",
      "Step: [6841] d_loss: 1.38034976, g_loss: 0.70348072\n",
      "Step: [6842] d_loss: 1.38283038, g_loss: 0.70217806\n",
      "Step: [6843] d_loss: 1.37607467, g_loss: 0.70603979\n",
      "Step: [6844] d_loss: 1.38206244, g_loss: 0.69799840\n",
      "Step: [6845] d_loss: 1.38097584, g_loss: 0.69890136\n",
      "Step: [6846] d_loss: 1.38275206, g_loss: 0.70045948\n",
      "Step: [6847] d_loss: 1.38453853, g_loss: 0.69714588\n",
      "Step: [6848] d_loss: 1.38634145, g_loss: 0.69921589\n",
      "Step: [6849] d_loss: 1.38972044, g_loss: 0.69561887\n",
      "Step: [6850] d_loss: 1.38482368, g_loss: 0.70098990\n",
      "Step: [6851] d_loss: 1.38338637, g_loss: 0.70269215\n",
      "Step: [6852] d_loss: 1.38671732, g_loss: 0.69912660\n",
      "Step: [6853] d_loss: 1.39132023, g_loss: 0.70156664\n",
      "Step: [6854] d_loss: 1.38927388, g_loss: 0.69726753\n",
      "Step: [6855] d_loss: 1.38742352, g_loss: 0.70329535\n",
      "Step: [6856] d_loss: 1.39002621, g_loss: 0.69969130\n",
      "Step: [6857] d_loss: 1.39058578, g_loss: 0.70077276\n",
      "Step: [6858] d_loss: 1.39409590, g_loss: 0.69835681\n",
      "Step: [6859] d_loss: 1.38743782, g_loss: 0.69473350\n",
      "Step: [6860] d_loss: 1.38612390, g_loss: 0.70256191\n",
      "Step: [6861] d_loss: 1.38409889, g_loss: 0.70135397\n",
      "Step: [6862] d_loss: 1.39101982, g_loss: 0.69925791\n",
      "Step: [6863] d_loss: 1.38977253, g_loss: 0.69614410\n",
      "Step: [6864] d_loss: 1.37885690, g_loss: 0.70312726\n",
      "Step: [6865] d_loss: 1.38542497, g_loss: 0.69572544\n",
      "Step: [6866] d_loss: 1.38093877, g_loss: 0.70413387\n",
      "Step: [6867] d_loss: 1.37954974, g_loss: 0.70382428\n",
      "Step: [6868] d_loss: 1.38563454, g_loss: 0.70083821\n",
      "Step: [6869] d_loss: 1.38558078, g_loss: 0.69892752\n",
      "Step: [6870] d_loss: 1.38554740, g_loss: 0.69790035\n",
      "Step: [6871] d_loss: 1.39111769, g_loss: 0.69443250\n",
      "Step: [6872] d_loss: 1.38453996, g_loss: 0.69701684\n",
      "Step: [6873] d_loss: 1.38473499, g_loss: 0.69991934\n",
      "Step: [6874] d_loss: 1.39053917, g_loss: 0.69345415\n",
      "Step: [6875] d_loss: 1.38354659, g_loss: 0.69659382\n",
      "Step: [6876] d_loss: 1.39142573, g_loss: 0.69297451\n",
      "Step: [6877] d_loss: 1.39086699, g_loss: 0.69720840\n",
      "Step: [6878] d_loss: 1.38924813, g_loss: 0.69384617\n",
      "Step: [6879] d_loss: 1.38861895, g_loss: 0.70481777\n",
      "Step: [6880] d_loss: 1.38990760, g_loss: 0.69833952\n",
      "Step: [6881] d_loss: 1.39265275, g_loss: 0.69729805\n",
      "Step: [6882] d_loss: 1.38261557, g_loss: 0.70353621\n",
      "Step: [6883] d_loss: 1.38360918, g_loss: 0.69599116\n",
      "Step: [6884] d_loss: 1.38485873, g_loss: 0.69990909\n",
      "Step: [6885] d_loss: 1.38446140, g_loss: 0.69557762\n",
      "Step: [6886] d_loss: 1.38504374, g_loss: 0.70031255\n",
      "Step: [6887] d_loss: 1.38769984, g_loss: 0.70042068\n",
      "Step: [6888] d_loss: 1.38530040, g_loss: 0.70563483\n",
      "Step: [6889] d_loss: 1.38238442, g_loss: 0.69732726\n",
      "Step: [6890] d_loss: 1.38497043, g_loss: 0.69635993\n",
      "Step: [6891] d_loss: 1.38137245, g_loss: 0.69797391\n",
      "Step: [6892] d_loss: 1.38465118, g_loss: 0.69394672\n",
      "Step: [6893] d_loss: 1.38406134, g_loss: 0.70085418\n",
      "Step: [6894] d_loss: 1.38783598, g_loss: 0.70410275\n",
      "Step: [6895] d_loss: 1.38904691, g_loss: 0.70418394\n",
      "Step: [6896] d_loss: 1.39269722, g_loss: 0.69532681\n",
      "Step: [6897] d_loss: 1.38310897, g_loss: 0.70249027\n",
      "Step: [6898] d_loss: 1.38226950, g_loss: 0.69826895\n",
      "Step: [6899] d_loss: 1.38550210, g_loss: 0.69604808\n",
      "Step: [6900] d_loss: 1.38962841, g_loss: 0.69889224\n",
      "Step: [6901] d_loss: 1.38870096, g_loss: 0.69892776\n",
      "Step: [6902] d_loss: 1.38981009, g_loss: 0.70134926\n",
      "Step: [6903] d_loss: 1.38131082, g_loss: 0.69895571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [6904] d_loss: 1.39010549, g_loss: 0.69681299\n",
      "Step: [6905] d_loss: 1.38853073, g_loss: 0.69406056\n",
      "Step: [6906] d_loss: 1.38476515, g_loss: 0.69780326\n",
      "Step: [6907] d_loss: 1.38350999, g_loss: 0.70346546\n",
      "Step: [6908] d_loss: 1.38867068, g_loss: 0.69726312\n",
      "Step: [6909] d_loss: 1.38823318, g_loss: 0.69640374\n",
      "Step: [6910] d_loss: 1.38936830, g_loss: 0.69738257\n",
      "Step: [6911] d_loss: 1.38627040, g_loss: 0.69497180\n",
      "Step: [6912] d_loss: 1.38600576, g_loss: 0.69636673\n",
      "Step: [6913] d_loss: 1.38419521, g_loss: 0.69669092\n",
      "Step: [6914] d_loss: 1.38381195, g_loss: 0.69908214\n",
      "Step: [6915] d_loss: 1.38763118, g_loss: 0.69917369\n",
      "Step: [6916] d_loss: 1.38279319, g_loss: 0.70087266\n",
      "Step: [6917] d_loss: 1.38374889, g_loss: 0.69969535\n",
      "Step: [6918] d_loss: 1.38545442, g_loss: 0.69121486\n",
      "Step: [6919] d_loss: 1.38624072, g_loss: 0.69806254\n",
      "Step: [6920] d_loss: 1.38713562, g_loss: 0.69556195\n",
      "Step: [6921] d_loss: 1.38135111, g_loss: 0.70085829\n",
      "Step: [6922] d_loss: 1.38365936, g_loss: 0.69721597\n",
      "Step: [6923] d_loss: 1.38330460, g_loss: 0.69869387\n",
      "Step: [6924] d_loss: 1.38463533, g_loss: 0.69648296\n",
      "Step: [6925] d_loss: 1.38006437, g_loss: 0.70278269\n",
      "Step: [6926] d_loss: 1.38499022, g_loss: 0.69285822\n",
      "Step: [6927] d_loss: 1.38359976, g_loss: 0.70053744\n",
      "Step: [6928] d_loss: 1.38364613, g_loss: 0.70547456\n",
      "Step: [6929] d_loss: 1.39038730, g_loss: 0.69618833\n",
      "Step: [6930] d_loss: 1.38469493, g_loss: 0.70439857\n",
      "Step: [6931] d_loss: 1.39261961, g_loss: 0.69229859\n",
      "Step: [6932] d_loss: 1.38796735, g_loss: 0.70441490\n",
      "Step: [6933] d_loss: 1.39304972, g_loss: 0.70016694\n",
      "Step: [6934] d_loss: 1.38842762, g_loss: 0.70232993\n",
      "Step: [6935] d_loss: 1.38796175, g_loss: 0.70065767\n",
      "Step: [6936] d_loss: 1.38610411, g_loss: 0.69625759\n",
      "Step: [6937] d_loss: 1.38767958, g_loss: 0.69484162\n",
      "Step: [6938] d_loss: 1.39058185, g_loss: 0.69144285\n",
      "Step: [6939] d_loss: 1.38844323, g_loss: 0.69646633\n",
      "Step: [6940] d_loss: 1.38701594, g_loss: 0.69974625\n",
      "Step: [6941] d_loss: 1.39130902, g_loss: 0.70031452\n",
      "Step: [6942] d_loss: 1.38592768, g_loss: 0.69652337\n",
      "Step: [6943] d_loss: 1.38465905, g_loss: 0.70061523\n",
      "Step: [6944] d_loss: 1.38264167, g_loss: 0.69613886\n",
      "Step: [6945] d_loss: 1.38837886, g_loss: 0.68999541\n",
      "Step: [6946] d_loss: 1.38642645, g_loss: 0.69636941\n",
      "Step: [6947] d_loss: 1.38096023, g_loss: 0.70410585\n",
      "Step: [6948] d_loss: 1.38976121, g_loss: 0.69607365\n",
      "Step: [6949] d_loss: 1.38902760, g_loss: 0.69491559\n",
      "Step: [6950] d_loss: 1.38940275, g_loss: 0.69246387\n",
      "Step: [6951] d_loss: 1.38983214, g_loss: 0.69381166\n",
      "Step: [6952] d_loss: 1.38728404, g_loss: 0.69464737\n",
      "Step: [6953] d_loss: 1.38611197, g_loss: 0.69880331\n",
      "Step: [6954] d_loss: 1.38691354, g_loss: 0.69583428\n",
      "Step: [6955] d_loss: 1.38519633, g_loss: 0.69125402\n",
      "Step: [6956] d_loss: 1.38525629, g_loss: 0.69860971\n",
      "Step: [6957] d_loss: 1.38311839, g_loss: 0.70018148\n",
      "Step: [6958] d_loss: 1.38392806, g_loss: 0.69820118\n",
      "Step: [6959] d_loss: 1.38166952, g_loss: 0.69581151\n",
      "Step: [6960] d_loss: 1.38442373, g_loss: 0.69423276\n",
      "Step: [6961] d_loss: 1.38519788, g_loss: 0.70016760\n",
      "Step: [6962] d_loss: 1.38677275, g_loss: 0.69942266\n",
      "Step: [6963] d_loss: 1.38572133, g_loss: 0.69705224\n",
      "Step: [6964] d_loss: 1.38288963, g_loss: 0.69571781\n",
      "Step: [6965] d_loss: 1.38472962, g_loss: 0.69907314\n",
      "Step: [6966] d_loss: 1.38803482, g_loss: 0.69156647\n",
      "Step: [6967] d_loss: 1.38441193, g_loss: 0.69542575\n",
      "Step: [6968] d_loss: 1.38754439, g_loss: 0.69882083\n",
      "Step: [6969] d_loss: 1.38685501, g_loss: 0.69256151\n",
      "Step: [6970] d_loss: 1.38890505, g_loss: 0.69444871\n",
      "Step: [6971] d_loss: 1.38726878, g_loss: 0.69639623\n",
      "Step: [6972] d_loss: 1.38672686, g_loss: 0.69997895\n",
      "Step: [6973] d_loss: 1.38303208, g_loss: 0.69667840\n",
      "Step: [6974] d_loss: 1.38916039, g_loss: 0.69540364\n",
      "Step: [6975] d_loss: 1.38344085, g_loss: 0.69938040\n",
      "Step: [6976] d_loss: 1.38714504, g_loss: 0.69662225\n",
      "Step: [6977] d_loss: 1.38871443, g_loss: 0.69725895\n",
      "Step: [6978] d_loss: 1.38495314, g_loss: 0.70009506\n",
      "Step: [6979] d_loss: 1.38737583, g_loss: 0.69753462\n",
      "Step: [6980] d_loss: 1.38505840, g_loss: 0.69842851\n",
      "Step: [6981] d_loss: 1.38521659, g_loss: 0.69283569\n",
      "Step: [6982] d_loss: 1.38783741, g_loss: 0.69640160\n",
      "Step: [6983] d_loss: 1.38538122, g_loss: 0.69965559\n",
      "Step: [6984] d_loss: 1.38489294, g_loss: 0.70005941\n",
      "Step: [6985] d_loss: 1.38377714, g_loss: 0.69648564\n",
      "Step: [6986] d_loss: 1.38305879, g_loss: 0.69994736\n",
      "Step: [6987] d_loss: 1.38809609, g_loss: 0.69525087\n",
      "Step: [6988] d_loss: 1.38526702, g_loss: 0.69763345\n",
      "Step: [6989] d_loss: 1.38832319, g_loss: 0.69307590\n",
      "Step: [6990] d_loss: 1.38473785, g_loss: 0.69589895\n",
      "Step: [6991] d_loss: 1.38899565, g_loss: 0.69194937\n",
      "Step: [6992] d_loss: 1.38461745, g_loss: 0.69628185\n",
      "Step: [6993] d_loss: 1.38360023, g_loss: 0.69647521\n",
      "Step: [6994] d_loss: 1.38619876, g_loss: 0.69333833\n",
      "Step: [6995] d_loss: 1.38759840, g_loss: 0.69522268\n",
      "Step: [6996] d_loss: 1.38514495, g_loss: 0.69336617\n",
      "Step: [6997] d_loss: 1.38544917, g_loss: 0.69721246\n",
      "Step: [6998] d_loss: 1.38377595, g_loss: 0.69661099\n",
      "Step: [6999] d_loss: 1.38781714, g_loss: 0.69418037\n",
      "Step: [7000] d_loss: 1.38470435, g_loss: 0.69633603\n",
      "Step: [7001] d_loss: 1.38450813, g_loss: 0.69478190\n",
      "Step: [7002] d_loss: 1.38160181, g_loss: 0.69933522\n",
      "Step: [7003] d_loss: 1.38774514, g_loss: 0.69709724\n",
      "Step: [7004] d_loss: 1.38636565, g_loss: 0.69546068\n",
      "Step: [7005] d_loss: 1.38705850, g_loss: 0.69634575\n",
      "Step: [7006] d_loss: 1.38577831, g_loss: 0.69540089\n",
      "Step: [7007] d_loss: 1.38426828, g_loss: 0.69554162\n",
      "Step: [7008] d_loss: 1.38622010, g_loss: 0.69503886\n",
      "Step: [7009] d_loss: 1.38053107, g_loss: 0.69754314\n",
      "Step: [7010] d_loss: 1.38654208, g_loss: 0.69263220\n",
      "Step: [7011] d_loss: 1.38788009, g_loss: 0.69498080\n",
      "Step: [7012] d_loss: 1.38944530, g_loss: 0.69329691\n",
      "Step: [7013] d_loss: 1.39002299, g_loss: 0.69309258\n",
      "Step: [7014] d_loss: 1.38763452, g_loss: 0.69430757\n",
      "Step: [7015] d_loss: 1.38778186, g_loss: 0.69591171\n",
      "Step: [7016] d_loss: 1.38620508, g_loss: 0.69536114\n",
      "Step: [7017] d_loss: 1.38827026, g_loss: 0.69624650\n",
      "Step: [7018] d_loss: 1.38322771, g_loss: 0.69696635\n",
      "Step: [7019] d_loss: 1.38861418, g_loss: 0.69359279\n",
      "Step: [7020] d_loss: 1.38488412, g_loss: 0.69171298\n",
      "Step: [7021] d_loss: 1.38493156, g_loss: 0.69846761\n",
      "Step: [7022] d_loss: 1.38637722, g_loss: 0.69585812\n",
      "Step: [7023] d_loss: 1.38636816, g_loss: 0.69636333\n",
      "Step: [7024] d_loss: 1.38637400, g_loss: 0.69541061\n",
      "Step: [7025] d_loss: 1.38458943, g_loss: 0.69489914\n",
      "Step: [7026] d_loss: 1.38114214, g_loss: 0.70304537\n",
      "Step: [7027] d_loss: 1.38298917, g_loss: 0.69575655\n",
      "Step: [7028] d_loss: 1.38585734, g_loss: 0.70015633\n",
      "Step: [7029] d_loss: 1.38504267, g_loss: 0.69905949\n",
      "Step: [7030] d_loss: 1.38419628, g_loss: 0.69598722\n",
      "Step: [7031] d_loss: 1.38630366, g_loss: 0.69182402\n",
      "Step: [7032] d_loss: 1.38616538, g_loss: 0.69857264\n",
      "Step: [7033] d_loss: 1.38658428, g_loss: 0.69500613\n",
      "Step: [7034] d_loss: 1.38998771, g_loss: 0.69289464\n",
      "Step: [7035] d_loss: 1.38368368, g_loss: 0.69836408\n",
      "Step: [7036] d_loss: 1.39008534, g_loss: 0.69334191\n",
      "Step: [7037] d_loss: 1.38989270, g_loss: 0.69498277\n",
      "Step: [7038] d_loss: 1.38414454, g_loss: 0.69772047\n",
      "Step: [7039] d_loss: 1.38796258, g_loss: 0.69329023\n",
      "Step: [7040] d_loss: 1.38947415, g_loss: 0.69411194\n",
      "Step: [7041] d_loss: 1.38636518, g_loss: 0.70026207\n",
      "Step: [7042] d_loss: 1.38426852, g_loss: 0.69268817\n",
      "Step: [7043] d_loss: 1.38453460, g_loss: 0.69941670\n",
      "Step: [7044] d_loss: 1.38404000, g_loss: 0.69878453\n",
      "Step: [7045] d_loss: 1.38336122, g_loss: 0.69839561\n",
      "Step: [7046] d_loss: 1.38633037, g_loss: 0.69687593\n",
      "Step: [7047] d_loss: 1.38577223, g_loss: 0.69721228\n",
      "Step: [7048] d_loss: 1.38278663, g_loss: 0.69437248\n",
      "Step: [7049] d_loss: 1.38511038, g_loss: 0.69722283\n",
      "Step: [7050] d_loss: 1.38491392, g_loss: 0.69830513\n",
      "Step: [7051] d_loss: 1.38746905, g_loss: 0.69770503\n",
      "Step: [7052] d_loss: 1.38729572, g_loss: 0.69282448\n",
      "Step: [7053] d_loss: 1.38542104, g_loss: 0.69352251\n",
      "Step: [7054] d_loss: 1.38594055, g_loss: 0.69248474\n",
      "Step: [7055] d_loss: 1.38121223, g_loss: 0.69889784\n",
      "Step: [7056] d_loss: 1.38845515, g_loss: 0.69679439\n",
      "Step: [7057] d_loss: 1.38691998, g_loss: 0.70232880\n",
      "Step: [7058] d_loss: 1.38724720, g_loss: 0.69475722\n",
      "Step: [7059] d_loss: 1.38490033, g_loss: 0.69411856\n",
      "Step: [7060] d_loss: 1.38485277, g_loss: 0.69626868\n",
      "Step: [7061] d_loss: 1.38570845, g_loss: 0.69557267\n",
      "Step: [7062] d_loss: 1.38560247, g_loss: 0.69718087\n",
      "Step: [7063] d_loss: 1.38392925, g_loss: 0.69608241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7064] d_loss: 1.38420558, g_loss: 0.69605255\n",
      "Step: [7065] d_loss: 1.38613021, g_loss: 0.69365358\n",
      "Step: [7066] d_loss: 1.38838696, g_loss: 0.69580811\n",
      "Step: [7067] d_loss: 1.38618219, g_loss: 0.69884044\n",
      "Step: [7068] d_loss: 1.38718009, g_loss: 0.69663942\n",
      "Step: [7069] d_loss: 1.38611412, g_loss: 0.69579929\n",
      "Step: [7070] d_loss: 1.38695681, g_loss: 0.69237053\n",
      "Step: [7071] d_loss: 1.38647532, g_loss: 0.69465935\n",
      "Step: [7072] d_loss: 1.39038765, g_loss: 0.69477409\n",
      "Step: [7073] d_loss: 1.38733137, g_loss: 0.69892049\n",
      "Step: [7074] d_loss: 1.38542223, g_loss: 0.69846231\n",
      "Step: [7075] d_loss: 1.38638759, g_loss: 0.69574869\n",
      "Step: [7076] d_loss: 1.38772583, g_loss: 0.69373345\n",
      "Step: [7077] d_loss: 1.38751411, g_loss: 0.69471025\n",
      "Step: [7078] d_loss: 1.38510394, g_loss: 0.69682163\n",
      "Step: [7079] d_loss: 1.38241506, g_loss: 0.69934613\n",
      "Step: [7080] d_loss: 1.38491404, g_loss: 0.69648111\n",
      "Step: [7081] d_loss: 1.38531566, g_loss: 0.69734228\n",
      "Step: [7082] d_loss: 1.38463032, g_loss: 0.69792271\n",
      "Step: [7083] d_loss: 1.38358808, g_loss: 0.69929242\n",
      "Step: [7084] d_loss: 1.38170123, g_loss: 0.69740677\n",
      "Step: [7085] d_loss: 1.38786292, g_loss: 0.69638532\n",
      "Step: [7086] d_loss: 1.38381040, g_loss: 0.69913417\n",
      "Step: [7087] d_loss: 1.38028276, g_loss: 0.69795924\n",
      "Step: [7088] d_loss: 1.38439631, g_loss: 0.69768035\n",
      "Step: [7089] d_loss: 1.38378763, g_loss: 0.69558519\n",
      "Step: [7090] d_loss: 1.38555574, g_loss: 0.69834220\n",
      "Step: [7091] d_loss: 1.38353622, g_loss: 0.69911480\n",
      "Step: [7092] d_loss: 1.38742840, g_loss: 0.69262481\n",
      "Step: [7093] d_loss: 1.38940299, g_loss: 0.69261670\n",
      "Step: [7094] d_loss: 1.38773537, g_loss: 0.69691920\n",
      "Step: [7095] d_loss: 1.38825727, g_loss: 0.69402540\n",
      "Step: [7096] d_loss: 1.38836765, g_loss: 0.69312632\n",
      "Step: [7097] d_loss: 1.38331354, g_loss: 0.69741935\n",
      "Step: [7098] d_loss: 1.38600588, g_loss: 0.69717187\n",
      "Step: [7099] d_loss: 1.38597608, g_loss: 0.69552386\n",
      "Step: [7100] d_loss: 1.38774943, g_loss: 0.69264090\n",
      "Step: [7101] d_loss: 1.38725567, g_loss: 0.69697988\n",
      "Step: [7102] d_loss: 1.38520837, g_loss: 0.69555306\n",
      "Step: [7103] d_loss: 1.38468885, g_loss: 0.69632047\n",
      "Step: [7104] d_loss: 1.38698316, g_loss: 0.69539416\n",
      "Step: [7105] d_loss: 1.38459563, g_loss: 0.69818079\n",
      "Step: [7106] d_loss: 1.38794470, g_loss: 0.69749749\n",
      "Step: [7107] d_loss: 1.39053464, g_loss: 0.69570386\n",
      "Step: [7108] d_loss: 1.38765705, g_loss: 0.69572091\n",
      "Step: [7109] d_loss: 1.38716412, g_loss: 0.69669104\n",
      "Step: [7110] d_loss: 1.38793707, g_loss: 0.69432318\n",
      "Step: [7111] d_loss: 1.38608789, g_loss: 0.69353032\n",
      "Step: [7112] d_loss: 1.38722706, g_loss: 0.69446099\n",
      "Step: [7113] d_loss: 1.38740480, g_loss: 0.69469666\n",
      "Step: [7114] d_loss: 1.38413596, g_loss: 0.69789839\n",
      "Step: [7115] d_loss: 1.38709378, g_loss: 0.69552112\n",
      "Step: [7116] d_loss: 1.38541150, g_loss: 0.69714618\n",
      "Step: [7117] d_loss: 1.38079560, g_loss: 0.70079160\n",
      "Step: [7118] d_loss: 1.38379741, g_loss: 0.70101100\n",
      "Step: [7119] d_loss: 1.38348281, g_loss: 0.69614577\n",
      "Step: [7120] d_loss: 1.38756657, g_loss: 0.69759625\n",
      "Step: [7121] d_loss: 1.39089215, g_loss: 0.69527066\n",
      "Step: [7122] d_loss: 1.38337851, g_loss: 0.70110595\n",
      "Step: [7123] d_loss: 1.38469577, g_loss: 0.69744664\n",
      "Step: [7124] d_loss: 1.38485181, g_loss: 0.70120120\n",
      "Step: [7125] d_loss: 1.38459373, g_loss: 0.69669634\n",
      "Step: [7126] d_loss: 1.38305032, g_loss: 0.69756967\n",
      "Step: [7127] d_loss: 1.38396001, g_loss: 0.69594193\n",
      "Step: [7128] d_loss: 1.38576829, g_loss: 0.69388127\n",
      "Step: [7129] d_loss: 1.38418019, g_loss: 0.69564718\n",
      "Step: [7130] d_loss: 1.38457584, g_loss: 0.69779861\n",
      "Step: [7131] d_loss: 1.38568377, g_loss: 0.69782144\n",
      "Step: [7132] d_loss: 1.38586950, g_loss: 0.69690621\n",
      "Step: [7133] d_loss: 1.38704586, g_loss: 0.69776654\n",
      "Step: [7134] d_loss: 1.38606083, g_loss: 0.69884443\n",
      "Step: [7135] d_loss: 1.38784003, g_loss: 0.69682723\n",
      "Step: [7136] d_loss: 1.38670325, g_loss: 0.69538105\n",
      "Step: [7137] d_loss: 1.38967299, g_loss: 0.69331706\n",
      "Step: [7138] d_loss: 1.38961887, g_loss: 0.69760811\n",
      "Step: [7139] d_loss: 1.38562894, g_loss: 0.69307339\n",
      "Step: [7140] d_loss: 1.38876510, g_loss: 0.69298840\n",
      "Step: [7141] d_loss: 1.38642108, g_loss: 0.69836473\n",
      "Step: [7142] d_loss: 1.38349378, g_loss: 0.69833958\n",
      "Step: [7143] d_loss: 1.38623166, g_loss: 0.70070350\n",
      "Step: [7144] d_loss: 1.38516927, g_loss: 0.69695246\n",
      "Step: [7145] d_loss: 1.38127017, g_loss: 0.70019513\n",
      "Step: [7146] d_loss: 1.38448739, g_loss: 0.69625711\n",
      "Step: [7147] d_loss: 1.38668311, g_loss: 0.69676125\n",
      "Step: [7148] d_loss: 1.39319575, g_loss: 0.69247293\n",
      "Step: [7149] d_loss: 1.38833356, g_loss: 0.69460738\n",
      "Step: [7150] d_loss: 1.38486338, g_loss: 0.69367898\n",
      "Step: [7151] d_loss: 1.38321161, g_loss: 0.69807220\n",
      "Step: [7152] d_loss: 1.39037299, g_loss: 0.69619983\n",
      "Step: [7153] d_loss: 1.38947964, g_loss: 0.69564492\n",
      "Step: [7154] d_loss: 1.38481998, g_loss: 0.69506550\n",
      "Step: [7155] d_loss: 1.38394380, g_loss: 0.69795132\n",
      "Step: [7156] d_loss: 1.38583648, g_loss: 0.69776928\n",
      "Step: [7157] d_loss: 1.38037300, g_loss: 0.70007086\n",
      "Step: [7158] d_loss: 1.38625419, g_loss: 0.69867802\n",
      "Step: [7159] d_loss: 1.38293767, g_loss: 0.69971597\n",
      "Step: [7160] d_loss: 1.38741207, g_loss: 0.69491875\n",
      "Step: [7161] d_loss: 1.38414526, g_loss: 0.69881994\n",
      "Step: [7162] d_loss: 1.38661397, g_loss: 0.70376056\n",
      "Step: [7163] d_loss: 1.38659286, g_loss: 0.69890332\n",
      "Step: [7164] d_loss: 1.38419938, g_loss: 0.69916576\n",
      "Step: [7165] d_loss: 1.38608694, g_loss: 0.69850659\n",
      "Step: [7166] d_loss: 1.38550687, g_loss: 0.70435524\n",
      "Step: [7167] d_loss: 1.38434255, g_loss: 0.70139301\n",
      "Step: [7168] d_loss: 1.38472414, g_loss: 0.69946396\n",
      "Step: [7169] d_loss: 1.38903880, g_loss: 0.69217759\n",
      "Step: [7170] d_loss: 1.38583755, g_loss: 0.69445837\n",
      "Step: [7171] d_loss: 1.38688064, g_loss: 0.69681364\n",
      "Step: [7172] d_loss: 1.38749528, g_loss: 0.70014375\n",
      "Step: [7173] d_loss: 1.39129257, g_loss: 0.70478588\n",
      "Step: [7174] d_loss: 1.38987422, g_loss: 0.69975930\n",
      "Step: [7175] d_loss: 1.39084744, g_loss: 0.69712222\n",
      "Step: [7176] d_loss: 1.38859582, g_loss: 0.69666255\n",
      "Step: [7177] d_loss: 1.38673830, g_loss: 0.69377208\n",
      "Step: [7178] d_loss: 1.38560677, g_loss: 0.69360232\n",
      "Step: [7179] d_loss: 1.38840508, g_loss: 0.69397175\n",
      "Step: [7180] d_loss: 1.38539124, g_loss: 0.69867945\n",
      "Step: [7181] d_loss: 1.38609040, g_loss: 0.69699335\n",
      "Step: [7182] d_loss: 1.38495636, g_loss: 0.69657230\n",
      "Step: [7183] d_loss: 1.38598096, g_loss: 0.69317126\n",
      "Step: [7184] d_loss: 1.38473153, g_loss: 0.70144439\n",
      "Step: [7185] d_loss: 1.38546598, g_loss: 0.69813991\n",
      "Step: [7186] d_loss: 1.38456297, g_loss: 0.69624555\n",
      "Step: [7187] d_loss: 1.38365877, g_loss: 0.69646072\n",
      "Step: [7188] d_loss: 1.38681030, g_loss: 0.69463265\n",
      "Step: [7189] d_loss: 1.39284754, g_loss: 0.69511610\n",
      "Step: [7190] d_loss: 1.38650107, g_loss: 0.69305432\n",
      "Step: [7191] d_loss: 1.38617802, g_loss: 0.70020187\n",
      "Step: [7192] d_loss: 1.38532960, g_loss: 0.69759655\n",
      "Step: [7193] d_loss: 1.38645792, g_loss: 0.69493508\n",
      "Step: [7194] d_loss: 1.38321137, g_loss: 0.69894731\n",
      "Step: [7195] d_loss: 1.38646889, g_loss: 0.69682908\n",
      "Step: [7196] d_loss: 1.38715339, g_loss: 0.69409484\n",
      "Step: [7197] d_loss: 1.38516593, g_loss: 0.69418049\n",
      "Step: [7198] d_loss: 1.38691568, g_loss: 0.69336665\n",
      "Step: [7199] d_loss: 1.38816166, g_loss: 0.69687641\n",
      "Step: [7200] d_loss: 1.38296962, g_loss: 0.69612765\n",
      "Step: [7201] d_loss: 1.38864112, g_loss: 0.69340742\n",
      "Step: [7202] d_loss: 1.38448346, g_loss: 0.69523203\n",
      "Step: [7203] d_loss: 1.38781786, g_loss: 0.69620943\n",
      "Step: [7204] d_loss: 1.38280034, g_loss: 0.69729853\n",
      "Step: [7205] d_loss: 1.38628125, g_loss: 0.69687098\n",
      "Step: [7206] d_loss: 1.38587749, g_loss: 0.69923604\n",
      "Step: [7207] d_loss: 1.38751650, g_loss: 0.69686818\n",
      "Step: [7208] d_loss: 1.38623500, g_loss: 0.69505882\n",
      "Step: [7209] d_loss: 1.38583541, g_loss: 0.69480300\n",
      "Step: [7210] d_loss: 1.38483763, g_loss: 0.69881892\n",
      "Step: [7211] d_loss: 1.38833165, g_loss: 0.69638824\n",
      "Step: [7212] d_loss: 1.38652027, g_loss: 0.69602710\n",
      "Step: [7213] d_loss: 1.38770533, g_loss: 0.69601864\n",
      "Step: [7214] d_loss: 1.38628709, g_loss: 0.69544756\n",
      "Step: [7215] d_loss: 1.38307667, g_loss: 0.69961494\n",
      "Step: [7216] d_loss: 1.38621283, g_loss: 0.69721490\n",
      "Step: [7217] d_loss: 1.38598061, g_loss: 0.69919944\n",
      "Step: [7218] d_loss: 1.38773370, g_loss: 0.69735384\n",
      "Step: [7219] d_loss: 1.38770950, g_loss: 0.69356906\n",
      "Step: [7220] d_loss: 1.38955510, g_loss: 0.69578254\n",
      "Step: [7221] d_loss: 1.38714981, g_loss: 0.69590700\n",
      "Step: [7222] d_loss: 1.38766503, g_loss: 0.69852990\n",
      "Step: [7223] d_loss: 1.38958645, g_loss: 0.69376278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7224] d_loss: 1.39139318, g_loss: 0.69698167\n",
      "Step: [7225] d_loss: 1.38794506, g_loss: 0.69375610\n",
      "Step: [7226] d_loss: 1.38903809, g_loss: 0.69482625\n",
      "Step: [7227] d_loss: 1.38738072, g_loss: 0.69743788\n",
      "Step: [7228] d_loss: 1.38894415, g_loss: 0.69479859\n",
      "Step: [7229] d_loss: 1.38750625, g_loss: 0.69696498\n",
      "Step: [7230] d_loss: 1.38399744, g_loss: 0.69553506\n",
      "Step: [7231] d_loss: 1.38710594, g_loss: 0.69535363\n",
      "Step: [7232] d_loss: 1.38546908, g_loss: 0.69771445\n",
      "Step: [7233] d_loss: 1.38462818, g_loss: 0.69808495\n",
      "Step: [7234] d_loss: 1.38411236, g_loss: 0.69585538\n",
      "Step: [7235] d_loss: 1.38647342, g_loss: 0.69493037\n",
      "Step: [7236] d_loss: 1.38467956, g_loss: 0.69801676\n",
      "Step: [7237] d_loss: 1.38637352, g_loss: 0.69683141\n",
      "Step: [7238] d_loss: 1.38329399, g_loss: 0.69879663\n",
      "Step: [7239] d_loss: 1.38327312, g_loss: 0.69908488\n",
      "Step: [7240] d_loss: 1.38715887, g_loss: 0.69758534\n",
      "Step: [7241] d_loss: 1.38639784, g_loss: 0.69348836\n",
      "Step: [7242] d_loss: 1.38610768, g_loss: 0.69564378\n",
      "Step: [7243] d_loss: 1.38466752, g_loss: 0.69709921\n",
      "Step: [7244] d_loss: 1.38526511, g_loss: 0.69614834\n",
      "Step: [7245] d_loss: 1.38626146, g_loss: 0.69687223\n",
      "Step: [7246] d_loss: 1.38409758, g_loss: 0.69721925\n",
      "Step: [7247] d_loss: 1.38372576, g_loss: 0.69507658\n",
      "Step: [7248] d_loss: 1.38179231, g_loss: 0.69804996\n",
      "Step: [7249] d_loss: 1.38795304, g_loss: 0.69688582\n",
      "Step: [7250] d_loss: 1.38872266, g_loss: 0.69635743\n",
      "Step: [7251] d_loss: 1.38275909, g_loss: 0.69166267\n",
      "Step: [7252] d_loss: 1.38577628, g_loss: 0.69417006\n",
      "Step: [7253] d_loss: 1.37814164, g_loss: 0.69857734\n",
      "Step: [7254] d_loss: 1.38821781, g_loss: 0.69167256\n",
      "Step: [7255] d_loss: 1.38558912, g_loss: 0.69667017\n",
      "Step: [7256] d_loss: 1.38745928, g_loss: 0.69369972\n",
      "Step: [7257] d_loss: 1.38579965, g_loss: 0.69522583\n",
      "Step: [7258] d_loss: 1.38644695, g_loss: 0.69551593\n",
      "Step: [7259] d_loss: 1.38494706, g_loss: 0.69443882\n",
      "Step: [7260] d_loss: 1.38548660, g_loss: 0.69187945\n",
      "Step: [7261] d_loss: 1.38463879, g_loss: 0.69450164\n",
      "Step: [7262] d_loss: 1.38648331, g_loss: 0.70101106\n",
      "Step: [7263] d_loss: 1.38927627, g_loss: 0.70086628\n",
      "Step: [7264] d_loss: 1.38632274, g_loss: 0.69781601\n",
      "Step: [7265] d_loss: 1.38638425, g_loss: 0.69429624\n",
      "Step: [7266] d_loss: 1.38915157, g_loss: 0.69385487\n",
      "Step: [7267] d_loss: 1.38546729, g_loss: 0.69512045\n",
      "Step: [7268] d_loss: 1.38568199, g_loss: 0.69555926\n",
      "Step: [7269] d_loss: 1.38372493, g_loss: 0.69707805\n",
      "Step: [7270] d_loss: 1.38380122, g_loss: 0.69399297\n",
      "Step: [7271] d_loss: 1.38313437, g_loss: 0.69591391\n",
      "Step: [7272] d_loss: 1.38696420, g_loss: 0.68933725\n",
      "Step: [7273] d_loss: 1.38723302, g_loss: 0.69347090\n",
      "Step: [7274] d_loss: 1.38648582, g_loss: 0.69689220\n",
      "Step: [7275] d_loss: 1.38595891, g_loss: 0.69544065\n",
      "Step: [7276] d_loss: 1.38745928, g_loss: 0.69622314\n",
      "Step: [7277] d_loss: 1.38601804, g_loss: 0.69348896\n",
      "Step: [7278] d_loss: 1.38548493, g_loss: 0.69710052\n",
      "Step: [7279] d_loss: 1.38496959, g_loss: 0.69276404\n",
      "Step: [7280] d_loss: 1.38586652, g_loss: 0.69534630\n",
      "Step: [7281] d_loss: 1.38628399, g_loss: 0.69637567\n",
      "Step: [7282] d_loss: 1.38782299, g_loss: 0.69467068\n",
      "Step: [7283] d_loss: 1.38448632, g_loss: 0.69718909\n",
      "Step: [7284] d_loss: 1.38624322, g_loss: 0.69523108\n",
      "Step: [7285] d_loss: 1.38609910, g_loss: 0.69364607\n",
      "Step: [7286] d_loss: 1.38602710, g_loss: 0.69789588\n",
      "Step: [7287] d_loss: 1.38512194, g_loss: 0.69749349\n",
      "Step: [7288] d_loss: 1.38521218, g_loss: 0.69453359\n",
      "Step: [7289] d_loss: 1.39011061, g_loss: 0.69413620\n",
      "Step: [7290] d_loss: 1.38646173, g_loss: 0.69926071\n",
      "Step: [7291] d_loss: 1.39099813, g_loss: 0.69552624\n",
      "Step: [7292] d_loss: 1.38818681, g_loss: 0.69381487\n",
      "Step: [7293] d_loss: 1.38987207, g_loss: 0.69179332\n",
      "Step: [7294] d_loss: 1.38799644, g_loss: 0.69643921\n",
      "Step: [7295] d_loss: 1.38555813, g_loss: 0.69828898\n",
      "Step: [7296] d_loss: 1.38713968, g_loss: 0.69781041\n",
      "Step: [7297] d_loss: 1.38811874, g_loss: 0.69567871\n",
      "Step: [7298] d_loss: 1.38558352, g_loss: 0.69466066\n",
      "Step: [7299] d_loss: 1.38354218, g_loss: 0.69724154\n",
      "Step: [7300] d_loss: 1.38519716, g_loss: 0.69687480\n",
      "Step: [7301] d_loss: 1.38784230, g_loss: 0.69654739\n",
      "Step: [7302] d_loss: 1.38832152, g_loss: 0.69361359\n",
      "Step: [7303] d_loss: 1.38415861, g_loss: 0.69712996\n",
      "Step: [7304] d_loss: 1.38231277, g_loss: 0.69792354\n",
      "Step: [7305] d_loss: 1.38610625, g_loss: 0.69327366\n",
      "Step: [7306] d_loss: 1.38487220, g_loss: 0.69686675\n",
      "Step: [7307] d_loss: 1.38403153, g_loss: 0.69755876\n",
      "Step: [7308] d_loss: 1.38476419, g_loss: 0.69544178\n",
      "Step: [7309] d_loss: 1.38980007, g_loss: 0.69274020\n",
      "Step: [7310] d_loss: 1.38422322, g_loss: 0.69802481\n",
      "Step: [7311] d_loss: 1.38473475, g_loss: 0.69460106\n",
      "Step: [7312] d_loss: 1.38604808, g_loss: 0.69438350\n",
      "Step: [7313] d_loss: 1.38585997, g_loss: 0.69538629\n",
      "Step: [7314] d_loss: 1.38343382, g_loss: 0.69627774\n",
      "Step: [7315] d_loss: 1.38703823, g_loss: 0.69836521\n",
      "Step: [7316] d_loss: 1.38583517, g_loss: 0.69671834\n",
      "Step: [7317] d_loss: 1.38638294, g_loss: 0.69442594\n",
      "Step: [7318] d_loss: 1.38748336, g_loss: 0.69335318\n",
      "Step: [7319] d_loss: 1.38483071, g_loss: 0.69663274\n",
      "Step: [7320] d_loss: 1.38975525, g_loss: 0.69754112\n",
      "Step: [7321] d_loss: 1.38825226, g_loss: 0.69389457\n",
      "Step: [7322] d_loss: 1.38659966, g_loss: 0.69511199\n",
      "Step: [7323] d_loss: 1.38512492, g_loss: 0.69683242\n",
      "Step: [7324] d_loss: 1.38531780, g_loss: 0.69309139\n",
      "Step: [7325] d_loss: 1.38768899, g_loss: 0.69212413\n",
      "Step: [7326] d_loss: 1.38501930, g_loss: 0.69478053\n",
      "Step: [7327] d_loss: 1.38933837, g_loss: 0.69374228\n",
      "Step: [7328] d_loss: 1.38453543, g_loss: 0.69666922\n",
      "Step: [7329] d_loss: 1.38575935, g_loss: 0.69957852\n",
      "Step: [7330] d_loss: 1.38502014, g_loss: 0.69609284\n",
      "Step: [7331] d_loss: 1.38528073, g_loss: 0.69798750\n",
      "Step: [7332] d_loss: 1.38501859, g_loss: 0.69325018\n",
      "Step: [7333] d_loss: 1.38616085, g_loss: 0.69233501\n",
      "Step: [7334] d_loss: 1.38667595, g_loss: 0.69571394\n",
      "Step: [7335] d_loss: 1.38646531, g_loss: 0.69661927\n",
      "Step: [7336] d_loss: 1.38372362, g_loss: 0.69823813\n",
      "Step: [7337] d_loss: 1.38756752, g_loss: 0.69314843\n",
      "Step: [7338] d_loss: 1.38449645, g_loss: 0.69482625\n",
      "Step: [7339] d_loss: 1.38838053, g_loss: 0.69725966\n",
      "Step: [7340] d_loss: 1.38905883, g_loss: 0.69145131\n",
      "Step: [7341] d_loss: 1.38572407, g_loss: 0.70057189\n",
      "Step: [7342] d_loss: 1.38667607, g_loss: 0.69646597\n",
      "Step: [7343] d_loss: 1.38926411, g_loss: 0.68978786\n",
      "Step: [7344] d_loss: 1.38376737, g_loss: 0.69319540\n",
      "Step: [7345] d_loss: 1.38479114, g_loss: 0.69447160\n",
      "Step: [7346] d_loss: 1.38727975, g_loss: 0.69808531\n",
      "Step: [7347] d_loss: 1.38687110, g_loss: 0.69303888\n",
      "Step: [7348] d_loss: 1.38500071, g_loss: 0.69596201\n",
      "Step: [7349] d_loss: 1.38573647, g_loss: 0.69429773\n",
      "Step: [7350] d_loss: 1.38426089, g_loss: 0.69672710\n",
      "Step: [7351] d_loss: 1.38374448, g_loss: 0.69722110\n",
      "Step: [7352] d_loss: 1.38662827, g_loss: 0.69517976\n",
      "Step: [7353] d_loss: 1.38704026, g_loss: 0.69914806\n",
      "Step: [7354] d_loss: 1.38594306, g_loss: 0.69743741\n",
      "Step: [7355] d_loss: 1.38534915, g_loss: 0.69984412\n",
      "Step: [7356] d_loss: 1.38628435, g_loss: 0.69646704\n",
      "Step: [7357] d_loss: 1.38815176, g_loss: 0.69356179\n",
      "Step: [7358] d_loss: 1.38584340, g_loss: 0.69330013\n",
      "Step: [7359] d_loss: 1.38548422, g_loss: 0.69384861\n",
      "Step: [7360] d_loss: 1.38598323, g_loss: 0.69649231\n",
      "Step: [7361] d_loss: 1.38717294, g_loss: 0.69432056\n",
      "Step: [7362] d_loss: 1.38617086, g_loss: 0.69315195\n",
      "Step: [7363] d_loss: 1.38678813, g_loss: 0.69741201\n",
      "Step: [7364] d_loss: 1.38655353, g_loss: 0.69455665\n",
      "Step: [7365] d_loss: 1.38530326, g_loss: 0.69811463\n",
      "Step: [7366] d_loss: 1.38545156, g_loss: 0.69228745\n",
      "Step: [7367] d_loss: 1.38779950, g_loss: 0.69800460\n",
      "Step: [7368] d_loss: 1.38936794, g_loss: 0.69819313\n",
      "Step: [7369] d_loss: 1.38871992, g_loss: 0.69991761\n",
      "Step: [7370] d_loss: 1.38732672, g_loss: 0.69501889\n",
      "Step: [7371] d_loss: 1.38661027, g_loss: 0.69240963\n",
      "Step: [7372] d_loss: 1.38227654, g_loss: 0.69805431\n",
      "Step: [7373] d_loss: 1.38380599, g_loss: 0.69556284\n",
      "Step: [7374] d_loss: 1.38680792, g_loss: 0.69560331\n",
      "Step: [7375] d_loss: 1.38236499, g_loss: 0.69534689\n",
      "Step: [7376] d_loss: 1.38343954, g_loss: 0.69631517\n",
      "Step: [7377] d_loss: 1.38580394, g_loss: 0.69514674\n",
      "Step: [7378] d_loss: 1.38504624, g_loss: 0.69675136\n",
      "Step: [7379] d_loss: 1.38652444, g_loss: 0.69270849\n",
      "Step: [7380] d_loss: 1.38306618, g_loss: 0.69822222\n",
      "Step: [7381] d_loss: 1.38787067, g_loss: 0.69605136\n",
      "Step: [7382] d_loss: 1.38062954, g_loss: 0.69916213\n",
      "Step: [7383] d_loss: 1.38695502, g_loss: 0.69263607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7384] d_loss: 1.38892674, g_loss: 0.69225448\n",
      "Step: [7385] d_loss: 1.38726473, g_loss: 0.69260585\n",
      "Step: [7386] d_loss: 1.38553894, g_loss: 0.69543999\n",
      "Step: [7387] d_loss: 1.38717735, g_loss: 0.69514722\n",
      "Step: [7388] d_loss: 1.38368821, g_loss: 0.69828540\n",
      "Step: [7389] d_loss: 1.38448524, g_loss: 0.69523919\n",
      "Step: [7390] d_loss: 1.38352537, g_loss: 0.69925058\n",
      "Step: [7391] d_loss: 1.38410425, g_loss: 0.69425273\n",
      "Step: [7392] d_loss: 1.38589668, g_loss: 0.69464725\n",
      "Step: [7393] d_loss: 1.38582182, g_loss: 0.69185948\n",
      "Step: [7394] d_loss: 1.38537335, g_loss: 0.69645846\n",
      "Step: [7395] d_loss: 1.38564146, g_loss: 0.70084971\n",
      "Step: [7396] d_loss: 1.38835764, g_loss: 0.69435710\n",
      "Step: [7397] d_loss: 1.38750827, g_loss: 0.69483364\n",
      "Step: [7398] d_loss: 1.38560283, g_loss: 0.69332123\n",
      "Step: [7399] d_loss: 1.38654172, g_loss: 0.69270647\n",
      "Step: [7400] d_loss: 1.38478136, g_loss: 0.69576621\n",
      "Step: [7401] d_loss: 1.38891757, g_loss: 0.70153308\n",
      "Step: [7402] d_loss: 1.38761103, g_loss: 0.69531018\n",
      "Step: [7403] d_loss: 1.38756943, g_loss: 0.69382775\n",
      "Step: [7404] d_loss: 1.38494253, g_loss: 0.68979073\n",
      "Step: [7405] d_loss: 1.38432074, g_loss: 0.69349813\n",
      "Step: [7406] d_loss: 1.38552594, g_loss: 0.69902849\n",
      "Step: [7407] d_loss: 1.38338423, g_loss: 0.69557345\n",
      "Step: [7408] d_loss: 1.38690984, g_loss: 0.69779408\n",
      "Step: [7409] d_loss: 1.38624167, g_loss: 0.69574589\n",
      "Step: [7410] d_loss: 1.38576293, g_loss: 0.69524735\n",
      "Step: [7411] d_loss: 1.38529527, g_loss: 0.69498616\n",
      "Step: [7412] d_loss: 1.38786626, g_loss: 0.69081593\n",
      "Step: [7413] d_loss: 1.38483596, g_loss: 0.69691175\n",
      "Step: [7414] d_loss: 1.38762355, g_loss: 0.69674635\n",
      "Step: [7415] d_loss: 1.38381660, g_loss: 0.69638073\n",
      "Step: [7416] d_loss: 1.38587630, g_loss: 0.69881159\n",
      "Step: [7417] d_loss: 1.38745213, g_loss: 0.69430310\n",
      "Step: [7418] d_loss: 1.38513350, g_loss: 0.69452322\n",
      "Step: [7419] d_loss: 1.38393641, g_loss: 0.69529867\n",
      "Step: [7420] d_loss: 1.38532281, g_loss: 0.69389546\n",
      "Step: [7421] d_loss: 1.38567150, g_loss: 0.69795656\n",
      "Step: [7422] d_loss: 1.38520741, g_loss: 0.69548678\n",
      "Step: [7423] d_loss: 1.38472533, g_loss: 0.69686699\n",
      "Step: [7424] d_loss: 1.38667357, g_loss: 0.70031536\n",
      "Step: [7425] d_loss: 1.38354886, g_loss: 0.69077611\n",
      "Step: [7426] d_loss: 1.38585746, g_loss: 0.69557059\n",
      "Step: [7427] d_loss: 1.38533044, g_loss: 0.69678587\n",
      "Step: [7428] d_loss: 1.38967848, g_loss: 0.69372326\n",
      "Step: [7429] d_loss: 1.38999057, g_loss: 0.69633698\n",
      "Step: [7430] d_loss: 1.39100087, g_loss: 0.68925768\n",
      "Step: [7431] d_loss: 1.38681459, g_loss: 0.69372082\n",
      "Step: [7432] d_loss: 1.38631105, g_loss: 0.69577718\n",
      "Step: [7433] d_loss: 1.38645613, g_loss: 0.69691515\n",
      "Step: [7434] d_loss: 1.38654923, g_loss: 0.69618577\n",
      "Step: [7435] d_loss: 1.38989878, g_loss: 0.69405818\n",
      "Step: [7436] d_loss: 1.38667870, g_loss: 0.69525814\n",
      "Step: [7437] d_loss: 1.38788390, g_loss: 0.69411981\n",
      "Step: [7438] d_loss: 1.38957548, g_loss: 0.69395667\n",
      "Step: [7439] d_loss: 1.38602805, g_loss: 0.69529563\n",
      "Step: [7440] d_loss: 1.38602269, g_loss: 0.69335073\n",
      "Step: [7441] d_loss: 1.38599348, g_loss: 0.69409919\n",
      "Step: [7442] d_loss: 1.38671756, g_loss: 0.69838804\n",
      "Step: [7443] d_loss: 1.38771558, g_loss: 0.69791263\n",
      "Step: [7444] d_loss: 1.38697553, g_loss: 0.69337296\n",
      "Step: [7445] d_loss: 1.38761425, g_loss: 0.69572484\n",
      "Step: [7446] d_loss: 1.38833213, g_loss: 0.69626522\n",
      "Step: [7447] d_loss: 1.38386655, g_loss: 0.69680905\n",
      "Step: [7448] d_loss: 1.38451707, g_loss: 0.69641787\n",
      "Step: [7449] d_loss: 1.38742352, g_loss: 0.69384658\n",
      "Step: [7450] d_loss: 1.38626337, g_loss: 0.69289577\n",
      "Step: [7451] d_loss: 1.38772452, g_loss: 0.69515771\n",
      "Step: [7452] d_loss: 1.38510609, g_loss: 0.69580007\n",
      "Step: [7453] d_loss: 1.38606548, g_loss: 0.69646436\n",
      "Step: [7454] d_loss: 1.38675499, g_loss: 0.69400930\n",
      "Step: [7455] d_loss: 1.38475943, g_loss: 0.69591463\n",
      "Step: [7456] d_loss: 1.38742900, g_loss: 0.69470179\n",
      "Step: [7457] d_loss: 1.38480496, g_loss: 0.69678849\n",
      "Step: [7458] d_loss: 1.38342404, g_loss: 0.69637185\n",
      "Step: [7459] d_loss: 1.38815451, g_loss: 0.69469160\n",
      "Step: [7460] d_loss: 1.38479233, g_loss: 0.69413233\n",
      "Step: [7461] d_loss: 1.38641226, g_loss: 0.69846070\n",
      "Step: [7462] d_loss: 1.38663232, g_loss: 0.69469458\n",
      "Step: [7463] d_loss: 1.38587594, g_loss: 0.69250643\n",
      "Step: [7464] d_loss: 1.38622451, g_loss: 0.69530845\n",
      "Step: [7465] d_loss: 1.38406968, g_loss: 0.69524598\n",
      "Step: [7466] d_loss: 1.38667727, g_loss: 0.69563001\n",
      "Step: [7467] d_loss: 1.38576066, g_loss: 0.69658577\n",
      "Step: [7468] d_loss: 1.38493669, g_loss: 0.69886756\n",
      "Step: [7469] d_loss: 1.38656640, g_loss: 0.69619870\n",
      "Step: [7470] d_loss: 1.38650560, g_loss: 0.69277960\n",
      "Step: [7471] d_loss: 1.38392258, g_loss: 0.69586313\n",
      "Step: [7472] d_loss: 1.38707423, g_loss: 0.69212800\n",
      "Step: [7473] d_loss: 1.38676405, g_loss: 0.69520521\n",
      "Step: [7474] d_loss: 1.38672566, g_loss: 0.69623858\n",
      "Step: [7475] d_loss: 1.38531113, g_loss: 0.69360834\n",
      "Step: [7476] d_loss: 1.38591886, g_loss: 0.69600213\n",
      "Step: [7477] d_loss: 1.38752770, g_loss: 0.69233131\n",
      "Step: [7478] d_loss: 1.38501883, g_loss: 0.69544721\n",
      "Step: [7479] d_loss: 1.38598049, g_loss: 0.69697118\n",
      "Step: [7480] d_loss: 1.38492846, g_loss: 0.69782770\n",
      "Step: [7481] d_loss: 1.38437593, g_loss: 0.69940853\n",
      "Step: [7482] d_loss: 1.38549161, g_loss: 0.69497013\n",
      "Step: [7483] d_loss: 1.38562298, g_loss: 0.69504970\n",
      "Step: [7484] d_loss: 1.38579571, g_loss: 0.69437701\n",
      "Step: [7485] d_loss: 1.38393497, g_loss: 0.69874024\n",
      "Step: [7486] d_loss: 1.38608575, g_loss: 0.69749242\n",
      "Step: [7487] d_loss: 1.38706136, g_loss: 0.69356376\n",
      "Step: [7488] d_loss: 1.38642001, g_loss: 0.69228888\n",
      "Step: [7489] d_loss: 1.38778222, g_loss: 0.69501615\n",
      "Step: [7490] d_loss: 1.38901734, g_loss: 0.69293946\n",
      "Step: [7491] d_loss: 1.38617277, g_loss: 0.69719028\n",
      "Step: [7492] d_loss: 1.38586199, g_loss: 0.69504797\n",
      "Step: [7493] d_loss: 1.38283014, g_loss: 0.69751143\n",
      "Step: [7494] d_loss: 1.38794291, g_loss: 0.69307613\n",
      "Step: [7495] d_loss: 1.38396549, g_loss: 0.69460338\n",
      "Step: [7496] d_loss: 1.38795471, g_loss: 0.69522065\n",
      "Step: [7497] d_loss: 1.38380301, g_loss: 0.69679856\n",
      "Step: [7498] d_loss: 1.38752198, g_loss: 0.69516611\n",
      "Step: [7499] d_loss: 1.38555646, g_loss: 0.69564438\n",
      "Step: [7500] d_loss: 1.38780165, g_loss: 0.69072694\n",
      "Step: [7501] d_loss: 1.38621950, g_loss: 0.69509101\n",
      "Step: [7502] d_loss: 1.38791633, g_loss: 0.69429535\n",
      "Step: [7503] d_loss: 1.38794935, g_loss: 0.69346380\n",
      "Step: [7504] d_loss: 1.38724041, g_loss: 0.69405776\n",
      "Step: [7505] d_loss: 1.38518775, g_loss: 0.69528890\n",
      "Step: [7506] d_loss: 1.38452089, g_loss: 0.69672298\n",
      "Step: [7507] d_loss: 1.38634396, g_loss: 0.69604301\n",
      "Step: [7508] d_loss: 1.38490605, g_loss: 0.69610083\n",
      "Step: [7509] d_loss: 1.38643932, g_loss: 0.69394898\n",
      "Step: [7510] d_loss: 1.38710570, g_loss: 0.69490397\n",
      "Step: [7511] d_loss: 1.38617301, g_loss: 0.69358844\n",
      "Step: [7512] d_loss: 1.38647079, g_loss: 0.69434476\n",
      "Step: [7513] d_loss: 1.38540924, g_loss: 0.69592363\n",
      "Step: [7514] d_loss: 1.38727701, g_loss: 0.69367564\n",
      "Step: [7515] d_loss: 1.38645053, g_loss: 0.69324100\n",
      "Step: [7516] d_loss: 1.38557267, g_loss: 0.69500053\n",
      "Step: [7517] d_loss: 1.38568902, g_loss: 0.69761813\n",
      "Step: [7518] d_loss: 1.38761210, g_loss: 0.69560432\n",
      "Step: [7519] d_loss: 1.38483500, g_loss: 0.69414794\n",
      "Step: [7520] d_loss: 1.38700771, g_loss: 0.69476533\n",
      "Step: [7521] d_loss: 1.38754392, g_loss: 0.69347280\n",
      "Step: [7522] d_loss: 1.38556206, g_loss: 0.69437885\n",
      "Step: [7523] d_loss: 1.38547635, g_loss: 0.69741058\n",
      "Step: [7524] d_loss: 1.38663054, g_loss: 0.69221771\n",
      "Step: [7525] d_loss: 1.38564730, g_loss: 0.69589996\n",
      "Step: [7526] d_loss: 1.38443303, g_loss: 0.69523501\n",
      "Step: [7527] d_loss: 1.38422155, g_loss: 0.69645315\n",
      "Step: [7528] d_loss: 1.38560927, g_loss: 0.69512200\n",
      "Step: [7529] d_loss: 1.38722658, g_loss: 0.69461775\n",
      "Step: [7530] d_loss: 1.38532615, g_loss: 0.69569278\n",
      "Step: [7531] d_loss: 1.38501000, g_loss: 0.69439793\n",
      "Step: [7532] d_loss: 1.38619733, g_loss: 0.69350088\n",
      "Step: [7533] d_loss: 1.38712347, g_loss: 0.69380838\n",
      "Step: [7534] d_loss: 1.38268137, g_loss: 0.69954324\n",
      "Step: [7535] d_loss: 1.38587439, g_loss: 0.69757581\n",
      "Step: [7536] d_loss: 1.38537347, g_loss: 0.69437325\n",
      "Step: [7537] d_loss: 1.38629556, g_loss: 0.69485676\n",
      "Step: [7538] d_loss: 1.38664913, g_loss: 0.69510150\n",
      "Step: [7539] d_loss: 1.38503027, g_loss: 0.69572091\n",
      "Step: [7540] d_loss: 1.38503695, g_loss: 0.69990325\n",
      "Step: [7541] d_loss: 1.38526344, g_loss: 0.70100880\n",
      "Step: [7542] d_loss: 1.38606441, g_loss: 0.69819725\n",
      "Step: [7543] d_loss: 1.38452721, g_loss: 0.69336331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7544] d_loss: 1.38566899, g_loss: 0.69495487\n",
      "Step: [7545] d_loss: 1.38665688, g_loss: 0.69528991\n",
      "Step: [7546] d_loss: 1.38437927, g_loss: 0.69604486\n",
      "Step: [7547] d_loss: 1.38800931, g_loss: 0.69627744\n",
      "Step: [7548] d_loss: 1.38681936, g_loss: 0.69425046\n",
      "Step: [7549] d_loss: 1.38833666, g_loss: 0.69421136\n",
      "Step: [7550] d_loss: 1.38728821, g_loss: 0.69636989\n",
      "Step: [7551] d_loss: 1.38928914, g_loss: 0.69257706\n",
      "Step: [7552] d_loss: 1.38691652, g_loss: 0.69668788\n",
      "Step: [7553] d_loss: 1.38677931, g_loss: 0.69477791\n",
      "Step: [7554] d_loss: 1.38419676, g_loss: 0.69926935\n",
      "Step: [7555] d_loss: 1.38578534, g_loss: 0.69656307\n",
      "Step: [7556] d_loss: 1.38585711, g_loss: 0.69770873\n",
      "Step: [7557] d_loss: 1.38499308, g_loss: 0.69514132\n",
      "Step: [7558] d_loss: 1.38364792, g_loss: 0.69474298\n",
      "Step: [7559] d_loss: 1.38614237, g_loss: 0.69250333\n",
      "Step: [7560] d_loss: 1.38706899, g_loss: 0.69629157\n",
      "Step: [7561] d_loss: 1.38683677, g_loss: 0.69695961\n",
      "Step: [7562] d_loss: 1.38688397, g_loss: 0.69699782\n",
      "Step: [7563] d_loss: 1.38380694, g_loss: 0.69308460\n",
      "Step: [7564] d_loss: 1.38517451, g_loss: 0.69309318\n",
      "Step: [7565] d_loss: 1.38749182, g_loss: 0.69423980\n",
      "Step: [7566] d_loss: 1.38786638, g_loss: 0.69762152\n",
      "Step: [7567] d_loss: 1.38778496, g_loss: 0.69553536\n",
      "Step: [7568] d_loss: 1.38894403, g_loss: 0.69172728\n",
      "Step: [7569] d_loss: 1.38780320, g_loss: 0.69324172\n",
      "Step: [7570] d_loss: 1.38841343, g_loss: 0.69605100\n",
      "Step: [7571] d_loss: 1.38848078, g_loss: 0.69043005\n",
      "Step: [7572] d_loss: 1.38580585, g_loss: 0.69726241\n",
      "Step: [7573] d_loss: 1.38695061, g_loss: 0.69406033\n",
      "Step: [7574] d_loss: 1.38551807, g_loss: 0.69700408\n",
      "Step: [7575] d_loss: 1.38796687, g_loss: 0.69407552\n",
      "Step: [7576] d_loss: 1.38311720, g_loss: 0.69558895\n",
      "Step: [7577] d_loss: 1.38496995, g_loss: 0.69488323\n",
      "Step: [7578] d_loss: 1.38433027, g_loss: 0.69458330\n",
      "Step: [7579] d_loss: 1.38380122, g_loss: 0.69060987\n",
      "Step: [7580] d_loss: 1.38486373, g_loss: 0.69793558\n",
      "Step: [7581] d_loss: 1.38588345, g_loss: 0.69634473\n",
      "Step: [7582] d_loss: 1.38249159, g_loss: 0.70026964\n",
      "Step: [7583] d_loss: 1.38524699, g_loss: 0.69985390\n",
      "Step: [7584] d_loss: 1.38430572, g_loss: 0.69384438\n",
      "Step: [7585] d_loss: 1.38458753, g_loss: 0.69525141\n",
      "Step: [7586] d_loss: 1.38573730, g_loss: 0.69636971\n",
      "Step: [7587] d_loss: 1.38692904, g_loss: 0.69689697\n",
      "Step: [7588] d_loss: 1.38784575, g_loss: 0.69481468\n",
      "Step: [7589] d_loss: 1.38450027, g_loss: 0.69317096\n",
      "Step: [7590] d_loss: 1.38568616, g_loss: 0.69523317\n",
      "Step: [7591] d_loss: 1.38622606, g_loss: 0.69389415\n",
      "Step: [7592] d_loss: 1.38730311, g_loss: 0.69777936\n",
      "Step: [7593] d_loss: 1.38348413, g_loss: 0.69960749\n",
      "Step: [7594] d_loss: 1.38562417, g_loss: 0.69834065\n",
      "Step: [7595] d_loss: 1.38586307, g_loss: 0.69454145\n",
      "Step: [7596] d_loss: 1.38622594, g_loss: 0.69775951\n",
      "Step: [7597] d_loss: 1.38748777, g_loss: 0.69544673\n",
      "Step: [7598] d_loss: 1.38760722, g_loss: 0.69368875\n",
      "Step: [7599] d_loss: 1.38475156, g_loss: 0.69691408\n",
      "Step: [7600] d_loss: 1.38791704, g_loss: 0.69712174\n",
      "Step: [7601] d_loss: 1.38863444, g_loss: 0.69313496\n",
      "Step: [7602] d_loss: 1.38701630, g_loss: 0.69199032\n",
      "Step: [7603] d_loss: 1.38648283, g_loss: 0.69626886\n",
      "Step: [7604] d_loss: 1.38643610, g_loss: 0.69858241\n",
      "Step: [7605] d_loss: 1.38802552, g_loss: 0.69864810\n",
      "Step: [7606] d_loss: 1.38500500, g_loss: 0.69619536\n",
      "Step: [7607] d_loss: 1.38511968, g_loss: 0.69934583\n",
      "Step: [7608] d_loss: 1.38499475, g_loss: 0.69524443\n",
      "Step: [7609] d_loss: 1.38703036, g_loss: 0.69881016\n",
      "Step: [7610] d_loss: 1.38777447, g_loss: 0.69313127\n",
      "Step: [7611] d_loss: 1.38660550, g_loss: 0.69540298\n",
      "Step: [7612] d_loss: 1.38645267, g_loss: 0.69507658\n",
      "Step: [7613] d_loss: 1.38484752, g_loss: 0.69791347\n",
      "Step: [7614] d_loss: 1.38540483, g_loss: 0.69667447\n",
      "Step: [7615] d_loss: 1.38384175, g_loss: 0.69328433\n",
      "Step: [7616] d_loss: 1.38490081, g_loss: 0.69223952\n",
      "Step: [7617] d_loss: 1.38748527, g_loss: 0.69523871\n",
      "Step: [7618] d_loss: 1.38687611, g_loss: 0.69716585\n",
      "Step: [7619] d_loss: 1.38592720, g_loss: 0.69717348\n",
      "Step: [7620] d_loss: 1.38403499, g_loss: 0.69637388\n",
      "Step: [7621] d_loss: 1.38631403, g_loss: 0.69500899\n",
      "Step: [7622] d_loss: 1.38825500, g_loss: 0.69337404\n",
      "Step: [7623] d_loss: 1.38490283, g_loss: 0.69448048\n",
      "Step: [7624] d_loss: 1.38966560, g_loss: 0.69436997\n",
      "Step: [7625] d_loss: 1.38509083, g_loss: 0.69672596\n",
      "Step: [7626] d_loss: 1.38694596, g_loss: 0.69129509\n",
      "Step: [7627] d_loss: 1.38691735, g_loss: 0.69086266\n",
      "Step: [7628] d_loss: 1.38736033, g_loss: 0.69298708\n",
      "Step: [7629] d_loss: 1.38865137, g_loss: 0.69496346\n",
      "Step: [7630] d_loss: 1.38422644, g_loss: 0.69680750\n",
      "Step: [7631] d_loss: 1.38689446, g_loss: 0.69417214\n",
      "Step: [7632] d_loss: 1.38302350, g_loss: 0.69530064\n",
      "Step: [7633] d_loss: 1.38633871, g_loss: 0.70087647\n",
      "Step: [7634] d_loss: 1.38469994, g_loss: 0.69789350\n",
      "Step: [7635] d_loss: 1.38674831, g_loss: 0.69454849\n",
      "Step: [7636] d_loss: 1.38885069, g_loss: 0.69829172\n",
      "Step: [7637] d_loss: 1.38917446, g_loss: 0.69956005\n",
      "Step: [7638] d_loss: 1.38579059, g_loss: 0.70126212\n",
      "Step: [7639] d_loss: 1.38744116, g_loss: 0.69034040\n",
      "Step: [7640] d_loss: 1.38517737, g_loss: 0.69524789\n",
      "Step: [7641] d_loss: 1.38711786, g_loss: 0.69525421\n",
      "Step: [7642] d_loss: 1.38828671, g_loss: 0.69457817\n",
      "Step: [7643] d_loss: 1.38483453, g_loss: 0.69797885\n",
      "Step: [7644] d_loss: 1.38731742, g_loss: 0.69530380\n",
      "Step: [7645] d_loss: 1.38554931, g_loss: 0.69775975\n",
      "Step: [7646] d_loss: 1.38572133, g_loss: 0.69562137\n",
      "Step: [7647] d_loss: 1.38991451, g_loss: 0.69148397\n",
      "Step: [7648] d_loss: 1.38878417, g_loss: 0.69852531\n",
      "Step: [7649] d_loss: 1.38641739, g_loss: 0.69726002\n",
      "Step: [7650] d_loss: 1.38486099, g_loss: 0.69643617\n",
      "Step: [7651] d_loss: 1.38642490, g_loss: 0.69541001\n",
      "Step: [7652] d_loss: 1.38641727, g_loss: 0.69657612\n",
      "Step: [7653] d_loss: 1.38616133, g_loss: 0.69588292\n",
      "Step: [7654] d_loss: 1.38533998, g_loss: 0.69593960\n",
      "Step: [7655] d_loss: 1.38513768, g_loss: 0.69580734\n",
      "Step: [7656] d_loss: 1.38507414, g_loss: 0.69500697\n",
      "Step: [7657] d_loss: 1.38363385, g_loss: 0.69677269\n",
      "Step: [7658] d_loss: 1.38408899, g_loss: 0.69720191\n",
      "Step: [7659] d_loss: 1.38769913, g_loss: 0.69709063\n",
      "Step: [7660] d_loss: 1.38816106, g_loss: 0.69342196\n",
      "Step: [7661] d_loss: 1.38496518, g_loss: 0.69521403\n",
      "Step: [7662] d_loss: 1.38678193, g_loss: 0.69615149\n",
      "Step: [7663] d_loss: 1.38664472, g_loss: 0.69665116\n",
      "Step: [7664] d_loss: 1.38683009, g_loss: 0.69471210\n",
      "Step: [7665] d_loss: 1.38577366, g_loss: 0.69418347\n",
      "Step: [7666] d_loss: 1.38645709, g_loss: 0.69650781\n",
      "Step: [7667] d_loss: 1.38626504, g_loss: 0.69533306\n",
      "Step: [7668] d_loss: 1.38529873, g_loss: 0.69216931\n",
      "Step: [7669] d_loss: 1.38898587, g_loss: 0.69448626\n",
      "Step: [7670] d_loss: 1.38701367, g_loss: 0.69546485\n",
      "Step: [7671] d_loss: 1.38527870, g_loss: 0.69833487\n",
      "Step: [7672] d_loss: 1.38358879, g_loss: 0.69787681\n",
      "Step: [7673] d_loss: 1.38401961, g_loss: 0.69536173\n",
      "Step: [7674] d_loss: 1.38915944, g_loss: 0.70664197\n",
      "Step: [7675] d_loss: 1.39276624, g_loss: 0.69726199\n",
      "Step: [7676] d_loss: 1.39030325, g_loss: 0.69147384\n",
      "Step: [7677] d_loss: 1.38803625, g_loss: 0.69371057\n",
      "Step: [7678] d_loss: 1.38555825, g_loss: 0.69608873\n",
      "Step: [7679] d_loss: 1.38500881, g_loss: 0.69696701\n",
      "Step: [7680] d_loss: 1.38716853, g_loss: 0.69527006\n",
      "Step: [7681] d_loss: 1.38362765, g_loss: 0.69568157\n",
      "Step: [7682] d_loss: 1.38946962, g_loss: 0.69026613\n",
      "Step: [7683] d_loss: 1.38590205, g_loss: 0.69849288\n",
      "Step: [7684] d_loss: 1.38493061, g_loss: 0.69979870\n",
      "Step: [7685] d_loss: 1.38639951, g_loss: 0.69846755\n",
      "Step: [7686] d_loss: 1.38372040, g_loss: 0.69994497\n",
      "Step: [7687] d_loss: 1.38355136, g_loss: 0.69737446\n",
      "Step: [7688] d_loss: 1.38354456, g_loss: 0.69523203\n",
      "Step: [7689] d_loss: 1.38695478, g_loss: 0.69553459\n",
      "Step: [7690] d_loss: 1.38730025, g_loss: 0.69569933\n",
      "Step: [7691] d_loss: 1.38921618, g_loss: 0.69611216\n",
      "Step: [7692] d_loss: 1.38578308, g_loss: 0.69661331\n",
      "Step: [7693] d_loss: 1.38690567, g_loss: 0.69115013\n",
      "Step: [7694] d_loss: 1.38892639, g_loss: 0.69363433\n",
      "Step: [7695] d_loss: 1.38678110, g_loss: 0.69213527\n",
      "Step: [7696] d_loss: 1.38628864, g_loss: 0.69573289\n",
      "Step: [7697] d_loss: 1.39104915, g_loss: 0.69850075\n",
      "Step: [7698] d_loss: 1.38780451, g_loss: 0.69382209\n",
      "Step: [7699] d_loss: 1.38922358, g_loss: 0.69417888\n",
      "Step: [7700] d_loss: 1.38681221, g_loss: 0.69513720\n",
      "Step: [7701] d_loss: 1.38712275, g_loss: 0.69768071\n",
      "Step: [7702] d_loss: 1.38462329, g_loss: 0.69786119\n",
      "Step: [7703] d_loss: 1.38799000, g_loss: 0.69266766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7704] d_loss: 1.38529110, g_loss: 0.69249332\n",
      "Step: [7705] d_loss: 1.38725305, g_loss: 0.70008409\n",
      "Step: [7706] d_loss: 1.38440061, g_loss: 0.69836271\n",
      "Step: [7707] d_loss: 1.38479304, g_loss: 0.69390118\n",
      "Step: [7708] d_loss: 1.38504720, g_loss: 0.69380695\n",
      "Step: [7709] d_loss: 1.38668764, g_loss: 0.69445008\n",
      "Step: [7710] d_loss: 1.38795233, g_loss: 0.69409752\n",
      "Step: [7711] d_loss: 1.38728237, g_loss: 0.69736969\n",
      "Step: [7712] d_loss: 1.38834023, g_loss: 0.69342065\n",
      "Step: [7713] d_loss: 1.39051485, g_loss: 0.68930161\n",
      "Step: [7714] d_loss: 1.38499284, g_loss: 0.69428074\n",
      "Step: [7715] d_loss: 1.38694119, g_loss: 0.69517660\n",
      "Step: [7716] d_loss: 1.38679433, g_loss: 0.69485307\n",
      "Step: [7717] d_loss: 1.38711953, g_loss: 0.69796759\n",
      "Step: [7718] d_loss: 1.38696384, g_loss: 0.69369006\n",
      "Step: [7719] d_loss: 1.38683462, g_loss: 0.69445789\n",
      "Step: [7720] d_loss: 1.38596344, g_loss: 0.69471407\n",
      "Step: [7721] d_loss: 1.38589573, g_loss: 0.69673789\n",
      "Step: [7722] d_loss: 1.38644946, g_loss: 0.69477296\n",
      "Step: [7723] d_loss: 1.38444805, g_loss: 0.69801974\n",
      "Step: [7724] d_loss: 1.38556468, g_loss: 0.69208264\n",
      "Step: [7725] d_loss: 1.38541019, g_loss: 0.69534433\n",
      "Step: [7726] d_loss: 1.38639855, g_loss: 0.69365203\n",
      "Step: [7727] d_loss: 1.38756120, g_loss: 0.69432753\n",
      "Step: [7728] d_loss: 1.38484931, g_loss: 0.69869316\n",
      "Step: [7729] d_loss: 1.38406909, g_loss: 0.69701254\n",
      "Step: [7730] d_loss: 1.38577271, g_loss: 0.69690847\n",
      "Step: [7731] d_loss: 1.38607812, g_loss: 0.69484878\n",
      "Step: [7732] d_loss: 1.38753009, g_loss: 0.69353664\n",
      "Step: [7733] d_loss: 1.38366508, g_loss: 0.69435084\n",
      "Step: [7734] d_loss: 1.38482189, g_loss: 0.69461000\n",
      "Step: [7735] d_loss: 1.38501620, g_loss: 0.69776690\n",
      "Step: [7736] d_loss: 1.38394713, g_loss: 0.69778973\n",
      "Step: [7737] d_loss: 1.38881755, g_loss: 0.69581431\n",
      "Step: [7738] d_loss: 1.38404047, g_loss: 0.69338524\n",
      "Step: [7739] d_loss: 1.38424087, g_loss: 0.69467884\n",
      "Step: [7740] d_loss: 1.38545036, g_loss: 0.69664943\n",
      "Step: [7741] d_loss: 1.38735640, g_loss: 0.69637150\n",
      "Step: [7742] d_loss: 1.38512087, g_loss: 0.69927984\n",
      "Step: [7743] d_loss: 1.38765669, g_loss: 0.69644940\n",
      "Step: [7744] d_loss: 1.38922429, g_loss: 0.69545758\n",
      "Step: [7745] d_loss: 1.38656902, g_loss: 0.69796711\n",
      "Step: [7746] d_loss: 1.38946056, g_loss: 0.69435585\n",
      "Step: [7747] d_loss: 1.38910091, g_loss: 0.69275355\n",
      "Step: [7748] d_loss: 1.38776028, g_loss: 0.69319463\n",
      "Step: [7749] d_loss: 1.38573146, g_loss: 0.69432622\n",
      "Step: [7750] d_loss: 1.38686633, g_loss: 0.69776249\n",
      "Step: [7751] d_loss: 1.38698184, g_loss: 0.69907343\n",
      "Step: [7752] d_loss: 1.38518715, g_loss: 0.69507802\n",
      "Step: [7753] d_loss: 1.38755393, g_loss: 0.69072181\n",
      "Step: [7754] d_loss: 1.38578153, g_loss: 0.69371510\n",
      "Step: [7755] d_loss: 1.38434649, g_loss: 0.69624245\n",
      "Step: [7756] d_loss: 1.38618958, g_loss: 0.69566119\n",
      "Step: [7757] d_loss: 1.38654709, g_loss: 0.69655144\n",
      "Step: [7758] d_loss: 1.38479114, g_loss: 0.69273341\n",
      "Step: [7759] d_loss: 1.38284862, g_loss: 0.69638860\n",
      "Step: [7760] d_loss: 1.38356829, g_loss: 0.69608009\n",
      "Step: [7761] d_loss: 1.38300538, g_loss: 0.69635612\n",
      "Step: [7762] d_loss: 1.38406587, g_loss: 0.69630045\n",
      "Step: [7763] d_loss: 1.38633275, g_loss: 0.69799972\n",
      "Step: [7764] d_loss: 1.38713455, g_loss: 0.69581050\n",
      "Step: [7765] d_loss: 1.38502312, g_loss: 0.69409621\n",
      "Step: [7766] d_loss: 1.38749015, g_loss: 0.69531381\n",
      "Step: [7767] d_loss: 1.38513422, g_loss: 0.69648576\n",
      "Step: [7768] d_loss: 1.38551927, g_loss: 0.69594646\n",
      "Step: [7769] d_loss: 1.38667428, g_loss: 0.69567817\n",
      "Step: [7770] d_loss: 1.38763618, g_loss: 0.69131804\n",
      "Step: [7771] d_loss: 1.38848317, g_loss: 0.69575524\n",
      "Step: [7772] d_loss: 1.38642478, g_loss: 0.69494021\n",
      "Step: [7773] d_loss: 1.38649070, g_loss: 0.69551867\n",
      "Step: [7774] d_loss: 1.38770843, g_loss: 0.69611126\n",
      "Step: [7775] d_loss: 1.38688469, g_loss: 0.69499588\n",
      "Step: [7776] d_loss: 1.38603926, g_loss: 0.69403708\n",
      "Step: [7777] d_loss: 1.38721967, g_loss: 0.69530213\n",
      "Step: [7778] d_loss: 1.38510180, g_loss: 0.69089299\n",
      "Step: [7779] d_loss: 1.38598764, g_loss: 0.69283873\n",
      "Step: [7780] d_loss: 1.38722575, g_loss: 0.69393170\n",
      "Step: [7781] d_loss: 1.38450253, g_loss: 0.69847369\n",
      "Step: [7782] d_loss: 1.38426471, g_loss: 0.69677949\n",
      "Step: [7783] d_loss: 1.38570690, g_loss: 0.69599593\n",
      "Step: [7784] d_loss: 1.38735914, g_loss: 0.69378310\n",
      "Step: [7785] d_loss: 1.38666725, g_loss: 0.69536537\n",
      "Step: [7786] d_loss: 1.38518286, g_loss: 0.69600767\n",
      "Step: [7787] d_loss: 1.39138114, g_loss: 0.69533062\n",
      "Step: [7788] d_loss: 1.38598371, g_loss: 0.70128179\n",
      "Step: [7789] d_loss: 1.38951969, g_loss: 0.68509012\n",
      "Step: [7790] d_loss: 1.38908076, g_loss: 0.69143105\n",
      "Step: [7791] d_loss: 1.38827205, g_loss: 0.69116563\n",
      "Step: [7792] d_loss: 1.38472319, g_loss: 0.69775754\n",
      "Step: [7793] d_loss: 1.38553870, g_loss: 0.69856906\n",
      "Step: [7794] d_loss: 1.38457775, g_loss: 0.69685996\n",
      "Step: [7795] d_loss: 1.38601565, g_loss: 0.69504142\n",
      "Step: [7796] d_loss: 1.38782239, g_loss: 0.69435245\n",
      "Step: [7797] d_loss: 1.38679409, g_loss: 0.69642246\n",
      "Step: [7798] d_loss: 1.38413072, g_loss: 0.69657290\n",
      "Step: [7799] d_loss: 1.38630271, g_loss: 0.69527817\n",
      "Step: [7800] d_loss: 1.38686991, g_loss: 0.69618034\n",
      "Step: [7801] d_loss: 1.38679969, g_loss: 0.69620711\n",
      "Step: [7802] d_loss: 1.38687396, g_loss: 0.69364333\n",
      "Step: [7803] d_loss: 1.38503289, g_loss: 0.69352359\n",
      "Step: [7804] d_loss: 1.38696325, g_loss: 0.69437641\n",
      "Step: [7805] d_loss: 1.38561320, g_loss: 0.69524115\n",
      "Step: [7806] d_loss: 1.38721561, g_loss: 0.69652003\n",
      "Step: [7807] d_loss: 1.38474953, g_loss: 0.69617611\n",
      "Step: [7808] d_loss: 1.38503408, g_loss: 0.69571644\n",
      "Step: [7809] d_loss: 1.38448763, g_loss: 0.69548607\n",
      "Step: [7810] d_loss: 1.38628626, g_loss: 0.69338018\n",
      "Step: [7811] d_loss: 1.38686347, g_loss: 0.69312716\n",
      "Step: [7812] d_loss: 1.38498950, g_loss: 0.69616610\n",
      "Step: [7813] d_loss: 1.38578439, g_loss: 0.69369435\n",
      "Step: [7814] d_loss: 1.38862848, g_loss: 0.69563669\n",
      "Step: [7815] d_loss: 1.38825440, g_loss: 0.69635546\n",
      "Step: [7816] d_loss: 1.38547087, g_loss: 0.69404304\n",
      "Step: [7817] d_loss: 1.38709402, g_loss: 0.69270122\n",
      "Step: [7818] d_loss: 1.38462090, g_loss: 0.69435853\n",
      "Step: [7819] d_loss: 1.38512135, g_loss: 0.69460505\n",
      "Step: [7820] d_loss: 1.38502336, g_loss: 0.69574243\n",
      "Step: [7821] d_loss: 1.38513267, g_loss: 0.69460702\n",
      "Step: [7822] d_loss: 1.38556433, g_loss: 0.69399023\n",
      "Step: [7823] d_loss: 1.38617885, g_loss: 0.69581270\n",
      "Step: [7824] d_loss: 1.38560927, g_loss: 0.69334626\n",
      "Step: [7825] d_loss: 1.38377738, g_loss: 0.69580299\n",
      "Step: [7826] d_loss: 1.38751006, g_loss: 0.69392169\n",
      "Step: [7827] d_loss: 1.38382828, g_loss: 0.69514680\n",
      "Step: [7828] d_loss: 1.38540506, g_loss: 0.69680637\n",
      "Step: [7829] d_loss: 1.38705683, g_loss: 0.69363016\n",
      "Step: [7830] d_loss: 1.38629556, g_loss: 0.69391930\n",
      "Step: [7831] d_loss: 1.38789022, g_loss: 0.69283044\n",
      "Step: [7832] d_loss: 1.38867569, g_loss: 0.69211739\n",
      "Step: [7833] d_loss: 1.38566279, g_loss: 0.69486582\n",
      "Step: [7834] d_loss: 1.38431382, g_loss: 0.69628298\n",
      "Step: [7835] d_loss: 1.38755262, g_loss: 0.69379723\n",
      "Step: [7836] d_loss: 1.38420343, g_loss: 0.69664055\n",
      "Step: [7837] d_loss: 1.38689375, g_loss: 0.69392973\n",
      "Step: [7838] d_loss: 1.38222003, g_loss: 0.69584262\n",
      "Step: [7839] d_loss: 1.38700914, g_loss: 0.69778532\n",
      "Step: [7840] d_loss: 1.38550210, g_loss: 0.69610184\n",
      "Step: [7841] d_loss: 1.38499129, g_loss: 0.69426531\n",
      "Step: [7842] d_loss: 1.38754666, g_loss: 0.69463658\n",
      "Step: [7843] d_loss: 1.38419271, g_loss: 0.69267488\n",
      "Step: [7844] d_loss: 1.38475037, g_loss: 0.69579947\n",
      "Step: [7845] d_loss: 1.38550067, g_loss: 0.69749177\n",
      "Step: [7846] d_loss: 1.38795054, g_loss: 0.69143009\n",
      "Step: [7847] d_loss: 1.38780105, g_loss: 0.69936198\n",
      "Step: [7848] d_loss: 1.38791025, g_loss: 0.69606358\n",
      "Step: [7849] d_loss: 1.38531446, g_loss: 0.69457197\n",
      "Step: [7850] d_loss: 1.38598609, g_loss: 0.69337869\n",
      "Step: [7851] d_loss: 1.38567042, g_loss: 0.69336349\n",
      "Step: [7852] d_loss: 1.38547587, g_loss: 0.69274104\n",
      "Step: [7853] d_loss: 1.38639617, g_loss: 0.69476938\n",
      "Step: [7854] d_loss: 1.38542783, g_loss: 0.69729018\n",
      "Step: [7855] d_loss: 1.38617539, g_loss: 0.69617456\n",
      "Step: [7856] d_loss: 1.38488746, g_loss: 0.69745004\n",
      "Step: [7857] d_loss: 1.38744187, g_loss: 0.69371390\n",
      "Step: [7858] d_loss: 1.38553536, g_loss: 0.69172812\n",
      "Step: [7859] d_loss: 1.38760233, g_loss: 0.69521761\n",
      "Step: [7860] d_loss: 1.38576984, g_loss: 0.69606715\n",
      "Step: [7861] d_loss: 1.38657784, g_loss: 0.69614017\n",
      "Step: [7862] d_loss: 1.38480306, g_loss: 0.69788265\n",
      "Step: [7863] d_loss: 1.38689280, g_loss: 0.69373584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [7864] d_loss: 1.38625813, g_loss: 0.69497180\n",
      "Step: [7865] d_loss: 1.38796294, g_loss: 0.69340146\n",
      "Step: [7866] d_loss: 1.38628316, g_loss: 0.69359607\n",
      "Step: [7867] d_loss: 1.38683856, g_loss: 0.69487345\n",
      "Step: [7868] d_loss: 1.38697195, g_loss: 0.69661480\n",
      "Step: [7869] d_loss: 1.38498116, g_loss: 0.69395924\n",
      "Step: [7870] d_loss: 1.38552880, g_loss: 0.69496560\n",
      "Step: [7871] d_loss: 1.38389564, g_loss: 0.69582272\n",
      "Step: [7872] d_loss: 1.38556170, g_loss: 0.69394851\n",
      "Step: [7873] d_loss: 1.38584709, g_loss: 0.69546849\n",
      "Step: [7874] d_loss: 1.38767457, g_loss: 0.69510055\n",
      "Step: [7875] d_loss: 1.38862705, g_loss: 0.69261920\n",
      "Step: [7876] d_loss: 1.38536584, g_loss: 0.69469434\n",
      "Step: [7877] d_loss: 1.38540435, g_loss: 0.69518375\n",
      "Step: [7878] d_loss: 1.38832569, g_loss: 0.69133908\n",
      "Step: [7879] d_loss: 1.38792014, g_loss: 0.69299376\n",
      "Step: [7880] d_loss: 1.38467360, g_loss: 0.69597018\n",
      "Step: [7881] d_loss: 1.38606453, g_loss: 0.69727242\n",
      "Step: [7882] d_loss: 1.38725591, g_loss: 0.69438922\n",
      "Step: [7883] d_loss: 1.38562655, g_loss: 0.69263178\n",
      "Step: [7884] d_loss: 1.38685441, g_loss: 0.69181812\n",
      "Step: [7885] d_loss: 1.38437557, g_loss: 0.69405085\n",
      "Step: [7886] d_loss: 1.38666880, g_loss: 0.69620109\n",
      "Step: [7887] d_loss: 1.38526571, g_loss: 0.69702888\n",
      "Step: [7888] d_loss: 1.38792825, g_loss: 0.69389534\n",
      "Step: [7889] d_loss: 1.38730764, g_loss: 0.69064850\n",
      "Step: [7890] d_loss: 1.38485813, g_loss: 0.69299626\n",
      "Step: [7891] d_loss: 1.38660586, g_loss: 0.69364560\n",
      "Step: [7892] d_loss: 1.38608730, g_loss: 0.69455338\n",
      "Step: [7893] d_loss: 1.38433647, g_loss: 0.69622320\n",
      "Step: [7894] d_loss: 1.38540554, g_loss: 0.69580853\n",
      "Step: [7895] d_loss: 1.38369536, g_loss: 0.69571638\n",
      "Step: [7896] d_loss: 1.38431609, g_loss: 0.69687486\n",
      "Step: [7897] d_loss: 1.38635278, g_loss: 0.69563943\n",
      "Step: [7898] d_loss: 1.38437438, g_loss: 0.69444180\n",
      "Step: [7899] d_loss: 1.38592649, g_loss: 0.69584203\n",
      "Step: [7900] d_loss: 1.38449323, g_loss: 0.69532585\n",
      "Step: [7901] d_loss: 1.38838625, g_loss: 0.69571179\n",
      "Step: [7902] d_loss: 1.38535583, g_loss: 0.69534838\n",
      "Step: [7903] d_loss: 1.38640189, g_loss: 0.69610256\n",
      "Step: [7904] d_loss: 1.38739216, g_loss: 0.68973541\n",
      "Step: [7905] d_loss: 1.38653469, g_loss: 0.69220936\n",
      "Step: [7906] d_loss: 1.38796318, g_loss: 0.69105101\n",
      "Step: [7907] d_loss: 1.38604045, g_loss: 0.69892573\n",
      "Step: [7908] d_loss: 1.38644755, g_loss: 0.69783938\n",
      "Step: [7909] d_loss: 1.38613939, g_loss: 0.69584334\n",
      "Step: [7910] d_loss: 1.38456762, g_loss: 0.69558978\n",
      "Step: [7911] d_loss: 1.38801038, g_loss: 0.69543624\n",
      "Step: [7912] d_loss: 1.38718498, g_loss: 0.69511861\n",
      "Step: [7913] d_loss: 1.38435650, g_loss: 0.69606662\n",
      "Step: [7914] d_loss: 1.38727093, g_loss: 0.69535935\n",
      "Step: [7915] d_loss: 1.38653851, g_loss: 0.69452453\n",
      "Step: [7916] d_loss: 1.38853550, g_loss: 0.69556332\n",
      "Step: [7917] d_loss: 1.38568389, g_loss: 0.69433570\n",
      "Step: [7918] d_loss: 1.38726997, g_loss: 0.69357479\n",
      "Step: [7919] d_loss: 1.38668430, g_loss: 0.69313228\n",
      "Step: [7920] d_loss: 1.38555503, g_loss: 0.69512737\n",
      "Step: [7921] d_loss: 1.38403213, g_loss: 0.69470417\n",
      "Step: [7922] d_loss: 1.38586664, g_loss: 0.69412518\n",
      "Step: [7923] d_loss: 1.38636088, g_loss: 0.69661468\n",
      "Step: [7924] d_loss: 1.38582826, g_loss: 0.69393331\n",
      "Step: [7925] d_loss: 1.38701296, g_loss: 0.69481134\n",
      "Step: [7926] d_loss: 1.38677454, g_loss: 0.69379073\n",
      "Step: [7927] d_loss: 1.38492990, g_loss: 0.69214892\n",
      "Step: [7928] d_loss: 1.38885856, g_loss: 0.69572937\n",
      "Step: [7929] d_loss: 1.38546801, g_loss: 0.69889498\n",
      "Step: [7930] d_loss: 1.38600683, g_loss: 0.69543397\n",
      "Step: [7931] d_loss: 1.38613510, g_loss: 0.69732475\n",
      "Step: [7932] d_loss: 1.38765311, g_loss: 0.69384503\n",
      "Step: [7933] d_loss: 1.38658047, g_loss: 0.69371456\n",
      "Step: [7934] d_loss: 1.38527846, g_loss: 0.69391060\n",
      "Step: [7935] d_loss: 1.38776875, g_loss: 0.69353992\n",
      "Step: [7936] d_loss: 1.38639307, g_loss: 0.69448709\n",
      "Step: [7937] d_loss: 1.38616467, g_loss: 0.69624895\n",
      "Step: [7938] d_loss: 1.38652182, g_loss: 0.69471556\n",
      "Step: [7939] d_loss: 1.38574600, g_loss: 0.69448376\n",
      "Step: [7940] d_loss: 1.38522136, g_loss: 0.69546068\n",
      "Step: [7941] d_loss: 1.38715172, g_loss: 0.69386947\n",
      "Step: [7942] d_loss: 1.38845444, g_loss: 0.69606316\n",
      "Step: [7943] d_loss: 1.38543487, g_loss: 0.69585681\n",
      "Step: [7944] d_loss: 1.38493896, g_loss: 0.69496810\n",
      "Step: [7945] d_loss: 1.38586044, g_loss: 0.69495851\n",
      "Step: [7946] d_loss: 1.38787460, g_loss: 0.69466865\n",
      "Step: [7947] d_loss: 1.38626254, g_loss: 0.69309556\n",
      "Step: [7948] d_loss: 1.38717997, g_loss: 0.69505370\n",
      "Step: [7949] d_loss: 1.38793206, g_loss: 0.69543278\n",
      "Step: [7950] d_loss: 1.38611758, g_loss: 0.69195664\n",
      "Step: [7951] d_loss: 1.38722301, g_loss: 0.69365245\n",
      "Step: [7952] d_loss: 1.38845801, g_loss: 0.69707477\n",
      "Step: [7953] d_loss: 1.38738060, g_loss: 0.69570202\n",
      "Step: [7954] d_loss: 1.38513160, g_loss: 0.69582880\n",
      "Step: [7955] d_loss: 1.38578010, g_loss: 0.69643241\n",
      "Step: [7956] d_loss: 1.38529086, g_loss: 0.69705927\n",
      "Step: [7957] d_loss: 1.38378477, g_loss: 0.69327116\n",
      "Step: [7958] d_loss: 1.38724864, g_loss: 0.69358772\n",
      "Step: [7959] d_loss: 1.38654613, g_loss: 0.69958580\n",
      "Step: [7960] d_loss: 1.38461041, g_loss: 0.69721889\n",
      "Step: [7961] d_loss: 1.38395119, g_loss: 0.69374913\n",
      "Step: [7962] d_loss: 1.38477051, g_loss: 0.69455528\n",
      "Step: [7963] d_loss: 1.38419366, g_loss: 0.69715941\n",
      "Step: [7964] d_loss: 1.38469291, g_loss: 0.69806814\n",
      "Step: [7965] d_loss: 1.38400507, g_loss: 0.69642699\n",
      "Step: [7966] d_loss: 1.38628697, g_loss: 0.69520509\n",
      "Step: [7967] d_loss: 1.38765502, g_loss: 0.69255912\n",
      "Step: [7968] d_loss: 1.38575661, g_loss: 0.69513655\n",
      "Step: [7969] d_loss: 1.38829160, g_loss: 0.69689953\n",
      "Step: [7970] d_loss: 1.38714123, g_loss: 0.69490469\n",
      "Step: [7971] d_loss: 1.38765728, g_loss: 0.69632953\n",
      "Step: [7972] d_loss: 1.38497710, g_loss: 0.69411361\n",
      "Step: [7973] d_loss: 1.38572669, g_loss: 0.69281280\n",
      "Step: [7974] d_loss: 1.38470697, g_loss: 0.69684112\n",
      "Step: [7975] d_loss: 1.38189292, g_loss: 0.69662201\n",
      "Step: [7976] d_loss: 1.38756025, g_loss: 0.69544601\n",
      "Step: [7977] d_loss: 1.38668406, g_loss: 0.69612223\n",
      "Step: [7978] d_loss: 1.38799810, g_loss: 0.69418395\n",
      "Step: [7979] d_loss: 1.38301277, g_loss: 0.69495046\n",
      "Step: [7980] d_loss: 1.38655531, g_loss: 0.69502819\n",
      "Step: [7981] d_loss: 1.38644624, g_loss: 0.69689089\n",
      "Step: [7982] d_loss: 1.39242887, g_loss: 0.69210631\n",
      "Step: [7983] d_loss: 1.38862085, g_loss: 0.69506085\n",
      "Step: [7984] d_loss: 1.38534927, g_loss: 0.69680554\n",
      "Step: [7985] d_loss: 1.38405657, g_loss: 0.69609821\n",
      "Step: [7986] d_loss: 1.38444281, g_loss: 0.69310188\n",
      "Step: [7987] d_loss: 1.38271761, g_loss: 0.69800770\n",
      "Step: [7988] d_loss: 1.38630629, g_loss: 0.69449675\n",
      "Step: [7989] d_loss: 1.38523507, g_loss: 0.69792873\n",
      "Step: [7990] d_loss: 1.38183928, g_loss: 0.69985759\n",
      "Step: [7991] d_loss: 1.38726938, g_loss: 0.69561136\n",
      "Step: [7992] d_loss: 1.38536036, g_loss: 0.69561779\n",
      "Step: [7993] d_loss: 1.38369703, g_loss: 0.69284296\n",
      "Step: [7994] d_loss: 1.38372779, g_loss: 0.69653261\n",
      "Step: [7995] d_loss: 1.38385963, g_loss: 0.69735008\n",
      "Step: [7996] d_loss: 1.38869929, g_loss: 0.69403827\n",
      "Step: [7997] d_loss: 1.38542461, g_loss: 0.69932461\n",
      "Step: [7998] d_loss: 1.38788676, g_loss: 0.69526029\n",
      "Step: [7999] d_loss: 1.39029288, g_loss: 0.69731647\n",
      "Step: [8000] d_loss: 1.38665342, g_loss: 0.69336599\n",
      "Step: [8001] d_loss: 1.38885522, g_loss: 0.69382024\n",
      "Step: [8002] d_loss: 1.38867593, g_loss: 0.68886590\n",
      "Step: [8003] d_loss: 1.38706970, g_loss: 0.69469059\n",
      "Step: [8004] d_loss: 1.38550186, g_loss: 0.69730735\n",
      "Step: [8005] d_loss: 1.38501704, g_loss: 0.70098460\n",
      "Step: [8006] d_loss: 1.39026165, g_loss: 0.69678771\n",
      "Step: [8007] d_loss: 1.38876486, g_loss: 0.69406915\n",
      "Step: [8008] d_loss: 1.38766968, g_loss: 0.69325209\n",
      "Step: [8009] d_loss: 1.38527548, g_loss: 0.69217104\n",
      "Step: [8010] d_loss: 1.38913071, g_loss: 0.69305003\n",
      "Step: [8011] d_loss: 1.38544703, g_loss: 0.69592381\n",
      "Step: [8012] d_loss: 1.38786411, g_loss: 0.69576389\n",
      "Step: [8013] d_loss: 1.38953495, g_loss: 0.69813991\n",
      "Step: [8014] d_loss: 1.38841879, g_loss: 0.69466507\n",
      "Step: [8015] d_loss: 1.38868356, g_loss: 0.69264972\n",
      "Step: [8016] d_loss: 1.38715792, g_loss: 0.69473326\n",
      "Step: [8017] d_loss: 1.38692963, g_loss: 0.69599128\n",
      "Step: [8018] d_loss: 1.38653564, g_loss: 0.69316739\n",
      "Step: [8019] d_loss: 1.38496101, g_loss: 0.69697261\n",
      "Step: [8020] d_loss: 1.38818312, g_loss: 0.69280994\n",
      "Step: [8021] d_loss: 1.38511467, g_loss: 0.69508356\n",
      "Step: [8022] d_loss: 1.38496304, g_loss: 0.69732165\n",
      "Step: [8023] d_loss: 1.38645101, g_loss: 0.69649374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8024] d_loss: 1.38306737, g_loss: 0.69941562\n",
      "Step: [8025] d_loss: 1.38563836, g_loss: 0.69676161\n",
      "Step: [8026] d_loss: 1.38663220, g_loss: 0.70437169\n",
      "Step: [8027] d_loss: 1.38720644, g_loss: 0.70354021\n",
      "Step: [8028] d_loss: 1.38231456, g_loss: 0.70199108\n",
      "Step: [8029] d_loss: 1.38605428, g_loss: 0.69703722\n",
      "Step: [8030] d_loss: 1.38414454, g_loss: 0.69395304\n",
      "Step: [8031] d_loss: 1.38725209, g_loss: 0.69725609\n",
      "Step: [8032] d_loss: 1.38476062, g_loss: 0.69690168\n",
      "Step: [8033] d_loss: 1.38317895, g_loss: 0.69538456\n",
      "Step: [8034] d_loss: 1.38613605, g_loss: 0.69528568\n",
      "Step: [8035] d_loss: 1.38612211, g_loss: 0.69835395\n",
      "Step: [8036] d_loss: 1.38680577, g_loss: 0.69906718\n",
      "Step: [8037] d_loss: 1.38613605, g_loss: 0.69300610\n",
      "Step: [8038] d_loss: 1.38672233, g_loss: 0.69566989\n",
      "Step: [8039] d_loss: 1.38615751, g_loss: 0.69375145\n",
      "Step: [8040] d_loss: 1.38682663, g_loss: 0.69503212\n",
      "Step: [8041] d_loss: 1.38886333, g_loss: 0.69621354\n",
      "Step: [8042] d_loss: 1.38727689, g_loss: 0.69577783\n",
      "Step: [8043] d_loss: 1.38815832, g_loss: 0.69613791\n",
      "Step: [8044] d_loss: 1.39287472, g_loss: 0.69063354\n",
      "Step: [8045] d_loss: 1.39252234, g_loss: 0.69303918\n",
      "Step: [8046] d_loss: 1.38734603, g_loss: 0.69231534\n",
      "Step: [8047] d_loss: 1.38862062, g_loss: 0.69547933\n",
      "Step: [8048] d_loss: 1.39157224, g_loss: 0.69366574\n",
      "Step: [8049] d_loss: 1.38733864, g_loss: 0.69483137\n",
      "Step: [8050] d_loss: 1.38837695, g_loss: 0.69585204\n",
      "Step: [8051] d_loss: 1.38642526, g_loss: 0.69504899\n",
      "Step: [8052] d_loss: 1.38487101, g_loss: 0.69515878\n",
      "Step: [8053] d_loss: 1.38712668, g_loss: 0.69336784\n",
      "Step: [8054] d_loss: 1.38466167, g_loss: 0.69712961\n",
      "Step: [8055] d_loss: 1.38788903, g_loss: 0.69686669\n",
      "Step: [8056] d_loss: 1.38542211, g_loss: 0.69464588\n",
      "Step: [8057] d_loss: 1.38655639, g_loss: 0.69595498\n",
      "Step: [8058] d_loss: 1.38560414, g_loss: 0.69740218\n",
      "Step: [8059] d_loss: 1.38575387, g_loss: 0.69489765\n",
      "Step: [8060] d_loss: 1.38141143, g_loss: 0.69446814\n",
      "Step: [8061] d_loss: 1.38472676, g_loss: 0.69621307\n",
      "Step: [8062] d_loss: 1.38208032, g_loss: 0.69576085\n",
      "Step: [8063] d_loss: 1.38409209, g_loss: 0.69909471\n",
      "Step: [8064] d_loss: 1.38943195, g_loss: 0.69349891\n",
      "Step: [8065] d_loss: 1.38392949, g_loss: 0.69830573\n",
      "Step: [8066] d_loss: 1.38582802, g_loss: 0.69748485\n",
      "Step: [8067] d_loss: 1.39093494, g_loss: 0.69203174\n",
      "Step: [8068] d_loss: 1.38871515, g_loss: 0.69585252\n",
      "Step: [8069] d_loss: 1.38489997, g_loss: 0.69456011\n",
      "Step: [8070] d_loss: 1.38543558, g_loss: 0.69738173\n",
      "Step: [8071] d_loss: 1.38244724, g_loss: 0.70022404\n",
      "Step: [8072] d_loss: 1.38069832, g_loss: 0.69976854\n",
      "Step: [8073] d_loss: 1.38314438, g_loss: 0.69873893\n",
      "Step: [8074] d_loss: 1.38478994, g_loss: 0.69388282\n",
      "Step: [8075] d_loss: 1.38483715, g_loss: 0.69689584\n",
      "Step: [8076] d_loss: 1.38479328, g_loss: 0.69872850\n",
      "Step: [8077] d_loss: 1.38248968, g_loss: 0.69931149\n",
      "Step: [8078] d_loss: 1.38222837, g_loss: 0.70109534\n",
      "Step: [8079] d_loss: 1.37764573, g_loss: 0.70061207\n",
      "Step: [8080] d_loss: 1.39063573, g_loss: 0.69690371\n",
      "Step: [8081] d_loss: 1.38508677, g_loss: 0.69710076\n",
      "Step: [8082] d_loss: 1.39055383, g_loss: 0.69134003\n",
      "Step: [8083] d_loss: 1.38647771, g_loss: 0.69951236\n",
      "Step: [8084] d_loss: 1.38403940, g_loss: 0.69544506\n",
      "Step: [8085] d_loss: 1.38611448, g_loss: 0.70201337\n",
      "Step: [8086] d_loss: 1.39522874, g_loss: 0.69192964\n",
      "Step: [8087] d_loss: 1.38854551, g_loss: 0.69204837\n",
      "Step: [8088] d_loss: 1.39132547, g_loss: 0.69454151\n",
      "Step: [8089] d_loss: 1.39387822, g_loss: 0.69507819\n",
      "Step: [8090] d_loss: 1.39095199, g_loss: 0.69952798\n",
      "Step: [8091] d_loss: 1.38697624, g_loss: 0.69870663\n",
      "Step: [8092] d_loss: 1.38349009, g_loss: 0.69653738\n",
      "Step: [8093] d_loss: 1.38716781, g_loss: 0.69597399\n",
      "Step: [8094] d_loss: 1.38748336, g_loss: 0.69104266\n",
      "Step: [8095] d_loss: 1.38622570, g_loss: 0.69454145\n",
      "Step: [8096] d_loss: 1.38478446, g_loss: 0.69623840\n",
      "Step: [8097] d_loss: 1.38650429, g_loss: 0.69692653\n",
      "Step: [8098] d_loss: 1.38772464, g_loss: 0.69842845\n",
      "Step: [8099] d_loss: 1.38102305, g_loss: 0.69703341\n",
      "Step: [8100] d_loss: 1.38262880, g_loss: 0.69987500\n",
      "Step: [8101] d_loss: 1.39074731, g_loss: 0.69975954\n",
      "Step: [8102] d_loss: 1.38609123, g_loss: 0.69794226\n",
      "Step: [8103] d_loss: 1.38481641, g_loss: 0.69366777\n",
      "Step: [8104] d_loss: 1.38496804, g_loss: 0.69692397\n",
      "Step: [8105] d_loss: 1.38889575, g_loss: 0.69469547\n",
      "Step: [8106] d_loss: 1.38361704, g_loss: 0.69673979\n",
      "Step: [8107] d_loss: 1.38711882, g_loss: 0.69884539\n",
      "Step: [8108] d_loss: 1.38847244, g_loss: 0.69475961\n",
      "Step: [8109] d_loss: 1.38338614, g_loss: 0.69554090\n",
      "Step: [8110] d_loss: 1.38434267, g_loss: 0.69656694\n",
      "Step: [8111] d_loss: 1.38968468, g_loss: 0.69263947\n",
      "Step: [8112] d_loss: 1.38201952, g_loss: 0.70147711\n",
      "Step: [8113] d_loss: 1.38817763, g_loss: 0.69423246\n",
      "Step: [8114] d_loss: 1.38305855, g_loss: 0.69712746\n",
      "Step: [8115] d_loss: 1.38820720, g_loss: 0.69305915\n",
      "Step: [8116] d_loss: 1.38563347, g_loss: 0.69767749\n",
      "Step: [8117] d_loss: 1.38597178, g_loss: 0.69422084\n",
      "Step: [8118] d_loss: 1.38772893, g_loss: 0.69811893\n",
      "Step: [8119] d_loss: 1.38580358, g_loss: 0.69210309\n",
      "Step: [8120] d_loss: 1.38436878, g_loss: 0.70025659\n",
      "Step: [8121] d_loss: 1.38707674, g_loss: 0.69552636\n",
      "Step: [8122] d_loss: 1.38719606, g_loss: 0.69675207\n",
      "Step: [8123] d_loss: 1.38690591, g_loss: 0.69577026\n",
      "Step: [8124] d_loss: 1.38728845, g_loss: 0.69493026\n",
      "Step: [8125] d_loss: 1.38863397, g_loss: 0.69502580\n",
      "Step: [8126] d_loss: 1.38394058, g_loss: 0.69776988\n",
      "Step: [8127] d_loss: 1.38880122, g_loss: 0.69119668\n",
      "Step: [8128] d_loss: 1.38597143, g_loss: 0.69788289\n",
      "Step: [8129] d_loss: 1.38776827, g_loss: 0.69107985\n",
      "Step: [8130] d_loss: 1.38813043, g_loss: 0.69931901\n",
      "Step: [8131] d_loss: 1.38607752, g_loss: 0.69701791\n",
      "Step: [8132] d_loss: 1.38477945, g_loss: 0.69670916\n",
      "Step: [8133] d_loss: 1.38408661, g_loss: 0.69786298\n",
      "Step: [8134] d_loss: 1.38432169, g_loss: 0.69597381\n",
      "Step: [8135] d_loss: 1.38586712, g_loss: 0.69531709\n",
      "Step: [8136] d_loss: 1.38279605, g_loss: 0.69693422\n",
      "Step: [8137] d_loss: 1.38450348, g_loss: 0.69425619\n",
      "Step: [8138] d_loss: 1.38181496, g_loss: 0.69806933\n",
      "Step: [8139] d_loss: 1.38424325, g_loss: 0.69781137\n",
      "Step: [8140] d_loss: 1.38666129, g_loss: 0.69447434\n",
      "Step: [8141] d_loss: 1.38761270, g_loss: 0.69309598\n",
      "Step: [8142] d_loss: 1.38387847, g_loss: 0.69527990\n",
      "Step: [8143] d_loss: 1.38459921, g_loss: 0.69644701\n",
      "Step: [8144] d_loss: 1.38411069, g_loss: 0.69659805\n",
      "Step: [8145] d_loss: 1.38794100, g_loss: 0.69656056\n",
      "Step: [8146] d_loss: 1.38561726, g_loss: 0.69104099\n",
      "Step: [8147] d_loss: 1.38535893, g_loss: 0.69602454\n",
      "Step: [8148] d_loss: 1.38583159, g_loss: 0.69584602\n",
      "Step: [8149] d_loss: 1.38730788, g_loss: 0.69824684\n",
      "Step: [8150] d_loss: 1.38922143, g_loss: 0.69197917\n",
      "Step: [8151] d_loss: 1.38805485, g_loss: 0.69289386\n",
      "Step: [8152] d_loss: 1.38664865, g_loss: 0.69599140\n",
      "Step: [8153] d_loss: 1.38337970, g_loss: 0.69494069\n",
      "Step: [8154] d_loss: 1.38526404, g_loss: 0.69485259\n",
      "Step: [8155] d_loss: 1.38610709, g_loss: 0.69270873\n",
      "Step: [8156] d_loss: 1.38588452, g_loss: 0.69549036\n",
      "Step: [8157] d_loss: 1.38997197, g_loss: 0.69442403\n",
      "Step: [8158] d_loss: 1.38737655, g_loss: 0.69344556\n",
      "Step: [8159] d_loss: 1.38965094, g_loss: 0.69601959\n",
      "Step: [8160] d_loss: 1.38372493, g_loss: 0.69266200\n",
      "Step: [8161] d_loss: 1.38490391, g_loss: 0.69395542\n",
      "Step: [8162] d_loss: 1.38645577, g_loss: 0.69707066\n",
      "Step: [8163] d_loss: 1.38573992, g_loss: 0.69485503\n",
      "Step: [8164] d_loss: 1.38679934, g_loss: 0.69636750\n",
      "Step: [8165] d_loss: 1.38591218, g_loss: 0.69563705\n",
      "Step: [8166] d_loss: 1.38397813, g_loss: 0.69315070\n",
      "Step: [8167] d_loss: 1.38555264, g_loss: 0.69325197\n",
      "Step: [8168] d_loss: 1.38495994, g_loss: 0.69440424\n",
      "Step: [8169] d_loss: 1.38589072, g_loss: 0.69385457\n",
      "Step: [8170] d_loss: 1.38542700, g_loss: 0.69543356\n",
      "Step: [8171] d_loss: 1.38547003, g_loss: 0.69607961\n",
      "Step: [8172] d_loss: 1.38407397, g_loss: 0.69949949\n",
      "Step: [8173] d_loss: 1.38491905, g_loss: 0.69464540\n",
      "Step: [8174] d_loss: 1.38334250, g_loss: 0.69432533\n",
      "Step: [8175] d_loss: 1.38549316, g_loss: 0.69532782\n",
      "Step: [8176] d_loss: 1.38564324, g_loss: 0.69114494\n",
      "Step: [8177] d_loss: 1.38709962, g_loss: 0.69611347\n",
      "Step: [8178] d_loss: 1.38414192, g_loss: 0.69484180\n",
      "Step: [8179] d_loss: 1.38983083, g_loss: 0.69782543\n",
      "Step: [8180] d_loss: 1.38918424, g_loss: 0.68958819\n",
      "Step: [8181] d_loss: 1.38847876, g_loss: 0.69909507\n",
      "Step: [8182] d_loss: 1.38648868, g_loss: 0.69412434\n",
      "Step: [8183] d_loss: 1.38574469, g_loss: 0.69640547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8184] d_loss: 1.38585424, g_loss: 0.69618374\n",
      "Step: [8185] d_loss: 1.38636684, g_loss: 0.69547176\n",
      "Step: [8186] d_loss: 1.38474917, g_loss: 0.69597149\n",
      "Step: [8187] d_loss: 1.38590503, g_loss: 0.69613093\n",
      "Step: [8188] d_loss: 1.38656747, g_loss: 0.69574285\n",
      "Step: [8189] d_loss: 1.38547146, g_loss: 0.69190532\n",
      "Step: [8190] d_loss: 1.38735747, g_loss: 0.69371557\n",
      "Step: [8191] d_loss: 1.38475013, g_loss: 0.69761121\n",
      "Step: [8192] d_loss: 1.38591695, g_loss: 0.69500268\n",
      "Step: [8193] d_loss: 1.38716245, g_loss: 0.69561476\n",
      "Step: [8194] d_loss: 1.38723648, g_loss: 0.69293559\n",
      "Step: [8195] d_loss: 1.38470411, g_loss: 0.69706237\n",
      "Step: [8196] d_loss: 1.38855243, g_loss: 0.69145250\n",
      "Step: [8197] d_loss: 1.38325477, g_loss: 0.69771564\n",
      "Step: [8198] d_loss: 1.38634312, g_loss: 0.69612753\n",
      "Step: [8199] d_loss: 1.38639522, g_loss: 0.69353294\n",
      "Step: [8200] d_loss: 1.38714302, g_loss: 0.69338620\n",
      "Step: [8201] d_loss: 1.38542163, g_loss: 0.69709074\n",
      "Step: [8202] d_loss: 1.38530302, g_loss: 0.69669211\n",
      "Step: [8203] d_loss: 1.38510633, g_loss: 0.69270825\n",
      "Step: [8204] d_loss: 1.38648999, g_loss: 0.69711387\n",
      "Step: [8205] d_loss: 1.38570344, g_loss: 0.69233966\n",
      "Step: [8206] d_loss: 1.38642991, g_loss: 0.69513249\n",
      "Step: [8207] d_loss: 1.38814414, g_loss: 0.69326937\n",
      "Step: [8208] d_loss: 1.38662255, g_loss: 0.69540507\n",
      "Step: [8209] d_loss: 1.38734269, g_loss: 0.69494462\n",
      "Step: [8210] d_loss: 1.38811100, g_loss: 0.69144082\n",
      "Step: [8211] d_loss: 1.38683605, g_loss: 0.69572645\n",
      "Step: [8212] d_loss: 1.38879395, g_loss: 0.68932235\n",
      "Step: [8213] d_loss: 1.38846016, g_loss: 0.69422638\n",
      "Step: [8214] d_loss: 1.38711834, g_loss: 0.69372606\n",
      "Step: [8215] d_loss: 1.38564014, g_loss: 0.69580203\n",
      "Step: [8216] d_loss: 1.38462687, g_loss: 0.69673920\n",
      "Step: [8217] d_loss: 1.38597488, g_loss: 0.69498956\n",
      "Step: [8218] d_loss: 1.38895607, g_loss: 0.69394249\n",
      "Step: [8219] d_loss: 1.38608623, g_loss: 0.69506723\n",
      "Step: [8220] d_loss: 1.38762724, g_loss: 0.69316685\n",
      "Step: [8221] d_loss: 1.38668275, g_loss: 0.69541764\n",
      "Step: [8222] d_loss: 1.38402081, g_loss: 0.69193113\n",
      "Step: [8223] d_loss: 1.38613522, g_loss: 0.69653213\n",
      "Step: [8224] d_loss: 1.38677692, g_loss: 0.69320214\n",
      "Step: [8225] d_loss: 1.38391078, g_loss: 0.69823420\n",
      "Step: [8226] d_loss: 1.38665295, g_loss: 0.69439179\n",
      "Step: [8227] d_loss: 1.38517809, g_loss: 0.69776094\n",
      "Step: [8228] d_loss: 1.38555276, g_loss: 0.69457060\n",
      "Step: [8229] d_loss: 1.38680875, g_loss: 0.69426274\n",
      "Step: [8230] d_loss: 1.38578153, g_loss: 0.69243109\n",
      "Step: [8231] d_loss: 1.38528717, g_loss: 0.69509339\n",
      "Step: [8232] d_loss: 1.38642633, g_loss: 0.69518590\n",
      "Step: [8233] d_loss: 1.38440871, g_loss: 0.69371676\n",
      "Step: [8234] d_loss: 1.38496196, g_loss: 0.69554353\n",
      "Step: [8235] d_loss: 1.38593614, g_loss: 0.69416374\n",
      "Step: [8236] d_loss: 1.38641644, g_loss: 0.69201756\n",
      "Step: [8237] d_loss: 1.38432181, g_loss: 0.69630265\n",
      "Step: [8238] d_loss: 1.38477993, g_loss: 0.69327629\n",
      "Step: [8239] d_loss: 1.38292253, g_loss: 0.69689524\n",
      "Step: [8240] d_loss: 1.38454962, g_loss: 0.69455612\n",
      "Step: [8241] d_loss: 1.38691473, g_loss: 0.69349027\n",
      "Step: [8242] d_loss: 1.38792968, g_loss: 0.69364256\n",
      "Step: [8243] d_loss: 1.38536096, g_loss: 0.69021809\n",
      "Step: [8244] d_loss: 1.38533139, g_loss: 0.69681954\n",
      "Step: [8245] d_loss: 1.38596439, g_loss: 0.69641125\n",
      "Step: [8246] d_loss: 1.38385344, g_loss: 0.69601101\n",
      "Step: [8247] d_loss: 1.38578808, g_loss: 0.69512451\n",
      "Step: [8248] d_loss: 1.38552451, g_loss: 0.69531858\n",
      "Step: [8249] d_loss: 1.38618231, g_loss: 0.69274342\n",
      "Step: [8250] d_loss: 1.38631058, g_loss: 0.69448566\n",
      "Step: [8251] d_loss: 1.38910675, g_loss: 0.68602014\n",
      "Step: [8252] d_loss: 1.38952971, g_loss: 0.69764209\n",
      "Step: [8253] d_loss: 1.38895690, g_loss: 0.69471538\n",
      "Step: [8254] d_loss: 1.38713837, g_loss: 0.69940621\n",
      "Step: [8255] d_loss: 1.38863027, g_loss: 0.69212896\n",
      "Step: [8256] d_loss: 1.38584936, g_loss: 0.69290644\n",
      "Step: [8257] d_loss: 1.38684297, g_loss: 0.69267690\n",
      "Step: [8258] d_loss: 1.38696313, g_loss: 0.69510037\n",
      "Step: [8259] d_loss: 1.38592327, g_loss: 0.69571644\n",
      "Step: [8260] d_loss: 1.38607073, g_loss: 0.69497287\n",
      "Step: [8261] d_loss: 1.38459027, g_loss: 0.69296300\n",
      "Step: [8262] d_loss: 1.38631403, g_loss: 0.69633693\n",
      "Step: [8263] d_loss: 1.38694763, g_loss: 0.69203120\n",
      "Step: [8264] d_loss: 1.38632751, g_loss: 0.69370723\n",
      "Step: [8265] d_loss: 1.38555002, g_loss: 0.69131124\n",
      "Step: [8266] d_loss: 1.38687468, g_loss: 0.69703352\n",
      "Step: [8267] d_loss: 1.38601363, g_loss: 0.69664729\n",
      "Step: [8268] d_loss: 1.38576531, g_loss: 0.69619536\n",
      "Step: [8269] d_loss: 1.38641596, g_loss: 0.69488782\n",
      "Step: [8270] d_loss: 1.38814425, g_loss: 0.69013417\n",
      "Step: [8271] d_loss: 1.38499069, g_loss: 0.69479579\n",
      "Step: [8272] d_loss: 1.38346171, g_loss: 0.69170791\n",
      "Step: [8273] d_loss: 1.38504040, g_loss: 0.69535947\n",
      "Step: [8274] d_loss: 1.38598442, g_loss: 0.69631577\n",
      "Step: [8275] d_loss: 1.38778174, g_loss: 0.69568020\n",
      "Step: [8276] d_loss: 1.38720441, g_loss: 0.69537365\n",
      "Step: [8277] d_loss: 1.38792324, g_loss: 0.69191635\n",
      "Step: [8278] d_loss: 1.38531137, g_loss: 0.69264674\n",
      "Step: [8279] d_loss: 1.38431430, g_loss: 0.69211888\n",
      "Step: [8280] d_loss: 1.38661599, g_loss: 0.69365644\n",
      "Step: [8281] d_loss: 1.38637781, g_loss: 0.69356334\n",
      "Step: [8282] d_loss: 1.38713121, g_loss: 0.69549650\n",
      "Step: [8283] d_loss: 1.38758278, g_loss: 0.69289464\n",
      "Step: [8284] d_loss: 1.38748395, g_loss: 0.69353592\n",
      "Step: [8285] d_loss: 1.38725877, g_loss: 0.69548380\n",
      "Step: [8286] d_loss: 1.38582921, g_loss: 0.69227380\n",
      "Step: [8287] d_loss: 1.38727617, g_loss: 0.69335288\n",
      "Step: [8288] d_loss: 1.38613248, g_loss: 0.69334137\n",
      "Step: [8289] d_loss: 1.38603711, g_loss: 0.69523585\n",
      "Step: [8290] d_loss: 1.38774776, g_loss: 0.69508135\n",
      "Step: [8291] d_loss: 1.38669896, g_loss: 0.69534546\n",
      "Step: [8292] d_loss: 1.38623834, g_loss: 0.69359338\n",
      "Step: [8293] d_loss: 1.38536739, g_loss: 0.69336003\n",
      "Step: [8294] d_loss: 1.38518155, g_loss: 0.69470203\n",
      "Step: [8295] d_loss: 1.38504052, g_loss: 0.69406110\n",
      "Step: [8296] d_loss: 1.38528132, g_loss: 0.69648057\n",
      "Step: [8297] d_loss: 1.38625884, g_loss: 0.69537264\n",
      "Step: [8298] d_loss: 1.38460624, g_loss: 0.69348419\n",
      "Step: [8299] d_loss: 1.38458633, g_loss: 0.69669336\n",
      "Step: [8300] d_loss: 1.38474238, g_loss: 0.69508547\n",
      "Step: [8301] d_loss: 1.38671565, g_loss: 0.69402635\n",
      "Step: [8302] d_loss: 1.38549399, g_loss: 0.69355971\n",
      "Step: [8303] d_loss: 1.38498008, g_loss: 0.69396794\n",
      "Step: [8304] d_loss: 1.38590717, g_loss: 0.69904375\n",
      "Step: [8305] d_loss: 1.38462925, g_loss: 0.69668770\n",
      "Step: [8306] d_loss: 1.38498092, g_loss: 0.69329739\n",
      "Step: [8307] d_loss: 1.38532066, g_loss: 0.69387478\n",
      "Step: [8308] d_loss: 1.38390303, g_loss: 0.69605339\n",
      "Step: [8309] d_loss: 1.38607657, g_loss: 0.69667304\n",
      "Step: [8310] d_loss: 1.38747144, g_loss: 0.69159222\n",
      "Step: [8311] d_loss: 1.38651609, g_loss: 0.69443965\n",
      "Step: [8312] d_loss: 1.38506711, g_loss: 0.69411421\n",
      "Step: [8313] d_loss: 1.38655806, g_loss: 0.69587106\n",
      "Step: [8314] d_loss: 1.38555360, g_loss: 0.69541597\n",
      "Step: [8315] d_loss: 1.38576365, g_loss: 0.69569445\n",
      "Step: [8316] d_loss: 1.38663006, g_loss: 0.70042968\n",
      "Step: [8317] d_loss: 1.38514495, g_loss: 0.69215566\n",
      "Step: [8318] d_loss: 1.38591123, g_loss: 0.69465482\n",
      "Step: [8319] d_loss: 1.38569427, g_loss: 0.69250166\n",
      "Step: [8320] d_loss: 1.38694382, g_loss: 0.69568616\n",
      "Step: [8321] d_loss: 1.38563275, g_loss: 0.69585836\n",
      "Step: [8322] d_loss: 1.38407409, g_loss: 0.69698805\n",
      "Step: [8323] d_loss: 1.38566899, g_loss: 0.69598311\n",
      "Step: [8324] d_loss: 1.38380361, g_loss: 0.69406497\n",
      "Step: [8325] d_loss: 1.38689566, g_loss: 0.69159126\n",
      "Step: [8326] d_loss: 1.38462329, g_loss: 0.69695950\n",
      "Step: [8327] d_loss: 1.38393164, g_loss: 0.69660622\n",
      "Step: [8328] d_loss: 1.38561106, g_loss: 0.70205957\n",
      "Step: [8329] d_loss: 1.38615549, g_loss: 0.69428927\n",
      "Step: [8330] d_loss: 1.38496280, g_loss: 0.69676840\n",
      "Step: [8331] d_loss: 1.38493109, g_loss: 0.69253820\n",
      "Step: [8332] d_loss: 1.38850367, g_loss: 0.69439435\n",
      "Step: [8333] d_loss: 1.38551950, g_loss: 0.69571543\n",
      "Step: [8334] d_loss: 1.38551009, g_loss: 0.69342148\n",
      "Step: [8335] d_loss: 1.38711452, g_loss: 0.69679165\n",
      "Step: [8336] d_loss: 1.38845265, g_loss: 0.69346172\n",
      "Step: [8337] d_loss: 1.38529980, g_loss: 0.69821167\n",
      "Step: [8338] d_loss: 1.38831055, g_loss: 0.69040895\n",
      "Step: [8339] d_loss: 1.38862205, g_loss: 0.69528335\n",
      "Step: [8340] d_loss: 1.38723457, g_loss: 0.69121873\n",
      "Step: [8341] d_loss: 1.38609850, g_loss: 0.69378150\n",
      "Step: [8342] d_loss: 1.38642478, g_loss: 0.69224995\n",
      "Step: [8343] d_loss: 1.38654613, g_loss: 0.69531161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8344] d_loss: 1.38500929, g_loss: 0.69608361\n",
      "Step: [8345] d_loss: 1.38627911, g_loss: 0.69459671\n",
      "Step: [8346] d_loss: 1.38640904, g_loss: 0.69479835\n",
      "Step: [8347] d_loss: 1.38592684, g_loss: 0.69240546\n",
      "Step: [8348] d_loss: 1.38997841, g_loss: 0.70291549\n",
      "Step: [8349] d_loss: 1.38940811, g_loss: 0.70119649\n",
      "Step: [8350] d_loss: 1.38906133, g_loss: 0.70576930\n",
      "Step: [8351] d_loss: 1.38555884, g_loss: 0.69623536\n",
      "Step: [8352] d_loss: 1.38502002, g_loss: 0.69094801\n",
      "Step: [8353] d_loss: 1.38485861, g_loss: 0.69216835\n",
      "Step: [8354] d_loss: 1.38773108, g_loss: 0.69444883\n",
      "Step: [8355] d_loss: 1.38544583, g_loss: 0.69478703\n",
      "Step: [8356] d_loss: 1.38836825, g_loss: 0.69414830\n",
      "Step: [8357] d_loss: 1.38742590, g_loss: 0.69159889\n",
      "Step: [8358] d_loss: 1.38689780, g_loss: 0.69400126\n",
      "Step: [8359] d_loss: 1.38747072, g_loss: 0.69245958\n",
      "Step: [8360] d_loss: 1.38590193, g_loss: 0.69456470\n",
      "Step: [8361] d_loss: 1.38485122, g_loss: 0.69646168\n",
      "Step: [8362] d_loss: 1.38681102, g_loss: 0.69428349\n",
      "Step: [8363] d_loss: 1.38500524, g_loss: 0.69370782\n",
      "Step: [8364] d_loss: 1.38629723, g_loss: 0.69478869\n",
      "Step: [8365] d_loss: 1.38735461, g_loss: 0.69637775\n",
      "Step: [8366] d_loss: 1.38782656, g_loss: 0.69433892\n",
      "Step: [8367] d_loss: 1.38513374, g_loss: 0.69464916\n",
      "Step: [8368] d_loss: 1.38728142, g_loss: 0.69453090\n",
      "Step: [8369] d_loss: 1.38785028, g_loss: 0.69257402\n",
      "Step: [8370] d_loss: 1.38431597, g_loss: 0.69641584\n",
      "Step: [8371] d_loss: 1.38617587, g_loss: 0.69392908\n",
      "Step: [8372] d_loss: 1.38723993, g_loss: 0.69296074\n",
      "Step: [8373] d_loss: 1.38630366, g_loss: 0.69434845\n",
      "Step: [8374] d_loss: 1.38585329, g_loss: 0.69517100\n",
      "Step: [8375] d_loss: 1.38644886, g_loss: 0.69511449\n",
      "Step: [8376] d_loss: 1.38554347, g_loss: 0.69512695\n",
      "Step: [8377] d_loss: 1.38529146, g_loss: 0.69319808\n",
      "Step: [8378] d_loss: 1.38392365, g_loss: 0.69531143\n",
      "Step: [8379] d_loss: 1.38518763, g_loss: 0.69024926\n",
      "Step: [8380] d_loss: 1.38764012, g_loss: 0.69432831\n",
      "Step: [8381] d_loss: 1.38635695, g_loss: 0.69317758\n",
      "Step: [8382] d_loss: 1.38748181, g_loss: 0.69739878\n",
      "Step: [8383] d_loss: 1.38703430, g_loss: 0.69652402\n",
      "Step: [8384] d_loss: 1.38639832, g_loss: 0.69779134\n",
      "Step: [8385] d_loss: 1.38746786, g_loss: 0.69519711\n",
      "Step: [8386] d_loss: 1.38563550, g_loss: 0.69108653\n",
      "Step: [8387] d_loss: 1.38629818, g_loss: 0.69087815\n",
      "Step: [8388] d_loss: 1.38505089, g_loss: 0.69502795\n",
      "Step: [8389] d_loss: 1.38700318, g_loss: 0.69649291\n",
      "Step: [8390] d_loss: 1.38672805, g_loss: 0.69276142\n",
      "Step: [8391] d_loss: 1.38572049, g_loss: 0.69671327\n",
      "Step: [8392] d_loss: 1.38615441, g_loss: 0.68916774\n",
      "Step: [8393] d_loss: 1.38517749, g_loss: 0.69524533\n",
      "Step: [8394] d_loss: 1.38655186, g_loss: 0.69481301\n",
      "Step: [8395] d_loss: 1.38560665, g_loss: 0.69548303\n",
      "Step: [8396] d_loss: 1.38548195, g_loss: 0.69146228\n",
      "Step: [8397] d_loss: 1.38659167, g_loss: 0.69548547\n",
      "Step: [8398] d_loss: 1.38787246, g_loss: 0.69420648\n",
      "Step: [8399] d_loss: 1.38657737, g_loss: 0.69590354\n",
      "Step: [8400] d_loss: 1.38599145, g_loss: 0.69185007\n",
      "Step: [8401] d_loss: 1.38662195, g_loss: 0.69273108\n",
      "Step: [8402] d_loss: 1.38504159, g_loss: 0.69468224\n",
      "Step: [8403] d_loss: 1.38692021, g_loss: 0.69531697\n",
      "Step: [8404] d_loss: 1.38488138, g_loss: 0.69429827\n",
      "Step: [8405] d_loss: 1.38538265, g_loss: 0.69695067\n",
      "Step: [8406] d_loss: 1.38695669, g_loss: 0.69404554\n",
      "Step: [8407] d_loss: 1.38568294, g_loss: 0.69499797\n",
      "Step: [8408] d_loss: 1.38618064, g_loss: 0.69084978\n",
      "Step: [8409] d_loss: 1.38704264, g_loss: 0.69373298\n",
      "Step: [8410] d_loss: 1.38671768, g_loss: 0.69398725\n",
      "Step: [8411] d_loss: 1.38585377, g_loss: 0.69414067\n",
      "Step: [8412] d_loss: 1.38574219, g_loss: 0.69618225\n",
      "Step: [8413] d_loss: 1.38559389, g_loss: 0.69492579\n",
      "Step: [8414] d_loss: 1.38664961, g_loss: 0.69330978\n",
      "Step: [8415] d_loss: 1.38548386, g_loss: 0.69085395\n",
      "Step: [8416] d_loss: 1.38652730, g_loss: 0.69746953\n",
      "Step: [8417] d_loss: 1.38527548, g_loss: 0.69791687\n",
      "Step: [8418] d_loss: 1.38620090, g_loss: 0.69546437\n",
      "Step: [8419] d_loss: 1.38433993, g_loss: 0.69716984\n",
      "Step: [8420] d_loss: 1.38484907, g_loss: 0.69083858\n",
      "Step: [8421] d_loss: 1.38543260, g_loss: 0.69436622\n",
      "Step: [8422] d_loss: 1.38604903, g_loss: 0.69514227\n",
      "Step: [8423] d_loss: 1.38565850, g_loss: 0.69564712\n",
      "Step: [8424] d_loss: 1.38437736, g_loss: 0.69834697\n",
      "Step: [8425] d_loss: 1.38468647, g_loss: 0.69375259\n",
      "Step: [8426] d_loss: 1.38498318, g_loss: 0.69590068\n",
      "Step: [8427] d_loss: 1.38577247, g_loss: 0.69496137\n",
      "Step: [8428] d_loss: 1.38354993, g_loss: 0.69648767\n",
      "Step: [8429] d_loss: 1.38556504, g_loss: 0.69856042\n",
      "Step: [8430] d_loss: 1.38654244, g_loss: 0.69387329\n",
      "Step: [8431] d_loss: 1.38295555, g_loss: 0.70121783\n",
      "Step: [8432] d_loss: 1.38405836, g_loss: 0.69323742\n",
      "Step: [8433] d_loss: 1.38380611, g_loss: 0.69771743\n",
      "Step: [8434] d_loss: 1.38489056, g_loss: 0.69681424\n",
      "Step: [8435] d_loss: 1.38705134, g_loss: 0.69393688\n",
      "Step: [8436] d_loss: 1.38305950, g_loss: 0.69626284\n",
      "Step: [8437] d_loss: 1.38716984, g_loss: 0.69274664\n",
      "Step: [8438] d_loss: 1.38367724, g_loss: 0.69715422\n",
      "Step: [8439] d_loss: 1.38660789, g_loss: 0.69536269\n",
      "Step: [8440] d_loss: 1.38822675, g_loss: 0.69551086\n",
      "Step: [8441] d_loss: 1.38469458, g_loss: 0.69796526\n",
      "Step: [8442] d_loss: 1.38794827, g_loss: 0.69239378\n",
      "Step: [8443] d_loss: 1.38876677, g_loss: 0.69395119\n",
      "Step: [8444] d_loss: 1.38709545, g_loss: 0.69270682\n",
      "Step: [8445] d_loss: 1.38343811, g_loss: 0.69763702\n",
      "Step: [8446] d_loss: 1.38742101, g_loss: 0.69327521\n",
      "Step: [8447] d_loss: 1.38673544, g_loss: 0.69442946\n",
      "Step: [8448] d_loss: 1.38489032, g_loss: 0.69555038\n",
      "Step: [8449] d_loss: 1.38862419, g_loss: 0.69234884\n",
      "Step: [8450] d_loss: 1.39045322, g_loss: 0.69487280\n",
      "Step: [8451] d_loss: 1.38666844, g_loss: 0.69315016\n",
      "Step: [8452] d_loss: 1.39126432, g_loss: 0.69446522\n",
      "Step: [8453] d_loss: 1.38970411, g_loss: 0.69072759\n",
      "Step: [8454] d_loss: 1.38903248, g_loss: 0.69565547\n",
      "Step: [8455] d_loss: 1.38763547, g_loss: 0.69555140\n",
      "Step: [8456] d_loss: 1.38752365, g_loss: 0.68804371\n",
      "Step: [8457] d_loss: 1.38543129, g_loss: 0.69469625\n",
      "Step: [8458] d_loss: 1.38694537, g_loss: 0.69470149\n",
      "Step: [8459] d_loss: 1.38682127, g_loss: 0.69761944\n",
      "Step: [8460] d_loss: 1.38872814, g_loss: 0.69495153\n",
      "Step: [8461] d_loss: 1.38648748, g_loss: 0.69596481\n",
      "Step: [8462] d_loss: 1.38707829, g_loss: 0.69154590\n",
      "Step: [8463] d_loss: 1.38713050, g_loss: 0.68945086\n",
      "Step: [8464] d_loss: 1.38570857, g_loss: 0.69753289\n",
      "Step: [8465] d_loss: 1.38614011, g_loss: 0.69561070\n",
      "Step: [8466] d_loss: 1.38508391, g_loss: 0.69542933\n",
      "Step: [8467] d_loss: 1.38622761, g_loss: 0.69380212\n",
      "Step: [8468] d_loss: 1.38453567, g_loss: 0.69785297\n",
      "Step: [8469] d_loss: 1.38526511, g_loss: 0.69418335\n",
      "Step: [8470] d_loss: 1.38203931, g_loss: 0.70014012\n",
      "Step: [8471] d_loss: 1.38432384, g_loss: 0.69446474\n",
      "Step: [8472] d_loss: 1.38571966, g_loss: 0.69786179\n",
      "Step: [8473] d_loss: 1.38784075, g_loss: 0.69003898\n",
      "Step: [8474] d_loss: 1.38828647, g_loss: 0.69846267\n",
      "Step: [8475] d_loss: 1.38544142, g_loss: 0.69615304\n",
      "Step: [8476] d_loss: 1.38492131, g_loss: 0.69711924\n",
      "Step: [8477] d_loss: 1.38615716, g_loss: 0.69599009\n",
      "Step: [8478] d_loss: 1.38616800, g_loss: 0.69314384\n",
      "Step: [8479] d_loss: 1.38398314, g_loss: 0.69684970\n",
      "Step: [8480] d_loss: 1.38582397, g_loss: 0.69632328\n",
      "Step: [8481] d_loss: 1.38509798, g_loss: 0.69326490\n",
      "Step: [8482] d_loss: 1.38669229, g_loss: 0.69955564\n",
      "Step: [8483] d_loss: 1.38728166, g_loss: 0.69640684\n",
      "Step: [8484] d_loss: 1.38512397, g_loss: 0.69302726\n",
      "Step: [8485] d_loss: 1.38554931, g_loss: 0.69948924\n",
      "Step: [8486] d_loss: 1.38778925, g_loss: 0.69083703\n",
      "Step: [8487] d_loss: 1.38547468, g_loss: 0.69361079\n",
      "Step: [8488] d_loss: 1.38583660, g_loss: 0.69365096\n",
      "Step: [8489] d_loss: 1.38444734, g_loss: 0.69491637\n",
      "Step: [8490] d_loss: 1.38413596, g_loss: 0.69825482\n",
      "Step: [8491] d_loss: 1.38524556, g_loss: 0.69229561\n",
      "Step: [8492] d_loss: 1.38700831, g_loss: 0.69236493\n",
      "Step: [8493] d_loss: 1.38653946, g_loss: 0.69555950\n",
      "Step: [8494] d_loss: 1.38657260, g_loss: 0.69624376\n",
      "Step: [8495] d_loss: 1.38556910, g_loss: 0.69581389\n",
      "Step: [8496] d_loss: 1.38598740, g_loss: 0.69408536\n",
      "Step: [8497] d_loss: 1.38541961, g_loss: 0.69409096\n",
      "Step: [8498] d_loss: 1.38456798, g_loss: 0.69488883\n",
      "Step: [8499] d_loss: 1.38637924, g_loss: 0.69572866\n",
      "Step: [8500] d_loss: 1.38687515, g_loss: 0.69545698\n",
      "Step: [8501] d_loss: 1.38690126, g_loss: 0.69412595\n",
      "Step: [8502] d_loss: 1.38527262, g_loss: 0.69183636\n",
      "Step: [8503] d_loss: 1.38564956, g_loss: 0.69526649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8504] d_loss: 1.38519239, g_loss: 0.69769979\n",
      "Step: [8505] d_loss: 1.38518071, g_loss: 0.69355595\n",
      "Step: [8506] d_loss: 1.38616145, g_loss: 0.69436443\n",
      "Step: [8507] d_loss: 1.38660693, g_loss: 0.69465154\n",
      "Step: [8508] d_loss: 1.38643408, g_loss: 0.69333076\n",
      "Step: [8509] d_loss: 1.38789082, g_loss: 0.69470143\n",
      "Step: [8510] d_loss: 1.38646972, g_loss: 0.69335514\n",
      "Step: [8511] d_loss: 1.38612950, g_loss: 0.69115478\n",
      "Step: [8512] d_loss: 1.38639390, g_loss: 0.69547445\n",
      "Step: [8513] d_loss: 1.38708949, g_loss: 0.69390905\n",
      "Step: [8514] d_loss: 1.38703656, g_loss: 0.69514930\n",
      "Step: [8515] d_loss: 1.38540602, g_loss: 0.69375789\n",
      "Step: [8516] d_loss: 1.38470852, g_loss: 0.69517231\n",
      "Step: [8517] d_loss: 1.38647437, g_loss: 0.69187570\n",
      "Step: [8518] d_loss: 1.38418674, g_loss: 0.69633722\n",
      "Step: [8519] d_loss: 1.38517928, g_loss: 0.69350237\n",
      "Step: [8520] d_loss: 1.38723063, g_loss: 0.69765335\n",
      "Step: [8521] d_loss: 1.38714004, g_loss: 0.69303620\n",
      "Step: [8522] d_loss: 1.38737714, g_loss: 0.69496453\n",
      "Step: [8523] d_loss: 1.38567877, g_loss: 0.69914669\n",
      "Step: [8524] d_loss: 1.38649261, g_loss: 0.69423091\n",
      "Step: [8525] d_loss: 1.38714194, g_loss: 0.69969213\n",
      "Step: [8526] d_loss: 1.39072609, g_loss: 0.68588579\n",
      "Step: [8527] d_loss: 1.38932681, g_loss: 0.69214559\n",
      "Step: [8528] d_loss: 1.38396108, g_loss: 0.69526815\n",
      "Step: [8529] d_loss: 1.38526022, g_loss: 0.69549274\n",
      "Step: [8530] d_loss: 1.38635647, g_loss: 0.69825262\n",
      "Step: [8531] d_loss: 1.38549042, g_loss: 0.69423103\n",
      "Step: [8532] d_loss: 1.38716936, g_loss: 0.69216025\n",
      "Step: [8533] d_loss: 1.38600302, g_loss: 0.69391435\n",
      "Step: [8534] d_loss: 1.38717270, g_loss: 0.69514650\n",
      "Step: [8535] d_loss: 1.38577533, g_loss: 0.69546437\n",
      "Step: [8536] d_loss: 1.38478637, g_loss: 0.69494694\n",
      "Step: [8537] d_loss: 1.38537598, g_loss: 0.69737697\n",
      "Step: [8538] d_loss: 1.38747668, g_loss: 0.69257802\n",
      "Step: [8539] d_loss: 1.38464689, g_loss: 0.70007563\n",
      "Step: [8540] d_loss: 1.38496375, g_loss: 0.69703025\n",
      "Step: [8541] d_loss: 1.38791966, g_loss: 0.69355100\n",
      "Step: [8542] d_loss: 1.38702011, g_loss: 0.69573534\n",
      "Step: [8543] d_loss: 1.38537407, g_loss: 0.69760931\n",
      "Step: [8544] d_loss: 1.38574767, g_loss: 0.69597840\n",
      "Step: [8545] d_loss: 1.38816512, g_loss: 0.69373024\n",
      "Step: [8546] d_loss: 1.38631105, g_loss: 0.69454014\n",
      "Step: [8547] d_loss: 1.38689470, g_loss: 0.69314170\n",
      "Step: [8548] d_loss: 1.38510966, g_loss: 0.69475985\n",
      "Step: [8549] d_loss: 1.38501608, g_loss: 0.69599324\n",
      "Step: [8550] d_loss: 1.38655066, g_loss: 0.69562197\n",
      "Step: [8551] d_loss: 1.38480806, g_loss: 0.69501299\n",
      "Step: [8552] d_loss: 1.38732231, g_loss: 0.69545591\n",
      "Step: [8553] d_loss: 1.38618755, g_loss: 0.69435453\n",
      "Step: [8554] d_loss: 1.38802636, g_loss: 0.69250369\n",
      "Step: [8555] d_loss: 1.38545024, g_loss: 0.69455564\n",
      "Step: [8556] d_loss: 1.38543427, g_loss: 0.69503915\n",
      "Step: [8557] d_loss: 1.38545132, g_loss: 0.69560504\n",
      "Step: [8558] d_loss: 1.38647783, g_loss: 0.69391489\n",
      "Step: [8559] d_loss: 1.38671637, g_loss: 0.69383132\n",
      "Step: [8560] d_loss: 1.38682306, g_loss: 0.69412774\n",
      "Step: [8561] d_loss: 1.38587284, g_loss: 0.69554782\n",
      "Step: [8562] d_loss: 1.38403368, g_loss: 0.69646180\n",
      "Step: [8563] d_loss: 1.38587713, g_loss: 0.69708163\n",
      "Step: [8564] d_loss: 1.38599086, g_loss: 0.69275594\n",
      "Step: [8565] d_loss: 1.38601255, g_loss: 0.69925642\n",
      "Step: [8566] d_loss: 1.38805699, g_loss: 0.69353795\n",
      "Step: [8567] d_loss: 1.39061952, g_loss: 0.69727933\n",
      "Step: [8568] d_loss: 1.38858056, g_loss: 0.68960547\n",
      "Step: [8569] d_loss: 1.38732553, g_loss: 0.68869227\n",
      "Step: [8570] d_loss: 1.38537383, g_loss: 0.69588882\n",
      "Step: [8571] d_loss: 1.38693810, g_loss: 0.69367379\n",
      "Step: [8572] d_loss: 1.38752961, g_loss: 0.70144171\n",
      "Step: [8573] d_loss: 1.38599277, g_loss: 0.69706774\n",
      "Step: [8574] d_loss: 1.38672662, g_loss: 0.69844556\n",
      "Step: [8575] d_loss: 1.38662565, g_loss: 0.69402432\n",
      "Step: [8576] d_loss: 1.38619304, g_loss: 0.69112039\n",
      "Step: [8577] d_loss: 1.38546896, g_loss: 0.69614685\n",
      "Step: [8578] d_loss: 1.38798380, g_loss: 0.69322860\n",
      "Step: [8579] d_loss: 1.38585150, g_loss: 0.70004350\n",
      "Step: [8580] d_loss: 1.38742483, g_loss: 0.69332647\n",
      "Step: [8581] d_loss: 1.38747513, g_loss: 0.69303554\n",
      "Step: [8582] d_loss: 1.38668358, g_loss: 0.69378287\n",
      "Step: [8583] d_loss: 1.38608587, g_loss: 0.69234729\n",
      "Step: [8584] d_loss: 1.38479662, g_loss: 0.69663060\n",
      "Step: [8585] d_loss: 1.38766229, g_loss: 0.69344974\n",
      "Step: [8586] d_loss: 1.38812530, g_loss: 0.69902766\n",
      "Step: [8587] d_loss: 1.38559055, g_loss: 0.69633955\n",
      "Step: [8588] d_loss: 1.38512301, g_loss: 0.69673222\n",
      "Step: [8589] d_loss: 1.38678896, g_loss: 0.69392884\n",
      "Step: [8590] d_loss: 1.38589406, g_loss: 0.69413304\n",
      "Step: [8591] d_loss: 1.38455343, g_loss: 0.69576597\n",
      "Step: [8592] d_loss: 1.38487792, g_loss: 0.69292009\n",
      "Step: [8593] d_loss: 1.38491178, g_loss: 0.69762886\n",
      "Step: [8594] d_loss: 1.38581133, g_loss: 0.69239825\n",
      "Step: [8595] d_loss: 1.38697314, g_loss: 0.69722515\n",
      "Step: [8596] d_loss: 1.38681126, g_loss: 0.69208765\n",
      "Step: [8597] d_loss: 1.38519144, g_loss: 0.69226038\n",
      "Step: [8598] d_loss: 1.38579226, g_loss: 0.69369423\n",
      "Step: [8599] d_loss: 1.38689446, g_loss: 0.69636685\n",
      "Step: [8600] d_loss: 1.38708699, g_loss: 0.69601136\n",
      "Step: [8601] d_loss: 1.38615608, g_loss: 0.69559652\n",
      "Step: [8602] d_loss: 1.38482499, g_loss: 0.69386840\n",
      "Step: [8603] d_loss: 1.38530779, g_loss: 0.69373775\n",
      "Step: [8604] d_loss: 1.38552094, g_loss: 0.69146246\n",
      "Step: [8605] d_loss: 1.38705206, g_loss: 0.69617331\n",
      "Step: [8606] d_loss: 1.38713312, g_loss: 0.69457299\n",
      "Step: [8607] d_loss: 1.38580537, g_loss: 0.69713652\n",
      "Step: [8608] d_loss: 1.38614166, g_loss: 0.69384181\n",
      "Step: [8609] d_loss: 1.38448572, g_loss: 0.69661915\n",
      "Step: [8610] d_loss: 1.38664103, g_loss: 0.69320595\n",
      "Step: [8611] d_loss: 1.38458872, g_loss: 0.69397819\n",
      "Step: [8612] d_loss: 1.38349390, g_loss: 0.69452453\n",
      "Step: [8613] d_loss: 1.38639021, g_loss: 0.69648898\n",
      "Step: [8614] d_loss: 1.38625252, g_loss: 0.69285154\n",
      "Step: [8615] d_loss: 1.38804603, g_loss: 0.70174915\n",
      "Step: [8616] d_loss: 1.38831878, g_loss: 0.69595027\n",
      "Step: [8617] d_loss: 1.38725352, g_loss: 0.69787270\n",
      "Step: [8618] d_loss: 1.38599873, g_loss: 0.69675624\n",
      "Step: [8619] d_loss: 1.38625157, g_loss: 0.69386029\n",
      "Step: [8620] d_loss: 1.38588715, g_loss: 0.69761503\n",
      "Step: [8621] d_loss: 1.38638628, g_loss: 0.69391614\n",
      "Step: [8622] d_loss: 1.38665104, g_loss: 0.69444716\n",
      "Step: [8623] d_loss: 1.38561642, g_loss: 0.69528997\n",
      "Step: [8624] d_loss: 1.38904095, g_loss: 0.69403839\n",
      "Step: [8625] d_loss: 1.38684559, g_loss: 0.69502747\n",
      "Step: [8626] d_loss: 1.38669896, g_loss: 0.69479257\n",
      "Step: [8627] d_loss: 1.38734841, g_loss: 0.69414657\n",
      "Step: [8628] d_loss: 1.38572955, g_loss: 0.69549298\n",
      "Step: [8629] d_loss: 1.38598144, g_loss: 0.69607002\n",
      "Step: [8630] d_loss: 1.38655972, g_loss: 0.69517589\n",
      "Step: [8631] d_loss: 1.38711393, g_loss: 0.69525969\n",
      "Step: [8632] d_loss: 1.38533103, g_loss: 0.69786704\n",
      "Step: [8633] d_loss: 1.38728523, g_loss: 0.69499099\n",
      "Step: [8634] d_loss: 1.38794458, g_loss: 0.69230962\n",
      "Step: [8635] d_loss: 1.38642561, g_loss: 0.69487101\n",
      "Step: [8636] d_loss: 1.38647640, g_loss: 0.69513625\n",
      "Step: [8637] d_loss: 1.38720667, g_loss: 0.69557428\n",
      "Step: [8638] d_loss: 1.38484740, g_loss: 0.69495964\n",
      "Step: [8639] d_loss: 1.38412273, g_loss: 0.69669813\n",
      "Step: [8640] d_loss: 1.38474953, g_loss: 0.69674003\n",
      "Step: [8641] d_loss: 1.38712621, g_loss: 0.69825625\n",
      "Step: [8642] d_loss: 1.38613796, g_loss: 0.69375938\n",
      "Step: [8643] d_loss: 1.38549709, g_loss: 0.69720757\n",
      "Step: [8644] d_loss: 1.38694501, g_loss: 0.69396806\n",
      "Step: [8645] d_loss: 1.38514459, g_loss: 0.69476002\n",
      "Step: [8646] d_loss: 1.38696480, g_loss: 0.69190747\n",
      "Step: [8647] d_loss: 1.38381672, g_loss: 0.69756973\n",
      "Step: [8648] d_loss: 1.38661885, g_loss: 0.69576186\n",
      "Step: [8649] d_loss: 1.38461328, g_loss: 0.69565988\n",
      "Step: [8650] d_loss: 1.38441348, g_loss: 0.69539350\n",
      "Step: [8651] d_loss: 1.38496923, g_loss: 0.69623756\n",
      "Step: [8652] d_loss: 1.38510907, g_loss: 0.69622433\n",
      "Step: [8653] d_loss: 1.38436997, g_loss: 0.69670343\n",
      "Step: [8654] d_loss: 1.38695550, g_loss: 0.69643009\n",
      "Step: [8655] d_loss: 1.38506639, g_loss: 0.69796842\n",
      "Step: [8656] d_loss: 1.38546181, g_loss: 0.69289231\n",
      "Step: [8657] d_loss: 1.38928747, g_loss: 0.69483948\n",
      "Step: [8658] d_loss: 1.38322377, g_loss: 0.69728339\n",
      "Step: [8659] d_loss: 1.38373113, g_loss: 0.69950533\n",
      "Step: [8660] d_loss: 1.38681960, g_loss: 0.69409811\n",
      "Step: [8661] d_loss: 1.38581061, g_loss: 0.69662935\n",
      "Step: [8662] d_loss: 1.38448620, g_loss: 0.69610721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8663] d_loss: 1.38640392, g_loss: 0.69420737\n",
      "Step: [8664] d_loss: 1.38632464, g_loss: 0.69616538\n",
      "Step: [8665] d_loss: 1.38531446, g_loss: 0.69470781\n",
      "Step: [8666] d_loss: 1.38824344, g_loss: 0.69424665\n",
      "Step: [8667] d_loss: 1.38706934, g_loss: 0.69587064\n",
      "Step: [8668] d_loss: 1.38626981, g_loss: 0.69616097\n",
      "Step: [8669] d_loss: 1.38634646, g_loss: 0.69332075\n",
      "Step: [8670] d_loss: 1.38844299, g_loss: 0.69183809\n",
      "Step: [8671] d_loss: 1.38693488, g_loss: 0.69543463\n",
      "Step: [8672] d_loss: 1.38969517, g_loss: 0.69406319\n",
      "Step: [8673] d_loss: 1.38934207, g_loss: 0.69388425\n",
      "Step: [8674] d_loss: 1.38924742, g_loss: 0.69227648\n",
      "Step: [8675] d_loss: 1.38538361, g_loss: 0.69471490\n",
      "Step: [8676] d_loss: 1.38671851, g_loss: 0.69195402\n",
      "Step: [8677] d_loss: 1.38496327, g_loss: 0.69590867\n",
      "Step: [8678] d_loss: 1.38692975, g_loss: 0.69529980\n",
      "Step: [8679] d_loss: 1.38531613, g_loss: 0.69578153\n",
      "Step: [8680] d_loss: 1.38773251, g_loss: 0.69378048\n",
      "Step: [8681] d_loss: 1.38502550, g_loss: 0.69453716\n",
      "Step: [8682] d_loss: 1.38716507, g_loss: 0.69308597\n",
      "Step: [8683] d_loss: 1.38868725, g_loss: 0.69340968\n",
      "Step: [8684] d_loss: 1.38529420, g_loss: 0.69562745\n",
      "Step: [8685] d_loss: 1.38536119, g_loss: 0.69489813\n",
      "Step: [8686] d_loss: 1.38591909, g_loss: 0.69358814\n",
      "Step: [8687] d_loss: 1.38514662, g_loss: 0.69265515\n",
      "Step: [8688] d_loss: 1.38562918, g_loss: 0.69594669\n",
      "Step: [8689] d_loss: 1.38473320, g_loss: 0.69669276\n",
      "Step: [8690] d_loss: 1.38425255, g_loss: 0.69969457\n",
      "Step: [8691] d_loss: 1.38415372, g_loss: 0.69608498\n",
      "Step: [8692] d_loss: 1.38510931, g_loss: 0.69549555\n",
      "Step: [8693] d_loss: 1.38565540, g_loss: 0.69414699\n",
      "Step: [8694] d_loss: 1.38625145, g_loss: 0.69645983\n",
      "Step: [8695] d_loss: 1.38613844, g_loss: 0.69786936\n",
      "Step: [8696] d_loss: 1.38510144, g_loss: 0.69821024\n",
      "Step: [8697] d_loss: 1.38670540, g_loss: 0.69370043\n",
      "Step: [8698] d_loss: 1.38403916, g_loss: 0.69659591\n",
      "Step: [8699] d_loss: 1.38640726, g_loss: 0.69026816\n",
      "Step: [8700] d_loss: 1.38436580, g_loss: 0.69592488\n",
      "Step: [8701] d_loss: 1.38356400, g_loss: 0.69427967\n",
      "Step: [8702] d_loss: 1.38478053, g_loss: 0.69811416\n",
      "Step: [8703] d_loss: 1.38907528, g_loss: 0.69420451\n",
      "Step: [8704] d_loss: 1.38614583, g_loss: 0.69714487\n",
      "Step: [8705] d_loss: 1.38590324, g_loss: 0.69521594\n",
      "Step: [8706] d_loss: 1.38686967, g_loss: 0.69677329\n",
      "Step: [8707] d_loss: 1.38723695, g_loss: 0.69123858\n",
      "Step: [8708] d_loss: 1.38490593, g_loss: 0.69612783\n",
      "Step: [8709] d_loss: 1.38845909, g_loss: 0.69240069\n",
      "Step: [8710] d_loss: 1.38678050, g_loss: 0.70169425\n",
      "Step: [8711] d_loss: 1.39146900, g_loss: 0.68380964\n",
      "Step: [8712] d_loss: 1.39377570, g_loss: 0.69666421\n",
      "Step: [8713] d_loss: 1.39233601, g_loss: 0.69738233\n",
      "Step: [8714] d_loss: 1.38563991, g_loss: 0.69953185\n",
      "Step: [8715] d_loss: 1.38747072, g_loss: 0.69642198\n",
      "Step: [8716] d_loss: 1.38934851, g_loss: 0.69114900\n",
      "Step: [8717] d_loss: 1.39026248, g_loss: 0.69239402\n",
      "Step: [8718] d_loss: 1.38610399, g_loss: 0.69901526\n",
      "Step: [8719] d_loss: 1.38925290, g_loss: 0.69377363\n",
      "Step: [8720] d_loss: 1.38567924, g_loss: 0.69726789\n",
      "Step: [8721] d_loss: 1.38720453, g_loss: 0.69273508\n",
      "Step: [8722] d_loss: 1.38608193, g_loss: 0.69430804\n",
      "Step: [8723] d_loss: 1.38480425, g_loss: 0.69522226\n",
      "Step: [8724] d_loss: 1.38696098, g_loss: 0.69622040\n",
      "Step: [8725] d_loss: 1.38620448, g_loss: 0.69540256\n",
      "Step: [8726] d_loss: 1.38591087, g_loss: 0.69662279\n",
      "Step: [8727] d_loss: 1.38767934, g_loss: 0.69398570\n",
      "Step: [8728] d_loss: 1.38681567, g_loss: 0.69498575\n",
      "Step: [8729] d_loss: 1.38532591, g_loss: 0.69318897\n",
      "Step: [8730] d_loss: 1.38506365, g_loss: 0.69486082\n",
      "Step: [8731] d_loss: 1.38691878, g_loss: 0.69483465\n",
      "Step: [8732] d_loss: 1.38588023, g_loss: 0.69704962\n",
      "Step: [8733] d_loss: 1.38628519, g_loss: 0.69899553\n",
      "Step: [8734] d_loss: 1.38594937, g_loss: 0.69275743\n",
      "Step: [8735] d_loss: 1.38606679, g_loss: 0.69491410\n",
      "Step: [8736] d_loss: 1.38708878, g_loss: 0.68970817\n",
      "Step: [8737] d_loss: 1.38595784, g_loss: 0.69653440\n",
      "Step: [8738] d_loss: 1.38623381, g_loss: 0.69570470\n",
      "Step: [8739] d_loss: 1.38572121, g_loss: 0.69456398\n",
      "Step: [8740] d_loss: 1.38586390, g_loss: 0.69564545\n",
      "Step: [8741] d_loss: 1.38720739, g_loss: 0.69257581\n",
      "Step: [8742] d_loss: 1.38597012, g_loss: 0.69479024\n",
      "Step: [8743] d_loss: 1.38116300, g_loss: 0.69660771\n",
      "Step: [8744] d_loss: 1.38595796, g_loss: 0.69353449\n",
      "Step: [8745] d_loss: 1.38441920, g_loss: 0.69741762\n",
      "Step: [8746] d_loss: 1.38562620, g_loss: 0.69747257\n",
      "Step: [8747] d_loss: 1.38578415, g_loss: 0.69550335\n",
      "Step: [8748] d_loss: 1.38573253, g_loss: 0.69624913\n",
      "Step: [8749] d_loss: 1.38847613, g_loss: 0.69361061\n",
      "Step: [8750] d_loss: 1.38661873, g_loss: 0.69361013\n",
      "Step: [8751] d_loss: 1.38602996, g_loss: 0.69400793\n",
      "Step: [8752] d_loss: 1.38523602, g_loss: 0.69625795\n",
      "Step: [8753] d_loss: 1.38596022, g_loss: 0.69522679\n",
      "Step: [8754] d_loss: 1.38596725, g_loss: 0.69531482\n",
      "Step: [8755] d_loss: 1.38609171, g_loss: 0.69522047\n",
      "Step: [8756] d_loss: 1.38654888, g_loss: 0.69620121\n",
      "Step: [8757] d_loss: 1.38698411, g_loss: 0.69204581\n",
      "Step: [8758] d_loss: 1.38593268, g_loss: 0.69679207\n",
      "Step: [8759] d_loss: 1.38758421, g_loss: 0.69543850\n",
      "Step: [8760] d_loss: 1.38706732, g_loss: 0.69985318\n",
      "Step: [8761] d_loss: 1.38600135, g_loss: 0.69798338\n",
      "Step: [8762] d_loss: 1.38606405, g_loss: 0.69643950\n",
      "Step: [8763] d_loss: 1.38661027, g_loss: 0.69433945\n",
      "Step: [8764] d_loss: 1.38276231, g_loss: 0.69997180\n",
      "Step: [8765] d_loss: 1.38559294, g_loss: 0.69465780\n",
      "Step: [8766] d_loss: 1.38448691, g_loss: 0.69596118\n",
      "Step: [8767] d_loss: 1.38750577, g_loss: 0.69383180\n",
      "Step: [8768] d_loss: 1.38633478, g_loss: 0.70127386\n",
      "Step: [8769] d_loss: 1.38921809, g_loss: 0.70026064\n",
      "Step: [8770] d_loss: 1.38780773, g_loss: 0.70774907\n",
      "Step: [8771] d_loss: 1.38894463, g_loss: 0.69493115\n",
      "Step: [8772] d_loss: 1.38576221, g_loss: 0.69412649\n",
      "Step: [8773] d_loss: 1.38399458, g_loss: 0.69398022\n",
      "Step: [8774] d_loss: 1.38595760, g_loss: 0.69487453\n",
      "Step: [8775] d_loss: 1.38581109, g_loss: 0.69889921\n",
      "Step: [8776] d_loss: 1.38603044, g_loss: 0.69449067\n",
      "Step: [8777] d_loss: 1.38715422, g_loss: 0.69510967\n",
      "Step: [8778] d_loss: 1.38534045, g_loss: 0.69560361\n",
      "Step: [8779] d_loss: 1.38267756, g_loss: 0.69815314\n",
      "Step: [8780] d_loss: 1.38724911, g_loss: 0.69438922\n",
      "Step: [8781] d_loss: 1.38557529, g_loss: 0.69446206\n",
      "Step: [8782] d_loss: 1.38735831, g_loss: 0.70014858\n",
      "Step: [8783] d_loss: 1.38813174, g_loss: 0.69788003\n",
      "Step: [8784] d_loss: 1.38766778, g_loss: 0.69176972\n",
      "Step: [8785] d_loss: 1.38673246, g_loss: 0.69312620\n",
      "Step: [8786] d_loss: 1.38938975, g_loss: 0.69272608\n",
      "Step: [8787] d_loss: 1.39043689, g_loss: 0.69514978\n",
      "Step: [8788] d_loss: 1.39183426, g_loss: 0.69109368\n",
      "Step: [8789] d_loss: 1.39009035, g_loss: 0.70223153\n",
      "Step: [8790] d_loss: 1.38934600, g_loss: 0.69206387\n",
      "Step: [8791] d_loss: 1.38800788, g_loss: 0.69623733\n",
      "Step: [8792] d_loss: 1.38632011, g_loss: 0.69416904\n",
      "Step: [8793] d_loss: 1.38546252, g_loss: 0.69451404\n",
      "Step: [8794] d_loss: 1.38783622, g_loss: 0.69563770\n",
      "Step: [8795] d_loss: 1.38895154, g_loss: 0.69511747\n",
      "Step: [8796] d_loss: 1.38692403, g_loss: 0.69981998\n",
      "Step: [8797] d_loss: 1.38899767, g_loss: 0.69430208\n",
      "Step: [8798] d_loss: 1.38994563, g_loss: 0.69362390\n",
      "Step: [8799] d_loss: 1.38707399, g_loss: 0.69729471\n",
      "Step: [8800] d_loss: 1.38570547, g_loss: 0.69970769\n",
      "Step: [8801] d_loss: 1.38495064, g_loss: 0.69495994\n",
      "Step: [8802] d_loss: 1.38506985, g_loss: 0.69917679\n",
      "Step: [8803] d_loss: 1.38545275, g_loss: 0.69570237\n",
      "Step: [8804] d_loss: 1.39025366, g_loss: 0.70099819\n",
      "Step: [8805] d_loss: 1.39123809, g_loss: 0.70277315\n",
      "Step: [8806] d_loss: 1.38914037, g_loss: 0.69474512\n",
      "Step: [8807] d_loss: 1.38834810, g_loss: 0.69167304\n",
      "Step: [8808] d_loss: 1.38850713, g_loss: 0.69833881\n",
      "Step: [8809] d_loss: 1.38739526, g_loss: 0.69612759\n",
      "Step: [8810] d_loss: 1.38719714, g_loss: 0.69610268\n",
      "Step: [8811] d_loss: 1.38659513, g_loss: 0.69685960\n",
      "Step: [8812] d_loss: 1.39067650, g_loss: 0.69769508\n",
      "Step: [8813] d_loss: 1.38572705, g_loss: 0.69703877\n",
      "Step: [8814] d_loss: 1.38655257, g_loss: 0.69329417\n",
      "Step: [8815] d_loss: 1.39014816, g_loss: 0.69101804\n",
      "Step: [8816] d_loss: 1.38799453, g_loss: 0.69019121\n",
      "Step: [8817] d_loss: 1.38669109, g_loss: 0.69820106\n",
      "Step: [8818] d_loss: 1.38822627, g_loss: 0.69894862\n",
      "Step: [8819] d_loss: 1.38756859, g_loss: 0.69406879\n",
      "Step: [8820] d_loss: 1.38713121, g_loss: 0.69145417\n",
      "Step: [8821] d_loss: 1.38568950, g_loss: 0.69586074\n",
      "Step: [8822] d_loss: 1.38797987, g_loss: 0.69484997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8823] d_loss: 1.38513398, g_loss: 0.69529450\n",
      "Step: [8824] d_loss: 1.38534701, g_loss: 0.69255364\n",
      "Step: [8825] d_loss: 1.38729346, g_loss: 0.69464397\n",
      "Step: [8826] d_loss: 1.38623214, g_loss: 0.69434988\n",
      "Step: [8827] d_loss: 1.38373876, g_loss: 0.69690788\n",
      "Step: [8828] d_loss: 1.38566184, g_loss: 0.69455487\n",
      "Step: [8829] d_loss: 1.38522935, g_loss: 0.69639826\n",
      "Step: [8830] d_loss: 1.38630652, g_loss: 0.69421512\n",
      "Step: [8831] d_loss: 1.38785434, g_loss: 0.69068670\n",
      "Step: [8832] d_loss: 1.38717341, g_loss: 0.69674730\n",
      "Step: [8833] d_loss: 1.38519144, g_loss: 0.69687778\n",
      "Step: [8834] d_loss: 1.38555717, g_loss: 0.69651282\n",
      "Step: [8835] d_loss: 1.38543475, g_loss: 0.69888628\n",
      "Step: [8836] d_loss: 1.38751459, g_loss: 0.69415689\n",
      "Step: [8837] d_loss: 1.38759053, g_loss: 0.69396484\n",
      "Step: [8838] d_loss: 1.38543153, g_loss: 0.69532543\n",
      "Step: [8839] d_loss: 1.38559079, g_loss: 0.69638091\n",
      "Step: [8840] d_loss: 1.38351154, g_loss: 0.69592458\n",
      "Step: [8841] d_loss: 1.38817763, g_loss: 0.69409144\n",
      "Step: [8842] d_loss: 1.38351011, g_loss: 0.69647473\n",
      "Step: [8843] d_loss: 1.38642859, g_loss: 0.69565964\n",
      "Step: [8844] d_loss: 1.38528001, g_loss: 0.69770575\n",
      "Step: [8845] d_loss: 1.38563204, g_loss: 0.69472575\n",
      "Step: [8846] d_loss: 1.38301528, g_loss: 0.69732964\n",
      "Step: [8847] d_loss: 1.38813746, g_loss: 0.69215846\n",
      "Step: [8848] d_loss: 1.38531876, g_loss: 0.69700491\n",
      "Step: [8849] d_loss: 1.38490975, g_loss: 0.69493020\n",
      "Step: [8850] d_loss: 1.38405466, g_loss: 0.69708031\n",
      "Step: [8851] d_loss: 1.38567638, g_loss: 0.69603014\n",
      "Step: [8852] d_loss: 1.38595510, g_loss: 0.69522798\n",
      "Step: [8853] d_loss: 1.38472724, g_loss: 0.69669199\n",
      "Step: [8854] d_loss: 1.38501132, g_loss: 0.69387519\n",
      "Step: [8855] d_loss: 1.38675094, g_loss: 0.69347721\n",
      "Step: [8856] d_loss: 1.38591230, g_loss: 0.69540536\n",
      "Step: [8857] d_loss: 1.38547349, g_loss: 0.69665539\n",
      "Step: [8858] d_loss: 1.38174152, g_loss: 0.69781679\n",
      "Step: [8859] d_loss: 1.38517284, g_loss: 0.69464517\n",
      "Step: [8860] d_loss: 1.38586378, g_loss: 0.69496840\n",
      "Step: [8861] d_loss: 1.38759422, g_loss: 0.69680643\n",
      "Step: [8862] d_loss: 1.38626909, g_loss: 0.69288558\n",
      "Step: [8863] d_loss: 1.38596737, g_loss: 0.69684184\n",
      "Step: [8864] d_loss: 1.39005303, g_loss: 0.69516492\n",
      "Step: [8865] d_loss: 1.38610554, g_loss: 0.69528735\n",
      "Step: [8866] d_loss: 1.38913298, g_loss: 0.69364154\n",
      "Step: [8867] d_loss: 1.38818669, g_loss: 0.69124317\n",
      "Step: [8868] d_loss: 1.38713622, g_loss: 0.69421780\n",
      "Step: [8869] d_loss: 1.38646948, g_loss: 0.69296795\n",
      "Step: [8870] d_loss: 1.38694787, g_loss: 0.69247001\n",
      "Step: [8871] d_loss: 1.38578939, g_loss: 0.69760889\n",
      "Step: [8872] d_loss: 1.38587117, g_loss: 0.69492042\n",
      "Step: [8873] d_loss: 1.38775110, g_loss: 0.69761765\n",
      "Step: [8874] d_loss: 1.38646531, g_loss: 0.69461322\n",
      "Step: [8875] d_loss: 1.38776743, g_loss: 0.69327712\n",
      "Step: [8876] d_loss: 1.38621640, g_loss: 0.69464207\n",
      "Step: [8877] d_loss: 1.38812637, g_loss: 0.69379634\n",
      "Step: [8878] d_loss: 1.38825321, g_loss: 0.69650519\n",
      "Step: [8879] d_loss: 1.38730812, g_loss: 0.69051230\n",
      "Step: [8880] d_loss: 1.38820577, g_loss: 0.69544601\n",
      "Step: [8881] d_loss: 1.38773072, g_loss: 0.69614047\n",
      "Step: [8882] d_loss: 1.38777220, g_loss: 0.69387376\n",
      "Step: [8883] d_loss: 1.38599157, g_loss: 0.69532466\n",
      "Step: [8884] d_loss: 1.38300228, g_loss: 0.69514036\n",
      "Step: [8885] d_loss: 1.38128757, g_loss: 0.69950068\n",
      "Step: [8886] d_loss: 1.38632810, g_loss: 0.69502157\n",
      "Step: [8887] d_loss: 1.38572478, g_loss: 0.69705069\n",
      "Step: [8888] d_loss: 1.38723063, g_loss: 0.69126272\n",
      "Step: [8889] d_loss: 1.38783383, g_loss: 0.69927049\n",
      "Step: [8890] d_loss: 1.38756108, g_loss: 0.69291997\n",
      "Step: [8891] d_loss: 1.38684213, g_loss: 0.70028114\n",
      "Step: [8892] d_loss: 1.38644385, g_loss: 0.69677681\n",
      "Step: [8893] d_loss: 1.38755965, g_loss: 0.69380176\n",
      "Step: [8894] d_loss: 1.38703930, g_loss: 0.69305170\n",
      "Step: [8895] d_loss: 1.38757825, g_loss: 0.69123536\n",
      "Step: [8896] d_loss: 1.38804078, g_loss: 0.69316125\n",
      "Step: [8897] d_loss: 1.38516545, g_loss: 0.69597375\n",
      "Step: [8898] d_loss: 1.38554621, g_loss: 0.69449753\n",
      "Step: [8899] d_loss: 1.38399374, g_loss: 0.69698578\n",
      "Step: [8900] d_loss: 1.38777900, g_loss: 0.69523388\n",
      "Step: [8901] d_loss: 1.38541365, g_loss: 0.69571728\n",
      "Step: [8902] d_loss: 1.38559353, g_loss: 0.69587708\n",
      "Step: [8903] d_loss: 1.38600421, g_loss: 0.69333410\n",
      "Step: [8904] d_loss: 1.38605499, g_loss: 0.69759327\n",
      "Step: [8905] d_loss: 1.38225412, g_loss: 0.69814849\n",
      "Step: [8906] d_loss: 1.38531303, g_loss: 0.69767165\n",
      "Step: [8907] d_loss: 1.38701153, g_loss: 0.69163859\n",
      "Step: [8908] d_loss: 1.38729501, g_loss: 0.69393826\n",
      "Step: [8909] d_loss: 1.38437271, g_loss: 0.69739705\n",
      "Step: [8910] d_loss: 1.38230348, g_loss: 0.69909430\n",
      "Step: [8911] d_loss: 1.38510871, g_loss: 0.69775045\n",
      "Step: [8912] d_loss: 1.38533115, g_loss: 0.69528008\n",
      "Step: [8913] d_loss: 1.38334322, g_loss: 0.69801277\n",
      "Step: [8914] d_loss: 1.38837028, g_loss: 0.69249701\n",
      "Step: [8915] d_loss: 1.38578331, g_loss: 0.69403231\n",
      "Step: [8916] d_loss: 1.38417554, g_loss: 0.69573557\n",
      "Step: [8917] d_loss: 1.38794422, g_loss: 0.69487983\n",
      "Step: [8918] d_loss: 1.38895655, g_loss: 0.69462800\n",
      "Step: [8919] d_loss: 1.38593471, g_loss: 0.69475842\n",
      "Step: [8920] d_loss: 1.38843822, g_loss: 0.69392997\n",
      "Step: [8921] d_loss: 1.38560009, g_loss: 0.69703138\n",
      "Step: [8922] d_loss: 1.38407850, g_loss: 0.69748092\n",
      "Step: [8923] d_loss: 1.38950539, g_loss: 0.69746119\n",
      "Step: [8924] d_loss: 1.38855481, g_loss: 0.69120276\n",
      "Step: [8925] d_loss: 1.39178801, g_loss: 0.70107782\n",
      "Step: [8926] d_loss: 1.39222956, g_loss: 0.69365364\n",
      "Step: [8927] d_loss: 1.38839686, g_loss: 0.69526392\n",
      "Step: [8928] d_loss: 1.38807988, g_loss: 0.69306892\n",
      "Step: [8929] d_loss: 1.38918638, g_loss: 0.69261611\n",
      "Step: [8930] d_loss: 1.38728356, g_loss: 0.69566184\n",
      "Step: [8931] d_loss: 1.38701642, g_loss: 0.69589639\n",
      "Step: [8932] d_loss: 1.38867855, g_loss: 0.69552565\n",
      "Step: [8933] d_loss: 1.38601470, g_loss: 0.69534451\n",
      "Step: [8934] d_loss: 1.38669753, g_loss: 0.69350898\n",
      "Step: [8935] d_loss: 1.38866973, g_loss: 0.69394678\n",
      "Step: [8936] d_loss: 1.38920105, g_loss: 0.69490236\n",
      "Step: [8937] d_loss: 1.38704014, g_loss: 0.69430804\n",
      "Step: [8938] d_loss: 1.38747013, g_loss: 0.70154673\n",
      "Step: [8939] d_loss: 1.38930082, g_loss: 0.70015466\n",
      "Step: [8940] d_loss: 1.38695776, g_loss: 0.69389153\n",
      "Step: [8941] d_loss: 1.38424242, g_loss: 0.69475079\n",
      "Step: [8942] d_loss: 1.38216233, g_loss: 0.69484514\n",
      "Step: [8943] d_loss: 1.38556504, g_loss: 0.69513094\n",
      "Step: [8944] d_loss: 1.38329351, g_loss: 0.69760180\n",
      "Step: [8945] d_loss: 1.38319802, g_loss: 0.69966412\n",
      "Step: [8946] d_loss: 1.38358986, g_loss: 0.69955802\n",
      "Step: [8947] d_loss: 1.38276505, g_loss: 0.69725853\n",
      "Step: [8948] d_loss: 1.38680267, g_loss: 0.69867814\n",
      "Step: [8949] d_loss: 1.38528466, g_loss: 0.69708723\n",
      "Step: [8950] d_loss: 1.37926245, g_loss: 0.69981104\n",
      "Step: [8951] d_loss: 1.38810468, g_loss: 0.69615185\n",
      "Step: [8952] d_loss: 1.38443089, g_loss: 0.69717538\n",
      "Step: [8953] d_loss: 1.38630533, g_loss: 0.69149029\n",
      "Step: [8954] d_loss: 1.38714719, g_loss: 0.69651556\n",
      "Step: [8955] d_loss: 1.38713193, g_loss: 0.69849753\n",
      "Step: [8956] d_loss: 1.38798511, g_loss: 0.69677335\n",
      "Step: [8957] d_loss: 1.38999176, g_loss: 0.69655883\n",
      "Step: [8958] d_loss: 1.39110804, g_loss: 0.69412428\n",
      "Step: [8959] d_loss: 1.39079404, g_loss: 0.69200855\n",
      "Step: [8960] d_loss: 1.39147460, g_loss: 0.69372720\n",
      "Step: [8961] d_loss: 1.38886213, g_loss: 0.69311976\n",
      "Step: [8962] d_loss: 1.38698030, g_loss: 0.69699329\n",
      "Step: [8963] d_loss: 1.38893247, g_loss: 0.69509977\n",
      "Step: [8964] d_loss: 1.38824177, g_loss: 0.69466609\n",
      "Step: [8965] d_loss: 1.38894105, g_loss: 0.69377822\n",
      "Step: [8966] d_loss: 1.38755560, g_loss: 0.69649208\n",
      "Step: [8967] d_loss: 1.38634205, g_loss: 0.69691616\n",
      "Step: [8968] d_loss: 1.38645089, g_loss: 0.69576335\n",
      "Step: [8969] d_loss: 1.38887048, g_loss: 0.69513631\n",
      "Step: [8970] d_loss: 1.38760734, g_loss: 0.69271773\n",
      "Step: [8971] d_loss: 1.38630664, g_loss: 0.69445360\n",
      "Step: [8972] d_loss: 1.38596869, g_loss: 0.69370562\n",
      "Step: [8973] d_loss: 1.38653564, g_loss: 0.69627035\n",
      "Step: [8974] d_loss: 1.38823247, g_loss: 0.69462240\n",
      "Step: [8975] d_loss: 1.38329959, g_loss: 0.69922501\n",
      "Step: [8976] d_loss: 1.38581169, g_loss: 0.69417441\n",
      "Step: [8977] d_loss: 1.38390625, g_loss: 0.69636023\n",
      "Step: [8978] d_loss: 1.38526630, g_loss: 0.69856238\n",
      "Step: [8979] d_loss: 1.38458300, g_loss: 0.69448072\n",
      "Step: [8980] d_loss: 1.38535070, g_loss: 0.69776422\n",
      "Step: [8981] d_loss: 1.38562977, g_loss: 0.69743598\n",
      "Step: [8982] d_loss: 1.38862264, g_loss: 0.69723082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [8983] d_loss: 1.39022565, g_loss: 0.69518304\n",
      "Step: [8984] d_loss: 1.38432670, g_loss: 0.69249892\n",
      "Step: [8985] d_loss: 1.38610005, g_loss: 0.69114912\n",
      "Step: [8986] d_loss: 1.38759065, g_loss: 0.69628429\n",
      "Step: [8987] d_loss: 1.38644159, g_loss: 0.69801897\n",
      "Step: [8988] d_loss: 1.38567567, g_loss: 0.69966620\n",
      "Step: [8989] d_loss: 1.38535118, g_loss: 0.69631976\n",
      "Step: [8990] d_loss: 1.38709342, g_loss: 0.69245052\n",
      "Step: [8991] d_loss: 1.38366425, g_loss: 0.69473290\n",
      "Step: [8992] d_loss: 1.38400459, g_loss: 0.69635916\n",
      "Step: [8993] d_loss: 1.38359809, g_loss: 0.70065475\n",
      "Step: [8994] d_loss: 1.38121486, g_loss: 0.69637054\n",
      "Step: [8995] d_loss: 1.38580966, g_loss: 0.69992363\n",
      "Step: [8996] d_loss: 1.38744020, g_loss: 0.69655776\n",
      "Step: [8997] d_loss: 1.38813567, g_loss: 0.69685972\n",
      "Step: [8998] d_loss: 1.38884115, g_loss: 0.69668937\n",
      "Step: [8999] d_loss: 1.38404179, g_loss: 0.69947731\n",
      "Step: [9000] d_loss: 1.38583219, g_loss: 0.69587815\n",
      "Step: [9001] d_loss: 1.38452196, g_loss: 0.69635683\n",
      "Step: [9002] d_loss: 1.39098668, g_loss: 0.69751644\n",
      "Step: [9003] d_loss: 1.38986421, g_loss: 0.69516784\n",
      "Step: [9004] d_loss: 1.39009857, g_loss: 0.69749177\n",
      "Step: [9005] d_loss: 1.38634157, g_loss: 0.69554794\n",
      "Step: [9006] d_loss: 1.38981795, g_loss: 0.69445932\n",
      "Step: [9007] d_loss: 1.38792396, g_loss: 0.69663805\n",
      "Step: [9008] d_loss: 1.38735282, g_loss: 0.69623965\n",
      "Step: [9009] d_loss: 1.38869047, g_loss: 0.69530451\n",
      "Step: [9010] d_loss: 1.38832128, g_loss: 0.69603288\n",
      "Step: [9011] d_loss: 1.38782907, g_loss: 0.69589025\n",
      "Step: [9012] d_loss: 1.38531423, g_loss: 0.70108473\n",
      "Step: [9013] d_loss: 1.38614368, g_loss: 0.69852066\n",
      "Step: [9014] d_loss: 1.38571990, g_loss: 0.69642639\n",
      "Step: [9015] d_loss: 1.39069033, g_loss: 0.69219774\n",
      "Step: [9016] d_loss: 1.38679242, g_loss: 0.69232547\n",
      "Step: [9017] d_loss: 1.39014983, g_loss: 0.69364738\n",
      "Step: [9018] d_loss: 1.38859820, g_loss: 0.69730943\n",
      "Step: [9019] d_loss: 1.38362336, g_loss: 0.70138919\n",
      "Step: [9020] d_loss: 1.38822246, g_loss: 0.69697273\n",
      "Step: [9021] d_loss: 1.38747942, g_loss: 0.69434208\n",
      "Step: [9022] d_loss: 1.38537550, g_loss: 0.69726247\n",
      "Step: [9023] d_loss: 1.38498664, g_loss: 0.69683558\n",
      "Step: [9024] d_loss: 1.38488793, g_loss: 0.69651419\n",
      "Step: [9025] d_loss: 1.38379025, g_loss: 0.69861740\n",
      "Step: [9026] d_loss: 1.38256621, g_loss: 0.69904697\n",
      "Step: [9027] d_loss: 1.38690662, g_loss: 0.69465864\n",
      "Step: [9028] d_loss: 1.38532567, g_loss: 0.69808012\n",
      "Step: [9029] d_loss: 1.38421655, g_loss: 0.70006818\n",
      "Step: [9030] d_loss: 1.38427305, g_loss: 0.69677919\n",
      "Step: [9031] d_loss: 1.38296306, g_loss: 0.69668722\n",
      "Step: [9032] d_loss: 1.38854122, g_loss: 0.69772404\n",
      "Step: [9033] d_loss: 1.38401079, g_loss: 0.69397724\n",
      "Step: [9034] d_loss: 1.38927841, g_loss: 0.69586694\n",
      "Step: [9035] d_loss: 1.38891423, g_loss: 0.69905043\n",
      "Step: [9036] d_loss: 1.38455439, g_loss: 0.69755149\n",
      "Step: [9037] d_loss: 1.38687682, g_loss: 0.69471908\n",
      "Step: [9038] d_loss: 1.38906431, g_loss: 0.69681865\n",
      "Step: [9039] d_loss: 1.38722110, g_loss: 0.69776022\n",
      "Step: [9040] d_loss: 1.38801503, g_loss: 0.69603693\n",
      "Step: [9041] d_loss: 1.38618958, g_loss: 0.69962704\n",
      "Step: [9042] d_loss: 1.38973665, g_loss: 0.69203043\n",
      "Step: [9043] d_loss: 1.38881075, g_loss: 0.69447213\n",
      "Step: [9044] d_loss: 1.38702691, g_loss: 0.69532835\n",
      "Step: [9045] d_loss: 1.38621783, g_loss: 0.69946373\n",
      "Step: [9046] d_loss: 1.38523912, g_loss: 0.69637853\n",
      "Step: [9047] d_loss: 1.38711345, g_loss: 0.69581461\n",
      "Step: [9048] d_loss: 1.38255787, g_loss: 0.69913721\n",
      "Step: [9049] d_loss: 1.38327014, g_loss: 0.69700181\n",
      "Step: [9050] d_loss: 1.38231003, g_loss: 0.69837314\n",
      "Step: [9051] d_loss: 1.38532650, g_loss: 0.69901156\n",
      "Step: [9052] d_loss: 1.38474631, g_loss: 0.69738293\n",
      "Step: [9053] d_loss: 1.38621378, g_loss: 0.69935131\n",
      "Step: [9054] d_loss: 1.38737631, g_loss: 0.69519371\n",
      "Step: [9055] d_loss: 1.38519871, g_loss: 0.69390363\n",
      "Step: [9056] d_loss: 1.38543391, g_loss: 0.69512618\n",
      "Step: [9057] d_loss: 1.38827085, g_loss: 0.69234151\n",
      "Step: [9058] d_loss: 1.38471317, g_loss: 0.69526076\n",
      "Step: [9059] d_loss: 1.38789749, g_loss: 0.69580758\n",
      "Step: [9060] d_loss: 1.38623536, g_loss: 0.69473600\n",
      "Step: [9061] d_loss: 1.38744771, g_loss: 0.69568628\n",
      "Step: [9062] d_loss: 1.38766003, g_loss: 0.69672763\n",
      "Step: [9063] d_loss: 1.38746738, g_loss: 0.69407588\n",
      "Step: [9064] d_loss: 1.38957047, g_loss: 0.69539905\n",
      "Step: [9065] d_loss: 1.38570881, g_loss: 0.69564837\n",
      "Step: [9066] d_loss: 1.38656604, g_loss: 0.69808233\n",
      "Step: [9067] d_loss: 1.38479209, g_loss: 0.69566286\n",
      "Step: [9068] d_loss: 1.38863909, g_loss: 0.69486189\n",
      "Step: [9069] d_loss: 1.38685215, g_loss: 0.69201517\n",
      "Step: [9070] d_loss: 1.38727415, g_loss: 0.69423693\n",
      "Step: [9071] d_loss: 1.38517499, g_loss: 0.69520855\n",
      "Step: [9072] d_loss: 1.38447881, g_loss: 0.69950610\n",
      "Step: [9073] d_loss: 1.38486981, g_loss: 0.69510245\n",
      "Step: [9074] d_loss: 1.38540173, g_loss: 0.70003414\n",
      "Step: [9075] d_loss: 1.38931799, g_loss: 0.69204116\n",
      "Step: [9076] d_loss: 1.38869476, g_loss: 0.69537890\n",
      "Step: [9077] d_loss: 1.38297999, g_loss: 0.69456077\n",
      "Step: [9078] d_loss: 1.38494837, g_loss: 0.69983482\n",
      "Step: [9079] d_loss: 1.38639736, g_loss: 0.69455439\n",
      "Step: [9080] d_loss: 1.38492334, g_loss: 0.69746870\n",
      "Step: [9081] d_loss: 1.38483369, g_loss: 0.69615388\n",
      "Step: [9082] d_loss: 1.38324833, g_loss: 0.69680893\n",
      "Step: [9083] d_loss: 1.38239849, g_loss: 0.69756258\n",
      "Step: [9084] d_loss: 1.38524914, g_loss: 0.69858313\n",
      "Step: [9085] d_loss: 1.38472939, g_loss: 0.69732332\n",
      "Step: [9086] d_loss: 1.38158619, g_loss: 0.70312530\n",
      "Step: [9087] d_loss: 1.38447261, g_loss: 0.70136577\n",
      "Step: [9088] d_loss: 1.38592231, g_loss: 0.69551152\n",
      "Step: [9089] d_loss: 1.38450098, g_loss: 0.69751108\n",
      "Step: [9090] d_loss: 1.38376105, g_loss: 0.70002687\n",
      "Step: [9091] d_loss: 1.38691258, g_loss: 0.68909490\n",
      "Step: [9092] d_loss: 1.38752782, g_loss: 0.69570696\n",
      "Step: [9093] d_loss: 1.38617754, g_loss: 0.69466078\n",
      "Step: [9094] d_loss: 1.38559783, g_loss: 0.70095211\n",
      "Step: [9095] d_loss: 1.38827276, g_loss: 0.69620359\n",
      "Step: [9096] d_loss: 1.39218068, g_loss: 0.69455713\n",
      "Step: [9097] d_loss: 1.38713825, g_loss: 0.69518125\n",
      "Step: [9098] d_loss: 1.38631189, g_loss: 0.69518405\n",
      "Step: [9099] d_loss: 1.38771558, g_loss: 0.69640112\n",
      "Step: [9100] d_loss: 1.38901520, g_loss: 0.69768208\n",
      "Step: [9101] d_loss: 1.39524782, g_loss: 0.69703245\n",
      "Step: [9102] d_loss: 1.39225388, g_loss: 0.69077635\n",
      "Step: [9103] d_loss: 1.38856018, g_loss: 0.69284391\n",
      "Step: [9104] d_loss: 1.38626409, g_loss: 0.70132685\n",
      "Step: [9105] d_loss: 1.38930500, g_loss: 0.69407141\n",
      "Step: [9106] d_loss: 1.38752627, g_loss: 0.69687510\n",
      "Step: [9107] d_loss: 1.38351047, g_loss: 0.69628775\n",
      "Step: [9108] d_loss: 1.38579059, g_loss: 0.69637203\n",
      "Step: [9109] d_loss: 1.38497448, g_loss: 0.69534993\n",
      "Step: [9110] d_loss: 1.38445473, g_loss: 0.69661665\n",
      "Step: [9111] d_loss: 1.38587928, g_loss: 0.69445682\n",
      "Step: [9112] d_loss: 1.38649380, g_loss: 0.69450212\n",
      "Step: [9113] d_loss: 1.38755143, g_loss: 0.69250679\n",
      "Step: [9114] d_loss: 1.38435435, g_loss: 0.70059347\n",
      "Step: [9115] d_loss: 1.38536620, g_loss: 0.69819272\n",
      "Step: [9116] d_loss: 1.38449299, g_loss: 0.69786870\n",
      "Step: [9117] d_loss: 1.38629365, g_loss: 0.69475430\n",
      "Step: [9118] d_loss: 1.38539219, g_loss: 0.69277757\n",
      "Step: [9119] d_loss: 1.38420010, g_loss: 0.69556808\n",
      "Step: [9120] d_loss: 1.38377619, g_loss: 0.70068556\n",
      "Step: [9121] d_loss: 1.38578320, g_loss: 0.69638038\n",
      "Step: [9122] d_loss: 1.38333058, g_loss: 0.69809735\n",
      "Step: [9123] d_loss: 1.38620305, g_loss: 0.69418740\n",
      "Step: [9124] d_loss: 1.38805294, g_loss: 0.70207238\n",
      "Step: [9125] d_loss: 1.38867664, g_loss: 0.69320244\n",
      "Step: [9126] d_loss: 1.38611615, g_loss: 0.69626248\n",
      "Step: [9127] d_loss: 1.38549197, g_loss: 0.69669038\n",
      "Step: [9128] d_loss: 1.38441694, g_loss: 0.69615883\n",
      "Step: [9129] d_loss: 1.38756490, g_loss: 0.69631588\n",
      "Step: [9130] d_loss: 1.38419819, g_loss: 0.69501543\n",
      "Step: [9131] d_loss: 1.38551998, g_loss: 0.69517386\n",
      "Step: [9132] d_loss: 1.38609195, g_loss: 0.69680792\n",
      "Step: [9133] d_loss: 1.38910306, g_loss: 0.69812417\n",
      "Step: [9134] d_loss: 1.38510633, g_loss: 0.69375277\n",
      "Step: [9135] d_loss: 1.38768697, g_loss: 0.69085622\n",
      "Step: [9136] d_loss: 1.38731039, g_loss: 0.69503367\n",
      "Step: [9137] d_loss: 1.38668871, g_loss: 0.69626606\n",
      "Step: [9138] d_loss: 1.38812482, g_loss: 0.69847864\n",
      "Step: [9139] d_loss: 1.38784766, g_loss: 0.69308358\n",
      "Step: [9140] d_loss: 1.39055872, g_loss: 0.70241189\n",
      "Step: [9141] d_loss: 1.38502109, g_loss: 0.69298458\n",
      "Step: [9142] d_loss: 1.38858044, g_loss: 0.69269186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9143] d_loss: 1.38429523, g_loss: 0.69721216\n",
      "Step: [9144] d_loss: 1.38531172, g_loss: 0.69172406\n",
      "Step: [9145] d_loss: 1.38243687, g_loss: 0.70352161\n",
      "Step: [9146] d_loss: 1.38648498, g_loss: 0.69666523\n",
      "Step: [9147] d_loss: 1.38539600, g_loss: 0.69781572\n",
      "Step: [9148] d_loss: 1.38589704, g_loss: 0.69337833\n",
      "Step: [9149] d_loss: 1.38637495, g_loss: 0.69430661\n",
      "Step: [9150] d_loss: 1.38356292, g_loss: 0.69584692\n",
      "Step: [9151] d_loss: 1.38790774, g_loss: 0.69471961\n",
      "Step: [9152] d_loss: 1.38266301, g_loss: 0.69805777\n",
      "Step: [9153] d_loss: 1.38466883, g_loss: 0.69553924\n",
      "Step: [9154] d_loss: 1.38637817, g_loss: 0.69676846\n",
      "Step: [9155] d_loss: 1.38689160, g_loss: 0.69178647\n",
      "Step: [9156] d_loss: 1.38769269, g_loss: 0.69744945\n",
      "Step: [9157] d_loss: 1.38910055, g_loss: 0.69077164\n",
      "Step: [9158] d_loss: 1.38632226, g_loss: 0.69636273\n",
      "Step: [9159] d_loss: 1.38455057, g_loss: 0.69579118\n",
      "Step: [9160] d_loss: 1.38735735, g_loss: 0.69410777\n",
      "Step: [9161] d_loss: 1.38625073, g_loss: 0.69602072\n",
      "Step: [9162] d_loss: 1.38505185, g_loss: 0.69828737\n",
      "Step: [9163] d_loss: 1.38733220, g_loss: 0.69360816\n",
      "Step: [9164] d_loss: 1.38747704, g_loss: 0.69188386\n",
      "Step: [9165] d_loss: 1.38581252, g_loss: 0.69570637\n",
      "Step: [9166] d_loss: 1.38824475, g_loss: 0.69160259\n",
      "Step: [9167] d_loss: 1.38628340, g_loss: 0.69372666\n",
      "Step: [9168] d_loss: 1.38917994, g_loss: 0.69487977\n",
      "Step: [9169] d_loss: 1.38710463, g_loss: 0.69829476\n",
      "Step: [9170] d_loss: 1.38673711, g_loss: 0.69731581\n",
      "Step: [9171] d_loss: 1.38749552, g_loss: 0.69088626\n",
      "Step: [9172] d_loss: 1.38686132, g_loss: 0.68999255\n",
      "Step: [9173] d_loss: 1.38683891, g_loss: 0.69354260\n",
      "Step: [9174] d_loss: 1.38926935, g_loss: 0.69602495\n",
      "Step: [9175] d_loss: 1.38681984, g_loss: 0.69616354\n",
      "Step: [9176] d_loss: 1.38685107, g_loss: 0.69570446\n",
      "Step: [9177] d_loss: 1.38530838, g_loss: 0.69545126\n",
      "Step: [9178] d_loss: 1.38571453, g_loss: 0.69566774\n",
      "Step: [9179] d_loss: 1.38326240, g_loss: 0.69607228\n",
      "Step: [9180] d_loss: 1.38392282, g_loss: 0.69493616\n",
      "Step: [9181] d_loss: 1.38524723, g_loss: 0.69694155\n",
      "Step: [9182] d_loss: 1.38608146, g_loss: 0.69305784\n",
      "Step: [9183] d_loss: 1.38557374, g_loss: 0.69574064\n",
      "Step: [9184] d_loss: 1.38731003, g_loss: 0.69310915\n",
      "Step: [9185] d_loss: 1.38702059, g_loss: 0.70369995\n",
      "Step: [9186] d_loss: 1.38718510, g_loss: 0.70017982\n",
      "Step: [9187] d_loss: 1.38779449, g_loss: 0.70189738\n",
      "Step: [9188] d_loss: 1.38712907, g_loss: 0.69287789\n",
      "Step: [9189] d_loss: 1.38762486, g_loss: 0.69103575\n",
      "Step: [9190] d_loss: 1.38663232, g_loss: 0.69575655\n",
      "Step: [9191] d_loss: 1.38495064, g_loss: 0.69488466\n",
      "Step: [9192] d_loss: 1.38507390, g_loss: 0.69555163\n",
      "Step: [9193] d_loss: 1.38528204, g_loss: 0.69513059\n",
      "Step: [9194] d_loss: 1.38428068, g_loss: 0.69781858\n",
      "Step: [9195] d_loss: 1.38615799, g_loss: 0.69408083\n",
      "Step: [9196] d_loss: 1.38921261, g_loss: 0.69390464\n",
      "Step: [9197] d_loss: 1.38610554, g_loss: 0.69305837\n",
      "Step: [9198] d_loss: 1.38718939, g_loss: 0.69467545\n",
      "Step: [9199] d_loss: 1.38722491, g_loss: 0.69827807\n",
      "Step: [9200] d_loss: 1.38500130, g_loss: 0.69353747\n",
      "Step: [9201] d_loss: 1.38674974, g_loss: 0.69260025\n",
      "Step: [9202] d_loss: 1.38711166, g_loss: 0.69303608\n",
      "Step: [9203] d_loss: 1.38930047, g_loss: 0.69426447\n",
      "Step: [9204] d_loss: 1.38688397, g_loss: 0.69456315\n",
      "Step: [9205] d_loss: 1.38668442, g_loss: 0.69556761\n",
      "Step: [9206] d_loss: 1.38800490, g_loss: 0.69327569\n",
      "Step: [9207] d_loss: 1.38888240, g_loss: 0.69244534\n",
      "Step: [9208] d_loss: 1.38706017, g_loss: 0.69622111\n",
      "Step: [9209] d_loss: 1.38639057, g_loss: 0.69443625\n",
      "Step: [9210] d_loss: 1.38766241, g_loss: 0.69411087\n",
      "Step: [9211] d_loss: 1.38467681, g_loss: 0.69727886\n",
      "Step: [9212] d_loss: 1.38539386, g_loss: 0.69462878\n",
      "Step: [9213] d_loss: 1.38727093, g_loss: 0.69322252\n",
      "Step: [9214] d_loss: 1.38488984, g_loss: 0.69366521\n",
      "Step: [9215] d_loss: 1.38541770, g_loss: 0.69492447\n",
      "Step: [9216] d_loss: 1.38608849, g_loss: 0.69641089\n",
      "Step: [9217] d_loss: 1.38644123, g_loss: 0.69522089\n",
      "Step: [9218] d_loss: 1.38522172, g_loss: 0.69601047\n",
      "Step: [9219] d_loss: 1.38558745, g_loss: 0.69624496\n",
      "Step: [9220] d_loss: 1.38724864, g_loss: 0.69541943\n",
      "Step: [9221] d_loss: 1.38657081, g_loss: 0.69386256\n",
      "Step: [9222] d_loss: 1.38417757, g_loss: 0.69517016\n",
      "Step: [9223] d_loss: 1.38337767, g_loss: 0.69559401\n",
      "Step: [9224] d_loss: 1.38516402, g_loss: 0.69382310\n",
      "Step: [9225] d_loss: 1.38482165, g_loss: 0.69433314\n",
      "Step: [9226] d_loss: 1.38661802, g_loss: 0.69493079\n",
      "Step: [9227] d_loss: 1.38550758, g_loss: 0.69777989\n",
      "Step: [9228] d_loss: 1.38568556, g_loss: 0.69502813\n",
      "Step: [9229] d_loss: 1.38531756, g_loss: 0.69762981\n",
      "Step: [9230] d_loss: 1.38536417, g_loss: 0.69291544\n",
      "Step: [9231] d_loss: 1.38673973, g_loss: 0.69270074\n",
      "Step: [9232] d_loss: 1.38646841, g_loss: 0.69776666\n",
      "Step: [9233] d_loss: 1.38640523, g_loss: 0.69198978\n",
      "Step: [9234] d_loss: 1.38582897, g_loss: 0.69843471\n",
      "Step: [9235] d_loss: 1.38830209, g_loss: 0.69260514\n",
      "Step: [9236] d_loss: 1.38815284, g_loss: 0.69514394\n",
      "Step: [9237] d_loss: 1.38666201, g_loss: 0.68987972\n",
      "Step: [9238] d_loss: 1.38600814, g_loss: 0.69509697\n",
      "Step: [9239] d_loss: 1.38341904, g_loss: 0.69634563\n",
      "Step: [9240] d_loss: 1.38787460, g_loss: 0.69083846\n",
      "Step: [9241] d_loss: 1.38700199, g_loss: 0.69476402\n",
      "Step: [9242] d_loss: 1.38693976, g_loss: 0.69279701\n",
      "Step: [9243] d_loss: 1.38463879, g_loss: 0.69403815\n",
      "Step: [9244] d_loss: 1.38615179, g_loss: 0.69602346\n",
      "Step: [9245] d_loss: 1.38696599, g_loss: 0.69100279\n",
      "Step: [9246] d_loss: 1.38623524, g_loss: 0.69928151\n",
      "Step: [9247] d_loss: 1.38495123, g_loss: 0.69613570\n",
      "Step: [9248] d_loss: 1.38678467, g_loss: 0.69383138\n",
      "Step: [9249] d_loss: 1.38692319, g_loss: 0.69281369\n",
      "Step: [9250] d_loss: 1.38615417, g_loss: 0.69520760\n",
      "Step: [9251] d_loss: 1.38501215, g_loss: 0.69359553\n",
      "Step: [9252] d_loss: 1.38444602, g_loss: 0.69542134\n",
      "Step: [9253] d_loss: 1.38453341, g_loss: 0.69428146\n",
      "Step: [9254] d_loss: 1.38587642, g_loss: 0.69423294\n",
      "Step: [9255] d_loss: 1.38770783, g_loss: 0.69477552\n",
      "Step: [9256] d_loss: 1.38413310, g_loss: 0.69785035\n",
      "Step: [9257] d_loss: 1.38386726, g_loss: 0.69686806\n",
      "Step: [9258] d_loss: 1.38614368, g_loss: 0.69628227\n",
      "Step: [9259] d_loss: 1.38502955, g_loss: 0.69407248\n",
      "Step: [9260] d_loss: 1.38549566, g_loss: 0.69482541\n",
      "Step: [9261] d_loss: 1.38602281, g_loss: 0.69399226\n",
      "Step: [9262] d_loss: 1.38512266, g_loss: 0.69696307\n",
      "Step: [9263] d_loss: 1.38224912, g_loss: 0.69619632\n",
      "Step: [9264] d_loss: 1.38549089, g_loss: 0.69761395\n",
      "Step: [9265] d_loss: 1.38491631, g_loss: 0.69658655\n",
      "Step: [9266] d_loss: 1.38600802, g_loss: 0.69347924\n",
      "Step: [9267] d_loss: 1.38633680, g_loss: 0.69507653\n",
      "Step: [9268] d_loss: 1.38690495, g_loss: 0.69689524\n",
      "Step: [9269] d_loss: 1.38705111, g_loss: 0.68826288\n",
      "Step: [9270] d_loss: 1.38441133, g_loss: 0.69740564\n",
      "Step: [9271] d_loss: 1.38539076, g_loss: 0.69410276\n",
      "Step: [9272] d_loss: 1.38576388, g_loss: 0.69512576\n",
      "Step: [9273] d_loss: 1.38667738, g_loss: 0.69755524\n",
      "Step: [9274] d_loss: 1.38615239, g_loss: 0.69367808\n",
      "Step: [9275] d_loss: 1.38649046, g_loss: 0.69413453\n",
      "Step: [9276] d_loss: 1.38778400, g_loss: 0.69432533\n",
      "Step: [9277] d_loss: 1.38975477, g_loss: 0.69359040\n",
      "Step: [9278] d_loss: 1.38638926, g_loss: 0.69357455\n",
      "Step: [9279] d_loss: 1.38690960, g_loss: 0.69897687\n",
      "Step: [9280] d_loss: 1.38459504, g_loss: 0.69743747\n",
      "Step: [9281] d_loss: 1.38633442, g_loss: 0.69323206\n",
      "Step: [9282] d_loss: 1.38766217, g_loss: 0.69125295\n",
      "Step: [9283] d_loss: 1.38744855, g_loss: 0.69471145\n",
      "Step: [9284] d_loss: 1.38741899, g_loss: 0.69419909\n",
      "Step: [9285] d_loss: 1.38882446, g_loss: 0.69254732\n",
      "Step: [9286] d_loss: 1.38708878, g_loss: 0.69301587\n",
      "Step: [9287] d_loss: 1.38754749, g_loss: 0.69498914\n",
      "Step: [9288] d_loss: 1.38656688, g_loss: 0.69727409\n",
      "Step: [9289] d_loss: 1.38669562, g_loss: 0.69627744\n",
      "Step: [9290] d_loss: 1.38607538, g_loss: 0.69465995\n",
      "Step: [9291] d_loss: 1.38322484, g_loss: 0.69590652\n",
      "Step: [9292] d_loss: 1.38518548, g_loss: 0.69681919\n",
      "Step: [9293] d_loss: 1.38613117, g_loss: 0.69332021\n",
      "Step: [9294] d_loss: 1.38529062, g_loss: 0.69325340\n",
      "Step: [9295] d_loss: 1.38523769, g_loss: 0.69430351\n",
      "Step: [9296] d_loss: 1.38607836, g_loss: 0.69638562\n",
      "Step: [9297] d_loss: 1.38396978, g_loss: 0.69745445\n",
      "Step: [9298] d_loss: 1.38554573, g_loss: 0.69592524\n",
      "Step: [9299] d_loss: 1.38554418, g_loss: 0.69364595\n",
      "Step: [9300] d_loss: 1.38520193, g_loss: 0.69388878\n",
      "Step: [9301] d_loss: 1.38426924, g_loss: 0.69419926\n",
      "Step: [9302] d_loss: 1.38682032, g_loss: 0.69550592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9303] d_loss: 1.38358426, g_loss: 0.69595987\n",
      "Step: [9304] d_loss: 1.38639951, g_loss: 0.69681686\n",
      "Step: [9305] d_loss: 1.38701999, g_loss: 0.69452953\n",
      "Step: [9306] d_loss: 1.38778555, g_loss: 0.69527078\n",
      "Step: [9307] d_loss: 1.38544726, g_loss: 0.69317424\n",
      "Step: [9308] d_loss: 1.38594770, g_loss: 0.69834137\n",
      "Step: [9309] d_loss: 1.38818610, g_loss: 0.69484860\n",
      "Step: [9310] d_loss: 1.38673127, g_loss: 0.69325316\n",
      "Step: [9311] d_loss: 1.38742971, g_loss: 0.69467831\n",
      "Step: [9312] d_loss: 1.38619566, g_loss: 0.69702691\n",
      "Step: [9313] d_loss: 1.38648248, g_loss: 0.69439518\n",
      "Step: [9314] d_loss: 1.38632059, g_loss: 0.69364941\n",
      "Step: [9315] d_loss: 1.38845515, g_loss: 0.69720501\n",
      "Step: [9316] d_loss: 1.38541675, g_loss: 0.69486582\n",
      "Step: [9317] d_loss: 1.38691378, g_loss: 0.69400179\n",
      "Step: [9318] d_loss: 1.38625395, g_loss: 0.69428676\n",
      "Step: [9319] d_loss: 1.38563299, g_loss: 0.69483793\n",
      "Step: [9320] d_loss: 1.38733149, g_loss: 0.69665396\n",
      "Step: [9321] d_loss: 1.38775861, g_loss: 0.69293201\n",
      "Step: [9322] d_loss: 1.38803267, g_loss: 0.69233561\n",
      "Step: [9323] d_loss: 1.38526249, g_loss: 0.69443274\n",
      "Step: [9324] d_loss: 1.38819337, g_loss: 0.69614244\n",
      "Step: [9325] d_loss: 1.38814020, g_loss: 0.69345653\n",
      "Step: [9326] d_loss: 1.38773751, g_loss: 0.69062269\n",
      "Step: [9327] d_loss: 1.38671267, g_loss: 0.69262928\n",
      "Step: [9328] d_loss: 1.38516092, g_loss: 0.69597191\n",
      "Step: [9329] d_loss: 1.38727415, g_loss: 0.69819158\n",
      "Step: [9330] d_loss: 1.38616395, g_loss: 0.69641602\n",
      "Step: [9331] d_loss: 1.38730383, g_loss: 0.68943077\n",
      "Step: [9332] d_loss: 1.38731718, g_loss: 0.69012839\n",
      "Step: [9333] d_loss: 1.38529480, g_loss: 0.69431698\n",
      "Step: [9334] d_loss: 1.38671994, g_loss: 0.69379759\n",
      "Step: [9335] d_loss: 1.38696051, g_loss: 0.69833839\n",
      "Step: [9336] d_loss: 1.38693869, g_loss: 0.69679898\n",
      "Step: [9337] d_loss: 1.38662314, g_loss: 0.69189930\n",
      "Step: [9338] d_loss: 1.38504124, g_loss: 0.69313723\n",
      "Step: [9339] d_loss: 1.38521194, g_loss: 0.69543529\n",
      "Step: [9340] d_loss: 1.38543141, g_loss: 0.69540250\n",
      "Step: [9341] d_loss: 1.38558233, g_loss: 0.69575262\n",
      "Step: [9342] d_loss: 1.38530159, g_loss: 0.69520152\n",
      "Step: [9343] d_loss: 1.38456500, g_loss: 0.69417739\n",
      "Step: [9344] d_loss: 1.38494658, g_loss: 0.69423747\n",
      "Step: [9345] d_loss: 1.38513684, g_loss: 0.69605577\n",
      "Step: [9346] d_loss: 1.38539171, g_loss: 0.69676888\n",
      "Step: [9347] d_loss: 1.38551521, g_loss: 0.69377130\n",
      "Step: [9348] d_loss: 1.38479471, g_loss: 0.69592273\n",
      "Step: [9349] d_loss: 1.38603127, g_loss: 0.69629091\n",
      "Step: [9350] d_loss: 1.38423574, g_loss: 0.69589347\n",
      "Step: [9351] d_loss: 1.38529897, g_loss: 0.69487804\n",
      "Step: [9352] d_loss: 1.38475859, g_loss: 0.69599229\n",
      "Step: [9353] d_loss: 1.38413525, g_loss: 0.69389069\n",
      "Step: [9354] d_loss: 1.38563836, g_loss: 0.69356304\n",
      "Step: [9355] d_loss: 1.38418245, g_loss: 0.69711214\n",
      "Step: [9356] d_loss: 1.38531137, g_loss: 0.69429350\n",
      "Step: [9357] d_loss: 1.38664365, g_loss: 0.69375348\n",
      "Step: [9358] d_loss: 1.38773298, g_loss: 0.69004548\n",
      "Step: [9359] d_loss: 1.38489115, g_loss: 0.69811928\n",
      "Step: [9360] d_loss: 1.38754010, g_loss: 0.69590080\n",
      "Step: [9361] d_loss: 1.38564301, g_loss: 0.69537354\n",
      "Step: [9362] d_loss: 1.38606548, g_loss: 0.69450223\n",
      "Step: [9363] d_loss: 1.38605082, g_loss: 0.69307935\n",
      "Step: [9364] d_loss: 1.38428831, g_loss: 0.69532299\n",
      "Step: [9365] d_loss: 1.38580441, g_loss: 0.69521958\n",
      "Step: [9366] d_loss: 1.38656223, g_loss: 0.69621217\n",
      "Step: [9367] d_loss: 1.38620996, g_loss: 0.69400734\n",
      "Step: [9368] d_loss: 1.38591063, g_loss: 0.69692481\n",
      "Step: [9369] d_loss: 1.38744950, g_loss: 0.69380915\n",
      "Step: [9370] d_loss: 1.38770509, g_loss: 0.69506025\n",
      "Step: [9371] d_loss: 1.38680673, g_loss: 0.69451165\n",
      "Step: [9372] d_loss: 1.38781416, g_loss: 0.69307530\n",
      "Step: [9373] d_loss: 1.38612580, g_loss: 0.69396341\n",
      "Step: [9374] d_loss: 1.38596892, g_loss: 0.69365668\n",
      "Step: [9375] d_loss: 1.38905227, g_loss: 0.69303524\n",
      "Step: [9376] d_loss: 1.38534570, g_loss: 0.69565243\n",
      "Step: [9377] d_loss: 1.38645196, g_loss: 0.69302940\n",
      "Step: [9378] d_loss: 1.38704324, g_loss: 0.69313729\n",
      "Step: [9379] d_loss: 1.38743591, g_loss: 0.69288558\n",
      "Step: [9380] d_loss: 1.38659513, g_loss: 0.69629419\n",
      "Step: [9381] d_loss: 1.38537514, g_loss: 0.69246161\n",
      "Step: [9382] d_loss: 1.38774991, g_loss: 0.69477475\n",
      "Step: [9383] d_loss: 1.38643098, g_loss: 0.69378710\n",
      "Step: [9384] d_loss: 1.38553143, g_loss: 0.69240862\n",
      "Step: [9385] d_loss: 1.38646686, g_loss: 0.69506651\n",
      "Step: [9386] d_loss: 1.38795924, g_loss: 0.69398439\n",
      "Step: [9387] d_loss: 1.38598585, g_loss: 0.69852781\n",
      "Step: [9388] d_loss: 1.38657093, g_loss: 0.69408029\n",
      "Step: [9389] d_loss: 1.38650036, g_loss: 0.69257683\n",
      "Step: [9390] d_loss: 1.38722229, g_loss: 0.69222796\n",
      "Step: [9391] d_loss: 1.38528037, g_loss: 0.69245607\n",
      "Step: [9392] d_loss: 1.38719678, g_loss: 0.69070828\n",
      "Step: [9393] d_loss: 1.38767290, g_loss: 0.70169699\n",
      "Step: [9394] d_loss: 1.39040053, g_loss: 0.69437981\n",
      "Step: [9395] d_loss: 1.39104104, g_loss: 0.69530368\n",
      "Step: [9396] d_loss: 1.38565469, g_loss: 0.69447452\n",
      "Step: [9397] d_loss: 1.38489985, g_loss: 0.69186926\n",
      "Step: [9398] d_loss: 1.38757062, g_loss: 0.69297761\n",
      "Step: [9399] d_loss: 1.38511980, g_loss: 0.69725931\n",
      "Step: [9400] d_loss: 1.38633800, g_loss: 0.69304121\n",
      "Step: [9401] d_loss: 1.38735652, g_loss: 0.69212914\n",
      "Step: [9402] d_loss: 1.38491213, g_loss: 0.69519281\n",
      "Step: [9403] d_loss: 1.38517332, g_loss: 0.69567823\n",
      "Step: [9404] d_loss: 1.38570011, g_loss: 0.69460404\n",
      "Step: [9405] d_loss: 1.38643527, g_loss: 0.69404918\n",
      "Step: [9406] d_loss: 1.38584852, g_loss: 0.69484913\n",
      "Step: [9407] d_loss: 1.38615870, g_loss: 0.69254035\n",
      "Step: [9408] d_loss: 1.38595068, g_loss: 0.69487929\n",
      "Step: [9409] d_loss: 1.38465083, g_loss: 0.69317973\n",
      "Step: [9410] d_loss: 1.38545179, g_loss: 0.69502246\n",
      "Step: [9411] d_loss: 1.38471961, g_loss: 0.69328094\n",
      "Step: [9412] d_loss: 1.38540494, g_loss: 0.69353282\n",
      "Step: [9413] d_loss: 1.38472366, g_loss: 0.69669229\n",
      "Step: [9414] d_loss: 1.38484967, g_loss: 0.69448358\n",
      "Step: [9415] d_loss: 1.38658297, g_loss: 0.69666106\n",
      "Step: [9416] d_loss: 1.38556814, g_loss: 0.69285220\n",
      "Step: [9417] d_loss: 1.38697767, g_loss: 0.69247246\n",
      "Step: [9418] d_loss: 1.38644338, g_loss: 0.69503939\n",
      "Step: [9419] d_loss: 1.38439846, g_loss: 0.69844997\n",
      "Step: [9420] d_loss: 1.38732219, g_loss: 0.69430506\n",
      "Step: [9421] d_loss: 1.38738942, g_loss: 0.69374537\n",
      "Step: [9422] d_loss: 1.38449693, g_loss: 0.69784606\n",
      "Step: [9423] d_loss: 1.38582921, g_loss: 0.69366580\n",
      "Step: [9424] d_loss: 1.38457084, g_loss: 0.69615626\n",
      "Step: [9425] d_loss: 1.38621044, g_loss: 0.69347841\n",
      "Step: [9426] d_loss: 1.38473678, g_loss: 0.69617891\n",
      "Step: [9427] d_loss: 1.38719606, g_loss: 0.69363654\n",
      "Step: [9428] d_loss: 1.38967156, g_loss: 0.69469327\n",
      "Step: [9429] d_loss: 1.38704395, g_loss: 0.69057441\n",
      "Step: [9430] d_loss: 1.38689351, g_loss: 0.69845819\n",
      "Step: [9431] d_loss: 1.38967860, g_loss: 0.68919551\n",
      "Step: [9432] d_loss: 1.38607216, g_loss: 0.69485319\n",
      "Step: [9433] d_loss: 1.38595748, g_loss: 0.69310081\n",
      "Step: [9434] d_loss: 1.38705778, g_loss: 0.69279373\n",
      "Step: [9435] d_loss: 1.38723505, g_loss: 0.69415611\n",
      "Step: [9436] d_loss: 1.38525271, g_loss: 0.69644427\n",
      "Step: [9437] d_loss: 1.38663244, g_loss: 0.69535118\n",
      "Step: [9438] d_loss: 1.38494277, g_loss: 0.69399440\n",
      "Step: [9439] d_loss: 1.38672948, g_loss: 0.69245410\n",
      "Step: [9440] d_loss: 1.38570142, g_loss: 0.69309938\n",
      "Step: [9441] d_loss: 1.38695049, g_loss: 0.69787818\n",
      "Step: [9442] d_loss: 1.38666320, g_loss: 0.69312257\n",
      "Step: [9443] d_loss: 1.38681078, g_loss: 0.69645333\n",
      "Step: [9444] d_loss: 1.38615417, g_loss: 0.69319695\n",
      "Step: [9445] d_loss: 1.38652861, g_loss: 0.69184220\n",
      "Step: [9446] d_loss: 1.38635802, g_loss: 0.69418359\n",
      "Step: [9447] d_loss: 1.38606596, g_loss: 0.69312656\n",
      "Step: [9448] d_loss: 1.38580775, g_loss: 0.69721103\n",
      "Step: [9449] d_loss: 1.38646698, g_loss: 0.69515991\n",
      "Step: [9450] d_loss: 1.38566136, g_loss: 0.69423199\n",
      "Step: [9451] d_loss: 1.38473773, g_loss: 0.69512373\n",
      "Step: [9452] d_loss: 1.38595819, g_loss: 0.69517088\n",
      "Step: [9453] d_loss: 1.38606131, g_loss: 0.69484389\n",
      "Step: [9454] d_loss: 1.38537550, g_loss: 0.69572699\n",
      "Step: [9455] d_loss: 1.38657248, g_loss: 0.69505286\n",
      "Step: [9456] d_loss: 1.38683963, g_loss: 0.69413495\n",
      "Step: [9457] d_loss: 1.38427329, g_loss: 0.69343394\n",
      "Step: [9458] d_loss: 1.38540626, g_loss: 0.69673359\n",
      "Step: [9459] d_loss: 1.38682497, g_loss: 0.69521070\n",
      "Step: [9460] d_loss: 1.38644528, g_loss: 0.69526935\n",
      "Step: [9461] d_loss: 1.38771749, g_loss: 0.68906677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9462] d_loss: 1.38618445, g_loss: 0.69346970\n",
      "Step: [9463] d_loss: 1.38660073, g_loss: 0.69330400\n",
      "Step: [9464] d_loss: 1.38654912, g_loss: 0.69182146\n",
      "Step: [9465] d_loss: 1.38649917, g_loss: 0.69398749\n",
      "Step: [9466] d_loss: 1.38732517, g_loss: 0.69335276\n",
      "Step: [9467] d_loss: 1.38555169, g_loss: 0.69373572\n",
      "Step: [9468] d_loss: 1.38485980, g_loss: 0.69473553\n",
      "Step: [9469] d_loss: 1.38500714, g_loss: 0.69345093\n",
      "Step: [9470] d_loss: 1.38580716, g_loss: 0.69326305\n",
      "Step: [9471] d_loss: 1.38565588, g_loss: 0.69632518\n",
      "Step: [9472] d_loss: 1.38638961, g_loss: 0.69404614\n",
      "Step: [9473] d_loss: 1.38537157, g_loss: 0.69471824\n",
      "Step: [9474] d_loss: 1.38586307, g_loss: 0.69358265\n",
      "Step: [9475] d_loss: 1.38561797, g_loss: 0.69587654\n",
      "Step: [9476] d_loss: 1.38491940, g_loss: 0.69583404\n",
      "Step: [9477] d_loss: 1.38484430, g_loss: 0.69213712\n",
      "Step: [9478] d_loss: 1.38587499, g_loss: 0.69456482\n",
      "Step: [9479] d_loss: 1.38621855, g_loss: 0.69464368\n",
      "Step: [9480] d_loss: 1.38651049, g_loss: 0.69568270\n",
      "Step: [9481] d_loss: 1.38715184, g_loss: 0.69119430\n",
      "Step: [9482] d_loss: 1.38559830, g_loss: 0.69563782\n",
      "Step: [9483] d_loss: 1.38600183, g_loss: 0.69696426\n",
      "Step: [9484] d_loss: 1.38742149, g_loss: 0.69226015\n",
      "Step: [9485] d_loss: 1.38502216, g_loss: 0.69357574\n",
      "Step: [9486] d_loss: 1.38710952, g_loss: 0.69324744\n",
      "Step: [9487] d_loss: 1.38566995, g_loss: 0.69284016\n",
      "Step: [9488] d_loss: 1.38539958, g_loss: 0.69409001\n",
      "Step: [9489] d_loss: 1.38849688, g_loss: 0.69191039\n",
      "Step: [9490] d_loss: 1.38769245, g_loss: 0.69498897\n",
      "Step: [9491] d_loss: 1.38678265, g_loss: 0.69420171\n",
      "Step: [9492] d_loss: 1.38663125, g_loss: 0.69144988\n",
      "Step: [9493] d_loss: 1.38429356, g_loss: 0.69683713\n",
      "Step: [9494] d_loss: 1.38712084, g_loss: 0.69583094\n",
      "Step: [9495] d_loss: 1.38815176, g_loss: 0.68799388\n",
      "Step: [9496] d_loss: 1.38678801, g_loss: 0.69118595\n",
      "Step: [9497] d_loss: 1.38660693, g_loss: 0.69153082\n",
      "Step: [9498] d_loss: 1.38648939, g_loss: 0.69338691\n",
      "Step: [9499] d_loss: 1.38562155, g_loss: 0.69449818\n",
      "Step: [9500] d_loss: 1.38586068, g_loss: 0.69455862\n",
      "Step: [9501] d_loss: 1.38780093, g_loss: 0.69332856\n",
      "Step: [9502] d_loss: 1.38474095, g_loss: 0.69581294\n",
      "Step: [9503] d_loss: 1.38628960, g_loss: 0.69322079\n",
      "Step: [9504] d_loss: 1.38699019, g_loss: 0.69092715\n",
      "Step: [9505] d_loss: 1.38526607, g_loss: 0.69397509\n",
      "Step: [9506] d_loss: 1.38640988, g_loss: 0.69473547\n",
      "Step: [9507] d_loss: 1.38497150, g_loss: 0.69568139\n",
      "Step: [9508] d_loss: 1.38557637, g_loss: 0.69402260\n",
      "Step: [9509] d_loss: 1.38632965, g_loss: 0.69356436\n",
      "Step: [9510] d_loss: 1.38850713, g_loss: 0.69087899\n",
      "Step: [9511] d_loss: 1.38472807, g_loss: 0.69550347\n",
      "Step: [9512] d_loss: 1.38655949, g_loss: 0.69550002\n",
      "Step: [9513] d_loss: 1.38699412, g_loss: 0.69085169\n",
      "Step: [9514] d_loss: 1.38741136, g_loss: 0.69484866\n",
      "Step: [9515] d_loss: 1.38632941, g_loss: 0.69180036\n",
      "Step: [9516] d_loss: 1.38539588, g_loss: 0.69576037\n",
      "Step: [9517] d_loss: 1.38713098, g_loss: 0.69322252\n",
      "Step: [9518] d_loss: 1.38689613, g_loss: 0.69431055\n",
      "Step: [9519] d_loss: 1.38744164, g_loss: 0.69540930\n",
      "Step: [9520] d_loss: 1.38672876, g_loss: 0.69366872\n",
      "Step: [9521] d_loss: 1.38512492, g_loss: 0.69514692\n",
      "Step: [9522] d_loss: 1.38697505, g_loss: 0.69265801\n",
      "Step: [9523] d_loss: 1.38737893, g_loss: 0.69356632\n",
      "Step: [9524] d_loss: 1.38640118, g_loss: 0.69357431\n",
      "Step: [9525] d_loss: 1.38590956, g_loss: 0.69316149\n",
      "Step: [9526] d_loss: 1.38740063, g_loss: 0.69485700\n",
      "Step: [9527] d_loss: 1.38716936, g_loss: 0.69572306\n",
      "Step: [9528] d_loss: 1.38631487, g_loss: 0.69401217\n",
      "Step: [9529] d_loss: 1.38508058, g_loss: 0.69455200\n",
      "Step: [9530] d_loss: 1.38700128, g_loss: 0.69341886\n",
      "Step: [9531] d_loss: 1.38639045, g_loss: 0.69438100\n",
      "Step: [9532] d_loss: 1.38616800, g_loss: 0.69325578\n",
      "Step: [9533] d_loss: 1.38680792, g_loss: 0.69225526\n",
      "Step: [9534] d_loss: 1.38717294, g_loss: 0.69307089\n",
      "Step: [9535] d_loss: 1.38677514, g_loss: 0.69614792\n",
      "Step: [9536] d_loss: 1.38611650, g_loss: 0.69316500\n",
      "Step: [9537] d_loss: 1.38681459, g_loss: 0.69312996\n",
      "Step: [9538] d_loss: 1.38577867, g_loss: 0.69540244\n",
      "Step: [9539] d_loss: 1.38698411, g_loss: 0.69152248\n",
      "Step: [9540] d_loss: 1.38573599, g_loss: 0.69444340\n",
      "Step: [9541] d_loss: 1.38646674, g_loss: 0.69807571\n",
      "Step: [9542] d_loss: 1.38628101, g_loss: 0.69398481\n",
      "Step: [9543] d_loss: 1.38559997, g_loss: 0.69813108\n",
      "Step: [9544] d_loss: 1.38634312, g_loss: 0.69323677\n",
      "Step: [9545] d_loss: 1.38588524, g_loss: 0.69356728\n",
      "Step: [9546] d_loss: 1.38529527, g_loss: 0.69567358\n",
      "Step: [9547] d_loss: 1.38581491, g_loss: 0.69253492\n",
      "Step: [9548] d_loss: 1.38657486, g_loss: 0.69987124\n",
      "Step: [9549] d_loss: 1.38674355, g_loss: 0.69555867\n",
      "Step: [9550] d_loss: 1.38570476, g_loss: 0.69689745\n",
      "Step: [9551] d_loss: 1.38482857, g_loss: 0.69472134\n",
      "Step: [9552] d_loss: 1.38472509, g_loss: 0.69257140\n",
      "Step: [9553] d_loss: 1.38642144, g_loss: 0.69386637\n",
      "Step: [9554] d_loss: 1.38581610, g_loss: 0.69409364\n",
      "Step: [9555] d_loss: 1.38532209, g_loss: 0.69465971\n",
      "Step: [9556] d_loss: 1.38544703, g_loss: 0.69408429\n",
      "Step: [9557] d_loss: 1.38600767, g_loss: 0.69290465\n",
      "Step: [9558] d_loss: 1.38589036, g_loss: 0.69384474\n",
      "Step: [9559] d_loss: 1.38668883, g_loss: 0.69552183\n",
      "Step: [9560] d_loss: 1.38680315, g_loss: 0.69420499\n",
      "Step: [9561] d_loss: 1.38617945, g_loss: 0.69347900\n",
      "Step: [9562] d_loss: 1.38527071, g_loss: 0.69599718\n",
      "Step: [9563] d_loss: 1.38502955, g_loss: 0.69554853\n",
      "Step: [9564] d_loss: 1.38481677, g_loss: 0.69359046\n",
      "Step: [9565] d_loss: 1.38573670, g_loss: 0.69524634\n",
      "Step: [9566] d_loss: 1.38560522, g_loss: 0.69375694\n",
      "Step: [9567] d_loss: 1.38518143, g_loss: 0.69800055\n",
      "Step: [9568] d_loss: 1.38832235, g_loss: 0.69077694\n",
      "Step: [9569] d_loss: 1.38592732, g_loss: 0.69808614\n",
      "Step: [9570] d_loss: 1.38674831, g_loss: 0.69597191\n",
      "Step: [9571] d_loss: 1.38656998, g_loss: 0.69244468\n",
      "Step: [9572] d_loss: 1.38638055, g_loss: 0.69449222\n",
      "Step: [9573] d_loss: 1.38598990, g_loss: 0.69039500\n",
      "Step: [9574] d_loss: 1.38646591, g_loss: 0.69131780\n",
      "Step: [9575] d_loss: 1.38586318, g_loss: 0.69176728\n",
      "Step: [9576] d_loss: 1.38675046, g_loss: 0.69540226\n",
      "Step: [9577] d_loss: 1.38609695, g_loss: 0.69279552\n",
      "Step: [9578] d_loss: 1.38759828, g_loss: 0.69256562\n",
      "Step: [9579] d_loss: 1.38597298, g_loss: 0.69465035\n",
      "Step: [9580] d_loss: 1.38417399, g_loss: 0.69562727\n",
      "Step: [9581] d_loss: 1.38545680, g_loss: 0.69315279\n",
      "Step: [9582] d_loss: 1.38651180, g_loss: 0.69444001\n",
      "Step: [9583] d_loss: 1.38704038, g_loss: 0.69289166\n",
      "Step: [9584] d_loss: 1.38508749, g_loss: 0.69547963\n",
      "Step: [9585] d_loss: 1.38753760, g_loss: 0.69372833\n",
      "Step: [9586] d_loss: 1.38562298, g_loss: 0.69398415\n",
      "Step: [9587] d_loss: 1.38546205, g_loss: 0.69314039\n",
      "Step: [9588] d_loss: 1.38578606, g_loss: 0.69274259\n",
      "Step: [9589] d_loss: 1.38764107, g_loss: 0.69505692\n",
      "Step: [9590] d_loss: 1.38485122, g_loss: 0.69653845\n",
      "Step: [9591] d_loss: 1.38519430, g_loss: 0.69351554\n",
      "Step: [9592] d_loss: 1.38567853, g_loss: 0.69666791\n",
      "Step: [9593] d_loss: 1.38831425, g_loss: 0.69162250\n",
      "Step: [9594] d_loss: 1.38597560, g_loss: 0.69553280\n",
      "Step: [9595] d_loss: 1.38590384, g_loss: 0.69460458\n",
      "Step: [9596] d_loss: 1.38660622, g_loss: 0.69353807\n",
      "Step: [9597] d_loss: 1.38594377, g_loss: 0.69159496\n",
      "Step: [9598] d_loss: 1.38654709, g_loss: 0.69351047\n",
      "Step: [9599] d_loss: 1.38741291, g_loss: 0.69641328\n",
      "Step: [9600] d_loss: 1.38659453, g_loss: 0.68968803\n",
      "Step: [9601] d_loss: 1.38596928, g_loss: 0.69265980\n",
      "Step: [9602] d_loss: 1.38588464, g_loss: 0.69137788\n",
      "Step: [9603] d_loss: 1.38558960, g_loss: 0.69355434\n",
      "Step: [9604] d_loss: 1.38641167, g_loss: 0.69510680\n",
      "Step: [9605] d_loss: 1.38529825, g_loss: 0.69489586\n",
      "Step: [9606] d_loss: 1.38595796, g_loss: 0.69509697\n",
      "Step: [9607] d_loss: 1.38625777, g_loss: 0.69259942\n",
      "Step: [9608] d_loss: 1.38656461, g_loss: 0.69347835\n",
      "Step: [9609] d_loss: 1.38513470, g_loss: 0.69863272\n",
      "Step: [9610] d_loss: 1.38778067, g_loss: 0.69232118\n",
      "Step: [9611] d_loss: 1.38637257, g_loss: 0.69419241\n",
      "Step: [9612] d_loss: 1.38644004, g_loss: 0.69225216\n",
      "Step: [9613] d_loss: 1.38697553, g_loss: 0.69340384\n",
      "Step: [9614] d_loss: 1.38608921, g_loss: 0.69711614\n",
      "Step: [9615] d_loss: 1.38576674, g_loss: 0.69191384\n",
      "Step: [9616] d_loss: 1.38591528, g_loss: 0.69615191\n",
      "Step: [9617] d_loss: 1.38713694, g_loss: 0.69196999\n",
      "Step: [9618] d_loss: 1.38726664, g_loss: 0.69410479\n",
      "Step: [9619] d_loss: 1.38574398, g_loss: 0.69614124\n",
      "Step: [9620] d_loss: 1.38525856, g_loss: 0.69425225\n",
      "Step: [9621] d_loss: 1.38778877, g_loss: 0.69269562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9622] d_loss: 1.38666010, g_loss: 0.69378489\n",
      "Step: [9623] d_loss: 1.38573647, g_loss: 0.69198906\n",
      "Step: [9624] d_loss: 1.38665688, g_loss: 0.69290948\n",
      "Step: [9625] d_loss: 1.38511515, g_loss: 0.69711745\n",
      "Step: [9626] d_loss: 1.38913560, g_loss: 0.69286287\n",
      "Step: [9627] d_loss: 1.38736784, g_loss: 0.69271767\n",
      "Step: [9628] d_loss: 1.38708568, g_loss: 0.69409752\n",
      "Step: [9629] d_loss: 1.38723743, g_loss: 0.69282258\n",
      "Step: [9630] d_loss: 1.38811135, g_loss: 0.69288784\n",
      "Step: [9631] d_loss: 1.38499808, g_loss: 0.69535822\n",
      "Step: [9632] d_loss: 1.38765037, g_loss: 0.69224739\n",
      "Step: [9633] d_loss: 1.38623071, g_loss: 0.69380492\n",
      "Step: [9634] d_loss: 1.38645983, g_loss: 0.69338256\n",
      "Step: [9635] d_loss: 1.38743329, g_loss: 0.69286120\n",
      "Step: [9636] d_loss: 1.38555360, g_loss: 0.69259453\n",
      "Step: [9637] d_loss: 1.38644135, g_loss: 0.69394040\n",
      "Step: [9638] d_loss: 1.38761330, g_loss: 0.69318771\n",
      "Step: [9639] d_loss: 1.38619971, g_loss: 0.69277108\n",
      "Step: [9640] d_loss: 1.38636923, g_loss: 0.69439375\n",
      "Step: [9641] d_loss: 1.38578558, g_loss: 0.69050109\n",
      "Step: [9642] d_loss: 1.38506222, g_loss: 0.69471085\n",
      "Step: [9643] d_loss: 1.38589311, g_loss: 0.69435817\n",
      "Step: [9644] d_loss: 1.38658476, g_loss: 0.69655347\n",
      "Step: [9645] d_loss: 1.38609707, g_loss: 0.69601268\n",
      "Step: [9646] d_loss: 1.38654983, g_loss: 0.69474816\n",
      "Step: [9647] d_loss: 1.38614392, g_loss: 0.69367397\n",
      "Step: [9648] d_loss: 1.38497603, g_loss: 0.69241071\n",
      "Step: [9649] d_loss: 1.38541651, g_loss: 0.69258678\n",
      "Step: [9650] d_loss: 1.38756835, g_loss: 0.69240963\n",
      "Step: [9651] d_loss: 1.38693810, g_loss: 0.69143307\n",
      "Step: [9652] d_loss: 1.38750505, g_loss: 0.69842470\n",
      "Step: [9653] d_loss: 1.38679373, g_loss: 0.69229728\n",
      "Step: [9654] d_loss: 1.38610029, g_loss: 0.69227165\n",
      "Step: [9655] d_loss: 1.38695860, g_loss: 0.69505543\n",
      "Step: [9656] d_loss: 1.38793135, g_loss: 0.69170088\n",
      "Step: [9657] d_loss: 1.38667679, g_loss: 0.69468355\n",
      "Step: [9658] d_loss: 1.38605928, g_loss: 0.69304723\n",
      "Step: [9659] d_loss: 1.38571656, g_loss: 0.69431996\n",
      "Step: [9660] d_loss: 1.38537955, g_loss: 0.69627064\n",
      "Step: [9661] d_loss: 1.38561368, g_loss: 0.69427228\n",
      "Step: [9662] d_loss: 1.38680840, g_loss: 0.69297558\n",
      "Step: [9663] d_loss: 1.38581014, g_loss: 0.69455153\n",
      "Step: [9664] d_loss: 1.38576150, g_loss: 0.69313264\n",
      "Step: [9665] d_loss: 1.38599527, g_loss: 0.69296801\n",
      "Step: [9666] d_loss: 1.38648319, g_loss: 0.69521356\n",
      "Step: [9667] d_loss: 1.38613486, g_loss: 0.69498736\n",
      "Step: [9668] d_loss: 1.38637745, g_loss: 0.69518816\n",
      "Step: [9669] d_loss: 1.38636649, g_loss: 0.69366527\n",
      "Step: [9670] d_loss: 1.38658214, g_loss: 0.69374830\n",
      "Step: [9671] d_loss: 1.38567102, g_loss: 0.69600034\n",
      "Step: [9672] d_loss: 1.38558745, g_loss: 0.69467211\n",
      "Step: [9673] d_loss: 1.38621855, g_loss: 0.69669330\n",
      "Step: [9674] d_loss: 1.38526654, g_loss: 0.69500762\n",
      "Step: [9675] d_loss: 1.38612401, g_loss: 0.69195783\n",
      "Step: [9676] d_loss: 1.38472462, g_loss: 0.69578254\n",
      "Step: [9677] d_loss: 1.38587737, g_loss: 0.69306028\n",
      "Step: [9678] d_loss: 1.38561189, g_loss: 0.69577664\n",
      "Step: [9679] d_loss: 1.38596642, g_loss: 0.69410253\n",
      "Step: [9680] d_loss: 1.38594484, g_loss: 0.69317335\n",
      "Step: [9681] d_loss: 1.38649285, g_loss: 0.69503212\n",
      "Step: [9682] d_loss: 1.38579619, g_loss: 0.69418144\n",
      "Step: [9683] d_loss: 1.38560629, g_loss: 0.69414341\n",
      "Step: [9684] d_loss: 1.38635230, g_loss: 0.69361490\n",
      "Step: [9685] d_loss: 1.38651168, g_loss: 0.69404238\n",
      "Step: [9686] d_loss: 1.38609374, g_loss: 0.69297290\n",
      "Step: [9687] d_loss: 1.38699806, g_loss: 0.69299346\n",
      "Step: [9688] d_loss: 1.38546884, g_loss: 0.69402736\n",
      "Step: [9689] d_loss: 1.38597083, g_loss: 0.69227147\n",
      "Step: [9690] d_loss: 1.38663173, g_loss: 0.69445598\n",
      "Step: [9691] d_loss: 1.38640976, g_loss: 0.69312024\n",
      "Step: [9692] d_loss: 1.38684821, g_loss: 0.69414920\n",
      "Step: [9693] d_loss: 1.38662195, g_loss: 0.69522274\n",
      "Step: [9694] d_loss: 1.38647795, g_loss: 0.69675183\n",
      "Step: [9695] d_loss: 1.38735628, g_loss: 0.69422102\n",
      "Step: [9696] d_loss: 1.38813591, g_loss: 0.69291055\n",
      "Step: [9697] d_loss: 1.38671410, g_loss: 0.69649470\n",
      "Step: [9698] d_loss: 1.38649416, g_loss: 0.69446182\n",
      "Step: [9699] d_loss: 1.38712406, g_loss: 0.69225383\n",
      "Step: [9700] d_loss: 1.38597023, g_loss: 0.69388509\n",
      "Step: [9701] d_loss: 1.38715994, g_loss: 0.69073331\n",
      "Step: [9702] d_loss: 1.38648534, g_loss: 0.69389081\n",
      "Step: [9703] d_loss: 1.38645756, g_loss: 0.69437706\n",
      "Step: [9704] d_loss: 1.38706422, g_loss: 0.69264245\n",
      "Step: [9705] d_loss: 1.38647795, g_loss: 0.69372022\n",
      "Step: [9706] d_loss: 1.38701391, g_loss: 0.69349706\n",
      "Step: [9707] d_loss: 1.38636577, g_loss: 0.69371003\n",
      "Step: [9708] d_loss: 1.38563967, g_loss: 0.69588280\n",
      "Step: [9709] d_loss: 1.38574338, g_loss: 0.69425225\n",
      "Step: [9710] d_loss: 1.38752294, g_loss: 0.69172800\n",
      "Step: [9711] d_loss: 1.38517725, g_loss: 0.69387925\n",
      "Step: [9712] d_loss: 1.38602543, g_loss: 0.69304627\n",
      "Step: [9713] d_loss: 1.38707197, g_loss: 0.69542181\n",
      "Step: [9714] d_loss: 1.38649988, g_loss: 0.69449377\n",
      "Step: [9715] d_loss: 1.38615131, g_loss: 0.69401336\n",
      "Step: [9716] d_loss: 1.38553143, g_loss: 0.69339526\n",
      "Step: [9717] d_loss: 1.38597798, g_loss: 0.69366217\n",
      "Step: [9718] d_loss: 1.38606501, g_loss: 0.69365746\n",
      "Step: [9719] d_loss: 1.38672042, g_loss: 0.69572353\n",
      "Step: [9720] d_loss: 1.38647115, g_loss: 0.69150931\n",
      "Step: [9721] d_loss: 1.38608670, g_loss: 0.69741428\n",
      "Step: [9722] d_loss: 1.38789093, g_loss: 0.68927389\n",
      "Step: [9723] d_loss: 1.38625586, g_loss: 0.69220769\n",
      "Step: [9724] d_loss: 1.38468027, g_loss: 0.69705600\n",
      "Step: [9725] d_loss: 1.38355827, g_loss: 0.69696152\n",
      "Step: [9726] d_loss: 1.38504100, g_loss: 0.69464636\n",
      "Step: [9727] d_loss: 1.38561499, g_loss: 0.69363952\n",
      "Step: [9728] d_loss: 1.38493371, g_loss: 0.69487154\n",
      "Step: [9729] d_loss: 1.38530838, g_loss: 0.69703454\n",
      "Step: [9730] d_loss: 1.38514709, g_loss: 0.69269460\n",
      "Step: [9731] d_loss: 1.38761997, g_loss: 0.69362330\n",
      "Step: [9732] d_loss: 1.38653612, g_loss: 0.69277698\n",
      "Step: [9733] d_loss: 1.38840222, g_loss: 0.69306624\n",
      "Step: [9734] d_loss: 1.38784647, g_loss: 0.69296944\n",
      "Step: [9735] d_loss: 1.38608503, g_loss: 0.69701743\n",
      "Step: [9736] d_loss: 1.38634562, g_loss: 0.69354463\n",
      "Step: [9737] d_loss: 1.39122844, g_loss: 0.69267571\n",
      "Step: [9738] d_loss: 1.38741469, g_loss: 0.69041538\n",
      "Step: [9739] d_loss: 1.38838816, g_loss: 0.69531000\n",
      "Step: [9740] d_loss: 1.38783121, g_loss: 0.69351363\n",
      "Step: [9741] d_loss: 1.38719976, g_loss: 0.69435728\n",
      "Step: [9742] d_loss: 1.38700628, g_loss: 0.69532055\n",
      "Step: [9743] d_loss: 1.38467300, g_loss: 0.69530714\n",
      "Step: [9744] d_loss: 1.38647807, g_loss: 0.69398129\n",
      "Step: [9745] d_loss: 1.38536501, g_loss: 0.69246852\n",
      "Step: [9746] d_loss: 1.38697982, g_loss: 0.69404793\n",
      "Step: [9747] d_loss: 1.38663173, g_loss: 0.69267672\n",
      "Step: [9748] d_loss: 1.38682342, g_loss: 0.69727898\n",
      "Step: [9749] d_loss: 1.38562310, g_loss: 0.69441593\n",
      "Step: [9750] d_loss: 1.38526046, g_loss: 0.69484210\n",
      "Step: [9751] d_loss: 1.38605738, g_loss: 0.69282877\n",
      "Step: [9752] d_loss: 1.38488364, g_loss: 0.69411540\n",
      "Step: [9753] d_loss: 1.38667512, g_loss: 0.69671160\n",
      "Step: [9754] d_loss: 1.38625956, g_loss: 0.69521654\n",
      "Step: [9755] d_loss: 1.38462663, g_loss: 0.69557047\n",
      "Step: [9756] d_loss: 1.38453889, g_loss: 0.69670188\n",
      "Step: [9757] d_loss: 1.38608348, g_loss: 0.69194150\n",
      "Step: [9758] d_loss: 1.38392150, g_loss: 0.69656265\n",
      "Step: [9759] d_loss: 1.38798857, g_loss: 0.69221687\n",
      "Step: [9760] d_loss: 1.38595366, g_loss: 0.69545889\n",
      "Step: [9761] d_loss: 1.38695729, g_loss: 0.69504547\n",
      "Step: [9762] d_loss: 1.38563585, g_loss: 0.69667065\n",
      "Step: [9763] d_loss: 1.38592291, g_loss: 0.69437301\n",
      "Step: [9764] d_loss: 1.38680983, g_loss: 0.69206977\n",
      "Step: [9765] d_loss: 1.38177717, g_loss: 0.69714797\n",
      "Step: [9766] d_loss: 1.38540792, g_loss: 0.69251078\n",
      "Step: [9767] d_loss: 1.38924873, g_loss: 0.69267857\n",
      "Step: [9768] d_loss: 1.38448620, g_loss: 0.69391811\n",
      "Step: [9769] d_loss: 1.38927805, g_loss: 0.69429040\n",
      "Step: [9770] d_loss: 1.38627577, g_loss: 0.69578743\n",
      "Step: [9771] d_loss: 1.38790393, g_loss: 0.69312447\n",
      "Step: [9772] d_loss: 1.38581848, g_loss: 0.69388115\n",
      "Step: [9773] d_loss: 1.38817525, g_loss: 0.69201505\n",
      "Step: [9774] d_loss: 1.38630700, g_loss: 0.69509864\n",
      "Step: [9775] d_loss: 1.38876295, g_loss: 0.69492489\n",
      "Step: [9776] d_loss: 1.38949180, g_loss: 0.69311285\n",
      "Step: [9777] d_loss: 1.38972998, g_loss: 0.69510794\n",
      "Step: [9778] d_loss: 1.38687384, g_loss: 0.69517136\n",
      "Step: [9779] d_loss: 1.38717914, g_loss: 0.69465810\n",
      "Step: [9780] d_loss: 1.38475442, g_loss: 0.69497448\n",
      "Step: [9781] d_loss: 1.38535130, g_loss: 0.69309443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9782] d_loss: 1.38711667, g_loss: 0.69230890\n",
      "Step: [9783] d_loss: 1.38582778, g_loss: 0.69479716\n",
      "Step: [9784] d_loss: 1.38559580, g_loss: 0.69606400\n",
      "Step: [9785] d_loss: 1.38547480, g_loss: 0.69395161\n",
      "Step: [9786] d_loss: 1.38605285, g_loss: 0.69554114\n",
      "Step: [9787] d_loss: 1.38597822, g_loss: 0.69027865\n",
      "Step: [9788] d_loss: 1.38776135, g_loss: 0.69744289\n",
      "Step: [9789] d_loss: 1.38709474, g_loss: 0.69384497\n",
      "Step: [9790] d_loss: 1.38751793, g_loss: 0.69437820\n",
      "Step: [9791] d_loss: 1.38669586, g_loss: 0.69329119\n",
      "Step: [9792] d_loss: 1.38636684, g_loss: 0.69368339\n",
      "Step: [9793] d_loss: 1.38616264, g_loss: 0.69285953\n",
      "Step: [9794] d_loss: 1.38641226, g_loss: 0.69553858\n",
      "Step: [9795] d_loss: 1.38723111, g_loss: 0.69252706\n",
      "Step: [9796] d_loss: 1.38780904, g_loss: 0.69407851\n",
      "Step: [9797] d_loss: 1.38843548, g_loss: 0.69377029\n",
      "Step: [9798] d_loss: 1.38579345, g_loss: 0.69684958\n",
      "Step: [9799] d_loss: 1.38713586, g_loss: 0.69378787\n",
      "Step: [9800] d_loss: 1.38463199, g_loss: 0.69457787\n",
      "Step: [9801] d_loss: 1.38627565, g_loss: 0.69561839\n",
      "Step: [9802] d_loss: 1.38578045, g_loss: 0.69305199\n",
      "Step: [9803] d_loss: 1.38541675, g_loss: 0.69661856\n",
      "Step: [9804] d_loss: 1.38683152, g_loss: 0.69297326\n",
      "Step: [9805] d_loss: 1.38524675, g_loss: 0.69442564\n",
      "Step: [9806] d_loss: 1.38407564, g_loss: 0.69557953\n",
      "Step: [9807] d_loss: 1.38497531, g_loss: 0.69244158\n",
      "Step: [9808] d_loss: 1.38540876, g_loss: 0.69465160\n",
      "Step: [9809] d_loss: 1.38485801, g_loss: 0.69399309\n",
      "Step: [9810] d_loss: 1.38338304, g_loss: 0.69369328\n",
      "Step: [9811] d_loss: 1.38372970, g_loss: 0.69791079\n",
      "Step: [9812] d_loss: 1.38395238, g_loss: 0.69455051\n",
      "Step: [9813] d_loss: 1.38694906, g_loss: 0.69575727\n",
      "Step: [9814] d_loss: 1.38547325, g_loss: 0.69568372\n",
      "Step: [9815] d_loss: 1.38607812, g_loss: 0.69582772\n",
      "Step: [9816] d_loss: 1.38598609, g_loss: 0.69319761\n",
      "Step: [9817] d_loss: 1.38836753, g_loss: 0.69329441\n",
      "Step: [9818] d_loss: 1.39006257, g_loss: 0.69266832\n",
      "Step: [9819] d_loss: 1.38827610, g_loss: 0.70065546\n",
      "Step: [9820] d_loss: 1.38886166, g_loss: 0.69203722\n",
      "Step: [9821] d_loss: 1.38875413, g_loss: 0.69284821\n",
      "Step: [9822] d_loss: 1.38692737, g_loss: 0.69288325\n",
      "Step: [9823] d_loss: 1.38735378, g_loss: 0.69479519\n",
      "Step: [9824] d_loss: 1.38479900, g_loss: 0.69483602\n",
      "Step: [9825] d_loss: 1.38686311, g_loss: 0.69501066\n",
      "Step: [9826] d_loss: 1.38588703, g_loss: 0.69509411\n",
      "Step: [9827] d_loss: 1.38579559, g_loss: 0.69204724\n",
      "Step: [9828] d_loss: 1.38330817, g_loss: 0.69440347\n",
      "Step: [9829] d_loss: 1.38637054, g_loss: 0.69667757\n",
      "Step: [9830] d_loss: 1.38673186, g_loss: 0.69375503\n",
      "Step: [9831] d_loss: 1.38615751, g_loss: 0.69527525\n",
      "Step: [9832] d_loss: 1.38775277, g_loss: 0.68992174\n",
      "Step: [9833] d_loss: 1.38728523, g_loss: 0.69604051\n",
      "Step: [9834] d_loss: 1.38714552, g_loss: 0.69422245\n",
      "Step: [9835] d_loss: 1.38420999, g_loss: 0.69398570\n",
      "Step: [9836] d_loss: 1.38568234, g_loss: 0.69625437\n",
      "Step: [9837] d_loss: 1.38632727, g_loss: 0.69418180\n",
      "Step: [9838] d_loss: 1.38718867, g_loss: 0.69547296\n",
      "Step: [9839] d_loss: 1.38716459, g_loss: 0.69279331\n",
      "Step: [9840] d_loss: 1.38492024, g_loss: 0.69322693\n",
      "Step: [9841] d_loss: 1.38735247, g_loss: 0.69236821\n",
      "Step: [9842] d_loss: 1.38646150, g_loss: 0.69346488\n",
      "Step: [9843] d_loss: 1.38691640, g_loss: 0.69426596\n",
      "Step: [9844] d_loss: 1.38605857, g_loss: 0.69391489\n",
      "Step: [9845] d_loss: 1.38610458, g_loss: 0.69591284\n",
      "Step: [9846] d_loss: 1.38518763, g_loss: 0.69537765\n",
      "Step: [9847] d_loss: 1.38482618, g_loss: 0.69545317\n",
      "Step: [9848] d_loss: 1.38623142, g_loss: 0.69170678\n",
      "Step: [9849] d_loss: 1.38583684, g_loss: 0.69393659\n",
      "Step: [9850] d_loss: 1.38500261, g_loss: 0.69427365\n",
      "Step: [9851] d_loss: 1.38495278, g_loss: 0.69750124\n",
      "Step: [9852] d_loss: 1.38717949, g_loss: 0.69662642\n",
      "Step: [9853] d_loss: 1.38663208, g_loss: 0.69331038\n",
      "Step: [9854] d_loss: 1.38685763, g_loss: 0.69397593\n",
      "Step: [9855] d_loss: 1.38650727, g_loss: 0.69170064\n",
      "Step: [9856] d_loss: 1.38696253, g_loss: 0.69429058\n",
      "Step: [9857] d_loss: 1.38618159, g_loss: 0.69643039\n",
      "Step: [9858] d_loss: 1.38456392, g_loss: 0.69808114\n",
      "Step: [9859] d_loss: 1.38806915, g_loss: 0.69333196\n",
      "Step: [9860] d_loss: 1.38531470, g_loss: 0.69887376\n",
      "Step: [9861] d_loss: 1.38704705, g_loss: 0.69110739\n",
      "Step: [9862] d_loss: 1.38698673, g_loss: 0.69564027\n",
      "Step: [9863] d_loss: 1.38601589, g_loss: 0.69366109\n",
      "Step: [9864] d_loss: 1.38623965, g_loss: 0.69284201\n",
      "Step: [9865] d_loss: 1.38299167, g_loss: 0.69714946\n",
      "Step: [9866] d_loss: 1.38729608, g_loss: 0.69126946\n",
      "Step: [9867] d_loss: 1.38840032, g_loss: 0.69793594\n",
      "Step: [9868] d_loss: 1.38817596, g_loss: 0.69345254\n",
      "Step: [9869] d_loss: 1.38794935, g_loss: 0.69439065\n",
      "Step: [9870] d_loss: 1.38807881, g_loss: 0.69386244\n",
      "Step: [9871] d_loss: 1.38677609, g_loss: 0.69818294\n",
      "Step: [9872] d_loss: 1.39002037, g_loss: 0.69345069\n",
      "Step: [9873] d_loss: 1.38956285, g_loss: 0.69240052\n",
      "Step: [9874] d_loss: 1.38800156, g_loss: 0.69070840\n",
      "Step: [9875] d_loss: 1.38821495, g_loss: 0.69195807\n",
      "Step: [9876] d_loss: 1.38602948, g_loss: 0.69356781\n",
      "Step: [9877] d_loss: 1.38571024, g_loss: 0.69634891\n",
      "Step: [9878] d_loss: 1.38569558, g_loss: 0.69321465\n",
      "Step: [9879] d_loss: 1.38637972, g_loss: 0.69724578\n",
      "Step: [9880] d_loss: 1.38416910, g_loss: 0.69290912\n",
      "Step: [9881] d_loss: 1.38485980, g_loss: 0.69851607\n",
      "Step: [9882] d_loss: 1.38631904, g_loss: 0.69258976\n",
      "Step: [9883] d_loss: 1.38496745, g_loss: 0.69361365\n",
      "Step: [9884] d_loss: 1.38530135, g_loss: 0.70001042\n",
      "Step: [9885] d_loss: 1.38370419, g_loss: 0.69685060\n",
      "Step: [9886] d_loss: 1.38422608, g_loss: 0.69642198\n",
      "Step: [9887] d_loss: 1.38590479, g_loss: 0.69016373\n",
      "Step: [9888] d_loss: 1.38521624, g_loss: 0.70161128\n",
      "Step: [9889] d_loss: 1.38725710, g_loss: 0.69496453\n",
      "Step: [9890] d_loss: 1.38732600, g_loss: 0.69399422\n",
      "Step: [9891] d_loss: 1.38501132, g_loss: 0.69422972\n",
      "Step: [9892] d_loss: 1.38669455, g_loss: 0.69236410\n",
      "Step: [9893] d_loss: 1.38869691, g_loss: 0.69356835\n",
      "Step: [9894] d_loss: 1.38574982, g_loss: 0.69693363\n",
      "Step: [9895] d_loss: 1.38549852, g_loss: 0.69674951\n",
      "Step: [9896] d_loss: 1.38631082, g_loss: 0.69230145\n",
      "Step: [9897] d_loss: 1.39004993, g_loss: 0.69561589\n",
      "Step: [9898] d_loss: 1.38810933, g_loss: 0.69508481\n",
      "Step: [9899] d_loss: 1.38739538, g_loss: 0.69415873\n",
      "Step: [9900] d_loss: 1.38916719, g_loss: 0.69311488\n",
      "Step: [9901] d_loss: 1.38784206, g_loss: 0.69337070\n",
      "Step: [9902] d_loss: 1.38599348, g_loss: 0.69395173\n",
      "Step: [9903] d_loss: 1.38676310, g_loss: 0.69431186\n",
      "Step: [9904] d_loss: 1.38637412, g_loss: 0.69527507\n",
      "Step: [9905] d_loss: 1.38701916, g_loss: 0.69374561\n",
      "Step: [9906] d_loss: 1.38572288, g_loss: 0.69433558\n",
      "Step: [9907] d_loss: 1.38674307, g_loss: 0.69626427\n",
      "Step: [9908] d_loss: 1.38585448, g_loss: 0.69330090\n",
      "Step: [9909] d_loss: 1.38679147, g_loss: 0.69568777\n",
      "Step: [9910] d_loss: 1.38600397, g_loss: 0.69249904\n",
      "Step: [9911] d_loss: 1.38665557, g_loss: 0.69462752\n",
      "Step: [9912] d_loss: 1.38546157, g_loss: 0.69544876\n",
      "Step: [9913] d_loss: 1.38491118, g_loss: 0.69519186\n",
      "Step: [9914] d_loss: 1.38494205, g_loss: 0.69526589\n",
      "Step: [9915] d_loss: 1.38573265, g_loss: 0.69137919\n",
      "Step: [9916] d_loss: 1.38738871, g_loss: 0.69203770\n",
      "Step: [9917] d_loss: 1.38622904, g_loss: 0.69370043\n",
      "Step: [9918] d_loss: 1.38466263, g_loss: 0.69535911\n",
      "Step: [9919] d_loss: 1.38492417, g_loss: 0.69697136\n",
      "Step: [9920] d_loss: 1.38563979, g_loss: 0.69357431\n",
      "Step: [9921] d_loss: 1.38733864, g_loss: 0.69470423\n",
      "Step: [9922] d_loss: 1.38777089, g_loss: 0.69385564\n",
      "Step: [9923] d_loss: 1.38534510, g_loss: 0.69350886\n",
      "Step: [9924] d_loss: 1.38670743, g_loss: 0.69295967\n",
      "Step: [9925] d_loss: 1.38600588, g_loss: 0.69541395\n",
      "Step: [9926] d_loss: 1.38573599, g_loss: 0.69317555\n",
      "Step: [9927] d_loss: 1.38472724, g_loss: 0.69414485\n",
      "Step: [9928] d_loss: 1.38622427, g_loss: 0.69616467\n",
      "Step: [9929] d_loss: 1.38616347, g_loss: 0.69398457\n",
      "Step: [9930] d_loss: 1.38695252, g_loss: 0.69369519\n",
      "Step: [9931] d_loss: 1.38627887, g_loss: 0.69487232\n",
      "Step: [9932] d_loss: 1.38552439, g_loss: 0.69209081\n",
      "Step: [9933] d_loss: 1.38441825, g_loss: 0.69517350\n",
      "Step: [9934] d_loss: 1.38427925, g_loss: 0.69738352\n",
      "Step: [9935] d_loss: 1.38557947, g_loss: 0.69467008\n",
      "Step: [9936] d_loss: 1.38586354, g_loss: 0.69676781\n",
      "Step: [9937] d_loss: 1.38601208, g_loss: 0.69172597\n",
      "Step: [9938] d_loss: 1.38564301, g_loss: 0.69642818\n",
      "Step: [9939] d_loss: 1.38555920, g_loss: 0.69268477\n",
      "Step: [9940] d_loss: 1.38705933, g_loss: 0.69361758\n",
      "Step: [9941] d_loss: 1.38529098, g_loss: 0.69451809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [9942] d_loss: 1.38542175, g_loss: 0.69442534\n",
      "Step: [9943] d_loss: 1.38595545, g_loss: 0.69427538\n",
      "Step: [9944] d_loss: 1.38651419, g_loss: 0.69178319\n",
      "Step: [9945] d_loss: 1.38592398, g_loss: 0.69398701\n",
      "Step: [9946] d_loss: 1.38736582, g_loss: 0.69550222\n",
      "Step: [9947] d_loss: 1.38558733, g_loss: 0.69554162\n",
      "Step: [9948] d_loss: 1.38670969, g_loss: 0.69658029\n",
      "Step: [9949] d_loss: 1.38739145, g_loss: 0.69025779\n",
      "Step: [9950] d_loss: 1.38720405, g_loss: 0.69633210\n",
      "Step: [9951] d_loss: 1.38840342, g_loss: 0.68941236\n",
      "Step: [9952] d_loss: 1.38841772, g_loss: 0.69582105\n",
      "Step: [9953] d_loss: 1.38866627, g_loss: 0.69016737\n",
      "Step: [9954] d_loss: 1.38664246, g_loss: 0.69241750\n",
      "Step: [9955] d_loss: 1.38812268, g_loss: 0.69489825\n",
      "Step: [9956] d_loss: 1.38793612, g_loss: 0.69152200\n",
      "Step: [9957] d_loss: 1.38799214, g_loss: 0.69431943\n",
      "Step: [9958] d_loss: 1.38728273, g_loss: 0.69530278\n",
      "Step: [9959] d_loss: 1.38621712, g_loss: 0.69400930\n",
      "Step: [9960] d_loss: 1.38748312, g_loss: 0.69251752\n",
      "Step: [9961] d_loss: 1.38737893, g_loss: 0.69145620\n",
      "Step: [9962] d_loss: 1.38590837, g_loss: 0.69200957\n",
      "Step: [9963] d_loss: 1.38593006, g_loss: 0.69316077\n",
      "Step: [9964] d_loss: 1.38538122, g_loss: 0.69622689\n",
      "Step: [9965] d_loss: 1.38700032, g_loss: 0.69186914\n",
      "Step: [9966] d_loss: 1.38628447, g_loss: 0.69444346\n",
      "Step: [9967] d_loss: 1.38671756, g_loss: 0.69592834\n",
      "Step: [9968] d_loss: 1.38639617, g_loss: 0.69265413\n",
      "Step: [9969] d_loss: 1.38528645, g_loss: 0.69246149\n",
      "Step: [9970] d_loss: 1.38608599, g_loss: 0.69320500\n",
      "Step: [9971] d_loss: 1.38523793, g_loss: 0.69478238\n",
      "Step: [9972] d_loss: 1.38503981, g_loss: 0.69303262\n",
      "Step: [9973] d_loss: 1.38609433, g_loss: 0.69428706\n",
      "Step: [9974] d_loss: 1.38428760, g_loss: 0.69429952\n",
      "Step: [9975] d_loss: 1.38577008, g_loss: 0.69337261\n",
      "Step: [9976] d_loss: 1.38506806, g_loss: 0.69638854\n",
      "Step: [9977] d_loss: 1.38554215, g_loss: 0.69530058\n",
      "Step: [9978] d_loss: 1.38636065, g_loss: 0.69230306\n",
      "Step: [9979] d_loss: 1.38627815, g_loss: 0.69298160\n",
      "Step: [9980] d_loss: 1.38459921, g_loss: 0.69241196\n",
      "Step: [9981] d_loss: 1.38528395, g_loss: 0.69399214\n",
      "Step: [9982] d_loss: 1.38603723, g_loss: 0.69593537\n",
      "Step: [9983] d_loss: 1.38500142, g_loss: 0.69344842\n",
      "Step: [9984] d_loss: 1.38847446, g_loss: 0.69381207\n",
      "Step: [9985] d_loss: 1.38580704, g_loss: 0.69276118\n",
      "Step: [9986] d_loss: 1.38505721, g_loss: 0.69299662\n",
      "Step: [9987] d_loss: 1.38586307, g_loss: 0.69474220\n",
      "Step: [9988] d_loss: 1.38493860, g_loss: 0.69467443\n",
      "Step: [9989] d_loss: 1.38723564, g_loss: 0.69538778\n",
      "Step: [9990] d_loss: 1.38597345, g_loss: 0.69510877\n",
      "Step: [9991] d_loss: 1.38720393, g_loss: 0.69558656\n",
      "Step: [9992] d_loss: 1.38710737, g_loss: 0.69205821\n",
      "Step: [9993] d_loss: 1.38678956, g_loss: 0.69400096\n",
      "Step: [9994] d_loss: 1.38717842, g_loss: 0.69307435\n",
      "Step: [9995] d_loss: 1.38760138, g_loss: 0.69053316\n",
      "Step: [9996] d_loss: 1.38649476, g_loss: 0.69543719\n",
      "Step: [9997] d_loss: 1.38780332, g_loss: 0.69255888\n",
      "Step: [9998] d_loss: 1.38693762, g_loss: 0.69883382\n",
      "Step: [9999] d_loss: 1.38816011, g_loss: 0.69372201\n",
      "Step: [10000] d_loss: 1.38637364, g_loss: 0.69535518\n",
      "Step: [10001] d_loss: 1.38743305, g_loss: 0.69588703\n",
      "Step: [10002] d_loss: 1.38529634, g_loss: 0.69258666\n",
      "Step: [10003] d_loss: 1.38770533, g_loss: 0.69173229\n",
      "Step: [10004] d_loss: 1.38551438, g_loss: 0.69670600\n",
      "Step: [10005] d_loss: 1.38649106, g_loss: 0.69487768\n",
      "Step: [10006] d_loss: 1.38589144, g_loss: 0.69202864\n",
      "Step: [10007] d_loss: 1.38736057, g_loss: 0.69386268\n",
      "Step: [10008] d_loss: 1.38627911, g_loss: 0.69471025\n",
      "Step: [10009] d_loss: 1.38678586, g_loss: 0.69421291\n",
      "Step: [10010] d_loss: 1.38578737, g_loss: 0.69443560\n",
      "Step: [10011] d_loss: 1.38657582, g_loss: 0.69409430\n",
      "Step: [10012] d_loss: 1.38636661, g_loss: 0.69309205\n",
      "Step: [10013] d_loss: 1.38527882, g_loss: 0.69532037\n",
      "Step: [10014] d_loss: 1.38498807, g_loss: 0.69622195\n",
      "Step: [10015] d_loss: 1.38562346, g_loss: 0.69541657\n",
      "Step: [10016] d_loss: 1.38573599, g_loss: 0.69424391\n",
      "Step: [10017] d_loss: 1.38644135, g_loss: 0.69300705\n",
      "Step: [10018] d_loss: 1.38568211, g_loss: 0.69761050\n",
      "Step: [10019] d_loss: 1.38594115, g_loss: 0.69229841\n",
      "Step: [10020] d_loss: 1.38607943, g_loss: 0.69725174\n",
      "Step: [10021] d_loss: 1.38681960, g_loss: 0.69055104\n",
      "Step: [10022] d_loss: 1.38490582, g_loss: 0.69396931\n",
      "Step: [10023] d_loss: 1.38518286, g_loss: 0.69100052\n",
      "Step: [10024] d_loss: 1.38556111, g_loss: 0.69226772\n",
      "Step: [10025] d_loss: 1.38535213, g_loss: 0.69707072\n",
      "Step: [10026] d_loss: 1.38566828, g_loss: 0.69586474\n",
      "Step: [10027] d_loss: 1.38565648, g_loss: 0.69538569\n",
      "Step: [10028] d_loss: 1.38449693, g_loss: 0.69402516\n",
      "Step: [10029] d_loss: 1.38592625, g_loss: 0.69378400\n",
      "Step: [10030] d_loss: 1.38458252, g_loss: 0.69357908\n",
      "Step: [10031] d_loss: 1.38701463, g_loss: 0.69372332\n",
      "Step: [10032] d_loss: 1.38747811, g_loss: 0.69404501\n",
      "Step: [10033] d_loss: 1.38619387, g_loss: 0.69506836\n",
      "Step: [10034] d_loss: 1.38630486, g_loss: 0.69212818\n",
      "Step: [10035] d_loss: 1.38598359, g_loss: 0.69319391\n",
      "Step: [10036] d_loss: 1.38672423, g_loss: 0.69637322\n",
      "Step: [10037] d_loss: 1.38664019, g_loss: 0.69291711\n",
      "Step: [10038] d_loss: 1.38444781, g_loss: 0.69436753\n",
      "Step: [10039] d_loss: 1.38613749, g_loss: 0.69357741\n",
      "Step: [10040] d_loss: 1.38575196, g_loss: 0.69430721\n",
      "Step: [10041] d_loss: 1.38640392, g_loss: 0.69198537\n",
      "Step: [10042] d_loss: 1.38664150, g_loss: 0.69187582\n",
      "Step: [10043] d_loss: 1.38734126, g_loss: 0.69309032\n",
      "Step: [10044] d_loss: 1.38548362, g_loss: 0.69695199\n",
      "Step: [10045] d_loss: 1.38662505, g_loss: 0.69508624\n",
      "Step: [10046] d_loss: 1.38702464, g_loss: 0.69318819\n",
      "Step: [10047] d_loss: 1.38860428, g_loss: 0.69213617\n",
      "Step: [10048] d_loss: 1.38621402, g_loss: 0.69301462\n",
      "Step: [10049] d_loss: 1.38699138, g_loss: 0.69356060\n",
      "Step: [10050] d_loss: 1.38616228, g_loss: 0.69319385\n",
      "Step: [10051] d_loss: 1.38602304, g_loss: 0.69419098\n",
      "Step: [10052] d_loss: 1.38605261, g_loss: 0.69265091\n",
      "Step: [10053] d_loss: 1.38623726, g_loss: 0.69381487\n",
      "Step: [10054] d_loss: 1.38578176, g_loss: 0.69416845\n",
      "Step: [10055] d_loss: 1.38713884, g_loss: 0.69397008\n",
      "Step: [10056] d_loss: 1.38764930, g_loss: 0.69233352\n",
      "Step: [10057] d_loss: 1.38680100, g_loss: 0.69087183\n",
      "Step: [10058] d_loss: 1.38559556, g_loss: 0.69299281\n",
      "Step: [10059] d_loss: 1.38643003, g_loss: 0.69543338\n",
      "Step: [10060] d_loss: 1.38653481, g_loss: 0.69553375\n",
      "Step: [10061] d_loss: 1.38587797, g_loss: 0.69440794\n",
      "Step: [10062] d_loss: 1.38652945, g_loss: 0.69263935\n",
      "Step: [10063] d_loss: 1.38691652, g_loss: 0.69294500\n",
      "Step: [10064] d_loss: 1.38658166, g_loss: 0.69541234\n",
      "Step: [10065] d_loss: 1.38553262, g_loss: 0.69241786\n",
      "Step: [10066] d_loss: 1.38494563, g_loss: 0.69444621\n",
      "Step: [10067] d_loss: 1.38442457, g_loss: 0.69268435\n",
      "Step: [10068] d_loss: 1.38605738, g_loss: 0.69374204\n",
      "Step: [10069] d_loss: 1.38505828, g_loss: 0.69388628\n",
      "Step: [10070] d_loss: 1.38677311, g_loss: 0.69170511\n",
      "Step: [10071] d_loss: 1.38567042, g_loss: 0.69506460\n",
      "Step: [10072] d_loss: 1.38689399, g_loss: 0.69403738\n",
      "Step: [10073] d_loss: 1.38617921, g_loss: 0.69410282\n",
      "Step: [10074] d_loss: 1.38612986, g_loss: 0.69336385\n",
      "Step: [10075] d_loss: 1.38664508, g_loss: 0.69311917\n",
      "Step: [10076] d_loss: 1.38500726, g_loss: 0.69565791\n",
      "Step: [10077] d_loss: 1.38580561, g_loss: 0.69441164\n",
      "Step: [10078] d_loss: 1.38578510, g_loss: 0.69421697\n",
      "Step: [10079] d_loss: 1.38703275, g_loss: 0.69602740\n",
      "Step: [10080] d_loss: 1.38519359, g_loss: 0.69330812\n",
      "Step: [10081] d_loss: 1.38569605, g_loss: 0.69502735\n",
      "Step: [10082] d_loss: 1.38555336, g_loss: 0.69437766\n",
      "Step: [10083] d_loss: 1.38583899, g_loss: 0.69543779\n",
      "Step: [10084] d_loss: 1.38461065, g_loss: 0.69247437\n",
      "Step: [10085] d_loss: 1.38699341, g_loss: 0.69388533\n",
      "Step: [10086] d_loss: 1.38581038, g_loss: 0.69562542\n",
      "Step: [10087] d_loss: 1.38653505, g_loss: 0.69329524\n",
      "Step: [10088] d_loss: 1.38802099, g_loss: 0.69271743\n",
      "Step: [10089] d_loss: 1.38629103, g_loss: 0.69188935\n",
      "Step: [10090] d_loss: 1.38638353, g_loss: 0.69115388\n",
      "Step: [10091] d_loss: 1.38528752, g_loss: 0.69393563\n",
      "Step: [10092] d_loss: 1.38496542, g_loss: 0.69632304\n",
      "Step: [10093] d_loss: 1.38548017, g_loss: 0.69675517\n",
      "Step: [10094] d_loss: 1.38585877, g_loss: 0.69450343\n",
      "Step: [10095] d_loss: 1.38613284, g_loss: 0.69411457\n",
      "Step: [10096] d_loss: 1.38486481, g_loss: 0.69253492\n",
      "Step: [10097] d_loss: 1.38485456, g_loss: 0.69395506\n",
      "Step: [10098] d_loss: 1.38612223, g_loss: 0.69446206\n",
      "Step: [10099] d_loss: 1.38583446, g_loss: 0.69534326\n",
      "Step: [10100] d_loss: 1.38664365, g_loss: 0.69180024\n",
      "Step: [10101] d_loss: 1.38601184, g_loss: 0.69403994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10102] d_loss: 1.38640261, g_loss: 0.69154787\n",
      "Step: [10103] d_loss: 1.38566399, g_loss: 0.69590795\n",
      "Step: [10104] d_loss: 1.38648713, g_loss: 0.69423705\n",
      "Step: [10105] d_loss: 1.38533330, g_loss: 0.69379413\n",
      "Step: [10106] d_loss: 1.38585794, g_loss: 0.69420183\n",
      "Step: [10107] d_loss: 1.38596511, g_loss: 0.69253576\n",
      "Step: [10108] d_loss: 1.38602829, g_loss: 0.69444573\n",
      "Step: [10109] d_loss: 1.38576245, g_loss: 0.69380486\n",
      "Step: [10110] d_loss: 1.38697803, g_loss: 0.69438100\n",
      "Step: [10111] d_loss: 1.38567710, g_loss: 0.69504321\n",
      "Step: [10112] d_loss: 1.38664126, g_loss: 0.69389820\n",
      "Step: [10113] d_loss: 1.38437223, g_loss: 0.69491625\n",
      "Step: [10114] d_loss: 1.38660073, g_loss: 0.68995202\n",
      "Step: [10115] d_loss: 1.38656163, g_loss: 0.69583225\n",
      "Step: [10116] d_loss: 1.38567281, g_loss: 0.69125855\n",
      "Step: [10117] d_loss: 1.38603997, g_loss: 0.69497645\n",
      "Step: [10118] d_loss: 1.38442183, g_loss: 0.69500107\n",
      "Step: [10119] d_loss: 1.38627362, g_loss: 0.69215071\n",
      "Step: [10120] d_loss: 1.38633513, g_loss: 0.69391519\n",
      "Step: [10121] d_loss: 1.38611460, g_loss: 0.69263351\n",
      "Step: [10122] d_loss: 1.38640881, g_loss: 0.69682729\n",
      "Step: [10123] d_loss: 1.38665974, g_loss: 0.69299042\n",
      "Step: [10124] d_loss: 1.38658500, g_loss: 0.69141102\n",
      "Step: [10125] d_loss: 1.38471413, g_loss: 0.69490296\n",
      "Step: [10126] d_loss: 1.38690293, g_loss: 0.69168735\n",
      "Step: [10127] d_loss: 1.38779163, g_loss: 0.69543064\n",
      "Step: [10128] d_loss: 1.38656402, g_loss: 0.69450337\n",
      "Step: [10129] d_loss: 1.38688564, g_loss: 0.69396645\n",
      "Step: [10130] d_loss: 1.38595510, g_loss: 0.69320381\n",
      "Step: [10131] d_loss: 1.38592529, g_loss: 0.69470096\n",
      "Step: [10132] d_loss: 1.38679028, g_loss: 0.69170058\n",
      "Step: [10133] d_loss: 1.38611746, g_loss: 0.69745553\n",
      "Step: [10134] d_loss: 1.38753080, g_loss: 0.69355410\n",
      "Step: [10135] d_loss: 1.38488412, g_loss: 0.69505477\n",
      "Step: [10136] d_loss: 1.38599730, g_loss: 0.69351745\n",
      "Step: [10137] d_loss: 1.38571048, g_loss: 0.69418424\n",
      "Step: [10138] d_loss: 1.38626802, g_loss: 0.69373506\n",
      "Step: [10139] d_loss: 1.38508391, g_loss: 0.69404513\n",
      "Step: [10140] d_loss: 1.38488865, g_loss: 0.69394583\n",
      "Step: [10141] d_loss: 1.38372457, g_loss: 0.69782317\n",
      "Step: [10142] d_loss: 1.38687682, g_loss: 0.69055676\n",
      "Step: [10143] d_loss: 1.38621855, g_loss: 0.69856262\n",
      "Step: [10144] d_loss: 1.38615680, g_loss: 0.69169796\n",
      "Step: [10145] d_loss: 1.38607216, g_loss: 0.69445741\n",
      "Step: [10146] d_loss: 1.38555074, g_loss: 0.69386655\n",
      "Step: [10147] d_loss: 1.38370597, g_loss: 0.69562405\n",
      "Step: [10148] d_loss: 1.38469887, g_loss: 0.69606125\n",
      "Step: [10149] d_loss: 1.38606453, g_loss: 0.69474936\n",
      "Step: [10150] d_loss: 1.38522911, g_loss: 0.69612896\n",
      "Step: [10151] d_loss: 1.38577819, g_loss: 0.69299382\n",
      "Step: [10152] d_loss: 1.38705683, g_loss: 0.69474733\n",
      "Step: [10153] d_loss: 1.38732934, g_loss: 0.69016826\n",
      "Step: [10154] d_loss: 1.38873184, g_loss: 0.69315600\n",
      "Step: [10155] d_loss: 1.38654542, g_loss: 0.69357657\n",
      "Step: [10156] d_loss: 1.38795877, g_loss: 0.69379574\n",
      "Step: [10157] d_loss: 1.38861716, g_loss: 0.69448644\n",
      "Step: [10158] d_loss: 1.38550782, g_loss: 0.69493288\n",
      "Step: [10159] d_loss: 1.38604379, g_loss: 0.69176418\n",
      "Step: [10160] d_loss: 1.38669765, g_loss: 0.69174117\n",
      "Step: [10161] d_loss: 1.38564193, g_loss: 0.69411719\n",
      "Step: [10162] d_loss: 1.38606751, g_loss: 0.69440579\n",
      "Step: [10163] d_loss: 1.38628113, g_loss: 0.69554973\n",
      "Step: [10164] d_loss: 1.38801670, g_loss: 0.69536632\n",
      "Step: [10165] d_loss: 1.38666999, g_loss: 0.69072723\n",
      "Step: [10166] d_loss: 1.38585079, g_loss: 0.69303668\n",
      "Step: [10167] d_loss: 1.38751602, g_loss: 0.69571596\n",
      "Step: [10168] d_loss: 1.38586903, g_loss: 0.69339454\n",
      "Step: [10169] d_loss: 1.38581395, g_loss: 0.69518459\n",
      "Step: [10170] d_loss: 1.38613069, g_loss: 0.69554985\n",
      "Step: [10171] d_loss: 1.38504648, g_loss: 0.69499457\n",
      "Step: [10172] d_loss: 1.38753629, g_loss: 0.69339287\n",
      "Step: [10173] d_loss: 1.38764715, g_loss: 0.69419086\n",
      "Step: [10174] d_loss: 1.38813424, g_loss: 0.69675446\n",
      "Step: [10175] d_loss: 1.38738108, g_loss: 0.69277251\n",
      "Step: [10176] d_loss: 1.38614225, g_loss: 0.69487798\n",
      "Step: [10177] d_loss: 1.38665235, g_loss: 0.68997419\n",
      "Step: [10178] d_loss: 1.38467312, g_loss: 0.69554329\n",
      "Step: [10179] d_loss: 1.38630104, g_loss: 0.69556820\n",
      "Step: [10180] d_loss: 1.38324857, g_loss: 0.69700533\n",
      "Step: [10181] d_loss: 1.38451314, g_loss: 0.69789773\n",
      "Step: [10182] d_loss: 1.38512230, g_loss: 0.69497156\n",
      "Step: [10183] d_loss: 1.38118052, g_loss: 0.69999903\n",
      "Step: [10184] d_loss: 1.38368535, g_loss: 0.69657767\n",
      "Step: [10185] d_loss: 1.38471007, g_loss: 0.69528854\n",
      "Step: [10186] d_loss: 1.38815427, g_loss: 0.69222778\n",
      "Step: [10187] d_loss: 1.38282132, g_loss: 0.69663393\n",
      "Step: [10188] d_loss: 1.38256264, g_loss: 0.69698787\n",
      "Step: [10189] d_loss: 1.38308084, g_loss: 0.69710636\n",
      "Step: [10190] d_loss: 1.38629079, g_loss: 0.69728452\n",
      "Step: [10191] d_loss: 1.38829994, g_loss: 0.69978285\n",
      "Step: [10192] d_loss: 1.39372015, g_loss: 0.68977869\n",
      "Step: [10193] d_loss: 1.39247346, g_loss: 0.69215953\n",
      "Step: [10194] d_loss: 1.39377737, g_loss: 0.68675226\n",
      "Step: [10195] d_loss: 1.39130235, g_loss: 0.69823420\n",
      "Step: [10196] d_loss: 1.38979220, g_loss: 0.69422841\n",
      "Step: [10197] d_loss: 1.38686299, g_loss: 0.69666851\n",
      "Step: [10198] d_loss: 1.38676512, g_loss: 0.69368744\n",
      "Step: [10199] d_loss: 1.38803053, g_loss: 0.69119418\n",
      "Step: [10200] d_loss: 1.38657308, g_loss: 0.69307554\n",
      "Step: [10201] d_loss: 1.38659489, g_loss: 0.69235468\n",
      "Step: [10202] d_loss: 1.38804007, g_loss: 0.69337356\n",
      "Step: [10203] d_loss: 1.38694823, g_loss: 0.69400895\n",
      "Step: [10204] d_loss: 1.38772070, g_loss: 0.69215840\n",
      "Step: [10205] d_loss: 1.38623857, g_loss: 0.69754994\n",
      "Step: [10206] d_loss: 1.38670731, g_loss: 0.69447178\n",
      "Step: [10207] d_loss: 1.38594544, g_loss: 0.69384867\n",
      "Step: [10208] d_loss: 1.38585925, g_loss: 0.69460785\n",
      "Step: [10209] d_loss: 1.38574016, g_loss: 0.69294000\n",
      "Step: [10210] d_loss: 1.38632679, g_loss: 0.69369197\n",
      "Step: [10211] d_loss: 1.38498425, g_loss: 0.69408858\n",
      "Step: [10212] d_loss: 1.38543296, g_loss: 0.69491613\n",
      "Step: [10213] d_loss: 1.38416469, g_loss: 0.69300908\n",
      "Step: [10214] d_loss: 1.38430595, g_loss: 0.69547939\n",
      "Step: [10215] d_loss: 1.38421810, g_loss: 0.69452685\n",
      "Step: [10216] d_loss: 1.38228250, g_loss: 0.69713402\n",
      "Step: [10217] d_loss: 1.38572478, g_loss: 0.69520420\n",
      "Step: [10218] d_loss: 1.38507199, g_loss: 0.68887162\n",
      "Step: [10219] d_loss: 1.38642907, g_loss: 0.69568646\n",
      "Step: [10220] d_loss: 1.38621235, g_loss: 0.69649303\n",
      "Step: [10221] d_loss: 1.38547969, g_loss: 0.69477904\n",
      "Step: [10222] d_loss: 1.38800287, g_loss: 0.69180632\n",
      "Step: [10223] d_loss: 1.38688052, g_loss: 0.69414234\n",
      "Step: [10224] d_loss: 1.38502717, g_loss: 0.69569361\n",
      "Step: [10225] d_loss: 1.38774037, g_loss: 0.69359267\n",
      "Step: [10226] d_loss: 1.38733459, g_loss: 0.69447064\n",
      "Step: [10227] d_loss: 1.38537025, g_loss: 0.69229543\n",
      "Step: [10228] d_loss: 1.38855028, g_loss: 0.69137174\n",
      "Step: [10229] d_loss: 1.38724339, g_loss: 0.69196171\n",
      "Step: [10230] d_loss: 1.38745308, g_loss: 0.69491369\n",
      "Step: [10231] d_loss: 1.38635516, g_loss: 0.69120151\n",
      "Step: [10232] d_loss: 1.38851821, g_loss: 0.69310904\n",
      "Step: [10233] d_loss: 1.38629425, g_loss: 0.69542903\n",
      "Step: [10234] d_loss: 1.38693905, g_loss: 0.69277942\n",
      "Step: [10235] d_loss: 1.38465261, g_loss: 0.69400513\n",
      "Step: [10236] d_loss: 1.38700271, g_loss: 0.69679582\n",
      "Step: [10237] d_loss: 1.38881826, g_loss: 0.69230783\n",
      "Step: [10238] d_loss: 1.38738370, g_loss: 0.69289571\n",
      "Step: [10239] d_loss: 1.38758171, g_loss: 0.69273895\n",
      "Step: [10240] d_loss: 1.38712406, g_loss: 0.69519448\n",
      "Step: [10241] d_loss: 1.38691354, g_loss: 0.69224572\n",
      "Step: [10242] d_loss: 1.38831151, g_loss: 0.69251615\n",
      "Step: [10243] d_loss: 1.38634610, g_loss: 0.69632411\n",
      "Step: [10244] d_loss: 1.38678336, g_loss: 0.69318175\n",
      "Step: [10245] d_loss: 1.38628352, g_loss: 0.69640374\n",
      "Step: [10246] d_loss: 1.38516665, g_loss: 0.69349378\n",
      "Step: [10247] d_loss: 1.38521683, g_loss: 0.69672656\n",
      "Step: [10248] d_loss: 1.38651884, g_loss: 0.69593322\n",
      "Step: [10249] d_loss: 1.38475072, g_loss: 0.69731230\n",
      "Step: [10250] d_loss: 1.38499784, g_loss: 0.69490945\n",
      "Step: [10251] d_loss: 1.38763332, g_loss: 0.69501293\n",
      "Step: [10252] d_loss: 1.38746142, g_loss: 0.68782055\n",
      "Step: [10253] d_loss: 1.38773680, g_loss: 0.69587326\n",
      "Step: [10254] d_loss: 1.38563085, g_loss: 0.69616860\n",
      "Step: [10255] d_loss: 1.38659251, g_loss: 0.69657242\n",
      "Step: [10256] d_loss: 1.38578153, g_loss: 0.69493830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10257] d_loss: 1.38287807, g_loss: 0.69796515\n",
      "Step: [10258] d_loss: 1.38421881, g_loss: 0.69891340\n",
      "Step: [10259] d_loss: 1.38502145, g_loss: 0.69435680\n",
      "Step: [10260] d_loss: 1.38761079, g_loss: 0.69756299\n",
      "Step: [10261] d_loss: 1.38634491, g_loss: 0.69217151\n",
      "Step: [10262] d_loss: 1.38629222, g_loss: 0.69464505\n",
      "Step: [10263] d_loss: 1.38848817, g_loss: 0.69480503\n",
      "Step: [10264] d_loss: 1.38676858, g_loss: 0.69386053\n",
      "Step: [10265] d_loss: 1.38925993, g_loss: 0.69270396\n",
      "Step: [10266] d_loss: 1.38831174, g_loss: 0.69177133\n",
      "Step: [10267] d_loss: 1.38730597, g_loss: 0.69275224\n",
      "Step: [10268] d_loss: 1.38772607, g_loss: 0.69070315\n",
      "Step: [10269] d_loss: 1.38954175, g_loss: 0.68905073\n",
      "Step: [10270] d_loss: 1.38666630, g_loss: 0.69083440\n",
      "Step: [10271] d_loss: 1.38716316, g_loss: 0.69205809\n",
      "Step: [10272] d_loss: 1.38806748, g_loss: 0.69286609\n",
      "Step: [10273] d_loss: 1.38627779, g_loss: 0.69239366\n",
      "Step: [10274] d_loss: 1.38549781, g_loss: 0.69383925\n",
      "Step: [10275] d_loss: 1.38581371, g_loss: 0.69317091\n",
      "Step: [10276] d_loss: 1.38592327, g_loss: 0.69256169\n",
      "Step: [10277] d_loss: 1.38544273, g_loss: 0.69277859\n",
      "Step: [10278] d_loss: 1.38526356, g_loss: 0.69359463\n",
      "Step: [10279] d_loss: 1.38393939, g_loss: 0.69379878\n",
      "Step: [10280] d_loss: 1.38539302, g_loss: 0.69232035\n",
      "Step: [10281] d_loss: 1.38412189, g_loss: 0.69518888\n",
      "Step: [10282] d_loss: 1.38445890, g_loss: 0.69413936\n",
      "Step: [10283] d_loss: 1.38485813, g_loss: 0.69242746\n",
      "Step: [10284] d_loss: 1.38457036, g_loss: 0.69508046\n",
      "Step: [10285] d_loss: 1.38433385, g_loss: 0.69439602\n",
      "Step: [10286] d_loss: 1.38598490, g_loss: 0.69523704\n",
      "Step: [10287] d_loss: 1.38792396, g_loss: 0.69377398\n",
      "Step: [10288] d_loss: 1.38528323, g_loss: 0.69520998\n",
      "Step: [10289] d_loss: 1.38529932, g_loss: 0.69386041\n",
      "Step: [10290] d_loss: 1.38676810, g_loss: 0.69534969\n",
      "Step: [10291] d_loss: 1.38480878, g_loss: 0.69590706\n",
      "Step: [10292] d_loss: 1.38617861, g_loss: 0.69664061\n",
      "Step: [10293] d_loss: 1.38585556, g_loss: 0.69491738\n",
      "Step: [10294] d_loss: 1.38853455, g_loss: 0.69895697\n",
      "Step: [10295] d_loss: 1.38659334, g_loss: 0.69516456\n",
      "Step: [10296] d_loss: 1.38764048, g_loss: 0.70044208\n",
      "Step: [10297] d_loss: 1.38636708, g_loss: 0.69782072\n",
      "Step: [10298] d_loss: 1.38533211, g_loss: 0.69774157\n",
      "Step: [10299] d_loss: 1.38560486, g_loss: 0.69512409\n",
      "Step: [10300] d_loss: 1.38581693, g_loss: 0.69427311\n",
      "Step: [10301] d_loss: 1.38623381, g_loss: 0.69473004\n",
      "Step: [10302] d_loss: 1.38578725, g_loss: 0.69618154\n",
      "Step: [10303] d_loss: 1.38587141, g_loss: 0.69779903\n",
      "Step: [10304] d_loss: 1.38479090, g_loss: 0.69508541\n",
      "Step: [10305] d_loss: 1.38616908, g_loss: 0.69237906\n",
      "Step: [10306] d_loss: 1.38605177, g_loss: 0.69314790\n",
      "Step: [10307] d_loss: 1.38607240, g_loss: 0.69623184\n",
      "Step: [10308] d_loss: 1.38687682, g_loss: 0.69345647\n",
      "Step: [10309] d_loss: 1.38925362, g_loss: 0.69144142\n",
      "Step: [10310] d_loss: 1.38777602, g_loss: 0.69237554\n",
      "Step: [10311] d_loss: 1.38954520, g_loss: 0.69044167\n",
      "Step: [10312] d_loss: 1.39094841, g_loss: 0.68991596\n",
      "Step: [10313] d_loss: 1.38938534, g_loss: 0.69341338\n",
      "Step: [10314] d_loss: 1.39098048, g_loss: 0.69015539\n",
      "Step: [10315] d_loss: 1.38795948, g_loss: 0.69190198\n",
      "Step: [10316] d_loss: 1.38698077, g_loss: 0.69224858\n",
      "Step: [10317] d_loss: 1.38684869, g_loss: 0.69109595\n",
      "Step: [10318] d_loss: 1.38614845, g_loss: 0.69289422\n",
      "Step: [10319] d_loss: 1.38541842, g_loss: 0.69115239\n",
      "Step: [10320] d_loss: 1.38489032, g_loss: 0.69232821\n",
      "Step: [10321] d_loss: 1.38452744, g_loss: 0.69379193\n",
      "Step: [10322] d_loss: 1.38475990, g_loss: 0.69451749\n",
      "Step: [10323] d_loss: 1.38572311, g_loss: 0.69545722\n",
      "Step: [10324] d_loss: 1.38574326, g_loss: 0.69455320\n",
      "Step: [10325] d_loss: 1.38533068, g_loss: 0.69542342\n",
      "Step: [10326] d_loss: 1.38430047, g_loss: 0.69410574\n",
      "Step: [10327] d_loss: 1.38520658, g_loss: 0.69201624\n",
      "Step: [10328] d_loss: 1.38566494, g_loss: 0.69470018\n",
      "Step: [10329] d_loss: 1.38519478, g_loss: 0.69432092\n",
      "Step: [10330] d_loss: 1.38605356, g_loss: 0.69460166\n",
      "Step: [10331] d_loss: 1.38537347, g_loss: 0.69443417\n",
      "Step: [10332] d_loss: 1.38607275, g_loss: 0.69558752\n",
      "Step: [10333] d_loss: 1.38647056, g_loss: 0.69404048\n",
      "Step: [10334] d_loss: 1.38489151, g_loss: 0.69789690\n",
      "Step: [10335] d_loss: 1.38509262, g_loss: 0.69229478\n",
      "Step: [10336] d_loss: 1.38475966, g_loss: 0.69653988\n",
      "Step: [10337] d_loss: 1.38581848, g_loss: 0.69259262\n",
      "Step: [10338] d_loss: 1.38664222, g_loss: 0.69840795\n",
      "Step: [10339] d_loss: 1.38622117, g_loss: 0.69197631\n",
      "Step: [10340] d_loss: 1.38584328, g_loss: 0.69269848\n",
      "Step: [10341] d_loss: 1.38491261, g_loss: 0.69525623\n",
      "Step: [10342] d_loss: 1.38726389, g_loss: 0.69512570\n",
      "Step: [10343] d_loss: 1.38594198, g_loss: 0.69493973\n",
      "Step: [10344] d_loss: 1.38726521, g_loss: 0.69139981\n",
      "Step: [10345] d_loss: 1.38634300, g_loss: 0.69794095\n",
      "Step: [10346] d_loss: 1.38733125, g_loss: 0.69260824\n",
      "Step: [10347] d_loss: 1.38640559, g_loss: 0.69306493\n",
      "Step: [10348] d_loss: 1.38600183, g_loss: 0.69543940\n",
      "Step: [10349] d_loss: 1.38755178, g_loss: 0.69490230\n",
      "Step: [10350] d_loss: 1.38737857, g_loss: 0.69501376\n",
      "Step: [10351] d_loss: 1.38771152, g_loss: 0.69404495\n",
      "Step: [10352] d_loss: 1.38721120, g_loss: 0.69382590\n",
      "Step: [10353] d_loss: 1.38693929, g_loss: 0.69234198\n",
      "Step: [10354] d_loss: 1.38694382, g_loss: 0.69264281\n",
      "Step: [10355] d_loss: 1.38688779, g_loss: 0.69689536\n",
      "Step: [10356] d_loss: 1.38795221, g_loss: 0.69243217\n",
      "Step: [10357] d_loss: 1.38635516, g_loss: 0.69559771\n",
      "Step: [10358] d_loss: 1.38641512, g_loss: 0.69473565\n",
      "Step: [10359] d_loss: 1.38630438, g_loss: 0.69644928\n",
      "Step: [10360] d_loss: 1.38590157, g_loss: 0.69661349\n",
      "Step: [10361] d_loss: 1.38495159, g_loss: 0.69479293\n",
      "Step: [10362] d_loss: 1.38599920, g_loss: 0.69345826\n",
      "Step: [10363] d_loss: 1.38603020, g_loss: 0.69252992\n",
      "Step: [10364] d_loss: 1.38516259, g_loss: 0.69514710\n",
      "Step: [10365] d_loss: 1.38692987, g_loss: 0.69419587\n",
      "Step: [10366] d_loss: 1.38521338, g_loss: 0.69396245\n",
      "Step: [10367] d_loss: 1.38668609, g_loss: 0.69110942\n",
      "Step: [10368] d_loss: 1.38619542, g_loss: 0.69577682\n",
      "Step: [10369] d_loss: 1.38542593, g_loss: 0.69537383\n",
      "Step: [10370] d_loss: 1.38545537, g_loss: 0.69384521\n",
      "Step: [10371] d_loss: 1.38626099, g_loss: 0.69356072\n",
      "Step: [10372] d_loss: 1.38613725, g_loss: 0.69380355\n",
      "Step: [10373] d_loss: 1.38668084, g_loss: 0.69319761\n",
      "Step: [10374] d_loss: 1.38581288, g_loss: 0.69622755\n",
      "Step: [10375] d_loss: 1.38502693, g_loss: 0.69519466\n",
      "Step: [10376] d_loss: 1.38487935, g_loss: 0.69399035\n",
      "Step: [10377] d_loss: 1.38626540, g_loss: 0.69221562\n",
      "Step: [10378] d_loss: 1.38639808, g_loss: 0.69226384\n",
      "Step: [10379] d_loss: 1.38855112, g_loss: 0.69276059\n",
      "Step: [10380] d_loss: 1.38718092, g_loss: 0.69074416\n",
      "Step: [10381] d_loss: 1.38600123, g_loss: 0.69228971\n",
      "Step: [10382] d_loss: 1.38723993, g_loss: 0.69361234\n",
      "Step: [10383] d_loss: 1.38668132, g_loss: 0.69500482\n",
      "Step: [10384] d_loss: 1.38852870, g_loss: 0.69372606\n",
      "Step: [10385] d_loss: 1.38692391, g_loss: 0.69357282\n",
      "Step: [10386] d_loss: 1.38724923, g_loss: 0.69403517\n",
      "Step: [10387] d_loss: 1.38544524, g_loss: 0.69340181\n",
      "Step: [10388] d_loss: 1.38644564, g_loss: 0.69581532\n",
      "Step: [10389] d_loss: 1.38680887, g_loss: 0.69509476\n",
      "Step: [10390] d_loss: 1.38573992, g_loss: 0.69562042\n",
      "Step: [10391] d_loss: 1.38576674, g_loss: 0.69445515\n",
      "Step: [10392] d_loss: 1.38658595, g_loss: 0.69509912\n",
      "Step: [10393] d_loss: 1.38476574, g_loss: 0.69392854\n",
      "Step: [10394] d_loss: 1.38417864, g_loss: 0.69651848\n",
      "Step: [10395] d_loss: 1.38579273, g_loss: 0.69500017\n",
      "Step: [10396] d_loss: 1.38401365, g_loss: 0.69538116\n",
      "Step: [10397] d_loss: 1.38506150, g_loss: 0.69506955\n",
      "Step: [10398] d_loss: 1.38554025, g_loss: 0.69295365\n",
      "Step: [10399] d_loss: 1.38563764, g_loss: 0.69500208\n",
      "Step: [10400] d_loss: 1.38403869, g_loss: 0.69587690\n",
      "Step: [10401] d_loss: 1.38471842, g_loss: 0.69550294\n",
      "Step: [10402] d_loss: 1.38606954, g_loss: 0.69240999\n",
      "Step: [10403] d_loss: 1.38603270, g_loss: 0.69310606\n",
      "Step: [10404] d_loss: 1.38666463, g_loss: 0.69289577\n",
      "Step: [10405] d_loss: 1.38531053, g_loss: 0.69569194\n",
      "Step: [10406] d_loss: 1.38602948, g_loss: 0.69333023\n",
      "Step: [10407] d_loss: 1.38738036, g_loss: 0.69519031\n",
      "Step: [10408] d_loss: 1.38631117, g_loss: 0.69033748\n",
      "Step: [10409] d_loss: 1.38682103, g_loss: 0.69180918\n",
      "Step: [10410] d_loss: 1.38620532, g_loss: 0.69517994\n",
      "Step: [10411] d_loss: 1.38526046, g_loss: 0.69693971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10412] d_loss: 1.38610315, g_loss: 0.69444245\n",
      "Step: [10413] d_loss: 1.38841331, g_loss: 0.69071323\n",
      "Step: [10414] d_loss: 1.38675320, g_loss: 0.69385582\n",
      "Step: [10415] d_loss: 1.38742089, g_loss: 0.69078732\n",
      "Step: [10416] d_loss: 1.38676023, g_loss: 0.69744432\n",
      "Step: [10417] d_loss: 1.38760030, g_loss: 0.68987906\n",
      "Step: [10418] d_loss: 1.38634586, g_loss: 0.69344276\n",
      "Step: [10419] d_loss: 1.38666785, g_loss: 0.69442713\n",
      "Step: [10420] d_loss: 1.38686037, g_loss: 0.69393945\n",
      "Step: [10421] d_loss: 1.38602555, g_loss: 0.69420308\n",
      "Step: [10422] d_loss: 1.38627040, g_loss: 0.69281954\n",
      "Step: [10423] d_loss: 1.38620949, g_loss: 0.69438243\n",
      "Step: [10424] d_loss: 1.38594317, g_loss: 0.69399005\n",
      "Step: [10425] d_loss: 1.38520944, g_loss: 0.69556165\n",
      "Step: [10426] d_loss: 1.38691509, g_loss: 0.69371760\n",
      "Step: [10427] d_loss: 1.38423204, g_loss: 0.69580019\n",
      "Step: [10428] d_loss: 1.38599873, g_loss: 0.69347793\n",
      "Step: [10429] d_loss: 1.38563097, g_loss: 0.69366628\n",
      "Step: [10430] d_loss: 1.38489866, g_loss: 0.69481844\n",
      "Step: [10431] d_loss: 1.38625324, g_loss: 0.69479561\n",
      "Step: [10432] d_loss: 1.38684213, g_loss: 0.69379556\n",
      "Step: [10433] d_loss: 1.38753724, g_loss: 0.69331658\n",
      "Step: [10434] d_loss: 1.38552380, g_loss: 0.69478798\n",
      "Step: [10435] d_loss: 1.38788891, g_loss: 0.69355035\n",
      "Step: [10436] d_loss: 1.38644958, g_loss: 0.69311112\n",
      "Step: [10437] d_loss: 1.38791382, g_loss: 0.69379818\n",
      "Step: [10438] d_loss: 1.38602781, g_loss: 0.69620144\n",
      "Step: [10439] d_loss: 1.38600743, g_loss: 0.69740820\n",
      "Step: [10440] d_loss: 1.38574672, g_loss: 0.69496405\n",
      "Step: [10441] d_loss: 1.38416958, g_loss: 0.69563687\n",
      "Step: [10442] d_loss: 1.38726711, g_loss: 0.69388062\n",
      "Step: [10443] d_loss: 1.38604808, g_loss: 0.69425726\n",
      "Step: [10444] d_loss: 1.38583231, g_loss: 0.69772691\n",
      "Step: [10445] d_loss: 1.38576412, g_loss: 0.69584781\n",
      "Step: [10446] d_loss: 1.38772941, g_loss: 0.69434869\n",
      "Step: [10447] d_loss: 1.38654685, g_loss: 0.69461459\n",
      "Step: [10448] d_loss: 1.38977599, g_loss: 0.68776274\n",
      "Step: [10449] d_loss: 1.38722408, g_loss: 0.69344282\n",
      "Step: [10450] d_loss: 1.38852000, g_loss: 0.69317472\n",
      "Step: [10451] d_loss: 1.38732696, g_loss: 0.69485492\n",
      "Step: [10452] d_loss: 1.38617885, g_loss: 0.69399714\n",
      "Step: [10453] d_loss: 1.38742018, g_loss: 0.69639909\n",
      "Step: [10454] d_loss: 1.38651907, g_loss: 0.69341886\n",
      "Step: [10455] d_loss: 1.38435912, g_loss: 0.69325441\n",
      "Step: [10456] d_loss: 1.38512075, g_loss: 0.69241327\n",
      "Step: [10457] d_loss: 1.38616991, g_loss: 0.69512320\n",
      "Step: [10458] d_loss: 1.38554311, g_loss: 0.69771552\n",
      "Step: [10459] d_loss: 1.38758254, g_loss: 0.69301480\n",
      "Step: [10460] d_loss: 1.38651824, g_loss: 0.69512498\n",
      "Step: [10461] d_loss: 1.38627815, g_loss: 0.69559896\n",
      "Step: [10462] d_loss: 1.38581324, g_loss: 0.69373989\n",
      "Step: [10463] d_loss: 1.38703275, g_loss: 0.69369113\n",
      "Step: [10464] d_loss: 1.38787079, g_loss: 0.69086760\n",
      "Step: [10465] d_loss: 1.38835549, g_loss: 0.69107330\n",
      "Step: [10466] d_loss: 1.38654828, g_loss: 0.69137317\n",
      "Step: [10467] d_loss: 1.38645256, g_loss: 0.69496614\n",
      "Step: [10468] d_loss: 1.38568640, g_loss: 0.69587791\n",
      "Step: [10469] d_loss: 1.38598013, g_loss: 0.69432646\n",
      "Step: [10470] d_loss: 1.38546383, g_loss: 0.69424826\n",
      "Step: [10471] d_loss: 1.38671231, g_loss: 0.69257277\n",
      "Step: [10472] d_loss: 1.38492298, g_loss: 0.69411546\n",
      "Step: [10473] d_loss: 1.38515890, g_loss: 0.69423640\n",
      "Step: [10474] d_loss: 1.38660717, g_loss: 0.69277769\n",
      "Step: [10475] d_loss: 1.38268018, g_loss: 0.69794011\n",
      "Step: [10476] d_loss: 1.38429546, g_loss: 0.69573903\n",
      "Step: [10477] d_loss: 1.38393891, g_loss: 0.69341254\n",
      "Step: [10478] d_loss: 1.38507509, g_loss: 0.69199967\n",
      "Step: [10479] d_loss: 1.38292909, g_loss: 0.69317174\n",
      "Step: [10480] d_loss: 1.38470542, g_loss: 0.69390959\n",
      "Step: [10481] d_loss: 1.38519859, g_loss: 0.69586575\n",
      "Step: [10482] d_loss: 1.38224125, g_loss: 0.69631416\n",
      "Step: [10483] d_loss: 1.38388538, g_loss: 0.69577575\n",
      "Step: [10484] d_loss: 1.38748598, g_loss: 0.69419861\n",
      "Step: [10485] d_loss: 1.38564944, g_loss: 0.69546866\n",
      "Step: [10486] d_loss: 1.38689089, g_loss: 0.69180000\n",
      "Step: [10487] d_loss: 1.38745999, g_loss: 0.69257188\n",
      "Step: [10488] d_loss: 1.38704967, g_loss: 0.69348365\n",
      "Step: [10489] d_loss: 1.38511443, g_loss: 0.69428742\n",
      "Step: [10490] d_loss: 1.38858414, g_loss: 0.69281054\n",
      "Step: [10491] d_loss: 1.38674223, g_loss: 0.69278342\n",
      "Step: [10492] d_loss: 1.38625336, g_loss: 0.69366980\n",
      "Step: [10493] d_loss: 1.38861942, g_loss: 0.69438571\n",
      "Step: [10494] d_loss: 1.38634527, g_loss: 0.69441235\n",
      "Step: [10495] d_loss: 1.38825154, g_loss: 0.69391131\n",
      "Step: [10496] d_loss: 1.38685858, g_loss: 0.69136089\n",
      "Step: [10497] d_loss: 1.38962579, g_loss: 0.69091696\n",
      "Step: [10498] d_loss: 1.38638043, g_loss: 0.69384980\n",
      "Step: [10499] d_loss: 1.38716865, g_loss: 0.69322228\n",
      "Step: [10500] d_loss: 1.38749075, g_loss: 0.69396114\n",
      "Step: [10501] d_loss: 1.38861191, g_loss: 0.69398451\n",
      "Step: [10502] d_loss: 1.38642812, g_loss: 0.69408083\n",
      "Step: [10503] d_loss: 1.38705814, g_loss: 0.69285476\n",
      "Step: [10504] d_loss: 1.38650644, g_loss: 0.69542730\n",
      "Step: [10505] d_loss: 1.38606977, g_loss: 0.69391167\n",
      "Step: [10506] d_loss: 1.38409615, g_loss: 0.69692147\n",
      "Step: [10507] d_loss: 1.38608694, g_loss: 0.69697535\n",
      "Step: [10508] d_loss: 1.38531923, g_loss: 0.69385803\n",
      "Step: [10509] d_loss: 1.38463974, g_loss: 0.69663858\n",
      "Step: [10510] d_loss: 1.38770390, g_loss: 0.69293845\n",
      "Step: [10511] d_loss: 1.38439274, g_loss: 0.69648445\n",
      "Step: [10512] d_loss: 1.38511276, g_loss: 0.69438970\n",
      "Step: [10513] d_loss: 1.38537848, g_loss: 0.69115114\n",
      "Step: [10514] d_loss: 1.38741350, g_loss: 0.69200188\n",
      "Step: [10515] d_loss: 1.38653719, g_loss: 0.69215620\n",
      "Step: [10516] d_loss: 1.38475299, g_loss: 0.69324356\n",
      "Step: [10517] d_loss: 1.38814569, g_loss: 0.69506955\n",
      "Step: [10518] d_loss: 1.38652515, g_loss: 0.69495726\n",
      "Step: [10519] d_loss: 1.38801312, g_loss: 0.69051182\n",
      "Step: [10520] d_loss: 1.38477254, g_loss: 0.69452643\n",
      "Step: [10521] d_loss: 1.38560665, g_loss: 0.69153607\n",
      "Step: [10522] d_loss: 1.38983917, g_loss: 0.69188309\n",
      "Step: [10523] d_loss: 1.38739669, g_loss: 0.69174206\n",
      "Step: [10524] d_loss: 1.38622510, g_loss: 0.69240117\n",
      "Step: [10525] d_loss: 1.38690269, g_loss: 0.69353580\n",
      "Step: [10526] d_loss: 1.38738620, g_loss: 0.69345546\n",
      "Step: [10527] d_loss: 1.38675404, g_loss: 0.69427013\n",
      "Step: [10528] d_loss: 1.38528633, g_loss: 0.69266438\n",
      "Step: [10529] d_loss: 1.38605714, g_loss: 0.69345480\n",
      "Step: [10530] d_loss: 1.38713193, g_loss: 0.69345385\n",
      "Step: [10531] d_loss: 1.38705420, g_loss: 0.69292557\n",
      "Step: [10532] d_loss: 1.38729334, g_loss: 0.69448900\n",
      "Step: [10533] d_loss: 1.38663197, g_loss: 0.69464761\n",
      "Step: [10534] d_loss: 1.38579535, g_loss: 0.69202125\n",
      "Step: [10535] d_loss: 1.38600254, g_loss: 0.69172502\n",
      "Step: [10536] d_loss: 1.38619971, g_loss: 0.69135714\n",
      "Step: [10537] d_loss: 1.38719463, g_loss: 0.70060945\n",
      "Step: [10538] d_loss: 1.38968408, g_loss: 0.68877906\n",
      "Step: [10539] d_loss: 1.38689423, g_loss: 0.69460142\n",
      "Step: [10540] d_loss: 1.38633180, g_loss: 0.69434083\n",
      "Step: [10541] d_loss: 1.38332462, g_loss: 0.69311363\n",
      "Step: [10542] d_loss: 1.38655686, g_loss: 0.69781613\n",
      "Step: [10543] d_loss: 1.38530064, g_loss: 0.69464564\n",
      "Step: [10544] d_loss: 1.38458753, g_loss: 0.69254720\n",
      "Step: [10545] d_loss: 1.38344955, g_loss: 0.69509149\n",
      "Step: [10546] d_loss: 1.38350868, g_loss: 0.69431472\n",
      "Step: [10547] d_loss: 1.38383591, g_loss: 0.69711906\n",
      "Step: [10548] d_loss: 1.38777208, g_loss: 0.69582546\n",
      "Step: [10549] d_loss: 1.38414001, g_loss: 0.69286001\n",
      "Step: [10550] d_loss: 1.38484645, g_loss: 0.69539577\n",
      "Step: [10551] d_loss: 1.38997161, g_loss: 0.69204468\n",
      "Step: [10552] d_loss: 1.38429332, g_loss: 0.69327515\n",
      "Step: [10553] d_loss: 1.38739645, g_loss: 0.69312584\n",
      "Step: [10554] d_loss: 1.38780439, g_loss: 0.69359601\n",
      "Step: [10555] d_loss: 1.38848686, g_loss: 0.69435167\n",
      "Step: [10556] d_loss: 1.38445377, g_loss: 0.69623381\n",
      "Step: [10557] d_loss: 1.38735723, g_loss: 0.69507962\n",
      "Step: [10558] d_loss: 1.38813436, g_loss: 0.69503701\n",
      "Step: [10559] d_loss: 1.38773322, g_loss: 0.69279271\n",
      "Step: [10560] d_loss: 1.38671517, g_loss: 0.69262695\n",
      "Step: [10561] d_loss: 1.38759673, g_loss: 0.69235229\n",
      "Step: [10562] d_loss: 1.38644576, g_loss: 0.69494766\n",
      "Step: [10563] d_loss: 1.38557947, g_loss: 0.69577718\n",
      "Step: [10564] d_loss: 1.38743973, g_loss: 0.69652295\n",
      "Step: [10565] d_loss: 1.38648963, g_loss: 0.69407374\n",
      "Step: [10566] d_loss: 1.38571978, g_loss: 0.69387066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10567] d_loss: 1.38504362, g_loss: 0.69405001\n",
      "Step: [10568] d_loss: 1.38628650, g_loss: 0.69255257\n",
      "Step: [10569] d_loss: 1.38500988, g_loss: 0.69308543\n",
      "Step: [10570] d_loss: 1.38547969, g_loss: 0.69570404\n",
      "Step: [10571] d_loss: 1.38512492, g_loss: 0.69530743\n",
      "Step: [10572] d_loss: 1.38445425, g_loss: 0.69538903\n",
      "Step: [10573] d_loss: 1.38740683, g_loss: 0.69198066\n",
      "Step: [10574] d_loss: 1.38513231, g_loss: 0.69367784\n",
      "Step: [10575] d_loss: 1.38486278, g_loss: 0.69594640\n",
      "Step: [10576] d_loss: 1.38510060, g_loss: 0.69453657\n",
      "Step: [10577] d_loss: 1.38716996, g_loss: 0.69377792\n",
      "Step: [10578] d_loss: 1.38603878, g_loss: 0.69443631\n",
      "Step: [10579] d_loss: 1.38661933, g_loss: 0.69450915\n",
      "Step: [10580] d_loss: 1.38485849, g_loss: 0.69595528\n",
      "Step: [10581] d_loss: 1.38912821, g_loss: 0.69468915\n",
      "Step: [10582] d_loss: 1.38454318, g_loss: 0.69130814\n",
      "Step: [10583] d_loss: 1.38378644, g_loss: 0.69922459\n",
      "Step: [10584] d_loss: 1.38686943, g_loss: 0.69831300\n",
      "Step: [10585] d_loss: 1.39171362, g_loss: 0.69514400\n",
      "Step: [10586] d_loss: 1.38561869, g_loss: 0.69272745\n",
      "Step: [10587] d_loss: 1.38750052, g_loss: 0.69484150\n",
      "Step: [10588] d_loss: 1.38812923, g_loss: 0.69268763\n",
      "Step: [10589] d_loss: 1.38656831, g_loss: 0.69555378\n",
      "Step: [10590] d_loss: 1.38619876, g_loss: 0.69450158\n",
      "Step: [10591] d_loss: 1.38723373, g_loss: 0.69388509\n",
      "Step: [10592] d_loss: 1.38602209, g_loss: 0.69298780\n",
      "Step: [10593] d_loss: 1.38587070, g_loss: 0.69395298\n",
      "Step: [10594] d_loss: 1.38587403, g_loss: 0.69402218\n",
      "Step: [10595] d_loss: 1.38547027, g_loss: 0.69328320\n",
      "Step: [10596] d_loss: 1.38506961, g_loss: 0.69373953\n",
      "Step: [10597] d_loss: 1.38458753, g_loss: 0.69703448\n",
      "Step: [10598] d_loss: 1.38519001, g_loss: 0.69408453\n",
      "Step: [10599] d_loss: 1.38518023, g_loss: 0.69544506\n",
      "Step: [10600] d_loss: 1.38422155, g_loss: 0.69298810\n",
      "Step: [10601] d_loss: 1.38311076, g_loss: 0.69523597\n",
      "Step: [10602] d_loss: 1.38419819, g_loss: 0.69618571\n",
      "Step: [10603] d_loss: 1.38536143, g_loss: 0.69185400\n",
      "Step: [10604] d_loss: 1.38542652, g_loss: 0.69273287\n",
      "Step: [10605] d_loss: 1.38632429, g_loss: 0.69566560\n",
      "Step: [10606] d_loss: 1.38741529, g_loss: 0.69774425\n",
      "Step: [10607] d_loss: 1.38565528, g_loss: 0.69425130\n",
      "Step: [10608] d_loss: 1.38644993, g_loss: 0.69595873\n",
      "Step: [10609] d_loss: 1.38653493, g_loss: 0.69129211\n",
      "Step: [10610] d_loss: 1.38833332, g_loss: 0.69307041\n",
      "Step: [10611] d_loss: 1.38641381, g_loss: 0.69392258\n",
      "Step: [10612] d_loss: 1.38732982, g_loss: 0.69450516\n",
      "Step: [10613] d_loss: 1.39087760, g_loss: 0.68902445\n",
      "Step: [10614] d_loss: 1.38876534, g_loss: 0.69129729\n",
      "Step: [10615] d_loss: 1.38611436, g_loss: 0.69363916\n",
      "Step: [10616] d_loss: 1.38648188, g_loss: 0.69598961\n",
      "Step: [10617] d_loss: 1.38863122, g_loss: 0.69243371\n",
      "Step: [10618] d_loss: 1.38664746, g_loss: 0.69534874\n",
      "Step: [10619] d_loss: 1.38601959, g_loss: 0.69400012\n",
      "Step: [10620] d_loss: 1.38546872, g_loss: 0.69555748\n",
      "Step: [10621] d_loss: 1.38583469, g_loss: 0.69205177\n",
      "Step: [10622] d_loss: 1.38636196, g_loss: 0.69476777\n",
      "Step: [10623] d_loss: 1.38629746, g_loss: 0.69329727\n",
      "Step: [10624] d_loss: 1.38685036, g_loss: 0.69530916\n",
      "Step: [10625] d_loss: 1.38558447, g_loss: 0.69508094\n",
      "Step: [10626] d_loss: 1.38657153, g_loss: 0.69572413\n",
      "Step: [10627] d_loss: 1.38819754, g_loss: 0.69261986\n",
      "Step: [10628] d_loss: 1.38964701, g_loss: 0.69494003\n",
      "Step: [10629] d_loss: 1.38783360, g_loss: 0.69395524\n",
      "Step: [10630] d_loss: 1.38716936, g_loss: 0.69425988\n",
      "Step: [10631] d_loss: 1.38566160, g_loss: 0.69493860\n",
      "Step: [10632] d_loss: 1.38571656, g_loss: 0.70151305\n",
      "Step: [10633] d_loss: 1.38270903, g_loss: 0.70283240\n",
      "Step: [10634] d_loss: 1.38604295, g_loss: 0.69290030\n",
      "Step: [10635] d_loss: 1.38363969, g_loss: 0.70093268\n",
      "Step: [10636] d_loss: 1.38478684, g_loss: 0.69407135\n",
      "Step: [10637] d_loss: 1.38490582, g_loss: 0.69538903\n",
      "Step: [10638] d_loss: 1.38475049, g_loss: 0.69624168\n",
      "Step: [10639] d_loss: 1.38415337, g_loss: 0.69317037\n",
      "Step: [10640] d_loss: 1.38707209, g_loss: 0.69356918\n",
      "Step: [10641] d_loss: 1.38506961, g_loss: 0.69191670\n",
      "Step: [10642] d_loss: 1.38472295, g_loss: 0.69723827\n",
      "Step: [10643] d_loss: 1.39059472, g_loss: 0.69381833\n",
      "Step: [10644] d_loss: 1.38804269, g_loss: 0.69067240\n",
      "Step: [10645] d_loss: 1.38679826, g_loss: 0.69590259\n",
      "Step: [10646] d_loss: 1.39188790, g_loss: 0.69458455\n",
      "Step: [10647] d_loss: 1.38682032, g_loss: 0.69276893\n",
      "Step: [10648] d_loss: 1.38769293, g_loss: 0.69270772\n",
      "Step: [10649] d_loss: 1.38794601, g_loss: 0.69226223\n",
      "Step: [10650] d_loss: 1.39065218, g_loss: 0.69309866\n",
      "Step: [10651] d_loss: 1.38488424, g_loss: 0.69520152\n",
      "Step: [10652] d_loss: 1.38939524, g_loss: 0.69736028\n",
      "Step: [10653] d_loss: 1.38662517, g_loss: 0.69183207\n",
      "Step: [10654] d_loss: 1.38665402, g_loss: 0.69537479\n",
      "Step: [10655] d_loss: 1.38695848, g_loss: 0.68931139\n",
      "Step: [10656] d_loss: 1.39027190, g_loss: 0.69220757\n",
      "Step: [10657] d_loss: 1.38491178, g_loss: 0.69482762\n",
      "Step: [10658] d_loss: 1.38398468, g_loss: 0.69441926\n",
      "Step: [10659] d_loss: 1.38319707, g_loss: 0.69368851\n",
      "Step: [10660] d_loss: 1.38573241, g_loss: 0.69640428\n",
      "Step: [10661] d_loss: 1.38663030, g_loss: 0.68578714\n",
      "Step: [10662] d_loss: 1.38591063, g_loss: 0.70122004\n",
      "Step: [10663] d_loss: 1.38656604, g_loss: 0.69528365\n",
      "Step: [10664] d_loss: 1.38310111, g_loss: 0.69962716\n",
      "Step: [10665] d_loss: 1.38531172, g_loss: 0.69819438\n",
      "Step: [10666] d_loss: 1.38341856, g_loss: 0.69688463\n",
      "Step: [10667] d_loss: 1.38891125, g_loss: 0.69457370\n",
      "Step: [10668] d_loss: 1.38580966, g_loss: 0.69540364\n",
      "Step: [10669] d_loss: 1.38683176, g_loss: 0.69148207\n",
      "Step: [10670] d_loss: 1.38725758, g_loss: 0.69301009\n",
      "Step: [10671] d_loss: 1.38879347, g_loss: 0.69163048\n",
      "Step: [10672] d_loss: 1.38741982, g_loss: 0.69165879\n",
      "Step: [10673] d_loss: 1.38754511, g_loss: 0.69745827\n",
      "Step: [10674] d_loss: 1.38744378, g_loss: 0.69310284\n",
      "Step: [10675] d_loss: 1.38634956, g_loss: 0.69198656\n",
      "Step: [10676] d_loss: 1.38658071, g_loss: 0.69387603\n",
      "Step: [10677] d_loss: 1.38677239, g_loss: 0.69675285\n",
      "Step: [10678] d_loss: 1.38732672, g_loss: 0.69486612\n",
      "Step: [10679] d_loss: 1.38584471, g_loss: 0.69364429\n",
      "Step: [10680] d_loss: 1.38587475, g_loss: 0.69281995\n",
      "Step: [10681] d_loss: 1.38637435, g_loss: 0.69300997\n",
      "Step: [10682] d_loss: 1.38726330, g_loss: 0.69661027\n",
      "Step: [10683] d_loss: 1.38515592, g_loss: 0.69157338\n",
      "Step: [10684] d_loss: 1.38643956, g_loss: 0.69297045\n",
      "Step: [10685] d_loss: 1.38561141, g_loss: 0.69054222\n",
      "Step: [10686] d_loss: 1.38641167, g_loss: 0.69322652\n",
      "Step: [10687] d_loss: 1.38594973, g_loss: 0.69585782\n",
      "Step: [10688] d_loss: 1.38604534, g_loss: 0.69545174\n",
      "Step: [10689] d_loss: 1.39008915, g_loss: 0.70041889\n",
      "Step: [10690] d_loss: 1.38523340, g_loss: 0.69423044\n",
      "Step: [10691] d_loss: 1.38627362, g_loss: 0.69309652\n",
      "Step: [10692] d_loss: 1.38442898, g_loss: 0.69591552\n",
      "Step: [10693] d_loss: 1.38676810, g_loss: 0.69166553\n",
      "Step: [10694] d_loss: 1.38585293, g_loss: 0.69445026\n",
      "Step: [10695] d_loss: 1.38691759, g_loss: 0.69033337\n",
      "Step: [10696] d_loss: 1.38512242, g_loss: 0.69308865\n",
      "Step: [10697] d_loss: 1.38530993, g_loss: 0.69439697\n",
      "Step: [10698] d_loss: 1.38621449, g_loss: 0.69397885\n",
      "Step: [10699] d_loss: 1.38605428, g_loss: 0.69294864\n",
      "Step: [10700] d_loss: 1.38544285, g_loss: 0.69312805\n",
      "Step: [10701] d_loss: 1.38789296, g_loss: 0.69265556\n",
      "Step: [10702] d_loss: 1.38637125, g_loss: 0.69326746\n",
      "Step: [10703] d_loss: 1.38621354, g_loss: 0.69078773\n",
      "Step: [10704] d_loss: 1.38598847, g_loss: 0.69309187\n",
      "Step: [10705] d_loss: 1.38677287, g_loss: 0.69150412\n",
      "Step: [10706] d_loss: 1.38425028, g_loss: 0.69072473\n",
      "Step: [10707] d_loss: 1.38660741, g_loss: 0.69995695\n",
      "Step: [10708] d_loss: 1.38509905, g_loss: 0.69389659\n",
      "Step: [10709] d_loss: 1.38631999, g_loss: 0.69384617\n",
      "Step: [10710] d_loss: 1.38503098, g_loss: 0.69257218\n",
      "Step: [10711] d_loss: 1.38644564, g_loss: 0.69192779\n",
      "Step: [10712] d_loss: 1.38610697, g_loss: 0.69813091\n",
      "Step: [10713] d_loss: 1.38677263, g_loss: 0.69144207\n",
      "Step: [10714] d_loss: 1.38759446, g_loss: 0.69667649\n",
      "Step: [10715] d_loss: 1.38597107, g_loss: 0.69410330\n",
      "Step: [10716] d_loss: 1.38653827, g_loss: 0.69223058\n",
      "Step: [10717] d_loss: 1.38701677, g_loss: 0.69306344\n",
      "Step: [10718] d_loss: 1.38763690, g_loss: 0.69353509\n",
      "Step: [10719] d_loss: 1.38652360, g_loss: 0.69356662\n",
      "Step: [10720] d_loss: 1.38421524, g_loss: 0.69598287\n",
      "Step: [10721] d_loss: 1.38735402, g_loss: 0.69453359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10722] d_loss: 1.38552380, g_loss: 0.69685638\n",
      "Step: [10723] d_loss: 1.38633597, g_loss: 0.69287515\n",
      "Step: [10724] d_loss: 1.38446069, g_loss: 0.69253182\n",
      "Step: [10725] d_loss: 1.38636625, g_loss: 0.69433969\n",
      "Step: [10726] d_loss: 1.38656902, g_loss: 0.69154376\n",
      "Step: [10727] d_loss: 1.38332975, g_loss: 0.69553554\n",
      "Step: [10728] d_loss: 1.38761616, g_loss: 0.69040889\n",
      "Step: [10729] d_loss: 1.38641047, g_loss: 0.69435287\n",
      "Step: [10730] d_loss: 1.38616896, g_loss: 0.69258887\n",
      "Step: [10731] d_loss: 1.38585937, g_loss: 0.69373703\n",
      "Step: [10732] d_loss: 1.38694394, g_loss: 0.69299877\n",
      "Step: [10733] d_loss: 1.38535810, g_loss: 0.69675177\n",
      "Step: [10734] d_loss: 1.38616550, g_loss: 0.69435209\n",
      "Step: [10735] d_loss: 1.38461041, g_loss: 0.69471890\n",
      "Step: [10736] d_loss: 1.38768208, g_loss: 0.69474161\n",
      "Step: [10737] d_loss: 1.38632441, g_loss: 0.69467163\n",
      "Step: [10738] d_loss: 1.38562131, g_loss: 0.69444728\n",
      "Step: [10739] d_loss: 1.38535309, g_loss: 0.69429481\n",
      "Step: [10740] d_loss: 1.38538694, g_loss: 0.69344890\n",
      "Step: [10741] d_loss: 1.38548660, g_loss: 0.69776112\n",
      "Step: [10742] d_loss: 1.38342547, g_loss: 0.69512922\n",
      "Step: [10743] d_loss: 1.38592470, g_loss: 0.69240868\n",
      "Step: [10744] d_loss: 1.38509822, g_loss: 0.69317687\n",
      "Step: [10745] d_loss: 1.38511419, g_loss: 0.69519103\n",
      "Step: [10746] d_loss: 1.38375914, g_loss: 0.69879663\n",
      "Step: [10747] d_loss: 1.38829470, g_loss: 0.68972075\n",
      "Step: [10748] d_loss: 1.38523245, g_loss: 0.69877076\n",
      "Step: [10749] d_loss: 1.38501406, g_loss: 0.69256616\n",
      "Step: [10750] d_loss: 1.38543344, g_loss: 0.69475305\n",
      "Step: [10751] d_loss: 1.38501465, g_loss: 0.69192266\n",
      "Step: [10752] d_loss: 1.38751149, g_loss: 0.69369125\n",
      "Step: [10753] d_loss: 1.38732302, g_loss: 0.69493300\n",
      "Step: [10754] d_loss: 1.38696694, g_loss: 0.69313765\n",
      "Step: [10755] d_loss: 1.38590264, g_loss: 0.69274974\n",
      "Step: [10756] d_loss: 1.38609576, g_loss: 0.69448352\n",
      "Step: [10757] d_loss: 1.38839793, g_loss: 0.69379103\n",
      "Step: [10758] d_loss: 1.38913321, g_loss: 0.69274122\n",
      "Step: [10759] d_loss: 1.38770032, g_loss: 0.69400316\n",
      "Step: [10760] d_loss: 1.38744664, g_loss: 0.69235682\n",
      "Step: [10761] d_loss: 1.38789773, g_loss: 0.69264579\n",
      "Step: [10762] d_loss: 1.38752127, g_loss: 0.69268429\n",
      "Step: [10763] d_loss: 1.38575327, g_loss: 0.69208115\n",
      "Step: [10764] d_loss: 1.38556480, g_loss: 0.69364858\n",
      "Step: [10765] d_loss: 1.38554847, g_loss: 0.69401765\n",
      "Step: [10766] d_loss: 1.38466966, g_loss: 0.69578969\n",
      "Step: [10767] d_loss: 1.38459158, g_loss: 0.69231421\n",
      "Step: [10768] d_loss: 1.38525116, g_loss: 0.69378954\n",
      "Step: [10769] d_loss: 1.38343883, g_loss: 0.69597042\n",
      "Step: [10770] d_loss: 1.38588440, g_loss: 0.69231218\n",
      "Step: [10771] d_loss: 1.38416815, g_loss: 0.69359505\n",
      "Step: [10772] d_loss: 1.38466990, g_loss: 0.69418561\n",
      "Step: [10773] d_loss: 1.38519120, g_loss: 0.69664848\n",
      "Step: [10774] d_loss: 1.38483334, g_loss: 0.69486225\n",
      "Step: [10775] d_loss: 1.38348901, g_loss: 0.69574702\n",
      "Step: [10776] d_loss: 1.38687420, g_loss: 0.69245023\n",
      "Step: [10777] d_loss: 1.38253415, g_loss: 0.69389784\n",
      "Step: [10778] d_loss: 1.38617969, g_loss: 0.69417191\n",
      "Step: [10779] d_loss: 1.38372409, g_loss: 0.69651449\n",
      "Step: [10780] d_loss: 1.38482308, g_loss: 0.69474483\n",
      "Step: [10781] d_loss: 1.38331401, g_loss: 0.69651556\n",
      "Step: [10782] d_loss: 1.38595819, g_loss: 0.69099915\n",
      "Step: [10783] d_loss: 1.38582349, g_loss: 0.69982970\n",
      "Step: [10784] d_loss: 1.38287306, g_loss: 0.69923913\n",
      "Step: [10785] d_loss: 1.39190447, g_loss: 0.69931614\n",
      "Step: [10786] d_loss: 1.39118826, g_loss: 0.68974221\n",
      "Step: [10787] d_loss: 1.38877988, g_loss: 0.70212901\n",
      "Step: [10788] d_loss: 1.38364899, g_loss: 0.69537246\n",
      "Step: [10789] d_loss: 1.38700724, g_loss: 0.69290137\n",
      "Step: [10790] d_loss: 1.38705063, g_loss: 0.69854528\n",
      "Step: [10791] d_loss: 1.38690031, g_loss: 0.69350648\n",
      "Step: [10792] d_loss: 1.39097536, g_loss: 0.69297993\n",
      "Step: [10793] d_loss: 1.38833356, g_loss: 0.69342625\n",
      "Step: [10794] d_loss: 1.38706708, g_loss: 0.69448799\n",
      "Step: [10795] d_loss: 1.38741827, g_loss: 0.69284403\n",
      "Step: [10796] d_loss: 1.38581979, g_loss: 0.69385523\n",
      "Step: [10797] d_loss: 1.38560319, g_loss: 0.69622982\n",
      "Step: [10798] d_loss: 1.38721418, g_loss: 0.69141114\n",
      "Step: [10799] d_loss: 1.38703823, g_loss: 0.69078839\n",
      "Step: [10800] d_loss: 1.38926649, g_loss: 0.69062728\n",
      "Step: [10801] d_loss: 1.38735127, g_loss: 0.69334120\n",
      "Step: [10802] d_loss: 1.38616705, g_loss: 0.69281590\n",
      "Step: [10803] d_loss: 1.38648391, g_loss: 0.69213593\n",
      "Step: [10804] d_loss: 1.38665843, g_loss: 0.69475883\n",
      "Step: [10805] d_loss: 1.38614130, g_loss: 0.69417346\n",
      "Step: [10806] d_loss: 1.38668013, g_loss: 0.69518089\n",
      "Step: [10807] d_loss: 1.38605237, g_loss: 0.69357747\n",
      "Step: [10808] d_loss: 1.38664699, g_loss: 0.69358629\n",
      "Step: [10809] d_loss: 1.38618422, g_loss: 0.69300854\n",
      "Step: [10810] d_loss: 1.38633204, g_loss: 0.69290608\n",
      "Step: [10811] d_loss: 1.38562918, g_loss: 0.69258964\n",
      "Step: [10812] d_loss: 1.38592100, g_loss: 0.69394600\n",
      "Step: [10813] d_loss: 1.38387442, g_loss: 0.69584858\n",
      "Step: [10814] d_loss: 1.38603663, g_loss: 0.69517100\n",
      "Step: [10815] d_loss: 1.38530183, g_loss: 0.69484413\n",
      "Step: [10816] d_loss: 1.38479972, g_loss: 0.69285667\n",
      "Step: [10817] d_loss: 1.38603604, g_loss: 0.69205356\n",
      "Step: [10818] d_loss: 1.38739455, g_loss: 0.69597733\n",
      "Step: [10819] d_loss: 1.38571954, g_loss: 0.69373327\n",
      "Step: [10820] d_loss: 1.38563824, g_loss: 0.69261277\n",
      "Step: [10821] d_loss: 1.38369012, g_loss: 0.69468063\n",
      "Step: [10822] d_loss: 1.38378572, g_loss: 0.69369072\n",
      "Step: [10823] d_loss: 1.38735557, g_loss: 0.69358408\n",
      "Step: [10824] d_loss: 1.38578796, g_loss: 0.69473398\n",
      "Step: [10825] d_loss: 1.38314295, g_loss: 0.69177526\n",
      "Step: [10826] d_loss: 1.38690579, g_loss: 0.69239670\n",
      "Step: [10827] d_loss: 1.38619339, g_loss: 0.69341809\n",
      "Step: [10828] d_loss: 1.38333416, g_loss: 0.69652498\n",
      "Step: [10829] d_loss: 1.38301480, g_loss: 0.69452471\n",
      "Step: [10830] d_loss: 1.38463521, g_loss: 0.69596791\n",
      "Step: [10831] d_loss: 1.38570213, g_loss: 0.69621241\n",
      "Step: [10832] d_loss: 1.38569927, g_loss: 0.69346768\n",
      "Step: [10833] d_loss: 1.38387918, g_loss: 0.69393390\n",
      "Step: [10834] d_loss: 1.38417935, g_loss: 0.69569874\n",
      "Step: [10835] d_loss: 1.38278699, g_loss: 0.69625390\n",
      "Step: [10836] d_loss: 1.38628185, g_loss: 0.69234705\n",
      "Step: [10837] d_loss: 1.38731802, g_loss: 0.69382083\n",
      "Step: [10838] d_loss: 1.38915229, g_loss: 0.70523107\n",
      "Step: [10839] d_loss: 1.39049590, g_loss: 0.69272405\n",
      "Step: [10840] d_loss: 1.39032447, g_loss: 0.69350153\n",
      "Step: [10841] d_loss: 1.38779819, g_loss: 0.69617695\n",
      "Step: [10842] d_loss: 1.38630295, g_loss: 0.69389826\n",
      "Step: [10843] d_loss: 1.38696814, g_loss: 0.69402552\n",
      "Step: [10844] d_loss: 1.38669825, g_loss: 0.69168502\n",
      "Step: [10845] d_loss: 1.38840473, g_loss: 0.69208026\n",
      "Step: [10846] d_loss: 1.38795578, g_loss: 0.69446629\n",
      "Step: [10847] d_loss: 1.38780451, g_loss: 0.69454193\n",
      "Step: [10848] d_loss: 1.38673544, g_loss: 0.69176900\n",
      "Step: [10849] d_loss: 1.38662016, g_loss: 0.69354117\n",
      "Step: [10850] d_loss: 1.38716221, g_loss: 0.69269198\n",
      "Step: [10851] d_loss: 1.38686442, g_loss: 0.69285583\n",
      "Step: [10852] d_loss: 1.38673997, g_loss: 0.69307762\n",
      "Step: [10853] d_loss: 1.38666201, g_loss: 0.69767737\n",
      "Step: [10854] d_loss: 1.38682270, g_loss: 0.69310153\n",
      "Step: [10855] d_loss: 1.38618207, g_loss: 0.69416511\n",
      "Step: [10856] d_loss: 1.38719130, g_loss: 0.69540799\n",
      "Step: [10857] d_loss: 1.38654733, g_loss: 0.69352078\n",
      "Step: [10858] d_loss: 1.38661838, g_loss: 0.69388068\n",
      "Step: [10859] d_loss: 1.38619089, g_loss: 0.69094986\n",
      "Step: [10860] d_loss: 1.38625216, g_loss: 0.69352341\n",
      "Step: [10861] d_loss: 1.38637173, g_loss: 0.69305491\n",
      "Step: [10862] d_loss: 1.38598371, g_loss: 0.69200605\n",
      "Step: [10863] d_loss: 1.38559389, g_loss: 0.69540364\n",
      "Step: [10864] d_loss: 1.38507080, g_loss: 0.69449782\n",
      "Step: [10865] d_loss: 1.38617325, g_loss: 0.69416261\n",
      "Step: [10866] d_loss: 1.38626659, g_loss: 0.69573218\n",
      "Step: [10867] d_loss: 1.38603485, g_loss: 0.69513226\n",
      "Step: [10868] d_loss: 1.38600814, g_loss: 0.69600314\n",
      "Step: [10869] d_loss: 1.38606453, g_loss: 0.69163138\n",
      "Step: [10870] d_loss: 1.38516259, g_loss: 0.69756496\n",
      "Step: [10871] d_loss: 1.38566566, g_loss: 0.69215292\n",
      "Step: [10872] d_loss: 1.38492632, g_loss: 0.69253296\n",
      "Step: [10873] d_loss: 1.38631034, g_loss: 0.69165146\n",
      "Step: [10874] d_loss: 1.38317323, g_loss: 0.69528347\n",
      "Step: [10875] d_loss: 1.38565946, g_loss: 0.69387662\n",
      "Step: [10876] d_loss: 1.38520992, g_loss: 0.69430447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [10877] d_loss: 1.38528752, g_loss: 0.69657409\n",
      "Step: [10878] d_loss: 1.38548899, g_loss: 0.69434661\n",
      "Step: [10879] d_loss: 1.38526940, g_loss: 0.69414592\n",
      "Step: [10880] d_loss: 1.38345432, g_loss: 0.69406474\n",
      "Step: [10881] d_loss: 1.38517714, g_loss: 0.69393361\n",
      "Step: [10882] d_loss: 1.38300741, g_loss: 0.69618219\n",
      "Step: [10883] d_loss: 1.38803840, g_loss: 0.69352102\n",
      "Step: [10884] d_loss: 1.38571906, g_loss: 0.69421148\n",
      "Step: [10885] d_loss: 1.38686526, g_loss: 0.69253826\n",
      "Step: [10886] d_loss: 1.38589478, g_loss: 0.69858921\n",
      "Step: [10887] d_loss: 1.38684547, g_loss: 0.69334376\n",
      "Step: [10888] d_loss: 1.38626695, g_loss: 0.69448507\n",
      "Step: [10889] d_loss: 1.38602638, g_loss: 0.69331646\n",
      "Step: [10890] d_loss: 1.39026964, g_loss: 0.69227076\n",
      "Step: [10891] d_loss: 1.38740468, g_loss: 0.69374222\n",
      "Step: [10892] d_loss: 1.38457167, g_loss: 0.69551051\n",
      "Step: [10893] d_loss: 1.38755322, g_loss: 0.69455546\n",
      "Step: [10894] d_loss: 1.38799739, g_loss: 0.69942939\n",
      "Step: [10895] d_loss: 1.38554132, g_loss: 0.69678402\n",
      "Step: [10896] d_loss: 1.38840699, g_loss: 0.69098854\n",
      "Step: [10897] d_loss: 1.38709760, g_loss: 0.69439036\n",
      "Step: [10898] d_loss: 1.38775718, g_loss: 0.69524789\n",
      "Step: [10899] d_loss: 1.38603067, g_loss: 0.69874358\n",
      "Step: [10900] d_loss: 1.38620520, g_loss: 0.69443309\n",
      "Step: [10901] d_loss: 1.38724434, g_loss: 0.69522613\n",
      "Step: [10902] d_loss: 1.38569427, g_loss: 0.69333506\n",
      "Step: [10903] d_loss: 1.38647175, g_loss: 0.69708395\n",
      "Step: [10904] d_loss: 1.38584590, g_loss: 0.69178164\n",
      "Step: [10905] d_loss: 1.38465095, g_loss: 0.69895536\n",
      "Step: [10906] d_loss: 1.38375592, g_loss: 0.70064163\n",
      "Step: [10907] d_loss: 1.38577688, g_loss: 0.69153523\n",
      "Step: [10908] d_loss: 1.38729537, g_loss: 0.69344938\n",
      "Step: [10909] d_loss: 1.38490033, g_loss: 0.69411874\n",
      "Step: [10910] d_loss: 1.38581038, g_loss: 0.68998039\n",
      "Step: [10911] d_loss: 1.38549638, g_loss: 0.69468093\n",
      "Step: [10912] d_loss: 1.38428140, g_loss: 0.69420230\n",
      "Step: [10913] d_loss: 1.38892221, g_loss: 0.69229269\n",
      "Step: [10914] d_loss: 1.38740158, g_loss: 0.69277179\n",
      "Step: [10915] d_loss: 1.38712347, g_loss: 0.69290376\n",
      "Step: [10916] d_loss: 1.39031935, g_loss: 0.69154263\n",
      "Step: [10917] d_loss: 1.38677645, g_loss: 0.69157946\n",
      "Step: [10918] d_loss: 1.38598883, g_loss: 0.69272399\n",
      "Step: [10919] d_loss: 1.38657308, g_loss: 0.69305491\n",
      "Step: [10920] d_loss: 1.38654280, g_loss: 0.69263691\n",
      "Step: [10921] d_loss: 1.38657737, g_loss: 0.69852901\n",
      "Step: [10922] d_loss: 1.38811541, g_loss: 0.68953395\n",
      "Step: [10923] d_loss: 1.38769817, g_loss: 0.69604063\n",
      "Step: [10924] d_loss: 1.38596344, g_loss: 0.69215369\n",
      "Step: [10925] d_loss: 1.38560057, g_loss: 0.69215029\n",
      "Step: [10926] d_loss: 1.38576996, g_loss: 0.69475877\n",
      "Step: [10927] d_loss: 1.38601351, g_loss: 0.69262183\n",
      "Step: [10928] d_loss: 1.38566160, g_loss: 0.69381678\n",
      "Step: [10929] d_loss: 1.38617218, g_loss: 0.69337052\n",
      "Step: [10930] d_loss: 1.38634658, g_loss: 0.69002968\n",
      "Step: [10931] d_loss: 1.38630915, g_loss: 0.69644177\n",
      "Step: [10932] d_loss: 1.38939905, g_loss: 0.68581676\n",
      "Step: [10933] d_loss: 1.38760495, g_loss: 0.69533551\n",
      "Step: [10934] d_loss: 1.38732576, g_loss: 0.69210958\n",
      "Step: [10935] d_loss: 1.38575768, g_loss: 0.69507819\n",
      "Step: [10936] d_loss: 1.38650775, g_loss: 0.69520587\n",
      "Step: [10937] d_loss: 1.38512838, g_loss: 0.68957138\n",
      "Step: [10938] d_loss: 1.38588071, g_loss: 0.69440258\n",
      "Step: [10939] d_loss: 1.38485098, g_loss: 0.69125217\n",
      "Step: [10940] d_loss: 1.38529277, g_loss: 0.69406378\n",
      "Step: [10941] d_loss: 1.38544631, g_loss: 0.69259572\n",
      "Step: [10942] d_loss: 1.38659632, g_loss: 0.69347692\n",
      "Step: [10943] d_loss: 1.38570321, g_loss: 0.69439757\n",
      "Step: [10944] d_loss: 1.38445151, g_loss: 0.69443953\n",
      "Step: [10945] d_loss: 1.38597107, g_loss: 0.69726992\n",
      "Step: [10946] d_loss: 1.38512516, g_loss: 0.69149387\n",
      "Step: [10947] d_loss: 1.38740468, g_loss: 0.69595432\n",
      "Step: [10948] d_loss: 1.38777685, g_loss: 0.69088554\n",
      "Step: [10949] d_loss: 1.38738132, g_loss: 0.69321454\n",
      "Step: [10950] d_loss: 1.38623309, g_loss: 0.69322169\n",
      "Step: [10951] d_loss: 1.38725567, g_loss: 0.69273698\n",
      "Step: [10952] d_loss: 1.38658452, g_loss: 0.69585800\n",
      "Step: [10953] d_loss: 1.38591743, g_loss: 0.69118869\n",
      "Step: [10954] d_loss: 1.38675523, g_loss: 0.69302905\n",
      "Step: [10955] d_loss: 1.38519526, g_loss: 0.69715905\n",
      "Step: [10956] d_loss: 1.38644218, g_loss: 0.69190037\n",
      "Step: [10957] d_loss: 1.38668895, g_loss: 0.69555652\n",
      "Step: [10958] d_loss: 1.38643515, g_loss: 0.69325948\n",
      "Step: [10959] d_loss: 1.38619447, g_loss: 0.69535238\n",
      "Step: [10960] d_loss: 1.38630724, g_loss: 0.69448829\n",
      "Step: [10961] d_loss: 1.38623428, g_loss: 0.69371533\n",
      "Step: [10962] d_loss: 1.38614392, g_loss: 0.69382787\n",
      "Step: [10963] d_loss: 1.38606286, g_loss: 0.69672060\n",
      "Step: [10964] d_loss: 1.38593340, g_loss: 0.69299281\n",
      "Step: [10965] d_loss: 1.38642979, g_loss: 0.69567829\n",
      "Step: [10966] d_loss: 1.38597846, g_loss: 0.69484079\n",
      "Step: [10967] d_loss: 1.38547826, g_loss: 0.69472307\n",
      "Step: [10968] d_loss: 1.38592505, g_loss: 0.69694698\n",
      "Step: [10969] d_loss: 1.38624740, g_loss: 0.69282544\n",
      "Step: [10970] d_loss: 1.38599062, g_loss: 0.69338512\n",
      "Step: [10971] d_loss: 1.38524556, g_loss: 0.69758272\n",
      "Step: [10972] d_loss: 1.38622284, g_loss: 0.68869257\n",
      "Step: [10973] d_loss: 1.38642263, g_loss: 0.69484198\n",
      "Step: [10974] d_loss: 1.38619304, g_loss: 0.69197273\n",
      "Step: [10975] d_loss: 1.38587952, g_loss: 0.69734621\n",
      "Step: [10976] d_loss: 1.38654315, g_loss: 0.69351023\n",
      "Step: [10977] d_loss: 1.38683450, g_loss: 0.69222236\n",
      "Step: [10978] d_loss: 1.38664460, g_loss: 0.69285715\n",
      "Step: [10979] d_loss: 1.38596141, g_loss: 0.69521356\n",
      "Step: [10980] d_loss: 1.38669205, g_loss: 0.69526345\n",
      "Step: [10981] d_loss: 1.38682306, g_loss: 0.69212067\n",
      "Step: [10982] d_loss: 1.38717055, g_loss: 0.69715679\n",
      "Step: [10983] d_loss: 1.38883328, g_loss: 0.68599635\n",
      "Step: [10984] d_loss: 1.38760495, g_loss: 0.69670165\n",
      "Step: [10985] d_loss: 1.38652623, g_loss: 0.69529879\n",
      "Step: [10986] d_loss: 1.38641596, g_loss: 0.69427526\n",
      "Step: [10987] d_loss: 1.38627756, g_loss: 0.69446498\n",
      "Step: [10988] d_loss: 1.38672686, g_loss: 0.69063735\n",
      "Step: [10989] d_loss: 1.38758004, g_loss: 0.69574761\n",
      "Step: [10990] d_loss: 1.38632703, g_loss: 0.69085240\n",
      "Step: [10991] d_loss: 1.38620245, g_loss: 0.69819057\n",
      "Step: [10992] d_loss: 1.38644552, g_loss: 0.69249249\n",
      "Step: [10993] d_loss: 1.38653219, g_loss: 0.69300842\n",
      "Step: [10994] d_loss: 1.38631356, g_loss: 0.69127607\n",
      "Step: [10995] d_loss: 1.38614726, g_loss: 0.69409406\n",
      "Step: [10996] d_loss: 1.38630927, g_loss: 0.69466347\n",
      "Step: [10997] d_loss: 1.38632941, g_loss: 0.69266391\n",
      "Step: [10998] d_loss: 1.38640022, g_loss: 0.69477600\n",
      "Step: [10999] d_loss: 1.38641167, g_loss: 0.69259238\n",
      "Step: [11000] d_loss: 1.38638663, g_loss: 0.69287670\n",
      "Step: [11001] d_loss: 1.38622117, g_loss: 0.69365817\n",
      "Step: [11002] d_loss: 1.38618052, g_loss: 0.69280922\n",
      "Step: [11003] d_loss: 1.38639843, g_loss: 0.69358540\n",
      "Step: [11004] d_loss: 1.38655639, g_loss: 0.69536740\n",
      "Step: [11005] d_loss: 1.38649678, g_loss: 0.69165719\n",
      "Step: [11006] d_loss: 1.38779676, g_loss: 0.69338977\n",
      "Step: [11007] d_loss: 1.38638556, g_loss: 0.69440925\n",
      "Step: [11008] d_loss: 1.38621950, g_loss: 0.69208670\n",
      "Step: [11009] d_loss: 1.38631272, g_loss: 0.69418895\n",
      "Step: [11010] d_loss: 1.38616586, g_loss: 0.69419593\n",
      "Step: [11011] d_loss: 1.38592720, g_loss: 0.69302994\n",
      "Step: [11012] d_loss: 1.38604832, g_loss: 0.69409633\n",
      "Step: [11013] d_loss: 1.38597059, g_loss: 0.69072711\n",
      "Step: [11014] d_loss: 1.38577795, g_loss: 0.69110781\n",
      "Step: [11015] d_loss: 1.38662517, g_loss: 0.69092512\n",
      "Step: [11016] d_loss: 1.38632596, g_loss: 0.69340557\n",
      "Step: [11017] d_loss: 1.38574219, g_loss: 0.69250160\n",
      "Step: [11018] d_loss: 1.38699746, g_loss: 0.69478238\n",
      "Step: [11019] d_loss: 1.38659930, g_loss: 0.69473791\n",
      "Step: [11020] d_loss: 1.38600254, g_loss: 0.69217795\n",
      "Step: [11021] d_loss: 1.38600707, g_loss: 0.69267607\n",
      "Step: [11022] d_loss: 1.38571286, g_loss: 0.69421798\n",
      "Step: [11023] d_loss: 1.38639092, g_loss: 0.69224179\n",
      "Step: [11024] d_loss: 1.38620794, g_loss: 0.69310939\n",
      "Step: [11025] d_loss: 1.38574600, g_loss: 0.69357562\n",
      "Step: [11026] d_loss: 1.38582492, g_loss: 0.69346452\n",
      "Step: [11027] d_loss: 1.38575995, g_loss: 0.69424915\n",
      "Step: [11028] d_loss: 1.38629651, g_loss: 0.69254792\n",
      "Step: [11029] d_loss: 1.38532078, g_loss: 0.69317532\n",
      "Step: [11030] d_loss: 1.38630629, g_loss: 0.69200838\n",
      "Step: [11031] d_loss: 1.38598144, g_loss: 0.69565499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11032] d_loss: 1.38583302, g_loss: 0.69215018\n",
      "Step: [11033] d_loss: 1.38671076, g_loss: 0.69787490\n",
      "Step: [11034] d_loss: 1.38658297, g_loss: 0.69145167\n",
      "Step: [11035] d_loss: 1.38591135, g_loss: 0.69367182\n",
      "Step: [11036] d_loss: 1.38620591, g_loss: 0.69359779\n",
      "Step: [11037] d_loss: 1.38568246, g_loss: 0.69543862\n",
      "Step: [11038] d_loss: 1.38524961, g_loss: 0.69283962\n",
      "Step: [11039] d_loss: 1.38623142, g_loss: 0.69515747\n",
      "Step: [11040] d_loss: 1.38647604, g_loss: 0.69288546\n",
      "Step: [11041] d_loss: 1.38523316, g_loss: 0.69374168\n",
      "Step: [11042] d_loss: 1.38698721, g_loss: 0.69483781\n",
      "Step: [11043] d_loss: 1.38647985, g_loss: 0.69355071\n",
      "Step: [11044] d_loss: 1.38625145, g_loss: 0.69311059\n",
      "Step: [11045] d_loss: 1.38665223, g_loss: 0.69344330\n",
      "Step: [11046] d_loss: 1.38619900, g_loss: 0.69498914\n",
      "Step: [11047] d_loss: 1.38620257, g_loss: 0.69536281\n",
      "Step: [11048] d_loss: 1.38578117, g_loss: 0.69169790\n",
      "Step: [11049] d_loss: 1.38619328, g_loss: 0.69116181\n",
      "Step: [11050] d_loss: 1.38658631, g_loss: 0.69133949\n",
      "Step: [11051] d_loss: 1.38608885, g_loss: 0.69459099\n",
      "Step: [11052] d_loss: 1.38620067, g_loss: 0.69320029\n",
      "Step: [11053] d_loss: 1.38653350, g_loss: 0.69594288\n",
      "Step: [11054] d_loss: 1.38640702, g_loss: 0.69221431\n",
      "Step: [11055] d_loss: 1.38637853, g_loss: 0.69415563\n",
      "Step: [11056] d_loss: 1.38592386, g_loss: 0.69318992\n",
      "Step: [11057] d_loss: 1.38644540, g_loss: 0.69327289\n",
      "Step: [11058] d_loss: 1.38611412, g_loss: 0.69295114\n",
      "Step: [11059] d_loss: 1.38631845, g_loss: 0.69502640\n",
      "Step: [11060] d_loss: 1.38619494, g_loss: 0.69284880\n",
      "Step: [11061] d_loss: 1.38692558, g_loss: 0.69405341\n",
      "Step: [11062] d_loss: 1.38678789, g_loss: 0.69124830\n",
      "Step: [11063] d_loss: 1.38722646, g_loss: 0.69520813\n",
      "Step: [11064] d_loss: 1.38628387, g_loss: 0.69258583\n",
      "Step: [11065] d_loss: 1.38654828, g_loss: 0.69275320\n",
      "Step: [11066] d_loss: 1.38598895, g_loss: 0.69341326\n",
      "Step: [11067] d_loss: 1.38644314, g_loss: 0.69338286\n",
      "Step: [11068] d_loss: 1.38683665, g_loss: 0.69310856\n",
      "Step: [11069] d_loss: 1.38662410, g_loss: 0.69284743\n",
      "Step: [11070] d_loss: 1.38681030, g_loss: 0.69200891\n",
      "Step: [11071] d_loss: 1.38591671, g_loss: 0.69172126\n",
      "Step: [11072] d_loss: 1.38619876, g_loss: 0.69297564\n",
      "Step: [11073] d_loss: 1.38641524, g_loss: 0.69658172\n",
      "Step: [11074] d_loss: 1.38692284, g_loss: 0.69070530\n",
      "Step: [11075] d_loss: 1.38705432, g_loss: 0.69699991\n",
      "Step: [11076] d_loss: 1.38748872, g_loss: 0.69168198\n",
      "Step: [11077] d_loss: 1.38710022, g_loss: 0.69721735\n",
      "Step: [11078] d_loss: 1.38596034, g_loss: 0.69433624\n",
      "Step: [11079] d_loss: 1.38579774, g_loss: 0.69276702\n",
      "Step: [11080] d_loss: 1.38679028, g_loss: 0.69113648\n",
      "Step: [11081] d_loss: 1.38621664, g_loss: 0.69359410\n",
      "Step: [11082] d_loss: 1.38713241, g_loss: 0.69280660\n",
      "Step: [11083] d_loss: 1.38613677, g_loss: 0.69378340\n",
      "Step: [11084] d_loss: 1.38606954, g_loss: 0.69356865\n",
      "Step: [11085] d_loss: 1.38492155, g_loss: 0.69881696\n",
      "Step: [11086] d_loss: 1.38651276, g_loss: 0.69420874\n",
      "Step: [11087] d_loss: 1.38655782, g_loss: 0.69191110\n",
      "Step: [11088] d_loss: 1.38662016, g_loss: 0.69261944\n",
      "Step: [11089] d_loss: 1.38680434, g_loss: 0.69362658\n",
      "Step: [11090] d_loss: 1.38604057, g_loss: 0.69418472\n",
      "Step: [11091] d_loss: 1.38578355, g_loss: 0.69425225\n",
      "Step: [11092] d_loss: 1.38580799, g_loss: 0.69336867\n",
      "Step: [11093] d_loss: 1.38581216, g_loss: 0.69524670\n",
      "Step: [11094] d_loss: 1.38636565, g_loss: 0.69238889\n",
      "Step: [11095] d_loss: 1.38618302, g_loss: 0.69461840\n",
      "Step: [11096] d_loss: 1.38649559, g_loss: 0.69331366\n",
      "Step: [11097] d_loss: 1.38660324, g_loss: 0.69404674\n",
      "Step: [11098] d_loss: 1.38574624, g_loss: 0.69406939\n",
      "Step: [11099] d_loss: 1.38565946, g_loss: 0.69233352\n",
      "Step: [11100] d_loss: 1.38786554, g_loss: 0.69722629\n",
      "Step: [11101] d_loss: 1.38749504, g_loss: 0.68994081\n",
      "Step: [11102] d_loss: 1.38711548, g_loss: 0.69114530\n",
      "Step: [11103] d_loss: 1.38634241, g_loss: 0.69130486\n",
      "Step: [11104] d_loss: 1.38616657, g_loss: 0.69204617\n",
      "Step: [11105] d_loss: 1.38659978, g_loss: 0.69330800\n",
      "Step: [11106] d_loss: 1.38575780, g_loss: 0.69357276\n",
      "Step: [11107] d_loss: 1.38633227, g_loss: 0.69401288\n",
      "Step: [11108] d_loss: 1.38677907, g_loss: 0.69329095\n",
      "Step: [11109] d_loss: 1.38617754, g_loss: 0.69383383\n",
      "Step: [11110] d_loss: 1.38609266, g_loss: 0.69301641\n",
      "Step: [11111] d_loss: 1.38636029, g_loss: 0.69268286\n",
      "Step: [11112] d_loss: 1.38696098, g_loss: 0.69329774\n",
      "Step: [11113] d_loss: 1.38616514, g_loss: 0.69302881\n",
      "Step: [11114] d_loss: 1.38638389, g_loss: 0.69324011\n",
      "Step: [11115] d_loss: 1.38675141, g_loss: 0.69300938\n",
      "Step: [11116] d_loss: 1.38616037, g_loss: 0.69248998\n",
      "Step: [11117] d_loss: 1.38592350, g_loss: 0.69302332\n",
      "Step: [11118] d_loss: 1.38594019, g_loss: 0.69490504\n",
      "Step: [11119] d_loss: 1.38636017, g_loss: 0.69287467\n",
      "Step: [11120] d_loss: 1.38624907, g_loss: 0.69278860\n",
      "Step: [11121] d_loss: 1.38622081, g_loss: 0.69098425\n",
      "Step: [11122] d_loss: 1.38598955, g_loss: 0.69266254\n",
      "Step: [11123] d_loss: 1.38620245, g_loss: 0.69250166\n",
      "Step: [11124] d_loss: 1.38637507, g_loss: 0.69377017\n",
      "Step: [11125] d_loss: 1.38575244, g_loss: 0.69328386\n",
      "Step: [11126] d_loss: 1.38630152, g_loss: 0.69406790\n",
      "Step: [11127] d_loss: 1.38636744, g_loss: 0.69007194\n",
      "Step: [11128] d_loss: 1.38628280, g_loss: 0.69300133\n",
      "Step: [11129] d_loss: 1.38622177, g_loss: 0.69363803\n",
      "Step: [11130] d_loss: 1.38589644, g_loss: 0.69378215\n",
      "Step: [11131] d_loss: 1.38593554, g_loss: 0.69478297\n",
      "Step: [11132] d_loss: 1.38569582, g_loss: 0.69241166\n",
      "Step: [11133] d_loss: 1.38589573, g_loss: 0.69335628\n",
      "Step: [11134] d_loss: 1.38624787, g_loss: 0.69435465\n",
      "Step: [11135] d_loss: 1.38608754, g_loss: 0.69205236\n",
      "Step: [11136] d_loss: 1.38626957, g_loss: 0.69289649\n",
      "Step: [11137] d_loss: 1.38579774, g_loss: 0.69277090\n",
      "Step: [11138] d_loss: 1.38604391, g_loss: 0.69357228\n",
      "Step: [11139] d_loss: 1.38608623, g_loss: 0.69385582\n",
      "Step: [11140] d_loss: 1.38593078, g_loss: 0.69358766\n",
      "Step: [11141] d_loss: 1.38566339, g_loss: 0.69233054\n",
      "Step: [11142] d_loss: 1.38613510, g_loss: 0.69362193\n",
      "Step: [11143] d_loss: 1.38656998, g_loss: 0.69384813\n",
      "Step: [11144] d_loss: 1.38644409, g_loss: 0.69381523\n",
      "Step: [11145] d_loss: 1.38617313, g_loss: 0.69630694\n",
      "Step: [11146] d_loss: 1.38596094, g_loss: 0.69328707\n",
      "Step: [11147] d_loss: 1.38731968, g_loss: 0.69892949\n",
      "Step: [11148] d_loss: 1.39375448, g_loss: 0.69378465\n",
      "Step: [11149] d_loss: 1.39935040, g_loss: 0.70441526\n",
      "Step: [11150] d_loss: 1.39051282, g_loss: 0.69343412\n",
      "Step: [11151] d_loss: 1.38620234, g_loss: 0.68723309\n",
      "Step: [11152] d_loss: 1.38772428, g_loss: 0.69491792\n",
      "Step: [11153] d_loss: 1.38798881, g_loss: 0.69217408\n",
      "Step: [11154] d_loss: 1.38705230, g_loss: 0.69781780\n",
      "Step: [11155] d_loss: 1.38648438, g_loss: 0.69412273\n",
      "Step: [11156] d_loss: 1.38661313, g_loss: 0.68993562\n",
      "Step: [11157] d_loss: 1.38705063, g_loss: 0.69466996\n",
      "Step: [11158] d_loss: 1.38619053, g_loss: 0.69409215\n",
      "Step: [11159] d_loss: 1.38638926, g_loss: 0.69481981\n",
      "Step: [11160] d_loss: 1.38632894, g_loss: 0.69300520\n",
      "Step: [11161] d_loss: 1.38625681, g_loss: 0.69460964\n",
      "Step: [11162] d_loss: 1.38623023, g_loss: 0.69419086\n",
      "Step: [11163] d_loss: 1.38598454, g_loss: 0.69398063\n",
      "Step: [11164] d_loss: 1.38584185, g_loss: 0.69812393\n",
      "Step: [11165] d_loss: 1.38615489, g_loss: 0.69089961\n",
      "Step: [11166] d_loss: 1.38666236, g_loss: 0.69368446\n",
      "Step: [11167] d_loss: 1.38681841, g_loss: 0.69315183\n",
      "Step: [11168] d_loss: 1.38624597, g_loss: 0.69429767\n",
      "Step: [11169] d_loss: 1.38617754, g_loss: 0.69566309\n",
      "Step: [11170] d_loss: 1.38596010, g_loss: 0.69377023\n",
      "Step: [11171] d_loss: 1.38658333, g_loss: 0.69586188\n",
      "Step: [11172] d_loss: 1.38682330, g_loss: 0.69212312\n",
      "Step: [11173] d_loss: 1.38653266, g_loss: 0.69302642\n",
      "Step: [11174] d_loss: 1.38608003, g_loss: 0.69323337\n",
      "Step: [11175] d_loss: 1.38644814, g_loss: 0.69300801\n",
      "Step: [11176] d_loss: 1.38615489, g_loss: 0.69321525\n",
      "Step: [11177] d_loss: 1.38614464, g_loss: 0.69378793\n",
      "Step: [11178] d_loss: 1.38618016, g_loss: 0.69324762\n",
      "Step: [11179] d_loss: 1.38614762, g_loss: 0.69290757\n",
      "Step: [11180] d_loss: 1.38615727, g_loss: 0.69254375\n",
      "Step: [11181] d_loss: 1.38778853, g_loss: 0.69247323\n",
      "Step: [11182] d_loss: 1.38610435, g_loss: 0.69474363\n",
      "Step: [11183] d_loss: 1.38620424, g_loss: 0.69429564\n",
      "Step: [11184] d_loss: 1.38642740, g_loss: 0.69235337\n",
      "Step: [11185] d_loss: 1.38639140, g_loss: 0.69338071\n",
      "Step: [11186] d_loss: 1.38654613, g_loss: 0.69323486\n",
      "Step: [11187] d_loss: 1.38646507, g_loss: 0.69377923\n",
      "Step: [11188] d_loss: 1.38650596, g_loss: 0.69439650\n",
      "Step: [11189] d_loss: 1.38591909, g_loss: 0.69367206\n",
      "Step: [11190] d_loss: 1.38637888, g_loss: 0.69415855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11191] d_loss: 1.38615155, g_loss: 0.69379479\n",
      "Step: [11192] d_loss: 1.38666451, g_loss: 0.69137144\n",
      "Step: [11193] d_loss: 1.38672209, g_loss: 0.69365203\n",
      "Step: [11194] d_loss: 1.38616812, g_loss: 0.69348001\n",
      "Step: [11195] d_loss: 1.38604736, g_loss: 0.69310093\n",
      "Step: [11196] d_loss: 1.38614726, g_loss: 0.69426358\n",
      "Step: [11197] d_loss: 1.38681006, g_loss: 0.69385988\n",
      "Step: [11198] d_loss: 1.38626528, g_loss: 0.69335979\n",
      "Step: [11199] d_loss: 1.38613498, g_loss: 0.69205558\n",
      "Step: [11200] d_loss: 1.38626027, g_loss: 0.69369358\n",
      "Step: [11201] d_loss: 1.38630486, g_loss: 0.69550312\n",
      "Step: [11202] d_loss: 1.38626730, g_loss: 0.69286001\n",
      "Step: [11203] d_loss: 1.38643479, g_loss: 0.69369513\n",
      "Step: [11204] d_loss: 1.38598573, g_loss: 0.69355929\n",
      "Step: [11205] d_loss: 1.38636243, g_loss: 0.69244826\n",
      "Step: [11206] d_loss: 1.38634825, g_loss: 0.69280505\n",
      "Step: [11207] d_loss: 1.38610685, g_loss: 0.69402337\n",
      "Step: [11208] d_loss: 1.38633060, g_loss: 0.69353241\n",
      "Step: [11209] d_loss: 1.38644052, g_loss: 0.69398832\n",
      "Step: [11210] d_loss: 1.38596225, g_loss: 0.69417048\n",
      "Step: [11211] d_loss: 1.38606429, g_loss: 0.69178963\n",
      "Step: [11212] d_loss: 1.38618755, g_loss: 0.69371355\n",
      "Step: [11213] d_loss: 1.38592207, g_loss: 0.69347632\n",
      "Step: [11214] d_loss: 1.38608968, g_loss: 0.69402409\n",
      "Step: [11215] d_loss: 1.38635349, g_loss: 0.69321865\n",
      "Step: [11216] d_loss: 1.38621068, g_loss: 0.69251239\n",
      "Step: [11217] d_loss: 1.38585043, g_loss: 0.69359428\n",
      "Step: [11218] d_loss: 1.38571548, g_loss: 0.69546819\n",
      "Step: [11219] d_loss: 1.38595676, g_loss: 0.69595307\n",
      "Step: [11220] d_loss: 1.38547552, g_loss: 0.69474095\n",
      "Step: [11221] d_loss: 1.38614964, g_loss: 0.69376373\n",
      "Step: [11222] d_loss: 1.38598824, g_loss: 0.69193864\n",
      "Step: [11223] d_loss: 1.38664389, g_loss: 0.69327974\n",
      "Step: [11224] d_loss: 1.38717484, g_loss: 0.69126022\n",
      "Step: [11225] d_loss: 1.38650680, g_loss: 0.69305623\n",
      "Step: [11226] d_loss: 1.38570976, g_loss: 0.69281638\n",
      "Step: [11227] d_loss: 1.38633347, g_loss: 0.69225991\n",
      "Step: [11228] d_loss: 1.38640416, g_loss: 0.69284570\n",
      "Step: [11229] d_loss: 1.38620329, g_loss: 0.69363636\n",
      "Step: [11230] d_loss: 1.38586354, g_loss: 0.69365418\n",
      "Step: [11231] d_loss: 1.38606071, g_loss: 0.69305778\n",
      "Step: [11232] d_loss: 1.38682044, g_loss: 0.69366443\n",
      "Step: [11233] d_loss: 1.38598001, g_loss: 0.69526052\n",
      "Step: [11234] d_loss: 1.38661194, g_loss: 0.69334352\n",
      "Step: [11235] d_loss: 1.38619483, g_loss: 0.69531447\n",
      "Step: [11236] d_loss: 1.38632703, g_loss: 0.69468594\n",
      "Step: [11237] d_loss: 1.38617432, g_loss: 0.69297409\n",
      "Step: [11238] d_loss: 1.38632202, g_loss: 0.69239217\n",
      "Step: [11239] d_loss: 1.38623548, g_loss: 0.69308227\n",
      "Step: [11240] d_loss: 1.38633871, g_loss: 0.69524050\n",
      "Step: [11241] d_loss: 1.38641548, g_loss: 0.69286931\n",
      "Step: [11242] d_loss: 1.38604534, g_loss: 0.69273067\n",
      "Step: [11243] d_loss: 1.38593102, g_loss: 0.69204444\n",
      "Step: [11244] d_loss: 1.38594759, g_loss: 0.69291270\n",
      "Step: [11245] d_loss: 1.38647985, g_loss: 0.69411218\n",
      "Step: [11246] d_loss: 1.38646352, g_loss: 0.69344360\n",
      "Step: [11247] d_loss: 1.38609290, g_loss: 0.69350809\n",
      "Step: [11248] d_loss: 1.38614511, g_loss: 0.69252360\n",
      "Step: [11249] d_loss: 1.38622963, g_loss: 0.69332236\n",
      "Step: [11250] d_loss: 1.38644052, g_loss: 0.69188643\n",
      "Step: [11251] d_loss: 1.38655853, g_loss: 0.69356966\n",
      "Step: [11252] d_loss: 1.38624394, g_loss: 0.69417310\n",
      "Step: [11253] d_loss: 1.38611388, g_loss: 0.69330418\n",
      "Step: [11254] d_loss: 1.38607430, g_loss: 0.69326401\n",
      "Step: [11255] d_loss: 1.38620496, g_loss: 0.69396329\n",
      "Step: [11256] d_loss: 1.38612294, g_loss: 0.69301057\n",
      "Step: [11257] d_loss: 1.38621581, g_loss: 0.69266272\n",
      "Step: [11258] d_loss: 1.38626623, g_loss: 0.69256985\n",
      "Step: [11259] d_loss: 1.38648605, g_loss: 0.69353420\n",
      "Step: [11260] d_loss: 1.38610196, g_loss: 0.69323641\n",
      "Step: [11261] d_loss: 1.38589048, g_loss: 0.69464242\n",
      "Step: [11262] d_loss: 1.38628352, g_loss: 0.69649005\n",
      "Step: [11263] d_loss: 1.38659048, g_loss: 0.69393033\n",
      "Step: [11264] d_loss: 1.38678575, g_loss: 0.69539165\n",
      "Step: [11265] d_loss: 1.38685215, g_loss: 0.69100666\n",
      "Step: [11266] d_loss: 1.38638842, g_loss: 0.69138145\n",
      "Step: [11267] d_loss: 1.38592398, g_loss: 0.69107997\n",
      "Step: [11268] d_loss: 1.38631964, g_loss: 0.69320166\n",
      "Step: [11269] d_loss: 1.38599575, g_loss: 0.69370294\n",
      "Step: [11270] d_loss: 1.38567734, g_loss: 0.69310343\n",
      "Step: [11271] d_loss: 1.38651693, g_loss: 0.69471383\n",
      "Step: [11272] d_loss: 1.38612258, g_loss: 0.69338942\n",
      "Step: [11273] d_loss: 1.38621426, g_loss: 0.69246584\n",
      "Step: [11274] d_loss: 1.38625526, g_loss: 0.69238698\n",
      "Step: [11275] d_loss: 1.38643396, g_loss: 0.69298255\n",
      "Step: [11276] d_loss: 1.38657904, g_loss: 0.69288903\n",
      "Step: [11277] d_loss: 1.38642144, g_loss: 0.69354010\n",
      "Step: [11278] d_loss: 1.38638902, g_loss: 0.69351667\n",
      "Step: [11279] d_loss: 1.38619304, g_loss: 0.69393992\n",
      "Step: [11280] d_loss: 1.38646543, g_loss: 0.69256520\n",
      "Step: [11281] d_loss: 1.38590908, g_loss: 0.69394410\n",
      "Step: [11282] d_loss: 1.38626170, g_loss: 0.69350094\n",
      "Step: [11283] d_loss: 1.38598096, g_loss: 0.69269454\n",
      "Step: [11284] d_loss: 1.38621831, g_loss: 0.69315523\n",
      "Step: [11285] d_loss: 1.38615704, g_loss: 0.69367510\n",
      "Step: [11286] d_loss: 1.38627601, g_loss: 0.69332671\n",
      "Step: [11287] d_loss: 1.38627779, g_loss: 0.69436324\n",
      "Step: [11288] d_loss: 1.38637519, g_loss: 0.69238877\n",
      "Step: [11289] d_loss: 1.38657176, g_loss: 0.69414687\n",
      "Step: [11290] d_loss: 1.38665640, g_loss: 0.69355094\n",
      "Step: [11291] d_loss: 1.38607574, g_loss: 0.69296759\n",
      "Step: [11292] d_loss: 1.38656044, g_loss: 0.69347894\n",
      "Step: [11293] d_loss: 1.38632417, g_loss: 0.69307971\n",
      "Step: [11294] d_loss: 1.38645291, g_loss: 0.69258010\n",
      "Step: [11295] d_loss: 1.38589501, g_loss: 0.69338822\n",
      "Step: [11296] d_loss: 1.38601911, g_loss: 0.69328207\n",
      "Step: [11297] d_loss: 1.38702524, g_loss: 0.69196111\n",
      "Step: [11298] d_loss: 1.38600445, g_loss: 0.69442940\n",
      "Step: [11299] d_loss: 1.38646722, g_loss: 0.69353044\n",
      "Step: [11300] d_loss: 1.38633442, g_loss: 0.69352448\n",
      "Step: [11301] d_loss: 1.38671994, g_loss: 0.69492054\n",
      "Step: [11302] d_loss: 1.38782609, g_loss: 0.68963921\n",
      "Step: [11303] d_loss: 1.38638878, g_loss: 0.69439316\n",
      "Step: [11304] d_loss: 1.38632107, g_loss: 0.69212580\n",
      "Step: [11305] d_loss: 1.38629878, g_loss: 0.69231522\n",
      "Step: [11306] d_loss: 1.38616443, g_loss: 0.69326520\n",
      "Step: [11307] d_loss: 1.38611746, g_loss: 0.69335562\n",
      "Step: [11308] d_loss: 1.38619208, g_loss: 0.69375521\n",
      "Step: [11309] d_loss: 1.38652086, g_loss: 0.69269437\n",
      "Step: [11310] d_loss: 1.38623476, g_loss: 0.69295657\n",
      "Step: [11311] d_loss: 1.38634729, g_loss: 0.69353312\n",
      "Step: [11312] d_loss: 1.38664627, g_loss: 0.69308424\n",
      "Step: [11313] d_loss: 1.38594115, g_loss: 0.69288802\n",
      "Step: [11314] d_loss: 1.38660860, g_loss: 0.69327551\n",
      "Step: [11315] d_loss: 1.38649201, g_loss: 0.69362104\n",
      "Step: [11316] d_loss: 1.38621008, g_loss: 0.69412780\n",
      "Step: [11317] d_loss: 1.38616276, g_loss: 0.69240284\n",
      "Step: [11318] d_loss: 1.38623989, g_loss: 0.69373894\n",
      "Step: [11319] d_loss: 1.38627720, g_loss: 0.69345307\n",
      "Step: [11320] d_loss: 1.38642979, g_loss: 0.69236481\n",
      "Step: [11321] d_loss: 1.38601637, g_loss: 0.69243306\n",
      "Step: [11322] d_loss: 1.38673925, g_loss: 0.69253683\n",
      "Step: [11323] d_loss: 1.38608384, g_loss: 0.69433254\n",
      "Step: [11324] d_loss: 1.38583374, g_loss: 0.69422305\n",
      "Step: [11325] d_loss: 1.38619637, g_loss: 0.69263053\n",
      "Step: [11326] d_loss: 1.38637280, g_loss: 0.69275749\n",
      "Step: [11327] d_loss: 1.38617754, g_loss: 0.69330132\n",
      "Step: [11328] d_loss: 1.38630831, g_loss: 0.69353390\n",
      "Step: [11329] d_loss: 1.38623643, g_loss: 0.69477564\n",
      "Step: [11330] d_loss: 1.38646245, g_loss: 0.69263643\n",
      "Step: [11331] d_loss: 1.38634574, g_loss: 0.69276559\n",
      "Step: [11332] d_loss: 1.38627219, g_loss: 0.69053924\n",
      "Step: [11333] d_loss: 1.38603020, g_loss: 0.69184637\n",
      "Step: [11334] d_loss: 1.38605666, g_loss: 0.69401962\n",
      "Step: [11335] d_loss: 1.38615131, g_loss: 0.69429332\n",
      "Step: [11336] d_loss: 1.38618231, g_loss: 0.69369340\n",
      "Step: [11337] d_loss: 1.38612580, g_loss: 0.69389999\n",
      "Step: [11338] d_loss: 1.38579321, g_loss: 0.69379336\n",
      "Step: [11339] d_loss: 1.38579249, g_loss: 0.69373155\n",
      "Step: [11340] d_loss: 1.38617945, g_loss: 0.69350916\n",
      "Step: [11341] d_loss: 1.38622093, g_loss: 0.69317281\n",
      "Step: [11342] d_loss: 1.38633966, g_loss: 0.69288027\n",
      "Step: [11343] d_loss: 1.38550782, g_loss: 0.69350338\n",
      "Step: [11344] d_loss: 1.38583231, g_loss: 0.69364631\n",
      "Step: [11345] d_loss: 1.38559675, g_loss: 0.69471955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11346] d_loss: 1.38650596, g_loss: 0.69389009\n",
      "Step: [11347] d_loss: 1.38609791, g_loss: 0.69352233\n",
      "Step: [11348] d_loss: 1.38590479, g_loss: 0.69374925\n",
      "Step: [11349] d_loss: 1.38623333, g_loss: 0.69393289\n",
      "Step: [11350] d_loss: 1.38648248, g_loss: 0.69293404\n",
      "Step: [11351] d_loss: 1.38657534, g_loss: 0.69214737\n",
      "Step: [11352] d_loss: 1.38644481, g_loss: 0.69246268\n",
      "Step: [11353] d_loss: 1.38626349, g_loss: 0.69160396\n",
      "Step: [11354] d_loss: 1.38671827, g_loss: 0.69686961\n",
      "Step: [11355] d_loss: 1.38703477, g_loss: 0.69227862\n",
      "Step: [11356] d_loss: 1.38631570, g_loss: 0.69303346\n",
      "Step: [11357] d_loss: 1.38594687, g_loss: 0.69350767\n",
      "Step: [11358] d_loss: 1.38684666, g_loss: 0.69342709\n",
      "Step: [11359] d_loss: 1.38609076, g_loss: 0.69361222\n",
      "Step: [11360] d_loss: 1.38613999, g_loss: 0.69311249\n",
      "Step: [11361] d_loss: 1.38611186, g_loss: 0.69180632\n",
      "Step: [11362] d_loss: 1.38622499, g_loss: 0.69647688\n",
      "Step: [11363] d_loss: 1.38711882, g_loss: 0.69299519\n",
      "Step: [11364] d_loss: 1.38748252, g_loss: 0.69750607\n",
      "Step: [11365] d_loss: 1.38620341, g_loss: 0.69382113\n",
      "Step: [11366] d_loss: 1.38655066, g_loss: 0.69453657\n",
      "Step: [11367] d_loss: 1.38674545, g_loss: 0.69486713\n",
      "Step: [11368] d_loss: 1.38789129, g_loss: 0.68821073\n",
      "Step: [11369] d_loss: 1.38760400, g_loss: 0.69445950\n",
      "Step: [11370] d_loss: 1.38694763, g_loss: 0.69267452\n",
      "Step: [11371] d_loss: 1.38636887, g_loss: 0.69280052\n",
      "Step: [11372] d_loss: 1.38596058, g_loss: 0.69349605\n",
      "Step: [11373] d_loss: 1.38655901, g_loss: 0.69251531\n",
      "Step: [11374] d_loss: 1.38631487, g_loss: 0.69474548\n",
      "Step: [11375] d_loss: 1.38623476, g_loss: 0.69214010\n",
      "Step: [11376] d_loss: 1.38622487, g_loss: 0.69394159\n",
      "Step: [11377] d_loss: 1.38621259, g_loss: 0.69389117\n",
      "Step: [11378] d_loss: 1.38601398, g_loss: 0.69348437\n",
      "Step: [11379] d_loss: 1.38602674, g_loss: 0.69498861\n",
      "Step: [11380] d_loss: 1.38691401, g_loss: 0.69262290\n",
      "Step: [11381] d_loss: 1.38667083, g_loss: 0.69583118\n",
      "Step: [11382] d_loss: 1.38638020, g_loss: 0.69463813\n",
      "Step: [11383] d_loss: 1.38631082, g_loss: 0.69309556\n",
      "Step: [11384] d_loss: 1.38598371, g_loss: 0.69329762\n",
      "Step: [11385] d_loss: 1.38640594, g_loss: 0.69444036\n",
      "Step: [11386] d_loss: 1.38608289, g_loss: 0.69163966\n",
      "Step: [11387] d_loss: 1.38627625, g_loss: 0.69436204\n",
      "Step: [11388] d_loss: 1.38655746, g_loss: 0.69132233\n",
      "Step: [11389] d_loss: 1.38584876, g_loss: 0.69447267\n",
      "Step: [11390] d_loss: 1.38611948, g_loss: 0.69457197\n",
      "Step: [11391] d_loss: 1.38691986, g_loss: 0.69104815\n",
      "Step: [11392] d_loss: 1.38712299, g_loss: 0.69338220\n",
      "Step: [11393] d_loss: 1.38664412, g_loss: 0.69304359\n",
      "Step: [11394] d_loss: 1.38683152, g_loss: 0.69428116\n",
      "Step: [11395] d_loss: 1.38610721, g_loss: 0.69460255\n",
      "Step: [11396] d_loss: 1.38624144, g_loss: 0.69059443\n",
      "Step: [11397] d_loss: 1.38692689, g_loss: 0.69725102\n",
      "Step: [11398] d_loss: 1.38663638, g_loss: 0.69305301\n",
      "Step: [11399] d_loss: 1.38637650, g_loss: 0.69293344\n",
      "Step: [11400] d_loss: 1.38617504, g_loss: 0.69191968\n",
      "Step: [11401] d_loss: 1.38607955, g_loss: 0.69246680\n",
      "Step: [11402] d_loss: 1.38611507, g_loss: 0.69325465\n",
      "Step: [11403] d_loss: 1.38632095, g_loss: 0.69099170\n",
      "Step: [11404] d_loss: 1.38618731, g_loss: 0.69476986\n",
      "Step: [11405] d_loss: 1.38623631, g_loss: 0.69416261\n",
      "Step: [11406] d_loss: 1.38632429, g_loss: 0.69344258\n",
      "Step: [11407] d_loss: 1.38599789, g_loss: 0.69324493\n",
      "Step: [11408] d_loss: 1.38602853, g_loss: 0.69231051\n",
      "Step: [11409] d_loss: 1.38614738, g_loss: 0.69409323\n",
      "Step: [11410] d_loss: 1.38646138, g_loss: 0.69341338\n",
      "Step: [11411] d_loss: 1.38613892, g_loss: 0.69346887\n",
      "Step: [11412] d_loss: 1.38603282, g_loss: 0.69303596\n",
      "Step: [11413] d_loss: 1.38597536, g_loss: 0.69277364\n",
      "Step: [11414] d_loss: 1.38622522, g_loss: 0.69141424\n",
      "Step: [11415] d_loss: 1.38624704, g_loss: 0.69136608\n",
      "Step: [11416] d_loss: 1.38638556, g_loss: 0.69428396\n",
      "Step: [11417] d_loss: 1.38642883, g_loss: 0.69148040\n",
      "Step: [11418] d_loss: 1.38606048, g_loss: 0.69407648\n",
      "Step: [11419] d_loss: 1.38634562, g_loss: 0.69515413\n",
      "Step: [11420] d_loss: 1.38630939, g_loss: 0.69455045\n",
      "Step: [11421] d_loss: 1.38644600, g_loss: 0.69450641\n",
      "Step: [11422] d_loss: 1.38592792, g_loss: 0.69557494\n",
      "Step: [11423] d_loss: 1.38631809, g_loss: 0.69276130\n",
      "Step: [11424] d_loss: 1.38663197, g_loss: 0.69526064\n",
      "Step: [11425] d_loss: 1.38641095, g_loss: 0.69233781\n",
      "Step: [11426] d_loss: 1.38634944, g_loss: 0.69203293\n",
      "Step: [11427] d_loss: 1.38646579, g_loss: 0.69353223\n",
      "Step: [11428] d_loss: 1.38648880, g_loss: 0.69294477\n",
      "Step: [11429] d_loss: 1.38619494, g_loss: 0.69431669\n",
      "Step: [11430] d_loss: 1.38587117, g_loss: 0.69432628\n",
      "Step: [11431] d_loss: 1.38597202, g_loss: 0.69444215\n",
      "Step: [11432] d_loss: 1.38614655, g_loss: 0.69393903\n",
      "Step: [11433] d_loss: 1.38644874, g_loss: 0.69293380\n",
      "Step: [11434] d_loss: 1.38609242, g_loss: 0.69321656\n",
      "Step: [11435] d_loss: 1.38634324, g_loss: 0.69288427\n",
      "Step: [11436] d_loss: 1.38610244, g_loss: 0.69262636\n",
      "Step: [11437] d_loss: 1.38603771, g_loss: 0.69264364\n",
      "Step: [11438] d_loss: 1.38627970, g_loss: 0.69319534\n",
      "Step: [11439] d_loss: 1.38627636, g_loss: 0.69423926\n",
      "Step: [11440] d_loss: 1.38625145, g_loss: 0.69446361\n",
      "Step: [11441] d_loss: 1.38658035, g_loss: 0.69309479\n",
      "Step: [11442] d_loss: 1.38605499, g_loss: 0.69346166\n",
      "Step: [11443] d_loss: 1.38623190, g_loss: 0.69070613\n",
      "Step: [11444] d_loss: 1.38623714, g_loss: 0.69240999\n",
      "Step: [11445] d_loss: 1.38627601, g_loss: 0.69411278\n",
      "Step: [11446] d_loss: 1.38638258, g_loss: 0.69612318\n",
      "Step: [11447] d_loss: 1.38652515, g_loss: 0.69449329\n",
      "Step: [11448] d_loss: 1.38611114, g_loss: 0.69031250\n",
      "Step: [11449] d_loss: 1.38636148, g_loss: 0.69067311\n",
      "Step: [11450] d_loss: 1.38634253, g_loss: 0.69155365\n",
      "Step: [11451] d_loss: 1.38626659, g_loss: 0.69460940\n",
      "Step: [11452] d_loss: 1.38649499, g_loss: 0.69241214\n",
      "Step: [11453] d_loss: 1.38623512, g_loss: 0.69565237\n",
      "Step: [11454] d_loss: 1.38628387, g_loss: 0.69371945\n",
      "Step: [11455] d_loss: 1.38641548, g_loss: 0.69549823\n",
      "Step: [11456] d_loss: 1.38632989, g_loss: 0.69277114\n",
      "Step: [11457] d_loss: 1.38609242, g_loss: 0.69428074\n",
      "Step: [11458] d_loss: 1.38631678, g_loss: 0.69451153\n",
      "Step: [11459] d_loss: 1.38643098, g_loss: 0.69409633\n",
      "Step: [11460] d_loss: 1.38657880, g_loss: 0.69129544\n",
      "Step: [11461] d_loss: 1.38627315, g_loss: 0.69334882\n",
      "Step: [11462] d_loss: 1.38609016, g_loss: 0.69452393\n",
      "Step: [11463] d_loss: 1.38618672, g_loss: 0.69425601\n",
      "Step: [11464] d_loss: 1.38618636, g_loss: 0.69409716\n",
      "Step: [11465] d_loss: 1.38606918, g_loss: 0.69409144\n",
      "Step: [11466] d_loss: 1.38617420, g_loss: 0.69184923\n",
      "Step: [11467] d_loss: 1.38608074, g_loss: 0.69283724\n",
      "Step: [11468] d_loss: 1.38634896, g_loss: 0.69338095\n",
      "Step: [11469] d_loss: 1.38613081, g_loss: 0.69325620\n",
      "Step: [11470] d_loss: 1.38589513, g_loss: 0.69404566\n",
      "Step: [11471] d_loss: 1.38621056, g_loss: 0.69409847\n",
      "Step: [11472] d_loss: 1.38629150, g_loss: 0.69511509\n",
      "Step: [11473] d_loss: 1.38798416, g_loss: 0.69455343\n",
      "Step: [11474] d_loss: 1.38687611, g_loss: 0.69603270\n",
      "Step: [11475] d_loss: 1.38689077, g_loss: 0.69175136\n",
      "Step: [11476] d_loss: 1.38636768, g_loss: 0.69103670\n",
      "Step: [11477] d_loss: 1.38592196, g_loss: 0.69282854\n",
      "Step: [11478] d_loss: 1.38618684, g_loss: 0.69191861\n",
      "Step: [11479] d_loss: 1.38625503, g_loss: 0.69322157\n",
      "Step: [11480] d_loss: 1.38628626, g_loss: 0.69354510\n",
      "Step: [11481] d_loss: 1.38620603, g_loss: 0.69397199\n",
      "Step: [11482] d_loss: 1.38613868, g_loss: 0.69341207\n",
      "Step: [11483] d_loss: 1.38649297, g_loss: 0.69367468\n",
      "Step: [11484] d_loss: 1.38627028, g_loss: 0.69166595\n",
      "Step: [11485] d_loss: 1.38626516, g_loss: 0.69338846\n",
      "Step: [11486] d_loss: 1.38664341, g_loss: 0.69432938\n",
      "Step: [11487] d_loss: 1.38612592, g_loss: 0.69334006\n",
      "Step: [11488] d_loss: 1.38614714, g_loss: 0.69211382\n",
      "Step: [11489] d_loss: 1.38641381, g_loss: 0.69344753\n",
      "Step: [11490] d_loss: 1.38614929, g_loss: 0.69335389\n",
      "Step: [11491] d_loss: 1.38633156, g_loss: 0.69380653\n",
      "Step: [11492] d_loss: 1.38627899, g_loss: 0.69374502\n",
      "Step: [11493] d_loss: 1.38631701, g_loss: 0.69301653\n",
      "Step: [11494] d_loss: 1.38611007, g_loss: 0.69257367\n",
      "Step: [11495] d_loss: 1.38637865, g_loss: 0.69319928\n",
      "Step: [11496] d_loss: 1.38490701, g_loss: 0.69394135\n",
      "Step: [11497] d_loss: 1.38598073, g_loss: 0.69404012\n",
      "Step: [11498] d_loss: 1.38650882, g_loss: 0.69360459\n",
      "Step: [11499] d_loss: 1.38630021, g_loss: 0.69396305\n",
      "Step: [11500] d_loss: 1.38621283, g_loss: 0.69294512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11501] d_loss: 1.38619733, g_loss: 0.69300491\n",
      "Step: [11502] d_loss: 1.38608193, g_loss: 0.69329256\n",
      "Step: [11503] d_loss: 1.38670075, g_loss: 0.69189501\n",
      "Step: [11504] d_loss: 1.38628435, g_loss: 0.69399899\n",
      "Step: [11505] d_loss: 1.38636303, g_loss: 0.69376457\n",
      "Step: [11506] d_loss: 1.38626134, g_loss: 0.69303662\n",
      "Step: [11507] d_loss: 1.38665962, g_loss: 0.69382310\n",
      "Step: [11508] d_loss: 1.38652706, g_loss: 0.69492877\n",
      "Step: [11509] d_loss: 1.38626218, g_loss: 0.69318414\n",
      "Step: [11510] d_loss: 1.38611150, g_loss: 0.69335860\n",
      "Step: [11511] d_loss: 1.38601089, g_loss: 0.69315636\n",
      "Step: [11512] d_loss: 1.38606560, g_loss: 0.69276613\n",
      "Step: [11513] d_loss: 1.38627362, g_loss: 0.69390857\n",
      "Step: [11514] d_loss: 1.38654637, g_loss: 0.69238406\n",
      "Step: [11515] d_loss: 1.38652062, g_loss: 0.69384861\n",
      "Step: [11516] d_loss: 1.38619471, g_loss: 0.69387329\n",
      "Step: [11517] d_loss: 1.38632298, g_loss: 0.69267726\n",
      "Step: [11518] d_loss: 1.38603520, g_loss: 0.69286215\n",
      "Step: [11519] d_loss: 1.38617802, g_loss: 0.69379711\n",
      "Step: [11520] d_loss: 1.38660669, g_loss: 0.69342208\n",
      "Step: [11521] d_loss: 1.38629436, g_loss: 0.69419295\n",
      "Step: [11522] d_loss: 1.38634455, g_loss: 0.69542867\n",
      "Step: [11523] d_loss: 1.38651478, g_loss: 0.69182694\n",
      "Step: [11524] d_loss: 1.38633895, g_loss: 0.69306099\n",
      "Step: [11525] d_loss: 1.38606238, g_loss: 0.69208735\n",
      "Step: [11526] d_loss: 1.38615203, g_loss: 0.69251704\n",
      "Step: [11527] d_loss: 1.38605368, g_loss: 0.69373643\n",
      "Step: [11528] d_loss: 1.38615143, g_loss: 0.69358897\n",
      "Step: [11529] d_loss: 1.38620830, g_loss: 0.69294608\n",
      "Step: [11530] d_loss: 1.38663077, g_loss: 0.69282866\n",
      "Step: [11531] d_loss: 1.38633573, g_loss: 0.69336128\n",
      "Step: [11532] d_loss: 1.38630080, g_loss: 0.69287276\n",
      "Step: [11533] d_loss: 1.38636088, g_loss: 0.69314301\n",
      "Step: [11534] d_loss: 1.38607359, g_loss: 0.69456106\n",
      "Step: [11535] d_loss: 1.38601494, g_loss: 0.69319558\n",
      "Step: [11536] d_loss: 1.38637018, g_loss: 0.69343650\n",
      "Step: [11537] d_loss: 1.38624310, g_loss: 0.69409406\n",
      "Step: [11538] d_loss: 1.38613582, g_loss: 0.69289935\n",
      "Step: [11539] d_loss: 1.38618827, g_loss: 0.69310188\n",
      "Step: [11540] d_loss: 1.38630128, g_loss: 0.69364488\n",
      "Step: [11541] d_loss: 1.38666070, g_loss: 0.69438618\n",
      "Step: [11542] d_loss: 1.38641238, g_loss: 0.69450915\n",
      "Step: [11543] d_loss: 1.38621855, g_loss: 0.69245464\n",
      "Step: [11544] d_loss: 1.38603520, g_loss: 0.69363272\n",
      "Step: [11545] d_loss: 1.38629937, g_loss: 0.69367427\n",
      "Step: [11546] d_loss: 1.38599050, g_loss: 0.69361311\n",
      "Step: [11547] d_loss: 1.38635004, g_loss: 0.69221270\n",
      "Step: [11548] d_loss: 1.38637018, g_loss: 0.69265211\n",
      "Step: [11549] d_loss: 1.38599622, g_loss: 0.69332117\n",
      "Step: [11550] d_loss: 1.38628650, g_loss: 0.69304997\n",
      "Step: [11551] d_loss: 1.38669825, g_loss: 0.69547737\n",
      "Step: [11552] d_loss: 1.38640594, g_loss: 0.69443595\n",
      "Step: [11553] d_loss: 1.38639569, g_loss: 0.69329405\n",
      "Step: [11554] d_loss: 1.38632727, g_loss: 0.69268036\n",
      "Step: [11555] d_loss: 1.38635373, g_loss: 0.69349849\n",
      "Step: [11556] d_loss: 1.38446450, g_loss: 0.69799209\n",
      "Step: [11557] d_loss: 1.38619375, g_loss: 0.69338185\n",
      "Step: [11558] d_loss: 1.38632393, g_loss: 0.69014037\n",
      "Step: [11559] d_loss: 1.38628006, g_loss: 0.69313341\n",
      "Step: [11560] d_loss: 1.38623595, g_loss: 0.69438446\n",
      "Step: [11561] d_loss: 1.38621151, g_loss: 0.69424456\n",
      "Step: [11562] d_loss: 1.38640332, g_loss: 0.69430768\n",
      "Step: [11563] d_loss: 1.38633585, g_loss: 0.69315600\n",
      "Step: [11564] d_loss: 1.38646841, g_loss: 0.69261491\n",
      "Step: [11565] d_loss: 1.38618219, g_loss: 0.69217610\n",
      "Step: [11566] d_loss: 1.38599730, g_loss: 0.69584554\n",
      "Step: [11567] d_loss: 1.38607454, g_loss: 0.69354302\n",
      "Step: [11568] d_loss: 1.38651276, g_loss: 0.69297707\n",
      "Step: [11569] d_loss: 1.38636732, g_loss: 0.69276071\n",
      "Step: [11570] d_loss: 1.38641882, g_loss: 0.69473267\n",
      "Step: [11571] d_loss: 1.38622880, g_loss: 0.69325781\n",
      "Step: [11572] d_loss: 1.38656092, g_loss: 0.69288427\n",
      "Step: [11573] d_loss: 1.38646781, g_loss: 0.69382435\n",
      "Step: [11574] d_loss: 1.38635361, g_loss: 0.69326532\n",
      "Step: [11575] d_loss: 1.38600016, g_loss: 0.69339496\n",
      "Step: [11576] d_loss: 1.38634229, g_loss: 0.69216686\n",
      "Step: [11577] d_loss: 1.38630998, g_loss: 0.69350243\n",
      "Step: [11578] d_loss: 1.38616073, g_loss: 0.69364583\n",
      "Step: [11579] d_loss: 1.38597608, g_loss: 0.69428056\n",
      "Step: [11580] d_loss: 1.38629591, g_loss: 0.69524896\n",
      "Step: [11581] d_loss: 1.38676250, g_loss: 0.69160414\n",
      "Step: [11582] d_loss: 1.38725567, g_loss: 0.69442952\n",
      "Step: [11583] d_loss: 1.38648379, g_loss: 0.69238979\n",
      "Step: [11584] d_loss: 1.38631392, g_loss: 0.69390684\n",
      "Step: [11585] d_loss: 1.38675117, g_loss: 0.69542933\n",
      "Step: [11586] d_loss: 1.38640523, g_loss: 0.69369042\n",
      "Step: [11587] d_loss: 1.38619435, g_loss: 0.69170272\n",
      "Step: [11588] d_loss: 1.38603091, g_loss: 0.69477570\n",
      "Step: [11589] d_loss: 1.38593733, g_loss: 0.69347560\n",
      "Step: [11590] d_loss: 1.38663018, g_loss: 0.69513422\n",
      "Step: [11591] d_loss: 1.38624406, g_loss: 0.69256878\n",
      "Step: [11592] d_loss: 1.38626969, g_loss: 0.69302583\n",
      "Step: [11593] d_loss: 1.38625193, g_loss: 0.69337022\n",
      "Step: [11594] d_loss: 1.38618898, g_loss: 0.69347173\n",
      "Step: [11595] d_loss: 1.38636911, g_loss: 0.69333470\n",
      "Step: [11596] d_loss: 1.38597155, g_loss: 0.69361043\n",
      "Step: [11597] d_loss: 1.38624251, g_loss: 0.69465923\n",
      "Step: [11598] d_loss: 1.38633060, g_loss: 0.69289231\n",
      "Step: [11599] d_loss: 1.38598716, g_loss: 0.69410640\n",
      "Step: [11600] d_loss: 1.38584304, g_loss: 0.69352078\n",
      "Step: [11601] d_loss: 1.38582134, g_loss: 0.69380546\n",
      "Step: [11602] d_loss: 1.38631773, g_loss: 0.69274986\n",
      "Step: [11603] d_loss: 1.38617468, g_loss: 0.69368726\n",
      "Step: [11604] d_loss: 1.38572931, g_loss: 0.69397604\n",
      "Step: [11605] d_loss: 1.38599610, g_loss: 0.69346952\n",
      "Step: [11606] d_loss: 1.38614488, g_loss: 0.69297338\n",
      "Step: [11607] d_loss: 1.38599384, g_loss: 0.69265425\n",
      "Step: [11608] d_loss: 1.38601422, g_loss: 0.69309485\n",
      "Step: [11609] d_loss: 1.38629019, g_loss: 0.69344425\n",
      "Step: [11610] d_loss: 1.38629615, g_loss: 0.69338495\n",
      "Step: [11611] d_loss: 1.38639879, g_loss: 0.69468021\n",
      "Step: [11612] d_loss: 1.38631773, g_loss: 0.69339991\n",
      "Step: [11613] d_loss: 1.38625836, g_loss: 0.69275457\n",
      "Step: [11614] d_loss: 1.38554657, g_loss: 0.69199777\n",
      "Step: [11615] d_loss: 1.38688922, g_loss: 0.69376659\n",
      "Step: [11616] d_loss: 1.38655567, g_loss: 0.69151223\n",
      "Step: [11617] d_loss: 1.38617718, g_loss: 0.69367909\n",
      "Step: [11618] d_loss: 1.38599443, g_loss: 0.69458812\n",
      "Step: [11619] d_loss: 1.38637185, g_loss: 0.69421506\n",
      "Step: [11620] d_loss: 1.38650942, g_loss: 0.69421089\n",
      "Step: [11621] d_loss: 1.38631666, g_loss: 0.69303381\n",
      "Step: [11622] d_loss: 1.38587439, g_loss: 0.69331157\n",
      "Step: [11623] d_loss: 1.38611197, g_loss: 0.69183689\n",
      "Step: [11624] d_loss: 1.38676810, g_loss: 0.69468719\n",
      "Step: [11625] d_loss: 1.38667727, g_loss: 0.69186294\n",
      "Step: [11626] d_loss: 1.38624799, g_loss: 0.69281447\n",
      "Step: [11627] d_loss: 1.38642478, g_loss: 0.69259632\n",
      "Step: [11628] d_loss: 1.38634658, g_loss: 0.69428766\n",
      "Step: [11629] d_loss: 1.38680565, g_loss: 0.69372648\n",
      "Step: [11630] d_loss: 1.38588548, g_loss: 0.69389349\n",
      "Step: [11631] d_loss: 1.38589811, g_loss: 0.69439769\n",
      "Step: [11632] d_loss: 1.38644791, g_loss: 0.69326460\n",
      "Step: [11633] d_loss: 1.38619149, g_loss: 0.69463247\n",
      "Step: [11634] d_loss: 1.38624978, g_loss: 0.69355983\n",
      "Step: [11635] d_loss: 1.38695312, g_loss: 0.69196087\n",
      "Step: [11636] d_loss: 1.38666177, g_loss: 0.69399649\n",
      "Step: [11637] d_loss: 1.38612413, g_loss: 0.69331062\n",
      "Step: [11638] d_loss: 1.38611960, g_loss: 0.69440317\n",
      "Step: [11639] d_loss: 1.38677347, g_loss: 0.69281018\n",
      "Step: [11640] d_loss: 1.38586497, g_loss: 0.69314843\n",
      "Step: [11641] d_loss: 1.38624156, g_loss: 0.69208348\n",
      "Step: [11642] d_loss: 1.38661170, g_loss: 0.69376218\n",
      "Step: [11643] d_loss: 1.38633513, g_loss: 0.69346434\n",
      "Step: [11644] d_loss: 1.38697267, g_loss: 0.69836777\n",
      "Step: [11645] d_loss: 1.38632095, g_loss: 0.69401062\n",
      "Step: [11646] d_loss: 1.38660002, g_loss: 0.69331229\n",
      "Step: [11647] d_loss: 1.38633478, g_loss: 0.69292277\n",
      "Step: [11648] d_loss: 1.38646960, g_loss: 0.69235021\n",
      "Step: [11649] d_loss: 1.38599324, g_loss: 0.69219387\n",
      "Step: [11650] d_loss: 1.38633156, g_loss: 0.69347346\n",
      "Step: [11651] d_loss: 1.38513446, g_loss: 0.69457746\n",
      "Step: [11652] d_loss: 1.38635695, g_loss: 0.69404006\n",
      "Step: [11653] d_loss: 1.38649535, g_loss: 0.69334042\n",
      "Step: [11654] d_loss: 1.38623166, g_loss: 0.69328046\n",
      "Step: [11655] d_loss: 1.38578141, g_loss: 0.69341761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11656] d_loss: 1.38598347, g_loss: 0.69324982\n",
      "Step: [11657] d_loss: 1.38605857, g_loss: 0.69287330\n",
      "Step: [11658] d_loss: 1.38593662, g_loss: 0.69339401\n",
      "Step: [11659] d_loss: 1.38665783, g_loss: 0.69341743\n",
      "Step: [11660] d_loss: 1.38710570, g_loss: 0.69411427\n",
      "Step: [11661] d_loss: 1.38665390, g_loss: 0.69355404\n",
      "Step: [11662] d_loss: 1.38676071, g_loss: 0.69336152\n",
      "Step: [11663] d_loss: 1.38574076, g_loss: 0.69386399\n",
      "Step: [11664] d_loss: 1.38624096, g_loss: 0.69388926\n",
      "Step: [11665] d_loss: 1.38616240, g_loss: 0.69505048\n",
      "Step: [11666] d_loss: 1.38622570, g_loss: 0.69190699\n",
      "Step: [11667] d_loss: 1.38669598, g_loss: 0.69202733\n",
      "Step: [11668] d_loss: 1.38668215, g_loss: 0.69120562\n",
      "Step: [11669] d_loss: 1.38672674, g_loss: 0.69340396\n",
      "Step: [11670] d_loss: 1.38579607, g_loss: 0.69490588\n",
      "Step: [11671] d_loss: 1.38580346, g_loss: 0.69246536\n",
      "Step: [11672] d_loss: 1.38629448, g_loss: 0.69412398\n",
      "Step: [11673] d_loss: 1.38565242, g_loss: 0.69421279\n",
      "Step: [11674] d_loss: 1.38593900, g_loss: 0.69554687\n",
      "Step: [11675] d_loss: 1.38662648, g_loss: 0.69189405\n",
      "Step: [11676] d_loss: 1.38543355, g_loss: 0.69783860\n",
      "Step: [11677] d_loss: 1.38621974, g_loss: 0.69649565\n",
      "Step: [11678] d_loss: 1.38593972, g_loss: 0.69478059\n",
      "Step: [11679] d_loss: 1.38784528, g_loss: 0.68873030\n",
      "Step: [11680] d_loss: 1.38793492, g_loss: 0.69402611\n",
      "Step: [11681] d_loss: 1.38707542, g_loss: 0.69265211\n",
      "Step: [11682] d_loss: 1.38688087, g_loss: 0.69545412\n",
      "Step: [11683] d_loss: 1.38646519, g_loss: 0.69642150\n",
      "Step: [11684] d_loss: 1.38665533, g_loss: 0.69450045\n",
      "Step: [11685] d_loss: 1.38600779, g_loss: 0.69310248\n",
      "Step: [11686] d_loss: 1.38666725, g_loss: 0.69209957\n",
      "Step: [11687] d_loss: 1.38623261, g_loss: 0.69146097\n",
      "Step: [11688] d_loss: 1.38597751, g_loss: 0.69397914\n",
      "Step: [11689] d_loss: 1.38691282, g_loss: 0.69275713\n",
      "Step: [11690] d_loss: 1.38634658, g_loss: 0.69207150\n",
      "Step: [11691] d_loss: 1.38610625, g_loss: 0.69459581\n",
      "Step: [11692] d_loss: 1.38663518, g_loss: 0.69319057\n",
      "Step: [11693] d_loss: 1.38625324, g_loss: 0.69465411\n",
      "Step: [11694] d_loss: 1.38584185, g_loss: 0.69389063\n",
      "Step: [11695] d_loss: 1.38632238, g_loss: 0.69294137\n",
      "Step: [11696] d_loss: 1.38632739, g_loss: 0.69438231\n",
      "Step: [11697] d_loss: 1.38625801, g_loss: 0.69307518\n",
      "Step: [11698] d_loss: 1.38613486, g_loss: 0.69471478\n",
      "Step: [11699] d_loss: 1.38635707, g_loss: 0.69549608\n",
      "Step: [11700] d_loss: 1.38589132, g_loss: 0.69404769\n",
      "Step: [11701] d_loss: 1.38615203, g_loss: 0.69284719\n",
      "Step: [11702] d_loss: 1.38619113, g_loss: 0.69296193\n",
      "Step: [11703] d_loss: 1.38584185, g_loss: 0.69458109\n",
      "Step: [11704] d_loss: 1.38552928, g_loss: 0.69598687\n",
      "Step: [11705] d_loss: 1.38597465, g_loss: 0.69362342\n",
      "Step: [11706] d_loss: 1.38577652, g_loss: 0.69174242\n",
      "Step: [11707] d_loss: 1.38620615, g_loss: 0.69240683\n",
      "Step: [11708] d_loss: 1.38625360, g_loss: 0.69440591\n",
      "Step: [11709] d_loss: 1.38917375, g_loss: 0.69932073\n",
      "Step: [11710] d_loss: 1.38636339, g_loss: 0.69321764\n",
      "Step: [11711] d_loss: 1.38621092, g_loss: 0.69457495\n",
      "Step: [11712] d_loss: 1.38648236, g_loss: 0.69265759\n",
      "Step: [11713] d_loss: 1.38625968, g_loss: 0.69331706\n",
      "Step: [11714] d_loss: 1.38586450, g_loss: 0.69668186\n",
      "Step: [11715] d_loss: 1.38604331, g_loss: 0.69248056\n",
      "Step: [11716] d_loss: 1.38601840, g_loss: 0.69502378\n",
      "Step: [11717] d_loss: 1.38616967, g_loss: 0.69421172\n",
      "Step: [11718] d_loss: 1.38584256, g_loss: 0.69277334\n",
      "Step: [11719] d_loss: 1.38645387, g_loss: 0.69189650\n",
      "Step: [11720] d_loss: 1.38634133, g_loss: 0.69224048\n",
      "Step: [11721] d_loss: 1.38688278, g_loss: 0.69147849\n",
      "Step: [11722] d_loss: 1.38642216, g_loss: 0.69296557\n",
      "Step: [11723] d_loss: 1.38607550, g_loss: 0.69408923\n",
      "Step: [11724] d_loss: 1.38611162, g_loss: 0.69328368\n",
      "Step: [11725] d_loss: 1.38630986, g_loss: 0.69393575\n",
      "Step: [11726] d_loss: 1.38608468, g_loss: 0.69339550\n",
      "Step: [11727] d_loss: 1.38624668, g_loss: 0.69312876\n",
      "Step: [11728] d_loss: 1.38641894, g_loss: 0.69288355\n",
      "Step: [11729] d_loss: 1.38619995, g_loss: 0.69406527\n",
      "Step: [11730] d_loss: 1.38630128, g_loss: 0.69321358\n",
      "Step: [11731] d_loss: 1.38620996, g_loss: 0.69288242\n",
      "Step: [11732] d_loss: 1.38645017, g_loss: 0.69406879\n",
      "Step: [11733] d_loss: 1.38669038, g_loss: 0.69329882\n",
      "Step: [11734] d_loss: 1.38606858, g_loss: 0.69228804\n",
      "Step: [11735] d_loss: 1.38599753, g_loss: 0.69371051\n",
      "Step: [11736] d_loss: 1.38596499, g_loss: 0.69383109\n",
      "Step: [11737] d_loss: 1.38639879, g_loss: 0.69436109\n",
      "Step: [11738] d_loss: 1.38616323, g_loss: 0.69377184\n",
      "Step: [11739] d_loss: 1.38606715, g_loss: 0.69288492\n",
      "Step: [11740] d_loss: 1.38612688, g_loss: 0.69227612\n",
      "Step: [11741] d_loss: 1.38617170, g_loss: 0.69355863\n",
      "Step: [11742] d_loss: 1.38634539, g_loss: 0.69458377\n",
      "Step: [11743] d_loss: 1.38645399, g_loss: 0.69309032\n",
      "Step: [11744] d_loss: 1.38643646, g_loss: 0.69163156\n",
      "Step: [11745] d_loss: 1.38638997, g_loss: 0.69266355\n",
      "Step: [11746] d_loss: 1.38613713, g_loss: 0.69398683\n",
      "Step: [11747] d_loss: 1.38643754, g_loss: 0.69419539\n",
      "Step: [11748] d_loss: 1.38608384, g_loss: 0.69343382\n",
      "Step: [11749] d_loss: 1.38583255, g_loss: 0.69530475\n",
      "Step: [11750] d_loss: 1.38723135, g_loss: 0.68958908\n",
      "Step: [11751] d_loss: 1.38829994, g_loss: 0.69217616\n",
      "Step: [11752] d_loss: 1.38764274, g_loss: 0.69037819\n",
      "Step: [11753] d_loss: 1.38667798, g_loss: 0.69499135\n",
      "Step: [11754] d_loss: 1.38663173, g_loss: 0.69488525\n",
      "Step: [11755] d_loss: 1.38634396, g_loss: 0.69374126\n",
      "Step: [11756] d_loss: 1.38618612, g_loss: 0.69286358\n",
      "Step: [11757] d_loss: 1.38625288, g_loss: 0.69267535\n",
      "Step: [11758] d_loss: 1.38631225, g_loss: 0.69300866\n",
      "Step: [11759] d_loss: 1.38630509, g_loss: 0.69339234\n",
      "Step: [11760] d_loss: 1.38629150, g_loss: 0.69325149\n",
      "Step: [11761] d_loss: 1.38624835, g_loss: 0.69285810\n",
      "Step: [11762] d_loss: 1.38617349, g_loss: 0.69377458\n",
      "Step: [11763] d_loss: 1.38638592, g_loss: 0.69297361\n",
      "Step: [11764] d_loss: 1.38640010, g_loss: 0.69289881\n",
      "Step: [11765] d_loss: 1.38635314, g_loss: 0.69330817\n",
      "Step: [11766] d_loss: 1.38595009, g_loss: 0.69312209\n",
      "Step: [11767] d_loss: 1.38634396, g_loss: 0.69279051\n",
      "Step: [11768] d_loss: 1.38612556, g_loss: 0.69398069\n",
      "Step: [11769] d_loss: 1.38598311, g_loss: 0.69285429\n",
      "Step: [11770] d_loss: 1.38622963, g_loss: 0.69299757\n",
      "Step: [11771] d_loss: 1.38614821, g_loss: 0.69341171\n",
      "Step: [11772] d_loss: 1.38672400, g_loss: 0.69230229\n",
      "Step: [11773] d_loss: 1.38599896, g_loss: 0.69378406\n",
      "Step: [11774] d_loss: 1.38658059, g_loss: 0.69623643\n",
      "Step: [11775] d_loss: 1.38700604, g_loss: 0.69344187\n",
      "Step: [11776] d_loss: 1.38676333, g_loss: 0.69207692\n",
      "Step: [11777] d_loss: 1.38653278, g_loss: 0.68903106\n",
      "Step: [11778] d_loss: 1.38623357, g_loss: 0.69095027\n",
      "Step: [11779] d_loss: 1.38608885, g_loss: 0.69213808\n",
      "Step: [11780] d_loss: 1.38641214, g_loss: 0.69598639\n",
      "Step: [11781] d_loss: 1.38648367, g_loss: 0.69341028\n",
      "Step: [11782] d_loss: 1.38648736, g_loss: 0.69317043\n",
      "Step: [11783] d_loss: 1.38648057, g_loss: 0.69181514\n",
      "Step: [11784] d_loss: 1.38627911, g_loss: 0.69293636\n",
      "Step: [11785] d_loss: 1.38623130, g_loss: 0.69224894\n",
      "Step: [11786] d_loss: 1.38634717, g_loss: 0.69499785\n",
      "Step: [11787] d_loss: 1.38640332, g_loss: 0.69414186\n",
      "Step: [11788] d_loss: 1.38643694, g_loss: 0.69401681\n",
      "Step: [11789] d_loss: 1.38614082, g_loss: 0.69277632\n",
      "Step: [11790] d_loss: 1.38630939, g_loss: 0.69345629\n",
      "Step: [11791] d_loss: 1.38659334, g_loss: 0.69646072\n",
      "Step: [11792] d_loss: 1.38630855, g_loss: 0.69268858\n",
      "Step: [11793] d_loss: 1.38677955, g_loss: 0.69482231\n",
      "Step: [11794] d_loss: 1.38661432, g_loss: 0.69146985\n",
      "Step: [11795] d_loss: 1.38635802, g_loss: 0.69244260\n",
      "Step: [11796] d_loss: 1.38636816, g_loss: 0.69302529\n",
      "Step: [11797] d_loss: 1.38590431, g_loss: 0.69319725\n",
      "Step: [11798] d_loss: 1.38583148, g_loss: 0.69365871\n",
      "Step: [11799] d_loss: 1.38591528, g_loss: 0.69320488\n",
      "Step: [11800] d_loss: 1.38603044, g_loss: 0.69455492\n",
      "Step: [11801] d_loss: 1.38636518, g_loss: 0.69271708\n",
      "Step: [11802] d_loss: 1.38612080, g_loss: 0.69376999\n",
      "Step: [11803] d_loss: 1.38688707, g_loss: 0.69810075\n",
      "Step: [11804] d_loss: 1.38748598, g_loss: 0.69155389\n",
      "Step: [11805] d_loss: 1.38748121, g_loss: 0.69524163\n",
      "Step: [11806] d_loss: 1.38674617, g_loss: 0.69344497\n",
      "Step: [11807] d_loss: 1.38634610, g_loss: 0.69477320\n",
      "Step: [11808] d_loss: 1.38647807, g_loss: 0.69400048\n",
      "Step: [11809] d_loss: 1.38603044, g_loss: 0.69310713\n",
      "Step: [11810] d_loss: 1.38567793, g_loss: 0.69595408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11811] d_loss: 1.38591790, g_loss: 0.69326568\n",
      "Step: [11812] d_loss: 1.38596976, g_loss: 0.69305116\n",
      "Step: [11813] d_loss: 1.38637948, g_loss: 0.69265312\n",
      "Step: [11814] d_loss: 1.38608885, g_loss: 0.69342750\n",
      "Step: [11815] d_loss: 1.38593531, g_loss: 0.69439042\n",
      "Step: [11816] d_loss: 1.38624597, g_loss: 0.69377875\n",
      "Step: [11817] d_loss: 1.38620317, g_loss: 0.69327474\n",
      "Step: [11818] d_loss: 1.38640165, g_loss: 0.69354451\n",
      "Step: [11819] d_loss: 1.38641977, g_loss: 0.69327033\n",
      "Step: [11820] d_loss: 1.38595772, g_loss: 0.69329047\n",
      "Step: [11821] d_loss: 1.38630474, g_loss: 0.69304574\n",
      "Step: [11822] d_loss: 1.38576484, g_loss: 0.69328022\n",
      "Step: [11823] d_loss: 1.38655543, g_loss: 0.69305384\n",
      "Step: [11824] d_loss: 1.38654006, g_loss: 0.69334888\n",
      "Step: [11825] d_loss: 1.38628054, g_loss: 0.69356126\n",
      "Step: [11826] d_loss: 1.38615561, g_loss: 0.69344485\n",
      "Step: [11827] d_loss: 1.38625765, g_loss: 0.69309604\n",
      "Step: [11828] d_loss: 1.38609147, g_loss: 0.69281554\n",
      "Step: [11829] d_loss: 1.38603282, g_loss: 0.69496781\n",
      "Step: [11830] d_loss: 1.38653874, g_loss: 0.69254059\n",
      "Step: [11831] d_loss: 1.38684773, g_loss: 0.69741231\n",
      "Step: [11832] d_loss: 1.38693142, g_loss: 0.69262171\n",
      "Step: [11833] d_loss: 1.38734555, g_loss: 0.69505906\n",
      "Step: [11834] d_loss: 1.38673210, g_loss: 0.69340193\n",
      "Step: [11835] d_loss: 1.38676310, g_loss: 0.69618464\n",
      "Step: [11836] d_loss: 1.38655424, g_loss: 0.69221652\n",
      "Step: [11837] d_loss: 1.38640141, g_loss: 0.69264495\n",
      "Step: [11838] d_loss: 1.38509738, g_loss: 0.69851160\n",
      "Step: [11839] d_loss: 1.38620114, g_loss: 0.69216084\n",
      "Step: [11840] d_loss: 1.38608217, g_loss: 0.69276822\n",
      "Step: [11841] d_loss: 1.38609195, g_loss: 0.69379115\n",
      "Step: [11842] d_loss: 1.38620877, g_loss: 0.69497347\n",
      "Step: [11843] d_loss: 1.38650632, g_loss: 0.69695807\n",
      "Step: [11844] d_loss: 1.38728642, g_loss: 0.69128072\n",
      "Step: [11845] d_loss: 1.38742185, g_loss: 0.69199026\n",
      "Step: [11846] d_loss: 1.38676918, g_loss: 0.68956453\n",
      "Step: [11847] d_loss: 1.38657475, g_loss: 0.69389296\n",
      "Step: [11848] d_loss: 1.38622546, g_loss: 0.69340402\n",
      "Step: [11849] d_loss: 1.38591528, g_loss: 0.69365299\n",
      "Step: [11850] d_loss: 1.40819716, g_loss: 0.83466852\n",
      "Step: [11851] d_loss: 1.39386892, g_loss: 0.71764606\n",
      "Step: [11852] d_loss: 1.39401126, g_loss: 0.69971299\n",
      "Step: [11853] d_loss: 1.39079475, g_loss: 0.69553351\n",
      "Step: [11854] d_loss: 1.38793695, g_loss: 0.69561255\n",
      "Step: [11855] d_loss: 1.38675475, g_loss: 0.69427776\n",
      "Step: [11856] d_loss: 1.38615882, g_loss: 0.69325274\n",
      "Step: [11857] d_loss: 1.38720655, g_loss: 0.69220585\n",
      "Step: [11858] d_loss: 1.38588786, g_loss: 0.69340611\n",
      "Step: [11859] d_loss: 1.38630950, g_loss: 0.69480419\n",
      "Step: [11860] d_loss: 1.38656878, g_loss: 0.69354963\n",
      "Step: [11861] d_loss: 1.38606107, g_loss: 0.69451988\n",
      "Step: [11862] d_loss: 1.38885188, g_loss: 0.69296885\n",
      "Step: [11863] d_loss: 1.38698244, g_loss: 0.69737905\n",
      "Step: [11864] d_loss: 1.38662207, g_loss: 0.69418204\n",
      "Step: [11865] d_loss: 1.38654625, g_loss: 0.69387186\n",
      "Step: [11866] d_loss: 1.38637066, g_loss: 0.69336820\n",
      "Step: [11867] d_loss: 1.39030278, g_loss: 0.69133836\n",
      "Step: [11868] d_loss: 1.38624454, g_loss: 0.69423866\n",
      "Step: [11869] d_loss: 1.38630319, g_loss: 0.69478393\n",
      "Step: [11870] d_loss: 1.38660169, g_loss: 0.69332564\n",
      "Step: [11871] d_loss: 1.38924766, g_loss: 0.71035856\n",
      "Step: [11872] d_loss: 1.39249527, g_loss: 0.69188184\n",
      "Step: [11873] d_loss: 1.38768756, g_loss: 0.69695711\n",
      "Step: [11874] d_loss: 1.38685441, g_loss: 0.69231826\n",
      "Step: [11875] d_loss: 1.38708270, g_loss: 0.69103456\n",
      "Step: [11876] d_loss: 1.38597524, g_loss: 0.69271326\n",
      "Step: [11877] d_loss: 1.38849342, g_loss: 0.69109643\n",
      "Step: [11878] d_loss: 1.38899231, g_loss: 0.69353622\n",
      "Step: [11879] d_loss: 1.38705266, g_loss: 0.69677544\n",
      "Step: [11880] d_loss: 1.38659430, g_loss: 0.69854236\n",
      "Step: [11881] d_loss: 1.38618016, g_loss: 0.69594693\n",
      "Step: [11882] d_loss: 1.38638568, g_loss: 0.69184041\n",
      "Step: [11883] d_loss: 1.38618398, g_loss: 0.69218904\n",
      "Step: [11884] d_loss: 1.38761520, g_loss: 0.69231719\n",
      "Step: [11885] d_loss: 1.38608301, g_loss: 0.69218856\n",
      "Step: [11886] d_loss: 1.38774490, g_loss: 0.69539553\n",
      "Step: [11887] d_loss: 1.38631570, g_loss: 0.69356513\n",
      "Step: [11888] d_loss: 1.38601708, g_loss: 0.69610882\n",
      "Step: [11889] d_loss: 1.38577187, g_loss: 0.69467843\n",
      "Step: [11890] d_loss: 1.38565636, g_loss: 0.69335532\n",
      "Step: [11891] d_loss: 1.38597524, g_loss: 0.69461483\n",
      "Step: [11892] d_loss: 1.38636589, g_loss: 0.69315463\n",
      "Step: [11893] d_loss: 1.38647151, g_loss: 0.69420946\n",
      "Step: [11894] d_loss: 1.38591647, g_loss: 0.69181478\n",
      "Step: [11895] d_loss: 1.38554859, g_loss: 0.69178474\n",
      "Step: [11896] d_loss: 1.38995862, g_loss: 0.69373387\n",
      "Step: [11897] d_loss: 1.38612819, g_loss: 0.69441462\n",
      "Step: [11898] d_loss: 1.38690531, g_loss: 0.69783771\n",
      "Step: [11899] d_loss: 1.38691950, g_loss: 0.69517308\n",
      "Step: [11900] d_loss: 1.38618219, g_loss: 0.69245005\n",
      "Step: [11901] d_loss: 1.38594306, g_loss: 0.69238758\n",
      "Step: [11902] d_loss: 1.38594794, g_loss: 0.69277120\n",
      "Step: [11903] d_loss: 1.38619328, g_loss: 0.69500256\n",
      "Step: [11904] d_loss: 1.38645327, g_loss: 0.69263154\n",
      "Step: [11905] d_loss: 1.38637388, g_loss: 0.69479662\n",
      "Step: [11906] d_loss: 1.38646173, g_loss: 0.69285917\n",
      "Step: [11907] d_loss: 1.38541198, g_loss: 0.69428301\n",
      "Step: [11908] d_loss: 1.38611293, g_loss: 0.69153279\n",
      "Step: [11909] d_loss: 1.38621700, g_loss: 0.69318640\n",
      "Step: [11910] d_loss: 1.38565111, g_loss: 0.69428992\n",
      "Step: [11911] d_loss: 1.38567376, g_loss: 0.69459689\n",
      "Step: [11912] d_loss: 1.38624644, g_loss: 0.69422048\n",
      "Step: [11913] d_loss: 1.38592982, g_loss: 0.69406295\n",
      "Step: [11914] d_loss: 1.38557529, g_loss: 0.69206023\n",
      "Step: [11915] d_loss: 1.38573146, g_loss: 0.69579136\n",
      "Step: [11916] d_loss: 1.38558459, g_loss: 0.69510746\n",
      "Step: [11917] d_loss: 1.38567090, g_loss: 0.69341004\n",
      "Step: [11918] d_loss: 1.38602173, g_loss: 0.69221485\n",
      "Step: [11919] d_loss: 1.38589299, g_loss: 0.69317245\n",
      "Step: [11920] d_loss: 1.38785279, g_loss: 0.69101107\n",
      "Step: [11921] d_loss: 1.38612056, g_loss: 0.69433576\n",
      "Step: [11922] d_loss: 1.38629293, g_loss: 0.69433784\n",
      "Step: [11923] d_loss: 1.38591301, g_loss: 0.69408453\n",
      "Step: [11924] d_loss: 1.38722074, g_loss: 0.69235253\n",
      "Step: [11925] d_loss: 1.38572502, g_loss: 0.69582880\n",
      "Step: [11926] d_loss: 1.38545561, g_loss: 0.69410950\n",
      "Step: [11927] d_loss: 1.38539624, g_loss: 0.69467926\n",
      "Step: [11928] d_loss: 1.38635731, g_loss: 0.69290841\n",
      "Step: [11929] d_loss: 1.38577795, g_loss: 0.69271159\n",
      "Step: [11930] d_loss: 1.38675785, g_loss: 0.69289124\n",
      "Step: [11931] d_loss: 1.38605094, g_loss: 0.69389331\n",
      "Step: [11932] d_loss: 1.38585877, g_loss: 0.69457054\n",
      "Step: [11933] d_loss: 1.38568568, g_loss: 0.69295132\n",
      "Step: [11934] d_loss: 1.38680148, g_loss: 0.69391501\n",
      "Step: [11935] d_loss: 1.38560867, g_loss: 0.69132000\n",
      "Step: [11936] d_loss: 1.38722730, g_loss: 0.69546175\n",
      "Step: [11937] d_loss: 1.38669610, g_loss: 0.69145149\n",
      "Step: [11938] d_loss: 1.38662362, g_loss: 0.69247723\n",
      "Step: [11939] d_loss: 1.38618445, g_loss: 0.69410849\n",
      "Step: [11940] d_loss: 1.38612223, g_loss: 0.69571006\n",
      "Step: [11941] d_loss: 1.38587320, g_loss: 0.69590604\n",
      "Step: [11942] d_loss: 1.38664806, g_loss: 0.69334078\n",
      "Step: [11943] d_loss: 1.38625991, g_loss: 0.69384795\n",
      "Step: [11944] d_loss: 1.38629270, g_loss: 0.69272244\n",
      "Step: [11945] d_loss: 1.38743472, g_loss: 0.69126546\n",
      "Step: [11946] d_loss: 1.38625145, g_loss: 0.69220042\n",
      "Step: [11947] d_loss: 1.38642514, g_loss: 0.69462860\n",
      "Step: [11948] d_loss: 1.38635421, g_loss: 0.69347608\n",
      "Step: [11949] d_loss: 1.38655603, g_loss: 0.69413334\n",
      "Step: [11950] d_loss: 1.38620234, g_loss: 0.69205046\n",
      "Step: [11951] d_loss: 1.38587666, g_loss: 0.69190228\n",
      "Step: [11952] d_loss: 1.38616705, g_loss: 0.69349372\n",
      "Step: [11953] d_loss: 1.38631666, g_loss: 0.69331419\n",
      "Step: [11954] d_loss: 1.38624859, g_loss: 0.69368637\n",
      "Step: [11955] d_loss: 1.38630962, g_loss: 0.69264853\n",
      "Step: [11956] d_loss: 1.38606644, g_loss: 0.69284707\n",
      "Step: [11957] d_loss: 1.38636780, g_loss: 0.69387305\n",
      "Step: [11958] d_loss: 1.38623202, g_loss: 0.69429731\n",
      "Step: [11959] d_loss: 1.38628471, g_loss: 0.69378352\n",
      "Step: [11960] d_loss: 1.38593698, g_loss: 0.69286066\n",
      "Step: [11961] d_loss: 1.38840032, g_loss: 0.70310056\n",
      "Step: [11962] d_loss: 1.38746810, g_loss: 0.68949848\n",
      "Step: [11963] d_loss: 1.38797426, g_loss: 0.69568127\n",
      "Step: [11964] d_loss: 1.38677180, g_loss: 0.69441557\n",
      "Step: [11965] d_loss: 1.38611174, g_loss: 0.69377792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [11966] d_loss: 1.38635790, g_loss: 0.69203329\n",
      "Step: [11967] d_loss: 1.38648105, g_loss: 0.69282591\n",
      "Step: [11968] d_loss: 1.38631129, g_loss: 0.69392300\n",
      "Step: [11969] d_loss: 1.38617611, g_loss: 0.69292212\n",
      "Step: [11970] d_loss: 1.38618970, g_loss: 0.69341624\n",
      "Step: [11971] d_loss: 1.38647342, g_loss: 0.69312632\n",
      "Step: [11972] d_loss: 1.38616633, g_loss: 0.69394326\n",
      "Step: [11973] d_loss: 1.38620138, g_loss: 0.69411659\n",
      "Step: [11974] d_loss: 1.38642979, g_loss: 0.69382000\n",
      "Step: [11975] d_loss: 1.38625789, g_loss: 0.69453567\n",
      "Step: [11976] d_loss: 1.38680458, g_loss: 0.69131732\n",
      "Step: [11977] d_loss: 1.38737500, g_loss: 0.69605654\n",
      "Step: [11978] d_loss: 1.38737273, g_loss: 0.69527495\n",
      "Step: [11979] d_loss: 1.38658273, g_loss: 0.69490552\n",
      "Step: [11980] d_loss: 1.38624358, g_loss: 0.69384897\n",
      "Step: [11981] d_loss: 1.38630605, g_loss: 0.69140267\n",
      "Step: [11982] d_loss: 1.38613749, g_loss: 0.69237256\n",
      "Step: [11983] d_loss: 1.38616002, g_loss: 0.69463080\n",
      "Step: [11984] d_loss: 1.38623118, g_loss: 0.69410503\n",
      "Step: [11985] d_loss: 1.38932180, g_loss: 0.69080782\n",
      "Step: [11986] d_loss: 1.38663459, g_loss: 0.69264680\n",
      "Step: [11987] d_loss: 1.38627648, g_loss: 0.69168830\n",
      "Step: [11988] d_loss: 1.38701105, g_loss: 0.69018805\n",
      "Step: [11989] d_loss: 1.38646257, g_loss: 0.69301033\n",
      "Step: [11990] d_loss: 1.38633263, g_loss: 0.69478256\n",
      "Step: [11991] d_loss: 1.38659000, g_loss: 0.69472539\n",
      "Step: [11992] d_loss: 1.38682497, g_loss: 0.68859756\n",
      "Step: [11993] d_loss: 1.38718498, g_loss: 0.69296014\n",
      "Step: [11994] d_loss: 1.38652170, g_loss: 0.69407547\n",
      "Step: [11995] d_loss: 1.38676143, g_loss: 0.69316864\n",
      "Step: [11996] d_loss: 1.38632226, g_loss: 0.69555038\n",
      "Step: [11997] d_loss: 1.38624465, g_loss: 0.69239861\n",
      "Step: [11998] d_loss: 1.38630140, g_loss: 0.69311833\n",
      "Step: [11999] d_loss: 1.38622499, g_loss: 0.69225407\n",
      "Step: [12000] d_loss: 1.38634419, g_loss: 0.69335055\n",
      "Step: [12001] d_loss: 1.38627768, g_loss: 0.69291139\n",
      "Step: [12002] d_loss: 1.38610744, g_loss: 0.69226861\n",
      "Step: [12003] d_loss: 1.38608027, g_loss: 0.69350016\n",
      "Step: [12004] d_loss: 1.38615668, g_loss: 0.69386280\n",
      "Step: [12005] d_loss: 1.38619339, g_loss: 0.69361413\n",
      "Step: [12006] d_loss: 1.38631678, g_loss: 0.69015199\n",
      "Step: [12007] d_loss: 1.38652873, g_loss: 0.69505167\n",
      "Step: [12008] d_loss: 1.38668382, g_loss: 0.69128680\n",
      "Step: [12009] d_loss: 1.38670087, g_loss: 0.69597799\n",
      "Step: [12010] d_loss: 1.38638687, g_loss: 0.69236213\n",
      "Step: [12011] d_loss: 1.38621426, g_loss: 0.69278431\n",
      "Step: [12012] d_loss: 1.38625598, g_loss: 0.69182622\n",
      "Step: [12013] d_loss: 1.38621449, g_loss: 0.69277751\n",
      "Step: [12014] d_loss: 1.38635159, g_loss: 0.69322848\n",
      "Step: [12015] d_loss: 1.38618350, g_loss: 0.69321156\n",
      "Step: [12016] d_loss: 1.38618016, g_loss: 0.69356990\n",
      "Step: [12017] d_loss: 1.38654470, g_loss: 0.69219911\n",
      "Step: [12018] d_loss: 1.38627529, g_loss: 0.69325781\n",
      "Step: [12019] d_loss: 1.38625383, g_loss: 0.69310927\n",
      "Step: [12020] d_loss: 1.38627827, g_loss: 0.69416654\n",
      "Step: [12021] d_loss: 1.38570058, g_loss: 0.69508380\n",
      "Step: [12022] d_loss: 1.38627005, g_loss: 0.69319576\n",
      "Step: [12023] d_loss: 1.38666153, g_loss: 0.69272923\n",
      "Step: [12024] d_loss: 1.38632488, g_loss: 0.69306511\n",
      "Step: [12025] d_loss: 1.38619876, g_loss: 0.69364160\n",
      "Step: [12026] d_loss: 1.38517618, g_loss: 0.69643140\n",
      "Step: [12027] d_loss: 1.38637066, g_loss: 0.69306266\n",
      "Step: [12028] d_loss: 1.38617623, g_loss: 0.69302195\n",
      "Step: [12029] d_loss: 1.38626003, g_loss: 0.69371545\n",
      "Step: [12030] d_loss: 1.38628495, g_loss: 0.69187582\n",
      "Step: [12031] d_loss: 1.38615298, g_loss: 0.69285059\n",
      "Step: [12032] d_loss: 1.38632274, g_loss: 0.69317520\n",
      "Step: [12033] d_loss: 1.38619447, g_loss: 0.69343674\n",
      "Step: [12034] d_loss: 1.38621831, g_loss: 0.69350785\n",
      "Step: [12035] d_loss: 1.38615894, g_loss: 0.69307542\n",
      "Step: [12036] d_loss: 1.38639414, g_loss: 0.69325066\n",
      "Step: [12037] d_loss: 1.38635719, g_loss: 0.69236350\n",
      "Step: [12038] d_loss: 1.38613391, g_loss: 0.69317460\n",
      "Step: [12039] d_loss: 1.38625050, g_loss: 0.69332981\n",
      "Step: [12040] d_loss: 1.38612008, g_loss: 0.69361389\n",
      "Step: [12041] d_loss: 1.38626981, g_loss: 0.69324327\n",
      "Step: [12042] d_loss: 1.38626003, g_loss: 0.69274944\n",
      "Step: [12043] d_loss: 1.38617539, g_loss: 0.69342673\n",
      "Step: [12044] d_loss: 1.38643694, g_loss: 0.69255471\n",
      "Step: [12045] d_loss: 1.38604164, g_loss: 0.69334769\n",
      "Step: [12046] d_loss: 1.38627887, g_loss: 0.69192231\n",
      "Step: [12047] d_loss: 1.38632405, g_loss: 0.69355845\n",
      "Step: [12048] d_loss: 1.38614118, g_loss: 0.69400847\n",
      "Step: [12049] d_loss: 1.38623571, g_loss: 0.69191211\n",
      "Step: [12050] d_loss: 1.38632190, g_loss: 0.69226813\n",
      "Step: [12051] d_loss: 1.38620162, g_loss: 0.69446772\n",
      "Step: [12052] d_loss: 1.38603091, g_loss: 0.69264090\n",
      "Step: [12053] d_loss: 1.38659143, g_loss: 0.69649953\n",
      "Step: [12054] d_loss: 1.38690567, g_loss: 0.69372880\n",
      "Step: [12055] d_loss: 1.38653827, g_loss: 0.69350278\n",
      "Step: [12056] d_loss: 1.38627863, g_loss: 0.69253039\n",
      "Step: [12057] d_loss: 1.38604856, g_loss: 0.69267905\n",
      "Step: [12058] d_loss: 1.38638735, g_loss: 0.69224870\n",
      "Step: [12059] d_loss: 1.38629580, g_loss: 0.69339657\n",
      "Step: [12060] d_loss: 1.38632298, g_loss: 0.69293934\n",
      "Step: [12061] d_loss: 1.38628769, g_loss: 0.69328666\n",
      "Step: [12062] d_loss: 1.38653600, g_loss: 0.69321609\n",
      "Step: [12063] d_loss: 1.38633585, g_loss: 0.69343901\n",
      "Step: [12064] d_loss: 1.38625669, g_loss: 0.69355416\n",
      "Step: [12065] d_loss: 1.38621557, g_loss: 0.69374418\n",
      "Step: [12066] d_loss: 1.38635671, g_loss: 0.69269371\n",
      "Step: [12067] d_loss: 1.38623095, g_loss: 0.69300377\n",
      "Step: [12068] d_loss: 1.38621116, g_loss: 0.69330025\n",
      "Step: [12069] d_loss: 1.38627779, g_loss: 0.69359136\n",
      "Step: [12070] d_loss: 1.38633311, g_loss: 0.69289935\n",
      "Step: [12071] d_loss: 1.38630629, g_loss: 0.69330299\n",
      "Step: [12072] d_loss: 1.38626313, g_loss: 0.69298232\n",
      "Step: [12073] d_loss: 1.38612986, g_loss: 0.69313335\n",
      "Step: [12074] d_loss: 1.38622797, g_loss: 0.69313234\n",
      "Step: [12075] d_loss: 1.38615680, g_loss: 0.69340050\n",
      "Step: [12076] d_loss: 1.38633347, g_loss: 0.69206107\n",
      "Step: [12077] d_loss: 1.38626969, g_loss: 0.69304264\n",
      "Step: [12078] d_loss: 1.38626218, g_loss: 0.69373620\n",
      "Step: [12079] d_loss: 1.38630128, g_loss: 0.69495887\n",
      "Step: [12080] d_loss: 1.38652813, g_loss: 0.69394743\n",
      "Step: [12081] d_loss: 1.38627768, g_loss: 0.69393015\n",
      "Step: [12082] d_loss: 1.38610983, g_loss: 0.69081944\n",
      "Step: [12083] d_loss: 1.38630533, g_loss: 0.69074130\n",
      "Step: [12084] d_loss: 1.38643980, g_loss: 0.69392860\n",
      "Step: [12085] d_loss: 1.38647735, g_loss: 0.69438326\n",
      "Step: [12086] d_loss: 1.38622308, g_loss: 0.69468033\n",
      "Step: [12087] d_loss: 1.38598442, g_loss: 0.69367313\n",
      "Step: [12088] d_loss: 1.38650572, g_loss: 0.69291836\n",
      "Step: [12089] d_loss: 1.38665569, g_loss: 0.69260204\n",
      "Step: [12090] d_loss: 1.38618326, g_loss: 0.69323385\n",
      "Step: [12091] d_loss: 1.38625312, g_loss: 0.69401062\n",
      "Step: [12092] d_loss: 1.38618433, g_loss: 0.69333005\n",
      "Step: [12093] d_loss: 1.38631022, g_loss: 0.69356185\n",
      "Step: [12094] d_loss: 1.38647127, g_loss: 0.69377160\n",
      "Step: [12095] d_loss: 1.38652313, g_loss: 0.69288963\n",
      "Step: [12096] d_loss: 1.38631535, g_loss: 0.69328845\n",
      "Step: [12097] d_loss: 1.38649988, g_loss: 0.69225597\n",
      "Step: [12098] d_loss: 1.38617373, g_loss: 0.69315529\n",
      "Step: [12099] d_loss: 1.38571298, g_loss: 0.69665849\n",
      "Step: [12100] d_loss: 1.38615108, g_loss: 0.69270980\n",
      "Step: [12101] d_loss: 1.38664579, g_loss: 0.69576502\n",
      "Step: [12102] d_loss: 1.38673818, g_loss: 0.69250685\n",
      "Step: [12103] d_loss: 1.38664949, g_loss: 0.69383794\n",
      "Step: [12104] d_loss: 1.38647437, g_loss: 0.69265771\n",
      "Step: [12105] d_loss: 1.38624811, g_loss: 0.69209313\n",
      "Step: [12106] d_loss: 1.38652992, g_loss: 0.69399256\n",
      "Step: [12107] d_loss: 1.38626587, g_loss: 0.69370955\n",
      "Step: [12108] d_loss: 1.38626623, g_loss: 0.69451624\n",
      "Step: [12109] d_loss: 1.38654399, g_loss: 0.69246411\n",
      "Step: [12110] d_loss: 1.38625896, g_loss: 0.69252634\n",
      "Step: [12111] d_loss: 1.38626790, g_loss: 0.69318795\n",
      "Step: [12112] d_loss: 1.38643503, g_loss: 0.69297725\n",
      "Step: [12113] d_loss: 1.38621998, g_loss: 0.69376117\n",
      "Step: [12114] d_loss: 1.38617897, g_loss: 0.69313127\n",
      "Step: [12115] d_loss: 1.38623881, g_loss: 0.69268084\n",
      "Step: [12116] d_loss: 1.38623810, g_loss: 0.69357002\n",
      "Step: [12117] d_loss: 1.38613868, g_loss: 0.69401610\n",
      "Step: [12118] d_loss: 1.38594460, g_loss: 0.69327116\n",
      "Step: [12119] d_loss: 1.38639307, g_loss: 0.69381469\n",
      "Step: [12120] d_loss: 1.38620949, g_loss: 0.69317865\n",
      "Step: [12121] d_loss: 1.38590026, g_loss: 0.69265282\n",
      "Step: [12122] d_loss: 1.38609743, g_loss: 0.69302201\n",
      "Step: [12123] d_loss: 1.38606131, g_loss: 0.69386935\n",
      "Step: [12124] d_loss: 1.38629842, g_loss: 0.69254053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12125] d_loss: 1.38601470, g_loss: 0.69374430\n",
      "Step: [12126] d_loss: 1.38622856, g_loss: 0.69196564\n",
      "Step: [12127] d_loss: 1.38645196, g_loss: 0.69414568\n",
      "Step: [12128] d_loss: 1.38630509, g_loss: 0.69352889\n",
      "Step: [12129] d_loss: 1.38637042, g_loss: 0.69358569\n",
      "Step: [12130] d_loss: 1.38668752, g_loss: 0.69405985\n",
      "Step: [12131] d_loss: 1.38655901, g_loss: 0.69136363\n",
      "Step: [12132] d_loss: 1.38659871, g_loss: 0.69373441\n",
      "Step: [12133] d_loss: 1.38630486, g_loss: 0.69319463\n",
      "Step: [12134] d_loss: 1.38572478, g_loss: 0.69324887\n",
      "Step: [12135] d_loss: 1.38609397, g_loss: 0.69295263\n",
      "Step: [12136] d_loss: 1.38688052, g_loss: 0.69206697\n",
      "Step: [12137] d_loss: 1.38631177, g_loss: 0.69423974\n",
      "Step: [12138] d_loss: 1.38628078, g_loss: 0.69264948\n",
      "Step: [12139] d_loss: 1.38649845, g_loss: 0.69164491\n",
      "Step: [12140] d_loss: 1.38593853, g_loss: 0.69342184\n",
      "Step: [12141] d_loss: 1.38645077, g_loss: 0.69434142\n",
      "Step: [12142] d_loss: 1.38670492, g_loss: 0.68971741\n",
      "Step: [12143] d_loss: 1.38673043, g_loss: 0.69260740\n",
      "Step: [12144] d_loss: 1.38648176, g_loss: 0.69129384\n",
      "Step: [12145] d_loss: 1.38652110, g_loss: 0.69298488\n",
      "Step: [12146] d_loss: 1.38637662, g_loss: 0.69176257\n",
      "Step: [12147] d_loss: 1.38629627, g_loss: 0.69239581\n",
      "Step: [12148] d_loss: 1.38621402, g_loss: 0.69604194\n",
      "Step: [12149] d_loss: 1.38677943, g_loss: 0.69321650\n",
      "Step: [12150] d_loss: 1.38647938, g_loss: 0.69470137\n",
      "Step: [12151] d_loss: 1.38646877, g_loss: 0.69275439\n",
      "Step: [12152] d_loss: 1.38651705, g_loss: 0.69668126\n",
      "Step: [12153] d_loss: 1.38679087, g_loss: 0.69175899\n",
      "Step: [12154] d_loss: 1.38715792, g_loss: 0.69249046\n",
      "Step: [12155] d_loss: 1.38767195, g_loss: 0.68685520\n",
      "Step: [12156] d_loss: 1.38757765, g_loss: 0.69408780\n",
      "Step: [12157] d_loss: 1.38660324, g_loss: 0.69337362\n",
      "Step: [12158] d_loss: 1.38616252, g_loss: 0.69416851\n",
      "Step: [12159] d_loss: 1.38636065, g_loss: 0.69551539\n",
      "Step: [12160] d_loss: 1.38637567, g_loss: 0.69241428\n",
      "Step: [12161] d_loss: 1.38670087, g_loss: 0.69374609\n",
      "Step: [12162] d_loss: 1.38682556, g_loss: 0.69263494\n",
      "Step: [12163] d_loss: 1.38621497, g_loss: 0.69092017\n",
      "Step: [12164] d_loss: 1.38663757, g_loss: 0.69193721\n",
      "Step: [12165] d_loss: 1.38617420, g_loss: 0.69322735\n",
      "Step: [12166] d_loss: 1.38624740, g_loss: 0.69413912\n",
      "Step: [12167] d_loss: 1.38642752, g_loss: 0.69329977\n",
      "Step: [12168] d_loss: 1.38624907, g_loss: 0.69344854\n",
      "Step: [12169] d_loss: 1.38623023, g_loss: 0.69292831\n",
      "Step: [12170] d_loss: 1.38630104, g_loss: 0.69305420\n",
      "Step: [12171] d_loss: 1.38623857, g_loss: 0.69326949\n",
      "Step: [12172] d_loss: 1.38625431, g_loss: 0.69329768\n",
      "Step: [12173] d_loss: 1.38610208, g_loss: 0.69384319\n",
      "Step: [12174] d_loss: 1.38615596, g_loss: 0.69375575\n",
      "Step: [12175] d_loss: 1.38621736, g_loss: 0.69364542\n",
      "Step: [12176] d_loss: 1.38632846, g_loss: 0.69311321\n",
      "Step: [12177] d_loss: 1.39000511, g_loss: 0.70507741\n",
      "Step: [12178] d_loss: 1.38651419, g_loss: 0.68943220\n",
      "Step: [12179] d_loss: 1.38864374, g_loss: 0.69869125\n",
      "Step: [12180] d_loss: 1.38725448, g_loss: 0.68950605\n",
      "Step: [12181] d_loss: 1.38672209, g_loss: 0.69272876\n",
      "Step: [12182] d_loss: 1.38629496, g_loss: 0.69346488\n",
      "Step: [12183] d_loss: 1.38627076, g_loss: 0.69418442\n",
      "Step: [12184] d_loss: 1.38617826, g_loss: 0.69485199\n",
      "Step: [12185] d_loss: 1.38627529, g_loss: 0.69332379\n",
      "Step: [12186] d_loss: 1.38610053, g_loss: 0.69383240\n",
      "Step: [12187] d_loss: 1.38606882, g_loss: 0.69335711\n",
      "Step: [12188] d_loss: 1.38602161, g_loss: 0.69330579\n",
      "Step: [12189] d_loss: 1.38618279, g_loss: 0.69142097\n",
      "Step: [12190] d_loss: 1.38676476, g_loss: 0.69556451\n",
      "Step: [12191] d_loss: 1.38718486, g_loss: 0.69220567\n",
      "Step: [12192] d_loss: 1.38668001, g_loss: 0.69372690\n",
      "Step: [12193] d_loss: 1.38639998, g_loss: 0.69157946\n",
      "Step: [12194] d_loss: 1.38645577, g_loss: 0.69235772\n",
      "Step: [12195] d_loss: 1.38606060, g_loss: 0.69367862\n",
      "Step: [12196] d_loss: 1.38620698, g_loss: 0.69285941\n",
      "Step: [12197] d_loss: 1.38638997, g_loss: 0.69259262\n",
      "Step: [12198] d_loss: 1.38645995, g_loss: 0.69378483\n",
      "Step: [12199] d_loss: 1.38624644, g_loss: 0.69287193\n",
      "Step: [12200] d_loss: 1.38640523, g_loss: 0.69476295\n",
      "Step: [12201] d_loss: 1.38662410, g_loss: 0.69186413\n",
      "Step: [12202] d_loss: 1.38622069, g_loss: 0.69326150\n",
      "Step: [12203] d_loss: 1.38617945, g_loss: 0.69235569\n",
      "Step: [12204] d_loss: 1.38633537, g_loss: 0.69307369\n",
      "Step: [12205] d_loss: 1.38609207, g_loss: 0.69376373\n",
      "Step: [12206] d_loss: 1.38633525, g_loss: 0.69282258\n",
      "Step: [12207] d_loss: 1.38599730, g_loss: 0.69348824\n",
      "Step: [12208] d_loss: 1.38615584, g_loss: 0.69176412\n",
      "Step: [12209] d_loss: 1.38635552, g_loss: 0.69391018\n",
      "Step: [12210] d_loss: 1.38620305, g_loss: 0.69309759\n",
      "Step: [12211] d_loss: 1.38638353, g_loss: 0.69244236\n",
      "Step: [12212] d_loss: 1.38637495, g_loss: 0.69203693\n",
      "Step: [12213] d_loss: 1.38624609, g_loss: 0.69281816\n",
      "Step: [12214] d_loss: 1.38645220, g_loss: 0.69253486\n",
      "Step: [12215] d_loss: 1.38626194, g_loss: 0.69448084\n",
      "Step: [12216] d_loss: 1.38643277, g_loss: 0.69319451\n",
      "Step: [12217] d_loss: 1.38628757, g_loss: 0.69371897\n",
      "Step: [12218] d_loss: 1.38620424, g_loss: 0.69355536\n",
      "Step: [12219] d_loss: 1.38644719, g_loss: 0.69350642\n",
      "Step: [12220] d_loss: 1.38607860, g_loss: 0.69296777\n",
      "Step: [12221] d_loss: 1.38632059, g_loss: 0.69241619\n",
      "Step: [12222] d_loss: 1.38611495, g_loss: 0.69336855\n",
      "Step: [12223] d_loss: 1.38645065, g_loss: 0.69250751\n",
      "Step: [12224] d_loss: 1.38610220, g_loss: 0.69378853\n",
      "Step: [12225] d_loss: 1.38651466, g_loss: 0.69303250\n",
      "Step: [12226] d_loss: 1.38624668, g_loss: 0.69329822\n",
      "Step: [12227] d_loss: 1.38619494, g_loss: 0.69295675\n",
      "Step: [12228] d_loss: 1.38632727, g_loss: 0.69258648\n",
      "Step: [12229] d_loss: 1.38617313, g_loss: 0.69268566\n",
      "Step: [12230] d_loss: 1.38628805, g_loss: 0.69342387\n",
      "Step: [12231] d_loss: 1.38631356, g_loss: 0.69330150\n",
      "Step: [12232] d_loss: 1.38623476, g_loss: 0.69477677\n",
      "Step: [12233] d_loss: 1.38603282, g_loss: 0.69447649\n",
      "Step: [12234] d_loss: 1.38633633, g_loss: 0.69249320\n",
      "Step: [12235] d_loss: 1.38653302, g_loss: 0.69103789\n",
      "Step: [12236] d_loss: 1.38685334, g_loss: 0.69514394\n",
      "Step: [12237] d_loss: 1.38650084, g_loss: 0.69124645\n",
      "Step: [12238] d_loss: 1.38775730, g_loss: 0.69431591\n",
      "Step: [12239] d_loss: 1.38627315, g_loss: 0.69275928\n",
      "Step: [12240] d_loss: 1.38619590, g_loss: 0.69325584\n",
      "Step: [12241] d_loss: 1.38633025, g_loss: 0.69287109\n",
      "Step: [12242] d_loss: 1.38624525, g_loss: 0.69330978\n",
      "Step: [12243] d_loss: 1.38608229, g_loss: 0.69416595\n",
      "Step: [12244] d_loss: 1.38616371, g_loss: 0.69375026\n",
      "Step: [12245] d_loss: 1.38614464, g_loss: 0.69351029\n",
      "Step: [12246] d_loss: 1.38632083, g_loss: 0.69369495\n",
      "Step: [12247] d_loss: 1.38613868, g_loss: 0.69304711\n",
      "Step: [12248] d_loss: 1.38618660, g_loss: 0.69301939\n",
      "Step: [12249] d_loss: 1.38653827, g_loss: 0.69624507\n",
      "Step: [12250] d_loss: 1.38617349, g_loss: 0.69295096\n",
      "Step: [12251] d_loss: 1.38634908, g_loss: 0.69321191\n",
      "Step: [12252] d_loss: 1.38628983, g_loss: 0.69306803\n",
      "Step: [12253] d_loss: 1.38631225, g_loss: 0.69360363\n",
      "Step: [12254] d_loss: 1.38624132, g_loss: 0.69442672\n",
      "Step: [12255] d_loss: 1.38625777, g_loss: 0.69354331\n",
      "Step: [12256] d_loss: 1.38633537, g_loss: 0.69377422\n",
      "Step: [12257] d_loss: 1.38647699, g_loss: 0.69152665\n",
      "Step: [12258] d_loss: 1.38659787, g_loss: 0.69288456\n",
      "Step: [12259] d_loss: 1.38664079, g_loss: 0.69202614\n",
      "Step: [12260] d_loss: 1.38643754, g_loss: 0.69252586\n",
      "Step: [12261] d_loss: 1.38673997, g_loss: 0.69084990\n",
      "Step: [12262] d_loss: 1.38630843, g_loss: 0.69235516\n",
      "Step: [12263] d_loss: 1.38625622, g_loss: 0.69432545\n",
      "Step: [12264] d_loss: 1.38612866, g_loss: 0.69404227\n",
      "Step: [12265] d_loss: 1.38631463, g_loss: 0.69193989\n",
      "Step: [12266] d_loss: 1.38650405, g_loss: 0.69389331\n",
      "Step: [12267] d_loss: 1.38637221, g_loss: 0.69201779\n",
      "Step: [12268] d_loss: 1.38633394, g_loss: 0.69372010\n",
      "Step: [12269] d_loss: 1.38608313, g_loss: 0.69280887\n",
      "Step: [12270] d_loss: 1.38669538, g_loss: 0.69752574\n",
      "Step: [12271] d_loss: 1.38641906, g_loss: 0.69224823\n",
      "Step: [12272] d_loss: 1.38655686, g_loss: 0.69541794\n",
      "Step: [12273] d_loss: 1.38739181, g_loss: 0.69563347\n",
      "Step: [12274] d_loss: 1.38840890, g_loss: 0.70259148\n",
      "Step: [12275] d_loss: 1.38836789, g_loss: 0.69368362\n",
      "Step: [12276] d_loss: 1.38724041, g_loss: 0.69375765\n",
      "Step: [12277] d_loss: 1.38644505, g_loss: 0.69190550\n",
      "Step: [12278] d_loss: 1.38628781, g_loss: 0.69209081\n",
      "Step: [12279] d_loss: 1.38633251, g_loss: 0.69401956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12280] d_loss: 1.38612294, g_loss: 0.69279099\n",
      "Step: [12281] d_loss: 1.38633442, g_loss: 0.69379997\n",
      "Step: [12282] d_loss: 1.38621402, g_loss: 0.69282246\n",
      "Step: [12283] d_loss: 1.38626552, g_loss: 0.69315022\n",
      "Step: [12284] d_loss: 1.38621271, g_loss: 0.69273084\n",
      "Step: [12285] d_loss: 1.38621020, g_loss: 0.69294709\n",
      "Step: [12286] d_loss: 1.38632607, g_loss: 0.69350410\n",
      "Step: [12287] d_loss: 1.38641644, g_loss: 0.69279516\n",
      "Step: [12288] d_loss: 1.38607979, g_loss: 0.69303918\n",
      "Step: [12289] d_loss: 1.38628674, g_loss: 0.69326037\n",
      "Step: [12290] d_loss: 1.38626289, g_loss: 0.69339859\n",
      "Step: [12291] d_loss: 1.38618398, g_loss: 0.69321156\n",
      "Step: [12292] d_loss: 1.38629937, g_loss: 0.69365215\n",
      "Step: [12293] d_loss: 1.38622785, g_loss: 0.69306505\n",
      "Step: [12294] d_loss: 1.38618636, g_loss: 0.69329339\n",
      "Step: [12295] d_loss: 1.38689828, g_loss: 0.69175231\n",
      "Step: [12296] d_loss: 1.38650632, g_loss: 0.69345695\n",
      "Step: [12297] d_loss: 1.38614535, g_loss: 0.69342566\n",
      "Step: [12298] d_loss: 1.38619363, g_loss: 0.69318056\n",
      "Step: [12299] d_loss: 1.38603616, g_loss: 0.69323242\n",
      "Step: [12300] d_loss: 1.38627243, g_loss: 0.69312036\n",
      "Step: [12301] d_loss: 1.38631368, g_loss: 0.69309735\n",
      "Step: [12302] d_loss: 1.38641894, g_loss: 0.69303894\n",
      "Step: [12303] d_loss: 1.38621390, g_loss: 0.69370067\n",
      "Step: [12304] d_loss: 1.38614631, g_loss: 0.69206262\n",
      "Step: [12305] d_loss: 1.38615537, g_loss: 0.69330478\n",
      "Step: [12306] d_loss: 1.38655174, g_loss: 0.69277120\n",
      "Step: [12307] d_loss: 1.38622594, g_loss: 0.69332761\n",
      "Step: [12308] d_loss: 1.38631737, g_loss: 0.69361687\n",
      "Step: [12309] d_loss: 1.38626933, g_loss: 0.69308645\n",
      "Step: [12310] d_loss: 1.38617373, g_loss: 0.69293368\n",
      "Step: [12311] d_loss: 1.38626492, g_loss: 0.69311750\n",
      "Step: [12312] d_loss: 1.38624954, g_loss: 0.69337255\n",
      "Step: [12313] d_loss: 1.38622475, g_loss: 0.69319153\n",
      "Step: [12314] d_loss: 1.38636720, g_loss: 0.69322360\n",
      "Step: [12315] d_loss: 1.38620090, g_loss: 0.69354159\n",
      "Step: [12316] d_loss: 1.38623083, g_loss: 0.69308758\n",
      "Step: [12317] d_loss: 1.38633895, g_loss: 0.69311857\n",
      "Step: [12318] d_loss: 1.38634062, g_loss: 0.69290960\n",
      "Step: [12319] d_loss: 1.38625956, g_loss: 0.69328797\n",
      "Step: [12320] d_loss: 1.38625932, g_loss: 0.69312215\n",
      "Step: [12321] d_loss: 1.38629687, g_loss: 0.69336534\n",
      "Step: [12322] d_loss: 1.38632441, g_loss: 0.69325370\n",
      "Step: [12323] d_loss: 1.38711762, g_loss: 0.69808066\n",
      "Step: [12324] d_loss: 1.38628900, g_loss: 0.69239175\n",
      "Step: [12325] d_loss: 1.38636959, g_loss: 0.69325650\n",
      "Step: [12326] d_loss: 1.38621688, g_loss: 0.69282782\n",
      "Step: [12327] d_loss: 1.38622522, g_loss: 0.69309819\n",
      "Step: [12328] d_loss: 1.38633049, g_loss: 0.69328177\n",
      "Step: [12329] d_loss: 1.38635802, g_loss: 0.69315761\n",
      "Step: [12330] d_loss: 1.38619673, g_loss: 0.69326866\n",
      "Step: [12331] d_loss: 1.38634062, g_loss: 0.69358218\n",
      "Step: [12332] d_loss: 1.38652897, g_loss: 0.69395161\n",
      "Step: [12333] d_loss: 1.38619351, g_loss: 0.69294947\n",
      "Step: [12334] d_loss: 1.38609064, g_loss: 0.69303429\n",
      "Step: [12335] d_loss: 1.38621116, g_loss: 0.69322371\n",
      "Step: [12336] d_loss: 1.38618135, g_loss: 0.69319904\n",
      "Step: [12337] d_loss: 1.38624406, g_loss: 0.69325423\n",
      "Step: [12338] d_loss: 1.38622570, g_loss: 0.69312561\n",
      "Step: [12339] d_loss: 1.38613856, g_loss: 0.69335210\n",
      "Step: [12340] d_loss: 1.38618875, g_loss: 0.69356143\n",
      "Step: [12341] d_loss: 1.38623440, g_loss: 0.69350219\n",
      "Step: [12342] d_loss: 1.38631320, g_loss: 0.69326884\n",
      "Step: [12343] d_loss: 1.38620639, g_loss: 0.69315439\n",
      "Step: [12344] d_loss: 1.38615870, g_loss: 0.69261324\n",
      "Step: [12345] d_loss: 1.38637996, g_loss: 0.69329339\n",
      "Step: [12346] d_loss: 1.38634109, g_loss: 0.69527125\n",
      "Step: [12347] d_loss: 1.38649440, g_loss: 0.69256824\n",
      "Step: [12348] d_loss: 1.38622165, g_loss: 0.69351500\n",
      "Step: [12349] d_loss: 1.38628554, g_loss: 0.69241667\n",
      "Step: [12350] d_loss: 1.38628423, g_loss: 0.69343722\n",
      "Step: [12351] d_loss: 1.38605917, g_loss: 0.69320917\n",
      "Step: [12352] d_loss: 1.38620567, g_loss: 0.69349360\n",
      "Step: [12353] d_loss: 1.38650477, g_loss: 0.69090259\n",
      "Step: [12354] d_loss: 1.38676023, g_loss: 0.69349015\n",
      "Step: [12355] d_loss: 1.38677561, g_loss: 0.69312489\n",
      "Step: [12356] d_loss: 1.38664222, g_loss: 0.69593847\n",
      "Step: [12357] d_loss: 1.38626409, g_loss: 0.69425434\n",
      "Step: [12358] d_loss: 1.38629341, g_loss: 0.69342566\n",
      "Step: [12359] d_loss: 1.38613355, g_loss: 0.69338584\n",
      "Step: [12360] d_loss: 1.38626635, g_loss: 0.69259131\n",
      "Step: [12361] d_loss: 1.38622975, g_loss: 0.69352353\n",
      "Step: [12362] d_loss: 1.38617301, g_loss: 0.69309616\n",
      "Step: [12363] d_loss: 1.38615942, g_loss: 0.69280112\n",
      "Step: [12364] d_loss: 1.38632965, g_loss: 0.69277650\n",
      "Step: [12365] d_loss: 1.38614202, g_loss: 0.69446492\n",
      "Step: [12366] d_loss: 1.38631213, g_loss: 0.69329703\n",
      "Step: [12367] d_loss: 1.38648939, g_loss: 0.69432294\n",
      "Step: [12368] d_loss: 1.38686895, g_loss: 0.69302815\n",
      "Step: [12369] d_loss: 1.38678253, g_loss: 0.69740850\n",
      "Step: [12370] d_loss: 1.38695192, g_loss: 0.69250607\n",
      "Step: [12371] d_loss: 1.38672590, g_loss: 0.69085985\n",
      "Step: [12372] d_loss: 1.38659167, g_loss: 0.68962681\n",
      "Step: [12373] d_loss: 1.38643253, g_loss: 0.69253939\n",
      "Step: [12374] d_loss: 1.38627458, g_loss: 0.69214636\n",
      "Step: [12375] d_loss: 1.38638163, g_loss: 0.69327319\n",
      "Step: [12376] d_loss: 1.38627982, g_loss: 0.69413835\n",
      "Step: [12377] d_loss: 1.38628137, g_loss: 0.69383478\n",
      "Step: [12378] d_loss: 1.38628507, g_loss: 0.69333065\n",
      "Step: [12379] d_loss: 1.38628638, g_loss: 0.69286561\n",
      "Step: [12380] d_loss: 1.38623762, g_loss: 0.69279444\n",
      "Step: [12381] d_loss: 1.38614500, g_loss: 0.69310975\n",
      "Step: [12382] d_loss: 1.38630474, g_loss: 0.69369251\n",
      "Step: [12383] d_loss: 1.38634574, g_loss: 0.69351190\n",
      "Step: [12384] d_loss: 1.38629055, g_loss: 0.69330627\n",
      "Step: [12385] d_loss: 1.38628006, g_loss: 0.69289947\n",
      "Step: [12386] d_loss: 1.38631034, g_loss: 0.69288540\n",
      "Step: [12387] d_loss: 1.38623381, g_loss: 0.69334662\n",
      "Step: [12388] d_loss: 1.38621283, g_loss: 0.69311702\n",
      "Step: [12389] d_loss: 1.38638794, g_loss: 0.69349688\n",
      "Step: [12390] d_loss: 1.38625288, g_loss: 0.69305354\n",
      "Step: [12391] d_loss: 1.38626182, g_loss: 0.69315290\n",
      "Step: [12392] d_loss: 1.38655567, g_loss: 0.69358921\n",
      "Step: [12393] d_loss: 1.38632655, g_loss: 0.69298047\n",
      "Step: [12394] d_loss: 1.38619328, g_loss: 0.69338214\n",
      "Step: [12395] d_loss: 1.38628185, g_loss: 0.69295365\n",
      "Step: [12396] d_loss: 1.38630080, g_loss: 0.69322294\n",
      "Step: [12397] d_loss: 1.38624990, g_loss: 0.69321561\n",
      "Step: [12398] d_loss: 1.38620186, g_loss: 0.69319141\n",
      "Step: [12399] d_loss: 1.38634205, g_loss: 0.69311488\n",
      "Step: [12400] d_loss: 1.38617396, g_loss: 0.69321644\n",
      "Step: [12401] d_loss: 1.38621688, g_loss: 0.69400519\n",
      "Step: [12402] d_loss: 1.38632274, g_loss: 0.69335473\n",
      "Step: [12403] d_loss: 1.38638055, g_loss: 0.69348586\n",
      "Step: [12404] d_loss: 1.38621974, g_loss: 0.69308853\n",
      "Step: [12405] d_loss: 1.38631320, g_loss: 0.69281363\n",
      "Step: [12406] d_loss: 1.38640904, g_loss: 0.69397724\n",
      "Step: [12407] d_loss: 1.38660669, g_loss: 0.69156814\n",
      "Step: [12408] d_loss: 1.38682890, g_loss: 0.69589901\n",
      "Step: [12409] d_loss: 1.38664973, g_loss: 0.69303381\n",
      "Step: [12410] d_loss: 1.38652396, g_loss: 0.69495571\n",
      "Step: [12411] d_loss: 1.38637471, g_loss: 0.69344032\n",
      "Step: [12412] d_loss: 1.38625693, g_loss: 0.69349211\n",
      "Step: [12413] d_loss: 1.38627934, g_loss: 0.69325042\n",
      "Step: [12414] d_loss: 1.38621664, g_loss: 0.69256675\n",
      "Step: [12415] d_loss: 1.38625300, g_loss: 0.69283783\n",
      "Step: [12416] d_loss: 1.38629699, g_loss: 0.69294029\n",
      "Step: [12417] d_loss: 1.38631511, g_loss: 0.69372636\n",
      "Step: [12418] d_loss: 1.38625479, g_loss: 0.69360912\n",
      "Step: [12419] d_loss: 1.38631105, g_loss: 0.69335920\n",
      "Step: [12420] d_loss: 1.38624179, g_loss: 0.69340879\n",
      "Step: [12421] d_loss: 1.38628340, g_loss: 0.69266701\n",
      "Step: [12422] d_loss: 1.38632870, g_loss: 0.69328332\n",
      "Step: [12423] d_loss: 1.38620567, g_loss: 0.69293010\n",
      "Step: [12424] d_loss: 1.38628411, g_loss: 0.69324327\n",
      "Step: [12425] d_loss: 1.38628614, g_loss: 0.69314367\n",
      "Step: [12426] d_loss: 1.38625479, g_loss: 0.69346058\n",
      "Step: [12427] d_loss: 1.38629091, g_loss: 0.69345671\n",
      "Step: [12428] d_loss: 1.38626158, g_loss: 0.69309485\n",
      "Step: [12429] d_loss: 1.38618279, g_loss: 0.69303274\n",
      "Step: [12430] d_loss: 1.38626122, g_loss: 0.69293201\n",
      "Step: [12431] d_loss: 1.38616550, g_loss: 0.69297522\n",
      "Step: [12432] d_loss: 1.38631511, g_loss: 0.69301045\n",
      "Step: [12433] d_loss: 1.38624132, g_loss: 0.69307536\n",
      "Step: [12434] d_loss: 1.38623011, g_loss: 0.69330591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12435] d_loss: 1.38621020, g_loss: 0.69328040\n",
      "Step: [12436] d_loss: 1.38619328, g_loss: 0.69332701\n",
      "Step: [12437] d_loss: 1.38615680, g_loss: 0.69302541\n",
      "Step: [12438] d_loss: 1.38624835, g_loss: 0.69322032\n",
      "Step: [12439] d_loss: 1.38652396, g_loss: 0.69593954\n",
      "Step: [12440] d_loss: 1.38667774, g_loss: 0.69247460\n",
      "Step: [12441] d_loss: 1.38657451, g_loss: 0.69405127\n",
      "Step: [12442] d_loss: 1.38642955, g_loss: 0.69205332\n",
      "Step: [12443] d_loss: 1.38635361, g_loss: 0.69362020\n",
      "Step: [12444] d_loss: 1.38627791, g_loss: 0.69362015\n",
      "Step: [12445] d_loss: 1.38615668, g_loss: 0.69340593\n",
      "Step: [12446] d_loss: 1.38640094, g_loss: 0.69335622\n",
      "Step: [12447] d_loss: 1.38626051, g_loss: 0.69318986\n",
      "Step: [12448] d_loss: 1.38618481, g_loss: 0.69291091\n",
      "Step: [12449] d_loss: 1.38634503, g_loss: 0.69321561\n",
      "Step: [12450] d_loss: 1.38626385, g_loss: 0.69296443\n",
      "Step: [12451] d_loss: 1.38619339, g_loss: 0.69326001\n",
      "Step: [12452] d_loss: 1.38634741, g_loss: 0.69309682\n",
      "Step: [12453] d_loss: 1.38626802, g_loss: 0.69328743\n",
      "Step: [12454] d_loss: 1.38628769, g_loss: 0.69319034\n",
      "Step: [12455] d_loss: 1.38626754, g_loss: 0.69303715\n",
      "Step: [12456] d_loss: 1.38624036, g_loss: 0.69340706\n",
      "Step: [12457] d_loss: 1.38623714, g_loss: 0.69286537\n",
      "Step: [12458] d_loss: 1.38622153, g_loss: 0.69308138\n",
      "Step: [12459] d_loss: 1.38618398, g_loss: 0.69306928\n",
      "Step: [12460] d_loss: 1.38624501, g_loss: 0.69301903\n",
      "Step: [12461] d_loss: 1.38585258, g_loss: 0.69415998\n",
      "Step: [12462] d_loss: 1.38617492, g_loss: 0.69388980\n",
      "Step: [12463] d_loss: 1.38608646, g_loss: 0.69336557\n",
      "Step: [12464] d_loss: 1.38623118, g_loss: 0.69301516\n",
      "Step: [12465] d_loss: 1.38620663, g_loss: 0.69276929\n",
      "Step: [12466] d_loss: 1.38627851, g_loss: 0.69313955\n",
      "Step: [12467] d_loss: 1.38597631, g_loss: 0.69244695\n",
      "Step: [12468] d_loss: 1.38608408, g_loss: 0.69245803\n",
      "Step: [12469] d_loss: 1.38627315, g_loss: 0.69266450\n",
      "Step: [12470] d_loss: 1.38626957, g_loss: 0.69401085\n",
      "Step: [12471] d_loss: 1.38622379, g_loss: 0.69505405\n",
      "Step: [12472] d_loss: 1.38617742, g_loss: 0.69349492\n",
      "Step: [12473] d_loss: 1.38638735, g_loss: 0.69415182\n",
      "Step: [12474] d_loss: 1.38662624, g_loss: 0.69404209\n",
      "Step: [12475] d_loss: 1.38641047, g_loss: 0.69234902\n",
      "Step: [12476] d_loss: 1.38641500, g_loss: 0.69282222\n",
      "Step: [12477] d_loss: 1.38634467, g_loss: 0.69284201\n",
      "Step: [12478] d_loss: 1.38642001, g_loss: 0.69340265\n",
      "Step: [12479] d_loss: 1.38638377, g_loss: 0.69341540\n",
      "Step: [12480] d_loss: 1.38636041, g_loss: 0.69377100\n",
      "Step: [12481] d_loss: 1.38616323, g_loss: 0.69326311\n",
      "Step: [12482] d_loss: 1.38618183, g_loss: 0.69262731\n",
      "Step: [12483] d_loss: 1.38615918, g_loss: 0.69310963\n",
      "Step: [12484] d_loss: 1.38625121, g_loss: 0.69345772\n",
      "Step: [12485] d_loss: 1.38613307, g_loss: 0.69360948\n",
      "Step: [12486] d_loss: 1.38648438, g_loss: 0.69308633\n",
      "Step: [12487] d_loss: 1.38652754, g_loss: 0.69278955\n",
      "Step: [12488] d_loss: 1.38619339, g_loss: 0.69243443\n",
      "Step: [12489] d_loss: 1.38631463, g_loss: 0.69360816\n",
      "Step: [12490] d_loss: 1.38638318, g_loss: 0.69382900\n",
      "Step: [12491] d_loss: 1.38619304, g_loss: 0.69341600\n",
      "Step: [12492] d_loss: 1.38632894, g_loss: 0.69331139\n",
      "Step: [12493] d_loss: 1.38618648, g_loss: 0.69310570\n",
      "Step: [12494] d_loss: 1.38625634, g_loss: 0.69181228\n",
      "Step: [12495] d_loss: 1.38635159, g_loss: 0.69413680\n",
      "Step: [12496] d_loss: 1.38640404, g_loss: 0.69246525\n",
      "Step: [12497] d_loss: 1.38624406, g_loss: 0.69371361\n",
      "Step: [12498] d_loss: 1.38640893, g_loss: 0.69240630\n",
      "Step: [12499] d_loss: 1.38646519, g_loss: 0.69459677\n",
      "Step: [12500] d_loss: 1.38678503, g_loss: 0.69240439\n",
      "Step: [12501] d_loss: 1.38653076, g_loss: 0.69557172\n",
      "Step: [12502] d_loss: 1.38634062, g_loss: 0.69612813\n",
      "Step: [12503] d_loss: 1.38644743, g_loss: 0.69233423\n",
      "Step: [12504] d_loss: 1.38644242, g_loss: 0.69158286\n",
      "Step: [12505] d_loss: 1.38639414, g_loss: 0.69106412\n",
      "Step: [12506] d_loss: 1.38633561, g_loss: 0.69346189\n",
      "Step: [12507] d_loss: 1.38568902, g_loss: 0.69314188\n",
      "Step: [12508] d_loss: 1.38655400, g_loss: 0.69363654\n",
      "Step: [12509] d_loss: 1.38622546, g_loss: 0.69319808\n",
      "Step: [12510] d_loss: 1.38620865, g_loss: 0.69372046\n",
      "Step: [12511] d_loss: 1.38622534, g_loss: 0.69301325\n",
      "Step: [12512] d_loss: 1.38661051, g_loss: 0.68972254\n",
      "Step: [12513] d_loss: 1.38709450, g_loss: 0.69606078\n",
      "Step: [12514] d_loss: 1.38735235, g_loss: 0.69405079\n",
      "Step: [12515] d_loss: 1.38709939, g_loss: 0.69835865\n",
      "Step: [12516] d_loss: 1.38682735, g_loss: 0.69360191\n",
      "Step: [12517] d_loss: 1.38671088, g_loss: 0.69320768\n",
      "Step: [12518] d_loss: 1.38622546, g_loss: 0.69112647\n",
      "Step: [12519] d_loss: 1.38624740, g_loss: 0.69228685\n",
      "Step: [12520] d_loss: 1.38617384, g_loss: 0.69321305\n",
      "Step: [12521] d_loss: 1.38612843, g_loss: 0.69388616\n",
      "Step: [12522] d_loss: 1.38641810, g_loss: 0.69349867\n",
      "Step: [12523] d_loss: 1.38628507, g_loss: 0.69304180\n",
      "Step: [12524] d_loss: 1.38620758, g_loss: 0.69342566\n",
      "Step: [12525] d_loss: 1.38630724, g_loss: 0.69254196\n",
      "Step: [12526] d_loss: 1.38630104, g_loss: 0.69377923\n",
      "Step: [12527] d_loss: 1.38596117, g_loss: 0.69355094\n",
      "Step: [12528] d_loss: 1.38611603, g_loss: 0.69301945\n",
      "Step: [12529] d_loss: 1.38624346, g_loss: 0.69308531\n",
      "Step: [12530] d_loss: 1.38618040, g_loss: 0.69430989\n",
      "Step: [12531] d_loss: 1.38595366, g_loss: 0.69350636\n",
      "Step: [12532] d_loss: 1.38630581, g_loss: 0.69293892\n",
      "Step: [12533] d_loss: 1.38657355, g_loss: 0.69168437\n",
      "Step: [12534] d_loss: 1.38659739, g_loss: 0.69431949\n",
      "Step: [12535] d_loss: 1.38734102, g_loss: 0.69347501\n",
      "Step: [12536] d_loss: 1.38833356, g_loss: 0.70113742\n",
      "Step: [12537] d_loss: 1.38871694, g_loss: 0.69294208\n",
      "Step: [12538] d_loss: 1.38799071, g_loss: 0.69527125\n",
      "Step: [12539] d_loss: 1.38735414, g_loss: 0.69103324\n",
      "Step: [12540] d_loss: 1.38679528, g_loss: 0.69357640\n",
      "Step: [12541] d_loss: 1.38690639, g_loss: 0.69024235\n",
      "Step: [12542] d_loss: 1.38581133, g_loss: 0.69310611\n",
      "Step: [12543] d_loss: 1.38640761, g_loss: 0.69413388\n",
      "Step: [12544] d_loss: 1.38624156, g_loss: 0.69393653\n",
      "Step: [12545] d_loss: 1.38632298, g_loss: 0.69328445\n",
      "Step: [12546] d_loss: 1.38617003, g_loss: 0.69314861\n",
      "Step: [12547] d_loss: 1.38624525, g_loss: 0.69268143\n",
      "Step: [12548] d_loss: 1.38611317, g_loss: 0.69345498\n",
      "Step: [12549] d_loss: 1.38621843, g_loss: 0.69274199\n",
      "Step: [12550] d_loss: 1.38631237, g_loss: 0.69329768\n",
      "Step: [12551] d_loss: 1.38632357, g_loss: 0.69344777\n",
      "Step: [12552] d_loss: 1.38634181, g_loss: 0.69314969\n",
      "Step: [12553] d_loss: 1.38627934, g_loss: 0.69275051\n",
      "Step: [12554] d_loss: 1.38639200, g_loss: 0.69337809\n",
      "Step: [12555] d_loss: 1.38628471, g_loss: 0.69300056\n",
      "Step: [12556] d_loss: 1.38636315, g_loss: 0.69375259\n",
      "Step: [12557] d_loss: 1.38594294, g_loss: 0.69300079\n",
      "Step: [12558] d_loss: 1.38639259, g_loss: 0.69338536\n",
      "Step: [12559] d_loss: 1.38637936, g_loss: 0.69326985\n",
      "Step: [12560] d_loss: 1.38614798, g_loss: 0.69289738\n",
      "Step: [12561] d_loss: 1.38623905, g_loss: 0.69332707\n",
      "Step: [12562] d_loss: 1.38623643, g_loss: 0.69267446\n",
      "Step: [12563] d_loss: 1.38610792, g_loss: 0.69193578\n",
      "Step: [12564] d_loss: 1.38634694, g_loss: 0.69403756\n",
      "Step: [12565] d_loss: 1.38652933, g_loss: 0.69626319\n",
      "Step: [12566] d_loss: 1.38660884, g_loss: 0.69361520\n",
      "Step: [12567] d_loss: 1.38643169, g_loss: 0.69362950\n",
      "Step: [12568] d_loss: 1.38640070, g_loss: 0.69277459\n",
      "Step: [12569] d_loss: 1.38623691, g_loss: 0.69276929\n",
      "Step: [12570] d_loss: 1.38638043, g_loss: 0.69340920\n",
      "Step: [12571] d_loss: 1.38624573, g_loss: 0.69310427\n",
      "Step: [12572] d_loss: 1.38630188, g_loss: 0.69309652\n",
      "Step: [12573] d_loss: 1.38624847, g_loss: 0.69312537\n",
      "Step: [12574] d_loss: 1.38625240, g_loss: 0.69365168\n",
      "Step: [12575] d_loss: 1.38619304, g_loss: 0.69361186\n",
      "Step: [12576] d_loss: 1.38636637, g_loss: 0.69340396\n",
      "Step: [12577] d_loss: 1.38635004, g_loss: 0.69336569\n",
      "Step: [12578] d_loss: 1.38639522, g_loss: 0.69243038\n",
      "Step: [12579] d_loss: 1.38628221, g_loss: 0.69254398\n",
      "Step: [12580] d_loss: 1.38629889, g_loss: 0.69407004\n",
      "Step: [12581] d_loss: 1.38633072, g_loss: 0.69357657\n",
      "Step: [12582] d_loss: 1.38630843, g_loss: 0.69348872\n",
      "Step: [12583] d_loss: 1.38629520, g_loss: 0.69287181\n",
      "Step: [12584] d_loss: 1.38625932, g_loss: 0.69310439\n",
      "Step: [12585] d_loss: 1.38625693, g_loss: 0.69285989\n",
      "Step: [12586] d_loss: 1.38619304, g_loss: 0.69304371\n",
      "Step: [12587] d_loss: 1.38623214, g_loss: 0.69317842\n",
      "Step: [12588] d_loss: 1.38623965, g_loss: 0.69339770\n",
      "Step: [12589] d_loss: 1.38614011, g_loss: 0.69295233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12590] d_loss: 1.38617885, g_loss: 0.69330144\n",
      "Step: [12591] d_loss: 1.38648713, g_loss: 0.69252592\n",
      "Step: [12592] d_loss: 1.38642621, g_loss: 0.69512868\n",
      "Step: [12593] d_loss: 1.38646817, g_loss: 0.69284040\n",
      "Step: [12594] d_loss: 1.38659310, g_loss: 0.69300485\n",
      "Step: [12595] d_loss: 1.38650727, g_loss: 0.69156563\n",
      "Step: [12596] d_loss: 1.38708246, g_loss: 0.69696522\n",
      "Step: [12597] d_loss: 1.38727713, g_loss: 0.69200557\n",
      "Step: [12598] d_loss: 1.38710999, g_loss: 0.69440782\n",
      "Step: [12599] d_loss: 1.38676262, g_loss: 0.69082093\n",
      "Step: [12600] d_loss: 1.38661242, g_loss: 0.69209051\n",
      "Step: [12601] d_loss: 1.38638520, g_loss: 0.69324124\n",
      "Step: [12602] d_loss: 1.38637161, g_loss: 0.69412756\n",
      "Step: [12603] d_loss: 1.38623750, g_loss: 0.69437450\n",
      "Step: [12604] d_loss: 1.38615441, g_loss: 0.69341636\n",
      "Step: [12605] d_loss: 1.38627732, g_loss: 0.69239426\n",
      "Step: [12606] d_loss: 1.38625646, g_loss: 0.69282436\n",
      "Step: [12607] d_loss: 1.38628054, g_loss: 0.69281751\n",
      "Step: [12608] d_loss: 1.38632917, g_loss: 0.69379812\n",
      "Step: [12609] d_loss: 1.38622129, g_loss: 0.69332373\n",
      "Step: [12610] d_loss: 1.38631976, g_loss: 0.69323069\n",
      "Step: [12611] d_loss: 1.38622618, g_loss: 0.69311482\n",
      "Step: [12612] d_loss: 1.38623369, g_loss: 0.69304252\n",
      "Step: [12613] d_loss: 1.38618994, g_loss: 0.69294924\n",
      "Step: [12614] d_loss: 1.38621473, g_loss: 0.69328654\n",
      "Step: [12615] d_loss: 1.38607407, g_loss: 0.69406116\n",
      "Step: [12616] d_loss: 1.38576317, g_loss: 0.69407856\n",
      "Step: [12617] d_loss: 1.38600516, g_loss: 0.69211650\n",
      "Step: [12618] d_loss: 1.38640261, g_loss: 0.69139719\n",
      "Step: [12619] d_loss: 1.38707709, g_loss: 0.69804645\n",
      "Step: [12620] d_loss: 1.38737559, g_loss: 0.69461387\n",
      "Step: [12621] d_loss: 1.38756740, g_loss: 0.69561851\n",
      "Step: [12622] d_loss: 1.38735545, g_loss: 0.69120038\n",
      "Step: [12623] d_loss: 1.38779402, g_loss: 0.69773257\n",
      "Step: [12624] d_loss: 1.38736832, g_loss: 0.69418675\n",
      "Step: [12625] d_loss: 1.38693690, g_loss: 0.69562161\n",
      "Step: [12626] d_loss: 1.38668656, g_loss: 0.69269300\n",
      "Step: [12627] d_loss: 1.38628221, g_loss: 0.69251728\n",
      "Step: [12628] d_loss: 1.38627863, g_loss: 0.69236469\n",
      "Step: [12629] d_loss: 1.38620210, g_loss: 0.69301224\n",
      "Step: [12630] d_loss: 1.38626599, g_loss: 0.69325781\n",
      "Step: [12631] d_loss: 1.38616562, g_loss: 0.69341576\n",
      "Step: [12632] d_loss: 1.38609791, g_loss: 0.69335389\n",
      "Step: [12633] d_loss: 1.38608146, g_loss: 0.69365162\n",
      "Step: [12634] d_loss: 1.38624358, g_loss: 0.69335383\n",
      "Step: [12635] d_loss: 1.38654375, g_loss: 0.69252151\n",
      "Step: [12636] d_loss: 1.38624954, g_loss: 0.69381601\n",
      "Step: [12637] d_loss: 1.38620651, g_loss: 0.69273126\n",
      "Step: [12638] d_loss: 1.38639629, g_loss: 0.69144738\n",
      "Step: [12639] d_loss: 1.38611794, g_loss: 0.69168091\n",
      "Step: [12640] d_loss: 1.38630056, g_loss: 0.69443738\n",
      "Step: [12641] d_loss: 1.38625574, g_loss: 0.69328284\n",
      "Step: [12642] d_loss: 1.38619936, g_loss: 0.69339228\n",
      "Step: [12643] d_loss: 1.38664341, g_loss: 0.69204479\n",
      "Step: [12644] d_loss: 1.38645458, g_loss: 0.69392264\n",
      "Step: [12645] d_loss: 1.38677645, g_loss: 0.69380963\n",
      "Step: [12646] d_loss: 1.38650703, g_loss: 0.69571859\n",
      "Step: [12647] d_loss: 1.38661373, g_loss: 0.69116592\n",
      "Step: [12648] d_loss: 1.38663137, g_loss: 0.69326168\n",
      "Step: [12649] d_loss: 1.38629293, g_loss: 0.69156837\n",
      "Step: [12650] d_loss: 1.38698173, g_loss: 0.69567436\n",
      "Step: [12651] d_loss: 1.38655472, g_loss: 0.69383979\n",
      "Step: [12652] d_loss: 1.38671124, g_loss: 0.69502878\n",
      "Step: [12653] d_loss: 1.38626325, g_loss: 0.69399977\n",
      "Step: [12654] d_loss: 1.38643730, g_loss: 0.69400996\n",
      "Step: [12655] d_loss: 1.38645315, g_loss: 0.69069636\n",
      "Step: [12656] d_loss: 1.38689971, g_loss: 0.69413835\n",
      "Step: [12657] d_loss: 1.38775015, g_loss: 0.69118118\n",
      "Step: [12658] d_loss: 1.38673711, g_loss: 0.69423187\n",
      "Step: [12659] d_loss: 1.38615084, g_loss: 0.69375098\n",
      "Step: [12660] d_loss: 1.38587523, g_loss: 0.69340563\n",
      "Step: [12661] d_loss: 1.38625956, g_loss: 0.69526148\n",
      "Step: [12662] d_loss: 1.38658774, g_loss: 0.69672328\n",
      "Step: [12663] d_loss: 1.38692200, g_loss: 0.69221854\n",
      "Step: [12664] d_loss: 1.38677716, g_loss: 0.69342476\n",
      "Step: [12665] d_loss: 1.38647258, g_loss: 0.69142759\n",
      "Step: [12666] d_loss: 1.38632536, g_loss: 0.69279432\n",
      "Step: [12667] d_loss: 1.38633931, g_loss: 0.69341946\n",
      "Step: [12668] d_loss: 1.38622880, g_loss: 0.69359905\n",
      "Step: [12669] d_loss: 1.38624275, g_loss: 0.69386810\n",
      "Step: [12670] d_loss: 1.38606381, g_loss: 0.69318140\n",
      "Step: [12671] d_loss: 1.38618672, g_loss: 0.69258344\n",
      "Step: [12672] d_loss: 1.38659668, g_loss: 0.69585913\n",
      "Step: [12673] d_loss: 1.38667107, g_loss: 0.69136548\n",
      "Step: [12674] d_loss: 1.38637948, g_loss: 0.69270134\n",
      "Step: [12675] d_loss: 1.38632643, g_loss: 0.69474334\n",
      "Step: [12676] d_loss: 1.38645601, g_loss: 0.69231105\n",
      "Step: [12677] d_loss: 1.38669121, g_loss: 0.69459295\n",
      "Step: [12678] d_loss: 1.38648355, g_loss: 0.69247657\n",
      "Step: [12679] d_loss: 1.38633406, g_loss: 0.69341195\n",
      "Step: [12680] d_loss: 1.38634241, g_loss: 0.69321394\n",
      "Step: [12681] d_loss: 1.38639867, g_loss: 0.69293016\n",
      "Step: [12682] d_loss: 1.38630486, g_loss: 0.69365609\n",
      "Step: [12683] d_loss: 1.38621426, g_loss: 0.69300199\n",
      "Step: [12684] d_loss: 1.38624406, g_loss: 0.69334120\n",
      "Step: [12685] d_loss: 1.38630676, g_loss: 0.69348520\n",
      "Step: [12686] d_loss: 1.38629866, g_loss: 0.69334698\n",
      "Step: [12687] d_loss: 1.38618374, g_loss: 0.69313610\n",
      "Step: [12688] d_loss: 1.38628805, g_loss: 0.69313502\n",
      "Step: [12689] d_loss: 1.38620532, g_loss: 0.69361091\n",
      "Step: [12690] d_loss: 1.38646829, g_loss: 0.69162107\n",
      "Step: [12691] d_loss: 1.38663375, g_loss: 0.69433999\n",
      "Step: [12692] d_loss: 1.38664508, g_loss: 0.69279033\n",
      "Step: [12693] d_loss: 1.38653350, g_loss: 0.69398797\n",
      "Step: [12694] d_loss: 1.38628256, g_loss: 0.69279981\n",
      "Step: [12695] d_loss: 1.38996184, g_loss: 0.69099760\n",
      "Step: [12696] d_loss: 1.38639927, g_loss: 0.69424951\n",
      "Step: [12697] d_loss: 1.38651776, g_loss: 0.69231999\n",
      "Step: [12698] d_loss: 1.38647401, g_loss: 0.69455749\n",
      "Step: [12699] d_loss: 1.38612831, g_loss: 0.69241309\n",
      "Step: [12700] d_loss: 1.38641453, g_loss: 0.69438577\n",
      "Step: [12701] d_loss: 1.38655341, g_loss: 0.69383609\n",
      "Step: [12702] d_loss: 1.38638043, g_loss: 0.69377112\n",
      "Step: [12703] d_loss: 1.38638043, g_loss: 0.69249159\n",
      "Step: [12704] d_loss: 1.38654518, g_loss: 0.69376028\n",
      "Step: [12705] d_loss: 1.38693154, g_loss: 0.68976545\n",
      "Step: [12706] d_loss: 1.38617373, g_loss: 0.69638801\n",
      "Step: [12707] d_loss: 1.38699508, g_loss: 0.69397426\n",
      "Step: [12708] d_loss: 1.38685441, g_loss: 0.69468117\n",
      "Step: [12709] d_loss: 1.38681078, g_loss: 0.69107264\n",
      "Step: [12710] d_loss: 1.38640749, g_loss: 0.69212675\n",
      "Step: [12711] d_loss: 1.38638854, g_loss: 0.69231844\n",
      "Step: [12712] d_loss: 1.38608515, g_loss: 0.69338942\n",
      "Step: [12713] d_loss: 1.38676190, g_loss: 0.69239640\n",
      "Step: [12714] d_loss: 1.38621664, g_loss: 0.69439757\n",
      "Step: [12715] d_loss: 1.38563645, g_loss: 0.69371134\n",
      "Step: [12716] d_loss: 1.38625813, g_loss: 0.69323641\n",
      "Step: [12717] d_loss: 1.38621557, g_loss: 0.69314760\n",
      "Step: [12718] d_loss: 1.38625741, g_loss: 0.69334751\n",
      "Step: [12719] d_loss: 1.38640511, g_loss: 0.69346476\n",
      "Step: [12720] d_loss: 1.38622105, g_loss: 0.69334877\n",
      "Step: [12721] d_loss: 1.38623524, g_loss: 0.69324005\n",
      "Step: [12722] d_loss: 1.38669395, g_loss: 0.69356304\n",
      "Step: [12723] d_loss: 1.38624406, g_loss: 0.69315070\n",
      "Step: [12724] d_loss: 1.38627791, g_loss: 0.69391418\n",
      "Step: [12725] d_loss: 1.38621926, g_loss: 0.69345331\n",
      "Step: [12726] d_loss: 1.38636708, g_loss: 0.69294602\n",
      "Step: [12727] d_loss: 1.38661051, g_loss: 0.68910992\n",
      "Step: [12728] d_loss: 1.38633800, g_loss: 0.69196141\n",
      "Step: [12729] d_loss: 1.38668120, g_loss: 0.69551247\n",
      "Step: [12730] d_loss: 1.38635743, g_loss: 0.69663119\n",
      "Step: [12731] d_loss: 1.38661933, g_loss: 0.69253039\n",
      "Step: [12732] d_loss: 1.38660002, g_loss: 0.69170594\n",
      "Step: [12733] d_loss: 1.38664603, g_loss: 0.69189507\n",
      "Step: [12734] d_loss: 1.38652718, g_loss: 0.69563723\n",
      "Step: [12735] d_loss: 1.38629568, g_loss: 0.69423670\n",
      "Step: [12736] d_loss: 1.38619924, g_loss: 0.69276077\n",
      "Step: [12737] d_loss: 1.38628268, g_loss: 0.69375926\n",
      "Step: [12738] d_loss: 1.38665032, g_loss: 0.69147849\n",
      "Step: [12739] d_loss: 1.38639736, g_loss: 0.69291437\n",
      "Step: [12740] d_loss: 1.38640726, g_loss: 0.69269824\n",
      "Step: [12741] d_loss: 1.38622534, g_loss: 0.69470811\n",
      "Step: [12742] d_loss: 1.38622189, g_loss: 0.69390666\n",
      "Step: [12743] d_loss: 1.38636327, g_loss: 0.69271833\n",
      "Step: [12744] d_loss: 1.38641047, g_loss: 0.69185823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12745] d_loss: 1.38628769, g_loss: 0.69296759\n",
      "Step: [12746] d_loss: 1.38628435, g_loss: 0.69296825\n",
      "Step: [12747] d_loss: 1.38624310, g_loss: 0.69406050\n",
      "Step: [12748] d_loss: 1.38623524, g_loss: 0.69358134\n",
      "Step: [12749] d_loss: 1.38623524, g_loss: 0.69312865\n",
      "Step: [12750] d_loss: 1.38625491, g_loss: 0.69328237\n",
      "Step: [12751] d_loss: 1.38584816, g_loss: 0.69339579\n",
      "Step: [12752] d_loss: 1.38625515, g_loss: 0.69358051\n",
      "Step: [12753] d_loss: 1.38623571, g_loss: 0.69331598\n",
      "Step: [12754] d_loss: 1.38618267, g_loss: 0.69353628\n",
      "Step: [12755] d_loss: 1.38619232, g_loss: 0.69327879\n",
      "Step: [12756] d_loss: 1.38630569, g_loss: 0.69316101\n",
      "Step: [12757] d_loss: 1.38635492, g_loss: 0.69430506\n",
      "Step: [12758] d_loss: 1.38643765, g_loss: 0.69260287\n",
      "Step: [12759] d_loss: 1.38634801, g_loss: 0.69288921\n",
      "Step: [12760] d_loss: 1.38783038, g_loss: 0.69434726\n",
      "Step: [12761] d_loss: 1.38599992, g_loss: 0.69424343\n",
      "Step: [12762] d_loss: 1.38629639, g_loss: 0.69262302\n",
      "Step: [12763] d_loss: 1.38636780, g_loss: 0.69443005\n",
      "Step: [12764] d_loss: 1.38706088, g_loss: 0.69306356\n",
      "Step: [12765] d_loss: 1.38725674, g_loss: 0.69430518\n",
      "Step: [12766] d_loss: 1.38574636, g_loss: 0.69252568\n",
      "Step: [12767] d_loss: 1.38639009, g_loss: 0.69172323\n",
      "Step: [12768] d_loss: 1.38629174, g_loss: 0.69201756\n",
      "Step: [12769] d_loss: 1.38615572, g_loss: 0.69321996\n",
      "Step: [12770] d_loss: 1.38625145, g_loss: 0.69369102\n",
      "Step: [12771] d_loss: 1.38623047, g_loss: 0.69349682\n",
      "Step: [12772] d_loss: 1.38644648, g_loss: 0.69314986\n",
      "Step: [12773] d_loss: 1.38622069, g_loss: 0.69299412\n",
      "Step: [12774] d_loss: 1.38622594, g_loss: 0.69312060\n",
      "Step: [12775] d_loss: 1.38611925, g_loss: 0.69333595\n",
      "Step: [12776] d_loss: 1.38624167, g_loss: 0.69299990\n",
      "Step: [12777] d_loss: 1.38632250, g_loss: 0.69323015\n",
      "Step: [12778] d_loss: 1.38608909, g_loss: 0.69315493\n",
      "Step: [12779] d_loss: 1.38682985, g_loss: 0.69333327\n",
      "Step: [12780] d_loss: 1.38622737, g_loss: 0.69322121\n",
      "Step: [12781] d_loss: 1.38626707, g_loss: 0.69307446\n",
      "Step: [12782] d_loss: 1.38642704, g_loss: 0.69540322\n",
      "Step: [12783] d_loss: 1.38638926, g_loss: 0.69365990\n",
      "Step: [12784] d_loss: 1.38642013, g_loss: 0.69258302\n",
      "Step: [12785] d_loss: 1.38693273, g_loss: 0.69124222\n",
      "Step: [12786] d_loss: 1.38628602, g_loss: 0.69297934\n",
      "Step: [12787] d_loss: 1.38620293, g_loss: 0.69275421\n",
      "Step: [12788] d_loss: 1.38656282, g_loss: 0.69522047\n",
      "Step: [12789] d_loss: 1.38716841, g_loss: 0.69139040\n",
      "Step: [12790] d_loss: 1.38800573, g_loss: 0.69501686\n",
      "Step: [12791] d_loss: 1.38787615, g_loss: 0.69153583\n",
      "Step: [12792] d_loss: 1.38717318, g_loss: 0.69766146\n",
      "Step: [12793] d_loss: 1.38659143, g_loss: 0.69435161\n",
      "Step: [12794] d_loss: 1.38626981, g_loss: 0.69328076\n",
      "Step: [12795] d_loss: 1.38636386, g_loss: 0.69219494\n",
      "Step: [12796] d_loss: 1.38615334, g_loss: 0.69295597\n",
      "Step: [12797] d_loss: 1.38606238, g_loss: 0.69329858\n",
      "Step: [12798] d_loss: 1.38632202, g_loss: 0.69336718\n",
      "Step: [12799] d_loss: 1.38631439, g_loss: 0.69356048\n",
      "Step: [12800] d_loss: 1.38714790, g_loss: 0.69250017\n",
      "Step: [12801] d_loss: 1.38609123, g_loss: 0.69393378\n",
      "Step: [12802] d_loss: 1.38608909, g_loss: 0.69284219\n",
      "Step: [12803] d_loss: 1.38613117, g_loss: 0.69291961\n",
      "Step: [12804] d_loss: 1.38630593, g_loss: 0.69262278\n",
      "Step: [12805] d_loss: 1.38606548, g_loss: 0.69432992\n",
      "Step: [12806] d_loss: 1.38603795, g_loss: 0.69451439\n",
      "Step: [12807] d_loss: 1.38613629, g_loss: 0.69296896\n",
      "Step: [12808] d_loss: 1.38619769, g_loss: 0.69367343\n",
      "Step: [12809] d_loss: 1.38632774, g_loss: 0.69269300\n",
      "Step: [12810] d_loss: 1.38579690, g_loss: 0.69385505\n",
      "Step: [12811] d_loss: 1.38608968, g_loss: 0.69286191\n",
      "Step: [12812] d_loss: 1.38603914, g_loss: 0.69382930\n",
      "Step: [12813] d_loss: 1.38599586, g_loss: 0.69348228\n",
      "Step: [12814] d_loss: 1.38614607, g_loss: 0.69276631\n",
      "Step: [12815] d_loss: 1.38661194, g_loss: 0.69308680\n",
      "Step: [12816] d_loss: 1.38625908, g_loss: 0.69277525\n",
      "Step: [12817] d_loss: 1.38630140, g_loss: 0.69427180\n",
      "Step: [12818] d_loss: 1.38617158, g_loss: 0.69419956\n",
      "Step: [12819] d_loss: 1.38611007, g_loss: 0.69309688\n",
      "Step: [12820] d_loss: 1.38768554, g_loss: 0.69196475\n",
      "Step: [12821] d_loss: 1.38615966, g_loss: 0.69357520\n",
      "Step: [12822] d_loss: 1.38650167, g_loss: 0.69225281\n",
      "Step: [12823] d_loss: 1.38588977, g_loss: 0.69379884\n",
      "Step: [12824] d_loss: 1.38605213, g_loss: 0.69487882\n",
      "Step: [12825] d_loss: 1.38623548, g_loss: 0.69221711\n",
      "Step: [12826] d_loss: 1.38654065, g_loss: 0.69339281\n",
      "Step: [12827] d_loss: 1.38755095, g_loss: 0.69273734\n",
      "Step: [12828] d_loss: 1.38863146, g_loss: 0.69700861\n",
      "Step: [12829] d_loss: 1.38776994, g_loss: 0.69214147\n",
      "Step: [12830] d_loss: 1.38703644, g_loss: 0.69527292\n",
      "Step: [12831] d_loss: 1.38655853, g_loss: 0.69260764\n",
      "Step: [12832] d_loss: 1.38624287, g_loss: 0.69250834\n",
      "Step: [12833] d_loss: 1.38615382, g_loss: 0.69229782\n",
      "Step: [12834] d_loss: 1.38619745, g_loss: 0.69236749\n",
      "Step: [12835] d_loss: 1.38618696, g_loss: 0.69334602\n",
      "Step: [12836] d_loss: 1.38651347, g_loss: 0.69175148\n",
      "Step: [12837] d_loss: 1.38689470, g_loss: 0.69626004\n",
      "Step: [12838] d_loss: 1.38770735, g_loss: 0.69358462\n",
      "Step: [12839] d_loss: 1.38733101, g_loss: 0.69676590\n",
      "Step: [12840] d_loss: 1.38689804, g_loss: 0.69234443\n",
      "Step: [12841] d_loss: 1.38634598, g_loss: 0.69158924\n",
      "Step: [12842] d_loss: 1.38622177, g_loss: 0.69238949\n",
      "Step: [12843] d_loss: 1.38626599, g_loss: 0.69288099\n",
      "Step: [12844] d_loss: 1.38639832, g_loss: 0.69356418\n",
      "Step: [12845] d_loss: 1.38634014, g_loss: 0.69366634\n",
      "Step: [12846] d_loss: 1.38648486, g_loss: 0.69448018\n",
      "Step: [12847] d_loss: 1.38617349, g_loss: 0.69349408\n",
      "Step: [12848] d_loss: 1.38620782, g_loss: 0.69278038\n",
      "Step: [12849] d_loss: 1.38662755, g_loss: 0.69327843\n",
      "Step: [12850] d_loss: 1.38605475, g_loss: 0.69323474\n",
      "Step: [12851] d_loss: 1.38625968, g_loss: 0.69307911\n",
      "Step: [12852] d_loss: 1.38613582, g_loss: 0.69304180\n",
      "Step: [12853] d_loss: 1.38618016, g_loss: 0.69312882\n",
      "Step: [12854] d_loss: 1.38630378, g_loss: 0.69307315\n",
      "Step: [12855] d_loss: 1.38617301, g_loss: 0.69372582\n",
      "Step: [12856] d_loss: 1.38611162, g_loss: 0.69295400\n",
      "Step: [12857] d_loss: 1.38606477, g_loss: 0.69318157\n",
      "Step: [12858] d_loss: 1.38633466, g_loss: 0.69291091\n",
      "Step: [12859] d_loss: 1.38634992, g_loss: 0.69323671\n",
      "Step: [12860] d_loss: 1.38613892, g_loss: 0.69306171\n",
      "Step: [12861] d_loss: 1.38616371, g_loss: 0.69476652\n",
      "Step: [12862] d_loss: 1.38615692, g_loss: 0.69353914\n",
      "Step: [12863] d_loss: 1.38645077, g_loss: 0.69342554\n",
      "Step: [12864] d_loss: 1.38619590, g_loss: 0.69199908\n",
      "Step: [12865] d_loss: 1.38639593, g_loss: 0.69251543\n",
      "Step: [12866] d_loss: 1.38629293, g_loss: 0.69363022\n",
      "Step: [12867] d_loss: 1.38625300, g_loss: 0.69372338\n",
      "Step: [12868] d_loss: 1.38630259, g_loss: 0.69330418\n",
      "Step: [12869] d_loss: 1.38631415, g_loss: 0.69301808\n",
      "Step: [12870] d_loss: 1.38626540, g_loss: 0.69287527\n",
      "Step: [12871] d_loss: 1.38620055, g_loss: 0.69296408\n",
      "Step: [12872] d_loss: 1.38628411, g_loss: 0.69357663\n",
      "Step: [12873] d_loss: 1.38618934, g_loss: 0.69335514\n",
      "Step: [12874] d_loss: 1.38629889, g_loss: 0.69387656\n",
      "Step: [12875] d_loss: 1.38646448, g_loss: 0.69114888\n",
      "Step: [12876] d_loss: 1.38647091, g_loss: 0.69367373\n",
      "Step: [12877] d_loss: 1.38641191, g_loss: 0.69272143\n",
      "Step: [12878] d_loss: 1.38633978, g_loss: 0.69366324\n",
      "Step: [12879] d_loss: 1.38654172, g_loss: 0.69256073\n",
      "Step: [12880] d_loss: 1.38605535, g_loss: 0.69332212\n",
      "Step: [12881] d_loss: 1.38625944, g_loss: 0.69290626\n",
      "Step: [12882] d_loss: 1.38619590, g_loss: 0.69332159\n",
      "Step: [12883] d_loss: 1.38629019, g_loss: 0.69373029\n",
      "Step: [12884] d_loss: 1.38632727, g_loss: 0.69355255\n",
      "Step: [12885] d_loss: 1.38627255, g_loss: 0.69329166\n",
      "Step: [12886] d_loss: 1.38630319, g_loss: 0.69432235\n",
      "Step: [12887] d_loss: 1.38646924, g_loss: 0.69317788\n",
      "Step: [12888] d_loss: 1.38639522, g_loss: 0.69339120\n",
      "Step: [12889] d_loss: 1.38653100, g_loss: 0.69269276\n",
      "Step: [12890] d_loss: 1.38639724, g_loss: 0.69275081\n",
      "Step: [12891] d_loss: 1.38630366, g_loss: 0.69293469\n",
      "Step: [12892] d_loss: 1.38626313, g_loss: 0.69320488\n",
      "Step: [12893] d_loss: 1.38624287, g_loss: 0.69344378\n",
      "Step: [12894] d_loss: 1.38626039, g_loss: 0.69322550\n",
      "Step: [12895] d_loss: 1.38618565, g_loss: 0.69334066\n",
      "Step: [12896] d_loss: 1.38624334, g_loss: 0.69330084\n",
      "Step: [12897] d_loss: 1.38626015, g_loss: 0.69338095\n",
      "Step: [12898] d_loss: 1.38627601, g_loss: 0.69300061\n",
      "Step: [12899] d_loss: 1.38627911, g_loss: 0.69314754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [12900] d_loss: 1.38619375, g_loss: 0.69382387\n",
      "Step: [12901] d_loss: 1.38632011, g_loss: 0.69384360\n",
      "Step: [12902] d_loss: 1.38649678, g_loss: 0.69311094\n",
      "Step: [12903] d_loss: 1.38634622, g_loss: 0.69257694\n",
      "Step: [12904] d_loss: 1.38643968, g_loss: 0.69391727\n",
      "Step: [12905] d_loss: 1.38655174, g_loss: 0.69336927\n",
      "Step: [12906] d_loss: 1.38629651, g_loss: 0.69323742\n",
      "Step: [12907] d_loss: 1.38618612, g_loss: 0.69322944\n",
      "Step: [12908] d_loss: 1.38626969, g_loss: 0.69327652\n",
      "Step: [12909] d_loss: 1.38634443, g_loss: 0.69359416\n",
      "Step: [12910] d_loss: 1.38644063, g_loss: 0.69427836\n",
      "Step: [12911] d_loss: 1.38624585, g_loss: 0.69253975\n",
      "Step: [12912] d_loss: 1.38628662, g_loss: 0.69326854\n",
      "Step: [12913] d_loss: 1.38627529, g_loss: 0.69311988\n",
      "Step: [12914] d_loss: 1.38627291, g_loss: 0.69299549\n",
      "Step: [12915] d_loss: 1.38622594, g_loss: 0.69368148\n",
      "Step: [12916] d_loss: 1.38647461, g_loss: 0.69357002\n",
      "Step: [12917] d_loss: 1.38631701, g_loss: 0.69333887\n",
      "Step: [12918] d_loss: 1.38612628, g_loss: 0.69326043\n",
      "Step: [12919] d_loss: 1.38635993, g_loss: 0.69286448\n",
      "Step: [12920] d_loss: 1.38636851, g_loss: 0.69309628\n",
      "Step: [12921] d_loss: 1.38631892, g_loss: 0.69324625\n",
      "Step: [12922] d_loss: 1.38633800, g_loss: 0.69369745\n",
      "Step: [12923] d_loss: 1.38630939, g_loss: 0.69352198\n",
      "Step: [12924] d_loss: 1.38621151, g_loss: 0.69238633\n",
      "Step: [12925] d_loss: 1.38619828, g_loss: 0.69264406\n",
      "Step: [12926] d_loss: 1.38629246, g_loss: 0.69294208\n",
      "Step: [12927] d_loss: 1.38649309, g_loss: 0.69358963\n",
      "Step: [12928] d_loss: 1.38621962, g_loss: 0.69305396\n",
      "Step: [12929] d_loss: 1.38639224, g_loss: 0.69334066\n",
      "Step: [12930] d_loss: 1.38661027, g_loss: 0.69356143\n",
      "Step: [12931] d_loss: 1.38622952, g_loss: 0.69286883\n",
      "Step: [12932] d_loss: 1.38628912, g_loss: 0.69319648\n",
      "Step: [12933] d_loss: 1.38651729, g_loss: 0.69373161\n",
      "Step: [12934] d_loss: 1.38617301, g_loss: 0.69334018\n",
      "Step: [12935] d_loss: 1.38635790, g_loss: 0.69465709\n",
      "Step: [12936] d_loss: 1.38615847, g_loss: 0.69274199\n",
      "Step: [12937] d_loss: 1.38636398, g_loss: 0.69312793\n",
      "Step: [12938] d_loss: 1.38630569, g_loss: 0.69300497\n",
      "Step: [12939] d_loss: 1.38628900, g_loss: 0.69559556\n",
      "Step: [12940] d_loss: 1.38656425, g_loss: 0.69215989\n",
      "Step: [12941] d_loss: 1.38658834, g_loss: 0.69472110\n",
      "Step: [12942] d_loss: 1.38702035, g_loss: 0.69175005\n",
      "Step: [12943] d_loss: 1.38640738, g_loss: 0.69080096\n",
      "Step: [12944] d_loss: 1.38644564, g_loss: 0.69355488\n",
      "Step: [12945] d_loss: 1.38713419, g_loss: 0.69606537\n",
      "Step: [12946] d_loss: 1.38757491, g_loss: 0.69076300\n",
      "Step: [12947] d_loss: 1.38778841, g_loss: 0.68935835\n",
      "Step: [12948] d_loss: 1.38691497, g_loss: 0.68912405\n",
      "Step: [12949] d_loss: 1.38638282, g_loss: 0.69196904\n",
      "Step: [12950] d_loss: 1.38652062, g_loss: 0.69589347\n",
      "Step: [12951] d_loss: 1.38631821, g_loss: 0.69336724\n",
      "Step: [12952] d_loss: 1.38624918, g_loss: 0.69639879\n",
      "Step: [12953] d_loss: 1.38626623, g_loss: 0.69351315\n",
      "Step: [12954] d_loss: 1.38638473, g_loss: 0.69479883\n",
      "Step: [12955] d_loss: 1.38624573, g_loss: 0.69279534\n",
      "Step: [12956] d_loss: 1.38628209, g_loss: 0.69315636\n",
      "Step: [12957] d_loss: 1.38600659, g_loss: 0.69373274\n",
      "Step: [12958] d_loss: 1.38618553, g_loss: 0.69246477\n",
      "Step: [12959] d_loss: 1.38603449, g_loss: 0.69318175\n",
      "Step: [12960] d_loss: 1.38618040, g_loss: 0.69324851\n",
      "Step: [12961] d_loss: 1.38626838, g_loss: 0.69380569\n",
      "Step: [12962] d_loss: 1.38615847, g_loss: 0.69326484\n",
      "Step: [12963] d_loss: 1.38627243, g_loss: 0.69296902\n",
      "Step: [12964] d_loss: 1.38631773, g_loss: 0.69305921\n",
      "Step: [12965] d_loss: 1.38615882, g_loss: 0.69331330\n",
      "Step: [12966] d_loss: 1.38635337, g_loss: 0.69323754\n",
      "Step: [12967] d_loss: 1.38618445, g_loss: 0.69379675\n",
      "Step: [12968] d_loss: 1.38683152, g_loss: 0.69280362\n",
      "Step: [12969] d_loss: 1.38808572, g_loss: 0.69489384\n",
      "Step: [12970] d_loss: 1.38755441, g_loss: 0.69752729\n",
      "Step: [12971] d_loss: 1.38824344, g_loss: 0.68939447\n",
      "Step: [12972] d_loss: 1.38701391, g_loss: 0.69190693\n",
      "Step: [12973] d_loss: 1.38653779, g_loss: 0.69860852\n",
      "Step: [12974] d_loss: 1.38616371, g_loss: 0.69741595\n",
      "Step: [12975] d_loss: 1.38659430, g_loss: 0.69353217\n",
      "Step: [12976] d_loss: 1.38671780, g_loss: 0.69483423\n",
      "Step: [12977] d_loss: 1.38669956, g_loss: 0.69189048\n",
      "Step: [12978] d_loss: 1.38637638, g_loss: 0.69267118\n",
      "Step: [12979] d_loss: 1.39043808, g_loss: 0.69379735\n",
      "Step: [12980] d_loss: 1.38665867, g_loss: 0.69149745\n",
      "Step: [12981] d_loss: 1.38648176, g_loss: 0.69274747\n",
      "Step: [12982] d_loss: 1.38647616, g_loss: 0.69301379\n",
      "Step: [12983] d_loss: 1.38643146, g_loss: 0.69439977\n",
      "Step: [12984] d_loss: 1.38535166, g_loss: 0.69562554\n",
      "Step: [12985] d_loss: 1.38634467, g_loss: 0.69298124\n",
      "Step: [12986] d_loss: 1.38626695, g_loss: 0.69243485\n",
      "Step: [12987] d_loss: 1.38637865, g_loss: 0.69335699\n",
      "Step: [12988] d_loss: 1.38626802, g_loss: 0.69338828\n",
      "Step: [12989] d_loss: 1.38627923, g_loss: 0.69376403\n",
      "Step: [12990] d_loss: 1.38625848, g_loss: 0.69400084\n",
      "Step: [12991] d_loss: 1.38627970, g_loss: 0.69448972\n",
      "Step: [12992] d_loss: 1.38625169, g_loss: 0.69272161\n",
      "Step: [12993] d_loss: 1.38619733, g_loss: 0.69338900\n",
      "Step: [12994] d_loss: 1.38624024, g_loss: 0.69411576\n",
      "Step: [12995] d_loss: 1.38629341, g_loss: 0.69439626\n",
      "Step: [12996] d_loss: 1.38635993, g_loss: 0.69204760\n",
      "Step: [12997] d_loss: 1.38636684, g_loss: 0.69186831\n",
      "Step: [12998] d_loss: 1.38623953, g_loss: 0.69255900\n",
      "Step: [12999] d_loss: 1.38609028, g_loss: 0.69277388\n",
      "Step: [13000] d_loss: 1.38626122, g_loss: 0.69480234\n",
      "Step: [13001] d_loss: 1.38620532, g_loss: 0.69295949\n",
      "Step: [13002] d_loss: 1.38643742, g_loss: 0.69539595\n",
      "Step: [13003] d_loss: 1.38639128, g_loss: 0.69359642\n",
      "Step: [13004] d_loss: 1.38606298, g_loss: 0.69299364\n",
      "Step: [13005] d_loss: 1.38599563, g_loss: 0.69348764\n",
      "Step: [13006] d_loss: 1.38648343, g_loss: 0.69735849\n",
      "Step: [13007] d_loss: 1.38629079, g_loss: 0.69481719\n",
      "Step: [13008] d_loss: 1.38635492, g_loss: 0.69337547\n",
      "Step: [13009] d_loss: 1.38613820, g_loss: 0.69120634\n",
      "Step: [13010] d_loss: 1.38622630, g_loss: 0.69351912\n",
      "Step: [13011] d_loss: 1.38630366, g_loss: 0.69431376\n",
      "Step: [13012] d_loss: 1.38641715, g_loss: 0.69411099\n",
      "Step: [13013] d_loss: 1.38618982, g_loss: 0.69264138\n",
      "Step: [13014] d_loss: 1.38791764, g_loss: 0.69633543\n",
      "Step: [13015] d_loss: 1.38594282, g_loss: 0.69233704\n",
      "Step: [13016] d_loss: 1.38595462, g_loss: 0.69252348\n",
      "Step: [13017] d_loss: 1.38648820, g_loss: 0.69290990\n",
      "Step: [13018] d_loss: 1.38599730, g_loss: 0.69449568\n",
      "Step: [13019] d_loss: 1.38611436, g_loss: 0.69512093\n",
      "Step: [13020] d_loss: 1.38615417, g_loss: 0.69379777\n",
      "Step: [13021] d_loss: 1.38615060, g_loss: 0.69269067\n",
      "Step: [13022] d_loss: 1.38623357, g_loss: 0.69263953\n",
      "Step: [13023] d_loss: 1.38631237, g_loss: 0.69257176\n",
      "Step: [13024] d_loss: 1.38627088, g_loss: 0.69493783\n",
      "Step: [13025] d_loss: 1.38596892, g_loss: 0.69453996\n",
      "Step: [13026] d_loss: 1.38592553, g_loss: 0.69452429\n",
      "Step: [13027] d_loss: 1.38632786, g_loss: 0.69346416\n",
      "Step: [13028] d_loss: 1.38624215, g_loss: 0.69245404\n",
      "Step: [13029] d_loss: 1.38623977, g_loss: 0.69253516\n",
      "Step: [13030] d_loss: 1.38658929, g_loss: 0.69540048\n",
      "Step: [13031] d_loss: 1.38641751, g_loss: 0.69367021\n",
      "Step: [13032] d_loss: 1.38649035, g_loss: 0.69544947\n",
      "Step: [13033] d_loss: 1.38627911, g_loss: 0.69629866\n",
      "Step: [13034] d_loss: 1.38674736, g_loss: 0.69348913\n",
      "Step: [13035] d_loss: 1.38677788, g_loss: 0.69309366\n",
      "Step: [13036] d_loss: 1.38676119, g_loss: 0.69141805\n",
      "Step: [13037] d_loss: 1.38625824, g_loss: 0.69538516\n",
      "Step: [13038] d_loss: 1.38659310, g_loss: 0.69380581\n",
      "Step: [13039] d_loss: 1.38619232, g_loss: 0.69513583\n",
      "Step: [13040] d_loss: 1.38623166, g_loss: 0.69451416\n",
      "Step: [13041] d_loss: 1.38632512, g_loss: 0.69330943\n",
      "Step: [13042] d_loss: 1.38656855, g_loss: 0.69195229\n",
      "Step: [13043] d_loss: 1.38608694, g_loss: 0.69226873\n",
      "Step: [13044] d_loss: 1.38668621, g_loss: 0.69338232\n",
      "Step: [13045] d_loss: 1.38664472, g_loss: 0.69377786\n",
      "Step: [13046] d_loss: 1.38657045, g_loss: 0.69590831\n",
      "Step: [13047] d_loss: 1.38657737, g_loss: 0.69305313\n",
      "Step: [13048] d_loss: 1.38653111, g_loss: 0.68891859\n",
      "Step: [13049] d_loss: 1.38645351, g_loss: 0.69129080\n",
      "Step: [13050] d_loss: 1.38643312, g_loss: 0.69466794\n",
      "Step: [13051] d_loss: 1.38614178, g_loss: 0.69474900\n",
      "Step: [13052] d_loss: 1.38628078, g_loss: 0.69365907\n",
      "Step: [13053] d_loss: 1.38609624, g_loss: 0.69273281\n",
      "Step: [13054] d_loss: 1.38597751, g_loss: 0.69350189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13055] d_loss: 1.38630569, g_loss: 0.69498616\n",
      "Step: [13056] d_loss: 1.38661742, g_loss: 0.69342864\n",
      "Step: [13057] d_loss: 1.38604462, g_loss: 0.69287652\n",
      "Step: [13058] d_loss: 1.38710260, g_loss: 0.69097066\n",
      "Step: [13059] d_loss: 1.38629961, g_loss: 0.69179821\n",
      "Step: [13060] d_loss: 1.38632905, g_loss: 0.69306487\n",
      "Step: [13061] d_loss: 1.38606560, g_loss: 0.69450867\n",
      "Step: [13062] d_loss: 1.38613260, g_loss: 0.69409430\n",
      "Step: [13063] d_loss: 1.38663220, g_loss: 0.69347644\n",
      "Step: [13064] d_loss: 1.38610113, g_loss: 0.69287503\n",
      "Step: [13065] d_loss: 1.38672328, g_loss: 0.69133270\n",
      "Step: [13066] d_loss: 1.38739252, g_loss: 0.69590557\n",
      "Step: [13067] d_loss: 1.38779354, g_loss: 0.69354123\n",
      "Step: [13068] d_loss: 1.38887668, g_loss: 0.69574529\n",
      "Step: [13069] d_loss: 1.38858962, g_loss: 0.68969697\n",
      "Step: [13070] d_loss: 1.38764882, g_loss: 0.69530028\n",
      "Step: [13071] d_loss: 1.38675952, g_loss: 0.69308901\n",
      "Step: [13072] d_loss: 1.38631439, g_loss: 0.69285631\n",
      "Step: [13073] d_loss: 1.38591588, g_loss: 0.69331956\n",
      "Step: [13074] d_loss: 1.38520718, g_loss: 0.69849348\n",
      "Step: [13075] d_loss: 1.38636780, g_loss: 0.69287473\n",
      "Step: [13076] d_loss: 1.38604176, g_loss: 0.69313473\n",
      "Step: [13077] d_loss: 1.38613462, g_loss: 0.69369888\n",
      "Step: [13078] d_loss: 1.38625216, g_loss: 0.69405490\n",
      "Step: [13079] d_loss: 1.39017558, g_loss: 0.69719356\n",
      "Step: [13080] d_loss: 1.38626552, g_loss: 0.69347167\n",
      "Step: [13081] d_loss: 1.38651896, g_loss: 0.69458210\n",
      "Step: [13082] d_loss: 1.38674128, g_loss: 0.69115669\n",
      "Step: [13083] d_loss: 1.38638234, g_loss: 0.69228327\n",
      "Step: [13084] d_loss: 1.38640618, g_loss: 0.69241202\n",
      "Step: [13085] d_loss: 1.38593614, g_loss: 0.69315362\n",
      "Step: [13086] d_loss: 1.38620877, g_loss: 0.69410515\n",
      "Step: [13087] d_loss: 1.38586116, g_loss: 0.69321406\n",
      "Step: [13088] d_loss: 1.38619232, g_loss: 0.69370151\n",
      "Step: [13089] d_loss: 1.38618827, g_loss: 0.69380659\n",
      "Step: [13090] d_loss: 1.38719106, g_loss: 0.69681710\n",
      "Step: [13091] d_loss: 1.38802481, g_loss: 0.69101250\n",
      "Step: [13092] d_loss: 1.38719320, g_loss: 0.69398367\n",
      "Step: [13093] d_loss: 1.38660669, g_loss: 0.69626933\n",
      "Step: [13094] d_loss: 1.38614821, g_loss: 0.69480491\n",
      "Step: [13095] d_loss: 1.38861108, g_loss: 0.69563830\n",
      "Step: [13096] d_loss: 1.38636053, g_loss: 0.69354570\n",
      "Step: [13097] d_loss: 1.38633823, g_loss: 0.69394946\n",
      "Step: [13098] d_loss: 1.38614941, g_loss: 0.69337356\n",
      "Step: [13099] d_loss: 1.38593268, g_loss: 0.69352704\n",
      "Step: [13100] d_loss: 1.38617861, g_loss: 0.69283044\n",
      "Step: [13101] d_loss: 1.38606954, g_loss: 0.69229925\n",
      "Step: [13102] d_loss: 1.38611162, g_loss: 0.69353312\n",
      "Step: [13103] d_loss: 1.38603067, g_loss: 0.69343781\n",
      "Step: [13104] d_loss: 1.38627815, g_loss: 0.69269854\n",
      "Step: [13105] d_loss: 1.38596153, g_loss: 0.69288170\n",
      "Step: [13106] d_loss: 1.38623810, g_loss: 0.69486809\n",
      "Step: [13107] d_loss: 1.38629222, g_loss: 0.69420588\n",
      "Step: [13108] d_loss: 1.38586092, g_loss: 0.69266915\n",
      "Step: [13109] d_loss: 1.38637710, g_loss: 0.69333720\n",
      "Step: [13110] d_loss: 1.38609910, g_loss: 0.69285691\n",
      "Step: [13111] d_loss: 1.38642240, g_loss: 0.69588470\n",
      "Step: [13112] d_loss: 1.38618040, g_loss: 0.69365633\n",
      "Step: [13113] d_loss: 1.38610935, g_loss: 0.69467038\n",
      "Step: [13114] d_loss: 1.38626468, g_loss: 0.69324970\n",
      "Step: [13115] d_loss: 1.38637352, g_loss: 0.69376361\n",
      "Step: [13116] d_loss: 1.38641953, g_loss: 0.69188714\n",
      "Step: [13117] d_loss: 1.38653672, g_loss: 0.69390345\n",
      "Step: [13118] d_loss: 1.38627052, g_loss: 0.69245994\n",
      "Step: [13119] d_loss: 1.38599372, g_loss: 0.69295037\n",
      "Step: [13120] d_loss: 1.38595951, g_loss: 0.69392633\n",
      "Step: [13121] d_loss: 1.38604212, g_loss: 0.69193757\n",
      "Step: [13122] d_loss: 1.38574922, g_loss: 0.69289035\n",
      "Step: [13123] d_loss: 1.38614607, g_loss: 0.69596523\n",
      "Step: [13124] d_loss: 1.38644493, g_loss: 0.69618440\n",
      "Step: [13125] d_loss: 1.38699460, g_loss: 0.69024575\n",
      "Step: [13126] d_loss: 1.38679194, g_loss: 0.69322067\n",
      "Step: [13127] d_loss: 1.38741469, g_loss: 0.69224465\n",
      "Step: [13128] d_loss: 1.38668823, g_loss: 0.69663614\n",
      "Step: [13129] d_loss: 1.38685870, g_loss: 0.69650149\n",
      "Step: [13130] d_loss: 1.38647246, g_loss: 0.69382524\n",
      "Step: [13131] d_loss: 1.38606787, g_loss: 0.69068408\n",
      "Step: [13132] d_loss: 1.38654399, g_loss: 0.69216502\n",
      "Step: [13133] d_loss: 1.38652444, g_loss: 0.69526660\n",
      "Step: [13134] d_loss: 1.38772964, g_loss: 0.69054925\n",
      "Step: [13135] d_loss: 1.38663483, g_loss: 0.69348347\n",
      "Step: [13136] d_loss: 1.38631082, g_loss: 0.69368100\n",
      "Step: [13137] d_loss: 1.38641453, g_loss: 0.69348544\n",
      "Step: [13138] d_loss: 1.38597870, g_loss: 0.69247299\n",
      "Step: [13139] d_loss: 1.38607681, g_loss: 0.69279909\n",
      "Step: [13140] d_loss: 1.38654506, g_loss: 0.69013679\n",
      "Step: [13141] d_loss: 1.38641620, g_loss: 0.69438255\n",
      "Step: [13142] d_loss: 1.38744295, g_loss: 0.69394290\n",
      "Step: [13143] d_loss: 1.38701987, g_loss: 0.69363964\n",
      "Step: [13144] d_loss: 1.38711977, g_loss: 0.68878341\n",
      "Step: [13145] d_loss: 1.38636768, g_loss: 0.69032145\n",
      "Step: [13146] d_loss: 1.38604975, g_loss: 0.69398069\n",
      "Step: [13147] d_loss: 1.38642025, g_loss: 0.69438601\n",
      "Step: [13148] d_loss: 1.38627768, g_loss: 0.69409275\n",
      "Step: [13149] d_loss: 1.38617897, g_loss: 0.69408751\n",
      "Step: [13150] d_loss: 1.38613725, g_loss: 0.69266558\n",
      "Step: [13151] d_loss: 1.38613367, g_loss: 0.69246882\n",
      "Step: [13152] d_loss: 1.38635516, g_loss: 0.69354117\n",
      "Step: [13153] d_loss: 1.38617659, g_loss: 0.69302952\n",
      "Step: [13154] d_loss: 1.38612223, g_loss: 0.69360709\n",
      "Step: [13155] d_loss: 1.38630462, g_loss: 0.69296187\n",
      "Step: [13156] d_loss: 1.38618386, g_loss: 0.69226444\n",
      "Step: [13157] d_loss: 1.38624918, g_loss: 0.69315565\n",
      "Step: [13158] d_loss: 1.38637376, g_loss: 0.69315863\n",
      "Step: [13159] d_loss: 1.38573480, g_loss: 0.69329482\n",
      "Step: [13160] d_loss: 1.38602352, g_loss: 0.69450760\n",
      "Step: [13161] d_loss: 1.38625860, g_loss: 0.69338018\n",
      "Step: [13162] d_loss: 1.38659620, g_loss: 0.69364738\n",
      "Step: [13163] d_loss: 1.38736892, g_loss: 0.69232142\n",
      "Step: [13164] d_loss: 1.38740706, g_loss: 0.68689311\n",
      "Step: [13165] d_loss: 1.38662314, g_loss: 0.68995136\n",
      "Step: [13166] d_loss: 1.38643432, g_loss: 0.69576865\n",
      "Step: [13167] d_loss: 1.38618267, g_loss: 0.69650453\n",
      "Step: [13168] d_loss: 1.38627172, g_loss: 0.69434106\n",
      "Step: [13169] d_loss: 1.38652718, g_loss: 0.69172370\n",
      "Step: [13170] d_loss: 1.38677144, g_loss: 0.69448191\n",
      "Step: [13171] d_loss: 1.38744473, g_loss: 0.69134104\n",
      "Step: [13172] d_loss: 1.38694227, g_loss: 0.69425756\n",
      "Step: [13173] d_loss: 1.38611078, g_loss: 0.69181585\n",
      "Step: [13174] d_loss: 1.38630998, g_loss: 0.69253862\n",
      "Step: [13175] d_loss: 1.38618100, g_loss: 0.69308865\n",
      "Step: [13176] d_loss: 1.38609076, g_loss: 0.69435602\n",
      "Step: [13177] d_loss: 1.38663173, g_loss: 0.69641018\n",
      "Step: [13178] d_loss: 1.38675022, g_loss: 0.69284475\n",
      "Step: [13179] d_loss: 1.38705611, g_loss: 0.69399238\n",
      "Step: [13180] d_loss: 1.38666880, g_loss: 0.69173789\n",
      "Step: [13181] d_loss: 1.38680792, g_loss: 0.69336265\n",
      "Step: [13182] d_loss: 1.38608110, g_loss: 0.69395494\n",
      "Step: [13183] d_loss: 1.38616037, g_loss: 0.69438833\n",
      "Step: [13184] d_loss: 1.38595927, g_loss: 0.69403374\n",
      "Step: [13185] d_loss: 1.38678622, g_loss: 0.69904351\n",
      "Step: [13186] d_loss: 1.38657451, g_loss: 0.69202322\n",
      "Step: [13187] d_loss: 1.38647985, g_loss: 0.69431782\n",
      "Step: [13188] d_loss: 1.38621068, g_loss: 0.69404519\n",
      "Step: [13189] d_loss: 1.38668418, g_loss: 0.69543529\n",
      "Step: [13190] d_loss: 1.38605905, g_loss: 0.69399762\n",
      "Step: [13191] d_loss: 1.38607621, g_loss: 0.69358432\n",
      "Step: [13192] d_loss: 1.38622451, g_loss: 0.69191265\n",
      "Step: [13193] d_loss: 1.38648522, g_loss: 0.69338977\n",
      "Step: [13194] d_loss: 1.38663983, g_loss: 0.69458330\n",
      "Step: [13195] d_loss: 1.38597369, g_loss: 0.69545794\n",
      "Step: [13196] d_loss: 1.38625693, g_loss: 0.69595528\n",
      "Step: [13197] d_loss: 1.38600492, g_loss: 0.69367880\n",
      "Step: [13198] d_loss: 1.38610327, g_loss: 0.69243473\n",
      "Step: [13199] d_loss: 1.38610351, g_loss: 0.69297934\n",
      "Step: [13200] d_loss: 1.38712358, g_loss: 0.69506639\n",
      "Step: [13201] d_loss: 1.38594389, g_loss: 0.69337487\n",
      "Step: [13202] d_loss: 1.38618731, g_loss: 0.69296640\n",
      "Step: [13203] d_loss: 1.38595915, g_loss: 0.69392461\n",
      "Step: [13204] d_loss: 1.38635659, g_loss: 0.69287217\n",
      "Step: [13205] d_loss: 1.38645220, g_loss: 0.69410276\n",
      "Step: [13206] d_loss: 1.38609219, g_loss: 0.69323313\n",
      "Step: [13207] d_loss: 1.38638163, g_loss: 0.69272768\n",
      "Step: [13208] d_loss: 1.38654649, g_loss: 0.69274688\n",
      "Step: [13209] d_loss: 1.38634729, g_loss: 0.69161314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13210] d_loss: 1.38623357, g_loss: 0.69280648\n",
      "Step: [13211] d_loss: 1.38616049, g_loss: 0.69369859\n",
      "Step: [13212] d_loss: 1.38639152, g_loss: 0.69268411\n",
      "Step: [13213] d_loss: 1.38601124, g_loss: 0.69353640\n",
      "Step: [13214] d_loss: 1.38643575, g_loss: 0.69454879\n",
      "Step: [13215] d_loss: 1.38650692, g_loss: 0.69441622\n",
      "Step: [13216] d_loss: 1.38596070, g_loss: 0.69287938\n",
      "Step: [13217] d_loss: 1.38628030, g_loss: 0.69239008\n",
      "Step: [13218] d_loss: 1.38695908, g_loss: 0.69088179\n",
      "Step: [13219] d_loss: 1.38789344, g_loss: 0.69568980\n",
      "Step: [13220] d_loss: 1.38731599, g_loss: 0.69174224\n",
      "Step: [13221] d_loss: 1.38667250, g_loss: 0.69220614\n",
      "Step: [13222] d_loss: 1.38655400, g_loss: 0.69350308\n",
      "Step: [13223] d_loss: 1.38643229, g_loss: 0.69152808\n",
      "Step: [13224] d_loss: 1.38631928, g_loss: 0.69297165\n",
      "Step: [13225] d_loss: 1.38629627, g_loss: 0.69255626\n",
      "Step: [13226] d_loss: 1.38628149, g_loss: 0.69282138\n",
      "Step: [13227] d_loss: 1.38594604, g_loss: 0.69423854\n",
      "Step: [13228] d_loss: 1.38644052, g_loss: 0.69341063\n",
      "Step: [13229] d_loss: 1.38633490, g_loss: 0.69387829\n",
      "Step: [13230] d_loss: 1.38610148, g_loss: 0.69349545\n",
      "Step: [13231] d_loss: 1.38615680, g_loss: 0.69315070\n",
      "Step: [13232] d_loss: 1.38633776, g_loss: 0.69351763\n",
      "Step: [13233] d_loss: 1.38666058, g_loss: 0.69279140\n",
      "Step: [13234] d_loss: 1.38626277, g_loss: 0.69347286\n",
      "Step: [13235] d_loss: 1.38618731, g_loss: 0.69298875\n",
      "Step: [13236] d_loss: 1.38633204, g_loss: 0.69343615\n",
      "Step: [13237] d_loss: 1.38636279, g_loss: 0.69312370\n",
      "Step: [13238] d_loss: 1.38634920, g_loss: 0.69266343\n",
      "Step: [13239] d_loss: 1.38631833, g_loss: 0.69283128\n",
      "Step: [13240] d_loss: 1.38661110, g_loss: 0.69337428\n",
      "Step: [13241] d_loss: 1.38638353, g_loss: 0.69513035\n",
      "Step: [13242] d_loss: 1.38690901, g_loss: 0.69278502\n",
      "Step: [13243] d_loss: 1.38688052, g_loss: 0.69554913\n",
      "Step: [13244] d_loss: 1.38641882, g_loss: 0.69397080\n",
      "Step: [13245] d_loss: 1.38643467, g_loss: 0.69351780\n",
      "Step: [13246] d_loss: 1.38629627, g_loss: 0.69301701\n",
      "Step: [13247] d_loss: 1.38621521, g_loss: 0.69202524\n",
      "Step: [13248] d_loss: 1.38608503, g_loss: 0.69305503\n",
      "Step: [13249] d_loss: 1.38640928, g_loss: 0.69354504\n",
      "Step: [13250] d_loss: 1.38615727, g_loss: 0.69337201\n",
      "Step: [13251] d_loss: 1.38626790, g_loss: 0.69371819\n",
      "Step: [13252] d_loss: 1.38621998, g_loss: 0.69312572\n",
      "Step: [13253] d_loss: 1.38626194, g_loss: 0.69264627\n",
      "Step: [13254] d_loss: 1.38600516, g_loss: 0.69319713\n",
      "Step: [13255] d_loss: 1.38644433, g_loss: 0.69268328\n",
      "Step: [13256] d_loss: 1.38651729, g_loss: 0.69329864\n",
      "Step: [13257] d_loss: 1.38632560, g_loss: 0.69306862\n",
      "Step: [13258] d_loss: 1.38666320, g_loss: 0.69291806\n",
      "Step: [13259] d_loss: 1.38639235, g_loss: 0.69435668\n",
      "Step: [13260] d_loss: 1.38611341, g_loss: 0.69382918\n",
      "Step: [13261] d_loss: 1.38623643, g_loss: 0.69132346\n",
      "Step: [13262] d_loss: 1.38629770, g_loss: 0.69245595\n",
      "Step: [13263] d_loss: 1.38626623, g_loss: 0.69364202\n",
      "Step: [13264] d_loss: 1.38624668, g_loss: 0.69345331\n",
      "Step: [13265] d_loss: 1.38635230, g_loss: 0.69397086\n",
      "Step: [13266] d_loss: 1.38633752, g_loss: 0.69289756\n",
      "Step: [13267] d_loss: 1.38630807, g_loss: 0.69261050\n",
      "Step: [13268] d_loss: 1.38623178, g_loss: 0.69376957\n",
      "Step: [13269] d_loss: 1.38641000, g_loss: 0.69344318\n",
      "Step: [13270] d_loss: 1.38643241, g_loss: 0.69345140\n",
      "Step: [13271] d_loss: 1.38660741, g_loss: 0.69187152\n",
      "Step: [13272] d_loss: 1.38706505, g_loss: 0.69324285\n",
      "Step: [13273] d_loss: 1.38699770, g_loss: 0.69107193\n",
      "Step: [13274] d_loss: 1.38633871, g_loss: 0.69240946\n",
      "Step: [13275] d_loss: 1.38639140, g_loss: 0.69345593\n",
      "Step: [13276] d_loss: 1.38635182, g_loss: 0.69286752\n",
      "Step: [13277] d_loss: 1.38620281, g_loss: 0.69432175\n",
      "Step: [13278] d_loss: 1.38603497, g_loss: 0.69508719\n",
      "Step: [13279] d_loss: 1.38622093, g_loss: 0.69449556\n",
      "Step: [13280] d_loss: 1.38628531, g_loss: 0.69255996\n",
      "Step: [13281] d_loss: 1.38700533, g_loss: 0.69157302\n",
      "Step: [13282] d_loss: 1.38684320, g_loss: 0.69220531\n",
      "Step: [13283] d_loss: 1.38752365, g_loss: 0.69028896\n",
      "Step: [13284] d_loss: 1.38726890, g_loss: 0.69262147\n",
      "Step: [13285] d_loss: 1.38660312, g_loss: 0.69303215\n",
      "Step: [13286] d_loss: 1.38631356, g_loss: 0.69248724\n",
      "Step: [13287] d_loss: 1.38624525, g_loss: 0.69440019\n",
      "Step: [13288] d_loss: 1.38650310, g_loss: 0.69333690\n",
      "Step: [13289] d_loss: 1.38615298, g_loss: 0.69283181\n",
      "Step: [13290] d_loss: 1.38610220, g_loss: 0.69294071\n",
      "Step: [13291] d_loss: 1.38627481, g_loss: 0.69337869\n",
      "Step: [13292] d_loss: 1.38598299, g_loss: 0.69434118\n",
      "Step: [13293] d_loss: 1.38636279, g_loss: 0.69441462\n",
      "Step: [13294] d_loss: 1.38671100, g_loss: 0.69745028\n",
      "Step: [13295] d_loss: 1.38659370, g_loss: 0.69256973\n",
      "Step: [13296] d_loss: 1.38697076, g_loss: 0.69331890\n",
      "Step: [13297] d_loss: 1.38663006, g_loss: 0.69137883\n",
      "Step: [13298] d_loss: 1.38640308, g_loss: 0.69192278\n",
      "Step: [13299] d_loss: 1.38615346, g_loss: 0.69418597\n",
      "Step: [13300] d_loss: 1.38653672, g_loss: 0.69397992\n",
      "Step: [13301] d_loss: 1.38614058, g_loss: 0.69213474\n",
      "Step: [13302] d_loss: 1.38617682, g_loss: 0.69594371\n",
      "Step: [13303] d_loss: 1.38647199, g_loss: 0.69352430\n",
      "Step: [13304] d_loss: 1.38558698, g_loss: 0.70223451\n",
      "Step: [13305] d_loss: 1.38660192, g_loss: 0.69332576\n",
      "Step: [13306] d_loss: 1.38637936, g_loss: 0.69491762\n",
      "Step: [13307] d_loss: 1.38635576, g_loss: 0.69289982\n",
      "Step: [13308] d_loss: 1.38647711, g_loss: 0.69195235\n",
      "Step: [13309] d_loss: 1.38638985, g_loss: 0.69263482\n",
      "Step: [13310] d_loss: 1.38616753, g_loss: 0.69417310\n",
      "Step: [13311] d_loss: 1.38652956, g_loss: 0.69381273\n",
      "Step: [13312] d_loss: 1.38638830, g_loss: 0.69273686\n",
      "Step: [13313] d_loss: 1.38619006, g_loss: 0.69365793\n",
      "Step: [13314] d_loss: 1.38604045, g_loss: 0.69288743\n",
      "Step: [13315] d_loss: 1.38614416, g_loss: 0.69351292\n",
      "Step: [13316] d_loss: 1.38613784, g_loss: 0.69336307\n",
      "Step: [13317] d_loss: 1.38608408, g_loss: 0.69355237\n",
      "Step: [13318] d_loss: 1.38610411, g_loss: 0.69319236\n",
      "Step: [13319] d_loss: 1.38638282, g_loss: 0.69378221\n",
      "Step: [13320] d_loss: 1.38609242, g_loss: 0.69328099\n",
      "Step: [13321] d_loss: 1.38605261, g_loss: 0.69302058\n",
      "Step: [13322] d_loss: 1.38612032, g_loss: 0.69307351\n",
      "Step: [13323] d_loss: 1.38608885, g_loss: 0.69348317\n",
      "Step: [13324] d_loss: 1.38611674, g_loss: 0.69374549\n",
      "Step: [13325] d_loss: 1.38636398, g_loss: 0.69354653\n",
      "Step: [13326] d_loss: 1.38604903, g_loss: 0.69286287\n",
      "Step: [13327] d_loss: 1.38622379, g_loss: 0.69280261\n",
      "Step: [13328] d_loss: 1.38647914, g_loss: 0.69319940\n",
      "Step: [13329] d_loss: 1.38668692, g_loss: 0.69466805\n",
      "Step: [13330] d_loss: 1.38671982, g_loss: 0.69200861\n",
      "Step: [13331] d_loss: 1.38636184, g_loss: 0.69252026\n",
      "Step: [13332] d_loss: 1.38633406, g_loss: 0.69244492\n",
      "Step: [13333] d_loss: 1.38636041, g_loss: 0.69326830\n",
      "Step: [13334] d_loss: 1.38633525, g_loss: 0.69426680\n",
      "Step: [13335] d_loss: 1.38645637, g_loss: 0.69349134\n",
      "Step: [13336] d_loss: 1.38635492, g_loss: 0.69336027\n",
      "Step: [13337] d_loss: 1.38634515, g_loss: 0.69319797\n",
      "Step: [13338] d_loss: 1.38632905, g_loss: 0.69153774\n",
      "Step: [13339] d_loss: 1.38626218, g_loss: 0.69208062\n",
      "Step: [13340] d_loss: 1.38634539, g_loss: 0.69298166\n",
      "Step: [13341] d_loss: 1.38642538, g_loss: 0.69557738\n",
      "Step: [13342] d_loss: 1.38629317, g_loss: 0.69440764\n",
      "Step: [13343] d_loss: 1.38633239, g_loss: 0.69397593\n",
      "Step: [13344] d_loss: 1.38645756, g_loss: 0.69315720\n",
      "Step: [13345] d_loss: 1.38622999, g_loss: 0.69293785\n",
      "Step: [13346] d_loss: 1.38625216, g_loss: 0.69328207\n",
      "Step: [13347] d_loss: 1.38620591, g_loss: 0.69275534\n",
      "Step: [13348] d_loss: 1.38616753, g_loss: 0.69377577\n",
      "Step: [13349] d_loss: 1.38615882, g_loss: 0.69331288\n",
      "Step: [13350] d_loss: 1.38632846, g_loss: 0.69348317\n",
      "Step: [13351] d_loss: 1.38620484, g_loss: 0.69319445\n",
      "Step: [13352] d_loss: 1.38620782, g_loss: 0.69327486\n",
      "Step: [13353] d_loss: 1.38607907, g_loss: 0.69289047\n",
      "Step: [13354] d_loss: 1.38618708, g_loss: 0.69318885\n",
      "Step: [13355] d_loss: 1.38615859, g_loss: 0.69326192\n",
      "Step: [13356] d_loss: 1.38605762, g_loss: 0.69298083\n",
      "Step: [13357] d_loss: 1.38615394, g_loss: 0.69217861\n",
      "Step: [13358] d_loss: 1.38608646, g_loss: 0.69472599\n",
      "Step: [13359] d_loss: 1.38629866, g_loss: 0.69334561\n",
      "Step: [13360] d_loss: 1.38643050, g_loss: 0.69385517\n",
      "Step: [13361] d_loss: 1.38648260, g_loss: 0.69227016\n",
      "Step: [13362] d_loss: 1.38633275, g_loss: 0.69252121\n",
      "Step: [13363] d_loss: 1.38648200, g_loss: 0.69425941\n",
      "Step: [13364] d_loss: 1.38733304, g_loss: 0.69713134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13365] d_loss: 1.38813376, g_loss: 0.69548565\n",
      "Step: [13366] d_loss: 1.38766479, g_loss: 0.69054115\n",
      "Step: [13367] d_loss: 1.38678479, g_loss: 0.69325286\n",
      "Step: [13368] d_loss: 1.38639247, g_loss: 0.69616175\n",
      "Step: [13369] d_loss: 1.38622034, g_loss: 0.69407237\n",
      "Step: [13370] d_loss: 1.38632989, g_loss: 0.69338536\n",
      "Step: [13371] d_loss: 1.38631475, g_loss: 0.69277692\n",
      "Step: [13372] d_loss: 1.38626969, g_loss: 0.69298184\n",
      "Step: [13373] d_loss: 1.38612199, g_loss: 0.69371718\n",
      "Step: [13374] d_loss: 1.38634348, g_loss: 0.69298369\n",
      "Step: [13375] d_loss: 1.38637781, g_loss: 0.69295335\n",
      "Step: [13376] d_loss: 1.38636291, g_loss: 0.69250739\n",
      "Step: [13377] d_loss: 1.38626194, g_loss: 0.69406706\n",
      "Step: [13378] d_loss: 1.38637114, g_loss: 0.69457698\n",
      "Step: [13379] d_loss: 1.38639545, g_loss: 0.69412875\n",
      "Step: [13380] d_loss: 1.38628280, g_loss: 0.69234741\n",
      "Step: [13381] d_loss: 1.38628685, g_loss: 0.69206506\n",
      "Step: [13382] d_loss: 1.38620472, g_loss: 0.69271082\n",
      "Step: [13383] d_loss: 1.38615072, g_loss: 0.69319612\n",
      "Step: [13384] d_loss: 1.38607514, g_loss: 0.69354379\n",
      "Step: [13385] d_loss: 1.38610220, g_loss: 0.69381773\n",
      "Step: [13386] d_loss: 1.38587296, g_loss: 0.69358075\n",
      "Step: [13387] d_loss: 1.38604236, g_loss: 0.69336796\n",
      "Step: [13388] d_loss: 1.38656139, g_loss: 0.69097650\n",
      "Step: [13389] d_loss: 1.38648248, g_loss: 0.69419074\n",
      "Step: [13390] d_loss: 1.38650298, g_loss: 0.69348949\n",
      "Step: [13391] d_loss: 1.38637960, g_loss: 0.69415963\n",
      "Step: [13392] d_loss: 1.38618493, g_loss: 0.69416130\n",
      "Step: [13393] d_loss: 1.38629949, g_loss: 0.69345927\n",
      "Step: [13394] d_loss: 1.38632119, g_loss: 0.69361341\n",
      "Step: [13395] d_loss: 1.38618851, g_loss: 0.69310606\n",
      "Step: [13396] d_loss: 1.38623834, g_loss: 0.69425476\n",
      "Step: [13397] d_loss: 1.38664651, g_loss: 0.69326770\n",
      "Step: [13398] d_loss: 1.38636720, g_loss: 0.69285834\n",
      "Step: [13399] d_loss: 1.38626528, g_loss: 0.69294971\n",
      "Step: [13400] d_loss: 1.38620877, g_loss: 0.69309574\n",
      "Step: [13401] d_loss: 1.38632834, g_loss: 0.69317234\n",
      "Step: [13402] d_loss: 1.38636339, g_loss: 0.69312006\n",
      "Step: [13403] d_loss: 1.38608098, g_loss: 0.69335109\n",
      "Step: [13404] d_loss: 1.38618445, g_loss: 0.69335389\n",
      "Step: [13405] d_loss: 1.38632250, g_loss: 0.69303030\n",
      "Step: [13406] d_loss: 1.38643384, g_loss: 0.69306362\n",
      "Step: [13407] d_loss: 1.38648963, g_loss: 0.69327235\n",
      "Step: [13408] d_loss: 1.38629055, g_loss: 0.69371009\n",
      "Step: [13409] d_loss: 1.38625336, g_loss: 0.69330120\n",
      "Step: [13410] d_loss: 1.38620853, g_loss: 0.69263357\n",
      "Step: [13411] d_loss: 1.38652706, g_loss: 0.69328564\n",
      "Step: [13412] d_loss: 1.38612676, g_loss: 0.69335711\n",
      "Step: [13413] d_loss: 1.38619423, g_loss: 0.69307768\n",
      "Step: [13414] d_loss: 1.38644743, g_loss: 0.69372118\n",
      "Step: [13415] d_loss: 1.38605738, g_loss: 0.69397354\n",
      "Step: [13416] d_loss: 1.38604760, g_loss: 0.69204617\n",
      "Step: [13417] d_loss: 1.38662601, g_loss: 0.69473577\n",
      "Step: [13418] d_loss: 1.38684380, g_loss: 0.69136834\n",
      "Step: [13419] d_loss: 1.38701606, g_loss: 0.69224888\n",
      "Step: [13420] d_loss: 1.38653016, g_loss: 0.69265831\n",
      "Step: [13421] d_loss: 1.38633204, g_loss: 0.69401318\n",
      "Step: [13422] d_loss: 1.38617325, g_loss: 0.69391489\n",
      "Step: [13423] d_loss: 1.38639879, g_loss: 0.69247079\n",
      "Step: [13424] d_loss: 1.38639593, g_loss: 0.69247234\n",
      "Step: [13425] d_loss: 1.38645279, g_loss: 0.69319248\n",
      "Step: [13426] d_loss: 1.38639283, g_loss: 0.69316244\n",
      "Step: [13427] d_loss: 1.38659298, g_loss: 0.69364405\n",
      "Step: [13428] d_loss: 1.38632822, g_loss: 0.69345683\n",
      "Step: [13429] d_loss: 1.38639224, g_loss: 0.69321167\n",
      "Step: [13430] d_loss: 1.38634133, g_loss: 0.69325519\n",
      "Step: [13431] d_loss: 1.38629472, g_loss: 0.69336736\n",
      "Step: [13432] d_loss: 1.38627923, g_loss: 0.69324583\n",
      "Step: [13433] d_loss: 1.38634169, g_loss: 0.69364697\n",
      "Step: [13434] d_loss: 1.38603997, g_loss: 0.69339043\n",
      "Step: [13435] d_loss: 1.38623130, g_loss: 0.69298995\n",
      "Step: [13436] d_loss: 1.38613057, g_loss: 0.69398701\n",
      "Step: [13437] d_loss: 1.38613772, g_loss: 0.69342256\n",
      "Step: [13438] d_loss: 1.38638973, g_loss: 0.69305164\n",
      "Step: [13439] d_loss: 1.38654912, g_loss: 0.69347954\n",
      "Step: [13440] d_loss: 1.38654470, g_loss: 0.69211167\n",
      "Step: [13441] d_loss: 1.38682961, g_loss: 0.69515812\n",
      "Step: [13442] d_loss: 1.38695621, g_loss: 0.69322765\n",
      "Step: [13443] d_loss: 1.38659120, g_loss: 0.69574648\n",
      "Step: [13444] d_loss: 1.38645816, g_loss: 0.69358981\n",
      "Step: [13445] d_loss: 1.38619900, g_loss: 0.69290221\n",
      "Step: [13446] d_loss: 1.38637817, g_loss: 0.69224024\n",
      "Step: [13447] d_loss: 1.38631701, g_loss: 0.69458544\n",
      "Step: [13448] d_loss: 1.38612509, g_loss: 0.69433504\n",
      "Step: [13449] d_loss: 1.38625622, g_loss: 0.69290686\n",
      "Step: [13450] d_loss: 1.38663554, g_loss: 0.69361860\n",
      "Step: [13451] d_loss: 1.38662004, g_loss: 0.69232517\n",
      "Step: [13452] d_loss: 1.38650382, g_loss: 0.69487548\n",
      "Step: [13453] d_loss: 1.38670564, g_loss: 0.69420427\n",
      "Step: [13454] d_loss: 1.38641059, g_loss: 0.69469368\n",
      "Step: [13455] d_loss: 1.38622046, g_loss: 0.69290590\n",
      "Step: [13456] d_loss: 1.38637424, g_loss: 0.69256330\n",
      "Step: [13457] d_loss: 1.38610864, g_loss: 0.69195342\n",
      "Step: [13458] d_loss: 1.38627362, g_loss: 0.69217360\n",
      "Step: [13459] d_loss: 1.38630414, g_loss: 0.69291705\n",
      "Step: [13460] d_loss: 1.38613272, g_loss: 0.69485569\n",
      "Step: [13461] d_loss: 1.38649428, g_loss: 0.69365692\n",
      "Step: [13462] d_loss: 1.38639808, g_loss: 0.69418877\n",
      "Step: [13463] d_loss: 1.38638902, g_loss: 0.69347453\n",
      "Step: [13464] d_loss: 1.38606799, g_loss: 0.69284070\n",
      "Step: [13465] d_loss: 1.38616657, g_loss: 0.69355720\n",
      "Step: [13466] d_loss: 1.38620222, g_loss: 0.69265407\n",
      "Step: [13467] d_loss: 1.38625002, g_loss: 0.69337934\n",
      "Step: [13468] d_loss: 1.38644934, g_loss: 0.69329381\n",
      "Step: [13469] d_loss: 1.38628876, g_loss: 0.69389200\n",
      "Step: [13470] d_loss: 1.38633394, g_loss: 0.69365925\n",
      "Step: [13471] d_loss: 1.38621414, g_loss: 0.69381744\n",
      "Step: [13472] d_loss: 1.38616121, g_loss: 0.69295108\n",
      "Step: [13473] d_loss: 1.38634074, g_loss: 0.69401336\n",
      "Step: [13474] d_loss: 1.38645077, g_loss: 0.69369870\n",
      "Step: [13475] d_loss: 1.38623142, g_loss: 0.69463760\n",
      "Step: [13476] d_loss: 1.38621378, g_loss: 0.69332290\n",
      "Step: [13477] d_loss: 1.38645792, g_loss: 0.69170654\n",
      "Step: [13478] d_loss: 1.38620305, g_loss: 0.69241595\n",
      "Step: [13479] d_loss: 1.38616598, g_loss: 0.69258571\n",
      "Step: [13480] d_loss: 1.38618183, g_loss: 0.69469428\n",
      "Step: [13481] d_loss: 1.38611507, g_loss: 0.69520986\n",
      "Step: [13482] d_loss: 1.38568652, g_loss: 0.69444513\n",
      "Step: [13483] d_loss: 1.38628638, g_loss: 0.69356000\n",
      "Step: [13484] d_loss: 1.38605487, g_loss: 0.69348228\n",
      "Step: [13485] d_loss: 1.38618767, g_loss: 0.69072568\n",
      "Step: [13486] d_loss: 1.38699532, g_loss: 0.69315040\n",
      "Step: [13487] d_loss: 1.38694000, g_loss: 0.69179165\n",
      "Step: [13488] d_loss: 1.38655186, g_loss: 0.69456720\n",
      "Step: [13489] d_loss: 1.38642192, g_loss: 0.69220984\n",
      "Step: [13490] d_loss: 1.38630927, g_loss: 0.69256181\n",
      "Step: [13491] d_loss: 1.38675547, g_loss: 0.69064307\n",
      "Step: [13492] d_loss: 1.38652825, g_loss: 0.69392121\n",
      "Step: [13493] d_loss: 1.38623762, g_loss: 0.69314623\n",
      "Step: [13494] d_loss: 1.38639760, g_loss: 0.69391447\n",
      "Step: [13495] d_loss: 1.38640511, g_loss: 0.69251359\n",
      "Step: [13496] d_loss: 1.38641000, g_loss: 0.69350207\n",
      "Step: [13497] d_loss: 1.38627887, g_loss: 0.69255799\n",
      "Step: [13498] d_loss: 1.38606560, g_loss: 0.69392359\n",
      "Step: [13499] d_loss: 1.38640141, g_loss: 0.69463789\n",
      "Step: [13500] d_loss: 1.38608718, g_loss: 0.69552523\n",
      "Step: [13501] d_loss: 1.38605809, g_loss: 0.69286788\n",
      "Step: [13502] d_loss: 1.38630843, g_loss: 0.69256186\n",
      "Step: [13503] d_loss: 1.38660443, g_loss: 0.69198072\n",
      "Step: [13504] d_loss: 1.38633966, g_loss: 0.69560838\n",
      "Step: [13505] d_loss: 1.38700223, g_loss: 0.69309938\n",
      "Step: [13506] d_loss: 1.38683593, g_loss: 0.69502556\n",
      "Step: [13507] d_loss: 1.38641000, g_loss: 0.69231832\n",
      "Step: [13508] d_loss: 1.38658285, g_loss: 0.69238448\n",
      "Step: [13509] d_loss: 1.38615370, g_loss: 0.69479954\n",
      "Step: [13510] d_loss: 1.38735294, g_loss: 0.69397670\n",
      "Step: [13511] d_loss: 1.38758993, g_loss: 0.69807899\n",
      "Step: [13512] d_loss: 1.38750684, g_loss: 0.69551420\n",
      "Step: [13513] d_loss: 1.38776231, g_loss: 0.69564968\n",
      "Step: [13514] d_loss: 1.38818240, g_loss: 0.68756843\n",
      "Step: [13515] d_loss: 1.38783050, g_loss: 0.69022626\n",
      "Step: [13516] d_loss: 1.38658893, g_loss: 0.68964505\n",
      "Step: [13517] d_loss: 1.38642526, g_loss: 0.69270086\n",
      "Step: [13518] d_loss: 1.38589180, g_loss: 0.69488364\n",
      "Step: [13519] d_loss: 1.38664877, g_loss: 0.69506532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13520] d_loss: 1.38671315, g_loss: 0.69369608\n",
      "Step: [13521] d_loss: 1.38655543, g_loss: 0.69313747\n",
      "Step: [13522] d_loss: 1.38623571, g_loss: 0.69229752\n",
      "Step: [13523] d_loss: 1.38652050, g_loss: 0.69125891\n",
      "Step: [13524] d_loss: 1.38636637, g_loss: 0.69416958\n",
      "Step: [13525] d_loss: 1.38642538, g_loss: 0.69418997\n",
      "Step: [13526] d_loss: 1.38635993, g_loss: 0.69264418\n",
      "Step: [13527] d_loss: 1.38620305, g_loss: 0.69394970\n",
      "Step: [13528] d_loss: 1.38645601, g_loss: 0.69401288\n",
      "Step: [13529] d_loss: 1.38618410, g_loss: 0.69352245\n",
      "Step: [13530] d_loss: 1.38582671, g_loss: 0.69270349\n",
      "Step: [13531] d_loss: 1.38612103, g_loss: 0.69213337\n",
      "Step: [13532] d_loss: 1.38703001, g_loss: 0.69354379\n",
      "Step: [13533] d_loss: 1.38626409, g_loss: 0.69446528\n",
      "Step: [13534] d_loss: 1.38620639, g_loss: 0.69516289\n",
      "Step: [13535] d_loss: 1.38696766, g_loss: 0.69172138\n",
      "Step: [13536] d_loss: 1.38675511, g_loss: 0.69360805\n",
      "Step: [13537] d_loss: 1.38645482, g_loss: 0.69343150\n",
      "Step: [13538] d_loss: 1.38641429, g_loss: 0.69492626\n",
      "Step: [13539] d_loss: 1.38650572, g_loss: 0.69061148\n",
      "Step: [13540] d_loss: 1.38648224, g_loss: 0.69365525\n",
      "Step: [13541] d_loss: 1.38676047, g_loss: 0.69571984\n",
      "Step: [13542] d_loss: 1.38595819, g_loss: 0.69499111\n",
      "Step: [13543] d_loss: 1.38608611, g_loss: 0.69573271\n",
      "Step: [13544] d_loss: 1.38640618, g_loss: 0.69274318\n",
      "Step: [13545] d_loss: 1.38637400, g_loss: 0.69285309\n",
      "Step: [13546] d_loss: 1.38598919, g_loss: 0.69273269\n",
      "Step: [13547] d_loss: 1.38592339, g_loss: 0.69351232\n",
      "Step: [13548] d_loss: 1.38671350, g_loss: 0.69362444\n",
      "Step: [13549] d_loss: 1.38634968, g_loss: 0.69332165\n",
      "Step: [13550] d_loss: 1.38666892, g_loss: 0.69205320\n",
      "Step: [13551] d_loss: 1.38604999, g_loss: 0.69308484\n",
      "Step: [13552] d_loss: 1.38596129, g_loss: 0.69307280\n",
      "Step: [13553] d_loss: 1.38634837, g_loss: 0.69356573\n",
      "Step: [13554] d_loss: 1.38632131, g_loss: 0.69453335\n",
      "Step: [13555] d_loss: 1.38602996, g_loss: 0.69390035\n",
      "Step: [13556] d_loss: 1.38630342, g_loss: 0.69309121\n",
      "Step: [13557] d_loss: 1.38627315, g_loss: 0.69100535\n",
      "Step: [13558] d_loss: 1.38653791, g_loss: 0.69378150\n",
      "Step: [13559] d_loss: 1.38659048, g_loss: 0.69245696\n",
      "Step: [13560] d_loss: 1.38637304, g_loss: 0.69401646\n",
      "Step: [13561] d_loss: 1.38645053, g_loss: 0.69238603\n",
      "Step: [13562] d_loss: 1.38621402, g_loss: 0.69311076\n",
      "Step: [13563] d_loss: 1.38617373, g_loss: 0.69365180\n",
      "Step: [13564] d_loss: 1.38668418, g_loss: 0.69607210\n",
      "Step: [13565] d_loss: 1.38690877, g_loss: 0.69313025\n",
      "Step: [13566] d_loss: 1.38682747, g_loss: 0.69268370\n",
      "Step: [13567] d_loss: 1.38648355, g_loss: 0.69093305\n",
      "Step: [13568] d_loss: 1.38629401, g_loss: 0.69172227\n",
      "Step: [13569] d_loss: 1.38620234, g_loss: 0.69358712\n",
      "Step: [13570] d_loss: 1.38629091, g_loss: 0.69350994\n",
      "Step: [13571] d_loss: 1.38626754, g_loss: 0.69358081\n",
      "Step: [13572] d_loss: 1.38647974, g_loss: 0.69280821\n",
      "Step: [13573] d_loss: 1.38610137, g_loss: 0.69312954\n",
      "Step: [13574] d_loss: 1.38619256, g_loss: 0.69337845\n",
      "Step: [13575] d_loss: 1.38610029, g_loss: 0.69344294\n",
      "Step: [13576] d_loss: 1.38609266, g_loss: 0.69458675\n",
      "Step: [13577] d_loss: 1.38632727, g_loss: 0.69259512\n",
      "Step: [13578] d_loss: 1.38633299, g_loss: 0.69504929\n",
      "Step: [13579] d_loss: 1.38656163, g_loss: 0.69258302\n",
      "Step: [13580] d_loss: 1.38705790, g_loss: 0.69411242\n",
      "Step: [13581] d_loss: 1.38712490, g_loss: 0.69239849\n",
      "Step: [13582] d_loss: 1.38603783, g_loss: 0.69477737\n",
      "Step: [13583] d_loss: 1.38627970, g_loss: 0.69404942\n",
      "Step: [13584] d_loss: 1.38653135, g_loss: 0.69134510\n",
      "Step: [13585] d_loss: 1.38665223, g_loss: 0.69303691\n",
      "Step: [13586] d_loss: 1.38632941, g_loss: 0.69415271\n",
      "Step: [13587] d_loss: 1.38629317, g_loss: 0.69293261\n",
      "Step: [13588] d_loss: 1.38635933, g_loss: 0.69386816\n",
      "Step: [13589] d_loss: 1.38650298, g_loss: 0.69315255\n",
      "Step: [13590] d_loss: 1.38639545, g_loss: 0.69355834\n",
      "Step: [13591] d_loss: 1.38617706, g_loss: 0.69423199\n",
      "Step: [13592] d_loss: 1.38631463, g_loss: 0.69273132\n",
      "Step: [13593] d_loss: 1.38630724, g_loss: 0.69335026\n",
      "Step: [13594] d_loss: 1.38625252, g_loss: 0.69329798\n",
      "Step: [13595] d_loss: 1.38608289, g_loss: 0.69370276\n",
      "Step: [13596] d_loss: 1.38614178, g_loss: 0.69300467\n",
      "Step: [13597] d_loss: 1.38612354, g_loss: 0.69318962\n",
      "Step: [13598] d_loss: 1.38624954, g_loss: 0.69340003\n",
      "Step: [13599] d_loss: 1.38627899, g_loss: 0.69298255\n",
      "Step: [13600] d_loss: 1.38612688, g_loss: 0.69313145\n",
      "Step: [13601] d_loss: 1.38624704, g_loss: 0.69336331\n",
      "Step: [13602] d_loss: 1.38629770, g_loss: 0.69339877\n",
      "Step: [13603] d_loss: 1.38625228, g_loss: 0.69310939\n",
      "Step: [13604] d_loss: 1.38644469, g_loss: 0.69318670\n",
      "Step: [13605] d_loss: 1.38612556, g_loss: 0.69392991\n",
      "Step: [13606] d_loss: 1.38608170, g_loss: 0.69395447\n",
      "Step: [13607] d_loss: 1.38622749, g_loss: 0.69230723\n",
      "Step: [13608] d_loss: 1.38662565, g_loss: 0.69073278\n",
      "Step: [13609] d_loss: 1.38678694, g_loss: 0.69262946\n",
      "Step: [13610] d_loss: 1.38650179, g_loss: 0.69558108\n",
      "Step: [13611] d_loss: 1.38634539, g_loss: 0.69534576\n",
      "Step: [13612] d_loss: 1.38626122, g_loss: 0.69495606\n",
      "Step: [13613] d_loss: 1.38675988, g_loss: 0.69766724\n",
      "Step: [13614] d_loss: 1.38742375, g_loss: 0.69356596\n",
      "Step: [13615] d_loss: 1.38711405, g_loss: 0.69553405\n",
      "Step: [13616] d_loss: 1.38669610, g_loss: 0.69315255\n",
      "Step: [13617] d_loss: 1.38637006, g_loss: 0.69391054\n",
      "Step: [13618] d_loss: 1.38642156, g_loss: 0.69303322\n",
      "Step: [13619] d_loss: 1.38626933, g_loss: 0.69325817\n",
      "Step: [13620] d_loss: 1.38625789, g_loss: 0.69367492\n",
      "Step: [13621] d_loss: 1.38635421, g_loss: 0.69248408\n",
      "Step: [13622] d_loss: 1.38587534, g_loss: 0.69365871\n",
      "Step: [13623] d_loss: 1.38638759, g_loss: 0.69501364\n",
      "Step: [13624] d_loss: 1.38607681, g_loss: 0.69531292\n",
      "Step: [13625] d_loss: 1.38637209, g_loss: 0.69390881\n",
      "Step: [13626] d_loss: 1.38640022, g_loss: 0.69205552\n",
      "Step: [13627] d_loss: 1.38621843, g_loss: 0.69284952\n",
      "Step: [13628] d_loss: 1.38607419, g_loss: 0.69323647\n",
      "Step: [13629] d_loss: 1.38667035, g_loss: 0.69369161\n",
      "Step: [13630] d_loss: 1.38649988, g_loss: 0.69203782\n",
      "Step: [13631] d_loss: 1.38667130, g_loss: 0.69356114\n",
      "Step: [13632] d_loss: 1.38608885, g_loss: 0.69299793\n",
      "Step: [13633] d_loss: 1.38613844, g_loss: 0.69359905\n",
      "Step: [13634] d_loss: 1.38656938, g_loss: 0.69189757\n",
      "Step: [13635] d_loss: 1.38626873, g_loss: 0.69183928\n",
      "Step: [13636] d_loss: 1.38624203, g_loss: 0.69339556\n",
      "Step: [13637] d_loss: 1.38626504, g_loss: 0.69400609\n",
      "Step: [13638] d_loss: 1.38626266, g_loss: 0.69415849\n",
      "Step: [13639] d_loss: 1.38626397, g_loss: 0.69317114\n",
      "Step: [13640] d_loss: 1.38636255, g_loss: 0.69315898\n",
      "Step: [13641] d_loss: 1.38634336, g_loss: 0.69364440\n",
      "Step: [13642] d_loss: 1.38618350, g_loss: 0.69243193\n",
      "Step: [13643] d_loss: 1.38616061, g_loss: 0.69094479\n",
      "Step: [13644] d_loss: 1.38651204, g_loss: 0.69326752\n",
      "Step: [13645] d_loss: 1.38636494, g_loss: 0.69278014\n",
      "Step: [13646] d_loss: 1.38631344, g_loss: 0.69409716\n",
      "Step: [13647] d_loss: 1.38626385, g_loss: 0.69424194\n",
      "Step: [13648] d_loss: 1.38643026, g_loss: 0.69337642\n",
      "Step: [13649] d_loss: 1.38630641, g_loss: 0.69332397\n",
      "Step: [13650] d_loss: 1.38625813, g_loss: 0.69297135\n",
      "Step: [13651] d_loss: 1.38640511, g_loss: 0.69264847\n",
      "Step: [13652] d_loss: 1.38636875, g_loss: 0.69383895\n",
      "Step: [13653] d_loss: 1.38655853, g_loss: 0.69230574\n",
      "Step: [13654] d_loss: 1.38668835, g_loss: 0.69502878\n",
      "Step: [13655] d_loss: 1.38737309, g_loss: 0.69030595\n",
      "Step: [13656] d_loss: 1.38727522, g_loss: 0.69363141\n",
      "Step: [13657] d_loss: 1.38681853, g_loss: 0.69152421\n",
      "Step: [13658] d_loss: 1.38663828, g_loss: 0.69380802\n",
      "Step: [13659] d_loss: 1.38634634, g_loss: 0.69358420\n",
      "Step: [13660] d_loss: 1.38635254, g_loss: 0.69290966\n",
      "Step: [13661] d_loss: 1.38627052, g_loss: 0.69316572\n",
      "Step: [13662] d_loss: 1.38649416, g_loss: 0.69284451\n",
      "Step: [13663] d_loss: 1.38650250, g_loss: 0.69270301\n",
      "Step: [13664] d_loss: 1.38638437, g_loss: 0.69297421\n",
      "Step: [13665] d_loss: 1.38635278, g_loss: 0.69313633\n",
      "Step: [13666] d_loss: 1.38622367, g_loss: 0.69787759\n",
      "Step: [13667] d_loss: 1.38633025, g_loss: 0.69333184\n",
      "Step: [13668] d_loss: 1.38620150, g_loss: 0.69303411\n",
      "Step: [13669] d_loss: 1.38643169, g_loss: 0.69289404\n",
      "Step: [13670] d_loss: 1.38633680, g_loss: 0.69214523\n",
      "Step: [13671] d_loss: 1.38650846, g_loss: 0.69432092\n",
      "Step: [13672] d_loss: 1.38638270, g_loss: 0.69234049\n",
      "Step: [13673] d_loss: 1.38606167, g_loss: 0.69283068\n",
      "Step: [13674] d_loss: 1.38633657, g_loss: 0.69455129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13675] d_loss: 1.38634098, g_loss: 0.69292772\n",
      "Step: [13676] d_loss: 1.38632751, g_loss: 0.69388008\n",
      "Step: [13677] d_loss: 1.38713360, g_loss: 0.69242048\n",
      "Step: [13678] d_loss: 1.38632536, g_loss: 0.69309288\n",
      "Step: [13679] d_loss: 1.38639164, g_loss: 0.69308984\n",
      "Step: [13680] d_loss: 1.38622987, g_loss: 0.69288218\n",
      "Step: [13681] d_loss: 1.38622081, g_loss: 0.69311416\n",
      "Step: [13682] d_loss: 1.38599277, g_loss: 0.69341183\n",
      "Step: [13683] d_loss: 1.38627911, g_loss: 0.69313997\n",
      "Step: [13684] d_loss: 1.38610649, g_loss: 0.69325644\n",
      "Step: [13685] d_loss: 1.38620245, g_loss: 0.69224942\n",
      "Step: [13686] d_loss: 1.38607359, g_loss: 0.69353426\n",
      "Step: [13687] d_loss: 1.38620186, g_loss: 0.69379771\n",
      "Step: [13688] d_loss: 1.38613391, g_loss: 0.69408453\n",
      "Step: [13689] d_loss: 1.38630128, g_loss: 0.69250160\n",
      "Step: [13690] d_loss: 1.38616514, g_loss: 0.69349676\n",
      "Step: [13691] d_loss: 1.38636279, g_loss: 0.69245124\n",
      "Step: [13692] d_loss: 1.38629556, g_loss: 0.69361627\n",
      "Step: [13693] d_loss: 1.38624287, g_loss: 0.69164050\n",
      "Step: [13694] d_loss: 1.38635898, g_loss: 0.69313288\n",
      "Step: [13695] d_loss: 1.38641548, g_loss: 0.69221753\n",
      "Step: [13696] d_loss: 1.38669670, g_loss: 0.69655693\n",
      "Step: [13697] d_loss: 1.38742685, g_loss: 0.69666141\n",
      "Step: [13698] d_loss: 1.38743138, g_loss: 0.69348395\n",
      "Step: [13699] d_loss: 1.38666880, g_loss: 0.68845773\n",
      "Step: [13700] d_loss: 1.38628888, g_loss: 0.69042903\n",
      "Step: [13701] d_loss: 1.38616157, g_loss: 0.69384027\n",
      "Step: [13702] d_loss: 1.38648939, g_loss: 0.69548291\n",
      "Step: [13703] d_loss: 1.38642907, g_loss: 0.69483268\n",
      "Step: [13704] d_loss: 1.38618016, g_loss: 0.69267046\n",
      "Step: [13705] d_loss: 1.38658476, g_loss: 0.69594425\n",
      "Step: [13706] d_loss: 1.38623989, g_loss: 0.69317436\n",
      "Step: [13707] d_loss: 1.38611948, g_loss: 0.69333220\n",
      "Step: [13708] d_loss: 1.38625717, g_loss: 0.69274265\n",
      "Step: [13709] d_loss: 1.38621271, g_loss: 0.69297671\n",
      "Step: [13710] d_loss: 1.38617158, g_loss: 0.69358784\n",
      "Step: [13711] d_loss: 1.38616419, g_loss: 0.69335413\n",
      "Step: [13712] d_loss: 1.38627589, g_loss: 0.69313854\n",
      "Step: [13713] d_loss: 1.38633108, g_loss: 0.69266176\n",
      "Step: [13714] d_loss: 1.38575935, g_loss: 0.69375789\n",
      "Step: [13715] d_loss: 1.38633132, g_loss: 0.69347012\n",
      "Step: [13716] d_loss: 1.38637912, g_loss: 0.69349742\n",
      "Step: [13717] d_loss: 1.38617444, g_loss: 0.69249976\n",
      "Step: [13718] d_loss: 1.38629639, g_loss: 0.69337672\n",
      "Step: [13719] d_loss: 1.38628697, g_loss: 0.69480836\n",
      "Step: [13720] d_loss: 1.38652122, g_loss: 0.69310510\n",
      "Step: [13721] d_loss: 1.38625586, g_loss: 0.69296193\n",
      "Step: [13722] d_loss: 1.38621354, g_loss: 0.69343901\n",
      "Step: [13723] d_loss: 1.38681710, g_loss: 0.69559920\n",
      "Step: [13724] d_loss: 1.38693357, g_loss: 0.69317752\n",
      "Step: [13725] d_loss: 1.38678765, g_loss: 0.69458950\n",
      "Step: [13726] d_loss: 1.38673139, g_loss: 0.69197673\n",
      "Step: [13727] d_loss: 1.38626468, g_loss: 0.69324565\n",
      "Step: [13728] d_loss: 1.38615370, g_loss: 0.69213396\n",
      "Step: [13729] d_loss: 1.38639688, g_loss: 0.69286501\n",
      "Step: [13730] d_loss: 1.38626361, g_loss: 0.69304562\n",
      "Step: [13731] d_loss: 1.38629687, g_loss: 0.69293773\n",
      "Step: [13732] d_loss: 1.38637972, g_loss: 0.69243771\n",
      "Step: [13733] d_loss: 1.38622928, g_loss: 0.69467545\n",
      "Step: [13734] d_loss: 1.38662434, g_loss: 0.69265032\n",
      "Step: [13735] d_loss: 1.38655567, g_loss: 0.69426978\n",
      "Step: [13736] d_loss: 1.38626146, g_loss: 0.69413030\n",
      "Step: [13737] d_loss: 1.38664079, g_loss: 0.69324172\n",
      "Step: [13738] d_loss: 1.38612342, g_loss: 0.69428849\n",
      "Step: [13739] d_loss: 1.38639903, g_loss: 0.69258088\n",
      "Step: [13740] d_loss: 1.38641369, g_loss: 0.69269520\n",
      "Step: [13741] d_loss: 1.38654804, g_loss: 0.69270545\n",
      "Step: [13742] d_loss: 1.38615680, g_loss: 0.69320494\n",
      "Step: [13743] d_loss: 1.38619113, g_loss: 0.69371396\n",
      "Step: [13744] d_loss: 1.38633275, g_loss: 0.69304609\n",
      "Step: [13745] d_loss: 1.38632405, g_loss: 0.69242024\n",
      "Step: [13746] d_loss: 1.38619280, g_loss: 0.69378519\n",
      "Step: [13747] d_loss: 1.38646734, g_loss: 0.69280261\n",
      "Step: [13748] d_loss: 1.38682592, g_loss: 0.69542086\n",
      "Step: [13749] d_loss: 1.38673091, g_loss: 0.69361222\n",
      "Step: [13750] d_loss: 1.38625133, g_loss: 0.69392800\n",
      "Step: [13751] d_loss: 1.38645029, g_loss: 0.69218302\n",
      "Step: [13752] d_loss: 1.38622427, g_loss: 0.69250226\n",
      "Step: [13753] d_loss: 1.38629532, g_loss: 0.69305062\n",
      "Step: [13754] d_loss: 1.38630247, g_loss: 0.69331074\n",
      "Step: [13755] d_loss: 1.38621545, g_loss: 0.69352520\n",
      "Step: [13756] d_loss: 1.38618255, g_loss: 0.69290662\n",
      "Step: [13757] d_loss: 1.38615179, g_loss: 0.69288290\n",
      "Step: [13758] d_loss: 1.38652718, g_loss: 0.69248879\n",
      "Step: [13759] d_loss: 1.38650179, g_loss: 0.69281363\n",
      "Step: [13760] d_loss: 1.38612747, g_loss: 0.69350791\n",
      "Step: [13761] d_loss: 1.38616800, g_loss: 0.69326377\n",
      "Step: [13762] d_loss: 1.38622856, g_loss: 0.69246125\n",
      "Step: [13763] d_loss: 1.38674140, g_loss: 0.69528842\n",
      "Step: [13764] d_loss: 1.38682652, g_loss: 0.69456595\n",
      "Step: [13765] d_loss: 1.38695860, g_loss: 0.69574928\n",
      "Step: [13766] d_loss: 1.38687396, g_loss: 0.69428027\n",
      "Step: [13767] d_loss: 1.38640177, g_loss: 0.69067669\n",
      "Step: [13768] d_loss: 1.38639402, g_loss: 0.69120389\n",
      "Step: [13769] d_loss: 1.38631201, g_loss: 0.69232774\n",
      "Step: [13770] d_loss: 1.38616347, g_loss: 0.69080317\n",
      "Step: [13771] d_loss: 1.38633323, g_loss: 0.69184124\n",
      "Step: [13772] d_loss: 1.38647544, g_loss: 0.69418776\n",
      "Step: [13773] d_loss: 1.38663495, g_loss: 0.69324911\n",
      "Step: [13774] d_loss: 1.38650823, g_loss: 0.69467282\n",
      "Step: [13775] d_loss: 1.38640094, g_loss: 0.69287610\n",
      "Step: [13776] d_loss: 1.38618159, g_loss: 0.69255447\n",
      "Step: [13777] d_loss: 1.38627899, g_loss: 0.69326180\n",
      "Step: [13778] d_loss: 1.38629174, g_loss: 0.69251829\n",
      "Step: [13779] d_loss: 1.38634014, g_loss: 0.69309509\n",
      "Step: [13780] d_loss: 1.38620830, g_loss: 0.69337904\n",
      "Step: [13781] d_loss: 1.38619566, g_loss: 0.69375277\n",
      "Step: [13782] d_loss: 1.38621306, g_loss: 0.69375813\n",
      "Step: [13783] d_loss: 1.38628173, g_loss: 0.69329500\n",
      "Step: [13784] d_loss: 1.38609529, g_loss: 0.69302106\n",
      "Step: [13785] d_loss: 1.38613045, g_loss: 0.69322479\n",
      "Step: [13786] d_loss: 1.38611293, g_loss: 0.69304973\n",
      "Step: [13787] d_loss: 1.38633084, g_loss: 0.69220233\n",
      "Step: [13788] d_loss: 1.38667798, g_loss: 0.69412541\n",
      "Step: [13789] d_loss: 1.38674808, g_loss: 0.69238973\n",
      "Step: [13790] d_loss: 1.38684273, g_loss: 0.69455874\n",
      "Step: [13791] d_loss: 1.38629508, g_loss: 0.69402218\n",
      "Step: [13792] d_loss: 1.38645923, g_loss: 0.69271672\n",
      "Step: [13793] d_loss: 1.38658559, g_loss: 0.69344354\n",
      "Step: [13794] d_loss: 1.38647151, g_loss: 0.69227380\n",
      "Step: [13795] d_loss: 1.38643122, g_loss: 0.69197702\n",
      "Step: [13796] d_loss: 1.38621891, g_loss: 0.69337749\n",
      "Step: [13797] d_loss: 1.38617468, g_loss: 0.69337130\n",
      "Step: [13798] d_loss: 1.38634849, g_loss: 0.69353843\n",
      "Step: [13799] d_loss: 1.38612103, g_loss: 0.69343352\n",
      "Step: [13800] d_loss: 1.38627672, g_loss: 0.69307220\n",
      "Step: [13801] d_loss: 1.38635659, g_loss: 0.69268548\n",
      "Step: [13802] d_loss: 1.38608098, g_loss: 0.69285607\n",
      "Step: [13803] d_loss: 1.38597775, g_loss: 0.69354737\n",
      "Step: [13804] d_loss: 1.38630533, g_loss: 0.69300073\n",
      "Step: [13805] d_loss: 1.38627625, g_loss: 0.69325316\n",
      "Step: [13806] d_loss: 1.38628936, g_loss: 0.69364786\n",
      "Step: [13807] d_loss: 1.38603783, g_loss: 0.69333422\n",
      "Step: [13808] d_loss: 1.38635314, g_loss: 0.69310987\n",
      "Step: [13809] d_loss: 1.38632464, g_loss: 0.69279635\n",
      "Step: [13810] d_loss: 1.38625717, g_loss: 0.69312871\n",
      "Step: [13811] d_loss: 1.38625479, g_loss: 0.69360751\n",
      "Step: [13812] d_loss: 1.38627183, g_loss: 0.69308096\n",
      "Step: [13813] d_loss: 1.38619626, g_loss: 0.69329178\n",
      "Step: [13814] d_loss: 1.38624704, g_loss: 0.69323277\n",
      "Step: [13815] d_loss: 1.38625193, g_loss: 0.69254208\n",
      "Step: [13816] d_loss: 1.38614106, g_loss: 0.69284391\n",
      "Step: [13817] d_loss: 1.38626969, g_loss: 0.69340575\n",
      "Step: [13818] d_loss: 1.38646066, g_loss: 0.69470406\n",
      "Step: [13819] d_loss: 1.38623929, g_loss: 0.69393164\n",
      "Step: [13820] d_loss: 1.38664234, g_loss: 0.69438607\n",
      "Step: [13821] d_loss: 1.38635707, g_loss: 0.69287527\n",
      "Step: [13822] d_loss: 1.38632154, g_loss: 0.69269598\n",
      "Step: [13823] d_loss: 1.38643646, g_loss: 0.69377023\n",
      "Step: [13824] d_loss: 1.38629985, g_loss: 0.69384283\n",
      "Step: [13825] d_loss: 1.38686323, g_loss: 0.69472468\n",
      "Step: [13826] d_loss: 1.38738704, g_loss: 0.69006461\n",
      "Step: [13827] d_loss: 1.38724017, g_loss: 0.69115174\n",
      "Step: [13828] d_loss: 1.38682628, g_loss: 0.69163620\n",
      "Step: [13829] d_loss: 1.38643646, g_loss: 0.69253576\n",
      "Step: [13830] d_loss: 1.38636243, g_loss: 0.69231892\n",
      "Step: [13831] d_loss: 1.38628578, g_loss: 0.69406164\n",
      "Step: [13832] d_loss: 1.38638735, g_loss: 0.69397455\n",
      "Step: [13833] d_loss: 1.38634765, g_loss: 0.69379008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13834] d_loss: 1.38638484, g_loss: 0.69286466\n",
      "Step: [13835] d_loss: 1.38651860, g_loss: 0.69330937\n",
      "Step: [13836] d_loss: 1.38630176, g_loss: 0.69259429\n",
      "Step: [13837] d_loss: 1.38627696, g_loss: 0.69321823\n",
      "Step: [13838] d_loss: 1.38639200, g_loss: 0.69345057\n",
      "Step: [13839] d_loss: 1.38636923, g_loss: 0.69347644\n",
      "Step: [13840] d_loss: 1.38628769, g_loss: 0.69321537\n",
      "Step: [13841] d_loss: 1.38624585, g_loss: 0.69331658\n",
      "Step: [13842] d_loss: 1.38625526, g_loss: 0.69318122\n",
      "Step: [13843] d_loss: 1.38636780, g_loss: 0.69289500\n",
      "Step: [13844] d_loss: 1.38627744, g_loss: 0.69317824\n",
      "Step: [13845] d_loss: 1.38633478, g_loss: 0.69270098\n",
      "Step: [13846] d_loss: 1.38651931, g_loss: 0.69437355\n",
      "Step: [13847] d_loss: 1.38617861, g_loss: 0.69308686\n",
      "Step: [13848] d_loss: 1.38624263, g_loss: 0.69319797\n",
      "Step: [13849] d_loss: 1.38617647, g_loss: 0.69313830\n",
      "Step: [13850] d_loss: 1.38635683, g_loss: 0.69306332\n",
      "Step: [13851] d_loss: 1.38614464, g_loss: 0.69341826\n",
      "Step: [13852] d_loss: 1.38632929, g_loss: 0.69331038\n",
      "Step: [13853] d_loss: 1.38634539, g_loss: 0.69250393\n",
      "Step: [13854] d_loss: 1.38631475, g_loss: 0.69319814\n",
      "Step: [13855] d_loss: 1.38647771, g_loss: 0.69260848\n",
      "Step: [13856] d_loss: 1.38650882, g_loss: 0.69290257\n",
      "Step: [13857] d_loss: 1.38639426, g_loss: 0.69187510\n",
      "Step: [13858] d_loss: 1.38632512, g_loss: 0.69209933\n",
      "Step: [13859] d_loss: 1.38621700, g_loss: 0.69360161\n",
      "Step: [13860] d_loss: 1.38637757, g_loss: 0.69372582\n",
      "Step: [13861] d_loss: 1.38655925, g_loss: 0.69367957\n",
      "Step: [13862] d_loss: 1.38631701, g_loss: 0.69356370\n",
      "Step: [13863] d_loss: 1.38640392, g_loss: 0.69252682\n",
      "Step: [13864] d_loss: 1.38660121, g_loss: 0.69425690\n",
      "Step: [13865] d_loss: 1.38638365, g_loss: 0.69352204\n",
      "Step: [13866] d_loss: 1.38636267, g_loss: 0.69266927\n",
      "Step: [13867] d_loss: 1.38635564, g_loss: 0.69306588\n",
      "Step: [13868] d_loss: 1.38627863, g_loss: 0.69292933\n",
      "Step: [13869] d_loss: 1.38633823, g_loss: 0.69360095\n",
      "Step: [13870] d_loss: 1.38630593, g_loss: 0.69334590\n",
      "Step: [13871] d_loss: 1.38650179, g_loss: 0.69481540\n",
      "Step: [13872] d_loss: 1.38624537, g_loss: 0.69306785\n",
      "Step: [13873] d_loss: 1.38622403, g_loss: 0.69287992\n",
      "Step: [13874] d_loss: 1.38625360, g_loss: 0.69334525\n",
      "Step: [13875] d_loss: 1.38620615, g_loss: 0.69404268\n",
      "Step: [13876] d_loss: 1.38640749, g_loss: 0.69269055\n",
      "Step: [13877] d_loss: 1.38622379, g_loss: 0.69413733\n",
      "Step: [13878] d_loss: 1.38618875, g_loss: 0.69265831\n",
      "Step: [13879] d_loss: 1.38616133, g_loss: 0.69245738\n",
      "Step: [13880] d_loss: 1.38625145, g_loss: 0.69398510\n",
      "Step: [13881] d_loss: 1.38621497, g_loss: 0.69375151\n",
      "Step: [13882] d_loss: 1.38618422, g_loss: 0.69390273\n",
      "Step: [13883] d_loss: 1.38630033, g_loss: 0.69248194\n",
      "Step: [13884] d_loss: 1.38628232, g_loss: 0.69353312\n",
      "Step: [13885] d_loss: 1.38618553, g_loss: 0.69371796\n",
      "Step: [13886] d_loss: 1.38618493, g_loss: 0.69456553\n",
      "Step: [13887] d_loss: 1.38636327, g_loss: 0.69366348\n",
      "Step: [13888] d_loss: 1.38620210, g_loss: 0.69284004\n",
      "Step: [13889] d_loss: 1.38613153, g_loss: 0.69226158\n",
      "Step: [13890] d_loss: 1.38627958, g_loss: 0.69288540\n",
      "Step: [13891] d_loss: 1.38637161, g_loss: 0.69305438\n",
      "Step: [13892] d_loss: 1.38623655, g_loss: 0.69323540\n",
      "Step: [13893] d_loss: 1.38610816, g_loss: 0.69335938\n",
      "Step: [13894] d_loss: 1.38609540, g_loss: 0.69342864\n",
      "Step: [13895] d_loss: 1.38617110, g_loss: 0.69289339\n",
      "Step: [13896] d_loss: 1.38659263, g_loss: 0.69437939\n",
      "Step: [13897] d_loss: 1.38643694, g_loss: 0.69319630\n",
      "Step: [13898] d_loss: 1.38634610, g_loss: 0.69222134\n",
      "Step: [13899] d_loss: 1.38621294, g_loss: 0.69239926\n",
      "Step: [13900] d_loss: 1.38635302, g_loss: 0.69307262\n",
      "Step: [13901] d_loss: 1.38640714, g_loss: 0.69374359\n",
      "Step: [13902] d_loss: 1.38628125, g_loss: 0.69523901\n",
      "Step: [13903] d_loss: 1.38612199, g_loss: 0.69334310\n",
      "Step: [13904] d_loss: 1.38622296, g_loss: 0.69256985\n",
      "Step: [13905] d_loss: 1.38629913, g_loss: 0.69303298\n",
      "Step: [13906] d_loss: 1.38625848, g_loss: 0.69338393\n",
      "Step: [13907] d_loss: 1.38635528, g_loss: 0.69351387\n",
      "Step: [13908] d_loss: 1.38630688, g_loss: 0.69318211\n",
      "Step: [13909] d_loss: 1.38621163, g_loss: 0.69306225\n",
      "Step: [13910] d_loss: 1.38622808, g_loss: 0.69319093\n",
      "Step: [13911] d_loss: 1.38685918, g_loss: 0.69345468\n",
      "Step: [13912] d_loss: 1.38625956, g_loss: 0.69286478\n",
      "Step: [13913] d_loss: 1.38624299, g_loss: 0.69286573\n",
      "Step: [13914] d_loss: 1.38624692, g_loss: 0.69401634\n",
      "Step: [13915] d_loss: 1.38616776, g_loss: 0.69334900\n",
      "Step: [13916] d_loss: 1.38616431, g_loss: 0.69419754\n",
      "Step: [13917] d_loss: 1.38624942, g_loss: 0.69299531\n",
      "Step: [13918] d_loss: 1.38604379, g_loss: 0.69313192\n",
      "Step: [13919] d_loss: 1.38636065, g_loss: 0.69493401\n",
      "Step: [13920] d_loss: 1.38660860, g_loss: 0.69262296\n",
      "Step: [13921] d_loss: 1.38665783, g_loss: 0.69442147\n",
      "Step: [13922] d_loss: 1.38675356, g_loss: 0.69372463\n",
      "Step: [13923] d_loss: 1.38674605, g_loss: 0.69729471\n",
      "Step: [13924] d_loss: 1.38629925, g_loss: 0.69442356\n",
      "Step: [13925] d_loss: 1.38650417, g_loss: 0.69461834\n",
      "Step: [13926] d_loss: 1.38645983, g_loss: 0.69206011\n",
      "Step: [13927] d_loss: 1.38629329, g_loss: 0.69374347\n",
      "Step: [13928] d_loss: 1.38597679, g_loss: 0.69410837\n",
      "Step: [13929] d_loss: 1.38639855, g_loss: 0.69298816\n",
      "Step: [13930] d_loss: 1.38631177, g_loss: 0.69285333\n",
      "Step: [13931] d_loss: 1.38628423, g_loss: 0.69339812\n",
      "Step: [13932] d_loss: 1.38640213, g_loss: 0.69300056\n",
      "Step: [13933] d_loss: 1.38631856, g_loss: 0.69346416\n",
      "Step: [13934] d_loss: 1.38631296, g_loss: 0.69339222\n",
      "Step: [13935] d_loss: 1.38622665, g_loss: 0.69354463\n",
      "Step: [13936] d_loss: 1.38641572, g_loss: 0.69297636\n",
      "Step: [13937] d_loss: 1.38632727, g_loss: 0.69298911\n",
      "Step: [13938] d_loss: 1.38614202, g_loss: 0.69359475\n",
      "Step: [13939] d_loss: 1.38616359, g_loss: 0.69379067\n",
      "Step: [13940] d_loss: 1.38612843, g_loss: 0.69362056\n",
      "Step: [13941] d_loss: 1.38636982, g_loss: 0.69301581\n",
      "Step: [13942] d_loss: 1.38636923, g_loss: 0.69306076\n",
      "Step: [13943] d_loss: 1.38620925, g_loss: 0.69306242\n",
      "Step: [13944] d_loss: 1.38635647, g_loss: 0.69308180\n",
      "Step: [13945] d_loss: 1.38627243, g_loss: 0.69331890\n",
      "Step: [13946] d_loss: 1.38624787, g_loss: 0.69324481\n",
      "Step: [13947] d_loss: 1.38634157, g_loss: 0.69321781\n",
      "Step: [13948] d_loss: 1.38625574, g_loss: 0.69302642\n",
      "Step: [13949] d_loss: 1.38628292, g_loss: 0.69318688\n",
      "Step: [13950] d_loss: 1.38616097, g_loss: 0.69333702\n",
      "Step: [13951] d_loss: 1.38624692, g_loss: 0.69316101\n",
      "Step: [13952] d_loss: 1.38618290, g_loss: 0.69309437\n",
      "Step: [13953] d_loss: 1.38619268, g_loss: 0.69320399\n",
      "Step: [13954] d_loss: 1.38625288, g_loss: 0.69340646\n",
      "Step: [13955] d_loss: 1.38626254, g_loss: 0.69343424\n",
      "Step: [13956] d_loss: 1.38617706, g_loss: 0.69322193\n",
      "Step: [13957] d_loss: 1.38639510, g_loss: 0.69294572\n",
      "Step: [13958] d_loss: 1.38634682, g_loss: 0.69319546\n",
      "Step: [13959] d_loss: 1.38627219, g_loss: 0.69337916\n",
      "Step: [13960] d_loss: 1.38660133, g_loss: 0.69448000\n",
      "Step: [13961] d_loss: 1.38693559, g_loss: 0.69447947\n",
      "Step: [13962] d_loss: 1.38728619, g_loss: 0.69897383\n",
      "Step: [13963] d_loss: 1.38723445, g_loss: 0.69404244\n",
      "Step: [13964] d_loss: 1.38694215, g_loss: 0.69265556\n",
      "Step: [13965] d_loss: 1.38671970, g_loss: 0.68899798\n",
      "Step: [13966] d_loss: 1.38639975, g_loss: 0.69125569\n",
      "Step: [13967] d_loss: 1.38625634, g_loss: 0.69545543\n",
      "Step: [13968] d_loss: 1.38637042, g_loss: 0.69577318\n",
      "Step: [13969] d_loss: 1.38633537, g_loss: 0.69478416\n",
      "Step: [13970] d_loss: 1.38628626, g_loss: 0.69219208\n",
      "Step: [13971] d_loss: 1.38627672, g_loss: 0.69239712\n",
      "Step: [13972] d_loss: 1.38634312, g_loss: 0.69282842\n",
      "Step: [13973] d_loss: 1.38632536, g_loss: 0.69482207\n",
      "Step: [13974] d_loss: 1.38651764, g_loss: 0.69364309\n",
      "Step: [13975] d_loss: 1.38669252, g_loss: 0.69307101\n",
      "Step: [13976] d_loss: 1.38660800, g_loss: 0.69154984\n",
      "Step: [13977] d_loss: 1.38642550, g_loss: 0.69348395\n",
      "Step: [13978] d_loss: 1.38630438, g_loss: 0.69369221\n",
      "Step: [13979] d_loss: 1.38632250, g_loss: 0.69315457\n",
      "Step: [13980] d_loss: 1.38630569, g_loss: 0.69255912\n",
      "Step: [13981] d_loss: 1.38624191, g_loss: 0.69259894\n",
      "Step: [13982] d_loss: 1.38626099, g_loss: 0.69337457\n",
      "Step: [13983] d_loss: 1.38627398, g_loss: 0.69327712\n",
      "Step: [13984] d_loss: 1.38624215, g_loss: 0.69327021\n",
      "Step: [13985] d_loss: 1.38625479, g_loss: 0.69278288\n",
      "Step: [13986] d_loss: 1.38638651, g_loss: 0.69426095\n",
      "Step: [13987] d_loss: 1.38632369, g_loss: 0.69522417\n",
      "Step: [13988] d_loss: 1.38630247, g_loss: 0.69325364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [13989] d_loss: 1.38640320, g_loss: 0.69377601\n",
      "Step: [13990] d_loss: 1.38634503, g_loss: 0.69287956\n",
      "Step: [13991] d_loss: 1.38619709, g_loss: 0.69274169\n",
      "Step: [13992] d_loss: 1.38633966, g_loss: 0.69261074\n",
      "Step: [13993] d_loss: 1.38631582, g_loss: 0.69270384\n",
      "Step: [13994] d_loss: 1.38615406, g_loss: 0.69352782\n",
      "Step: [13995] d_loss: 1.38619435, g_loss: 0.69331282\n",
      "Step: [13996] d_loss: 1.38622081, g_loss: 0.69381070\n",
      "Step: [13997] d_loss: 1.38623929, g_loss: 0.69168907\n",
      "Step: [13998] d_loss: 1.38661134, g_loss: 0.69498062\n",
      "Step: [13999] d_loss: 1.38671017, g_loss: 0.69510341\n",
      "Step: [14000] d_loss: 1.38689983, g_loss: 0.69571406\n",
      "Step: [14001] d_loss: 1.38662577, g_loss: 0.69223517\n",
      "Step: [14002] d_loss: 1.38611937, g_loss: 0.69142973\n",
      "Step: [14003] d_loss: 1.38620102, g_loss: 0.69253969\n",
      "Step: [14004] d_loss: 1.38624620, g_loss: 0.69326687\n",
      "Step: [14005] d_loss: 1.38666165, g_loss: 0.69376493\n",
      "Step: [14006] d_loss: 1.38711512, g_loss: 0.69276500\n",
      "Step: [14007] d_loss: 1.38748622, g_loss: 0.69505835\n",
      "Step: [14008] d_loss: 1.38732123, g_loss: 0.68939221\n",
      "Step: [14009] d_loss: 1.38692284, g_loss: 0.69036853\n",
      "Step: [14010] d_loss: 1.38638389, g_loss: 0.69263726\n",
      "Step: [14011] d_loss: 1.38622224, g_loss: 0.69495726\n",
      "Step: [14012] d_loss: 1.38620102, g_loss: 0.69516730\n",
      "Step: [14013] d_loss: 1.38620651, g_loss: 0.69359410\n",
      "Step: [14014] d_loss: 1.38615179, g_loss: 0.69316757\n",
      "Step: [14015] d_loss: 1.38635635, g_loss: 0.69258308\n",
      "Step: [14016] d_loss: 1.38613725, g_loss: 0.69318575\n",
      "Step: [14017] d_loss: 1.38612247, g_loss: 0.69361138\n",
      "Step: [14018] d_loss: 1.38637829, g_loss: 0.69327945\n",
      "Step: [14019] d_loss: 1.38652539, g_loss: 0.69320345\n",
      "Step: [14020] d_loss: 1.38651741, g_loss: 0.69418234\n",
      "Step: [14021] d_loss: 1.38637114, g_loss: 0.69252050\n",
      "Step: [14022] d_loss: 1.38633573, g_loss: 0.69306219\n",
      "Step: [14023] d_loss: 1.38629460, g_loss: 0.69352639\n",
      "Step: [14024] d_loss: 1.38630438, g_loss: 0.69367975\n",
      "Step: [14025] d_loss: 1.38621831, g_loss: 0.69353664\n",
      "Step: [14026] d_loss: 1.38627052, g_loss: 0.69316357\n",
      "Step: [14027] d_loss: 1.38634598, g_loss: 0.69335973\n",
      "Step: [14028] d_loss: 1.38627923, g_loss: 0.69301999\n",
      "Step: [14029] d_loss: 1.38625669, g_loss: 0.69263899\n",
      "Step: [14030] d_loss: 1.38636053, g_loss: 0.69298184\n",
      "Step: [14031] d_loss: 1.38631058, g_loss: 0.69354200\n",
      "Step: [14032] d_loss: 1.38628185, g_loss: 0.69355977\n",
      "Step: [14033] d_loss: 1.38631821, g_loss: 0.69325548\n",
      "Step: [14034] d_loss: 1.38620377, g_loss: 0.69301271\n",
      "Step: [14035] d_loss: 1.38639534, g_loss: 0.69288802\n",
      "Step: [14036] d_loss: 1.38627970, g_loss: 0.69308555\n",
      "Step: [14037] d_loss: 1.38631535, g_loss: 0.69315386\n",
      "Step: [14038] d_loss: 1.38635850, g_loss: 0.69323277\n",
      "Step: [14039] d_loss: 1.38631713, g_loss: 0.69314355\n",
      "Step: [14040] d_loss: 1.38634789, g_loss: 0.69323802\n",
      "Step: [14041] d_loss: 1.38622749, g_loss: 0.69314235\n",
      "Step: [14042] d_loss: 1.38630486, g_loss: 0.69301236\n",
      "Step: [14043] d_loss: 1.38627481, g_loss: 0.69292665\n",
      "Step: [14044] d_loss: 1.38630199, g_loss: 0.69318455\n",
      "Step: [14045] d_loss: 1.38650703, g_loss: 0.69291711\n",
      "Step: [14046] d_loss: 1.38623357, g_loss: 0.69326264\n",
      "Step: [14047] d_loss: 1.38614154, g_loss: 0.69272029\n",
      "Step: [14048] d_loss: 1.38628113, g_loss: 0.69306684\n",
      "Step: [14049] d_loss: 1.38631940, g_loss: 0.69229114\n",
      "Step: [14050] d_loss: 1.38650608, g_loss: 0.69366920\n",
      "Step: [14051] d_loss: 1.38628078, g_loss: 0.69405997\n",
      "Step: [14052] d_loss: 1.38622236, g_loss: 0.69335115\n",
      "Step: [14053] d_loss: 1.38629460, g_loss: 0.69268024\n",
      "Step: [14054] d_loss: 1.38628578, g_loss: 0.69254678\n",
      "Step: [14055] d_loss: 1.38628972, g_loss: 0.69301164\n",
      "Step: [14056] d_loss: 1.38621712, g_loss: 0.69345558\n",
      "Step: [14057] d_loss: 1.38613045, g_loss: 0.69404495\n",
      "Step: [14058] d_loss: 1.38605618, g_loss: 0.69372237\n",
      "Step: [14059] d_loss: 1.38640583, g_loss: 0.69272834\n",
      "Step: [14060] d_loss: 1.38625908, g_loss: 0.69319344\n",
      "Step: [14061] d_loss: 1.38644814, g_loss: 0.69413424\n",
      "Step: [14062] d_loss: 1.38655663, g_loss: 0.69351918\n",
      "Step: [14063] d_loss: 1.38637018, g_loss: 0.69314611\n",
      "Step: [14064] d_loss: 1.38627529, g_loss: 0.69222939\n",
      "Step: [14065] d_loss: 1.38620305, g_loss: 0.69293165\n",
      "Step: [14066] d_loss: 1.38632822, g_loss: 0.69349706\n",
      "Step: [14067] d_loss: 1.38627863, g_loss: 0.69328940\n",
      "Step: [14068] d_loss: 1.38630807, g_loss: 0.69334197\n",
      "Step: [14069] d_loss: 1.38678098, g_loss: 0.69144964\n",
      "Step: [14070] d_loss: 1.38621163, g_loss: 0.69333184\n",
      "Step: [14071] d_loss: 1.38626790, g_loss: 0.69235122\n",
      "Step: [14072] d_loss: 1.38643193, g_loss: 0.69329435\n",
      "Step: [14073] d_loss: 1.38635659, g_loss: 0.69224089\n",
      "Step: [14074] d_loss: 1.38632941, g_loss: 0.69225520\n",
      "Step: [14075] d_loss: 1.38628292, g_loss: 0.69237345\n",
      "Step: [14076] d_loss: 1.38622761, g_loss: 0.69283962\n",
      "Step: [14077] d_loss: 1.38614035, g_loss: 0.69379735\n",
      "Step: [14078] d_loss: 1.38633394, g_loss: 0.69391608\n",
      "Step: [14079] d_loss: 1.38623881, g_loss: 0.69396150\n",
      "Step: [14080] d_loss: 1.38612866, g_loss: 0.69414020\n",
      "Step: [14081] d_loss: 1.38603818, g_loss: 0.69380665\n",
      "Step: [14082] d_loss: 1.38615894, g_loss: 0.69340265\n",
      "Step: [14083] d_loss: 1.38619518, g_loss: 0.69359148\n",
      "Step: [14084] d_loss: 1.38615632, g_loss: 0.69424379\n",
      "Step: [14085] d_loss: 1.38622451, g_loss: 0.69264328\n",
      "Step: [14086] d_loss: 1.38630819, g_loss: 0.69249058\n",
      "Step: [14087] d_loss: 1.38642263, g_loss: 0.69337189\n",
      "Step: [14088] d_loss: 1.38636041, g_loss: 0.69760299\n",
      "Step: [14089] d_loss: 1.38661027, g_loss: 0.69448346\n",
      "Step: [14090] d_loss: 1.38703692, g_loss: 0.69502765\n",
      "Step: [14091] d_loss: 1.38702226, g_loss: 0.68994212\n",
      "Step: [14092] d_loss: 1.38770390, g_loss: 0.69397795\n",
      "Step: [14093] d_loss: 1.38752866, g_loss: 0.69421744\n",
      "Step: [14094] d_loss: 1.38720107, g_loss: 0.69731617\n",
      "Step: [14095] d_loss: 1.38676929, g_loss: 0.69368482\n",
      "Step: [14096] d_loss: 1.38667178, g_loss: 0.69435287\n",
      "Step: [14097] d_loss: 1.38649678, g_loss: 0.69213796\n",
      "Step: [14098] d_loss: 1.38640046, g_loss: 0.69311160\n",
      "Step: [14099] d_loss: 1.38634992, g_loss: 0.69271946\n",
      "Step: [14100] d_loss: 1.38621902, g_loss: 0.69320786\n",
      "Step: [14101] d_loss: 1.38641357, g_loss: 0.69332236\n",
      "Step: [14102] d_loss: 1.38615036, g_loss: 0.69287241\n",
      "Step: [14103] d_loss: 1.38614964, g_loss: 0.69203532\n",
      "Step: [14104] d_loss: 1.38634253, g_loss: 0.69298977\n",
      "Step: [14105] d_loss: 1.38615990, g_loss: 0.69382620\n",
      "Step: [14106] d_loss: 1.38638520, g_loss: 0.69277310\n",
      "Step: [14107] d_loss: 1.38636565, g_loss: 0.69154704\n",
      "Step: [14108] d_loss: 1.38627195, g_loss: 0.69124776\n",
      "Step: [14109] d_loss: 1.38622844, g_loss: 0.69295311\n",
      "Step: [14110] d_loss: 1.38614905, g_loss: 0.69438565\n",
      "Step: [14111] d_loss: 1.38641906, g_loss: 0.69397879\n",
      "Step: [14112] d_loss: 1.38614893, g_loss: 0.69312119\n",
      "Step: [14113] d_loss: 1.38620150, g_loss: 0.69226551\n",
      "Step: [14114] d_loss: 1.38614345, g_loss: 0.69309616\n",
      "Step: [14115] d_loss: 1.38630247, g_loss: 0.69347930\n",
      "Step: [14116] d_loss: 1.38624144, g_loss: 0.69360626\n",
      "Step: [14117] d_loss: 1.38607836, g_loss: 0.69417310\n",
      "Step: [14118] d_loss: 1.38587761, g_loss: 0.69446456\n",
      "Step: [14119] d_loss: 1.38616538, g_loss: 0.69285917\n",
      "Step: [14120] d_loss: 1.38618886, g_loss: 0.69311929\n",
      "Step: [14121] d_loss: 1.38636410, g_loss: 0.69291937\n",
      "Step: [14122] d_loss: 1.38626480, g_loss: 0.69296443\n",
      "Step: [14123] d_loss: 1.38654041, g_loss: 0.69371551\n",
      "Step: [14124] d_loss: 1.38671327, g_loss: 0.69148189\n",
      "Step: [14125] d_loss: 1.38701415, g_loss: 0.69354033\n",
      "Step: [14126] d_loss: 1.38665676, g_loss: 0.69166428\n",
      "Step: [14127] d_loss: 1.38642907, g_loss: 0.69045722\n",
      "Step: [14128] d_loss: 1.38634372, g_loss: 0.69287670\n",
      "Step: [14129] d_loss: 1.38620377, g_loss: 0.69538927\n",
      "Step: [14130] d_loss: 1.38637686, g_loss: 0.69370699\n",
      "Step: [14131] d_loss: 1.38659906, g_loss: 0.69475377\n",
      "Step: [14132] d_loss: 1.38678575, g_loss: 0.69300640\n",
      "Step: [14133] d_loss: 1.38647008, g_loss: 0.69472891\n",
      "Step: [14134] d_loss: 1.38715386, g_loss: 0.69486547\n",
      "Step: [14135] d_loss: 1.38643134, g_loss: 0.69181705\n",
      "Step: [14136] d_loss: 1.38643909, g_loss: 0.69250262\n",
      "Step: [14137] d_loss: 1.38631415, g_loss: 0.69388992\n",
      "Step: [14138] d_loss: 1.38624048, g_loss: 0.69383413\n",
      "Step: [14139] d_loss: 1.38635039, g_loss: 0.69336486\n",
      "Step: [14140] d_loss: 1.38637304, g_loss: 0.69308007\n",
      "Step: [14141] d_loss: 1.38631678, g_loss: 0.69291425\n",
      "Step: [14142] d_loss: 1.38623023, g_loss: 0.69374955\n",
      "Step: [14143] d_loss: 1.38625646, g_loss: 0.69356054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14144] d_loss: 1.38624895, g_loss: 0.69363326\n",
      "Step: [14145] d_loss: 1.38636184, g_loss: 0.69226325\n",
      "Step: [14146] d_loss: 1.38627517, g_loss: 0.69239652\n",
      "Step: [14147] d_loss: 1.38611174, g_loss: 0.69266760\n",
      "Step: [14148] d_loss: 1.38630533, g_loss: 0.69342804\n",
      "Step: [14149] d_loss: 1.38628447, g_loss: 0.69409704\n",
      "Step: [14150] d_loss: 1.38546467, g_loss: 0.69439638\n",
      "Step: [14151] d_loss: 1.38635600, g_loss: 0.69327509\n",
      "Step: [14152] d_loss: 1.38638079, g_loss: 0.69243193\n",
      "Step: [14153] d_loss: 1.38628840, g_loss: 0.69247949\n",
      "Step: [14154] d_loss: 1.38610315, g_loss: 0.69387782\n",
      "Step: [14155] d_loss: 1.38629436, g_loss: 0.69399095\n",
      "Step: [14156] d_loss: 1.38628721, g_loss: 0.69309324\n",
      "Step: [14157] d_loss: 1.38613474, g_loss: 0.69323242\n",
      "Step: [14158] d_loss: 1.38615417, g_loss: 0.69296819\n",
      "Step: [14159] d_loss: 1.38611341, g_loss: 0.69210362\n",
      "Step: [14160] d_loss: 1.38651443, g_loss: 0.69296455\n",
      "Step: [14161] d_loss: 1.38620043, g_loss: 0.69440079\n",
      "Step: [14162] d_loss: 1.38656998, g_loss: 0.69399327\n",
      "Step: [14163] d_loss: 1.38627696, g_loss: 0.69250739\n",
      "Step: [14164] d_loss: 1.38633192, g_loss: 0.69207948\n",
      "Step: [14165] d_loss: 1.38635015, g_loss: 0.69271451\n",
      "Step: [14166] d_loss: 1.38619554, g_loss: 0.69367766\n",
      "Step: [14167] d_loss: 1.38622236, g_loss: 0.69349033\n",
      "Step: [14168] d_loss: 1.38636041, g_loss: 0.69424379\n",
      "Step: [14169] d_loss: 1.38621569, g_loss: 0.69339871\n",
      "Step: [14170] d_loss: 1.38618946, g_loss: 0.69266123\n",
      "Step: [14171] d_loss: 1.38598371, g_loss: 0.69410568\n",
      "Step: [14172] d_loss: 1.38623810, g_loss: 0.69318700\n",
      "Step: [14173] d_loss: 1.38614166, g_loss: 0.69411802\n",
      "Step: [14174] d_loss: 1.38636827, g_loss: 0.69290483\n",
      "Step: [14175] d_loss: 1.38621962, g_loss: 0.69370395\n",
      "Step: [14176] d_loss: 1.38649154, g_loss: 0.69245237\n",
      "Step: [14177] d_loss: 1.38644588, g_loss: 0.69347578\n",
      "Step: [14178] d_loss: 1.38636649, g_loss: 0.69278795\n",
      "Step: [14179] d_loss: 1.38616514, g_loss: 0.69337177\n",
      "Step: [14180] d_loss: 1.38628054, g_loss: 0.69319618\n",
      "Step: [14181] d_loss: 1.38628268, g_loss: 0.69309026\n",
      "Step: [14182] d_loss: 1.38625729, g_loss: 0.69347554\n",
      "Step: [14183] d_loss: 1.38636875, g_loss: 0.69328701\n",
      "Step: [14184] d_loss: 1.38643026, g_loss: 0.69358474\n",
      "Step: [14185] d_loss: 1.38608253, g_loss: 0.69324195\n",
      "Step: [14186] d_loss: 1.38621640, g_loss: 0.69253045\n",
      "Step: [14187] d_loss: 1.38630033, g_loss: 0.69359440\n",
      "Step: [14188] d_loss: 1.38619959, g_loss: 0.69366014\n",
      "Step: [14189] d_loss: 1.38623476, g_loss: 0.69282436\n",
      "Step: [14190] d_loss: 1.38630557, g_loss: 0.69312394\n",
      "Step: [14191] d_loss: 1.38620341, g_loss: 0.69353402\n",
      "Step: [14192] d_loss: 1.38620305, g_loss: 0.69403303\n",
      "Step: [14193] d_loss: 1.38619328, g_loss: 0.69295549\n",
      "Step: [14194] d_loss: 1.38620400, g_loss: 0.69348729\n",
      "Step: [14195] d_loss: 1.38638997, g_loss: 0.69285500\n",
      "Step: [14196] d_loss: 1.38603568, g_loss: 0.69342875\n",
      "Step: [14197] d_loss: 1.38648844, g_loss: 0.69482899\n",
      "Step: [14198] d_loss: 1.38613737, g_loss: 0.69268370\n",
      "Step: [14199] d_loss: 1.38623381, g_loss: 0.69141024\n",
      "Step: [14200] d_loss: 1.38649511, g_loss: 0.69425493\n",
      "Step: [14201] d_loss: 1.38654029, g_loss: 0.69292915\n",
      "Step: [14202] d_loss: 1.38637805, g_loss: 0.69510007\n",
      "Step: [14203] d_loss: 1.38649797, g_loss: 0.69378084\n",
      "Step: [14204] d_loss: 1.38637662, g_loss: 0.69371605\n",
      "Step: [14205] d_loss: 1.38634419, g_loss: 0.69200742\n",
      "Step: [14206] d_loss: 1.38645911, g_loss: 0.69338787\n",
      "Step: [14207] d_loss: 1.38667464, g_loss: 0.69717634\n",
      "Step: [14208] d_loss: 1.38674128, g_loss: 0.69357538\n",
      "Step: [14209] d_loss: 1.38676953, g_loss: 0.69465423\n",
      "Step: [14210] d_loss: 1.38680339, g_loss: 0.69602275\n",
      "Step: [14211] d_loss: 1.38645792, g_loss: 0.69372165\n",
      "Step: [14212] d_loss: 1.38621330, g_loss: 0.69353926\n",
      "Step: [14213] d_loss: 1.38633716, g_loss: 0.69279355\n",
      "Step: [14214] d_loss: 1.38635266, g_loss: 0.69284242\n",
      "Step: [14215] d_loss: 1.38634038, g_loss: 0.69293106\n",
      "Step: [14216] d_loss: 1.38623667, g_loss: 0.69321781\n",
      "Step: [14217] d_loss: 1.38625646, g_loss: 0.69341385\n",
      "Step: [14218] d_loss: 1.38640571, g_loss: 0.69321585\n",
      "Step: [14219] d_loss: 1.38633144, g_loss: 0.69314635\n",
      "Step: [14220] d_loss: 1.38615370, g_loss: 0.69328010\n",
      "Step: [14221] d_loss: 1.38631785, g_loss: 0.69300169\n",
      "Step: [14222] d_loss: 1.38626552, g_loss: 0.69298744\n",
      "Step: [14223] d_loss: 1.38608503, g_loss: 0.69338369\n",
      "Step: [14224] d_loss: 1.38611841, g_loss: 0.69319981\n",
      "Step: [14225] d_loss: 1.38648963, g_loss: 0.69318080\n",
      "Step: [14226] d_loss: 1.38610649, g_loss: 0.69360489\n",
      "Step: [14227] d_loss: 1.38646150, g_loss: 0.69381368\n",
      "Step: [14228] d_loss: 1.38631272, g_loss: 0.69308043\n",
      "Step: [14229] d_loss: 1.38481569, g_loss: 0.69691855\n",
      "Step: [14230] d_loss: 1.38619268, g_loss: 0.69297838\n",
      "Step: [14231] d_loss: 1.38645577, g_loss: 0.69444412\n",
      "Step: [14232] d_loss: 1.38651013, g_loss: 0.69385767\n",
      "Step: [14233] d_loss: 1.38628554, g_loss: 0.69435036\n",
      "Step: [14234] d_loss: 1.38621998, g_loss: 0.69444621\n",
      "Step: [14235] d_loss: 1.38617361, g_loss: 0.69232786\n",
      "Step: [14236] d_loss: 1.38772869, g_loss: 0.69433653\n",
      "Step: [14237] d_loss: 1.38616598, g_loss: 0.69259465\n",
      "Step: [14238] d_loss: 1.38602579, g_loss: 0.69386041\n",
      "Step: [14239] d_loss: 1.38626027, g_loss: 0.69414318\n",
      "Step: [14240] d_loss: 1.38715315, g_loss: 0.69402254\n",
      "Step: [14241] d_loss: 1.38619041, g_loss: 0.69345886\n",
      "Step: [14242] d_loss: 1.38654542, g_loss: 0.69301569\n",
      "Step: [14243] d_loss: 1.38627827, g_loss: 0.69292498\n",
      "Step: [14244] d_loss: 1.38646483, g_loss: 0.69418776\n",
      "Step: [14245] d_loss: 1.38621426, g_loss: 0.69374281\n",
      "Step: [14246] d_loss: 1.38620758, g_loss: 0.69391567\n",
      "Step: [14247] d_loss: 1.38583589, g_loss: 0.69182938\n",
      "Step: [14248] d_loss: 1.38649690, g_loss: 0.69258380\n",
      "Step: [14249] d_loss: 1.38630283, g_loss: 0.69341016\n",
      "Step: [14250] d_loss: 1.38628244, g_loss: 0.69337142\n",
      "Step: [14251] d_loss: 1.38626039, g_loss: 0.69189519\n",
      "Step: [14252] d_loss: 1.38665140, g_loss: 0.69326985\n",
      "Step: [14253] d_loss: 1.38638163, g_loss: 0.69454056\n",
      "Step: [14254] d_loss: 1.38634562, g_loss: 0.69271362\n",
      "Step: [14255] d_loss: 1.38651538, g_loss: 0.69299340\n",
      "Step: [14256] d_loss: 1.38631725, g_loss: 0.69256902\n",
      "Step: [14257] d_loss: 1.38658559, g_loss: 0.69364095\n",
      "Step: [14258] d_loss: 1.38626969, g_loss: 0.69396645\n",
      "Step: [14259] d_loss: 1.38636053, g_loss: 0.69367248\n",
      "Step: [14260] d_loss: 1.38628137, g_loss: 0.69330299\n",
      "Step: [14261] d_loss: 1.38636565, g_loss: 0.69329906\n",
      "Step: [14262] d_loss: 1.38641429, g_loss: 0.69279826\n",
      "Step: [14263] d_loss: 1.38635266, g_loss: 0.69316912\n",
      "Step: [14264] d_loss: 1.38621736, g_loss: 0.69292104\n",
      "Step: [14265] d_loss: 1.38621116, g_loss: 0.69322932\n",
      "Step: [14266] d_loss: 1.38620067, g_loss: 0.69363850\n",
      "Step: [14267] d_loss: 1.38641357, g_loss: 0.69290411\n",
      "Step: [14268] d_loss: 1.38632309, g_loss: 0.69346380\n",
      "Step: [14269] d_loss: 1.38605642, g_loss: 0.69434738\n",
      "Step: [14270] d_loss: 1.38630152, g_loss: 0.69231957\n",
      "Step: [14271] d_loss: 1.38612604, g_loss: 0.69288152\n",
      "Step: [14272] d_loss: 1.38626575, g_loss: 0.69405323\n",
      "Step: [14273] d_loss: 1.38647902, g_loss: 0.69370091\n",
      "Step: [14274] d_loss: 1.38609099, g_loss: 0.69293934\n",
      "Step: [14275] d_loss: 1.38632381, g_loss: 0.69337887\n",
      "Step: [14276] d_loss: 1.38600671, g_loss: 0.69366157\n",
      "Step: [14277] d_loss: 1.38635600, g_loss: 0.69374698\n",
      "Step: [14278] d_loss: 1.38652539, g_loss: 0.69239116\n",
      "Step: [14279] d_loss: 1.38649392, g_loss: 0.69430673\n",
      "Step: [14280] d_loss: 1.38645458, g_loss: 0.69445342\n",
      "Step: [14281] d_loss: 1.38611376, g_loss: 0.69448721\n",
      "Step: [14282] d_loss: 1.38635087, g_loss: 0.69259167\n",
      "Step: [14283] d_loss: 1.38611054, g_loss: 0.69218391\n",
      "Step: [14284] d_loss: 1.38637674, g_loss: 0.69221175\n",
      "Step: [14285] d_loss: 1.38642728, g_loss: 0.69375646\n",
      "Step: [14286] d_loss: 1.38638139, g_loss: 0.69446450\n",
      "Step: [14287] d_loss: 1.38659525, g_loss: 0.69127584\n",
      "Step: [14288] d_loss: 1.38803649, g_loss: 0.69703221\n",
      "Step: [14289] d_loss: 1.38838077, g_loss: 0.69919366\n",
      "Step: [14290] d_loss: 1.38795376, g_loss: 0.69974047\n",
      "Step: [14291] d_loss: 1.38769746, g_loss: 0.69331169\n",
      "Step: [14292] d_loss: 1.38707960, g_loss: 0.69194353\n",
      "Step: [14293] d_loss: 1.38667166, g_loss: 0.68914676\n",
      "Step: [14294] d_loss: 1.38643599, g_loss: 0.69179142\n",
      "Step: [14295] d_loss: 1.38617778, g_loss: 0.69477308\n",
      "Step: [14296] d_loss: 1.38625789, g_loss: 0.69503081\n",
      "Step: [14297] d_loss: 1.38646269, g_loss: 0.69433141\n",
      "Step: [14298] d_loss: 1.38611245, g_loss: 0.69297814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14299] d_loss: 1.38630748, g_loss: 0.69188499\n",
      "Step: [14300] d_loss: 1.38627601, g_loss: 0.69274092\n",
      "Step: [14301] d_loss: 1.38626528, g_loss: 0.69417536\n",
      "Step: [14302] d_loss: 1.38689590, g_loss: 0.69200873\n",
      "Step: [14303] d_loss: 1.38631225, g_loss: 0.69331014\n",
      "Step: [14304] d_loss: 1.38626719, g_loss: 0.69284254\n",
      "Step: [14305] d_loss: 1.38622379, g_loss: 0.69312561\n",
      "Step: [14306] d_loss: 1.38622546, g_loss: 0.69344115\n",
      "Step: [14307] d_loss: 1.38652730, g_loss: 0.69308400\n",
      "Step: [14308] d_loss: 1.38610864, g_loss: 0.69313896\n",
      "Step: [14309] d_loss: 1.38627720, g_loss: 0.69344968\n",
      "Step: [14310] d_loss: 1.38610721, g_loss: 0.69304675\n",
      "Step: [14311] d_loss: 1.38632321, g_loss: 0.69354993\n",
      "Step: [14312] d_loss: 1.38600874, g_loss: 0.69431615\n",
      "Step: [14313] d_loss: 1.38623691, g_loss: 0.69250798\n",
      "Step: [14314] d_loss: 1.38613725, g_loss: 0.69259661\n",
      "Step: [14315] d_loss: 1.38618362, g_loss: 0.69373846\n",
      "Step: [14316] d_loss: 1.38677382, g_loss: 0.69323635\n",
      "Step: [14317] d_loss: 1.38695085, g_loss: 0.69023156\n",
      "Step: [14318] d_loss: 1.38662839, g_loss: 0.68833947\n",
      "Step: [14319] d_loss: 1.38630843, g_loss: 0.69275188\n",
      "Step: [14320] d_loss: 1.38613307, g_loss: 0.69473028\n",
      "Step: [14321] d_loss: 1.38622773, g_loss: 0.69543481\n",
      "Step: [14322] d_loss: 1.38615108, g_loss: 0.69449204\n",
      "Step: [14323] d_loss: 1.38638735, g_loss: 0.69294381\n",
      "Step: [14324] d_loss: 1.38663530, g_loss: 0.69216561\n",
      "Step: [14325] d_loss: 1.38678002, g_loss: 0.69236159\n",
      "Step: [14326] d_loss: 1.38621855, g_loss: 0.69375837\n",
      "Step: [14327] d_loss: 1.38606131, g_loss: 0.69541496\n",
      "Step: [14328] d_loss: 1.38686264, g_loss: 0.69300377\n",
      "Step: [14329] d_loss: 1.38673234, g_loss: 0.69021261\n",
      "Step: [14330] d_loss: 1.38669157, g_loss: 0.69371927\n",
      "Step: [14331] d_loss: 1.38657546, g_loss: 0.69693166\n",
      "Step: [14332] d_loss: 1.38655090, g_loss: 0.69525731\n",
      "Step: [14333] d_loss: 1.38618326, g_loss: 0.69341558\n",
      "Step: [14334] d_loss: 1.38637447, g_loss: 0.68985748\n",
      "Step: [14335] d_loss: 1.38633180, g_loss: 0.69102347\n",
      "Step: [14336] d_loss: 1.38620567, g_loss: 0.69374609\n",
      "Step: [14337] d_loss: 1.38625741, g_loss: 0.69423193\n",
      "Step: [14338] d_loss: 1.38654017, g_loss: 0.69327068\n",
      "Step: [14339] d_loss: 1.38624597, g_loss: 0.69416595\n",
      "Step: [14340] d_loss: 1.38614416, g_loss: 0.69269252\n",
      "Step: [14341] d_loss: 1.38636804, g_loss: 0.69131231\n",
      "Step: [14342] d_loss: 1.38624692, g_loss: 0.69265711\n",
      "Step: [14343] d_loss: 1.38606095, g_loss: 0.69331151\n",
      "Step: [14344] d_loss: 1.38619626, g_loss: 0.69409025\n",
      "Step: [14345] d_loss: 1.38601875, g_loss: 0.69379377\n",
      "Step: [14346] d_loss: 1.38642836, g_loss: 0.69320363\n",
      "Step: [14347] d_loss: 1.38632560, g_loss: 0.69305587\n",
      "Step: [14348] d_loss: 1.38623834, g_loss: 0.69267249\n",
      "Step: [14349] d_loss: 1.38623369, g_loss: 0.69386160\n",
      "Step: [14350] d_loss: 1.38634348, g_loss: 0.69464099\n",
      "Step: [14351] d_loss: 1.38621199, g_loss: 0.69375384\n",
      "Step: [14352] d_loss: 1.38640225, g_loss: 0.69161403\n",
      "Step: [14353] d_loss: 1.38627291, g_loss: 0.69214165\n",
      "Step: [14354] d_loss: 1.38616896, g_loss: 0.69395339\n",
      "Step: [14355] d_loss: 1.38602185, g_loss: 0.69475877\n",
      "Step: [14356] d_loss: 1.38630581, g_loss: 0.69585919\n",
      "Step: [14357] d_loss: 1.38767183, g_loss: 0.69400799\n",
      "Step: [14358] d_loss: 1.38854754, g_loss: 0.69851077\n",
      "Step: [14359] d_loss: 1.38849330, g_loss: 0.69763774\n",
      "Step: [14360] d_loss: 1.38742399, g_loss: 0.69625270\n",
      "Step: [14361] d_loss: 1.38652706, g_loss: 0.69429815\n",
      "Step: [14362] d_loss: 1.38640738, g_loss: 0.69205844\n",
      "Step: [14363] d_loss: 1.38622904, g_loss: 0.69169611\n",
      "Step: [14364] d_loss: 1.38641429, g_loss: 0.69136447\n",
      "Step: [14365] d_loss: 1.38637114, g_loss: 0.69332606\n",
      "Step: [14366] d_loss: 1.38632345, g_loss: 0.69459265\n",
      "Step: [14367] d_loss: 1.38641369, g_loss: 0.69448674\n",
      "Step: [14368] d_loss: 1.38629913, g_loss: 0.69505215\n",
      "Step: [14369] d_loss: 1.38634908, g_loss: 0.69148529\n",
      "Step: [14370] d_loss: 1.38632059, g_loss: 0.69324464\n",
      "Step: [14371] d_loss: 1.38653207, g_loss: 0.69519383\n",
      "Step: [14372] d_loss: 1.38649535, g_loss: 0.69315267\n",
      "Step: [14373] d_loss: 1.38641202, g_loss: 0.69297564\n",
      "Step: [14374] d_loss: 1.38641226, g_loss: 0.69330221\n",
      "Step: [14375] d_loss: 1.38644695, g_loss: 0.69390905\n",
      "Step: [14376] d_loss: 1.38646626, g_loss: 0.69298714\n",
      "Step: [14377] d_loss: 1.38652635, g_loss: 0.69251728\n",
      "Step: [14378] d_loss: 1.38626862, g_loss: 0.68853527\n",
      "Step: [14379] d_loss: 1.38631046, g_loss: 0.69240028\n",
      "Step: [14380] d_loss: 1.38619137, g_loss: 0.69441944\n",
      "Step: [14381] d_loss: 1.38627970, g_loss: 0.69264257\n",
      "Step: [14382] d_loss: 1.38626337, g_loss: 0.69554263\n",
      "Step: [14383] d_loss: 1.38651156, g_loss: 0.69202268\n",
      "Step: [14384] d_loss: 1.38638043, g_loss: 0.69255018\n",
      "Step: [14385] d_loss: 1.38644052, g_loss: 0.69259173\n",
      "Step: [14386] d_loss: 1.38620353, g_loss: 0.69324440\n",
      "Step: [14387] d_loss: 1.38592005, g_loss: 0.69388855\n",
      "Step: [14388] d_loss: 1.38621688, g_loss: 0.69364208\n",
      "Step: [14389] d_loss: 1.38645673, g_loss: 0.69334269\n",
      "Step: [14390] d_loss: 1.38634658, g_loss: 0.69061399\n",
      "Step: [14391] d_loss: 1.38619518, g_loss: 0.69199222\n",
      "Step: [14392] d_loss: 1.38628149, g_loss: 0.69336772\n",
      "Step: [14393] d_loss: 1.38625467, g_loss: 0.69577938\n",
      "Step: [14394] d_loss: 1.38624752, g_loss: 0.69399393\n",
      "Step: [14395] d_loss: 1.38646603, g_loss: 0.68991333\n",
      "Step: [14396] d_loss: 1.38646328, g_loss: 0.69353163\n",
      "Step: [14397] d_loss: 1.38645148, g_loss: 0.69532955\n",
      "Step: [14398] d_loss: 1.38629735, g_loss: 0.69303048\n",
      "Step: [14399] d_loss: 1.38632333, g_loss: 0.69189656\n",
      "Step: [14400] d_loss: 1.38656247, g_loss: 0.69224113\n",
      "Step: [14401] d_loss: 1.38611221, g_loss: 0.69163001\n",
      "Step: [14402] d_loss: 1.38655388, g_loss: 0.69530284\n",
      "Step: [14403] d_loss: 1.38620090, g_loss: 0.69407773\n",
      "Step: [14404] d_loss: 1.38664675, g_loss: 0.69432473\n",
      "Step: [14405] d_loss: 1.38630831, g_loss: 0.68992943\n",
      "Step: [14406] d_loss: 1.38647389, g_loss: 0.69129574\n",
      "Step: [14407] d_loss: 1.38676310, g_loss: 0.69067526\n",
      "Step: [14408] d_loss: 1.38672149, g_loss: 0.69420260\n",
      "Step: [14409] d_loss: 1.38646650, g_loss: 0.69518125\n",
      "Step: [14410] d_loss: 1.38640308, g_loss: 0.69465911\n",
      "Step: [14411] d_loss: 1.38622689, g_loss: 0.69379044\n",
      "Step: [14412] d_loss: 1.38628972, g_loss: 0.69322419\n",
      "Step: [14413] d_loss: 1.38648105, g_loss: 0.69269025\n",
      "Step: [14414] d_loss: 1.38619685, g_loss: 0.69285262\n",
      "Step: [14415] d_loss: 1.38632858, g_loss: 0.69314182\n",
      "Step: [14416] d_loss: 1.38619184, g_loss: 0.69338119\n",
      "Step: [14417] d_loss: 1.38624263, g_loss: 0.69311011\n",
      "Step: [14418] d_loss: 1.38634121, g_loss: 0.69137836\n",
      "Step: [14419] d_loss: 1.38666892, g_loss: 0.69312745\n",
      "Step: [14420] d_loss: 1.38618326, g_loss: 0.69332111\n",
      "Step: [14421] d_loss: 1.38628697, g_loss: 0.69293064\n",
      "Step: [14422] d_loss: 1.38636792, g_loss: 0.69474196\n",
      "Step: [14423] d_loss: 1.38621712, g_loss: 0.69235837\n",
      "Step: [14424] d_loss: 1.38619661, g_loss: 0.69199681\n",
      "Step: [14425] d_loss: 1.38633871, g_loss: 0.69313699\n",
      "Step: [14426] d_loss: 1.38609588, g_loss: 0.69417721\n",
      "Step: [14427] d_loss: 1.38658500, g_loss: 0.69258744\n",
      "Step: [14428] d_loss: 1.38658059, g_loss: 0.69455898\n",
      "Step: [14429] d_loss: 1.38654530, g_loss: 0.69331992\n",
      "Step: [14430] d_loss: 1.38647628, g_loss: 0.69398236\n",
      "Step: [14431] d_loss: 1.38628173, g_loss: 0.69326651\n",
      "Step: [14432] d_loss: 1.38634324, g_loss: 0.69317836\n",
      "Step: [14433] d_loss: 1.38617015, g_loss: 0.69417608\n",
      "Step: [14434] d_loss: 1.38616550, g_loss: 0.69266617\n",
      "Step: [14435] d_loss: 1.38618493, g_loss: 0.69277763\n",
      "Step: [14436] d_loss: 1.38621926, g_loss: 0.69313669\n",
      "Step: [14437] d_loss: 1.38616478, g_loss: 0.69198489\n",
      "Step: [14438] d_loss: 1.38618052, g_loss: 0.69368279\n",
      "Step: [14439] d_loss: 1.38640571, g_loss: 0.69184613\n",
      "Step: [14440] d_loss: 1.38642430, g_loss: 0.69432467\n",
      "Step: [14441] d_loss: 1.38632214, g_loss: 0.69298816\n",
      "Step: [14442] d_loss: 1.38633335, g_loss: 0.69334483\n",
      "Step: [14443] d_loss: 1.38620138, g_loss: 0.69442821\n",
      "Step: [14444] d_loss: 1.38621068, g_loss: 0.69405621\n",
      "Step: [14445] d_loss: 1.38648272, g_loss: 0.69156700\n",
      "Step: [14446] d_loss: 1.38642454, g_loss: 0.69215298\n",
      "Step: [14447] d_loss: 1.38635504, g_loss: 0.69193769\n",
      "Step: [14448] d_loss: 1.38667774, g_loss: 0.69387317\n",
      "Step: [14449] d_loss: 1.38657582, g_loss: 0.69128764\n",
      "Step: [14450] d_loss: 1.38678598, g_loss: 0.69412065\n",
      "Step: [14451] d_loss: 1.38678110, g_loss: 0.69458222\n",
      "Step: [14452] d_loss: 1.38635206, g_loss: 0.69482255\n",
      "Step: [14453] d_loss: 1.38624144, g_loss: 0.69314635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14454] d_loss: 1.38604176, g_loss: 0.69345027\n",
      "Step: [14455] d_loss: 1.38637757, g_loss: 0.69366473\n",
      "Step: [14456] d_loss: 1.38642168, g_loss: 0.69346130\n",
      "Step: [14457] d_loss: 1.38622928, g_loss: 0.69260126\n",
      "Step: [14458] d_loss: 1.38632798, g_loss: 0.69301724\n",
      "Step: [14459] d_loss: 1.38612711, g_loss: 0.69370097\n",
      "Step: [14460] d_loss: 1.38621855, g_loss: 0.69344974\n",
      "Step: [14461] d_loss: 1.38626075, g_loss: 0.69080883\n",
      "Step: [14462] d_loss: 1.38645601, g_loss: 0.69599712\n",
      "Step: [14463] d_loss: 1.38708162, g_loss: 0.69465560\n",
      "Step: [14464] d_loss: 1.38784802, g_loss: 0.69699645\n",
      "Step: [14465] d_loss: 1.38890290, g_loss: 0.69182706\n",
      "Step: [14466] d_loss: 1.38711178, g_loss: 0.68913126\n",
      "Step: [14467] d_loss: 1.38650286, g_loss: 0.69185233\n",
      "Step: [14468] d_loss: 1.38638520, g_loss: 0.69385970\n",
      "Step: [14469] d_loss: 1.38646340, g_loss: 0.69397748\n",
      "Step: [14470] d_loss: 1.38635874, g_loss: 0.69027841\n",
      "Step: [14471] d_loss: 1.38649821, g_loss: 0.69338846\n",
      "Step: [14472] d_loss: 1.38607430, g_loss: 0.69367296\n",
      "Step: [14473] d_loss: 1.38615108, g_loss: 0.69112360\n",
      "Step: [14474] d_loss: 1.38669372, g_loss: 0.69489110\n",
      "Step: [14475] d_loss: 1.38720727, g_loss: 0.69071382\n",
      "Step: [14476] d_loss: 1.38742566, g_loss: 0.69463909\n",
      "Step: [14477] d_loss: 1.38721442, g_loss: 0.69240880\n",
      "Step: [14478] d_loss: 1.38705420, g_loss: 0.69520259\n",
      "Step: [14479] d_loss: 1.38657129, g_loss: 0.69552803\n",
      "Step: [14480] d_loss: 1.38630188, g_loss: 0.69314694\n",
      "Step: [14481] d_loss: 1.38632274, g_loss: 0.69061565\n",
      "Step: [14482] d_loss: 1.38654995, g_loss: 0.69114709\n",
      "Step: [14483] d_loss: 1.38708246, g_loss: 0.69602776\n",
      "Step: [14484] d_loss: 1.38693345, g_loss: 0.69701618\n",
      "Step: [14485] d_loss: 1.38655639, g_loss: 0.69717479\n",
      "Step: [14486] d_loss: 1.38653386, g_loss: 0.69312328\n",
      "Step: [14487] d_loss: 1.38706136, g_loss: 0.69354695\n",
      "Step: [14488] d_loss: 1.38729572, g_loss: 0.69102454\n",
      "Step: [14489] d_loss: 1.38750756, g_loss: 0.69229341\n",
      "Step: [14490] d_loss: 1.38725162, g_loss: 0.69245255\n",
      "Step: [14491] d_loss: 1.38716996, g_loss: 0.69691795\n",
      "Step: [14492] d_loss: 1.38640332, g_loss: 0.69675368\n",
      "Step: [14493] d_loss: 1.38645518, g_loss: 0.69195044\n",
      "Step: [14494] d_loss: 1.38608599, g_loss: 0.68878019\n",
      "Step: [14495] d_loss: 1.38627386, g_loss: 0.69221711\n",
      "Step: [14496] d_loss: 1.38642418, g_loss: 0.69267845\n",
      "Step: [14497] d_loss: 1.38639498, g_loss: 0.69581246\n",
      "Step: [14498] d_loss: 1.38629603, g_loss: 0.69511408\n",
      "Step: [14499] d_loss: 1.38677073, g_loss: 0.69309551\n",
      "Step: [14500] d_loss: 1.38680100, g_loss: 0.69338220\n",
      "Step: [14501] d_loss: 1.38667488, g_loss: 0.69291270\n",
      "Step: [14502] d_loss: 1.38670027, g_loss: 0.69112968\n",
      "Step: [14503] d_loss: 1.38699365, g_loss: 0.68880475\n",
      "Step: [14504] d_loss: 1.38638067, g_loss: 0.69101912\n",
      "Step: [14505] d_loss: 1.38647175, g_loss: 0.69320041\n",
      "Step: [14506] d_loss: 1.38634956, g_loss: 0.69496071\n",
      "Step: [14507] d_loss: 1.38625741, g_loss: 0.69428098\n",
      "Step: [14508] d_loss: 1.38633716, g_loss: 0.69328809\n",
      "Step: [14509] d_loss: 1.38646293, g_loss: 0.69254178\n",
      "Step: [14510] d_loss: 1.38731074, g_loss: 0.69307518\n",
      "Step: [14511] d_loss: 1.38735890, g_loss: 0.68950295\n",
      "Step: [14512] d_loss: 1.38714790, g_loss: 0.69336617\n",
      "Step: [14513] d_loss: 1.38663971, g_loss: 0.69586039\n",
      "Step: [14514] d_loss: 1.38606215, g_loss: 0.69701523\n",
      "Step: [14515] d_loss: 1.38603950, g_loss: 0.69462287\n",
      "Step: [14516] d_loss: 1.38633418, g_loss: 0.69319820\n",
      "Step: [14517] d_loss: 1.38621521, g_loss: 0.69255900\n",
      "Step: [14518] d_loss: 1.38630319, g_loss: 0.69353169\n",
      "Step: [14519] d_loss: 1.38614225, g_loss: 0.69371891\n",
      "Step: [14520] d_loss: 1.38652372, g_loss: 0.69209576\n",
      "Step: [14521] d_loss: 1.38651705, g_loss: 0.69358075\n",
      "Step: [14522] d_loss: 1.38671279, g_loss: 0.69081521\n",
      "Step: [14523] d_loss: 1.38691497, g_loss: 0.69171351\n",
      "Step: [14524] d_loss: 1.38640845, g_loss: 0.69276553\n",
      "Step: [14525] d_loss: 1.38663340, g_loss: 0.69494396\n",
      "Step: [14526] d_loss: 1.38654661, g_loss: 0.69588876\n",
      "Step: [14527] d_loss: 1.38643336, g_loss: 0.69369721\n",
      "Step: [14528] d_loss: 1.38641405, g_loss: 0.69295073\n",
      "Step: [14529] d_loss: 1.38625431, g_loss: 0.69255829\n",
      "Step: [14530] d_loss: 1.38625169, g_loss: 0.69246489\n",
      "Step: [14531] d_loss: 1.38625562, g_loss: 0.69317311\n",
      "Step: [14532] d_loss: 1.38621116, g_loss: 0.69349170\n",
      "Step: [14533] d_loss: 1.38627338, g_loss: 0.69351941\n",
      "Step: [14534] d_loss: 1.38625908, g_loss: 0.69324511\n",
      "Step: [14535] d_loss: 1.38616824, g_loss: 0.69320869\n",
      "Step: [14536] d_loss: 1.38629174, g_loss: 0.69299316\n",
      "Step: [14537] d_loss: 1.38618779, g_loss: 0.69321311\n",
      "Step: [14538] d_loss: 1.38623095, g_loss: 0.69315445\n",
      "Step: [14539] d_loss: 1.38629341, g_loss: 0.69304061\n",
      "Step: [14540] d_loss: 1.38625574, g_loss: 0.69323695\n",
      "Step: [14541] d_loss: 1.38632619, g_loss: 0.69304842\n",
      "Step: [14542] d_loss: 1.38628340, g_loss: 0.69318748\n",
      "Step: [14543] d_loss: 1.38619208, g_loss: 0.69340461\n",
      "Step: [14544] d_loss: 1.38605714, g_loss: 0.69292462\n",
      "Step: [14545] d_loss: 1.38633728, g_loss: 0.69303024\n",
      "Step: [14546] d_loss: 1.38619554, g_loss: 0.69287241\n",
      "Step: [14547] d_loss: 1.38641536, g_loss: 0.69332433\n",
      "Step: [14548] d_loss: 1.38636398, g_loss: 0.69301659\n",
      "Step: [14549] d_loss: 1.38631213, g_loss: 0.69346046\n",
      "Step: [14550] d_loss: 1.38637900, g_loss: 0.69323379\n",
      "Step: [14551] d_loss: 1.38617063, g_loss: 0.69325703\n",
      "Step: [14552] d_loss: 1.38627529, g_loss: 0.69319212\n",
      "Step: [14553] d_loss: 1.38628983, g_loss: 0.69319135\n",
      "Step: [14554] d_loss: 1.38625371, g_loss: 0.69306993\n",
      "Step: [14555] d_loss: 1.38621354, g_loss: 0.69315141\n",
      "Step: [14556] d_loss: 1.38630497, g_loss: 0.69316417\n",
      "Step: [14557] d_loss: 1.38629985, g_loss: 0.69315720\n",
      "Step: [14558] d_loss: 1.38633370, g_loss: 0.69302225\n",
      "Step: [14559] d_loss: 1.38626122, g_loss: 0.69341207\n",
      "Step: [14560] d_loss: 1.38628149, g_loss: 0.69266284\n",
      "Step: [14561] d_loss: 1.38620400, g_loss: 0.69305342\n",
      "Step: [14562] d_loss: 1.38638687, g_loss: 0.69246840\n",
      "Step: [14563] d_loss: 1.38652921, g_loss: 0.69468451\n",
      "Step: [14564] d_loss: 1.38644314, g_loss: 0.69349682\n",
      "Step: [14565] d_loss: 1.38632727, g_loss: 0.69267422\n",
      "Step: [14566] d_loss: 1.38633156, g_loss: 0.69199491\n",
      "Step: [14567] d_loss: 1.38631678, g_loss: 0.69284040\n",
      "Step: [14568] d_loss: 1.38644707, g_loss: 0.69426072\n",
      "Step: [14569] d_loss: 1.38652372, g_loss: 0.69371027\n",
      "Step: [14570] d_loss: 1.38639081, g_loss: 0.69489306\n",
      "Step: [14571] d_loss: 1.38627350, g_loss: 0.69387358\n",
      "Step: [14572] d_loss: 1.38658237, g_loss: 0.69118363\n",
      "Step: [14573] d_loss: 1.38700247, g_loss: 0.69557375\n",
      "Step: [14574] d_loss: 1.38736618, g_loss: 0.69429147\n",
      "Step: [14575] d_loss: 1.38799834, g_loss: 0.69348609\n",
      "Step: [14576] d_loss: 1.38752294, g_loss: 0.69302356\n",
      "Step: [14577] d_loss: 1.38681138, g_loss: 0.69547832\n",
      "Step: [14578] d_loss: 1.38634980, g_loss: 0.69323480\n",
      "Step: [14579] d_loss: 1.38598824, g_loss: 0.69445956\n",
      "Step: [14580] d_loss: 1.38617253, g_loss: 0.69247907\n",
      "Step: [14581] d_loss: 1.38618565, g_loss: 0.69273752\n",
      "Step: [14582] d_loss: 1.38626266, g_loss: 0.69415641\n",
      "Step: [14583] d_loss: 1.38615704, g_loss: 0.69291383\n",
      "Step: [14584] d_loss: 1.38649893, g_loss: 0.69522953\n",
      "Step: [14585] d_loss: 1.38665020, g_loss: 0.69298983\n",
      "Step: [14586] d_loss: 1.38669157, g_loss: 0.69345862\n",
      "Step: [14587] d_loss: 1.38653612, g_loss: 0.69158494\n",
      "Step: [14588] d_loss: 1.38616729, g_loss: 0.69187784\n",
      "Step: [14589] d_loss: 1.38627863, g_loss: 0.69276607\n",
      "Step: [14590] d_loss: 1.38620365, g_loss: 0.69384825\n",
      "Step: [14591] d_loss: 1.38623047, g_loss: 0.69355112\n",
      "Step: [14592] d_loss: 1.38633668, g_loss: 0.69223207\n",
      "Step: [14593] d_loss: 1.38604021, g_loss: 0.69223052\n",
      "Step: [14594] d_loss: 1.38600993, g_loss: 0.69416499\n",
      "Step: [14595] d_loss: 1.38649511, g_loss: 0.69393665\n",
      "Step: [14596] d_loss: 1.38618708, g_loss: 0.69259739\n",
      "Step: [14597] d_loss: 1.38643789, g_loss: 0.69284213\n",
      "Step: [14598] d_loss: 1.38656521, g_loss: 0.69319129\n",
      "Step: [14599] d_loss: 1.38645554, g_loss: 0.69245613\n",
      "Step: [14600] d_loss: 1.38638330, g_loss: 0.69302571\n",
      "Step: [14601] d_loss: 1.38645053, g_loss: 0.69294125\n",
      "Step: [14602] d_loss: 1.38652825, g_loss: 0.69378245\n",
      "Step: [14603] d_loss: 1.38667011, g_loss: 0.69241428\n",
      "Step: [14604] d_loss: 1.38659382, g_loss: 0.69395697\n",
      "Step: [14605] d_loss: 1.38637161, g_loss: 0.69513404\n",
      "Step: [14606] d_loss: 1.38637865, g_loss: 0.69466650\n",
      "Step: [14607] d_loss: 1.38657427, g_loss: 0.69411600\n",
      "Step: [14608] d_loss: 1.38627625, g_loss: 0.69242585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14609] d_loss: 1.38640857, g_loss: 0.69284749\n",
      "Step: [14610] d_loss: 1.38660467, g_loss: 0.69363999\n",
      "Step: [14611] d_loss: 1.38722968, g_loss: 0.68897521\n",
      "Step: [14612] d_loss: 1.38868213, g_loss: 0.68849355\n",
      "Step: [14613] d_loss: 1.38940072, g_loss: 0.69636309\n",
      "Step: [14614] d_loss: 1.38873827, g_loss: 0.69628465\n",
      "Step: [14615] d_loss: 1.38692331, g_loss: 0.69691610\n",
      "Step: [14616] d_loss: 1.38622034, g_loss: 0.69488549\n",
      "Step: [14617] d_loss: 1.38626671, g_loss: 0.69206405\n",
      "Step: [14618] d_loss: 1.38654852, g_loss: 0.69304943\n",
      "Step: [14619] d_loss: 1.38622546, g_loss: 0.69264162\n",
      "Step: [14620] d_loss: 1.38632381, g_loss: 0.69338512\n",
      "Step: [14621] d_loss: 1.38647032, g_loss: 0.69342542\n",
      "Step: [14622] d_loss: 1.38630080, g_loss: 0.69333774\n",
      "Step: [14623] d_loss: 1.38628852, g_loss: 0.69304013\n",
      "Step: [14624] d_loss: 1.38637209, g_loss: 0.69301814\n",
      "Step: [14625] d_loss: 1.38630772, g_loss: 0.69279218\n",
      "Step: [14626] d_loss: 1.38632679, g_loss: 0.69331872\n",
      "Step: [14627] d_loss: 1.38618445, g_loss: 0.69406366\n",
      "Step: [14628] d_loss: 1.38627017, g_loss: 0.69317377\n",
      "Step: [14629] d_loss: 1.38635015, g_loss: 0.69224226\n",
      "Step: [14630] d_loss: 1.38613415, g_loss: 0.69253033\n",
      "Step: [14631] d_loss: 1.38627601, g_loss: 0.69266832\n",
      "Step: [14632] d_loss: 1.38616300, g_loss: 0.69336033\n",
      "Step: [14633] d_loss: 1.38630199, g_loss: 0.69348866\n",
      "Step: [14634] d_loss: 1.38640165, g_loss: 0.69318581\n",
      "Step: [14635] d_loss: 1.38633907, g_loss: 0.69290686\n",
      "Step: [14636] d_loss: 1.38641143, g_loss: 0.69316435\n",
      "Step: [14637] d_loss: 1.38632441, g_loss: 0.69354832\n",
      "Step: [14638] d_loss: 1.38627386, g_loss: 0.69319314\n",
      "Step: [14639] d_loss: 1.38653421, g_loss: 0.69369727\n",
      "Step: [14640] d_loss: 1.38642645, g_loss: 0.69184077\n",
      "Step: [14641] d_loss: 1.38626277, g_loss: 0.69348991\n",
      "Step: [14642] d_loss: 1.38612282, g_loss: 0.69357806\n",
      "Step: [14643] d_loss: 1.38609886, g_loss: 0.69378060\n",
      "Step: [14644] d_loss: 1.38609385, g_loss: 0.69357103\n",
      "Step: [14645] d_loss: 1.38620985, g_loss: 0.69266784\n",
      "Step: [14646] d_loss: 1.38612068, g_loss: 0.69238257\n",
      "Step: [14647] d_loss: 1.38622332, g_loss: 0.69265878\n",
      "Step: [14648] d_loss: 1.38631475, g_loss: 0.69319642\n",
      "Step: [14649] d_loss: 1.38615847, g_loss: 0.69364226\n",
      "Step: [14650] d_loss: 1.38643670, g_loss: 0.69349658\n",
      "Step: [14651] d_loss: 1.38628936, g_loss: 0.69313967\n",
      "Step: [14652] d_loss: 1.38629580, g_loss: 0.69294941\n",
      "Step: [14653] d_loss: 1.38624692, g_loss: 0.69312698\n",
      "Step: [14654] d_loss: 1.38626397, g_loss: 0.69326288\n",
      "Step: [14655] d_loss: 1.38623428, g_loss: 0.69328755\n",
      "Step: [14656] d_loss: 1.38628602, g_loss: 0.69297171\n",
      "Step: [14657] d_loss: 1.38634622, g_loss: 0.69308102\n",
      "Step: [14658] d_loss: 1.38635278, g_loss: 0.69361192\n",
      "Step: [14659] d_loss: 1.38626289, g_loss: 0.69335818\n",
      "Step: [14660] d_loss: 1.38638783, g_loss: 0.69271147\n",
      "Step: [14661] d_loss: 1.38633037, g_loss: 0.69265246\n",
      "Step: [14662] d_loss: 1.38632894, g_loss: 0.69315499\n",
      "Step: [14663] d_loss: 1.38641095, g_loss: 0.69330502\n",
      "Step: [14664] d_loss: 1.38640618, g_loss: 0.69346368\n",
      "Step: [14665] d_loss: 1.38621020, g_loss: 0.69314045\n",
      "Step: [14666] d_loss: 1.38634551, g_loss: 0.69303000\n",
      "Step: [14667] d_loss: 1.38635921, g_loss: 0.69318545\n",
      "Step: [14668] d_loss: 1.38623905, g_loss: 0.69322324\n",
      "Step: [14669] d_loss: 1.38622952, g_loss: 0.69344783\n",
      "Step: [14670] d_loss: 1.38637590, g_loss: 0.69287366\n",
      "Step: [14671] d_loss: 1.38632703, g_loss: 0.69359976\n",
      "Step: [14672] d_loss: 1.38628983, g_loss: 0.69374526\n",
      "Step: [14673] d_loss: 1.38649344, g_loss: 0.69361627\n",
      "Step: [14674] d_loss: 1.38642430, g_loss: 0.69246674\n",
      "Step: [14675] d_loss: 1.38633943, g_loss: 0.69444549\n",
      "Step: [14676] d_loss: 1.38640237, g_loss: 0.69339776\n",
      "Step: [14677] d_loss: 1.38634789, g_loss: 0.69238722\n",
      "Step: [14678] d_loss: 1.38611341, g_loss: 0.69275832\n",
      "Step: [14679] d_loss: 1.38630438, g_loss: 0.69388771\n",
      "Step: [14680] d_loss: 1.38631558, g_loss: 0.69337571\n",
      "Step: [14681] d_loss: 1.38632226, g_loss: 0.69356173\n",
      "Step: [14682] d_loss: 1.38634825, g_loss: 0.69473016\n",
      "Step: [14683] d_loss: 1.38646460, g_loss: 0.69305396\n",
      "Step: [14684] d_loss: 1.38626766, g_loss: 0.69275522\n",
      "Step: [14685] d_loss: 1.38609672, g_loss: 0.69354117\n",
      "Step: [14686] d_loss: 1.38626516, g_loss: 0.69318891\n",
      "Step: [14687] d_loss: 1.38619792, g_loss: 0.69321197\n",
      "Step: [14688] d_loss: 1.38620996, g_loss: 0.69274950\n",
      "Step: [14689] d_loss: 1.38645506, g_loss: 0.69325602\n",
      "Step: [14690] d_loss: 1.38698530, g_loss: 0.69456035\n",
      "Step: [14691] d_loss: 1.38666093, g_loss: 0.69416869\n",
      "Step: [14692] d_loss: 1.38635635, g_loss: 0.69367689\n",
      "Step: [14693] d_loss: 1.38649106, g_loss: 0.69341779\n",
      "Step: [14694] d_loss: 1.38629222, g_loss: 0.69236481\n",
      "Step: [14695] d_loss: 1.38625729, g_loss: 0.69295990\n",
      "Step: [14696] d_loss: 1.38623405, g_loss: 0.69347185\n",
      "Step: [14697] d_loss: 1.38627911, g_loss: 0.69326812\n",
      "Step: [14698] d_loss: 1.38629162, g_loss: 0.69327879\n",
      "Step: [14699] d_loss: 1.38643122, g_loss: 0.69394958\n",
      "Step: [14700] d_loss: 1.38625860, g_loss: 0.69294721\n",
      "Step: [14701] d_loss: 1.38602710, g_loss: 0.69410372\n",
      "Step: [14702] d_loss: 1.38624859, g_loss: 0.69367433\n",
      "Step: [14703] d_loss: 1.38640857, g_loss: 0.69345289\n",
      "Step: [14704] d_loss: 1.38629413, g_loss: 0.69241273\n",
      "Step: [14705] d_loss: 1.38649213, g_loss: 0.69206691\n",
      "Step: [14706] d_loss: 1.38653398, g_loss: 0.69452775\n",
      "Step: [14707] d_loss: 1.38655615, g_loss: 0.69350088\n",
      "Step: [14708] d_loss: 1.38631892, g_loss: 0.69338071\n",
      "Step: [14709] d_loss: 1.38623953, g_loss: 0.69328213\n",
      "Step: [14710] d_loss: 1.38621020, g_loss: 0.69341338\n",
      "Step: [14711] d_loss: 1.38622427, g_loss: 0.69341940\n",
      "Step: [14712] d_loss: 1.38625538, g_loss: 0.69328690\n",
      "Step: [14713] d_loss: 1.38635778, g_loss: 0.69370407\n",
      "Step: [14714] d_loss: 1.38624048, g_loss: 0.69265819\n",
      "Step: [14715] d_loss: 1.38629711, g_loss: 0.69311523\n",
      "Step: [14716] d_loss: 1.38613164, g_loss: 0.69377983\n",
      "Step: [14717] d_loss: 1.38616109, g_loss: 0.69329756\n",
      "Step: [14718] d_loss: 1.38613951, g_loss: 0.69341624\n",
      "Step: [14719] d_loss: 1.38618398, g_loss: 0.69395494\n",
      "Step: [14720] d_loss: 1.38632131, g_loss: 0.69417644\n",
      "Step: [14721] d_loss: 1.38720798, g_loss: 0.69116300\n",
      "Step: [14722] d_loss: 1.38818932, g_loss: 0.68962550\n",
      "Step: [14723] d_loss: 1.38872814, g_loss: 0.69000280\n",
      "Step: [14724] d_loss: 1.38828933, g_loss: 0.69687128\n",
      "Step: [14725] d_loss: 1.38706589, g_loss: 0.69495118\n",
      "Step: [14726] d_loss: 1.38621616, g_loss: 0.69193232\n",
      "Step: [14727] d_loss: 1.38628888, g_loss: 0.69204164\n",
      "Step: [14728] d_loss: 1.38618934, g_loss: 0.69248903\n",
      "Step: [14729] d_loss: 1.38637877, g_loss: 0.69429517\n",
      "Step: [14730] d_loss: 1.38622832, g_loss: 0.69344127\n",
      "Step: [14731] d_loss: 1.38640571, g_loss: 0.69394386\n",
      "Step: [14732] d_loss: 1.38634586, g_loss: 0.69418430\n",
      "Step: [14733] d_loss: 1.38630772, g_loss: 0.69206965\n",
      "Step: [14734] d_loss: 1.38631654, g_loss: 0.69387507\n",
      "Step: [14735] d_loss: 1.38638425, g_loss: 0.69375348\n",
      "Step: [14736] d_loss: 1.38615894, g_loss: 0.69226253\n",
      "Step: [14737] d_loss: 1.38627958, g_loss: 0.69343889\n",
      "Step: [14738] d_loss: 1.38614702, g_loss: 0.69308698\n",
      "Step: [14739] d_loss: 1.38627458, g_loss: 0.69344348\n",
      "Step: [14740] d_loss: 1.38617373, g_loss: 0.69441664\n",
      "Step: [14741] d_loss: 1.38643956, g_loss: 0.69392562\n",
      "Step: [14742] d_loss: 1.38622510, g_loss: 0.69203627\n",
      "Step: [14743] d_loss: 1.38644052, g_loss: 0.69185448\n",
      "Step: [14744] d_loss: 1.38637257, g_loss: 0.69291270\n",
      "Step: [14745] d_loss: 1.38614058, g_loss: 0.69338632\n",
      "Step: [14746] d_loss: 1.38639307, g_loss: 0.69392216\n",
      "Step: [14747] d_loss: 1.38628078, g_loss: 0.69439709\n",
      "Step: [14748] d_loss: 1.38627601, g_loss: 0.69295853\n",
      "Step: [14749] d_loss: 1.38632560, g_loss: 0.69274783\n",
      "Step: [14750] d_loss: 1.38629448, g_loss: 0.69316232\n",
      "Step: [14751] d_loss: 1.38626337, g_loss: 0.69297576\n",
      "Step: [14752] d_loss: 1.38630748, g_loss: 0.69303077\n",
      "Step: [14753] d_loss: 1.38632035, g_loss: 0.69398808\n",
      "Step: [14754] d_loss: 1.38641405, g_loss: 0.69301075\n",
      "Step: [14755] d_loss: 1.38631332, g_loss: 0.69310141\n",
      "Step: [14756] d_loss: 1.38619387, g_loss: 0.69333398\n",
      "Step: [14757] d_loss: 1.38633645, g_loss: 0.69273454\n",
      "Step: [14758] d_loss: 1.38630676, g_loss: 0.69303203\n",
      "Step: [14759] d_loss: 1.38633716, g_loss: 0.69329202\n",
      "Step: [14760] d_loss: 1.38627899, g_loss: 0.69416022\n",
      "Step: [14761] d_loss: 1.38634861, g_loss: 0.69334400\n",
      "Step: [14762] d_loss: 1.38634896, g_loss: 0.69317257\n",
      "Step: [14763] d_loss: 1.38634324, g_loss: 0.69310224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14764] d_loss: 1.38629842, g_loss: 0.69259894\n",
      "Step: [14765] d_loss: 1.38635743, g_loss: 0.69289327\n",
      "Step: [14766] d_loss: 1.38640869, g_loss: 0.69459844\n",
      "Step: [14767] d_loss: 1.38662744, g_loss: 0.69277120\n",
      "Step: [14768] d_loss: 1.38699150, g_loss: 0.69294167\n",
      "Step: [14769] d_loss: 1.38660407, g_loss: 0.69160438\n",
      "Step: [14770] d_loss: 1.38644195, g_loss: 0.69228297\n",
      "Step: [14771] d_loss: 1.38623166, g_loss: 0.69340938\n",
      "Step: [14772] d_loss: 1.38636351, g_loss: 0.69382918\n",
      "Step: [14773] d_loss: 1.38624513, g_loss: 0.69365847\n",
      "Step: [14774] d_loss: 1.38627148, g_loss: 0.69342411\n",
      "Step: [14775] d_loss: 1.38622355, g_loss: 0.69258082\n",
      "Step: [14776] d_loss: 1.38644981, g_loss: 0.69391161\n",
      "Step: [14777] d_loss: 1.38636589, g_loss: 0.69300222\n",
      "Step: [14778] d_loss: 1.38623786, g_loss: 0.69294381\n",
      "Step: [14779] d_loss: 1.38628697, g_loss: 0.69317889\n",
      "Step: [14780] d_loss: 1.38627028, g_loss: 0.69413567\n",
      "Step: [14781] d_loss: 1.38623309, g_loss: 0.69298577\n",
      "Step: [14782] d_loss: 1.38634086, g_loss: 0.69543922\n",
      "Step: [14783] d_loss: 1.38656592, g_loss: 0.69622141\n",
      "Step: [14784] d_loss: 1.38711357, g_loss: 0.69237339\n",
      "Step: [14785] d_loss: 1.38741207, g_loss: 0.69486916\n",
      "Step: [14786] d_loss: 1.38667083, g_loss: 0.69473368\n",
      "Step: [14787] d_loss: 1.38632393, g_loss: 0.69110411\n",
      "Step: [14788] d_loss: 1.38623047, g_loss: 0.69121814\n",
      "Step: [14789] d_loss: 1.38645840, g_loss: 0.69186509\n",
      "Step: [14790] d_loss: 1.38631701, g_loss: 0.69222593\n",
      "Step: [14791] d_loss: 1.38621759, g_loss: 0.69376051\n",
      "Step: [14792] d_loss: 1.38644361, g_loss: 0.69323885\n",
      "Step: [14793] d_loss: 1.38648188, g_loss: 0.69321692\n",
      "Step: [14794] d_loss: 1.38643062, g_loss: 0.69291973\n",
      "Step: [14795] d_loss: 1.38634086, g_loss: 0.69274694\n",
      "Step: [14796] d_loss: 1.38631821, g_loss: 0.69252074\n",
      "Step: [14797] d_loss: 1.38631129, g_loss: 0.69295108\n",
      "Step: [14798] d_loss: 1.38637006, g_loss: 0.69374496\n",
      "Step: [14799] d_loss: 1.38631189, g_loss: 0.69328767\n",
      "Step: [14800] d_loss: 1.38631797, g_loss: 0.69308293\n",
      "Step: [14801] d_loss: 1.38616419, g_loss: 0.69334966\n",
      "Step: [14802] d_loss: 1.38620877, g_loss: 0.69320500\n",
      "Step: [14803] d_loss: 1.38632500, g_loss: 0.69296235\n",
      "Step: [14804] d_loss: 1.38626122, g_loss: 0.69365293\n",
      "Step: [14805] d_loss: 1.38620746, g_loss: 0.69316417\n",
      "Step: [14806] d_loss: 1.38623130, g_loss: 0.69290525\n",
      "Step: [14807] d_loss: 1.38621902, g_loss: 0.69324958\n",
      "Step: [14808] d_loss: 1.38623405, g_loss: 0.69414967\n",
      "Step: [14809] d_loss: 1.38611913, g_loss: 0.69305378\n",
      "Step: [14810] d_loss: 1.38618505, g_loss: 0.69353801\n",
      "Step: [14811] d_loss: 1.38624823, g_loss: 0.69322836\n",
      "Step: [14812] d_loss: 1.38619578, g_loss: 0.69323802\n",
      "Step: [14813] d_loss: 1.38619256, g_loss: 0.69297719\n",
      "Step: [14814] d_loss: 1.38622200, g_loss: 0.69316006\n",
      "Step: [14815] d_loss: 1.38616121, g_loss: 0.69319367\n",
      "Step: [14816] d_loss: 1.38637030, g_loss: 0.69420713\n",
      "Step: [14817] d_loss: 1.38630068, g_loss: 0.69318360\n",
      "Step: [14818] d_loss: 1.38615131, g_loss: 0.69353139\n",
      "Step: [14819] d_loss: 1.38627648, g_loss: 0.69307977\n",
      "Step: [14820] d_loss: 1.38624132, g_loss: 0.69324422\n",
      "Step: [14821] d_loss: 1.38607097, g_loss: 0.69374108\n",
      "Step: [14822] d_loss: 1.38714087, g_loss: 0.69486535\n",
      "Step: [14823] d_loss: 1.38625753, g_loss: 0.69369948\n",
      "Step: [14824] d_loss: 1.38631082, g_loss: 0.69332546\n",
      "Step: [14825] d_loss: 1.38611722, g_loss: 0.69127184\n",
      "Step: [14826] d_loss: 1.38629007, g_loss: 0.69315279\n",
      "Step: [14827] d_loss: 1.38633728, g_loss: 0.69449341\n",
      "Step: [14828] d_loss: 1.38680243, g_loss: 0.69519275\n",
      "Step: [14829] d_loss: 1.38638616, g_loss: 0.69213772\n",
      "Step: [14830] d_loss: 1.38746881, g_loss: 0.69105059\n",
      "Step: [14831] d_loss: 1.38845158, g_loss: 0.69461375\n",
      "Step: [14832] d_loss: 1.38794303, g_loss: 0.69121969\n",
      "Step: [14833] d_loss: 1.38685346, g_loss: 0.69145715\n",
      "Step: [14834] d_loss: 1.38650179, g_loss: 0.69253647\n",
      "Step: [14835] d_loss: 1.38631761, g_loss: 0.69300592\n",
      "Step: [14836] d_loss: 1.38630009, g_loss: 0.69462061\n",
      "Step: [14837] d_loss: 1.38621068, g_loss: 0.69406426\n",
      "Step: [14838] d_loss: 1.38637495, g_loss: 0.69445920\n",
      "Step: [14839] d_loss: 1.38639808, g_loss: 0.69382870\n",
      "Step: [14840] d_loss: 1.38626933, g_loss: 0.69204623\n",
      "Step: [14841] d_loss: 1.38624620, g_loss: 0.69274265\n",
      "Step: [14842] d_loss: 1.38632011, g_loss: 0.69407713\n",
      "Step: [14843] d_loss: 1.38631129, g_loss: 0.69407320\n",
      "Step: [14844] d_loss: 1.38630140, g_loss: 0.69334877\n",
      "Step: [14845] d_loss: 1.38629675, g_loss: 0.69238794\n",
      "Step: [14846] d_loss: 1.38623857, g_loss: 0.69321179\n",
      "Step: [14847] d_loss: 1.38625836, g_loss: 0.69315034\n",
      "Step: [14848] d_loss: 1.38630366, g_loss: 0.69391608\n",
      "Step: [14849] d_loss: 1.38628125, g_loss: 0.69287074\n",
      "Step: [14850] d_loss: 1.38631129, g_loss: 0.69363523\n",
      "Step: [14851] d_loss: 1.38620961, g_loss: 0.69328630\n",
      "Step: [14852] d_loss: 1.38609385, g_loss: 0.69336474\n",
      "Step: [14853] d_loss: 1.38619506, g_loss: 0.69360626\n",
      "Step: [14854] d_loss: 1.38633049, g_loss: 0.69323462\n",
      "Step: [14855] d_loss: 1.38635540, g_loss: 0.69282341\n",
      "Step: [14856] d_loss: 1.38618183, g_loss: 0.69337308\n",
      "Step: [14857] d_loss: 1.38629091, g_loss: 0.69398022\n",
      "Step: [14858] d_loss: 1.38627291, g_loss: 0.69335866\n",
      "Step: [14859] d_loss: 1.38618565, g_loss: 0.69263828\n",
      "Step: [14860] d_loss: 1.38635612, g_loss: 0.69306135\n",
      "Step: [14861] d_loss: 1.38623714, g_loss: 0.69314742\n",
      "Step: [14862] d_loss: 1.38631272, g_loss: 0.69278914\n",
      "Step: [14863] d_loss: 1.38624740, g_loss: 0.69354081\n",
      "Step: [14864] d_loss: 1.38626671, g_loss: 0.69414079\n",
      "Step: [14865] d_loss: 1.38640666, g_loss: 0.69479030\n",
      "Step: [14866] d_loss: 1.38654101, g_loss: 0.69281161\n",
      "Step: [14867] d_loss: 1.38637614, g_loss: 0.69250143\n",
      "Step: [14868] d_loss: 1.38624096, g_loss: 0.69346762\n",
      "Step: [14869] d_loss: 1.38674462, g_loss: 0.69486868\n",
      "Step: [14870] d_loss: 1.38712049, g_loss: 0.69183350\n",
      "Step: [14871] d_loss: 1.38702834, g_loss: 0.69428027\n",
      "Step: [14872] d_loss: 1.38656664, g_loss: 0.69413590\n",
      "Step: [14873] d_loss: 1.38636994, g_loss: 0.69411528\n",
      "Step: [14874] d_loss: 1.38623786, g_loss: 0.69345027\n",
      "Step: [14875] d_loss: 1.38628685, g_loss: 0.69259918\n",
      "Step: [14876] d_loss: 1.38618076, g_loss: 0.69268072\n",
      "Step: [14877] d_loss: 1.38622975, g_loss: 0.69332206\n",
      "Step: [14878] d_loss: 1.38619137, g_loss: 0.69388771\n",
      "Step: [14879] d_loss: 1.38635206, g_loss: 0.69363272\n",
      "Step: [14880] d_loss: 1.38620424, g_loss: 0.69307303\n",
      "Step: [14881] d_loss: 1.38629436, g_loss: 0.69301748\n",
      "Step: [14882] d_loss: 1.38623333, g_loss: 0.69283497\n",
      "Step: [14883] d_loss: 1.38624620, g_loss: 0.69323820\n",
      "Step: [14884] d_loss: 1.38626289, g_loss: 0.69342428\n",
      "Step: [14885] d_loss: 1.38625121, g_loss: 0.69346154\n",
      "Step: [14886] d_loss: 1.38630080, g_loss: 0.69370472\n",
      "Step: [14887] d_loss: 1.38623500, g_loss: 0.69349700\n",
      "Step: [14888] d_loss: 1.38627982, g_loss: 0.69276768\n",
      "Step: [14889] d_loss: 1.38627422, g_loss: 0.69275469\n",
      "Step: [14890] d_loss: 1.38650119, g_loss: 0.69368094\n",
      "Step: [14891] d_loss: 1.38639069, g_loss: 0.69291639\n",
      "Step: [14892] d_loss: 1.38645673, g_loss: 0.69451809\n",
      "Step: [14893] d_loss: 1.38617408, g_loss: 0.69313139\n",
      "Step: [14894] d_loss: 1.38615072, g_loss: 0.69210267\n",
      "Step: [14895] d_loss: 1.38639081, g_loss: 0.69313645\n",
      "Step: [14896] d_loss: 1.38642752, g_loss: 0.69300735\n",
      "Step: [14897] d_loss: 1.38633204, g_loss: 0.69429898\n",
      "Step: [14898] d_loss: 1.38641167, g_loss: 0.69316077\n",
      "Step: [14899] d_loss: 1.38662839, g_loss: 0.69294131\n",
      "Step: [14900] d_loss: 1.38668478, g_loss: 0.69332767\n",
      "Step: [14901] d_loss: 1.38630295, g_loss: 0.69424903\n",
      "Step: [14902] d_loss: 1.38630557, g_loss: 0.69359839\n",
      "Step: [14903] d_loss: 1.38619828, g_loss: 0.69292444\n",
      "Step: [14904] d_loss: 1.38626361, g_loss: 0.69302261\n",
      "Step: [14905] d_loss: 1.38640547, g_loss: 0.69349205\n",
      "Step: [14906] d_loss: 1.38643217, g_loss: 0.69264472\n",
      "Step: [14907] d_loss: 1.38636220, g_loss: 0.69291198\n",
      "Step: [14908] d_loss: 1.38633394, g_loss: 0.69371355\n",
      "Step: [14909] d_loss: 1.38620853, g_loss: 0.69521034\n",
      "Step: [14910] d_loss: 1.38638735, g_loss: 0.69371510\n",
      "Step: [14911] d_loss: 1.38639343, g_loss: 0.69336236\n",
      "Step: [14912] d_loss: 1.38634598, g_loss: 0.69264996\n",
      "Step: [14913] d_loss: 1.38621366, g_loss: 0.69197965\n",
      "Step: [14914] d_loss: 1.38622141, g_loss: 0.69365370\n",
      "Step: [14915] d_loss: 1.38647747, g_loss: 0.69294131\n",
      "Step: [14916] d_loss: 1.38622022, g_loss: 0.69375497\n",
      "Step: [14917] d_loss: 1.38615155, g_loss: 0.69444990\n",
      "Step: [14918] d_loss: 1.38645482, g_loss: 0.69445133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [14919] d_loss: 1.38645554, g_loss: 0.69633448\n",
      "Step: [14920] d_loss: 1.38650227, g_loss: 0.69550937\n",
      "Step: [14921] d_loss: 1.38623488, g_loss: 0.69487005\n",
      "Step: [14922] d_loss: 1.38628137, g_loss: 0.69182289\n",
      "Step: [14923] d_loss: 1.38616848, g_loss: 0.69269466\n",
      "Step: [14924] d_loss: 1.38643706, g_loss: 0.69200724\n",
      "Step: [14925] d_loss: 1.38645685, g_loss: 0.69338489\n",
      "Step: [14926] d_loss: 1.38646984, g_loss: 0.69476295\n",
      "Step: [14927] d_loss: 1.38651407, g_loss: 0.69504589\n",
      "Step: [14928] d_loss: 1.38630474, g_loss: 0.69423580\n",
      "Step: [14929] d_loss: 1.38617253, g_loss: 0.69199502\n",
      "Step: [14930] d_loss: 1.38639307, g_loss: 0.69150960\n",
      "Step: [14931] d_loss: 1.38631022, g_loss: 0.69275665\n",
      "Step: [14932] d_loss: 1.38626981, g_loss: 0.69312799\n",
      "Step: [14933] d_loss: 1.38570595, g_loss: 0.69538772\n",
      "Step: [14934] d_loss: 1.38633895, g_loss: 0.69384897\n",
      "Step: [14935] d_loss: 1.38621521, g_loss: 0.69311434\n",
      "Step: [14936] d_loss: 1.38632190, g_loss: 0.69293320\n",
      "Step: [14937] d_loss: 1.38638771, g_loss: 0.69346023\n",
      "Step: [14938] d_loss: 1.38622844, g_loss: 0.69296151\n",
      "Step: [14939] d_loss: 1.38632607, g_loss: 0.69224966\n",
      "Step: [14940] d_loss: 1.38682497, g_loss: 0.69668937\n",
      "Step: [14941] d_loss: 1.38705981, g_loss: 0.69667149\n",
      "Step: [14942] d_loss: 1.38707471, g_loss: 0.69634724\n",
      "Step: [14943] d_loss: 1.38654745, g_loss: 0.69332480\n",
      "Step: [14944] d_loss: 1.38640141, g_loss: 0.69198298\n",
      "Step: [14945] d_loss: 1.38623655, g_loss: 0.69267797\n",
      "Step: [14946] d_loss: 1.38631296, g_loss: 0.69375932\n",
      "Step: [14947] d_loss: 1.38637638, g_loss: 0.69419616\n",
      "Step: [14948] d_loss: 1.38627410, g_loss: 0.69289553\n",
      "Step: [14949] d_loss: 1.38617873, g_loss: 0.69239366\n",
      "Step: [14950] d_loss: 1.38634229, g_loss: 0.69295311\n",
      "Step: [14951] d_loss: 1.38645661, g_loss: 0.69259971\n",
      "Step: [14952] d_loss: 1.38620794, g_loss: 0.69283617\n",
      "Step: [14953] d_loss: 1.38633227, g_loss: 0.69462758\n",
      "Step: [14954] d_loss: 1.38647699, g_loss: 0.69333398\n",
      "Step: [14955] d_loss: 1.38651252, g_loss: 0.69256020\n",
      "Step: [14956] d_loss: 1.38635111, g_loss: 0.69204712\n",
      "Step: [14957] d_loss: 1.38618422, g_loss: 0.69317389\n",
      "Step: [14958] d_loss: 1.38621044, g_loss: 0.69400251\n",
      "Step: [14959] d_loss: 1.38620472, g_loss: 0.69356084\n",
      "Step: [14960] d_loss: 1.38637042, g_loss: 0.69299448\n",
      "Step: [14961] d_loss: 1.38620818, g_loss: 0.69296324\n",
      "Step: [14962] d_loss: 1.38620448, g_loss: 0.69309986\n",
      "Step: [14963] d_loss: 1.38621819, g_loss: 0.69255900\n",
      "Step: [14964] d_loss: 1.38637900, g_loss: 0.69319975\n",
      "Step: [14965] d_loss: 1.38634443, g_loss: 0.69399118\n",
      "Step: [14966] d_loss: 1.38661575, g_loss: 0.69379210\n",
      "Step: [14967] d_loss: 1.38647664, g_loss: 0.69422489\n",
      "Step: [14968] d_loss: 1.38653445, g_loss: 0.69326746\n",
      "Step: [14969] d_loss: 1.38634515, g_loss: 0.69367027\n",
      "Step: [14970] d_loss: 1.38640547, g_loss: 0.69367456\n",
      "Step: [14971] d_loss: 1.38606286, g_loss: 0.69346189\n",
      "Step: [14972] d_loss: 1.38643527, g_loss: 0.69566905\n",
      "Step: [14973] d_loss: 1.38667202, g_loss: 0.69381213\n",
      "Step: [14974] d_loss: 1.38683784, g_loss: 0.69293237\n",
      "Step: [14975] d_loss: 1.38649833, g_loss: 0.69001412\n",
      "Step: [14976] d_loss: 1.38624811, g_loss: 0.69137394\n",
      "Step: [14977] d_loss: 1.38637829, g_loss: 0.69439739\n",
      "Step: [14978] d_loss: 1.38665366, g_loss: 0.69360155\n",
      "Step: [14979] d_loss: 1.38641691, g_loss: 0.69479722\n",
      "Step: [14980] d_loss: 1.38676262, g_loss: 0.69496024\n",
      "Step: [14981] d_loss: 1.38676023, g_loss: 0.69669908\n",
      "Step: [14982] d_loss: 1.38657832, g_loss: 0.69475007\n",
      "Step: [14983] d_loss: 1.38630044, g_loss: 0.69253528\n",
      "Step: [14984] d_loss: 1.38617992, g_loss: 0.69216424\n",
      "Step: [14985] d_loss: 1.38625312, g_loss: 0.69126189\n",
      "Step: [14986] d_loss: 1.38609624, g_loss: 0.69357753\n",
      "Step: [14987] d_loss: 1.38624573, g_loss: 0.69260943\n",
      "Step: [14988] d_loss: 1.38634813, g_loss: 0.69316679\n",
      "Step: [14989] d_loss: 1.38606596, g_loss: 0.69329798\n",
      "Step: [14990] d_loss: 1.38606071, g_loss: 0.69340289\n",
      "Step: [14991] d_loss: 1.38636947, g_loss: 0.69349670\n",
      "Step: [14992] d_loss: 1.38712144, g_loss: 0.69651985\n",
      "Step: [14993] d_loss: 1.38724828, g_loss: 0.69574666\n",
      "Step: [14994] d_loss: 1.38661444, g_loss: 0.69340473\n",
      "Step: [14995] d_loss: 1.38620245, g_loss: 0.69120336\n",
      "Step: [14996] d_loss: 1.38627839, g_loss: 0.69260436\n",
      "Step: [14997] d_loss: 1.38624358, g_loss: 0.69232309\n",
      "Step: [14998] d_loss: 1.38652027, g_loss: 0.69366723\n",
      "Step: [14999] d_loss: 1.38641882, g_loss: 0.69365168\n",
      "Step: [15000] d_loss: 1.38629055, g_loss: 0.69415367\n",
      "Step: [15001] d_loss: 1.38630378, g_loss: 0.69364560\n",
      "Step: [15002] d_loss: 1.38626695, g_loss: 0.69352823\n",
      "Step: [15003] d_loss: 1.38606608, g_loss: 0.69335639\n",
      "Step: [15004] d_loss: 1.38618088, g_loss: 0.69337857\n",
      "Step: [15005] d_loss: 1.38624740, g_loss: 0.69336760\n",
      "Step: [15006] d_loss: 1.38608718, g_loss: 0.69285566\n",
      "Step: [15007] d_loss: 1.38629127, g_loss: 0.69340885\n",
      "Step: [15008] d_loss: 1.38617456, g_loss: 0.69299954\n",
      "Step: [15009] d_loss: 1.38638687, g_loss: 0.69219625\n",
      "Step: [15010] d_loss: 1.38654280, g_loss: 0.69439143\n",
      "Step: [15011] d_loss: 1.38705683, g_loss: 0.69322330\n",
      "Step: [15012] d_loss: 1.38692749, g_loss: 0.69500697\n",
      "Step: [15013] d_loss: 1.38691044, g_loss: 0.69162512\n",
      "Step: [15014] d_loss: 1.38636982, g_loss: 0.69279134\n",
      "Step: [15015] d_loss: 1.38634443, g_loss: 0.69310015\n",
      "Step: [15016] d_loss: 1.38616323, g_loss: 0.69334495\n",
      "Step: [15017] d_loss: 1.38639915, g_loss: 0.69292098\n",
      "Step: [15018] d_loss: 1.38630676, g_loss: 0.69321650\n",
      "Step: [15019] d_loss: 1.38640857, g_loss: 0.69352865\n",
      "Step: [15020] d_loss: 1.38654971, g_loss: 0.69338870\n",
      "Step: [15021] d_loss: 1.38646889, g_loss: 0.69245875\n",
      "Step: [15022] d_loss: 1.38615978, g_loss: 0.69396091\n",
      "Step: [15023] d_loss: 1.38621831, g_loss: 0.69386446\n",
      "Step: [15024] d_loss: 1.38675010, g_loss: 0.69458735\n",
      "Step: [15025] d_loss: 1.38760161, g_loss: 0.69171941\n",
      "Step: [15026] d_loss: 1.38852513, g_loss: 0.69422114\n",
      "Step: [15027] d_loss: 1.38808203, g_loss: 0.68977928\n",
      "Step: [15028] d_loss: 1.38685727, g_loss: 0.69174767\n",
      "Step: [15029] d_loss: 1.38600981, g_loss: 0.69294536\n",
      "Step: [15030] d_loss: 1.38545108, g_loss: 0.69413561\n",
      "Step: [15031] d_loss: 1.38635707, g_loss: 0.69390291\n",
      "Step: [15032] d_loss: 1.38636386, g_loss: 0.69419903\n",
      "Step: [15033] d_loss: 1.38712978, g_loss: 0.69418085\n",
      "Step: [15034] d_loss: 1.38607454, g_loss: 0.69330025\n",
      "Step: [15035] d_loss: 1.38624048, g_loss: 0.69343185\n",
      "Step: [15036] d_loss: 1.38658786, g_loss: 0.69509673\n",
      "Step: [15037] d_loss: 1.38637245, g_loss: 0.69443166\n",
      "Step: [15038] d_loss: 1.38741779, g_loss: 0.69728410\n",
      "Step: [15039] d_loss: 1.38691556, g_loss: 0.69561863\n",
      "Step: [15040] d_loss: 1.38660276, g_loss: 0.69480628\n",
      "Step: [15041] d_loss: 1.38797832, g_loss: 0.69346184\n",
      "Step: [15042] d_loss: 1.38917935, g_loss: 0.69706762\n",
      "Step: [15043] d_loss: 1.39170527, g_loss: 0.69378150\n",
      "Step: [15044] d_loss: 1.38792074, g_loss: 0.68497658\n",
      "Step: [15045] d_loss: 1.38683319, g_loss: 0.68649513\n",
      "Step: [15046] d_loss: 1.38654029, g_loss: 0.69152349\n",
      "Step: [15047] d_loss: 1.38615990, g_loss: 0.69483608\n",
      "Step: [15048] d_loss: 1.38644171, g_loss: 0.69581723\n",
      "Step: [15049] d_loss: 1.38627887, g_loss: 0.69423574\n",
      "Step: [15050] d_loss: 1.38632405, g_loss: 0.69226241\n",
      "Step: [15051] d_loss: 1.38613725, g_loss: 0.69192857\n",
      "Step: [15052] d_loss: 1.38639092, g_loss: 0.69245857\n",
      "Step: [15053] d_loss: 1.38631725, g_loss: 0.69293702\n",
      "Step: [15054] d_loss: 1.38658667, g_loss: 0.69544059\n",
      "Step: [15055] d_loss: 1.38709831, g_loss: 0.69281667\n",
      "Step: [15056] d_loss: 1.38661337, g_loss: 0.69272494\n",
      "Step: [15057] d_loss: 1.38614321, g_loss: 0.69234699\n",
      "Step: [15058] d_loss: 1.38632464, g_loss: 0.69318509\n",
      "Step: [15059] d_loss: 1.38612378, g_loss: 0.69375986\n",
      "Step: [15060] d_loss: 1.38619709, g_loss: 0.69319695\n",
      "Step: [15061] d_loss: 1.38628936, g_loss: 0.69430625\n",
      "Step: [15062] d_loss: 1.38641381, g_loss: 0.69383132\n",
      "Step: [15063] d_loss: 1.38610792, g_loss: 0.69391018\n",
      "Step: [15064] d_loss: 1.38618469, g_loss: 0.69438958\n",
      "Step: [15065] d_loss: 1.38633466, g_loss: 0.69313002\n",
      "Step: [15066] d_loss: 1.38629317, g_loss: 0.69214630\n",
      "Step: [15067] d_loss: 1.38625431, g_loss: 0.69266391\n",
      "Step: [15068] d_loss: 1.38615382, g_loss: 0.69383359\n",
      "Step: [15069] d_loss: 1.38596058, g_loss: 0.69472259\n",
      "Step: [15070] d_loss: 1.38612795, g_loss: 0.69280714\n",
      "Step: [15071] d_loss: 1.38650918, g_loss: 0.69402587\n",
      "Step: [15072] d_loss: 1.38639200, g_loss: 0.69192165\n",
      "Step: [15073] d_loss: 1.38666618, g_loss: 0.69334292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15074] d_loss: 1.38650274, g_loss: 0.69269627\n",
      "Step: [15075] d_loss: 1.38629997, g_loss: 0.69205880\n",
      "Step: [15076] d_loss: 1.38644063, g_loss: 0.69314027\n",
      "Step: [15077] d_loss: 1.38619661, g_loss: 0.69428754\n",
      "Step: [15078] d_loss: 1.38614631, g_loss: 0.69444656\n",
      "Step: [15079] d_loss: 1.38620424, g_loss: 0.69340563\n",
      "Step: [15080] d_loss: 1.38614607, g_loss: 0.69309860\n",
      "Step: [15081] d_loss: 1.38637805, g_loss: 0.69193572\n",
      "Step: [15082] d_loss: 1.38607478, g_loss: 0.69326812\n",
      "Step: [15083] d_loss: 1.38625336, g_loss: 0.69192994\n",
      "Step: [15084] d_loss: 1.38646245, g_loss: 0.69473076\n",
      "Step: [15085] d_loss: 1.38646507, g_loss: 0.69313759\n",
      "Step: [15086] d_loss: 1.38622165, g_loss: 0.69329178\n",
      "Step: [15087] d_loss: 1.38670802, g_loss: 0.69516939\n",
      "Step: [15088] d_loss: 1.38630414, g_loss: 0.69352663\n",
      "Step: [15089] d_loss: 1.38644171, g_loss: 0.69255340\n",
      "Step: [15090] d_loss: 1.38643384, g_loss: 0.69209969\n",
      "Step: [15091] d_loss: 1.38648665, g_loss: 0.69219613\n",
      "Step: [15092] d_loss: 1.38645673, g_loss: 0.69262469\n",
      "Step: [15093] d_loss: 1.38620770, g_loss: 0.69397163\n",
      "Step: [15094] d_loss: 1.38638365, g_loss: 0.69451207\n",
      "Step: [15095] d_loss: 1.38630223, g_loss: 0.69296396\n",
      "Step: [15096] d_loss: 1.38621521, g_loss: 0.69330406\n",
      "Step: [15097] d_loss: 1.38643718, g_loss: 0.69341469\n",
      "Step: [15098] d_loss: 1.38629842, g_loss: 0.69279861\n",
      "Step: [15099] d_loss: 1.38645935, g_loss: 0.69182837\n",
      "Step: [15100] d_loss: 1.38679326, g_loss: 0.69201648\n",
      "Step: [15101] d_loss: 1.38652468, g_loss: 0.69288391\n",
      "Step: [15102] d_loss: 1.38642216, g_loss: 0.69348973\n",
      "Step: [15103] d_loss: 1.38624966, g_loss: 0.69239199\n",
      "Step: [15104] d_loss: 1.38640952, g_loss: 0.69272602\n",
      "Step: [15105] d_loss: 1.38641202, g_loss: 0.69345629\n",
      "Step: [15106] d_loss: 1.38635051, g_loss: 0.69282639\n",
      "Step: [15107] d_loss: 1.38630235, g_loss: 0.69347668\n",
      "Step: [15108] d_loss: 1.38617671, g_loss: 0.69298923\n",
      "Step: [15109] d_loss: 1.38640499, g_loss: 0.69499177\n",
      "Step: [15110] d_loss: 1.38607740, g_loss: 0.69359434\n",
      "Step: [15111] d_loss: 1.38635874, g_loss: 0.69197381\n",
      "Step: [15112] d_loss: 1.38637590, g_loss: 0.69167197\n",
      "Step: [15113] d_loss: 1.38640952, g_loss: 0.69529408\n",
      "Step: [15114] d_loss: 1.38637340, g_loss: 0.69642103\n",
      "Step: [15115] d_loss: 1.38618040, g_loss: 0.69440353\n",
      "Step: [15116] d_loss: 1.38606548, g_loss: 0.69210958\n",
      "Step: [15117] d_loss: 1.38624239, g_loss: 0.69297302\n",
      "Step: [15118] d_loss: 1.38607335, g_loss: 0.69340456\n",
      "Step: [15119] d_loss: 1.38675725, g_loss: 0.69434834\n",
      "Step: [15120] d_loss: 1.38627219, g_loss: 0.69345921\n",
      "Step: [15121] d_loss: 1.38623869, g_loss: 0.69424498\n",
      "Step: [15122] d_loss: 1.38610125, g_loss: 0.69326311\n",
      "Step: [15123] d_loss: 1.38631344, g_loss: 0.69298184\n",
      "Step: [15124] d_loss: 1.38620305, g_loss: 0.69367665\n",
      "Step: [15125] d_loss: 1.38608956, g_loss: 0.69311357\n",
      "Step: [15126] d_loss: 1.38621616, g_loss: 0.69260192\n",
      "Step: [15127] d_loss: 1.38649511, g_loss: 0.69337618\n",
      "Step: [15128] d_loss: 1.38639247, g_loss: 0.69236404\n",
      "Step: [15129] d_loss: 1.38635039, g_loss: 0.69338369\n",
      "Step: [15130] d_loss: 1.38601339, g_loss: 0.69342089\n",
      "Step: [15131] d_loss: 1.38634646, g_loss: 0.69503945\n",
      "Step: [15132] d_loss: 1.38693571, g_loss: 0.69192004\n",
      "Step: [15133] d_loss: 1.38680506, g_loss: 0.69366390\n",
      "Step: [15134] d_loss: 1.38663816, g_loss: 0.69364214\n",
      "Step: [15135] d_loss: 1.38613987, g_loss: 0.69297117\n",
      "Step: [15136] d_loss: 1.38641846, g_loss: 0.69338000\n",
      "Step: [15137] d_loss: 1.38640642, g_loss: 0.69290143\n",
      "Step: [15138] d_loss: 1.38656616, g_loss: 0.69398129\n",
      "Step: [15139] d_loss: 1.38636971, g_loss: 0.69429326\n",
      "Step: [15140] d_loss: 1.38668942, g_loss: 0.69494379\n",
      "Step: [15141] d_loss: 1.38709104, g_loss: 0.69731009\n",
      "Step: [15142] d_loss: 1.38724351, g_loss: 0.69655067\n",
      "Step: [15143] d_loss: 1.38667655, g_loss: 0.69652253\n",
      "Step: [15144] d_loss: 1.38644624, g_loss: 0.69415236\n",
      "Step: [15145] d_loss: 1.38618374, g_loss: 0.69218981\n",
      "Step: [15146] d_loss: 1.38638830, g_loss: 0.69156456\n",
      "Step: [15147] d_loss: 1.38641238, g_loss: 0.69172454\n",
      "Step: [15148] d_loss: 1.38635123, g_loss: 0.69300562\n",
      "Step: [15149] d_loss: 1.38646364, g_loss: 0.69392920\n",
      "Step: [15150] d_loss: 1.38675952, g_loss: 0.69392526\n",
      "Step: [15151] d_loss: 1.38652956, g_loss: 0.69133794\n",
      "Step: [15152] d_loss: 1.38640046, g_loss: 0.69202232\n",
      "Step: [15153] d_loss: 1.38630867, g_loss: 0.69297498\n",
      "Step: [15154] d_loss: 1.38621831, g_loss: 0.69380939\n",
      "Step: [15155] d_loss: 1.38628340, g_loss: 0.69412202\n",
      "Step: [15156] d_loss: 1.38638783, g_loss: 0.69326079\n",
      "Step: [15157] d_loss: 1.38632488, g_loss: 0.69351262\n",
      "Step: [15158] d_loss: 1.38612032, g_loss: 0.69309282\n",
      "Step: [15159] d_loss: 1.38630462, g_loss: 0.69314671\n",
      "Step: [15160] d_loss: 1.38624263, g_loss: 0.69317311\n",
      "Step: [15161] d_loss: 1.38607132, g_loss: 0.69366658\n",
      "Step: [15162] d_loss: 1.38619781, g_loss: 0.69301414\n",
      "Step: [15163] d_loss: 1.38633132, g_loss: 0.69314647\n",
      "Step: [15164] d_loss: 1.38579488, g_loss: 0.69313866\n",
      "Step: [15165] d_loss: 1.38630271, g_loss: 0.69387597\n",
      "Step: [15166] d_loss: 1.38620484, g_loss: 0.69504154\n",
      "Step: [15167] d_loss: 1.38627291, g_loss: 0.69352412\n",
      "Step: [15168] d_loss: 1.38651967, g_loss: 0.69249213\n",
      "Step: [15169] d_loss: 1.38617599, g_loss: 0.69338024\n",
      "Step: [15170] d_loss: 1.38653362, g_loss: 0.69346035\n",
      "Step: [15171] d_loss: 1.38817358, g_loss: 0.69380248\n",
      "Step: [15172] d_loss: 1.38898754, g_loss: 0.69283128\n",
      "Step: [15173] d_loss: 1.38820362, g_loss: 0.69531107\n",
      "Step: [15174] d_loss: 1.38674951, g_loss: 0.69774687\n",
      "Step: [15175] d_loss: 1.38635373, g_loss: 0.69580621\n",
      "Step: [15176] d_loss: 1.38615477, g_loss: 0.69259870\n",
      "Step: [15177] d_loss: 1.38636398, g_loss: 0.69178081\n",
      "Step: [15178] d_loss: 1.38613224, g_loss: 0.69086695\n",
      "Step: [15179] d_loss: 1.38626420, g_loss: 0.69201106\n",
      "Step: [15180] d_loss: 1.38615942, g_loss: 0.69478822\n",
      "Step: [15181] d_loss: 1.38632953, g_loss: 0.69381773\n",
      "Step: [15182] d_loss: 1.38674998, g_loss: 0.69322872\n",
      "Step: [15183] d_loss: 1.38716066, g_loss: 0.69552958\n",
      "Step: [15184] d_loss: 1.38659966, g_loss: 0.69595748\n",
      "Step: [15185] d_loss: 1.38644123, g_loss: 0.69387901\n",
      "Step: [15186] d_loss: 1.38618350, g_loss: 0.69508386\n",
      "Step: [15187] d_loss: 1.38639498, g_loss: 0.69261634\n",
      "Step: [15188] d_loss: 1.38625026, g_loss: 0.69212222\n",
      "Step: [15189] d_loss: 1.38659239, g_loss: 0.69114590\n",
      "Step: [15190] d_loss: 1.38697386, g_loss: 0.69344741\n",
      "Step: [15191] d_loss: 1.38729835, g_loss: 0.69111955\n",
      "Step: [15192] d_loss: 1.38703632, g_loss: 0.69356215\n",
      "Step: [15193] d_loss: 1.38646173, g_loss: 0.69199944\n",
      "Step: [15194] d_loss: 1.38629103, g_loss: 0.69163388\n",
      "Step: [15195] d_loss: 1.38636053, g_loss: 0.69278419\n",
      "Step: [15196] d_loss: 1.38623834, g_loss: 0.69295692\n",
      "Step: [15197] d_loss: 1.38637543, g_loss: 0.69385844\n",
      "Step: [15198] d_loss: 1.38611472, g_loss: 0.69451714\n",
      "Step: [15199] d_loss: 1.38635206, g_loss: 0.69351375\n",
      "Step: [15200] d_loss: 1.38616490, g_loss: 0.69277453\n",
      "Step: [15201] d_loss: 1.38641787, g_loss: 0.69230795\n",
      "Step: [15202] d_loss: 1.38607383, g_loss: 0.69331288\n",
      "Step: [15203] d_loss: 1.38617420, g_loss: 0.69413316\n",
      "Step: [15204] d_loss: 1.38613760, g_loss: 0.69273579\n",
      "Step: [15205] d_loss: 1.38622880, g_loss: 0.69313008\n",
      "Step: [15206] d_loss: 1.38618338, g_loss: 0.69262421\n",
      "Step: [15207] d_loss: 1.38610005, g_loss: 0.69274503\n",
      "Step: [15208] d_loss: 1.38608027, g_loss: 0.69336110\n",
      "Step: [15209] d_loss: 1.38690925, g_loss: 0.69591904\n",
      "Step: [15210] d_loss: 1.38753033, g_loss: 0.68979013\n",
      "Step: [15211] d_loss: 1.38712013, g_loss: 0.68968558\n",
      "Step: [15212] d_loss: 1.38648510, g_loss: 0.69388855\n",
      "Step: [15213] d_loss: 1.38621294, g_loss: 0.69602060\n",
      "Step: [15214] d_loss: 1.38662839, g_loss: 0.69319201\n",
      "Step: [15215] d_loss: 1.38735414, g_loss: 0.69370210\n",
      "Step: [15216] d_loss: 1.38888597, g_loss: 0.69034457\n",
      "Step: [15217] d_loss: 1.38917160, g_loss: 0.69801325\n",
      "Step: [15218] d_loss: 1.38744509, g_loss: 0.69876003\n",
      "Step: [15219] d_loss: 1.38643634, g_loss: 0.69781947\n",
      "Step: [15220] d_loss: 1.38617897, g_loss: 0.69569552\n",
      "Step: [15221] d_loss: 1.38637853, g_loss: 0.69229680\n",
      "Step: [15222] d_loss: 1.38635492, g_loss: 0.69275767\n",
      "Step: [15223] d_loss: 1.38607764, g_loss: 0.69268811\n",
      "Step: [15224] d_loss: 1.38625717, g_loss: 0.69237059\n",
      "Step: [15225] d_loss: 1.38628185, g_loss: 0.69315672\n",
      "Step: [15226] d_loss: 1.38637197, g_loss: 0.69395912\n",
      "Step: [15227] d_loss: 1.38634098, g_loss: 0.69358593\n",
      "Step: [15228] d_loss: 1.38629889, g_loss: 0.69370615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15229] d_loss: 1.38628960, g_loss: 0.69309711\n",
      "Step: [15230] d_loss: 1.38593149, g_loss: 0.69422096\n",
      "Step: [15231] d_loss: 1.38620865, g_loss: 0.69315147\n",
      "Step: [15232] d_loss: 1.38650942, g_loss: 0.69320679\n",
      "Step: [15233] d_loss: 1.38626194, g_loss: 0.69389570\n",
      "Step: [15234] d_loss: 1.38623393, g_loss: 0.69248438\n",
      "Step: [15235] d_loss: 1.38668370, g_loss: 0.69361973\n",
      "Step: [15236] d_loss: 1.38652766, g_loss: 0.69343680\n",
      "Step: [15237] d_loss: 1.38658094, g_loss: 0.69244540\n",
      "Step: [15238] d_loss: 1.38645446, g_loss: 0.69178772\n",
      "Step: [15239] d_loss: 1.38640380, g_loss: 0.69358170\n",
      "Step: [15240] d_loss: 1.38622284, g_loss: 0.69472986\n",
      "Step: [15241] d_loss: 1.38622642, g_loss: 0.69298714\n",
      "Step: [15242] d_loss: 1.38630462, g_loss: 0.69383383\n",
      "Step: [15243] d_loss: 1.38640213, g_loss: 0.69367993\n",
      "Step: [15244] d_loss: 1.38633037, g_loss: 0.69310284\n",
      "Step: [15245] d_loss: 1.38623583, g_loss: 0.69296515\n",
      "Step: [15246] d_loss: 1.38640213, g_loss: 0.69289672\n",
      "Step: [15247] d_loss: 1.38649631, g_loss: 0.69300288\n",
      "Step: [15248] d_loss: 1.38630629, g_loss: 0.69320703\n",
      "Step: [15249] d_loss: 1.38626504, g_loss: 0.69311285\n",
      "Step: [15250] d_loss: 1.38629377, g_loss: 0.69324875\n",
      "Step: [15251] d_loss: 1.38620090, g_loss: 0.69304889\n",
      "Step: [15252] d_loss: 1.38629258, g_loss: 0.69319737\n",
      "Step: [15253] d_loss: 1.38619018, g_loss: 0.69312906\n",
      "Step: [15254] d_loss: 1.38632250, g_loss: 0.69313645\n",
      "Step: [15255] d_loss: 1.38630867, g_loss: 0.69314212\n",
      "Step: [15256] d_loss: 1.38627243, g_loss: 0.69318038\n",
      "Step: [15257] d_loss: 1.38628697, g_loss: 0.69321340\n",
      "Step: [15258] d_loss: 1.38624096, g_loss: 0.69315326\n",
      "Step: [15259] d_loss: 1.38604808, g_loss: 0.69376218\n",
      "Step: [15260] d_loss: 1.38623106, g_loss: 0.69328332\n",
      "Step: [15261] d_loss: 1.38621449, g_loss: 0.69251597\n",
      "Step: [15262] d_loss: 1.38644099, g_loss: 0.69367385\n",
      "Step: [15263] d_loss: 1.38630629, g_loss: 0.69495523\n",
      "Step: [15264] d_loss: 1.38726115, g_loss: 0.69462609\n",
      "Step: [15265] d_loss: 1.38649642, g_loss: 0.69328594\n",
      "Step: [15266] d_loss: 1.38646531, g_loss: 0.69156152\n",
      "Step: [15267] d_loss: 1.38633800, g_loss: 0.69149470\n",
      "Step: [15268] d_loss: 1.38626528, g_loss: 0.69274276\n",
      "Step: [15269] d_loss: 1.38624954, g_loss: 0.69383830\n",
      "Step: [15270] d_loss: 1.38635015, g_loss: 0.69363827\n",
      "Step: [15271] d_loss: 1.38624775, g_loss: 0.69333982\n",
      "Step: [15272] d_loss: 1.38625991, g_loss: 0.69311780\n",
      "Step: [15273] d_loss: 1.38626158, g_loss: 0.69287902\n",
      "Step: [15274] d_loss: 1.38725150, g_loss: 0.69293141\n",
      "Step: [15275] d_loss: 1.38616800, g_loss: 0.69362831\n",
      "Step: [15276] d_loss: 1.38628626, g_loss: 0.69409221\n",
      "Step: [15277] d_loss: 1.38625717, g_loss: 0.69270504\n",
      "Step: [15278] d_loss: 1.38665557, g_loss: 0.69487625\n",
      "Step: [15279] d_loss: 1.38725471, g_loss: 0.69169706\n",
      "Step: [15280] d_loss: 1.38683987, g_loss: 0.69312316\n",
      "Step: [15281] d_loss: 1.38637781, g_loss: 0.69463795\n",
      "Step: [15282] d_loss: 1.38626170, g_loss: 0.69439280\n",
      "Step: [15283] d_loss: 1.38626003, g_loss: 0.69292337\n",
      "Step: [15284] d_loss: 1.38629198, g_loss: 0.69197869\n",
      "Step: [15285] d_loss: 1.38630319, g_loss: 0.69262052\n",
      "Step: [15286] d_loss: 1.38630033, g_loss: 0.69328249\n",
      "Step: [15287] d_loss: 1.38625693, g_loss: 0.69361007\n",
      "Step: [15288] d_loss: 1.38627648, g_loss: 0.69356263\n",
      "Step: [15289] d_loss: 1.38620543, g_loss: 0.69319725\n",
      "Step: [15290] d_loss: 1.38624752, g_loss: 0.69309652\n",
      "Step: [15291] d_loss: 1.38632441, g_loss: 0.69305444\n",
      "Step: [15292] d_loss: 1.38623476, g_loss: 0.69313914\n",
      "Step: [15293] d_loss: 1.39409351, g_loss: 0.89187908\n",
      "Step: [15294] d_loss: 1.38774443, g_loss: 0.68689823\n",
      "Step: [15295] d_loss: 1.38799918, g_loss: 0.69449675\n",
      "Step: [15296] d_loss: 1.38697076, g_loss: 0.69982135\n",
      "Step: [15297] d_loss: 1.38701665, g_loss: 0.69907260\n",
      "Step: [15298] d_loss: 1.38658214, g_loss: 0.69623542\n",
      "Step: [15299] d_loss: 1.38666916, g_loss: 0.69279814\n",
      "Step: [15300] d_loss: 1.38641191, g_loss: 0.69276178\n",
      "Step: [15301] d_loss: 1.38639903, g_loss: 0.69122791\n",
      "Step: [15302] d_loss: 1.38638997, g_loss: 0.69226944\n",
      "Step: [15303] d_loss: 1.38636208, g_loss: 0.69474208\n",
      "Step: [15304] d_loss: 1.38646364, g_loss: 0.69354302\n",
      "Step: [15305] d_loss: 1.38678920, g_loss: 0.69477749\n",
      "Step: [15306] d_loss: 1.38767052, g_loss: 0.69215572\n",
      "Step: [15307] d_loss: 1.38641870, g_loss: 0.69164824\n",
      "Step: [15308] d_loss: 1.38658559, g_loss: 0.69249284\n",
      "Step: [15309] d_loss: 1.38660336, g_loss: 0.69262969\n",
      "Step: [15310] d_loss: 1.38640857, g_loss: 0.69397801\n",
      "Step: [15311] d_loss: 1.38614345, g_loss: 0.69312298\n",
      "Step: [15312] d_loss: 1.38653708, g_loss: 0.69278777\n",
      "Step: [15313] d_loss: 1.38658917, g_loss: 0.69183975\n",
      "Step: [15314] d_loss: 1.38644993, g_loss: 0.69193095\n",
      "Step: [15315] d_loss: 1.38632131, g_loss: 0.69306231\n",
      "Step: [15316] d_loss: 1.38599432, g_loss: 0.69390142\n",
      "Step: [15317] d_loss: 1.38627386, g_loss: 0.69424582\n",
      "Step: [15318] d_loss: 1.38614774, g_loss: 0.69423229\n",
      "Step: [15319] d_loss: 1.38632905, g_loss: 0.69363916\n",
      "Step: [15320] d_loss: 1.38638306, g_loss: 0.69312435\n",
      "Step: [15321] d_loss: 1.38631749, g_loss: 0.69266444\n",
      "Step: [15322] d_loss: 1.38505578, g_loss: 0.69154996\n",
      "Step: [15323] d_loss: 1.38656843, g_loss: 0.69359297\n",
      "Step: [15324] d_loss: 1.38655448, g_loss: 0.69327807\n",
      "Step: [15325] d_loss: 1.38632059, g_loss: 0.69423282\n",
      "Step: [15326] d_loss: 1.38622475, g_loss: 0.69402742\n",
      "Step: [15327] d_loss: 1.38358951, g_loss: 0.69548839\n",
      "Step: [15328] d_loss: 1.38617456, g_loss: 0.69317341\n",
      "Step: [15329] d_loss: 1.38610959, g_loss: 0.69311643\n",
      "Step: [15330] d_loss: 1.38632846, g_loss: 0.69401324\n",
      "Step: [15331] d_loss: 1.38612282, g_loss: 0.69353133\n",
      "Step: [15332] d_loss: 1.38652706, g_loss: 0.69344765\n",
      "Step: [15333] d_loss: 1.38602448, g_loss: 0.69311523\n",
      "Step: [15334] d_loss: 1.38615894, g_loss: 0.69301587\n",
      "Step: [15335] d_loss: 1.38600147, g_loss: 0.69314027\n",
      "Step: [15336] d_loss: 1.38669014, g_loss: 0.69580352\n",
      "Step: [15337] d_loss: 1.38887691, g_loss: 0.69382024\n",
      "Step: [15338] d_loss: 1.38658977, g_loss: 0.69331789\n",
      "Step: [15339] d_loss: 1.38643003, g_loss: 0.69250047\n",
      "Step: [15340] d_loss: 1.38634264, g_loss: 0.69333345\n",
      "Step: [15341] d_loss: 1.38639283, g_loss: 0.69427168\n",
      "Step: [15342] d_loss: 1.38625002, g_loss: 0.69414914\n",
      "Step: [15343] d_loss: 1.38608241, g_loss: 0.69516337\n",
      "Step: [15344] d_loss: 1.38611758, g_loss: 0.69427216\n",
      "Step: [15345] d_loss: 1.38633049, g_loss: 0.69272316\n",
      "Step: [15346] d_loss: 1.38630939, g_loss: 0.69222927\n",
      "Step: [15347] d_loss: 1.38523674, g_loss: 0.69194943\n",
      "Step: [15348] d_loss: 1.38621521, g_loss: 0.69372684\n",
      "Step: [15349] d_loss: 1.38636041, g_loss: 0.69331801\n",
      "Step: [15350] d_loss: 1.38624465, g_loss: 0.69361913\n",
      "Step: [15351] d_loss: 1.38623595, g_loss: 0.69337809\n",
      "Step: [15352] d_loss: 1.38625312, g_loss: 0.69333452\n",
      "Step: [15353] d_loss: 1.38650441, g_loss: 0.69375414\n",
      "Step: [15354] d_loss: 1.38571155, g_loss: 0.69357258\n",
      "Step: [15355] d_loss: 1.38626146, g_loss: 0.69320703\n",
      "Step: [15356] d_loss: 1.38625920, g_loss: 0.69677484\n",
      "Step: [15357] d_loss: 1.38648891, g_loss: 0.69605672\n",
      "Step: [15358] d_loss: 1.38626695, g_loss: 0.69471282\n",
      "Step: [15359] d_loss: 1.38619184, g_loss: 0.69315767\n",
      "Step: [15360] d_loss: 1.38604665, g_loss: 0.69254112\n",
      "Step: [15361] d_loss: 1.38632965, g_loss: 0.69344199\n",
      "Step: [15362] d_loss: 1.38607764, g_loss: 0.69288737\n",
      "Step: [15363] d_loss: 1.38624835, g_loss: 0.69344449\n",
      "Step: [15364] d_loss: 1.38621402, g_loss: 0.69261879\n",
      "Step: [15365] d_loss: 1.38614345, g_loss: 0.69351292\n",
      "Step: [15366] d_loss: 1.38664591, g_loss: 0.69405890\n",
      "Step: [15367] d_loss: 1.39341486, g_loss: 0.69156867\n",
      "Step: [15368] d_loss: 1.38779569, g_loss: 0.69731772\n",
      "Step: [15369] d_loss: 1.38707387, g_loss: 0.69023478\n",
      "Step: [15370] d_loss: 1.38645482, g_loss: 0.68939829\n",
      "Step: [15371] d_loss: 1.38627589, g_loss: 0.69125819\n",
      "Step: [15372] d_loss: 1.38671863, g_loss: 0.69431913\n",
      "Step: [15373] d_loss: 1.38655353, g_loss: 0.69466090\n",
      "Step: [15374] d_loss: 1.38630581, g_loss: 0.69337034\n",
      "Step: [15375] d_loss: 1.38624144, g_loss: 0.69268692\n",
      "Step: [15376] d_loss: 1.38619399, g_loss: 0.69237256\n",
      "Step: [15377] d_loss: 1.38621426, g_loss: 0.69272280\n",
      "Step: [15378] d_loss: 1.38627815, g_loss: 0.69237822\n",
      "Step: [15379] d_loss: 1.38622451, g_loss: 0.69328678\n",
      "Step: [15380] d_loss: 1.38629484, g_loss: 0.69353235\n",
      "Step: [15381] d_loss: 1.38623023, g_loss: 0.69382149\n",
      "Step: [15382] d_loss: 1.38655829, g_loss: 0.69332874\n",
      "Step: [15383] d_loss: 1.38621533, g_loss: 0.69327533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15384] d_loss: 1.38624716, g_loss: 0.69307774\n",
      "Step: [15385] d_loss: 1.38630652, g_loss: 0.69308448\n",
      "Step: [15386] d_loss: 1.38616633, g_loss: 0.69324231\n",
      "Step: [15387] d_loss: 1.38618076, g_loss: 0.69340670\n",
      "Step: [15388] d_loss: 1.38636446, g_loss: 0.69328928\n",
      "Step: [15389] d_loss: 1.38626206, g_loss: 0.69315380\n",
      "Step: [15390] d_loss: 1.38620281, g_loss: 0.69282722\n",
      "Step: [15391] d_loss: 1.38618064, g_loss: 0.69321334\n",
      "Step: [15392] d_loss: 1.38618660, g_loss: 0.69279659\n",
      "Step: [15393] d_loss: 1.38635302, g_loss: 0.69318974\n",
      "Step: [15394] d_loss: 1.38638568, g_loss: 0.69447541\n",
      "Step: [15395] d_loss: 1.38618612, g_loss: 0.69361162\n",
      "Step: [15396] d_loss: 1.38621569, g_loss: 0.69294119\n",
      "Step: [15397] d_loss: 1.38634455, g_loss: 0.69341731\n",
      "Step: [15398] d_loss: 1.38618708, g_loss: 0.69311750\n",
      "Step: [15399] d_loss: 1.38879156, g_loss: 0.69265151\n",
      "Step: [15400] d_loss: 1.38657081, g_loss: 0.69287211\n",
      "Step: [15401] d_loss: 1.38653421, g_loss: 0.69293511\n",
      "Step: [15402] d_loss: 1.38638401, g_loss: 0.69478589\n",
      "Step: [15403] d_loss: 1.38630867, g_loss: 0.69395512\n",
      "Step: [15404] d_loss: 1.38630581, g_loss: 0.69381994\n",
      "Step: [15405] d_loss: 1.38632393, g_loss: 0.69312406\n",
      "Step: [15406] d_loss: 1.38620961, g_loss: 0.69289672\n",
      "Step: [15407] d_loss: 1.38616443, g_loss: 0.69244814\n",
      "Step: [15408] d_loss: 1.38618851, g_loss: 0.69363534\n",
      "Step: [15409] d_loss: 1.38623571, g_loss: 0.69359589\n",
      "Step: [15410] d_loss: 1.38628435, g_loss: 0.69316185\n",
      "Step: [15411] d_loss: 1.38610375, g_loss: 0.69329298\n",
      "Step: [15412] d_loss: 1.38621378, g_loss: 0.69299519\n",
      "Step: [15413] d_loss: 1.38611317, g_loss: 0.69327343\n",
      "Step: [15414] d_loss: 1.38617694, g_loss: 0.69321597\n",
      "Step: [15415] d_loss: 1.38624191, g_loss: 0.69346672\n",
      "Step: [15416] d_loss: 1.38625598, g_loss: 0.69378495\n",
      "Step: [15417] d_loss: 1.38627982, g_loss: 0.69343662\n",
      "Step: [15418] d_loss: 1.38615906, g_loss: 0.69291365\n",
      "Step: [15419] d_loss: 1.38615751, g_loss: 0.69362301\n",
      "Step: [15420] d_loss: 1.38615417, g_loss: 0.69301885\n",
      "Step: [15421] d_loss: 1.38612747, g_loss: 0.69293308\n",
      "Step: [15422] d_loss: 1.38603282, g_loss: 0.69396275\n",
      "Step: [15423] d_loss: 1.38623095, g_loss: 0.69429362\n",
      "Step: [15424] d_loss: 1.38621593, g_loss: 0.69310260\n",
      "Step: [15425] d_loss: 1.38630962, g_loss: 0.69274509\n",
      "Step: [15426] d_loss: 1.38606012, g_loss: 0.69351184\n",
      "Step: [15427] d_loss: 1.38611984, g_loss: 0.69228899\n",
      "Step: [15428] d_loss: 1.38710666, g_loss: 0.69774842\n",
      "Step: [15429] d_loss: 1.38615394, g_loss: 0.69207454\n",
      "Step: [15430] d_loss: 1.38630462, g_loss: 0.69260240\n",
      "Step: [15431] d_loss: 1.38550162, g_loss: 0.69982427\n",
      "Step: [15432] d_loss: 1.38628352, g_loss: 0.69317669\n",
      "Step: [15433] d_loss: 1.38622797, g_loss: 0.69406390\n",
      "Step: [15434] d_loss: 1.38611937, g_loss: 0.69361281\n",
      "Step: [15435] d_loss: 1.38635683, g_loss: 0.69315875\n",
      "Step: [15436] d_loss: 1.38675594, g_loss: 0.69471794\n",
      "Step: [15437] d_loss: 1.38677931, g_loss: 0.69184220\n",
      "Step: [15438] d_loss: 1.38671839, g_loss: 0.69217169\n",
      "Step: [15439] d_loss: 1.38626051, g_loss: 0.69290596\n",
      "Step: [15440] d_loss: 1.38615656, g_loss: 0.69380271\n",
      "Step: [15441] d_loss: 1.38602233, g_loss: 0.69407827\n",
      "Step: [15442] d_loss: 1.38634098, g_loss: 0.69350231\n",
      "Step: [15443] d_loss: 1.38624859, g_loss: 0.69255251\n",
      "Step: [15444] d_loss: 1.38638973, g_loss: 0.69298816\n",
      "Step: [15445] d_loss: 1.38617373, g_loss: 0.69345570\n",
      "Step: [15446] d_loss: 1.38618612, g_loss: 0.69343853\n",
      "Step: [15447] d_loss: 1.38626254, g_loss: 0.69335389\n",
      "Step: [15448] d_loss: 1.38630736, g_loss: 0.69288599\n",
      "Step: [15449] d_loss: 1.38624918, g_loss: 0.69301605\n",
      "Step: [15450] d_loss: 1.38607931, g_loss: 0.69334602\n",
      "Step: [15451] d_loss: 1.38610137, g_loss: 0.69340974\n",
      "Step: [15452] d_loss: 1.38625932, g_loss: 0.69338089\n",
      "Step: [15453] d_loss: 1.38707256, g_loss: 0.69197613\n",
      "Step: [15454] d_loss: 1.38635623, g_loss: 0.69274092\n",
      "Step: [15455] d_loss: 1.38626814, g_loss: 0.69287241\n",
      "Step: [15456] d_loss: 1.38947582, g_loss: 0.69359910\n",
      "Step: [15457] d_loss: 1.38620043, g_loss: 0.69224441\n",
      "Step: [15458] d_loss: 1.38603663, g_loss: 0.69367254\n",
      "Step: [15459] d_loss: 1.38611436, g_loss: 0.69265521\n",
      "Step: [15460] d_loss: 1.38621616, g_loss: 0.69454086\n",
      "Step: [15461] d_loss: 1.38635087, g_loss: 0.69350553\n",
      "Step: [15462] d_loss: 1.38627565, g_loss: 0.69334537\n",
      "Step: [15463] d_loss: 1.38650227, g_loss: 0.69055820\n",
      "Step: [15464] d_loss: 1.38660216, g_loss: 0.69408303\n",
      "Step: [15465] d_loss: 1.38645864, g_loss: 0.69407690\n",
      "Step: [15466] d_loss: 1.38667357, g_loss: 0.69471967\n",
      "Step: [15467] d_loss: 1.38640034, g_loss: 0.69362223\n",
      "Step: [15468] d_loss: 1.38625824, g_loss: 0.69453329\n",
      "Step: [15469] d_loss: 1.38627291, g_loss: 0.69358313\n",
      "Step: [15470] d_loss: 1.38622975, g_loss: 0.69251919\n",
      "Step: [15471] d_loss: 1.38622832, g_loss: 0.69295758\n",
      "Step: [15472] d_loss: 1.38631666, g_loss: 0.69335705\n",
      "Step: [15473] d_loss: 1.38646531, g_loss: 0.69244909\n",
      "Step: [15474] d_loss: 1.38645864, g_loss: 0.69359136\n",
      "Step: [15475] d_loss: 1.38621902, g_loss: 0.69354677\n",
      "Step: [15476] d_loss: 1.38620448, g_loss: 0.69374061\n",
      "Step: [15477] d_loss: 1.38624179, g_loss: 0.69346958\n",
      "Step: [15478] d_loss: 1.38608575, g_loss: 0.69298261\n",
      "Step: [15479] d_loss: 1.38623977, g_loss: 0.69284600\n",
      "Step: [15480] d_loss: 1.38606572, g_loss: 0.69345903\n",
      "Step: [15481] d_loss: 1.38631332, g_loss: 0.69366586\n",
      "Step: [15482] d_loss: 1.38638127, g_loss: 0.69339466\n",
      "Step: [15483] d_loss: 1.38622439, g_loss: 0.69331139\n",
      "Step: [15484] d_loss: 1.38609934, g_loss: 0.69295096\n",
      "Step: [15485] d_loss: 1.38625014, g_loss: 0.69309926\n",
      "Step: [15486] d_loss: 1.38615179, g_loss: 0.69355887\n",
      "Step: [15487] d_loss: 1.38624692, g_loss: 0.69338113\n",
      "Step: [15488] d_loss: 1.38645577, g_loss: 0.69259846\n",
      "Step: [15489] d_loss: 1.38625932, g_loss: 0.69284070\n",
      "Step: [15490] d_loss: 1.38639891, g_loss: 0.69343305\n",
      "Step: [15491] d_loss: 1.38620949, g_loss: 0.69416279\n",
      "Step: [15492] d_loss: 1.38622212, g_loss: 0.69344664\n",
      "Step: [15493] d_loss: 1.38632154, g_loss: 0.69315785\n",
      "Step: [15494] d_loss: 1.38632500, g_loss: 0.69279128\n",
      "Step: [15495] d_loss: 1.38652301, g_loss: 0.69292259\n",
      "Step: [15496] d_loss: 1.38623190, g_loss: 0.69331896\n",
      "Step: [15497] d_loss: 1.38632035, g_loss: 0.69326460\n",
      "Step: [15498] d_loss: 1.38624120, g_loss: 0.69290400\n",
      "Step: [15499] d_loss: 1.38629913, g_loss: 0.69331479\n",
      "Step: [15500] d_loss: 1.38645387, g_loss: 0.69327307\n",
      "Step: [15501] d_loss: 1.38629878, g_loss: 0.69380355\n",
      "Step: [15502] d_loss: 1.38616586, g_loss: 0.69323206\n",
      "Step: [15503] d_loss: 1.38622475, g_loss: 0.69258231\n",
      "Step: [15504] d_loss: 1.38619649, g_loss: 0.69240904\n",
      "Step: [15505] d_loss: 1.38619232, g_loss: 0.69233567\n",
      "Step: [15506] d_loss: 1.38629794, g_loss: 0.69411099\n",
      "Step: [15507] d_loss: 1.38623583, g_loss: 0.69337487\n",
      "Step: [15508] d_loss: 1.38640070, g_loss: 0.69239378\n",
      "Step: [15509] d_loss: 1.38648939, g_loss: 0.69462520\n",
      "Step: [15510] d_loss: 1.38638866, g_loss: 0.69472146\n",
      "Step: [15511] d_loss: 1.38649595, g_loss: 0.69271088\n",
      "Step: [15512] d_loss: 1.38644063, g_loss: 0.69275600\n",
      "Step: [15513] d_loss: 1.38632107, g_loss: 0.69236338\n",
      "Step: [15514] d_loss: 1.38616323, g_loss: 0.69409490\n",
      "Step: [15515] d_loss: 1.38616061, g_loss: 0.69367158\n",
      "Step: [15516] d_loss: 1.38619173, g_loss: 0.69302601\n",
      "Step: [15517] d_loss: 1.38629580, g_loss: 0.69351816\n",
      "Step: [15518] d_loss: 1.38636184, g_loss: 0.69289136\n",
      "Step: [15519] d_loss: 1.38625681, g_loss: 0.69316143\n",
      "Step: [15520] d_loss: 1.38615584, g_loss: 0.69325769\n",
      "Step: [15521] d_loss: 1.38596892, g_loss: 0.69355488\n",
      "Step: [15522] d_loss: 1.38638508, g_loss: 0.69262302\n",
      "Step: [15523] d_loss: 1.38627195, g_loss: 0.69321299\n",
      "Step: [15524] d_loss: 1.38596165, g_loss: 0.69346750\n",
      "Step: [15525] d_loss: 1.38598645, g_loss: 0.69329226\n",
      "Step: [15526] d_loss: 1.38628769, g_loss: 0.69339383\n",
      "Step: [15527] d_loss: 1.38610983, g_loss: 0.69360918\n",
      "Step: [15528] d_loss: 1.38638723, g_loss: 0.69352382\n",
      "Step: [15529] d_loss: 1.38617861, g_loss: 0.69330055\n",
      "Step: [15530] d_loss: 1.38625383, g_loss: 0.69309807\n",
      "Step: [15531] d_loss: 1.38632226, g_loss: 0.69324285\n",
      "Step: [15532] d_loss: 1.38607454, g_loss: 0.69295007\n",
      "Step: [15533] d_loss: 1.38623953, g_loss: 0.69337046\n",
      "Step: [15534] d_loss: 1.38621259, g_loss: 0.69323987\n",
      "Step: [15535] d_loss: 1.38607073, g_loss: 0.69347346\n",
      "Step: [15536] d_loss: 1.38613832, g_loss: 0.69406378\n",
      "Step: [15537] d_loss: 1.38605571, g_loss: 0.69329429\n",
      "Step: [15538] d_loss: 1.38668346, g_loss: 0.69270587\n",
      "Step: [15539] d_loss: 1.38625646, g_loss: 0.69300514\n",
      "Step: [15540] d_loss: 1.38630593, g_loss: 0.69373631\n",
      "Step: [15541] d_loss: 1.38634086, g_loss: 0.69381404\n",
      "Step: [15542] d_loss: 1.38636732, g_loss: 0.69326872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15543] d_loss: 1.38618290, g_loss: 0.69312584\n",
      "Step: [15544] d_loss: 1.38635421, g_loss: 0.69317603\n",
      "Step: [15545] d_loss: 1.38621867, g_loss: 0.69337958\n",
      "Step: [15546] d_loss: 1.38615894, g_loss: 0.69316685\n",
      "Step: [15547] d_loss: 1.38653755, g_loss: 0.69313335\n",
      "Step: [15548] d_loss: 1.38647079, g_loss: 0.69314021\n",
      "Step: [15549] d_loss: 1.38646960, g_loss: 0.69377863\n",
      "Step: [15550] d_loss: 1.38630486, g_loss: 0.69541574\n",
      "Step: [15551] d_loss: 1.38672447, g_loss: 0.69397336\n",
      "Step: [15552] d_loss: 1.38630557, g_loss: 0.69249225\n",
      "Step: [15553] d_loss: 1.38621664, g_loss: 0.69241846\n",
      "Step: [15554] d_loss: 1.38629079, g_loss: 0.69343805\n",
      "Step: [15555] d_loss: 1.38632727, g_loss: 0.69390678\n",
      "Step: [15556] d_loss: 1.38620198, g_loss: 0.69345057\n",
      "Step: [15557] d_loss: 1.38626432, g_loss: 0.69274616\n",
      "Step: [15558] d_loss: 1.38601518, g_loss: 0.69315326\n",
      "Step: [15559] d_loss: 1.38608766, g_loss: 0.69319457\n",
      "Step: [15560] d_loss: 1.38616621, g_loss: 0.69330031\n",
      "Step: [15561] d_loss: 1.38627410, g_loss: 0.69316304\n",
      "Step: [15562] d_loss: 1.38619423, g_loss: 0.69230431\n",
      "Step: [15563] d_loss: 1.38635516, g_loss: 0.69361854\n",
      "Step: [15564] d_loss: 1.38627362, g_loss: 0.69356573\n",
      "Step: [15565] d_loss: 1.38636351, g_loss: 0.69395804\n",
      "Step: [15566] d_loss: 1.38624620, g_loss: 0.69328982\n",
      "Step: [15567] d_loss: 1.38627052, g_loss: 0.69258577\n",
      "Step: [15568] d_loss: 1.38637757, g_loss: 0.69296730\n",
      "Step: [15569] d_loss: 1.38631618, g_loss: 0.69362873\n",
      "Step: [15570] d_loss: 1.38625777, g_loss: 0.69371945\n",
      "Step: [15571] d_loss: 1.38626719, g_loss: 0.69320273\n",
      "Step: [15572] d_loss: 1.38629556, g_loss: 0.69277287\n",
      "Step: [15573] d_loss: 1.38622820, g_loss: 0.69325566\n",
      "Step: [15574] d_loss: 1.38638091, g_loss: 0.69390202\n",
      "Step: [15575] d_loss: 1.38643003, g_loss: 0.69257241\n",
      "Step: [15576] d_loss: 1.38635457, g_loss: 0.69382364\n",
      "Step: [15577] d_loss: 1.38629484, g_loss: 0.69376886\n",
      "Step: [15578] d_loss: 1.38648403, g_loss: 0.69330895\n",
      "Step: [15579] d_loss: 1.38626599, g_loss: 0.69272375\n",
      "Step: [15580] d_loss: 1.38615692, g_loss: 0.69263887\n",
      "Step: [15581] d_loss: 1.38624585, g_loss: 0.69262409\n",
      "Step: [15582] d_loss: 1.38638830, g_loss: 0.69317901\n",
      "Step: [15583] d_loss: 1.38638604, g_loss: 0.69308591\n",
      "Step: [15584] d_loss: 1.38615966, g_loss: 0.69293869\n",
      "Step: [15585] d_loss: 1.38637161, g_loss: 0.69363666\n",
      "Step: [15586] d_loss: 1.38624239, g_loss: 0.69360083\n",
      "Step: [15587] d_loss: 1.38630724, g_loss: 0.69316351\n",
      "Step: [15588] d_loss: 1.38632941, g_loss: 0.69290847\n",
      "Step: [15589] d_loss: 1.38623726, g_loss: 0.69299012\n",
      "Step: [15590] d_loss: 1.38621724, g_loss: 0.69304544\n",
      "Step: [15591] d_loss: 1.38617539, g_loss: 0.69323099\n",
      "Step: [15592] d_loss: 1.38636231, g_loss: 0.69294584\n",
      "Step: [15593] d_loss: 1.38618708, g_loss: 0.69364071\n",
      "Step: [15594] d_loss: 1.38633454, g_loss: 0.69388127\n",
      "Step: [15595] d_loss: 1.38626015, g_loss: 0.69389516\n",
      "Step: [15596] d_loss: 1.38636160, g_loss: 0.69317055\n",
      "Step: [15597] d_loss: 1.38618338, g_loss: 0.69275063\n",
      "Step: [15598] d_loss: 1.38623595, g_loss: 0.69291103\n",
      "Step: [15599] d_loss: 1.38629222, g_loss: 0.69293118\n",
      "Step: [15600] d_loss: 1.38642478, g_loss: 0.69358170\n",
      "Step: [15601] d_loss: 1.38641858, g_loss: 0.69451177\n",
      "Step: [15602] d_loss: 1.38627291, g_loss: 0.69237554\n",
      "Step: [15603] d_loss: 1.38632274, g_loss: 0.69390148\n",
      "Step: [15604] d_loss: 1.38683307, g_loss: 0.69254303\n",
      "Step: [15605] d_loss: 1.38679242, g_loss: 0.69249928\n",
      "Step: [15606] d_loss: 1.38645971, g_loss: 0.69248611\n",
      "Step: [15607] d_loss: 1.38639963, g_loss: 0.69179803\n",
      "Step: [15608] d_loss: 1.38639390, g_loss: 0.69352776\n",
      "Step: [15609] d_loss: 1.38639879, g_loss: 0.69394517\n",
      "Step: [15610] d_loss: 1.38639069, g_loss: 0.69465542\n",
      "Step: [15611] d_loss: 1.38645577, g_loss: 0.69347119\n",
      "Step: [15612] d_loss: 1.38628221, g_loss: 0.69334245\n",
      "Step: [15613] d_loss: 1.38627529, g_loss: 0.69298339\n",
      "Step: [15614] d_loss: 1.38640571, g_loss: 0.69303888\n",
      "Step: [15615] d_loss: 1.38635302, g_loss: 0.69322824\n",
      "Step: [15616] d_loss: 1.38637257, g_loss: 0.69316185\n",
      "Step: [15617] d_loss: 1.38636923, g_loss: 0.69315541\n",
      "Step: [15618] d_loss: 1.38627803, g_loss: 0.69307935\n",
      "Step: [15619] d_loss: 1.38631105, g_loss: 0.69317806\n",
      "Step: [15620] d_loss: 1.38633466, g_loss: 0.69311368\n",
      "Step: [15621] d_loss: 1.38623953, g_loss: 0.69319850\n",
      "Step: [15622] d_loss: 1.38631999, g_loss: 0.69338369\n",
      "Step: [15623] d_loss: 1.38618064, g_loss: 0.69377750\n",
      "Step: [15624] d_loss: 1.38642120, g_loss: 0.69378555\n",
      "Step: [15625] d_loss: 1.38681960, g_loss: 0.69312799\n",
      "Step: [15626] d_loss: 1.38729131, g_loss: 0.69502985\n",
      "Step: [15627] d_loss: 1.38714707, g_loss: 0.69320917\n",
      "Step: [15628] d_loss: 1.38730502, g_loss: 0.69449270\n",
      "Step: [15629] d_loss: 1.38727331, g_loss: 0.69242311\n",
      "Step: [15630] d_loss: 1.38695264, g_loss: 0.69490492\n",
      "Step: [15631] d_loss: 1.38657773, g_loss: 0.69302809\n",
      "Step: [15632] d_loss: 1.38627720, g_loss: 0.69405609\n",
      "Step: [15633] d_loss: 1.38702321, g_loss: 0.69308805\n",
      "Step: [15634] d_loss: 1.38930941, g_loss: 0.69644409\n",
      "Step: [15635] d_loss: 1.39130199, g_loss: 0.69008243\n",
      "Step: [15636] d_loss: 1.38981438, g_loss: 0.69450819\n",
      "Step: [15637] d_loss: 1.38742590, g_loss: 0.69468522\n",
      "Step: [15638] d_loss: 1.38636947, g_loss: 0.69482207\n",
      "Step: [15639] d_loss: 1.38629866, g_loss: 0.69394445\n",
      "Step: [15640] d_loss: 1.38646102, g_loss: 0.69195902\n",
      "Step: [15641] d_loss: 1.38642955, g_loss: 0.69273239\n",
      "Step: [15642] d_loss: 1.38637912, g_loss: 0.69253600\n",
      "Step: [15643] d_loss: 1.38634229, g_loss: 0.69297791\n",
      "Step: [15644] d_loss: 1.38620138, g_loss: 0.69357669\n",
      "Step: [15645] d_loss: 1.38624406, g_loss: 0.69342482\n",
      "Step: [15646] d_loss: 1.38627946, g_loss: 0.69335604\n",
      "Step: [15647] d_loss: 1.38624322, g_loss: 0.69310135\n",
      "Step: [15648] d_loss: 1.38627827, g_loss: 0.69283438\n",
      "Step: [15649] d_loss: 1.38618445, g_loss: 0.69376010\n",
      "Step: [15650] d_loss: 1.38632524, g_loss: 0.69401252\n",
      "Step: [15651] d_loss: 1.38651633, g_loss: 0.69223809\n",
      "Step: [15652] d_loss: 1.38645113, g_loss: 0.69341022\n",
      "Step: [15653] d_loss: 1.38630593, g_loss: 0.69352770\n",
      "Step: [15654] d_loss: 1.38627720, g_loss: 0.69362360\n",
      "Step: [15655] d_loss: 1.38625073, g_loss: 0.69348109\n",
      "Step: [15656] d_loss: 1.38626647, g_loss: 0.69301921\n",
      "Step: [15657] d_loss: 1.38627911, g_loss: 0.69293875\n",
      "Step: [15658] d_loss: 1.38620877, g_loss: 0.69315767\n",
      "Step: [15659] d_loss: 1.38622165, g_loss: 0.69335461\n",
      "Step: [15660] d_loss: 1.38621378, g_loss: 0.69338489\n",
      "Step: [15661] d_loss: 1.38617587, g_loss: 0.69287932\n",
      "Step: [15662] d_loss: 1.38642085, g_loss: 0.69329661\n",
      "Step: [15663] d_loss: 1.38653612, g_loss: 0.69339108\n",
      "Step: [15664] d_loss: 1.38655269, g_loss: 0.69448400\n",
      "Step: [15665] d_loss: 1.38642502, g_loss: 0.69584405\n",
      "Step: [15666] d_loss: 1.38623595, g_loss: 0.69425708\n",
      "Step: [15667] d_loss: 1.38629568, g_loss: 0.69276637\n",
      "Step: [15668] d_loss: 1.38629532, g_loss: 0.69086224\n",
      "Step: [15669] d_loss: 1.38641834, g_loss: 0.69205296\n",
      "Step: [15670] d_loss: 1.38622570, g_loss: 0.69450289\n",
      "Step: [15671] d_loss: 1.38629770, g_loss: 0.69413567\n",
      "Step: [15672] d_loss: 1.38632154, g_loss: 0.69338512\n",
      "Step: [15673] d_loss: 1.38638878, g_loss: 0.69385976\n",
      "Step: [15674] d_loss: 1.38644743, g_loss: 0.69172591\n",
      "Step: [15675] d_loss: 1.38648868, g_loss: 0.69035298\n",
      "Step: [15676] d_loss: 1.38634419, g_loss: 0.69171739\n",
      "Step: [15677] d_loss: 1.38644516, g_loss: 0.69277740\n",
      "Step: [15678] d_loss: 1.38615108, g_loss: 0.69335842\n",
      "Step: [15679] d_loss: 1.38637424, g_loss: 0.69551468\n",
      "Step: [15680] d_loss: 1.38675177, g_loss: 0.69312054\n",
      "Step: [15681] d_loss: 1.38708901, g_loss: 0.69493842\n",
      "Step: [15682] d_loss: 1.38703322, g_loss: 0.69447625\n",
      "Step: [15683] d_loss: 1.38648152, g_loss: 0.69502687\n",
      "Step: [15684] d_loss: 1.38622117, g_loss: 0.69396996\n",
      "Step: [15685] d_loss: 1.38628531, g_loss: 0.69297540\n",
      "Step: [15686] d_loss: 1.38636446, g_loss: 0.69272506\n",
      "Step: [15687] d_loss: 1.38630104, g_loss: 0.69216216\n",
      "Step: [15688] d_loss: 1.38636851, g_loss: 0.69311255\n",
      "Step: [15689] d_loss: 1.38626051, g_loss: 0.69283879\n",
      "Step: [15690] d_loss: 1.38628006, g_loss: 0.69324648\n",
      "Step: [15691] d_loss: 1.38620520, g_loss: 0.69332635\n",
      "Step: [15692] d_loss: 1.38628531, g_loss: 0.69375968\n",
      "Step: [15693] d_loss: 1.38632166, g_loss: 0.69283444\n",
      "Step: [15694] d_loss: 1.38638401, g_loss: 0.69188714\n",
      "Step: [15695] d_loss: 1.38639259, g_loss: 0.69233954\n",
      "Step: [15696] d_loss: 1.38634014, g_loss: 0.69256783\n",
      "Step: [15697] d_loss: 1.38637078, g_loss: 0.69341367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15698] d_loss: 1.38628173, g_loss: 0.69325697\n",
      "Step: [15699] d_loss: 1.38626575, g_loss: 0.69334042\n",
      "Step: [15700] d_loss: 1.38636303, g_loss: 0.69324595\n",
      "Step: [15701] d_loss: 1.38625133, g_loss: 0.69316298\n",
      "Step: [15702] d_loss: 1.38620543, g_loss: 0.69311047\n",
      "Step: [15703] d_loss: 1.38616586, g_loss: 0.69324815\n",
      "Step: [15704] d_loss: 1.38627076, g_loss: 0.69312119\n",
      "Step: [15705] d_loss: 1.38626981, g_loss: 0.69323027\n",
      "Step: [15706] d_loss: 1.38630843, g_loss: 0.69322425\n",
      "Step: [15707] d_loss: 1.38626373, g_loss: 0.69323200\n",
      "Step: [15708] d_loss: 1.38624299, g_loss: 0.69326997\n",
      "Step: [15709] d_loss: 1.38618135, g_loss: 0.69318002\n",
      "Step: [15710] d_loss: 1.38620543, g_loss: 0.69318676\n",
      "Step: [15711] d_loss: 1.38628161, g_loss: 0.69314420\n",
      "Step: [15712] d_loss: 1.38625026, g_loss: 0.69320488\n",
      "Step: [15713] d_loss: 1.38617539, g_loss: 0.69336104\n",
      "Step: [15714] d_loss: 1.38632500, g_loss: 0.69319546\n",
      "Step: [15715] d_loss: 1.38625062, g_loss: 0.69324100\n",
      "Step: [15716] d_loss: 1.38617694, g_loss: 0.69327831\n",
      "Step: [15717] d_loss: 1.38618541, g_loss: 0.69330120\n",
      "Step: [15718] d_loss: 1.38623333, g_loss: 0.69305891\n",
      "Step: [15719] d_loss: 1.38630259, g_loss: 0.69329095\n",
      "Step: [15720] d_loss: 1.38623381, g_loss: 0.69347078\n",
      "Step: [15721] d_loss: 1.38628006, g_loss: 0.69312549\n",
      "Step: [15722] d_loss: 1.38641119, g_loss: 0.69310313\n",
      "Step: [15723] d_loss: 1.38622892, g_loss: 0.69310343\n",
      "Step: [15724] d_loss: 1.38629615, g_loss: 0.69231248\n",
      "Step: [15725] d_loss: 1.38660121, g_loss: 0.69236320\n",
      "Step: [15726] d_loss: 1.38655043, g_loss: 0.69361699\n",
      "Step: [15727] d_loss: 1.38644648, g_loss: 0.69432479\n",
      "Step: [15728] d_loss: 1.38635898, g_loss: 0.69330645\n",
      "Step: [15729] d_loss: 1.38631856, g_loss: 0.69347459\n",
      "Step: [15730] d_loss: 1.38622046, g_loss: 0.69331616\n",
      "Step: [15731] d_loss: 1.38626552, g_loss: 0.69334239\n",
      "Step: [15732] d_loss: 1.38624012, g_loss: 0.69316983\n",
      "Step: [15733] d_loss: 1.38639379, g_loss: 0.69337875\n",
      "Step: [15734] d_loss: 1.38681495, g_loss: 0.69342756\n",
      "Step: [15735] d_loss: 1.38653040, g_loss: 0.69315481\n",
      "Step: [15736] d_loss: 1.38635385, g_loss: 0.69187009\n",
      "Step: [15737] d_loss: 1.38633728, g_loss: 0.69245386\n",
      "Step: [15738] d_loss: 1.38635778, g_loss: 0.69320476\n",
      "Step: [15739] d_loss: 1.38623345, g_loss: 0.69349819\n",
      "Step: [15740] d_loss: 1.38629222, g_loss: 0.69359875\n",
      "Step: [15741] d_loss: 1.38630867, g_loss: 0.69326806\n",
      "Step: [15742] d_loss: 1.38622403, g_loss: 0.69278264\n",
      "Step: [15743] d_loss: 1.38626695, g_loss: 0.69299936\n",
      "Step: [15744] d_loss: 1.38638258, g_loss: 0.69314563\n",
      "Step: [15745] d_loss: 1.38628590, g_loss: 0.69319379\n",
      "Step: [15746] d_loss: 1.38633323, g_loss: 0.69263947\n",
      "Step: [15747] d_loss: 1.38610435, g_loss: 0.69313598\n",
      "Step: [15748] d_loss: 1.38630712, g_loss: 0.69355202\n",
      "Step: [15749] d_loss: 1.38706040, g_loss: 0.69788080\n",
      "Step: [15750] d_loss: 1.38772190, g_loss: 0.69835812\n",
      "Step: [15751] d_loss: 1.38751030, g_loss: 0.69319850\n",
      "Step: [15752] d_loss: 1.38683796, g_loss: 0.68825096\n",
      "Step: [15753] d_loss: 1.38644779, g_loss: 0.68864334\n",
      "Step: [15754] d_loss: 1.38631320, g_loss: 0.69246709\n",
      "Step: [15755] d_loss: 1.38624263, g_loss: 0.69502407\n",
      "Step: [15756] d_loss: 1.38629425, g_loss: 0.69461989\n",
      "Step: [15757] d_loss: 1.38631082, g_loss: 0.69352221\n",
      "Step: [15758] d_loss: 1.38628101, g_loss: 0.69225121\n",
      "Step: [15759] d_loss: 1.38635659, g_loss: 0.69191748\n",
      "Step: [15760] d_loss: 1.38633609, g_loss: 0.69275296\n",
      "Step: [15761] d_loss: 1.38640964, g_loss: 0.69401932\n",
      "Step: [15762] d_loss: 1.38683331, g_loss: 0.69560421\n",
      "Step: [15763] d_loss: 1.38748074, g_loss: 0.69440049\n",
      "Step: [15764] d_loss: 1.38741827, g_loss: 0.69126320\n",
      "Step: [15765] d_loss: 1.38674808, g_loss: 0.69056612\n",
      "Step: [15766] d_loss: 1.38639486, g_loss: 0.69073784\n",
      "Step: [15767] d_loss: 1.38626957, g_loss: 0.69220579\n",
      "Step: [15768] d_loss: 1.38634920, g_loss: 0.69380587\n",
      "Step: [15769] d_loss: 1.38628554, g_loss: 0.69436848\n",
      "Step: [15770] d_loss: 1.38627052, g_loss: 0.69362056\n",
      "Step: [15771] d_loss: 1.38626671, g_loss: 0.69284564\n",
      "Step: [15772] d_loss: 1.38622677, g_loss: 0.69272047\n",
      "Step: [15773] d_loss: 1.38626122, g_loss: 0.69300765\n",
      "Step: [15774] d_loss: 1.38620985, g_loss: 0.69327730\n",
      "Step: [15775] d_loss: 1.38629150, g_loss: 0.69324982\n",
      "Step: [15776] d_loss: 1.38639426, g_loss: 0.69303858\n",
      "Step: [15777] d_loss: 1.38626194, g_loss: 0.69302046\n",
      "Step: [15778] d_loss: 1.38627684, g_loss: 0.69303370\n",
      "Step: [15779] d_loss: 1.38623524, g_loss: 0.69308400\n",
      "Step: [15780] d_loss: 1.38628006, g_loss: 0.69303644\n",
      "Step: [15781] d_loss: 1.38633478, g_loss: 0.69416100\n",
      "Step: [15782] d_loss: 1.38641715, g_loss: 0.69351918\n",
      "Step: [15783] d_loss: 1.38640809, g_loss: 0.69359827\n",
      "Step: [15784] d_loss: 1.38633573, g_loss: 0.69225669\n",
      "Step: [15785] d_loss: 1.38624048, g_loss: 0.69266164\n",
      "Step: [15786] d_loss: 1.38634443, g_loss: 0.69368798\n",
      "Step: [15787] d_loss: 1.38630915, g_loss: 0.69363284\n",
      "Step: [15788] d_loss: 1.38630795, g_loss: 0.69412792\n",
      "Step: [15789] d_loss: 1.38631248, g_loss: 0.69398618\n",
      "Step: [15790] d_loss: 1.38626468, g_loss: 0.69300973\n",
      "Step: [15791] d_loss: 1.38625145, g_loss: 0.69285488\n",
      "Step: [15792] d_loss: 1.38627326, g_loss: 0.69251573\n",
      "Step: [15793] d_loss: 1.38620758, g_loss: 0.69310665\n",
      "Step: [15794] d_loss: 1.38632083, g_loss: 0.69341671\n",
      "Step: [15795] d_loss: 1.38625789, g_loss: 0.69352901\n",
      "Step: [15796] d_loss: 1.38628638, g_loss: 0.69341743\n",
      "Step: [15797] d_loss: 1.38626099, g_loss: 0.69305247\n",
      "Step: [15798] d_loss: 1.38616431, g_loss: 0.69310808\n",
      "Step: [15799] d_loss: 1.38628888, g_loss: 0.69391125\n",
      "Step: [15800] d_loss: 1.38617289, g_loss: 0.69277191\n",
      "Step: [15801] d_loss: 1.38621628, g_loss: 0.69315082\n",
      "Step: [15802] d_loss: 1.38638234, g_loss: 0.69228101\n",
      "Step: [15803] d_loss: 1.38635898, g_loss: 0.69238383\n",
      "Step: [15804] d_loss: 1.38633847, g_loss: 0.69222641\n",
      "Step: [15805] d_loss: 1.38623035, g_loss: 0.69367069\n",
      "Step: [15806] d_loss: 1.38619304, g_loss: 0.69434953\n",
      "Step: [15807] d_loss: 1.38610697, g_loss: 0.69345146\n",
      "Step: [15808] d_loss: 1.38618243, g_loss: 0.69274443\n",
      "Step: [15809] d_loss: 1.38624740, g_loss: 0.69309574\n",
      "Step: [15810] d_loss: 1.38610315, g_loss: 0.69417268\n",
      "Step: [15811] d_loss: 1.38639641, g_loss: 0.69266993\n",
      "Step: [15812] d_loss: 1.38617492, g_loss: 0.69216830\n",
      "Step: [15813] d_loss: 1.38642192, g_loss: 0.69342852\n",
      "Step: [15814] d_loss: 1.38668418, g_loss: 0.69315195\n",
      "Step: [15815] d_loss: 1.38667583, g_loss: 0.69592196\n",
      "Step: [15816] d_loss: 1.38656449, g_loss: 0.69732761\n",
      "Step: [15817] d_loss: 1.38632011, g_loss: 0.69559926\n",
      "Step: [15818] d_loss: 1.38617468, g_loss: 0.69283587\n",
      "Step: [15819] d_loss: 1.38603449, g_loss: 0.69106597\n",
      "Step: [15820] d_loss: 1.38639355, g_loss: 0.69151980\n",
      "Step: [15821] d_loss: 1.38631463, g_loss: 0.69347113\n",
      "Step: [15822] d_loss: 1.38628912, g_loss: 0.69231248\n",
      "Step: [15823] d_loss: 1.38620806, g_loss: 0.69250989\n",
      "Step: [15824] d_loss: 1.38620317, g_loss: 0.69339108\n",
      "Step: [15825] d_loss: 1.38620138, g_loss: 0.69346941\n",
      "Step: [15826] d_loss: 1.38627815, g_loss: 0.69340301\n",
      "Step: [15827] d_loss: 1.38621438, g_loss: 0.69326538\n",
      "Step: [15828] d_loss: 1.38620210, g_loss: 0.69353503\n",
      "Step: [15829] d_loss: 1.38618183, g_loss: 0.69303918\n",
      "Step: [15830] d_loss: 1.38618588, g_loss: 0.69222319\n",
      "Step: [15831] d_loss: 1.38634944, g_loss: 0.69427800\n",
      "Step: [15832] d_loss: 1.38642311, g_loss: 0.69549406\n",
      "Step: [15833] d_loss: 1.38657069, g_loss: 0.69395083\n",
      "Step: [15834] d_loss: 1.38666642, g_loss: 0.69502497\n",
      "Step: [15835] d_loss: 1.38644743, g_loss: 0.69354188\n",
      "Step: [15836] d_loss: 1.38641584, g_loss: 0.69404066\n",
      "Step: [15837] d_loss: 1.38620019, g_loss: 0.69368052\n",
      "Step: [15838] d_loss: 1.38611722, g_loss: 0.69251031\n",
      "Step: [15839] d_loss: 1.38605595, g_loss: 0.69306600\n",
      "Step: [15840] d_loss: 1.38617122, g_loss: 0.69148552\n",
      "Step: [15841] d_loss: 1.38604689, g_loss: 0.69419849\n",
      "Step: [15842] d_loss: 1.38711762, g_loss: 0.69117296\n",
      "Step: [15843] d_loss: 1.38779867, g_loss: 0.69366109\n",
      "Step: [15844] d_loss: 1.38732183, g_loss: 0.69464952\n",
      "Step: [15845] d_loss: 1.38656795, g_loss: 0.69202697\n",
      "Step: [15846] d_loss: 1.38634253, g_loss: 0.69218063\n",
      "Step: [15847] d_loss: 1.38643372, g_loss: 0.69441259\n",
      "Step: [15848] d_loss: 1.38650656, g_loss: 0.69655335\n",
      "Step: [15849] d_loss: 1.38631678, g_loss: 0.69579566\n",
      "Step: [15850] d_loss: 1.38626361, g_loss: 0.69282317\n",
      "Step: [15851] d_loss: 1.38610852, g_loss: 0.69179273\n",
      "Step: [15852] d_loss: 1.38618696, g_loss: 0.69130039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [15853] d_loss: 1.38634670, g_loss: 0.69304973\n",
      "Step: [15854] d_loss: 1.38604856, g_loss: 0.69382668\n",
      "Step: [15855] d_loss: 1.38628614, g_loss: 0.69292372\n",
      "Step: [15856] d_loss: 1.38636351, g_loss: 0.69283402\n",
      "Step: [15857] d_loss: 1.38651907, g_loss: 0.69162875\n",
      "Step: [15858] d_loss: 1.38643181, g_loss: 0.69352931\n",
      "Step: [15859] d_loss: 1.38645661, g_loss: 0.69392753\n",
      "Step: [15860] d_loss: 1.38637614, g_loss: 0.69421065\n",
      "Step: [15861] d_loss: 1.38614655, g_loss: 0.69342417\n",
      "Step: [15862] d_loss: 1.38628829, g_loss: 0.69286990\n",
      "Step: [15863] d_loss: 1.38619041, g_loss: 0.69278622\n",
      "Step: [15864] d_loss: 1.38623309, g_loss: 0.69333333\n",
      "Step: [15865] d_loss: 1.38558793, g_loss: 0.69292867\n",
      "Step: [15866] d_loss: 1.38620830, g_loss: 0.69298530\n",
      "Step: [15867] d_loss: 1.38653564, g_loss: 0.69407213\n",
      "Step: [15868] d_loss: 1.38594985, g_loss: 0.69296443\n",
      "Step: [15869] d_loss: 1.38621485, g_loss: 0.69266665\n",
      "Step: [15870] d_loss: 1.38651705, g_loss: 0.69296992\n",
      "Step: [15871] d_loss: 1.38611639, g_loss: 0.69350708\n",
      "Step: [15872] d_loss: 1.38620543, g_loss: 0.69256085\n",
      "Step: [15873] d_loss: 1.38648987, g_loss: 0.69251239\n",
      "Step: [15874] d_loss: 1.38702869, g_loss: 0.69291484\n",
      "Step: [15875] d_loss: 1.38689351, g_loss: 0.69056034\n",
      "Step: [15876] d_loss: 1.38675904, g_loss: 0.69357902\n",
      "Step: [15877] d_loss: 1.38633275, g_loss: 0.69511819\n",
      "Step: [15878] d_loss: 1.38633060, g_loss: 0.69546604\n",
      "Step: [15879] d_loss: 1.38622761, g_loss: 0.69384384\n",
      "Step: [15880] d_loss: 1.38616228, g_loss: 0.69174260\n",
      "Step: [15881] d_loss: 1.38617945, g_loss: 0.69396746\n",
      "Step: [15882] d_loss: 1.38653231, g_loss: 0.69242728\n",
      "Step: [15883] d_loss: 1.38691568, g_loss: 0.69399655\n",
      "Step: [15884] d_loss: 1.38643861, g_loss: 0.69354427\n",
      "Step: [15885] d_loss: 1.38648939, g_loss: 0.69348764\n",
      "Step: [15886] d_loss: 1.38634562, g_loss: 0.69231331\n",
      "Step: [15887] d_loss: 1.38667369, g_loss: 0.69166285\n",
      "Step: [15888] d_loss: 1.38649642, g_loss: 0.69234186\n",
      "Step: [15889] d_loss: 1.38656211, g_loss: 0.69483733\n",
      "Step: [15890] d_loss: 1.38633537, g_loss: 0.69648021\n",
      "Step: [15891] d_loss: 1.38637018, g_loss: 0.69417739\n",
      "Step: [15892] d_loss: 1.38632631, g_loss: 0.69290543\n",
      "Step: [15893] d_loss: 1.38618040, g_loss: 0.69230813\n",
      "Step: [15894] d_loss: 1.38627815, g_loss: 0.69271016\n",
      "Step: [15895] d_loss: 1.38636470, g_loss: 0.69332856\n",
      "Step: [15896] d_loss: 1.38630307, g_loss: 0.69331777\n",
      "Step: [15897] d_loss: 1.38633764, g_loss: 0.69338173\n",
      "Step: [15898] d_loss: 1.38621831, g_loss: 0.69312209\n",
      "Step: [15899] d_loss: 1.38629508, g_loss: 0.69301248\n",
      "Step: [15900] d_loss: 1.38625932, g_loss: 0.69315183\n",
      "Step: [15901] d_loss: 1.38625896, g_loss: 0.69321489\n",
      "Step: [15902] d_loss: 1.38665390, g_loss: 0.69616216\n",
      "Step: [15903] d_loss: 1.38649726, g_loss: 0.69569153\n",
      "Step: [15904] d_loss: 1.38637757, g_loss: 0.69547844\n",
      "Step: [15905] d_loss: 1.38632512, g_loss: 0.69412208\n",
      "Step: [15906] d_loss: 1.38619208, g_loss: 0.69254154\n",
      "Step: [15907] d_loss: 1.38649559, g_loss: 0.69381475\n",
      "Step: [15908] d_loss: 1.38713312, g_loss: 0.69373810\n",
      "Step: [15909] d_loss: 1.38744712, g_loss: 0.69726187\n",
      "Step: [15910] d_loss: 1.38707364, g_loss: 0.69493777\n",
      "Step: [15911] d_loss: 1.38668978, g_loss: 0.69440973\n",
      "Step: [15912] d_loss: 1.38643444, g_loss: 0.69413686\n",
      "Step: [15913] d_loss: 1.38637662, g_loss: 0.69383466\n",
      "Step: [15914] d_loss: 1.38626719, g_loss: 0.69293648\n",
      "Step: [15915] d_loss: 1.38628042, g_loss: 0.69264525\n",
      "Step: [15916] d_loss: 1.38625097, g_loss: 0.69251955\n",
      "Step: [15917] d_loss: 1.38628638, g_loss: 0.69321758\n",
      "Step: [15918] d_loss: 1.38631821, g_loss: 0.69336832\n",
      "Step: [15919] d_loss: 1.38621473, g_loss: 0.69314241\n",
      "Step: [15920] d_loss: 1.38637841, g_loss: 0.69269729\n",
      "Step: [15921] d_loss: 1.38618994, g_loss: 0.69369209\n",
      "Step: [15922] d_loss: 1.38630152, g_loss: 0.69343966\n",
      "Step: [15923] d_loss: 1.38633442, g_loss: 0.69276118\n",
      "Step: [15924] d_loss: 1.38617158, g_loss: 0.69269037\n",
      "Step: [15925] d_loss: 1.38620269, g_loss: 0.69306833\n",
      "Step: [15926] d_loss: 1.38629556, g_loss: 0.69365180\n",
      "Step: [15927] d_loss: 1.38641059, g_loss: 0.69334972\n",
      "Step: [15928] d_loss: 1.38635814, g_loss: 0.69227564\n",
      "Step: [15929] d_loss: 1.38643205, g_loss: 0.69064283\n",
      "Step: [15930] d_loss: 1.38628912, g_loss: 0.69254529\n",
      "Step: [15931] d_loss: 1.38636780, g_loss: 0.69451648\n",
      "Step: [15932] d_loss: 1.38617539, g_loss: 0.69246590\n",
      "Step: [15933] d_loss: 1.38648665, g_loss: 0.69475091\n",
      "Step: [15934] d_loss: 1.38635921, g_loss: 0.69275546\n",
      "Step: [15935] d_loss: 1.38641417, g_loss: 0.69192004\n",
      "Step: [15936] d_loss: 1.38625836, g_loss: 0.69265234\n",
      "Step: [15937] d_loss: 1.38635969, g_loss: 0.69351065\n",
      "Step: [15938] d_loss: 1.38634133, g_loss: 0.69404459\n",
      "Step: [15939] d_loss: 1.38645124, g_loss: 0.69332826\n",
      "Step: [15940] d_loss: 1.38678122, g_loss: 0.69289297\n",
      "Step: [15941] d_loss: 1.38673770, g_loss: 0.69478393\n",
      "Step: [15942] d_loss: 1.38650560, g_loss: 0.69428635\n",
      "Step: [15943] d_loss: 1.38628554, g_loss: 0.69341528\n",
      "Step: [15944] d_loss: 1.38635683, g_loss: 0.69118083\n",
      "Step: [15945] d_loss: 1.38625765, g_loss: 0.69258356\n",
      "Step: [15946] d_loss: 1.38628554, g_loss: 0.69298112\n",
      "Step: [15947] d_loss: 1.38627958, g_loss: 0.69357574\n",
      "Step: [15948] d_loss: 1.38632059, g_loss: 0.69265634\n",
      "Step: [15949] d_loss: 1.38621390, g_loss: 0.69394636\n",
      "Step: [15950] d_loss: 1.38621426, g_loss: 0.69394851\n",
      "Step: [15951] d_loss: 1.38622367, g_loss: 0.69311464\n",
      "Step: [15952] d_loss: 1.38614976, g_loss: 0.69261175\n",
      "Step: [15953] d_loss: 1.38621497, g_loss: 0.69266331\n",
      "Step: [15954] d_loss: 1.38604343, g_loss: 0.69354808\n",
      "Step: [15955] d_loss: 1.38623309, g_loss: 0.69301873\n",
      "Step: [15956] d_loss: 1.38624978, g_loss: 0.69328618\n",
      "Step: [15957] d_loss: 1.38623238, g_loss: 0.69275475\n",
      "Step: [15958] d_loss: 1.38695407, g_loss: 0.69215310\n",
      "Step: [15959] d_loss: 1.38624704, g_loss: 0.69498229\n",
      "Step: [15960] d_loss: 1.38627291, g_loss: 0.69367254\n",
      "Step: [15961] d_loss: 1.38637209, g_loss: 0.69332957\n",
      "Step: [15962] d_loss: 1.38636088, g_loss: 0.69225717\n",
      "Step: [15963] d_loss: 1.38634884, g_loss: 0.69293308\n",
      "Step: [15964] d_loss: 1.38632643, g_loss: 0.69303465\n",
      "Step: [15965] d_loss: 1.38626838, g_loss: 0.69331306\n",
      "Step: [15966] d_loss: 1.38629746, g_loss: 0.69326431\n",
      "Step: [15967] d_loss: 1.38633060, g_loss: 0.69317245\n",
      "Step: [15968] d_loss: 1.38628304, g_loss: 0.69332248\n",
      "Step: [15969] d_loss: 1.38617110, g_loss: 0.69324958\n",
      "Step: [15970] d_loss: 1.38627243, g_loss: 0.69344532\n",
      "Step: [15971] d_loss: 1.38623571, g_loss: 0.69345915\n",
      "Step: [15972] d_loss: 1.38633764, g_loss: 0.69297588\n",
      "Step: [15973] d_loss: 1.38633323, g_loss: 0.69247127\n",
      "Step: [15974] d_loss: 1.38622367, g_loss: 0.69337547\n",
      "Step: [15975] d_loss: 1.38642716, g_loss: 0.69385612\n",
      "Step: [15976] d_loss: 1.38674378, g_loss: 0.69252753\n",
      "Step: [15977] d_loss: 1.38677549, g_loss: 0.69405818\n",
      "Step: [15978] d_loss: 1.38659859, g_loss: 0.69188839\n",
      "Step: [15979] d_loss: 1.38640678, g_loss: 0.69318384\n",
      "Step: [15980] d_loss: 1.38630486, g_loss: 0.69292855\n",
      "Step: [15981] d_loss: 1.38627815, g_loss: 0.69284934\n",
      "Step: [15982] d_loss: 1.38619852, g_loss: 0.69273400\n",
      "Step: [15983] d_loss: 1.38629556, g_loss: 0.69306016\n",
      "Step: [15984] d_loss: 1.38631892, g_loss: 0.69320613\n",
      "Step: [15985] d_loss: 1.38625360, g_loss: 0.69387156\n",
      "Step: [15986] d_loss: 1.38642418, g_loss: 0.69271535\n",
      "Step: [15987] d_loss: 1.38637376, g_loss: 0.69298875\n",
      "Step: [15988] d_loss: 1.38632941, g_loss: 0.69345391\n",
      "Step: [15989] d_loss: 1.38638353, g_loss: 0.69323790\n",
      "Step: [15990] d_loss: 1.38630199, g_loss: 0.69366658\n",
      "Step: [15991] d_loss: 1.38635564, g_loss: 0.69293523\n",
      "Step: [15992] d_loss: 1.38625264, g_loss: 0.69308019\n",
      "Step: [15993] d_loss: 1.38623667, g_loss: 0.69308883\n",
      "Step: [15994] d_loss: 1.38628888, g_loss: 0.69312322\n",
      "Step: [15995] d_loss: 1.38626766, g_loss: 0.69302714\n",
      "Step: [15996] d_loss: 1.38630295, g_loss: 0.69326472\n",
      "Step: [15997] d_loss: 1.38639450, g_loss: 0.69322163\n",
      "Step: [15998] d_loss: 1.38620424, g_loss: 0.69325364\n",
      "Step: [15999] d_loss: 1.38632846, g_loss: 0.69303846\n",
      "Step: [16000] d_loss: 1.38632202, g_loss: 0.69300878\n",
      "Step: [16001] d_loss: 1.38629270, g_loss: 0.69283020\n",
      "Step: [16002] d_loss: 1.38624692, g_loss: 0.69301087\n",
      "Step: [16003] d_loss: 1.38621831, g_loss: 0.69293571\n",
      "Step: [16004] d_loss: 1.38627994, g_loss: 0.69334829\n",
      "Step: [16005] d_loss: 1.38621759, g_loss: 0.69335413\n",
      "Step: [16006] d_loss: 1.38631558, g_loss: 0.69318020\n",
      "Step: [16007] d_loss: 1.38630056, g_loss: 0.69294220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16008] d_loss: 1.38624406, g_loss: 0.69289041\n",
      "Step: [16009] d_loss: 1.38625002, g_loss: 0.69360501\n",
      "Step: [16010] d_loss: 1.38623953, g_loss: 0.69337475\n",
      "Step: [16011] d_loss: 1.38637447, g_loss: 0.69327426\n",
      "Step: [16012] d_loss: 1.38647282, g_loss: 0.69293159\n",
      "Step: [16013] d_loss: 1.38638830, g_loss: 0.69321781\n",
      "Step: [16014] d_loss: 1.38621867, g_loss: 0.69379497\n",
      "Step: [16015] d_loss: 1.38626838, g_loss: 0.69367892\n",
      "Step: [16016] d_loss: 1.38626981, g_loss: 0.69319093\n",
      "Step: [16017] d_loss: 1.38633513, g_loss: 0.69267023\n",
      "Step: [16018] d_loss: 1.38630295, g_loss: 0.69301605\n",
      "Step: [16019] d_loss: 1.38628221, g_loss: 0.69295162\n",
      "Step: [16020] d_loss: 1.38634145, g_loss: 0.69306701\n",
      "Step: [16021] d_loss: 1.38637030, g_loss: 0.69273657\n",
      "Step: [16022] d_loss: 1.38624322, g_loss: 0.69334114\n",
      "Step: [16023] d_loss: 1.38624716, g_loss: 0.69287318\n",
      "Step: [16024] d_loss: 1.38633418, g_loss: 0.69150907\n",
      "Step: [16025] d_loss: 1.38631153, g_loss: 0.69220632\n",
      "Step: [16026] d_loss: 1.38635302, g_loss: 0.69381058\n",
      "Step: [16027] d_loss: 1.38641107, g_loss: 0.69327390\n",
      "Step: [16028] d_loss: 1.38636410, g_loss: 0.69406796\n",
      "Step: [16029] d_loss: 1.38637114, g_loss: 0.69284749\n",
      "Step: [16030] d_loss: 1.38636386, g_loss: 0.69359756\n",
      "Step: [16031] d_loss: 1.38634503, g_loss: 0.69370770\n",
      "Step: [16032] d_loss: 1.38630509, g_loss: 0.69342494\n",
      "Step: [16033] d_loss: 1.38623953, g_loss: 0.69322062\n",
      "Step: [16034] d_loss: 1.38634729, g_loss: 0.69287682\n",
      "Step: [16035] d_loss: 1.38629031, g_loss: 0.69317096\n",
      "Step: [16036] d_loss: 1.38630843, g_loss: 0.69306982\n",
      "Step: [16037] d_loss: 1.38624644, g_loss: 0.69322121\n",
      "Step: [16038] d_loss: 1.38628185, g_loss: 0.69312000\n",
      "Step: [16039] d_loss: 1.38627505, g_loss: 0.69323039\n",
      "Step: [16040] d_loss: 1.38631940, g_loss: 0.69321716\n",
      "Step: [16041] d_loss: 1.38626921, g_loss: 0.69373393\n",
      "Step: [16042] d_loss: 1.38623142, g_loss: 0.69337118\n",
      "Step: [16043] d_loss: 1.38640332, g_loss: 0.69298458\n",
      "Step: [16044] d_loss: 1.38629866, g_loss: 0.69342220\n",
      "Step: [16045] d_loss: 1.38631284, g_loss: 0.69310951\n",
      "Step: [16046] d_loss: 1.38632941, g_loss: 0.69328111\n",
      "Step: [16047] d_loss: 1.38624811, g_loss: 0.69325662\n",
      "Step: [16048] d_loss: 1.38624477, g_loss: 0.69319832\n",
      "Step: [16049] d_loss: 1.38624859, g_loss: 0.69314998\n",
      "Step: [16050] d_loss: 1.38635278, g_loss: 0.69298702\n",
      "Step: [16051] d_loss: 1.38630354, g_loss: 0.69325602\n",
      "Step: [16052] d_loss: 1.38633800, g_loss: 0.69304490\n",
      "Step: [16053] d_loss: 1.38628340, g_loss: 0.69326073\n",
      "Step: [16054] d_loss: 1.38629484, g_loss: 0.69303930\n",
      "Step: [16055] d_loss: 1.38622308, g_loss: 0.69319630\n",
      "Step: [16056] d_loss: 1.38632751, g_loss: 0.69318444\n",
      "Step: [16057] d_loss: 1.38630056, g_loss: 0.69328070\n",
      "Step: [16058] d_loss: 1.38630235, g_loss: 0.69331962\n",
      "Step: [16059] d_loss: 1.38624513, g_loss: 0.69326341\n",
      "Step: [16060] d_loss: 1.38626456, g_loss: 0.69318014\n",
      "Step: [16061] d_loss: 1.38631129, g_loss: 0.69319248\n",
      "Step: [16062] d_loss: 1.38628423, g_loss: 0.69306779\n",
      "Step: [16063] d_loss: 1.38624680, g_loss: 0.69320232\n",
      "Step: [16064] d_loss: 1.38627768, g_loss: 0.69316614\n",
      "Step: [16065] d_loss: 1.38633180, g_loss: 0.69281828\n",
      "Step: [16066] d_loss: 1.38629341, g_loss: 0.69339025\n",
      "Step: [16067] d_loss: 1.38626432, g_loss: 0.69306761\n",
      "Step: [16068] d_loss: 1.38626337, g_loss: 0.69327646\n",
      "Step: [16069] d_loss: 1.38625026, g_loss: 0.69303536\n",
      "Step: [16070] d_loss: 1.38624573, g_loss: 0.69340646\n",
      "Step: [16071] d_loss: 1.38624144, g_loss: 0.69320393\n",
      "Step: [16072] d_loss: 1.38627517, g_loss: 0.69324756\n",
      "Step: [16073] d_loss: 1.38623357, g_loss: 0.69306874\n",
      "Step: [16074] d_loss: 1.38621342, g_loss: 0.69276118\n",
      "Step: [16075] d_loss: 1.38620734, g_loss: 0.69411063\n",
      "Step: [16076] d_loss: 1.38665581, g_loss: 0.69259995\n",
      "Step: [16077] d_loss: 1.38707817, g_loss: 0.69526213\n",
      "Step: [16078] d_loss: 1.38717401, g_loss: 0.69154882\n",
      "Step: [16079] d_loss: 1.38703465, g_loss: 0.69418573\n",
      "Step: [16080] d_loss: 1.38678598, g_loss: 0.69324034\n",
      "Step: [16081] d_loss: 1.38662839, g_loss: 0.69532371\n",
      "Step: [16082] d_loss: 1.38642061, g_loss: 0.69526494\n",
      "Step: [16083] d_loss: 1.38626814, g_loss: 0.69373292\n",
      "Step: [16084] d_loss: 1.38626170, g_loss: 0.69188595\n",
      "Step: [16085] d_loss: 1.38632274, g_loss: 0.69142246\n",
      "Step: [16086] d_loss: 1.38635075, g_loss: 0.69287592\n",
      "Step: [16087] d_loss: 1.38635850, g_loss: 0.69327211\n",
      "Step: [16088] d_loss: 1.38634717, g_loss: 0.69247711\n",
      "Step: [16089] d_loss: 1.38638330, g_loss: 0.69179404\n",
      "Step: [16090] d_loss: 1.38628864, g_loss: 0.69288087\n",
      "Step: [16091] d_loss: 1.38629866, g_loss: 0.69272149\n",
      "Step: [16092] d_loss: 1.38630128, g_loss: 0.69248545\n",
      "Step: [16093] d_loss: 1.38623357, g_loss: 0.69300187\n",
      "Step: [16094] d_loss: 1.38622236, g_loss: 0.69402421\n",
      "Step: [16095] d_loss: 1.38625622, g_loss: 0.69374752\n",
      "Step: [16096] d_loss: 1.38623011, g_loss: 0.69249392\n",
      "Step: [16097] d_loss: 1.38623571, g_loss: 0.69279230\n",
      "Step: [16098] d_loss: 1.38627863, g_loss: 0.69308698\n",
      "Step: [16099] d_loss: 1.38627481, g_loss: 0.69355917\n",
      "Step: [16100] d_loss: 1.38626349, g_loss: 0.69346786\n",
      "Step: [16101] d_loss: 1.38626766, g_loss: 0.69326377\n",
      "Step: [16102] d_loss: 1.38628674, g_loss: 0.69306487\n",
      "Step: [16103] d_loss: 1.38629699, g_loss: 0.69309068\n",
      "Step: [16104] d_loss: 1.38627541, g_loss: 0.69326472\n",
      "Step: [16105] d_loss: 1.38617814, g_loss: 0.69331151\n",
      "Step: [16106] d_loss: 1.38629580, g_loss: 0.69325930\n",
      "Step: [16107] d_loss: 1.38625169, g_loss: 0.69312036\n",
      "Step: [16108] d_loss: 1.38623178, g_loss: 0.69310772\n",
      "Step: [16109] d_loss: 1.38623726, g_loss: 0.69335151\n",
      "Step: [16110] d_loss: 1.38622999, g_loss: 0.69356686\n",
      "Step: [16111] d_loss: 1.38659108, g_loss: 0.69356620\n",
      "Step: [16112] d_loss: 1.38636279, g_loss: 0.69153690\n",
      "Step: [16113] d_loss: 1.38680744, g_loss: 0.69457835\n",
      "Step: [16114] d_loss: 1.38692522, g_loss: 0.69246453\n",
      "Step: [16115] d_loss: 1.38691056, g_loss: 0.69324279\n",
      "Step: [16116] d_loss: 1.38713312, g_loss: 0.69488794\n",
      "Step: [16117] d_loss: 1.38710022, g_loss: 0.69761705\n",
      "Step: [16118] d_loss: 1.38672972, g_loss: 0.69571435\n",
      "Step: [16119] d_loss: 1.38644063, g_loss: 0.69259095\n",
      "Step: [16120] d_loss: 1.38641322, g_loss: 0.69174182\n",
      "Step: [16121] d_loss: 1.38629913, g_loss: 0.69369543\n",
      "Step: [16122] d_loss: 1.38696313, g_loss: 0.69775140\n",
      "Step: [16123] d_loss: 1.38808990, g_loss: 0.69238782\n",
      "Step: [16124] d_loss: 1.38806605, g_loss: 0.69226092\n",
      "Step: [16125] d_loss: 1.38752842, g_loss: 0.68882990\n",
      "Step: [16126] d_loss: 1.38680422, g_loss: 0.68914044\n",
      "Step: [16127] d_loss: 1.38634801, g_loss: 0.69118047\n",
      "Step: [16128] d_loss: 1.38625360, g_loss: 0.69263792\n",
      "Step: [16129] d_loss: 1.38632607, g_loss: 0.69417191\n",
      "Step: [16130] d_loss: 1.38628113, g_loss: 0.69414306\n",
      "Step: [16131] d_loss: 1.38623977, g_loss: 0.69298542\n",
      "Step: [16132] d_loss: 1.38630176, g_loss: 0.69279522\n",
      "Step: [16133] d_loss: 1.38629842, g_loss: 0.69371617\n",
      "Step: [16134] d_loss: 1.38624823, g_loss: 0.69325644\n",
      "Step: [16135] d_loss: 1.38624644, g_loss: 0.69347632\n",
      "Step: [16136] d_loss: 1.38623452, g_loss: 0.69310689\n",
      "Step: [16137] d_loss: 1.38627112, g_loss: 0.69306946\n",
      "Step: [16138] d_loss: 1.38611555, g_loss: 0.69312626\n",
      "Step: [16139] d_loss: 1.38628054, g_loss: 0.69276059\n",
      "Step: [16140] d_loss: 1.38651383, g_loss: 0.69348216\n",
      "Step: [16141] d_loss: 1.38621223, g_loss: 0.69320345\n",
      "Step: [16142] d_loss: 1.38628030, g_loss: 0.69339406\n",
      "Step: [16143] d_loss: 1.38620114, g_loss: 0.69292486\n",
      "Step: [16144] d_loss: 1.38635695, g_loss: 0.69272524\n",
      "Step: [16145] d_loss: 1.38627791, g_loss: 0.69306660\n",
      "Step: [16146] d_loss: 1.38620949, g_loss: 0.69277513\n",
      "Step: [16147] d_loss: 1.38626528, g_loss: 0.69395220\n",
      "Step: [16148] d_loss: 1.38621879, g_loss: 0.69383258\n",
      "Step: [16149] d_loss: 1.38617551, g_loss: 0.69309926\n",
      "Step: [16150] d_loss: 1.38649142, g_loss: 0.69302863\n",
      "Step: [16151] d_loss: 1.38683295, g_loss: 0.69107199\n",
      "Step: [16152] d_loss: 1.38656306, g_loss: 0.69261909\n",
      "Step: [16153] d_loss: 1.38631725, g_loss: 0.69553685\n",
      "Step: [16154] d_loss: 1.38658762, g_loss: 0.69314504\n",
      "Step: [16155] d_loss: 1.38652241, g_loss: 0.69192231\n",
      "Step: [16156] d_loss: 1.38659549, g_loss: 0.69437069\n",
      "Step: [16157] d_loss: 1.38685918, g_loss: 0.69167030\n",
      "Step: [16158] d_loss: 1.38701093, g_loss: 0.69535106\n",
      "Step: [16159] d_loss: 1.38709056, g_loss: 0.69571841\n",
      "Step: [16160] d_loss: 1.38668084, g_loss: 0.69247568\n",
      "Step: [16161] d_loss: 1.38638592, g_loss: 0.69108605\n",
      "Step: [16162] d_loss: 1.38637483, g_loss: 0.69112074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16163] d_loss: 1.38630247, g_loss: 0.69331199\n",
      "Step: [16164] d_loss: 1.38619876, g_loss: 0.69439143\n",
      "Step: [16165] d_loss: 1.38630724, g_loss: 0.69417977\n",
      "Step: [16166] d_loss: 1.38632166, g_loss: 0.69333160\n",
      "Step: [16167] d_loss: 1.38619709, g_loss: 0.69269514\n",
      "Step: [16168] d_loss: 1.38621402, g_loss: 0.69269842\n",
      "Step: [16169] d_loss: 1.38627791, g_loss: 0.69348788\n",
      "Step: [16170] d_loss: 1.38630545, g_loss: 0.69183457\n",
      "Step: [16171] d_loss: 1.38645411, g_loss: 0.69384134\n",
      "Step: [16172] d_loss: 1.38700759, g_loss: 0.69482142\n",
      "Step: [16173] d_loss: 1.38778782, g_loss: 0.69286776\n",
      "Step: [16174] d_loss: 1.38758183, g_loss: 0.69527137\n",
      "Step: [16175] d_loss: 1.38708830, g_loss: 0.69487488\n",
      "Step: [16176] d_loss: 1.38666749, g_loss: 0.69469857\n",
      "Step: [16177] d_loss: 1.38634825, g_loss: 0.69318408\n",
      "Step: [16178] d_loss: 1.38624001, g_loss: 0.69286579\n",
      "Step: [16179] d_loss: 1.38621402, g_loss: 0.69271302\n",
      "Step: [16180] d_loss: 1.38637102, g_loss: 0.69401938\n",
      "Step: [16181] d_loss: 1.38658822, g_loss: 0.69483620\n",
      "Step: [16182] d_loss: 1.38622868, g_loss: 0.69436669\n",
      "Step: [16183] d_loss: 1.38632321, g_loss: 0.69320375\n",
      "Step: [16184] d_loss: 1.38621831, g_loss: 0.69248056\n",
      "Step: [16185] d_loss: 1.38630509, g_loss: 0.69328833\n",
      "Step: [16186] d_loss: 1.38633585, g_loss: 0.69248283\n",
      "Step: [16187] d_loss: 1.38650501, g_loss: 0.69293022\n",
      "Step: [16188] d_loss: 1.38660157, g_loss: 0.69432211\n",
      "Step: [16189] d_loss: 1.38627577, g_loss: 0.69350564\n",
      "Step: [16190] d_loss: 1.38638926, g_loss: 0.69277060\n",
      "Step: [16191] d_loss: 1.38616896, g_loss: 0.69437766\n",
      "Step: [16192] d_loss: 1.38629079, g_loss: 0.69704771\n",
      "Step: [16193] d_loss: 1.38646412, g_loss: 0.69414389\n",
      "Step: [16194] d_loss: 1.38637245, g_loss: 0.69166994\n",
      "Step: [16195] d_loss: 1.38635063, g_loss: 0.69222510\n",
      "Step: [16196] d_loss: 1.38615906, g_loss: 0.69401681\n",
      "Step: [16197] d_loss: 1.38639998, g_loss: 0.69445282\n",
      "Step: [16198] d_loss: 1.38647366, g_loss: 0.69295204\n",
      "Step: [16199] d_loss: 1.38639843, g_loss: 0.69360495\n",
      "Step: [16200] d_loss: 1.38640189, g_loss: 0.69272447\n",
      "Step: [16201] d_loss: 1.38626552, g_loss: 0.69343126\n",
      "Step: [16202] d_loss: 1.38662374, g_loss: 0.69281209\n",
      "Step: [16203] d_loss: 1.38609576, g_loss: 0.69206703\n",
      "Step: [16204] d_loss: 1.38612437, g_loss: 0.69292629\n",
      "Step: [16205] d_loss: 1.38648105, g_loss: 0.69353276\n",
      "Step: [16206] d_loss: 1.38651919, g_loss: 0.69369602\n",
      "Step: [16207] d_loss: 1.38652539, g_loss: 0.69217646\n",
      "Step: [16208] d_loss: 1.38641477, g_loss: 0.69217300\n",
      "Step: [16209] d_loss: 1.38650525, g_loss: 0.69226372\n",
      "Step: [16210] d_loss: 1.38635826, g_loss: 0.69259363\n",
      "Step: [16211] d_loss: 1.38629484, g_loss: 0.69309533\n",
      "Step: [16212] d_loss: 1.38633919, g_loss: 0.69441164\n",
      "Step: [16213] d_loss: 1.38646221, g_loss: 0.69383502\n",
      "Step: [16214] d_loss: 1.38735080, g_loss: 0.69764906\n",
      "Step: [16215] d_loss: 1.38819027, g_loss: 0.69543660\n",
      "Step: [16216] d_loss: 1.38784099, g_loss: 0.69589752\n",
      "Step: [16217] d_loss: 1.38750887, g_loss: 0.69168061\n",
      "Step: [16218] d_loss: 1.38696826, g_loss: 0.69185108\n",
      "Step: [16219] d_loss: 1.38654900, g_loss: 0.69139373\n",
      "Step: [16220] d_loss: 1.38634825, g_loss: 0.69285613\n",
      "Step: [16221] d_loss: 1.38639331, g_loss: 0.69358188\n",
      "Step: [16222] d_loss: 1.38626301, g_loss: 0.69366437\n",
      "Step: [16223] d_loss: 1.38626671, g_loss: 0.69333404\n",
      "Step: [16224] d_loss: 1.38624310, g_loss: 0.69283038\n",
      "Step: [16225] d_loss: 1.38626528, g_loss: 0.69291592\n",
      "Step: [16226] d_loss: 1.38624597, g_loss: 0.69277447\n",
      "Step: [16227] d_loss: 1.38626277, g_loss: 0.69270939\n",
      "Step: [16228] d_loss: 1.38635588, g_loss: 0.69413698\n",
      "Step: [16229] d_loss: 1.38628340, g_loss: 0.69314563\n",
      "Step: [16230] d_loss: 1.38623226, g_loss: 0.69493294\n",
      "Step: [16231] d_loss: 1.38670707, g_loss: 0.69227028\n",
      "Step: [16232] d_loss: 1.38675761, g_loss: 0.69383711\n",
      "Step: [16233] d_loss: 1.38666081, g_loss: 0.69278526\n",
      "Step: [16234] d_loss: 1.38637233, g_loss: 0.69032103\n",
      "Step: [16235] d_loss: 1.38620305, g_loss: 0.69092572\n",
      "Step: [16236] d_loss: 1.38621283, g_loss: 0.69227588\n",
      "Step: [16237] d_loss: 1.38624072, g_loss: 0.69373894\n",
      "Step: [16238] d_loss: 1.38625073, g_loss: 0.69478559\n",
      "Step: [16239] d_loss: 1.38638711, g_loss: 0.69473964\n",
      "Step: [16240] d_loss: 1.38633251, g_loss: 0.69372654\n",
      "Step: [16241] d_loss: 1.38631964, g_loss: 0.69252968\n",
      "Step: [16242] d_loss: 1.38630152, g_loss: 0.69246686\n",
      "Step: [16243] d_loss: 1.38627613, g_loss: 0.69315767\n",
      "Step: [16244] d_loss: 1.38612843, g_loss: 0.69329065\n",
      "Step: [16245] d_loss: 1.38628840, g_loss: 0.69299418\n",
      "Step: [16246] d_loss: 1.38631308, g_loss: 0.69324642\n",
      "Step: [16247] d_loss: 1.38624215, g_loss: 0.69340909\n",
      "Step: [16248] d_loss: 1.38629818, g_loss: 0.69319618\n",
      "Step: [16249] d_loss: 1.38621259, g_loss: 0.69294912\n",
      "Step: [16250] d_loss: 1.38625240, g_loss: 0.69301355\n",
      "Step: [16251] d_loss: 1.38630962, g_loss: 0.69322032\n",
      "Step: [16252] d_loss: 1.38627028, g_loss: 0.69316012\n",
      "Step: [16253] d_loss: 1.38645387, g_loss: 0.69307315\n",
      "Step: [16254] d_loss: 1.38624978, g_loss: 0.69322246\n",
      "Step: [16255] d_loss: 1.38624430, g_loss: 0.69333816\n",
      "Step: [16256] d_loss: 1.38633132, g_loss: 0.69303310\n",
      "Step: [16257] d_loss: 1.38631558, g_loss: 0.69324988\n",
      "Step: [16258] d_loss: 1.38630533, g_loss: 0.69317371\n",
      "Step: [16259] d_loss: 1.38626647, g_loss: 0.69307435\n",
      "Step: [16260] d_loss: 1.38624656, g_loss: 0.69299543\n",
      "Step: [16261] d_loss: 1.38622677, g_loss: 0.69371343\n",
      "Step: [16262] d_loss: 1.38641596, g_loss: 0.69327939\n",
      "Step: [16263] d_loss: 1.38632894, g_loss: 0.69289637\n",
      "Step: [16264] d_loss: 1.38627303, g_loss: 0.69312811\n",
      "Step: [16265] d_loss: 1.38657534, g_loss: 0.69283903\n",
      "Step: [16266] d_loss: 1.38640666, g_loss: 0.69317031\n",
      "Step: [16267] d_loss: 1.38634539, g_loss: 0.69322199\n",
      "Step: [16268] d_loss: 1.38627052, g_loss: 0.69332600\n",
      "Step: [16269] d_loss: 1.38626885, g_loss: 0.69316816\n",
      "Step: [16270] d_loss: 1.38626158, g_loss: 0.69300818\n",
      "Step: [16271] d_loss: 1.38628471, g_loss: 0.69323850\n",
      "Step: [16272] d_loss: 1.38625419, g_loss: 0.69343209\n",
      "Step: [16273] d_loss: 1.38638580, g_loss: 0.69327927\n",
      "Step: [16274] d_loss: 1.38632762, g_loss: 0.69304478\n",
      "Step: [16275] d_loss: 1.38630879, g_loss: 0.69309115\n",
      "Step: [16276] d_loss: 1.38627017, g_loss: 0.69326842\n",
      "Step: [16277] d_loss: 1.38628304, g_loss: 0.69377410\n",
      "Step: [16278] d_loss: 1.38628650, g_loss: 0.69344413\n",
      "Step: [16279] d_loss: 1.38617992, g_loss: 0.69323903\n",
      "Step: [16280] d_loss: 1.38626862, g_loss: 0.69277155\n",
      "Step: [16281] d_loss: 1.38625979, g_loss: 0.69317675\n",
      "Step: [16282] d_loss: 1.38621199, g_loss: 0.69316918\n",
      "Step: [16283] d_loss: 1.38625717, g_loss: 0.69314730\n",
      "Step: [16284] d_loss: 1.38629532, g_loss: 0.69319707\n",
      "Step: [16285] d_loss: 1.38624501, g_loss: 0.69344759\n",
      "Step: [16286] d_loss: 1.38621497, g_loss: 0.69339544\n",
      "Step: [16287] d_loss: 1.38629889, g_loss: 0.69305921\n",
      "Step: [16288] d_loss: 1.38631558, g_loss: 0.69329000\n",
      "Step: [16289] d_loss: 1.38619697, g_loss: 0.69400924\n",
      "Step: [16290] d_loss: 1.38631296, g_loss: 0.69349504\n",
      "Step: [16291] d_loss: 1.38630080, g_loss: 0.69302362\n",
      "Step: [16292] d_loss: 1.38626862, g_loss: 0.69298458\n",
      "Step: [16293] d_loss: 1.38624454, g_loss: 0.69359779\n",
      "Step: [16294] d_loss: 1.38625658, g_loss: 0.69326878\n",
      "Step: [16295] d_loss: 1.38762760, g_loss: 0.69322646\n",
      "Step: [16296] d_loss: 1.38629293, g_loss: 0.69330609\n",
      "Step: [16297] d_loss: 1.38625371, g_loss: 0.69304109\n",
      "Step: [16298] d_loss: 1.44201148, g_loss: 0.71386838\n",
      "Step: [16299] d_loss: 1.39887452, g_loss: 0.69588816\n",
      "Step: [16300] d_loss: 1.39918506, g_loss: 0.69984198\n",
      "Step: [16301] d_loss: 1.39233351, g_loss: 0.70395368\n",
      "Step: [16302] d_loss: 1.38690591, g_loss: 0.69971520\n",
      "Step: [16303] d_loss: 1.38774729, g_loss: 0.69415689\n",
      "Step: [16304] d_loss: 1.38977814, g_loss: 0.69297177\n",
      "Step: [16305] d_loss: 1.38798702, g_loss: 0.68988311\n",
      "Step: [16306] d_loss: 1.38690627, g_loss: 0.69392169\n",
      "Step: [16307] d_loss: 1.38639617, g_loss: 0.69545519\n",
      "Step: [16308] d_loss: 1.38648164, g_loss: 0.69462037\n",
      "Step: [16309] d_loss: 1.38644814, g_loss: 0.69341624\n",
      "Step: [16310] d_loss: 1.38641953, g_loss: 0.69226438\n",
      "Step: [16311] d_loss: 1.38642871, g_loss: 0.69164616\n",
      "Step: [16312] d_loss: 1.38638711, g_loss: 0.69367343\n",
      "Step: [16313] d_loss: 1.38637257, g_loss: 0.69421881\n",
      "Step: [16314] d_loss: 1.38633776, g_loss: 0.69385356\n",
      "Step: [16315] d_loss: 1.38635731, g_loss: 0.69305933\n",
      "Step: [16316] d_loss: 1.38630009, g_loss: 0.69285119\n",
      "Step: [16317] d_loss: 1.38641739, g_loss: 0.69393975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16318] d_loss: 1.38638031, g_loss: 0.69312221\n",
      "Step: [16319] d_loss: 1.38636434, g_loss: 0.69276345\n",
      "Step: [16320] d_loss: 1.38632739, g_loss: 0.69322157\n",
      "Step: [16321] d_loss: 1.38634014, g_loss: 0.69375235\n",
      "Step: [16322] d_loss: 1.38624227, g_loss: 0.69365615\n",
      "Step: [16323] d_loss: 1.38548565, g_loss: 0.69205356\n",
      "Step: [16324] d_loss: 1.38464689, g_loss: 0.69059569\n",
      "Step: [16325] d_loss: 1.38654852, g_loss: 0.69215560\n",
      "Step: [16326] d_loss: 1.38640523, g_loss: 0.69363886\n",
      "Step: [16327] d_loss: 1.38644695, g_loss: 0.69500625\n",
      "Step: [16328] d_loss: 1.38647294, g_loss: 0.69349772\n",
      "Step: [16329] d_loss: 1.38701856, g_loss: 0.68901372\n",
      "Step: [16330] d_loss: 1.38651288, g_loss: 0.69346106\n",
      "Step: [16331] d_loss: 1.38640213, g_loss: 0.69406939\n",
      "Step: [16332] d_loss: 1.38627434, g_loss: 0.69414932\n",
      "Step: [16333] d_loss: 1.38633823, g_loss: 0.69348001\n",
      "Step: [16334] d_loss: 1.38637865, g_loss: 0.69269943\n",
      "Step: [16335] d_loss: 1.38634217, g_loss: 0.69338155\n",
      "Step: [16336] d_loss: 1.38722825, g_loss: 0.69280708\n",
      "Step: [16337] d_loss: 1.38637590, g_loss: 0.69286561\n",
      "Step: [16338] d_loss: 1.38629174, g_loss: 0.69325042\n",
      "Step: [16339] d_loss: 1.38643050, g_loss: 0.69405860\n",
      "Step: [16340] d_loss: 1.38631296, g_loss: 0.69361663\n",
      "Step: [16341] d_loss: 1.38643074, g_loss: 0.69365478\n",
      "Step: [16342] d_loss: 1.38657117, g_loss: 0.69578820\n",
      "Step: [16343] d_loss: 1.38635850, g_loss: 0.69386142\n",
      "Step: [16344] d_loss: 1.38631725, g_loss: 0.69180185\n",
      "Step: [16345] d_loss: 1.38627946, g_loss: 0.69260556\n",
      "Step: [16346] d_loss: 1.38637257, g_loss: 0.69319528\n",
      "Step: [16347] d_loss: 1.38631749, g_loss: 0.69324827\n",
      "Step: [16348] d_loss: 1.38626575, g_loss: 0.69382811\n",
      "Step: [16349] d_loss: 1.38627791, g_loss: 0.69329298\n",
      "Step: [16350] d_loss: 1.38629675, g_loss: 0.69269407\n",
      "Step: [16351] d_loss: 1.38624251, g_loss: 0.69302809\n",
      "Step: [16352] d_loss: 1.38636374, g_loss: 0.69301879\n",
      "Step: [16353] d_loss: 1.38632655, g_loss: 0.69310379\n",
      "Step: [16354] d_loss: 1.38625968, g_loss: 0.69375682\n",
      "Step: [16355] d_loss: 1.38665533, g_loss: 0.69253236\n",
      "Step: [16356] d_loss: 1.38653255, g_loss: 0.69241261\n",
      "Step: [16357] d_loss: 1.38629973, g_loss: 0.69301152\n",
      "Step: [16358] d_loss: 1.38633025, g_loss: 0.69341314\n",
      "Step: [16359] d_loss: 1.38636231, g_loss: 0.69285715\n",
      "Step: [16360] d_loss: 1.38818467, g_loss: 0.69498408\n",
      "Step: [16361] d_loss: 1.38640428, g_loss: 0.69332969\n",
      "Step: [16362] d_loss: 1.38640416, g_loss: 0.69415802\n",
      "Step: [16363] d_loss: 1.38630700, g_loss: 0.69347703\n",
      "Step: [16364] d_loss: 1.38623452, g_loss: 0.69299018\n",
      "Step: [16365] d_loss: 1.38739157, g_loss: 0.69449866\n",
      "Step: [16366] d_loss: 1.38629866, g_loss: 0.69280243\n",
      "Step: [16367] d_loss: 1.38629210, g_loss: 0.69353926\n",
      "Step: [16368] d_loss: 1.38632703, g_loss: 0.69356102\n",
      "Step: [16369] d_loss: 1.38635874, g_loss: 0.69285619\n",
      "Step: [16370] d_loss: 1.38628471, g_loss: 0.69274962\n",
      "Step: [16371] d_loss: 1.38747072, g_loss: 0.69360709\n",
      "Step: [16372] d_loss: 1.38636148, g_loss: 0.69347787\n",
      "Step: [16373] d_loss: 1.38660908, g_loss: 0.69599414\n",
      "Step: [16374] d_loss: 1.38633084, g_loss: 0.69518101\n",
      "Step: [16375] d_loss: 1.38636732, g_loss: 0.69321603\n",
      "Step: [16376] d_loss: 1.38629365, g_loss: 0.69234490\n",
      "Step: [16377] d_loss: 1.38628578, g_loss: 0.69113952\n",
      "Step: [16378] d_loss: 1.38622129, g_loss: 0.69249684\n",
      "Step: [16379] d_loss: 1.38636494, g_loss: 0.69360703\n",
      "Step: [16380] d_loss: 1.38621545, g_loss: 0.69413948\n",
      "Step: [16381] d_loss: 1.38623369, g_loss: 0.69474649\n",
      "Step: [16382] d_loss: 1.38642085, g_loss: 0.69300878\n",
      "Step: [16383] d_loss: 1.38628507, g_loss: 0.69303173\n",
      "Step: [16384] d_loss: 1.38622594, g_loss: 0.69195557\n",
      "Step: [16385] d_loss: 1.38626647, g_loss: 0.69263738\n",
      "Step: [16386] d_loss: 1.38626611, g_loss: 0.69351447\n",
      "Step: [16387] d_loss: 1.38620186, g_loss: 0.69395363\n",
      "Step: [16388] d_loss: 1.38615215, g_loss: 0.69419539\n",
      "Step: [16389] d_loss: 1.38577485, g_loss: 0.69644964\n",
      "Step: [16390] d_loss: 1.38644207, g_loss: 0.69251537\n",
      "Step: [16391] d_loss: 1.38620782, g_loss: 0.69236267\n",
      "Step: [16392] d_loss: 1.38627958, g_loss: 0.69256610\n",
      "Step: [16393] d_loss: 1.38614440, g_loss: 0.69347441\n",
      "Step: [16394] d_loss: 1.38676906, g_loss: 0.69572890\n",
      "Step: [16395] d_loss: 1.38712192, g_loss: 0.69444597\n",
      "Step: [16396] d_loss: 1.38724446, g_loss: 0.69620150\n",
      "Step: [16397] d_loss: 1.38665247, g_loss: 0.69594032\n",
      "Step: [16398] d_loss: 1.38675332, g_loss: 0.69224191\n",
      "Step: [16399] d_loss: 1.38646317, g_loss: 0.69145370\n",
      "Step: [16400] d_loss: 1.38626695, g_loss: 0.69243360\n",
      "Step: [16401] d_loss: 1.38627255, g_loss: 0.69345987\n",
      "Step: [16402] d_loss: 1.38628364, g_loss: 0.69345152\n",
      "Step: [16403] d_loss: 1.38627100, g_loss: 0.69351321\n",
      "Step: [16404] d_loss: 1.38622284, g_loss: 0.69346225\n",
      "Step: [16405] d_loss: 1.38626230, g_loss: 0.69295394\n",
      "Step: [16406] d_loss: 1.38627350, g_loss: 0.69316578\n",
      "Step: [16407] d_loss: 1.38623190, g_loss: 0.69217604\n",
      "Step: [16408] d_loss: 1.38641965, g_loss: 0.69263119\n",
      "Step: [16409] d_loss: 1.38628292, g_loss: 0.69366860\n",
      "Step: [16410] d_loss: 1.38651156, g_loss: 0.69420469\n",
      "Step: [16411] d_loss: 1.39004791, g_loss: 0.70853394\n",
      "Step: [16412] d_loss: 1.38658857, g_loss: 0.68857044\n",
      "Step: [16413] d_loss: 1.38669610, g_loss: 0.68973804\n",
      "Step: [16414] d_loss: 1.38834715, g_loss: 0.68828100\n",
      "Step: [16415] d_loss: 1.38647485, g_loss: 0.69171208\n",
      "Step: [16416] d_loss: 1.38659084, g_loss: 0.69447249\n",
      "Step: [16417] d_loss: 1.38642037, g_loss: 0.69547659\n",
      "Step: [16418] d_loss: 1.38634217, g_loss: 0.69428313\n",
      "Step: [16419] d_loss: 1.38630509, g_loss: 0.69307178\n",
      "Step: [16420] d_loss: 1.38628066, g_loss: 0.69193035\n",
      "Step: [16421] d_loss: 1.38631439, g_loss: 0.69258714\n",
      "Step: [16422] d_loss: 1.38627446, g_loss: 0.69273186\n",
      "Step: [16423] d_loss: 1.38626027, g_loss: 0.69326478\n",
      "Step: [16424] d_loss: 1.38608193, g_loss: 0.69320005\n",
      "Step: [16425] d_loss: 1.38624680, g_loss: 0.69299787\n",
      "Step: [16426] d_loss: 1.38617849, g_loss: 0.69278592\n",
      "Step: [16427] d_loss: 1.38638902, g_loss: 0.69362152\n",
      "Step: [16428] d_loss: 1.38654733, g_loss: 0.69561207\n",
      "Step: [16429] d_loss: 1.38694286, g_loss: 0.69318199\n",
      "Step: [16430] d_loss: 1.38671875, g_loss: 0.69278407\n",
      "Step: [16431] d_loss: 1.38635087, g_loss: 0.69147992\n",
      "Step: [16432] d_loss: 1.38629436, g_loss: 0.69224834\n",
      "Step: [16433] d_loss: 1.38626850, g_loss: 0.69343019\n",
      "Step: [16434] d_loss: 1.38630962, g_loss: 0.69414562\n",
      "Step: [16435] d_loss: 1.38627028, g_loss: 0.69291425\n",
      "Step: [16436] d_loss: 1.38628459, g_loss: 0.69317049\n",
      "Step: [16437] d_loss: 1.38628602, g_loss: 0.69294000\n",
      "Step: [16438] d_loss: 1.38627958, g_loss: 0.69304156\n",
      "Step: [16439] d_loss: 1.38626218, g_loss: 0.69284129\n",
      "Step: [16440] d_loss: 1.38627577, g_loss: 0.69323719\n",
      "Step: [16441] d_loss: 1.38630509, g_loss: 0.69312173\n",
      "Step: [16442] d_loss: 1.38626671, g_loss: 0.69307458\n",
      "Step: [16443] d_loss: 1.38628507, g_loss: 0.69334114\n",
      "Step: [16444] d_loss: 1.38627529, g_loss: 0.69317418\n",
      "Step: [16445] d_loss: 1.38631308, g_loss: 0.69291502\n",
      "Step: [16446] d_loss: 1.38631856, g_loss: 0.69263303\n",
      "Step: [16447] d_loss: 1.38626742, g_loss: 0.69309503\n",
      "Step: [16448] d_loss: 1.38626766, g_loss: 0.69340944\n",
      "Step: [16449] d_loss: 1.38625407, g_loss: 0.69327235\n",
      "Step: [16450] d_loss: 1.38625252, g_loss: 0.69329011\n",
      "Step: [16451] d_loss: 1.38622975, g_loss: 0.69336712\n",
      "Step: [16452] d_loss: 1.38625860, g_loss: 0.69340551\n",
      "Step: [16453] d_loss: 1.38627410, g_loss: 0.69306487\n",
      "Step: [16454] d_loss: 1.38627338, g_loss: 0.69278640\n",
      "Step: [16455] d_loss: 1.38628042, g_loss: 0.69292939\n",
      "Step: [16456] d_loss: 1.38624048, g_loss: 0.69324434\n",
      "Step: [16457] d_loss: 1.38627946, g_loss: 0.69308710\n",
      "Step: [16458] d_loss: 1.38625765, g_loss: 0.69349068\n",
      "Step: [16459] d_loss: 1.38625705, g_loss: 0.69338173\n",
      "Step: [16460] d_loss: 1.38625836, g_loss: 0.69292742\n",
      "Step: [16461] d_loss: 1.38631475, g_loss: 0.69296110\n",
      "Step: [16462] d_loss: 1.38625455, g_loss: 0.69262910\n",
      "Step: [16463] d_loss: 1.38627863, g_loss: 0.69310248\n",
      "Step: [16464] d_loss: 1.38625669, g_loss: 0.69286275\n",
      "Step: [16465] d_loss: 1.38625145, g_loss: 0.69334167\n",
      "Step: [16466] d_loss: 1.38622379, g_loss: 0.69351643\n",
      "Step: [16467] d_loss: 1.38626921, g_loss: 0.69299197\n",
      "Step: [16468] d_loss: 1.38627505, g_loss: 0.69331741\n",
      "Step: [16469] d_loss: 1.38627076, g_loss: 0.69300503\n",
      "Step: [16470] d_loss: 1.38629711, g_loss: 0.69333398\n",
      "Step: [16471] d_loss: 1.38627493, g_loss: 0.69319171\n",
      "Step: [16472] d_loss: 1.38631141, g_loss: 0.69311500\n",
      "Step: [16473] d_loss: 1.38626742, g_loss: 0.69310927\n",
      "Step: [16474] d_loss: 1.38623679, g_loss: 0.69312704\n",
      "Step: [16475] d_loss: 1.38627028, g_loss: 0.69324672\n",
      "Step: [16476] d_loss: 1.38626218, g_loss: 0.69310951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16477] d_loss: 1.38627720, g_loss: 0.69310087\n",
      "Step: [16478] d_loss: 1.38627446, g_loss: 0.69312721\n",
      "Step: [16479] d_loss: 1.38625431, g_loss: 0.69318068\n",
      "Step: [16480] d_loss: 1.38625014, g_loss: 0.69294053\n",
      "Step: [16481] d_loss: 1.38629150, g_loss: 0.69315189\n",
      "Step: [16482] d_loss: 1.38627720, g_loss: 0.69295269\n",
      "Step: [16483] d_loss: 1.38631344, g_loss: 0.69319123\n",
      "Step: [16484] d_loss: 1.38626575, g_loss: 0.69338405\n",
      "Step: [16485] d_loss: 1.38626051, g_loss: 0.69311917\n",
      "Step: [16486] d_loss: 1.38626325, g_loss: 0.69291496\n",
      "Step: [16487] d_loss: 1.38646984, g_loss: 0.69249910\n",
      "Step: [16488] d_loss: 1.38633442, g_loss: 0.69298410\n",
      "Step: [16489] d_loss: 1.38629556, g_loss: 0.69379175\n",
      "Step: [16490] d_loss: 1.38625801, g_loss: 0.69347620\n",
      "Step: [16491] d_loss: 1.38613248, g_loss: 0.69321746\n",
      "Step: [16492] d_loss: 1.38625646, g_loss: 0.69297004\n",
      "Step: [16493] d_loss: 1.38630259, g_loss: 0.69307458\n",
      "Step: [16494] d_loss: 1.38621259, g_loss: 0.69300985\n",
      "Step: [16495] d_loss: 1.38629556, g_loss: 0.69312084\n",
      "Step: [16496] d_loss: 1.38631332, g_loss: 0.69239628\n",
      "Step: [16497] d_loss: 1.38629413, g_loss: 0.69259000\n",
      "Step: [16498] d_loss: 1.38624918, g_loss: 0.69280088\n",
      "Step: [16499] d_loss: 1.38848495, g_loss: 0.69665176\n",
      "Step: [16500] d_loss: 1.38630223, g_loss: 0.69152570\n",
      "Step: [16501] d_loss: 1.38631451, g_loss: 0.69233680\n",
      "Step: [16502] d_loss: 1.38625169, g_loss: 0.69329095\n",
      "Step: [16503] d_loss: 1.38624501, g_loss: 0.69374394\n",
      "Step: [16504] d_loss: 1.38618028, g_loss: 0.69329143\n",
      "Step: [16505] d_loss: 1.38628089, g_loss: 0.69327283\n",
      "Step: [16506] d_loss: 1.38625836, g_loss: 0.69308162\n",
      "Step: [16507] d_loss: 1.38624811, g_loss: 0.69294012\n",
      "Step: [16508] d_loss: 1.38626480, g_loss: 0.69287264\n",
      "Step: [16509] d_loss: 1.38630939, g_loss: 0.69291461\n",
      "Step: [16510] d_loss: 1.38628101, g_loss: 0.69325835\n",
      "Step: [16511] d_loss: 1.38629222, g_loss: 0.69333333\n",
      "Step: [16512] d_loss: 1.38630247, g_loss: 0.69266337\n",
      "Step: [16513] d_loss: 1.38617659, g_loss: 0.69308889\n",
      "Step: [16514] d_loss: 1.38627434, g_loss: 0.69342655\n",
      "Step: [16515] d_loss: 1.38624740, g_loss: 0.69326127\n",
      "Step: [16516] d_loss: 1.38620305, g_loss: 0.69242644\n",
      "Step: [16517] d_loss: 1.38622248, g_loss: 0.69294155\n",
      "Step: [16518] d_loss: 1.38627315, g_loss: 0.69343466\n",
      "Step: [16519] d_loss: 1.38629937, g_loss: 0.69309992\n",
      "Step: [16520] d_loss: 1.38628030, g_loss: 0.69289219\n",
      "Step: [16521] d_loss: 1.38622737, g_loss: 0.69304204\n",
      "Step: [16522] d_loss: 1.38621831, g_loss: 0.69355112\n",
      "Step: [16523] d_loss: 1.38616800, g_loss: 0.69366026\n",
      "Step: [16524] d_loss: 1.38615203, g_loss: 0.69389069\n",
      "Step: [16525] d_loss: 1.38618886, g_loss: 0.69263321\n",
      "Step: [16526] d_loss: 1.38628626, g_loss: 0.69251752\n",
      "Step: [16527] d_loss: 1.38625717, g_loss: 0.69332170\n",
      "Step: [16528] d_loss: 1.38623953, g_loss: 0.69392008\n",
      "Step: [16529] d_loss: 1.38621628, g_loss: 0.69381666\n",
      "Step: [16530] d_loss: 1.38687038, g_loss: 0.69255400\n",
      "Step: [16531] d_loss: 1.38693905, g_loss: 0.69331539\n",
      "Step: [16532] d_loss: 1.38639593, g_loss: 0.69222629\n",
      "Step: [16533] d_loss: 1.38655114, g_loss: 0.69274718\n",
      "Step: [16534] d_loss: 1.38638711, g_loss: 0.69134676\n",
      "Step: [16535] d_loss: 1.38645732, g_loss: 0.69153583\n",
      "Step: [16536] d_loss: 1.38664532, g_loss: 0.69147182\n",
      "Step: [16537] d_loss: 1.38636708, g_loss: 0.69255018\n",
      "Step: [16538] d_loss: 1.38627374, g_loss: 0.69371128\n",
      "Step: [16539] d_loss: 1.38623440, g_loss: 0.69445479\n",
      "Step: [16540] d_loss: 1.38621640, g_loss: 0.69371247\n",
      "Step: [16541] d_loss: 1.38648665, g_loss: 0.69385576\n",
      "Step: [16542] d_loss: 1.38668180, g_loss: 0.69418097\n",
      "Step: [16543] d_loss: 1.38658452, g_loss: 0.69222081\n",
      "Step: [16544] d_loss: 1.38644707, g_loss: 0.69317651\n",
      "Step: [16545] d_loss: 1.38631952, g_loss: 0.69294214\n",
      "Step: [16546] d_loss: 1.38628268, g_loss: 0.69315666\n",
      "Step: [16547] d_loss: 1.38618004, g_loss: 0.69340378\n",
      "Step: [16548] d_loss: 1.38635385, g_loss: 0.69332272\n",
      "Step: [16549] d_loss: 1.38637996, g_loss: 0.69237900\n",
      "Step: [16550] d_loss: 1.38630414, g_loss: 0.69313586\n",
      "Step: [16551] d_loss: 1.38628268, g_loss: 0.69395959\n",
      "Step: [16552] d_loss: 1.38630247, g_loss: 0.69360095\n",
      "Step: [16553] d_loss: 1.38608193, g_loss: 0.69280392\n",
      "Step: [16554] d_loss: 1.38622928, g_loss: 0.69279629\n",
      "Step: [16555] d_loss: 1.38620031, g_loss: 0.69253767\n",
      "Step: [16556] d_loss: 1.38625216, g_loss: 0.69371974\n",
      "Step: [16557] d_loss: 1.38813889, g_loss: 0.69490486\n",
      "Step: [16558] d_loss: 1.38825512, g_loss: 0.69574761\n",
      "Step: [16559] d_loss: 1.38845825, g_loss: 0.69565928\n",
      "Step: [16560] d_loss: 1.38745809, g_loss: 0.69366872\n",
      "Step: [16561] d_loss: 1.38668776, g_loss: 0.69131136\n",
      "Step: [16562] d_loss: 1.38634229, g_loss: 0.69154525\n",
      "Step: [16563] d_loss: 1.38626444, g_loss: 0.69309866\n",
      "Step: [16564] d_loss: 1.38628852, g_loss: 0.69359779\n",
      "Step: [16565] d_loss: 1.38633609, g_loss: 0.69319284\n",
      "Step: [16566] d_loss: 1.38629675, g_loss: 0.69322443\n",
      "Step: [16567] d_loss: 1.38627815, g_loss: 0.69336009\n",
      "Step: [16568] d_loss: 1.38626528, g_loss: 0.69311637\n",
      "Step: [16569] d_loss: 1.38629794, g_loss: 0.69342142\n",
      "Step: [16570] d_loss: 1.38626528, g_loss: 0.69312268\n",
      "Step: [16571] d_loss: 1.38626170, g_loss: 0.69302320\n",
      "Step: [16572] d_loss: 1.38625383, g_loss: 0.69304770\n",
      "Step: [16573] d_loss: 1.38621473, g_loss: 0.69329518\n",
      "Step: [16574] d_loss: 1.38675082, g_loss: 0.69380486\n",
      "Step: [16575] d_loss: 1.38630474, g_loss: 0.69204104\n",
      "Step: [16576] d_loss: 1.38637924, g_loss: 0.69280875\n",
      "Step: [16577] d_loss: 1.38635349, g_loss: 0.69370651\n",
      "Step: [16578] d_loss: 1.38626170, g_loss: 0.69397473\n",
      "Step: [16579] d_loss: 1.38625813, g_loss: 0.69339585\n",
      "Step: [16580] d_loss: 1.38627160, g_loss: 0.69288695\n",
      "Step: [16581] d_loss: 1.38627911, g_loss: 0.69327569\n",
      "Step: [16582] d_loss: 1.38625789, g_loss: 0.69365764\n",
      "Step: [16583] d_loss: 1.38626027, g_loss: 0.69279277\n",
      "Step: [16584] d_loss: 1.38631463, g_loss: 0.69386208\n",
      "Step: [16585] d_loss: 1.38616514, g_loss: 0.69309473\n",
      "Step: [16586] d_loss: 1.38628221, g_loss: 0.69294679\n",
      "Step: [16587] d_loss: 1.38767302, g_loss: 0.69341749\n",
      "Step: [16588] d_loss: 1.38628972, g_loss: 0.69229865\n",
      "Step: [16589] d_loss: 1.38627756, g_loss: 0.69319654\n",
      "Step: [16590] d_loss: 1.38629794, g_loss: 0.69374764\n",
      "Step: [16591] d_loss: 1.38628483, g_loss: 0.69425237\n",
      "Step: [16592] d_loss: 1.38627219, g_loss: 0.69360465\n",
      "Step: [16593] d_loss: 1.38625908, g_loss: 0.69357955\n",
      "Step: [16594] d_loss: 1.38629878, g_loss: 0.69228089\n",
      "Step: [16595] d_loss: 1.38636541, g_loss: 0.69198602\n",
      "Step: [16596] d_loss: 1.38641238, g_loss: 0.69351166\n",
      "Step: [16597] d_loss: 1.38637710, g_loss: 0.69360167\n",
      "Step: [16598] d_loss: 1.38632154, g_loss: 0.69294274\n",
      "Step: [16599] d_loss: 1.38628364, g_loss: 0.69290090\n",
      "Step: [16600] d_loss: 1.38629985, g_loss: 0.69260073\n",
      "Step: [16601] d_loss: 1.38625121, g_loss: 0.69319212\n",
      "Step: [16602] d_loss: 1.38626659, g_loss: 0.69348812\n",
      "Step: [16603] d_loss: 1.38625300, g_loss: 0.69377613\n",
      "Step: [16604] d_loss: 1.39451170, g_loss: 0.90377426\n",
      "Step: [16605] d_loss: 1.39018297, g_loss: 0.72696841\n",
      "Step: [16606] d_loss: 1.39098883, g_loss: 0.69975162\n",
      "Step: [16607] d_loss: 1.38775265, g_loss: 0.67926753\n",
      "Step: [16608] d_loss: 1.38710368, g_loss: 0.67954493\n",
      "Step: [16609] d_loss: 1.38714910, g_loss: 0.68853533\n",
      "Step: [16610] d_loss: 1.38671327, g_loss: 0.69728714\n",
      "Step: [16611] d_loss: 1.38673401, g_loss: 0.69615090\n",
      "Step: [16612] d_loss: 1.38646317, g_loss: 0.69480163\n",
      "Step: [16613] d_loss: 1.39718974, g_loss: 0.68400681\n",
      "Step: [16614] d_loss: 1.38645172, g_loss: 0.68926585\n",
      "Step: [16615] d_loss: 1.38652301, g_loss: 0.69144356\n",
      "Step: [16616] d_loss: 1.38637984, g_loss: 0.69453889\n",
      "Step: [16617] d_loss: 1.38635397, g_loss: 0.69286883\n",
      "Step: [16618] d_loss: 1.38638067, g_loss: 0.69390172\n",
      "Step: [16619] d_loss: 1.38605380, g_loss: 0.69146991\n",
      "Step: [16620] d_loss: 1.38632941, g_loss: 0.69360816\n",
      "Step: [16621] d_loss: 1.38647568, g_loss: 0.69477946\n",
      "Step: [16622] d_loss: 1.38638353, g_loss: 0.69432580\n",
      "Step: [16623] d_loss: 1.38641655, g_loss: 0.69494748\n",
      "Step: [16624] d_loss: 1.38618684, g_loss: 0.69289410\n",
      "Step: [16625] d_loss: 1.38624632, g_loss: 0.69358653\n",
      "Step: [16626] d_loss: 1.38614523, g_loss: 0.69138318\n",
      "Step: [16627] d_loss: 1.38620400, g_loss: 0.69313258\n",
      "Step: [16628] d_loss: 1.38609838, g_loss: 0.69186926\n",
      "Step: [16629] d_loss: 1.38606799, g_loss: 0.69471991\n",
      "Step: [16630] d_loss: 1.38611889, g_loss: 0.69371164\n",
      "Step: [16631] d_loss: 1.38602495, g_loss: 0.69185030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16632] d_loss: 1.38587689, g_loss: 0.69240832\n",
      "Step: [16633] d_loss: 1.38616467, g_loss: 0.69453549\n",
      "Step: [16634] d_loss: 1.38607907, g_loss: 0.69232559\n",
      "Step: [16635] d_loss: 1.38619089, g_loss: 0.69308978\n",
      "Step: [16636] d_loss: 1.38628423, g_loss: 0.69151211\n",
      "Step: [16637] d_loss: 1.38635707, g_loss: 0.69387484\n",
      "Step: [16638] d_loss: 1.38618731, g_loss: 0.69430846\n",
      "Step: [16639] d_loss: 1.39153981, g_loss: 0.68627638\n",
      "Step: [16640] d_loss: 1.38596916, g_loss: 0.69235158\n",
      "Step: [16641] d_loss: 1.38626814, g_loss: 0.69122869\n",
      "Step: [16642] d_loss: 1.38620257, g_loss: 0.69386935\n",
      "Step: [16643] d_loss: 1.38625050, g_loss: 0.69424403\n",
      "Step: [16644] d_loss: 1.38644576, g_loss: 0.69468838\n",
      "Step: [16645] d_loss: 1.38665581, g_loss: 0.69065541\n",
      "Step: [16646] d_loss: 1.38663554, g_loss: 0.69269425\n",
      "Step: [16647] d_loss: 1.38650727, g_loss: 0.68949032\n",
      "Step: [16648] d_loss: 1.38669491, g_loss: 0.69472027\n",
      "Step: [16649] d_loss: 1.38656616, g_loss: 0.69319665\n",
      "Step: [16650] d_loss: 1.38725615, g_loss: 0.69259930\n",
      "Step: [16651] d_loss: 1.38638175, g_loss: 0.69725025\n",
      "Step: [16652] d_loss: 1.38631153, g_loss: 0.69382453\n",
      "Step: [16653] d_loss: 1.38638401, g_loss: 0.69166899\n",
      "Step: [16654] d_loss: 1.38629961, g_loss: 0.69073337\n",
      "Step: [16655] d_loss: 1.38633895, g_loss: 0.69210613\n",
      "Step: [16656] d_loss: 1.38634562, g_loss: 0.69386208\n",
      "Step: [16657] d_loss: 1.38628626, g_loss: 0.69403595\n",
      "Step: [16658] d_loss: 1.38630545, g_loss: 0.69322592\n",
      "Step: [16659] d_loss: 1.38628995, g_loss: 0.69291091\n",
      "Step: [16660] d_loss: 1.38627028, g_loss: 0.69291675\n",
      "Step: [16661] d_loss: 1.38629031, g_loss: 0.69310874\n",
      "Step: [16662] d_loss: 1.38630319, g_loss: 0.69298029\n",
      "Step: [16663] d_loss: 1.38630652, g_loss: 0.69326991\n",
      "Step: [16664] d_loss: 1.38628352, g_loss: 0.69326496\n",
      "Step: [16665] d_loss: 1.38630819, g_loss: 0.69315267\n",
      "Step: [16666] d_loss: 1.38660753, g_loss: 0.69050288\n",
      "Step: [16667] d_loss: 1.38632989, g_loss: 0.69327563\n",
      "Step: [16668] d_loss: 1.38633990, g_loss: 0.69198108\n",
      "Step: [16669] d_loss: 1.38632345, g_loss: 0.69271898\n",
      "Step: [16670] d_loss: 1.38629615, g_loss: 0.69363403\n",
      "Step: [16671] d_loss: 1.38625181, g_loss: 0.69340241\n",
      "Step: [16672] d_loss: 1.38626635, g_loss: 0.69417101\n",
      "Step: [16673] d_loss: 1.38630795, g_loss: 0.69299978\n",
      "Step: [16674] d_loss: 1.38628983, g_loss: 0.69286239\n",
      "Step: [16675] d_loss: 1.38628054, g_loss: 0.69276178\n",
      "Step: [16676] d_loss: 1.38627362, g_loss: 0.69287682\n",
      "Step: [16677] d_loss: 1.38624668, g_loss: 0.69333422\n",
      "Step: [16678] d_loss: 1.38627589, g_loss: 0.69334352\n",
      "Step: [16679] d_loss: 1.38624966, g_loss: 0.69310522\n",
      "Step: [16680] d_loss: 1.38634765, g_loss: 0.69205832\n",
      "Step: [16681] d_loss: 1.38620913, g_loss: 0.69379598\n",
      "Step: [16682] d_loss: 1.38627791, g_loss: 0.69326437\n",
      "Step: [16683] d_loss: 1.38645709, g_loss: 0.69350755\n",
      "Step: [16684] d_loss: 1.38627219, g_loss: 0.69326282\n",
      "Step: [16685] d_loss: 1.38624287, g_loss: 0.69294399\n",
      "Step: [16686] d_loss: 1.38625920, g_loss: 0.69274986\n",
      "Step: [16687] d_loss: 1.38648856, g_loss: 0.69106644\n",
      "Step: [16688] d_loss: 1.38625538, g_loss: 0.69389188\n",
      "Step: [16689] d_loss: 1.38651800, g_loss: 0.69140863\n",
      "Step: [16690] d_loss: 1.38638985, g_loss: 0.69339705\n",
      "Step: [16691] d_loss: 1.38670182, g_loss: 0.69098389\n",
      "Step: [16692] d_loss: 1.38637924, g_loss: 0.69288921\n",
      "Step: [16693] d_loss: 1.38629389, g_loss: 0.69248474\n",
      "Step: [16694] d_loss: 1.38625634, g_loss: 0.69388032\n",
      "Step: [16695] d_loss: 1.38619781, g_loss: 0.69428885\n",
      "Step: [16696] d_loss: 1.38633573, g_loss: 0.69385314\n",
      "Step: [16697] d_loss: 1.38630581, g_loss: 0.69370693\n",
      "Step: [16698] d_loss: 1.38626337, g_loss: 0.69343865\n",
      "Step: [16699] d_loss: 1.38682866, g_loss: 0.68935651\n",
      "Step: [16700] d_loss: 1.38628817, g_loss: 0.69473833\n",
      "Step: [16701] d_loss: 1.38627195, g_loss: 0.69280255\n",
      "Step: [16702] d_loss: 1.38631463, g_loss: 0.69267809\n",
      "Step: [16703] d_loss: 1.38622415, g_loss: 0.69288635\n",
      "Step: [16704] d_loss: 1.38626790, g_loss: 0.69292104\n",
      "Step: [16705] d_loss: 1.38625455, g_loss: 0.69314790\n",
      "Step: [16706] d_loss: 1.38624191, g_loss: 0.69367766\n",
      "Step: [16707] d_loss: 1.38622534, g_loss: 0.69372821\n",
      "Step: [16708] d_loss: 1.38629174, g_loss: 0.69352281\n",
      "Step: [16709] d_loss: 1.38618553, g_loss: 0.69359690\n",
      "Step: [16710] d_loss: 1.38625252, g_loss: 0.69326270\n",
      "Step: [16711] d_loss: 1.38625360, g_loss: 0.69311392\n",
      "Step: [16712] d_loss: 1.38624692, g_loss: 0.69334084\n",
      "Step: [16713] d_loss: 1.38622522, g_loss: 0.69355392\n",
      "Step: [16714] d_loss: 1.38624215, g_loss: 0.69322658\n",
      "Step: [16715] d_loss: 1.38613594, g_loss: 0.69289744\n",
      "Step: [16716] d_loss: 1.38624275, g_loss: 0.69293588\n",
      "Step: [16717] d_loss: 1.38625479, g_loss: 0.69366539\n",
      "Step: [16718] d_loss: 1.38626635, g_loss: 0.69418013\n",
      "Step: [16719] d_loss: 1.38625526, g_loss: 0.69346756\n",
      "Step: [16720] d_loss: 1.38621783, g_loss: 0.69262695\n",
      "Step: [16721] d_loss: 1.38624918, g_loss: 0.69230080\n",
      "Step: [16722] d_loss: 1.38624787, g_loss: 0.69264883\n",
      "Step: [16723] d_loss: 1.38629496, g_loss: 0.69313961\n",
      "Step: [16724] d_loss: 1.38628125, g_loss: 0.69408393\n",
      "Step: [16725] d_loss: 1.38616538, g_loss: 0.69369096\n",
      "Step: [16726] d_loss: 1.38615906, g_loss: 0.69315803\n",
      "Step: [16727] d_loss: 1.38620019, g_loss: 0.69242197\n",
      "Step: [16728] d_loss: 1.38630545, g_loss: 0.69312978\n",
      "Step: [16729] d_loss: 1.38634086, g_loss: 0.69396031\n",
      "Step: [16730] d_loss: 1.38629699, g_loss: 0.69319248\n",
      "Step: [16731] d_loss: 1.38630080, g_loss: 0.69332802\n",
      "Step: [16732] d_loss: 1.38629174, g_loss: 0.69342595\n",
      "Step: [16733] d_loss: 1.38621402, g_loss: 0.69343966\n",
      "Step: [16734] d_loss: 1.38630664, g_loss: 0.69318062\n",
      "Step: [16735] d_loss: 1.38640070, g_loss: 0.69255209\n",
      "Step: [16736] d_loss: 1.38627934, g_loss: 0.69291639\n",
      "Step: [16737] d_loss: 1.38631630, g_loss: 0.69263756\n",
      "Step: [16738] d_loss: 1.38629198, g_loss: 0.69335711\n",
      "Step: [16739] d_loss: 1.38629091, g_loss: 0.69385350\n",
      "Step: [16740] d_loss: 1.38633513, g_loss: 0.69425857\n",
      "Step: [16741] d_loss: 1.38623619, g_loss: 0.69348407\n",
      "Step: [16742] d_loss: 1.38624048, g_loss: 0.69282037\n",
      "Step: [16743] d_loss: 1.38623798, g_loss: 0.69317389\n",
      "Step: [16744] d_loss: 1.38630176, g_loss: 0.69298708\n",
      "Step: [16745] d_loss: 1.38628483, g_loss: 0.69332671\n",
      "Step: [16746] d_loss: 1.38624835, g_loss: 0.69314712\n",
      "Step: [16747] d_loss: 1.38635266, g_loss: 0.69331741\n",
      "Step: [16748] d_loss: 1.38628447, g_loss: 0.69339460\n",
      "Step: [16749] d_loss: 1.38629496, g_loss: 0.69294906\n",
      "Step: [16750] d_loss: 1.38636494, g_loss: 0.69312382\n",
      "Step: [16751] d_loss: 1.38687932, g_loss: 0.68869114\n",
      "Step: [16752] d_loss: 1.38631439, g_loss: 0.69345450\n",
      "Step: [16753] d_loss: 1.38637757, g_loss: 0.69347799\n",
      "Step: [16754] d_loss: 1.38632190, g_loss: 0.69408083\n",
      "Step: [16755] d_loss: 1.38641536, g_loss: 0.69384640\n",
      "Step: [16756] d_loss: 1.38636971, g_loss: 0.69275439\n",
      "Step: [16757] d_loss: 1.38614953, g_loss: 0.69300866\n",
      "Step: [16758] d_loss: 1.38640308, g_loss: 0.69234663\n",
      "Step: [16759] d_loss: 1.38633704, g_loss: 0.69459701\n",
      "Step: [16760] d_loss: 1.38646078, g_loss: 0.69416225\n",
      "Step: [16761] d_loss: 1.38647091, g_loss: 0.69515777\n",
      "Step: [16762] d_loss: 1.38628006, g_loss: 0.69400960\n",
      "Step: [16763] d_loss: 1.38623130, g_loss: 0.69407117\n",
      "Step: [16764] d_loss: 1.38631272, g_loss: 0.69308823\n",
      "Step: [16765] d_loss: 1.38625312, g_loss: 0.69262177\n",
      "Step: [16766] d_loss: 1.38622987, g_loss: 0.69318140\n",
      "Step: [16767] d_loss: 1.38633013, g_loss: 0.69370806\n",
      "Step: [16768] d_loss: 1.38621736, g_loss: 0.69418442\n",
      "Step: [16769] d_loss: 1.38652945, g_loss: 0.69350749\n",
      "Step: [16770] d_loss: 1.38676763, g_loss: 0.69124281\n",
      "Step: [16771] d_loss: 1.38760400, g_loss: 0.69416916\n",
      "Step: [16772] d_loss: 1.38707221, g_loss: 0.69383454\n",
      "Step: [16773] d_loss: 1.38661695, g_loss: 0.69107866\n",
      "Step: [16774] d_loss: 1.38661456, g_loss: 0.69015861\n",
      "Step: [16775] d_loss: 1.38644695, g_loss: 0.69203794\n",
      "Step: [16776] d_loss: 1.38621926, g_loss: 0.69305646\n",
      "Step: [16777] d_loss: 1.38631463, g_loss: 0.69336855\n",
      "Step: [16778] d_loss: 1.38621914, g_loss: 0.69418961\n",
      "Step: [16779] d_loss: 1.38624668, g_loss: 0.69376487\n",
      "Step: [16780] d_loss: 1.38638449, g_loss: 0.69352889\n",
      "Step: [16781] d_loss: 1.38629758, g_loss: 0.69278741\n",
      "Step: [16782] d_loss: 1.38644099, g_loss: 0.69355905\n",
      "Step: [16783] d_loss: 1.38627326, g_loss: 0.69306529\n",
      "Step: [16784] d_loss: 1.38628960, g_loss: 0.69349360\n",
      "Step: [16785] d_loss: 1.38633394, g_loss: 0.69339049\n",
      "Step: [16786] d_loss: 1.38628745, g_loss: 0.69311690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16787] d_loss: 1.38626850, g_loss: 0.69293672\n",
      "Step: [16788] d_loss: 1.38634992, g_loss: 0.69317353\n",
      "Step: [16789] d_loss: 1.38633525, g_loss: 0.69313258\n",
      "Step: [16790] d_loss: 1.38642490, g_loss: 0.69385576\n",
      "Step: [16791] d_loss: 1.38632941, g_loss: 0.69353855\n",
      "Step: [16792] d_loss: 1.38633776, g_loss: 0.69275361\n",
      "Step: [16793] d_loss: 1.38629794, g_loss: 0.69299281\n",
      "Step: [16794] d_loss: 1.38628602, g_loss: 0.69344908\n",
      "Step: [16795] d_loss: 1.38635695, g_loss: 0.69352472\n",
      "Step: [16796] d_loss: 1.38626003, g_loss: 0.69310266\n",
      "Step: [16797] d_loss: 1.38637316, g_loss: 0.69287938\n",
      "Step: [16798] d_loss: 1.38644707, g_loss: 0.69376522\n",
      "Step: [16799] d_loss: 1.38638961, g_loss: 0.69330758\n",
      "Step: [16800] d_loss: 1.38628387, g_loss: 0.69330657\n",
      "Step: [16801] d_loss: 1.38623774, g_loss: 0.69285965\n",
      "Step: [16802] d_loss: 1.38627386, g_loss: 0.69345832\n",
      "Step: [16803] d_loss: 1.38627458, g_loss: 0.69359148\n",
      "Step: [16804] d_loss: 1.38625944, g_loss: 0.69326603\n",
      "Step: [16805] d_loss: 1.38628304, g_loss: 0.69290274\n",
      "Step: [16806] d_loss: 1.38627374, g_loss: 0.69309884\n",
      "Step: [16807] d_loss: 1.38629758, g_loss: 0.69312298\n",
      "Step: [16808] d_loss: 1.38628173, g_loss: 0.69356549\n",
      "Step: [16809] d_loss: 1.38624525, g_loss: 0.69319963\n",
      "Step: [16810] d_loss: 1.38625836, g_loss: 0.69307578\n",
      "Step: [16811] d_loss: 1.38626206, g_loss: 0.69337213\n",
      "Step: [16812] d_loss: 1.38624334, g_loss: 0.69309270\n",
      "Step: [16813] d_loss: 1.38623607, g_loss: 0.69324851\n",
      "Step: [16814] d_loss: 1.38622355, g_loss: 0.69367754\n",
      "Step: [16815] d_loss: 1.38621986, g_loss: 0.69266015\n",
      "Step: [16816] d_loss: 1.38623929, g_loss: 0.69337404\n",
      "Step: [16817] d_loss: 1.38624179, g_loss: 0.69372094\n",
      "Step: [16818] d_loss: 1.38632786, g_loss: 0.69417566\n",
      "Step: [16819] d_loss: 1.38622665, g_loss: 0.69288117\n",
      "Step: [16820] d_loss: 1.38617992, g_loss: 0.69289398\n",
      "Step: [16821] d_loss: 1.38616753, g_loss: 0.69382811\n",
      "Step: [16822] d_loss: 1.38663578, g_loss: 0.69256443\n",
      "Step: [16823] d_loss: 1.38625979, g_loss: 0.69244254\n",
      "Step: [16824] d_loss: 1.38655877, g_loss: 0.69292724\n",
      "Step: [16825] d_loss: 1.38634014, g_loss: 0.69253445\n",
      "Step: [16826] d_loss: 1.38635576, g_loss: 0.69329393\n",
      "Step: [16827] d_loss: 1.38632464, g_loss: 0.69432783\n",
      "Step: [16828] d_loss: 1.38627815, g_loss: 0.69385159\n",
      "Step: [16829] d_loss: 1.38631988, g_loss: 0.69445753\n",
      "Step: [16830] d_loss: 1.38629544, g_loss: 0.69334096\n",
      "Step: [16831] d_loss: 1.38629246, g_loss: 0.69227409\n",
      "Step: [16832] d_loss: 1.38626099, g_loss: 0.69348264\n",
      "Step: [16833] d_loss: 1.38627934, g_loss: 0.69262660\n",
      "Step: [16834] d_loss: 1.38627100, g_loss: 0.69307792\n",
      "Step: [16835] d_loss: 1.38714933, g_loss: 0.69079584\n",
      "Step: [16836] d_loss: 1.38623345, g_loss: 0.69441909\n",
      "Step: [16837] d_loss: 1.38629532, g_loss: 0.69308925\n",
      "Step: [16838] d_loss: 1.38628149, g_loss: 0.69303107\n",
      "Step: [16839] d_loss: 1.38627064, g_loss: 0.69338214\n",
      "Step: [16840] d_loss: 1.38626099, g_loss: 0.69370657\n",
      "Step: [16841] d_loss: 1.38627052, g_loss: 0.69324541\n",
      "Step: [16842] d_loss: 1.38624847, g_loss: 0.69274342\n",
      "Step: [16843] d_loss: 1.38625026, g_loss: 0.69344360\n",
      "Step: [16844] d_loss: 1.38629937, g_loss: 0.69322979\n",
      "Step: [16845] d_loss: 1.38625133, g_loss: 0.69433653\n",
      "Step: [16846] d_loss: 1.38628054, g_loss: 0.69337475\n",
      "Step: [16847] d_loss: 1.38625252, g_loss: 0.69313264\n",
      "Step: [16848] d_loss: 1.38624954, g_loss: 0.69269198\n",
      "Step: [16849] d_loss: 1.38623881, g_loss: 0.69378459\n",
      "Step: [16850] d_loss: 1.38625264, g_loss: 0.69338059\n",
      "Step: [16851] d_loss: 1.38624036, g_loss: 0.69303024\n",
      "Step: [16852] d_loss: 1.38622904, g_loss: 0.69346023\n",
      "Step: [16853] d_loss: 1.38626981, g_loss: 0.69350314\n",
      "Step: [16854] d_loss: 1.38591778, g_loss: 0.69428098\n",
      "Step: [16855] d_loss: 1.38622880, g_loss: 0.69286263\n",
      "Step: [16856] d_loss: 1.38622737, g_loss: 0.69382560\n",
      "Step: [16857] d_loss: 1.38620245, g_loss: 0.69325554\n",
      "Step: [16858] d_loss: 1.38627696, g_loss: 0.69296217\n",
      "Step: [16859] d_loss: 1.38620234, g_loss: 0.69401205\n",
      "Step: [16860] d_loss: 1.38620615, g_loss: 0.69246292\n",
      "Step: [16861] d_loss: 1.38627625, g_loss: 0.69284815\n",
      "Step: [16862] d_loss: 1.38626337, g_loss: 0.69416255\n",
      "Step: [16863] d_loss: 1.38629115, g_loss: 0.69326901\n",
      "Step: [16864] d_loss: 1.38629949, g_loss: 0.69390917\n",
      "Step: [16865] d_loss: 1.38631892, g_loss: 0.69268519\n",
      "Step: [16866] d_loss: 1.38630199, g_loss: 0.69278157\n",
      "Step: [16867] d_loss: 1.38626063, g_loss: 0.69308484\n",
      "Step: [16868] d_loss: 1.38627839, g_loss: 0.69389802\n",
      "Step: [16869] d_loss: 1.38627791, g_loss: 0.69314003\n",
      "Step: [16870] d_loss: 1.38628745, g_loss: 0.69304955\n",
      "Step: [16871] d_loss: 1.38629091, g_loss: 0.69318056\n",
      "Step: [16872] d_loss: 1.38623106, g_loss: 0.69273508\n",
      "Step: [16873] d_loss: 1.38626420, g_loss: 0.69356489\n",
      "Step: [16874] d_loss: 1.38624561, g_loss: 0.69284523\n",
      "Step: [16875] d_loss: 1.38628030, g_loss: 0.69322246\n",
      "Step: [16876] d_loss: 1.38624167, g_loss: 0.69340920\n",
      "Step: [16877] d_loss: 1.38624001, g_loss: 0.69359833\n",
      "Step: [16878] d_loss: 1.38632584, g_loss: 0.69406259\n",
      "Step: [16879] d_loss: 1.38636851, g_loss: 0.69219244\n",
      "Step: [16880] d_loss: 1.38651228, g_loss: 0.69270957\n",
      "Step: [16881] d_loss: 1.38648367, g_loss: 0.69195485\n",
      "Step: [16882] d_loss: 1.38655221, g_loss: 0.69339621\n",
      "Step: [16883] d_loss: 1.38654733, g_loss: 0.69262004\n",
      "Step: [16884] d_loss: 1.38648820, g_loss: 0.69436771\n",
      "Step: [16885] d_loss: 1.38641810, g_loss: 0.69394803\n",
      "Step: [16886] d_loss: 1.38634288, g_loss: 0.69541013\n",
      "Step: [16887] d_loss: 1.38636208, g_loss: 0.69377697\n",
      "Step: [16888] d_loss: 1.38608611, g_loss: 0.69449437\n",
      "Step: [16889] d_loss: 1.38629913, g_loss: 0.69357526\n",
      "Step: [16890] d_loss: 1.38644052, g_loss: 0.69214982\n",
      "Step: [16891] d_loss: 1.38655484, g_loss: 0.69439709\n",
      "Step: [16892] d_loss: 1.38655043, g_loss: 0.69355369\n",
      "Step: [16893] d_loss: 1.38648772, g_loss: 0.69454783\n",
      "Step: [16894] d_loss: 1.38645077, g_loss: 0.69280916\n",
      "Step: [16895] d_loss: 1.38638294, g_loss: 0.69327283\n",
      "Step: [16896] d_loss: 1.38630581, g_loss: 0.69246733\n",
      "Step: [16897] d_loss: 1.38628972, g_loss: 0.69361496\n",
      "Step: [16898] d_loss: 1.38633633, g_loss: 0.69424868\n",
      "Step: [16899] d_loss: 1.38638234, g_loss: 0.69287813\n",
      "Step: [16900] d_loss: 1.38644695, g_loss: 0.69310153\n",
      "Step: [16901] d_loss: 1.38647938, g_loss: 0.69247246\n",
      "Step: [16902] d_loss: 1.38638365, g_loss: 0.69465083\n",
      "Step: [16903] d_loss: 1.38634515, g_loss: 0.69399482\n",
      "Step: [16904] d_loss: 1.38636303, g_loss: 0.69286454\n",
      "Step: [16905] d_loss: 1.38627481, g_loss: 0.69196439\n",
      "Step: [16906] d_loss: 1.38629532, g_loss: 0.69199276\n",
      "Step: [16907] d_loss: 1.38624096, g_loss: 0.69308877\n",
      "Step: [16908] d_loss: 1.38631964, g_loss: 0.69332451\n",
      "Step: [16909] d_loss: 1.38623202, g_loss: 0.69329107\n",
      "Step: [16910] d_loss: 1.38626361, g_loss: 0.69361484\n",
      "Step: [16911] d_loss: 1.38630152, g_loss: 0.69300950\n",
      "Step: [16912] d_loss: 1.38627374, g_loss: 0.69365489\n",
      "Step: [16913] d_loss: 1.38620663, g_loss: 0.69325638\n",
      "Step: [16914] d_loss: 1.38622713, g_loss: 0.69310474\n",
      "Step: [16915] d_loss: 1.38619101, g_loss: 0.69309270\n",
      "Step: [16916] d_loss: 1.38623118, g_loss: 0.69330639\n",
      "Step: [16917] d_loss: 1.38620996, g_loss: 0.69323117\n",
      "Step: [16918] d_loss: 1.38621747, g_loss: 0.69352627\n",
      "Step: [16919] d_loss: 1.38644183, g_loss: 0.69318521\n",
      "Step: [16920] d_loss: 1.38677979, g_loss: 0.69124401\n",
      "Step: [16921] d_loss: 1.38688970, g_loss: 0.69419324\n",
      "Step: [16922] d_loss: 1.38692617, g_loss: 0.69301617\n",
      "Step: [16923] d_loss: 1.38676000, g_loss: 0.69411284\n",
      "Step: [16924] d_loss: 1.38666940, g_loss: 0.69245070\n",
      "Step: [16925] d_loss: 1.38629019, g_loss: 0.69521236\n",
      "Step: [16926] d_loss: 1.38648665, g_loss: 0.69432324\n",
      "Step: [16927] d_loss: 1.38634336, g_loss: 0.69475770\n",
      "Step: [16928] d_loss: 1.38622451, g_loss: 0.69319886\n",
      "Step: [16929] d_loss: 1.38626742, g_loss: 0.69263709\n",
      "Step: [16930] d_loss: 1.38621676, g_loss: 0.69234359\n",
      "Step: [16931] d_loss: 1.38625383, g_loss: 0.69242382\n",
      "Step: [16932] d_loss: 1.38625741, g_loss: 0.69382709\n",
      "Step: [16933] d_loss: 1.38640738, g_loss: 0.69518894\n",
      "Step: [16934] d_loss: 1.38668060, g_loss: 0.69722569\n",
      "Step: [16935] d_loss: 1.38667619, g_loss: 0.69431049\n",
      "Step: [16936] d_loss: 1.38679540, g_loss: 0.69233793\n",
      "Step: [16937] d_loss: 1.38674784, g_loss: 0.68953991\n",
      "Step: [16938] d_loss: 1.38672447, g_loss: 0.69226182\n",
      "Step: [16939] d_loss: 1.38654947, g_loss: 0.69532609\n",
      "Step: [16940] d_loss: 1.38641596, g_loss: 0.69684660\n",
      "Step: [16941] d_loss: 1.38644481, g_loss: 0.69643891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [16942] d_loss: 1.38665104, g_loss: 0.69398469\n",
      "Step: [16943] d_loss: 1.38676906, g_loss: 0.68997526\n",
      "Step: [16944] d_loss: 1.38685882, g_loss: 0.68966174\n",
      "Step: [16945] d_loss: 1.38676894, g_loss: 0.69518960\n",
      "Step: [16946] d_loss: 1.38638496, g_loss: 0.69616073\n",
      "Step: [16947] d_loss: 1.38641977, g_loss: 0.69699091\n",
      "Step: [16948] d_loss: 1.38615561, g_loss: 0.69208658\n",
      "Step: [16949] d_loss: 1.38615811, g_loss: 0.69075608\n",
      "Step: [16950] d_loss: 1.38631439, g_loss: 0.69173855\n",
      "Step: [16951] d_loss: 1.38806856, g_loss: 0.68716407\n",
      "Step: [16952] d_loss: 1.38631487, g_loss: 0.69494426\n",
      "Step: [16953] d_loss: 1.38634968, g_loss: 0.69379854\n",
      "Step: [16954] d_loss: 1.38630199, g_loss: 0.69249332\n",
      "Step: [16955] d_loss: 1.38635921, g_loss: 0.69229299\n",
      "Step: [16956] d_loss: 1.38610685, g_loss: 0.69331574\n",
      "Step: [16957] d_loss: 1.38649678, g_loss: 0.69204652\n",
      "Step: [16958] d_loss: 1.38692594, g_loss: 0.69405371\n",
      "Step: [16959] d_loss: 1.38730061, g_loss: 0.69736254\n",
      "Step: [16960] d_loss: 1.38710952, g_loss: 0.69976234\n",
      "Step: [16961] d_loss: 1.38683677, g_loss: 0.69705218\n",
      "Step: [16962] d_loss: 1.38652492, g_loss: 0.69300425\n",
      "Step: [16963] d_loss: 1.38635325, g_loss: 0.69109678\n",
      "Step: [16964] d_loss: 1.38625765, g_loss: 0.69152212\n",
      "Step: [16965] d_loss: 1.38628888, g_loss: 0.69321275\n",
      "Step: [16966] d_loss: 1.38632381, g_loss: 0.69390905\n",
      "Step: [16967] d_loss: 1.38633370, g_loss: 0.69386762\n",
      "Step: [16968] d_loss: 1.38634372, g_loss: 0.69316685\n",
      "Step: [16969] d_loss: 1.38631082, g_loss: 0.69278336\n",
      "Step: [16970] d_loss: 1.38626337, g_loss: 0.69284260\n",
      "Step: [16971] d_loss: 1.38632774, g_loss: 0.69275534\n",
      "Step: [16972] d_loss: 1.38627028, g_loss: 0.69312185\n",
      "Step: [16973] d_loss: 1.38630044, g_loss: 0.69272679\n",
      "Step: [16974] d_loss: 1.38631070, g_loss: 0.69301081\n",
      "Step: [16975] d_loss: 1.38714135, g_loss: 0.69205487\n",
      "Step: [16976] d_loss: 1.38640404, g_loss: 0.69331491\n",
      "Step: [16977] d_loss: 1.38646054, g_loss: 0.69375056\n",
      "Step: [16978] d_loss: 1.38648474, g_loss: 0.69420147\n",
      "Step: [16979] d_loss: 1.38640809, g_loss: 0.69294453\n",
      "Step: [16980] d_loss: 1.38637972, g_loss: 0.69327480\n",
      "Step: [16981] d_loss: 1.38635862, g_loss: 0.69320846\n",
      "Step: [16982] d_loss: 1.38626873, g_loss: 0.69316643\n",
      "Step: [16983] d_loss: 1.38628042, g_loss: 0.69289482\n",
      "Step: [16984] d_loss: 1.38643670, g_loss: 0.69282299\n",
      "Step: [16985] d_loss: 1.38661814, g_loss: 0.69424188\n",
      "Step: [16986] d_loss: 1.38688922, g_loss: 0.69182789\n",
      "Step: [16987] d_loss: 1.38685465, g_loss: 0.69084132\n",
      "Step: [16988] d_loss: 1.38670397, g_loss: 0.68961602\n",
      "Step: [16989] d_loss: 1.38656604, g_loss: 0.69131315\n",
      "Step: [16990] d_loss: 1.38639283, g_loss: 0.69309556\n",
      "Step: [16991] d_loss: 1.38631594, g_loss: 0.69440007\n",
      "Step: [16992] d_loss: 1.38636947, g_loss: 0.69346154\n",
      "Step: [16993] d_loss: 1.38634753, g_loss: 0.69325233\n",
      "Step: [16994] d_loss: 1.38632464, g_loss: 0.69308829\n",
      "Step: [16995] d_loss: 1.38627720, g_loss: 0.69298434\n",
      "Step: [16996] d_loss: 1.38634133, g_loss: 0.69327444\n",
      "Step: [16997] d_loss: 1.38631189, g_loss: 0.69325006\n",
      "Step: [16998] d_loss: 1.38632655, g_loss: 0.69314110\n",
      "Step: [16999] d_loss: 1.38627744, g_loss: 0.69313622\n",
      "Step: [17000] d_loss: 1.38628817, g_loss: 0.69323182\n",
      "Step: [17001] d_loss: 1.38628626, g_loss: 0.69313014\n",
      "Step: [17002] d_loss: 1.38636100, g_loss: 0.69310987\n",
      "Step: [17003] d_loss: 1.38633764, g_loss: 0.69318175\n",
      "Step: [17004] d_loss: 1.38626623, g_loss: 0.69315428\n",
      "Step: [17005] d_loss: 1.38629007, g_loss: 0.69336009\n",
      "Step: [17006] d_loss: 1.38631678, g_loss: 0.69327331\n",
      "Step: [17007] d_loss: 1.38627923, g_loss: 0.69322157\n",
      "Step: [17008] d_loss: 1.38628983, g_loss: 0.69312501\n",
      "Step: [17009] d_loss: 1.38626015, g_loss: 0.69324285\n",
      "Step: [17010] d_loss: 1.38628387, g_loss: 0.69335771\n",
      "Step: [17011] d_loss: 1.38628626, g_loss: 0.69335693\n",
      "Step: [17012] d_loss: 1.38630128, g_loss: 0.69306028\n",
      "Step: [17013] d_loss: 1.38630152, g_loss: 0.69292963\n",
      "Step: [17014] d_loss: 1.38629639, g_loss: 0.69287807\n",
      "Step: [17015] d_loss: 1.38629770, g_loss: 0.69311762\n",
      "Step: [17016] d_loss: 1.38627529, g_loss: 0.69319904\n",
      "Step: [17017] d_loss: 1.38630199, g_loss: 0.69327879\n",
      "Step: [17018] d_loss: 1.38628721, g_loss: 0.69318396\n",
      "Step: [17019] d_loss: 1.38627958, g_loss: 0.69309193\n",
      "Step: [17020] d_loss: 1.38628197, g_loss: 0.69313049\n",
      "Step: [17021] d_loss: 1.38630581, g_loss: 0.69317669\n",
      "Step: [17022] d_loss: 1.38628459, g_loss: 0.69316918\n",
      "Step: [17023] d_loss: 1.38624001, g_loss: 0.69311583\n",
      "Step: [17024] d_loss: 1.38629937, g_loss: 0.69315672\n",
      "Step: [17025] d_loss: 1.38626111, g_loss: 0.69290543\n",
      "Step: [17026] d_loss: 1.38628244, g_loss: 0.69309938\n",
      "Step: [17027] d_loss: 1.38633192, g_loss: 0.69363266\n",
      "Step: [17028] d_loss: 1.38636923, g_loss: 0.69198096\n",
      "Step: [17029] d_loss: 1.38644147, g_loss: 0.69285947\n",
      "Step: [17030] d_loss: 1.38661110, g_loss: 0.69447446\n",
      "Step: [17031] d_loss: 1.38687027, g_loss: 0.69409311\n",
      "Step: [17032] d_loss: 1.38718653, g_loss: 0.69000638\n",
      "Step: [17033] d_loss: 1.38725352, g_loss: 0.69236153\n",
      "Step: [17034] d_loss: 1.38705564, g_loss: 0.69313741\n",
      "Step: [17035] d_loss: 1.38678277, g_loss: 0.69589597\n",
      "Step: [17036] d_loss: 1.38652909, g_loss: 0.69590729\n",
      "Step: [17037] d_loss: 1.38637912, g_loss: 0.69350183\n",
      "Step: [17038] d_loss: 1.38634229, g_loss: 0.69163787\n",
      "Step: [17039] d_loss: 1.38631272, g_loss: 0.69179845\n",
      "Step: [17040] d_loss: 1.38625729, g_loss: 0.69325942\n",
      "Step: [17041] d_loss: 1.38627207, g_loss: 0.69391894\n",
      "Step: [17042] d_loss: 1.38627672, g_loss: 0.69357336\n",
      "Step: [17043] d_loss: 1.38630855, g_loss: 0.69294935\n",
      "Step: [17044] d_loss: 1.38636434, g_loss: 0.69251430\n",
      "Step: [17045] d_loss: 1.38631201, g_loss: 0.69345343\n",
      "Step: [17046] d_loss: 1.38631511, g_loss: 0.69387662\n",
      "Step: [17047] d_loss: 1.38637590, g_loss: 0.69425845\n",
      "Step: [17048] d_loss: 1.38629484, g_loss: 0.69385076\n",
      "Step: [17049] d_loss: 1.38622165, g_loss: 0.69317842\n",
      "Step: [17050] d_loss: 1.38630176, g_loss: 0.69269168\n",
      "Step: [17051] d_loss: 1.38629508, g_loss: 0.69274259\n",
      "Step: [17052] d_loss: 1.38626349, g_loss: 0.69314384\n",
      "Step: [17053] d_loss: 1.38627565, g_loss: 0.69309068\n",
      "Step: [17054] d_loss: 1.38626349, g_loss: 0.69326067\n",
      "Step: [17055] d_loss: 1.38629138, g_loss: 0.69316578\n",
      "Step: [17056] d_loss: 1.38623071, g_loss: 0.69339693\n",
      "Step: [17057] d_loss: 1.38625562, g_loss: 0.69314116\n",
      "Step: [17058] d_loss: 1.38627100, g_loss: 0.69287753\n",
      "Step: [17059] d_loss: 1.38630283, g_loss: 0.69292867\n",
      "Step: [17060] d_loss: 1.38626695, g_loss: 0.69331706\n",
      "Step: [17061] d_loss: 1.38623571, g_loss: 0.69295746\n",
      "Step: [17062] d_loss: 1.38626635, g_loss: 0.69302368\n",
      "Step: [17063] d_loss: 1.38626802, g_loss: 0.69351447\n",
      "Step: [17064] d_loss: 1.38628852, g_loss: 0.69303423\n",
      "Step: [17065] d_loss: 1.38633060, g_loss: 0.69313204\n",
      "Step: [17066] d_loss: 1.38631153, g_loss: 0.69264388\n",
      "Step: [17067] d_loss: 1.38638902, g_loss: 0.69328821\n",
      "Step: [17068] d_loss: 1.38645148, g_loss: 0.69322807\n",
      "Step: [17069] d_loss: 1.38650203, g_loss: 0.69281489\n",
      "Step: [17070] d_loss: 1.38674700, g_loss: 0.69204307\n",
      "Step: [17071] d_loss: 1.38705492, g_loss: 0.69418573\n",
      "Step: [17072] d_loss: 1.38708997, g_loss: 0.69495332\n",
      "Step: [17073] d_loss: 1.38687587, g_loss: 0.69622964\n",
      "Step: [17074] d_loss: 1.38661635, g_loss: 0.69301844\n",
      "Step: [17075] d_loss: 1.38648546, g_loss: 0.69275582\n",
      "Step: [17076] d_loss: 1.38637769, g_loss: 0.69159073\n",
      "Step: [17077] d_loss: 1.38632309, g_loss: 0.69224876\n",
      "Step: [17078] d_loss: 1.38628244, g_loss: 0.69283164\n",
      "Step: [17079] d_loss: 1.38630319, g_loss: 0.69337219\n",
      "Step: [17080] d_loss: 1.38622463, g_loss: 0.69321483\n",
      "Step: [17081] d_loss: 1.38616955, g_loss: 0.69351947\n",
      "Step: [17082] d_loss: 1.38626742, g_loss: 0.69426656\n",
      "Step: [17083] d_loss: 1.38651514, g_loss: 0.69430089\n",
      "Step: [17084] d_loss: 1.38719606, g_loss: 0.69488293\n",
      "Step: [17085] d_loss: 1.38806868, g_loss: 0.69832069\n",
      "Step: [17086] d_loss: 1.38794684, g_loss: 0.69416285\n",
      "Step: [17087] d_loss: 1.38737392, g_loss: 0.69502693\n",
      "Step: [17088] d_loss: 1.38731468, g_loss: 0.69141436\n",
      "Step: [17089] d_loss: 1.38713241, g_loss: 0.69311398\n",
      "Step: [17090] d_loss: 1.38678622, g_loss: 0.69076037\n",
      "Step: [17091] d_loss: 1.38655460, g_loss: 0.69224215\n",
      "Step: [17092] d_loss: 1.38641953, g_loss: 0.69254351\n",
      "Step: [17093] d_loss: 1.38639832, g_loss: 0.69372320\n",
      "Step: [17094] d_loss: 1.38625526, g_loss: 0.69352937\n",
      "Step: [17095] d_loss: 1.38625979, g_loss: 0.69320649\n",
      "Step: [17096] d_loss: 1.38637686, g_loss: 0.69274676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17097] d_loss: 1.38639402, g_loss: 0.69198179\n",
      "Step: [17098] d_loss: 1.38650632, g_loss: 0.69335711\n",
      "Step: [17099] d_loss: 1.38629246, g_loss: 0.69404030\n",
      "Step: [17100] d_loss: 1.38634014, g_loss: 0.69406372\n",
      "Step: [17101] d_loss: 1.38633060, g_loss: 0.69356692\n",
      "Step: [17102] d_loss: 1.38627291, g_loss: 0.69252098\n",
      "Step: [17103] d_loss: 1.38626003, g_loss: 0.69373405\n",
      "Step: [17104] d_loss: 1.38663363, g_loss: 0.69437408\n",
      "Step: [17105] d_loss: 1.38642240, g_loss: 0.69414860\n",
      "Step: [17106] d_loss: 1.38632488, g_loss: 0.69303036\n",
      "Step: [17107] d_loss: 1.38630140, g_loss: 0.69244087\n",
      "Step: [17108] d_loss: 1.38636303, g_loss: 0.69266272\n",
      "Step: [17109] d_loss: 1.38611460, g_loss: 0.69268990\n",
      "Step: [17110] d_loss: 1.38631821, g_loss: 0.69308454\n",
      "Step: [17111] d_loss: 1.38628602, g_loss: 0.69330847\n",
      "Step: [17112] d_loss: 1.38629889, g_loss: 0.69335264\n",
      "Step: [17113] d_loss: 1.38626218, g_loss: 0.69324428\n",
      "Step: [17114] d_loss: 1.38659108, g_loss: 0.69354284\n",
      "Step: [17115] d_loss: 1.38621259, g_loss: 0.69320273\n",
      "Step: [17116] d_loss: 1.38635588, g_loss: 0.69332457\n",
      "Step: [17117] d_loss: 1.38621688, g_loss: 0.69296122\n",
      "Step: [17118] d_loss: 1.38643622, g_loss: 0.69336843\n",
      "Step: [17119] d_loss: 1.38670325, g_loss: 0.69389772\n",
      "Step: [17120] d_loss: 1.38679278, g_loss: 0.69209021\n",
      "Step: [17121] d_loss: 1.38717568, g_loss: 0.69239771\n",
      "Step: [17122] d_loss: 1.38817024, g_loss: 0.69131887\n",
      "Step: [17123] d_loss: 1.38836050, g_loss: 0.69450223\n",
      "Step: [17124] d_loss: 1.38760638, g_loss: 0.69198394\n",
      "Step: [17125] d_loss: 1.38698900, g_loss: 0.69503188\n",
      "Step: [17126] d_loss: 1.38646269, g_loss: 0.69408655\n",
      "Step: [17127] d_loss: 1.38634551, g_loss: 0.69374865\n",
      "Step: [17128] d_loss: 1.38633084, g_loss: 0.69322497\n",
      "Step: [17129] d_loss: 1.38630414, g_loss: 0.69276011\n",
      "Step: [17130] d_loss: 1.38625884, g_loss: 0.69271499\n",
      "Step: [17131] d_loss: 1.38632584, g_loss: 0.69268060\n",
      "Step: [17132] d_loss: 1.38629389, g_loss: 0.69284189\n",
      "Step: [17133] d_loss: 1.38627887, g_loss: 0.69329345\n",
      "Step: [17134] d_loss: 1.38630748, g_loss: 0.69350487\n",
      "Step: [17135] d_loss: 1.38635504, g_loss: 0.69340765\n",
      "Step: [17136] d_loss: 1.38632059, g_loss: 0.69309771\n",
      "Step: [17137] d_loss: 1.38630879, g_loss: 0.69309819\n",
      "Step: [17138] d_loss: 1.38627577, g_loss: 0.69295180\n",
      "Step: [17139] d_loss: 1.38621998, g_loss: 0.69297218\n",
      "Step: [17140] d_loss: 1.38629866, g_loss: 0.69306421\n",
      "Step: [17141] d_loss: 1.38630843, g_loss: 0.69310474\n",
      "Step: [17142] d_loss: 1.38623774, g_loss: 0.69327605\n",
      "Step: [17143] d_loss: 1.38628864, g_loss: 0.69331217\n",
      "Step: [17144] d_loss: 1.38629127, g_loss: 0.69295722\n",
      "Step: [17145] d_loss: 1.38636148, g_loss: 0.69317603\n",
      "Step: [17146] d_loss: 1.38631058, g_loss: 0.69320798\n",
      "Step: [17147] d_loss: 1.38593841, g_loss: 0.69421387\n",
      "Step: [17148] d_loss: 1.38633144, g_loss: 0.69293487\n",
      "Step: [17149] d_loss: 1.38632822, g_loss: 0.69280756\n",
      "Step: [17150] d_loss: 1.38630390, g_loss: 0.69311285\n",
      "Step: [17151] d_loss: 1.38629007, g_loss: 0.69322926\n",
      "Step: [17152] d_loss: 1.38628387, g_loss: 0.69319940\n",
      "Step: [17153] d_loss: 1.38629854, g_loss: 0.69362748\n",
      "Step: [17154] d_loss: 1.38630056, g_loss: 0.69425881\n",
      "Step: [17155] d_loss: 1.38642073, g_loss: 0.69231939\n",
      "Step: [17156] d_loss: 1.38651359, g_loss: 0.69212878\n",
      "Step: [17157] d_loss: 1.38647342, g_loss: 0.69350606\n",
      "Step: [17158] d_loss: 1.38643348, g_loss: 0.69283932\n",
      "Step: [17159] d_loss: 1.38628483, g_loss: 0.69291854\n",
      "Step: [17160] d_loss: 1.38632703, g_loss: 0.69277489\n",
      "Step: [17161] d_loss: 1.38628840, g_loss: 0.69288945\n",
      "Step: [17162] d_loss: 1.38627028, g_loss: 0.69306183\n",
      "Step: [17163] d_loss: 1.38626993, g_loss: 0.69330513\n",
      "Step: [17164] d_loss: 1.38629472, g_loss: 0.69328445\n",
      "Step: [17165] d_loss: 1.38628888, g_loss: 0.69324338\n",
      "Step: [17166] d_loss: 1.38629985, g_loss: 0.69321698\n",
      "Step: [17167] d_loss: 1.38630104, g_loss: 0.69306076\n",
      "Step: [17168] d_loss: 1.38629198, g_loss: 0.69293129\n",
      "Step: [17169] d_loss: 1.38627648, g_loss: 0.69315022\n",
      "Step: [17170] d_loss: 1.38629770, g_loss: 0.69329643\n",
      "Step: [17171] d_loss: 1.38621128, g_loss: 0.69330204\n",
      "Step: [17172] d_loss: 1.38627434, g_loss: 0.69323063\n",
      "Step: [17173] d_loss: 1.38625813, g_loss: 0.69313359\n",
      "Step: [17174] d_loss: 1.38627577, g_loss: 0.69312346\n",
      "Step: [17175] d_loss: 1.38629544, g_loss: 0.69318545\n",
      "Step: [17176] d_loss: 1.38620579, g_loss: 0.69324386\n",
      "Step: [17177] d_loss: 1.38671303, g_loss: 0.69345671\n",
      "Step: [17178] d_loss: 1.38630474, g_loss: 0.69294345\n",
      "Step: [17179] d_loss: 1.38626862, g_loss: 0.69321668\n",
      "Step: [17180] d_loss: 1.38629603, g_loss: 0.69314963\n",
      "Step: [17181] d_loss: 1.38626695, g_loss: 0.69319338\n",
      "Step: [17182] d_loss: 1.38621354, g_loss: 0.69331354\n",
      "Step: [17183] d_loss: 1.38622904, g_loss: 0.69345027\n",
      "Step: [17184] d_loss: 1.38630772, g_loss: 0.69322497\n",
      "Step: [17185] d_loss: 1.38625145, g_loss: 0.69338369\n",
      "Step: [17186] d_loss: 1.38631153, g_loss: 0.69342268\n",
      "Step: [17187] d_loss: 1.38627934, g_loss: 0.69286549\n",
      "Step: [17188] d_loss: 1.38629031, g_loss: 0.69304103\n",
      "Step: [17189] d_loss: 1.38630152, g_loss: 0.69298202\n",
      "Step: [17190] d_loss: 1.38625395, g_loss: 0.69338238\n",
      "Step: [17191] d_loss: 1.38617814, g_loss: 0.69369119\n",
      "Step: [17192] d_loss: 1.38631916, g_loss: 0.69279039\n",
      "Step: [17193] d_loss: 1.38626266, g_loss: 0.69278163\n",
      "Step: [17194] d_loss: 1.38624990, g_loss: 0.69314468\n",
      "Step: [17195] d_loss: 1.38628411, g_loss: 0.69326961\n",
      "Step: [17196] d_loss: 1.38617659, g_loss: 0.69345796\n",
      "Step: [17197] d_loss: 1.38622403, g_loss: 0.69327331\n",
      "Step: [17198] d_loss: 1.38621974, g_loss: 0.69361484\n",
      "Step: [17199] d_loss: 1.38624859, g_loss: 0.69302583\n",
      "Step: [17200] d_loss: 1.38646054, g_loss: 0.69390559\n",
      "Step: [17201] d_loss: 1.38626158, g_loss: 0.69332218\n",
      "Step: [17202] d_loss: 1.38623023, g_loss: 0.69310045\n",
      "Step: [17203] d_loss: 1.38634050, g_loss: 0.69241440\n",
      "Step: [17204] d_loss: 1.38662839, g_loss: 0.69375908\n",
      "Step: [17205] d_loss: 1.38671303, g_loss: 0.69207287\n",
      "Step: [17206] d_loss: 1.38666201, g_loss: 0.69315547\n",
      "Step: [17207] d_loss: 1.38632822, g_loss: 0.69442594\n",
      "Step: [17208] d_loss: 1.38648319, g_loss: 0.69138420\n",
      "Step: [17209] d_loss: 1.38620639, g_loss: 0.69375157\n",
      "Step: [17210] d_loss: 1.38628912, g_loss: 0.69442689\n",
      "Step: [17211] d_loss: 1.38629448, g_loss: 0.69371676\n",
      "Step: [17212] d_loss: 1.38631213, g_loss: 0.69313622\n",
      "Step: [17213] d_loss: 1.38629615, g_loss: 0.69243693\n",
      "Step: [17214] d_loss: 1.38628328, g_loss: 0.69383729\n",
      "Step: [17215] d_loss: 1.38629639, g_loss: 0.69517732\n",
      "Step: [17216] d_loss: 1.38632417, g_loss: 0.69375843\n",
      "Step: [17217] d_loss: 1.38632965, g_loss: 0.69352436\n",
      "Step: [17218] d_loss: 1.38634479, g_loss: 0.69284260\n",
      "Step: [17219] d_loss: 1.38628840, g_loss: 0.69319785\n",
      "Step: [17220] d_loss: 1.38636541, g_loss: 0.69311607\n",
      "Step: [17221] d_loss: 1.38634634, g_loss: 0.69296598\n",
      "Step: [17222] d_loss: 1.38633990, g_loss: 0.69314575\n",
      "Step: [17223] d_loss: 1.38621294, g_loss: 0.69302440\n",
      "Step: [17224] d_loss: 1.38632250, g_loss: 0.69372642\n",
      "Step: [17225] d_loss: 1.38627577, g_loss: 0.69322896\n",
      "Step: [17226] d_loss: 1.38634956, g_loss: 0.69279945\n",
      "Step: [17227] d_loss: 1.38627291, g_loss: 0.69313073\n",
      "Step: [17228] d_loss: 1.38632822, g_loss: 0.69300634\n",
      "Step: [17229] d_loss: 1.38622427, g_loss: 0.69357955\n",
      "Step: [17230] d_loss: 1.38667583, g_loss: 0.69402027\n",
      "Step: [17231] d_loss: 1.38695574, g_loss: 0.69280124\n",
      "Step: [17232] d_loss: 1.38790631, g_loss: 0.69676197\n",
      "Step: [17233] d_loss: 1.38820863, g_loss: 0.69458711\n",
      "Step: [17234] d_loss: 1.38790154, g_loss: 0.69728184\n",
      "Step: [17235] d_loss: 1.38721383, g_loss: 0.69353962\n",
      "Step: [17236] d_loss: 1.38677049, g_loss: 0.69221663\n",
      "Step: [17237] d_loss: 1.38650715, g_loss: 0.69241744\n",
      "Step: [17238] d_loss: 1.38632107, g_loss: 0.69306517\n",
      "Step: [17239] d_loss: 1.38628364, g_loss: 0.69332194\n",
      "Step: [17240] d_loss: 1.38627410, g_loss: 0.69302481\n",
      "Step: [17241] d_loss: 1.38628554, g_loss: 0.69319946\n",
      "Step: [17242] d_loss: 1.38625240, g_loss: 0.69306868\n",
      "Step: [17243] d_loss: 1.38628697, g_loss: 0.69371104\n",
      "Step: [17244] d_loss: 1.38638604, g_loss: 0.69349396\n",
      "Step: [17245] d_loss: 1.38647532, g_loss: 0.69257283\n",
      "Step: [17246] d_loss: 1.38649678, g_loss: 0.69389588\n",
      "Step: [17247] d_loss: 1.38644826, g_loss: 0.69407630\n",
      "Step: [17248] d_loss: 1.38638806, g_loss: 0.69365346\n",
      "Step: [17249] d_loss: 1.38634562, g_loss: 0.69263393\n",
      "Step: [17250] d_loss: 1.38627195, g_loss: 0.69250578\n",
      "Step: [17251] d_loss: 1.38625193, g_loss: 0.69310981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17252] d_loss: 1.38629043, g_loss: 0.69334412\n",
      "Step: [17253] d_loss: 1.38627064, g_loss: 0.69343615\n",
      "Step: [17254] d_loss: 1.38622499, g_loss: 0.69324052\n",
      "Step: [17255] d_loss: 1.38624358, g_loss: 0.69294286\n",
      "Step: [17256] d_loss: 1.38628054, g_loss: 0.69290185\n",
      "Step: [17257] d_loss: 1.38630044, g_loss: 0.69293201\n",
      "Step: [17258] d_loss: 1.38631153, g_loss: 0.69331002\n",
      "Step: [17259] d_loss: 1.38624525, g_loss: 0.69342005\n",
      "Step: [17260] d_loss: 1.38629842, g_loss: 0.69290090\n",
      "Step: [17261] d_loss: 1.38625431, g_loss: 0.69312781\n",
      "Step: [17262] d_loss: 1.38632810, g_loss: 0.69251299\n",
      "Step: [17263] d_loss: 1.38626981, g_loss: 0.69307816\n",
      "Step: [17264] d_loss: 1.38627887, g_loss: 0.69279051\n",
      "Step: [17265] d_loss: 1.38631499, g_loss: 0.69311452\n",
      "Step: [17266] d_loss: 1.38632059, g_loss: 0.69327354\n",
      "Step: [17267] d_loss: 1.38626671, g_loss: 0.69335204\n",
      "Step: [17268] d_loss: 1.38628435, g_loss: 0.69336671\n",
      "Step: [17269] d_loss: 1.38624668, g_loss: 0.69301963\n",
      "Step: [17270] d_loss: 1.38623989, g_loss: 0.69300026\n",
      "Step: [17271] d_loss: 1.38630319, g_loss: 0.69269586\n",
      "Step: [17272] d_loss: 1.38627636, g_loss: 0.69307923\n",
      "Step: [17273] d_loss: 1.38627672, g_loss: 0.69323504\n",
      "Step: [17274] d_loss: 1.38625503, g_loss: 0.69307137\n",
      "Step: [17275] d_loss: 1.38624763, g_loss: 0.69307518\n",
      "Step: [17276] d_loss: 1.38625884, g_loss: 0.69297802\n",
      "Step: [17277] d_loss: 1.38629067, g_loss: 0.69389260\n",
      "Step: [17278] d_loss: 1.38639450, g_loss: 0.69373107\n",
      "Step: [17279] d_loss: 1.38655782, g_loss: 0.69380295\n",
      "Step: [17280] d_loss: 1.38792157, g_loss: 0.69449091\n",
      "Step: [17281] d_loss: 1.38654256, g_loss: 0.69585550\n",
      "Step: [17282] d_loss: 1.38648129, g_loss: 0.69504535\n",
      "Step: [17283] d_loss: 1.38637209, g_loss: 0.69298917\n",
      "Step: [17284] d_loss: 1.38635755, g_loss: 0.69307226\n",
      "Step: [17285] d_loss: 1.38628006, g_loss: 0.69104600\n",
      "Step: [17286] d_loss: 1.38640285, g_loss: 0.69133246\n",
      "Step: [17287] d_loss: 1.38658416, g_loss: 0.69416815\n",
      "Step: [17288] d_loss: 1.38665700, g_loss: 0.69165766\n",
      "Step: [17289] d_loss: 1.38677597, g_loss: 0.69373655\n",
      "Step: [17290] d_loss: 1.38665438, g_loss: 0.69283819\n",
      "Step: [17291] d_loss: 1.38655663, g_loss: 0.69408321\n",
      "Step: [17292] d_loss: 1.38640976, g_loss: 0.69297230\n",
      "Step: [17293] d_loss: 1.38636017, g_loss: 0.69340467\n",
      "Step: [17294] d_loss: 1.38629746, g_loss: 0.69315708\n",
      "Step: [17295] d_loss: 1.38630414, g_loss: 0.69268596\n",
      "Step: [17296] d_loss: 1.38629782, g_loss: 0.69274378\n",
      "Step: [17297] d_loss: 1.38626301, g_loss: 0.69316649\n",
      "Step: [17298] d_loss: 1.38628399, g_loss: 0.69292784\n",
      "Step: [17299] d_loss: 1.38628054, g_loss: 0.69339657\n",
      "Step: [17300] d_loss: 1.38628435, g_loss: 0.69317985\n",
      "Step: [17301] d_loss: 1.38630962, g_loss: 0.69323683\n",
      "Step: [17302] d_loss: 1.38625538, g_loss: 0.69302309\n",
      "Step: [17303] d_loss: 1.38628292, g_loss: 0.69294560\n",
      "Step: [17304] d_loss: 1.38625872, g_loss: 0.69219172\n",
      "Step: [17305] d_loss: 1.38619995, g_loss: 0.69280732\n",
      "Step: [17306] d_loss: 1.38627613, g_loss: 0.69329882\n",
      "Step: [17307] d_loss: 1.38625085, g_loss: 0.69358009\n",
      "Step: [17308] d_loss: 1.38624108, g_loss: 0.69352853\n",
      "Step: [17309] d_loss: 1.38619149, g_loss: 0.69344330\n",
      "Step: [17310] d_loss: 1.38643765, g_loss: 0.69213986\n",
      "Step: [17311] d_loss: 1.38686299, g_loss: 0.69486749\n",
      "Step: [17312] d_loss: 1.38705850, g_loss: 0.69366443\n",
      "Step: [17313] d_loss: 1.38702655, g_loss: 0.69218761\n",
      "Step: [17314] d_loss: 1.38681364, g_loss: 0.69110787\n",
      "Step: [17315] d_loss: 1.38658643, g_loss: 0.69378865\n",
      "Step: [17316] d_loss: 1.38643992, g_loss: 0.69418752\n",
      "Step: [17317] d_loss: 1.38632226, g_loss: 0.69421059\n",
      "Step: [17318] d_loss: 1.38630939, g_loss: 0.69343835\n",
      "Step: [17319] d_loss: 1.38624740, g_loss: 0.69273639\n",
      "Step: [17320] d_loss: 1.38626206, g_loss: 0.69287574\n",
      "Step: [17321] d_loss: 1.38629174, g_loss: 0.69286430\n",
      "Step: [17322] d_loss: 1.38626599, g_loss: 0.69308066\n",
      "Step: [17323] d_loss: 1.38630748, g_loss: 0.69242358\n",
      "Step: [17324] d_loss: 1.38633275, g_loss: 0.69301015\n",
      "Step: [17325] d_loss: 1.38629794, g_loss: 0.69301718\n",
      "Step: [17326] d_loss: 1.38629448, g_loss: 0.69303668\n",
      "Step: [17327] d_loss: 1.38633657, g_loss: 0.69292331\n",
      "Step: [17328] d_loss: 1.38634658, g_loss: 0.69113427\n",
      "Step: [17329] d_loss: 1.38636208, g_loss: 0.69128418\n",
      "Step: [17330] d_loss: 1.38638759, g_loss: 0.69303143\n",
      "Step: [17331] d_loss: 1.38631797, g_loss: 0.69433904\n",
      "Step: [17332] d_loss: 1.38631415, g_loss: 0.69483113\n",
      "Step: [17333] d_loss: 1.38629997, g_loss: 0.69376135\n",
      "Step: [17334] d_loss: 1.38627601, g_loss: 0.69267899\n",
      "Step: [17335] d_loss: 1.38628399, g_loss: 0.69248450\n",
      "Step: [17336] d_loss: 1.38626862, g_loss: 0.69268441\n",
      "Step: [17337] d_loss: 1.38627958, g_loss: 0.69318080\n",
      "Step: [17338] d_loss: 1.38625181, g_loss: 0.69334406\n",
      "Step: [17339] d_loss: 1.38626409, g_loss: 0.69326067\n",
      "Step: [17340] d_loss: 1.38624287, g_loss: 0.69272208\n",
      "Step: [17341] d_loss: 1.38628852, g_loss: 0.69343162\n",
      "Step: [17342] d_loss: 1.38628149, g_loss: 0.69386303\n",
      "Step: [17343] d_loss: 1.38631952, g_loss: 0.69301909\n",
      "Step: [17344] d_loss: 1.38634288, g_loss: 0.69339287\n",
      "Step: [17345] d_loss: 1.38628924, g_loss: 0.69307148\n",
      "Step: [17346] d_loss: 1.38631439, g_loss: 0.69344538\n",
      "Step: [17347] d_loss: 1.38628995, g_loss: 0.69337237\n",
      "Step: [17348] d_loss: 1.38628089, g_loss: 0.69342220\n",
      "Step: [17349] d_loss: 1.38627052, g_loss: 0.69335359\n",
      "Step: [17350] d_loss: 1.38625789, g_loss: 0.69284034\n",
      "Step: [17351] d_loss: 1.38634932, g_loss: 0.69253320\n",
      "Step: [17352] d_loss: 1.38627791, g_loss: 0.69269550\n",
      "Step: [17353] d_loss: 1.38632107, g_loss: 0.69355392\n",
      "Step: [17354] d_loss: 1.38624120, g_loss: 0.69423538\n",
      "Step: [17355] d_loss: 1.38628078, g_loss: 0.69395244\n",
      "Step: [17356] d_loss: 1.38624835, g_loss: 0.69302738\n",
      "Step: [17357] d_loss: 1.38628840, g_loss: 0.69257283\n",
      "Step: [17358] d_loss: 1.38640070, g_loss: 0.69306511\n",
      "Step: [17359] d_loss: 1.38629889, g_loss: 0.69290698\n",
      "Step: [17360] d_loss: 1.38634527, g_loss: 0.69240022\n",
      "Step: [17361] d_loss: 1.38633466, g_loss: 0.69356036\n",
      "Step: [17362] d_loss: 1.38635826, g_loss: 0.69297254\n",
      "Step: [17363] d_loss: 1.38629246, g_loss: 0.69324780\n",
      "Step: [17364] d_loss: 1.38632298, g_loss: 0.69299424\n",
      "Step: [17365] d_loss: 1.38627696, g_loss: 0.69305527\n",
      "Step: [17366] d_loss: 1.38627410, g_loss: 0.69350743\n",
      "Step: [17367] d_loss: 1.38628578, g_loss: 0.69391692\n",
      "Step: [17368] d_loss: 1.38627541, g_loss: 0.69356441\n",
      "Step: [17369] d_loss: 1.38626170, g_loss: 0.69319159\n",
      "Step: [17370] d_loss: 1.38626695, g_loss: 0.69310635\n",
      "Step: [17371] d_loss: 1.38631606, g_loss: 0.69286388\n",
      "Step: [17372] d_loss: 1.38627458, g_loss: 0.69322932\n",
      "Step: [17373] d_loss: 1.38626361, g_loss: 0.69319695\n",
      "Step: [17374] d_loss: 1.38628042, g_loss: 0.69311392\n",
      "Step: [17375] d_loss: 1.38627791, g_loss: 0.69318116\n",
      "Step: [17376] d_loss: 1.38627446, g_loss: 0.69329625\n",
      "Step: [17377] d_loss: 1.38630056, g_loss: 0.69316119\n",
      "Step: [17378] d_loss: 1.38629341, g_loss: 0.69316459\n",
      "Step: [17379] d_loss: 1.38625813, g_loss: 0.69311255\n",
      "Step: [17380] d_loss: 1.38626099, g_loss: 0.69320440\n",
      "Step: [17381] d_loss: 1.38627279, g_loss: 0.69320548\n",
      "Step: [17382] d_loss: 1.38627279, g_loss: 0.69312572\n",
      "Step: [17383] d_loss: 1.38626814, g_loss: 0.69317073\n",
      "Step: [17384] d_loss: 1.38627410, g_loss: 0.69318390\n",
      "Step: [17385] d_loss: 1.38626885, g_loss: 0.69312930\n",
      "Step: [17386] d_loss: 1.38629973, g_loss: 0.69337523\n",
      "Step: [17387] d_loss: 1.38628531, g_loss: 0.69320285\n",
      "Step: [17388] d_loss: 1.38623917, g_loss: 0.69342810\n",
      "Step: [17389] d_loss: 1.38628626, g_loss: 0.69323456\n",
      "Step: [17390] d_loss: 1.38630700, g_loss: 0.69325328\n",
      "Step: [17391] d_loss: 1.38626194, g_loss: 0.69351363\n",
      "Step: [17392] d_loss: 1.38629544, g_loss: 0.69318646\n",
      "Step: [17393] d_loss: 1.38630509, g_loss: 0.69313163\n",
      "Step: [17394] d_loss: 1.38626420, g_loss: 0.69306672\n",
      "Step: [17395] d_loss: 1.38626027, g_loss: 0.69300622\n",
      "Step: [17396] d_loss: 1.38632035, g_loss: 0.69317544\n",
      "Step: [17397] d_loss: 1.38633251, g_loss: 0.69336420\n",
      "Step: [17398] d_loss: 1.38626337, g_loss: 0.69352692\n",
      "Step: [17399] d_loss: 1.38625634, g_loss: 0.69364697\n",
      "Step: [17400] d_loss: 1.38629901, g_loss: 0.69325852\n",
      "Step: [17401] d_loss: 1.38626695, g_loss: 0.69312948\n",
      "Step: [17402] d_loss: 1.38625264, g_loss: 0.69306409\n",
      "Step: [17403] d_loss: 1.38628006, g_loss: 0.69319600\n",
      "Step: [17404] d_loss: 1.38626218, g_loss: 0.69330704\n",
      "Step: [17405] d_loss: 1.38625455, g_loss: 0.69319892\n",
      "Step: [17406] d_loss: 1.38629603, g_loss: 0.69330913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17407] d_loss: 1.38626254, g_loss: 0.69312942\n",
      "Step: [17408] d_loss: 1.38629580, g_loss: 0.69315588\n",
      "Step: [17409] d_loss: 1.38631606, g_loss: 0.69302213\n",
      "Step: [17410] d_loss: 1.38631690, g_loss: 0.69317877\n",
      "Step: [17411] d_loss: 1.38632464, g_loss: 0.69317830\n",
      "Step: [17412] d_loss: 1.38626981, g_loss: 0.69337249\n",
      "Step: [17413] d_loss: 1.38625312, g_loss: 0.69330013\n",
      "Step: [17414] d_loss: 1.38629472, g_loss: 0.69323128\n",
      "Step: [17415] d_loss: 1.38629580, g_loss: 0.69311225\n",
      "Step: [17416] d_loss: 1.38624907, g_loss: 0.69312966\n",
      "Step: [17417] d_loss: 1.38628626, g_loss: 0.69317269\n",
      "Step: [17418] d_loss: 1.38625813, g_loss: 0.69319326\n",
      "Step: [17419] d_loss: 1.38627005, g_loss: 0.69316125\n",
      "Step: [17420] d_loss: 1.38626671, g_loss: 0.69308156\n",
      "Step: [17421] d_loss: 1.38624585, g_loss: 0.69314909\n",
      "Step: [17422] d_loss: 1.38640380, g_loss: 0.69180191\n",
      "Step: [17423] d_loss: 1.38627291, g_loss: 0.69349211\n",
      "Step: [17424] d_loss: 1.38626742, g_loss: 0.69342315\n",
      "Step: [17425] d_loss: 1.38630903, g_loss: 0.69373524\n",
      "Step: [17426] d_loss: 1.38635719, g_loss: 0.69320345\n",
      "Step: [17427] d_loss: 1.38632035, g_loss: 0.69294691\n",
      "Step: [17428] d_loss: 1.38638687, g_loss: 0.69271785\n",
      "Step: [17429] d_loss: 1.38628531, g_loss: 0.69300568\n",
      "Step: [17430] d_loss: 1.38628685, g_loss: 0.69302899\n",
      "Step: [17431] d_loss: 1.38629556, g_loss: 0.69349349\n",
      "Step: [17432] d_loss: 1.38628554, g_loss: 0.69322079\n",
      "Step: [17433] d_loss: 1.38631070, g_loss: 0.69332856\n",
      "Step: [17434] d_loss: 1.38630891, g_loss: 0.69303709\n",
      "Step: [17435] d_loss: 1.38626933, g_loss: 0.69330978\n",
      "Step: [17436] d_loss: 1.38631749, g_loss: 0.69299912\n",
      "Step: [17437] d_loss: 1.38630104, g_loss: 0.69304973\n",
      "Step: [17438] d_loss: 1.38631797, g_loss: 0.69317919\n",
      "Step: [17439] d_loss: 1.38630998, g_loss: 0.69343352\n",
      "Step: [17440] d_loss: 1.38628399, g_loss: 0.69323045\n",
      "Step: [17441] d_loss: 1.38630700, g_loss: 0.69350177\n",
      "Step: [17442] d_loss: 1.38627088, g_loss: 0.69318408\n",
      "Step: [17443] d_loss: 1.38629866, g_loss: 0.69321454\n",
      "Step: [17444] d_loss: 1.38630164, g_loss: 0.69292140\n",
      "Step: [17445] d_loss: 1.38627028, g_loss: 0.69332570\n",
      "Step: [17446] d_loss: 1.38630140, g_loss: 0.69326961\n",
      "Step: [17447] d_loss: 1.38628328, g_loss: 0.69350100\n",
      "Step: [17448] d_loss: 1.38629055, g_loss: 0.69325757\n",
      "Step: [17449] d_loss: 1.38627934, g_loss: 0.69297177\n",
      "Step: [17450] d_loss: 1.38631141, g_loss: 0.69285464\n",
      "Step: [17451] d_loss: 1.38629150, g_loss: 0.69325370\n",
      "Step: [17452] d_loss: 1.38629699, g_loss: 0.69323194\n",
      "Step: [17453] d_loss: 1.38625550, g_loss: 0.69302112\n",
      "Step: [17454] d_loss: 1.38630176, g_loss: 0.69336581\n",
      "Step: [17455] d_loss: 1.38629651, g_loss: 0.69353169\n",
      "Step: [17456] d_loss: 1.38628387, g_loss: 0.69300187\n",
      "Step: [17457] d_loss: 1.38628936, g_loss: 0.69305885\n",
      "Step: [17458] d_loss: 1.38629568, g_loss: 0.69297469\n",
      "Step: [17459] d_loss: 1.38628125, g_loss: 0.69355929\n",
      "Step: [17460] d_loss: 1.38626969, g_loss: 0.69306743\n",
      "Step: [17461] d_loss: 1.38628817, g_loss: 0.69340110\n",
      "Step: [17462] d_loss: 1.38632047, g_loss: 0.69319606\n",
      "Step: [17463] d_loss: 1.38634241, g_loss: 0.69297528\n",
      "Step: [17464] d_loss: 1.38632917, g_loss: 0.69197822\n",
      "Step: [17465] d_loss: 1.38635874, g_loss: 0.69208646\n",
      "Step: [17466] d_loss: 1.38642633, g_loss: 0.69193244\n",
      "Step: [17467] d_loss: 1.38639760, g_loss: 0.69366503\n",
      "Step: [17468] d_loss: 1.38641024, g_loss: 0.69311762\n",
      "Step: [17469] d_loss: 1.38648653, g_loss: 0.69395125\n",
      "Step: [17470] d_loss: 1.38657808, g_loss: 0.69209850\n",
      "Step: [17471] d_loss: 1.38655531, g_loss: 0.69096231\n",
      "Step: [17472] d_loss: 1.38656843, g_loss: 0.69279414\n",
      "Step: [17473] d_loss: 1.38660836, g_loss: 0.69615281\n",
      "Step: [17474] d_loss: 1.38665891, g_loss: 0.69636649\n",
      "Step: [17475] d_loss: 1.38671970, g_loss: 0.69300580\n",
      "Step: [17476] d_loss: 1.38672805, g_loss: 0.69065356\n",
      "Step: [17477] d_loss: 1.38673806, g_loss: 0.69418055\n",
      "Step: [17478] d_loss: 1.38681042, g_loss: 0.69248724\n",
      "Step: [17479] d_loss: 1.38691759, g_loss: 0.69505131\n",
      "Step: [17480] d_loss: 1.38694596, g_loss: 0.69248581\n",
      "Step: [17481] d_loss: 1.38706160, g_loss: 0.69536793\n",
      "Step: [17482] d_loss: 1.38712025, g_loss: 0.69644320\n",
      "Step: [17483] d_loss: 1.38707304, g_loss: 0.69335246\n",
      "Step: [17484] d_loss: 1.38698995, g_loss: 0.69008791\n",
      "Step: [17485] d_loss: 1.38692808, g_loss: 0.69271088\n",
      "Step: [17486] d_loss: 1.38680744, g_loss: 0.69213307\n",
      "Step: [17487] d_loss: 1.38683414, g_loss: 0.69514334\n",
      "Step: [17488] d_loss: 1.38662469, g_loss: 0.69607854\n",
      "Step: [17489] d_loss: 1.38656211, g_loss: 0.69307679\n",
      "Step: [17490] d_loss: 1.38647819, g_loss: 0.69116777\n",
      "Step: [17491] d_loss: 1.38642550, g_loss: 0.69235367\n",
      "Step: [17492] d_loss: 1.38635874, g_loss: 0.69317168\n",
      "Step: [17493] d_loss: 1.38634765, g_loss: 0.69397533\n",
      "Step: [17494] d_loss: 1.38633847, g_loss: 0.69304073\n",
      "Step: [17495] d_loss: 1.38633573, g_loss: 0.69340706\n",
      "Step: [17496] d_loss: 1.38628221, g_loss: 0.69281387\n",
      "Step: [17497] d_loss: 1.38634050, g_loss: 0.69336021\n",
      "Step: [17498] d_loss: 1.38628840, g_loss: 0.69334233\n",
      "Step: [17499] d_loss: 1.38628900, g_loss: 0.69340104\n",
      "Step: [17500] d_loss: 1.38631523, g_loss: 0.69324464\n",
      "Step: [17501] d_loss: 1.38628936, g_loss: 0.69347221\n",
      "Step: [17502] d_loss: 1.38630581, g_loss: 0.69315124\n",
      "Step: [17503] d_loss: 1.38625646, g_loss: 0.69291747\n",
      "Step: [17504] d_loss: 1.38626564, g_loss: 0.69299310\n",
      "Step: [17505] d_loss: 1.38627768, g_loss: 0.69334072\n",
      "Step: [17506] d_loss: 1.38625741, g_loss: 0.69332743\n",
      "Step: [17507] d_loss: 1.38631248, g_loss: 0.69314754\n",
      "Step: [17508] d_loss: 1.38627303, g_loss: 0.69308162\n",
      "Step: [17509] d_loss: 1.38626993, g_loss: 0.69284689\n",
      "Step: [17510] d_loss: 1.38627422, g_loss: 0.69322455\n",
      "Step: [17511] d_loss: 1.38625956, g_loss: 0.69333589\n",
      "Step: [17512] d_loss: 1.38628328, g_loss: 0.69341636\n",
      "Step: [17513] d_loss: 1.38673329, g_loss: 0.69168407\n",
      "Step: [17514] d_loss: 1.38628614, g_loss: 0.69442922\n",
      "Step: [17515] d_loss: 1.38628769, g_loss: 0.69317627\n",
      "Step: [17516] d_loss: 1.38631678, g_loss: 0.69290882\n",
      "Step: [17517] d_loss: 1.38627791, g_loss: 0.69274807\n",
      "Step: [17518] d_loss: 1.38627911, g_loss: 0.69292086\n",
      "Step: [17519] d_loss: 1.38622546, g_loss: 0.69328582\n",
      "Step: [17520] d_loss: 1.38630795, g_loss: 0.69316119\n",
      "Step: [17521] d_loss: 1.38624191, g_loss: 0.69378787\n",
      "Step: [17522] d_loss: 1.38627827, g_loss: 0.69378591\n",
      "Step: [17523] d_loss: 1.38627887, g_loss: 0.69313878\n",
      "Step: [17524] d_loss: 1.38630033, g_loss: 0.69310373\n",
      "Step: [17525] d_loss: 1.38628912, g_loss: 0.69303685\n",
      "Step: [17526] d_loss: 1.38626444, g_loss: 0.69318533\n",
      "Step: [17527] d_loss: 1.38629699, g_loss: 0.69322455\n",
      "Step: [17528] d_loss: 1.38630390, g_loss: 0.69341183\n",
      "Step: [17529] d_loss: 1.38627219, g_loss: 0.69318300\n",
      "Step: [17530] d_loss: 1.38624716, g_loss: 0.69350171\n",
      "Step: [17531] d_loss: 1.38635278, g_loss: 0.69300616\n",
      "Step: [17532] d_loss: 1.38639927, g_loss: 0.69262415\n",
      "Step: [17533] d_loss: 1.38651919, g_loss: 0.69475281\n",
      "Step: [17534] d_loss: 1.38660681, g_loss: 0.69276214\n",
      "Step: [17535] d_loss: 1.38667727, g_loss: 0.69323796\n",
      "Step: [17536] d_loss: 1.38669896, g_loss: 0.69226807\n",
      "Step: [17537] d_loss: 1.38675249, g_loss: 0.69348860\n",
      "Step: [17538] d_loss: 1.38669574, g_loss: 0.69380736\n",
      "Step: [17539] d_loss: 1.38664293, g_loss: 0.69584882\n",
      "Step: [17540] d_loss: 1.38655305, g_loss: 0.69482374\n",
      "Step: [17541] d_loss: 1.38648200, g_loss: 0.69344616\n",
      "Step: [17542] d_loss: 1.38644290, g_loss: 0.69280732\n",
      "Step: [17543] d_loss: 1.38640070, g_loss: 0.69219589\n",
      "Step: [17544] d_loss: 1.38634944, g_loss: 0.69394940\n",
      "Step: [17545] d_loss: 1.38633108, g_loss: 0.69452226\n",
      "Step: [17546] d_loss: 1.38633060, g_loss: 0.69322288\n",
      "Step: [17547] d_loss: 1.38631797, g_loss: 0.69343048\n",
      "Step: [17548] d_loss: 1.38634682, g_loss: 0.69278252\n",
      "Step: [17549] d_loss: 1.38632214, g_loss: 0.69328207\n",
      "Step: [17550] d_loss: 1.38629866, g_loss: 0.69298971\n",
      "Step: [17551] d_loss: 1.38631296, g_loss: 0.69327796\n",
      "Step: [17552] d_loss: 1.38632965, g_loss: 0.69269407\n",
      "Step: [17553] d_loss: 1.38625276, g_loss: 0.69305319\n",
      "Step: [17554] d_loss: 1.38631070, g_loss: 0.69268596\n",
      "Step: [17555] d_loss: 1.38633156, g_loss: 0.69328856\n",
      "Step: [17556] d_loss: 1.38635635, g_loss: 0.69399905\n",
      "Step: [17557] d_loss: 1.38626766, g_loss: 0.69342041\n",
      "Step: [17558] d_loss: 1.38628781, g_loss: 0.69282228\n",
      "Step: [17559] d_loss: 1.38626456, g_loss: 0.69281721\n",
      "Step: [17560] d_loss: 1.38627744, g_loss: 0.69335568\n",
      "Step: [17561] d_loss: 1.38630414, g_loss: 0.69376189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17562] d_loss: 1.38627195, g_loss: 0.69359326\n",
      "Step: [17563] d_loss: 1.38628817, g_loss: 0.69296044\n",
      "Step: [17564] d_loss: 1.38623214, g_loss: 0.69289541\n",
      "Step: [17565] d_loss: 1.38626504, g_loss: 0.69294763\n",
      "Step: [17566] d_loss: 1.38629103, g_loss: 0.69322073\n",
      "Step: [17567] d_loss: 1.38623571, g_loss: 0.69327664\n",
      "Step: [17568] d_loss: 1.38626790, g_loss: 0.69396234\n",
      "Step: [17569] d_loss: 1.38652349, g_loss: 0.69275957\n",
      "Step: [17570] d_loss: 1.38631475, g_loss: 0.69279444\n",
      "Step: [17571] d_loss: 1.38629365, g_loss: 0.69302785\n",
      "Step: [17572] d_loss: 1.38626039, g_loss: 0.69353569\n",
      "Step: [17573] d_loss: 1.38644981, g_loss: 0.69135505\n",
      "Step: [17574] d_loss: 1.38680434, g_loss: 0.69594449\n",
      "Step: [17575] d_loss: 1.38758898, g_loss: 0.69304997\n",
      "Step: [17576] d_loss: 1.38804853, g_loss: 0.68906701\n",
      "Step: [17577] d_loss: 1.38810313, g_loss: 0.69273770\n",
      "Step: [17578] d_loss: 1.38782156, g_loss: 0.69691372\n",
      "Step: [17579] d_loss: 1.38735867, g_loss: 0.70030189\n",
      "Step: [17580] d_loss: 1.38692284, g_loss: 0.69578141\n",
      "Step: [17581] d_loss: 1.38662291, g_loss: 0.69167960\n",
      "Step: [17582] d_loss: 1.38644361, g_loss: 0.69071364\n",
      "Step: [17583] d_loss: 1.38634968, g_loss: 0.69230419\n",
      "Step: [17584] d_loss: 1.38631892, g_loss: 0.69383264\n",
      "Step: [17585] d_loss: 1.38629031, g_loss: 0.69403601\n",
      "Step: [17586] d_loss: 1.38627827, g_loss: 0.69365180\n",
      "Step: [17587] d_loss: 1.38627064, g_loss: 0.69306910\n",
      "Step: [17588] d_loss: 1.38629413, g_loss: 0.69236505\n",
      "Step: [17589] d_loss: 1.38630617, g_loss: 0.69291294\n",
      "Step: [17590] d_loss: 1.38630271, g_loss: 0.69351482\n",
      "Step: [17591] d_loss: 1.38631153, g_loss: 0.69344854\n",
      "Step: [17592] d_loss: 1.38628507, g_loss: 0.69336951\n",
      "Step: [17593] d_loss: 1.38635349, g_loss: 0.69286424\n",
      "Step: [17594] d_loss: 1.38629270, g_loss: 0.69278610\n",
      "Step: [17595] d_loss: 1.38628566, g_loss: 0.69262064\n",
      "Step: [17596] d_loss: 1.38631916, g_loss: 0.69241929\n",
      "Step: [17597] d_loss: 1.38631201, g_loss: 0.69335449\n",
      "Step: [17598] d_loss: 1.38629699, g_loss: 0.69352001\n",
      "Step: [17599] d_loss: 1.38630652, g_loss: 0.69340014\n",
      "Step: [17600] d_loss: 1.38630271, g_loss: 0.69257700\n",
      "Step: [17601] d_loss: 1.38631403, g_loss: 0.69284344\n",
      "Step: [17602] d_loss: 1.38628888, g_loss: 0.69318002\n",
      "Step: [17603] d_loss: 1.38624549, g_loss: 0.69367176\n",
      "Step: [17604] d_loss: 1.38633847, g_loss: 0.69368088\n",
      "Step: [17605] d_loss: 1.38626420, g_loss: 0.69298518\n",
      "Step: [17606] d_loss: 1.38626719, g_loss: 0.69296634\n",
      "Step: [17607] d_loss: 1.38629246, g_loss: 0.69307709\n",
      "Step: [17608] d_loss: 1.38625026, g_loss: 0.69318402\n",
      "Step: [17609] d_loss: 1.38631463, g_loss: 0.69298553\n",
      "Step: [17610] d_loss: 1.38630688, g_loss: 0.69322133\n",
      "Step: [17611] d_loss: 1.38627410, g_loss: 0.69348443\n",
      "Step: [17612] d_loss: 1.38628864, g_loss: 0.69313896\n",
      "Step: [17613] d_loss: 1.38629150, g_loss: 0.69313693\n",
      "Step: [17614] d_loss: 1.38632703, g_loss: 0.69311684\n",
      "Step: [17615] d_loss: 1.38631666, g_loss: 0.69309056\n",
      "Step: [17616] d_loss: 1.38626385, g_loss: 0.69304794\n",
      "Step: [17617] d_loss: 1.38631177, g_loss: 0.69341284\n",
      "Step: [17618] d_loss: 1.38624573, g_loss: 0.69343013\n",
      "Step: [17619] d_loss: 1.38644409, g_loss: 0.69287407\n",
      "Step: [17620] d_loss: 1.38626134, g_loss: 0.69303638\n",
      "Step: [17621] d_loss: 1.38629532, g_loss: 0.69314212\n",
      "Step: [17622] d_loss: 1.38631105, g_loss: 0.69323111\n",
      "Step: [17623] d_loss: 1.38625121, g_loss: 0.69327706\n",
      "Step: [17624] d_loss: 1.38628173, g_loss: 0.69313133\n",
      "Step: [17625] d_loss: 1.38627756, g_loss: 0.69310021\n",
      "Step: [17626] d_loss: 1.38629782, g_loss: 0.69309247\n",
      "Step: [17627] d_loss: 1.38634229, g_loss: 0.69267285\n",
      "Step: [17628] d_loss: 1.38630176, g_loss: 0.69221282\n",
      "Step: [17629] d_loss: 1.38634288, g_loss: 0.69289845\n",
      "Step: [17630] d_loss: 1.38585556, g_loss: 0.69054669\n",
      "Step: [17631] d_loss: 1.38635695, g_loss: 0.69050276\n",
      "Step: [17632] d_loss: 1.38639772, g_loss: 0.69147068\n",
      "Step: [17633] d_loss: 1.38672471, g_loss: 0.69470489\n",
      "Step: [17634] d_loss: 1.38698912, g_loss: 0.69933975\n",
      "Step: [17635] d_loss: 1.38754439, g_loss: 0.69565475\n",
      "Step: [17636] d_loss: 1.38800061, g_loss: 0.68857092\n",
      "Step: [17637] d_loss: 1.38801980, g_loss: 0.69113290\n",
      "Step: [17638] d_loss: 1.38773322, g_loss: 0.69224679\n",
      "Step: [17639] d_loss: 1.38721991, g_loss: 0.69587862\n",
      "Step: [17640] d_loss: 1.38683248, g_loss: 0.69488102\n",
      "Step: [17641] d_loss: 1.38655412, g_loss: 0.69433188\n",
      "Step: [17642] d_loss: 1.38638902, g_loss: 0.69261909\n",
      "Step: [17643] d_loss: 1.38633943, g_loss: 0.69288599\n",
      "Step: [17644] d_loss: 1.38624763, g_loss: 0.69279492\n",
      "Step: [17645] d_loss: 1.38627565, g_loss: 0.69296145\n",
      "Step: [17646] d_loss: 1.38629365, g_loss: 0.69342041\n",
      "Step: [17647] d_loss: 1.38628697, g_loss: 0.69343066\n",
      "Step: [17648] d_loss: 1.38627493, g_loss: 0.69325161\n",
      "Step: [17649] d_loss: 1.38630092, g_loss: 0.69328892\n",
      "Step: [17650] d_loss: 1.38626552, g_loss: 0.69353151\n",
      "Step: [17651] d_loss: 1.38626003, g_loss: 0.69308197\n",
      "Step: [17652] d_loss: 1.38625479, g_loss: 0.69337463\n",
      "Step: [17653] d_loss: 1.38636649, g_loss: 0.69323158\n",
      "Step: [17654] d_loss: 1.38652647, g_loss: 0.69160712\n",
      "Step: [17655] d_loss: 1.38660240, g_loss: 0.69090414\n",
      "Step: [17656] d_loss: 1.38649547, g_loss: 0.69325387\n",
      "Step: [17657] d_loss: 1.38646185, g_loss: 0.69343424\n",
      "Step: [17658] d_loss: 1.38638997, g_loss: 0.69385970\n",
      "Step: [17659] d_loss: 1.38633704, g_loss: 0.69348335\n",
      "Step: [17660] d_loss: 1.38629270, g_loss: 0.69309461\n",
      "Step: [17661] d_loss: 1.38628697, g_loss: 0.69289482\n",
      "Step: [17662] d_loss: 1.38627887, g_loss: 0.69268262\n",
      "Step: [17663] d_loss: 1.38628650, g_loss: 0.69349623\n",
      "Step: [17664] d_loss: 1.38626993, g_loss: 0.69324863\n",
      "Step: [17665] d_loss: 1.38630176, g_loss: 0.69375145\n",
      "Step: [17666] d_loss: 1.38633192, g_loss: 0.69338286\n",
      "Step: [17667] d_loss: 1.38628650, g_loss: 0.69304371\n",
      "Step: [17668] d_loss: 1.38628876, g_loss: 0.69278264\n",
      "Step: [17669] d_loss: 1.38628078, g_loss: 0.69322181\n",
      "Step: [17670] d_loss: 1.38627386, g_loss: 0.69329476\n",
      "Step: [17671] d_loss: 1.38625896, g_loss: 0.69335955\n",
      "Step: [17672] d_loss: 1.38628268, g_loss: 0.69323701\n",
      "Step: [17673] d_loss: 1.38621259, g_loss: 0.69352436\n",
      "Step: [17674] d_loss: 1.38628697, g_loss: 0.69289464\n",
      "Step: [17675] d_loss: 1.38628531, g_loss: 0.69304085\n",
      "Step: [17676] d_loss: 1.38628006, g_loss: 0.69354498\n",
      "Step: [17677] d_loss: 1.38629127, g_loss: 0.69312793\n",
      "Step: [17678] d_loss: 1.38629520, g_loss: 0.69307798\n",
      "Step: [17679] d_loss: 1.38626790, g_loss: 0.69334245\n",
      "Step: [17680] d_loss: 1.38627398, g_loss: 0.69315934\n",
      "Step: [17681] d_loss: 1.38624406, g_loss: 0.69310009\n",
      "Step: [17682] d_loss: 1.38642800, g_loss: 0.69258726\n",
      "Step: [17683] d_loss: 1.38659525, g_loss: 0.69017535\n",
      "Step: [17684] d_loss: 1.38672078, g_loss: 0.69085151\n",
      "Step: [17685] d_loss: 1.38659275, g_loss: 0.69263202\n",
      "Step: [17686] d_loss: 1.38645422, g_loss: 0.69422400\n",
      "Step: [17687] d_loss: 1.38639212, g_loss: 0.69422388\n",
      "Step: [17688] d_loss: 1.38630974, g_loss: 0.69329834\n",
      "Step: [17689] d_loss: 1.38638294, g_loss: 0.69384557\n",
      "Step: [17690] d_loss: 1.38631797, g_loss: 0.69286561\n",
      "Step: [17691] d_loss: 1.38632250, g_loss: 0.69336468\n",
      "Step: [17692] d_loss: 1.38639641, g_loss: 0.69328284\n",
      "Step: [17693] d_loss: 1.38631058, g_loss: 0.69247043\n",
      "Step: [17694] d_loss: 1.38625968, g_loss: 0.69345045\n",
      "Step: [17695] d_loss: 1.38630176, g_loss: 0.69374204\n",
      "Step: [17696] d_loss: 1.38629091, g_loss: 0.69389468\n",
      "Step: [17697] d_loss: 1.38627005, g_loss: 0.69327122\n",
      "Step: [17698] d_loss: 1.38629746, g_loss: 0.69437915\n",
      "Step: [17699] d_loss: 1.38630223, g_loss: 0.69315219\n",
      "Step: [17700] d_loss: 1.38631642, g_loss: 0.69310278\n",
      "Step: [17701] d_loss: 1.38632536, g_loss: 0.69292486\n",
      "Step: [17702] d_loss: 1.38630652, g_loss: 0.69333398\n",
      "Step: [17703] d_loss: 1.38628626, g_loss: 0.69324899\n",
      "Step: [17704] d_loss: 1.38626623, g_loss: 0.69281602\n",
      "Step: [17705] d_loss: 1.38629532, g_loss: 0.69336808\n",
      "Step: [17706] d_loss: 1.38627946, g_loss: 0.69314796\n",
      "Step: [17707] d_loss: 1.38629103, g_loss: 0.69348377\n",
      "Step: [17708] d_loss: 1.38636494, g_loss: 0.69364381\n",
      "Step: [17709] d_loss: 1.38627386, g_loss: 0.69284749\n",
      "Step: [17710] d_loss: 1.38628411, g_loss: 0.69288319\n",
      "Step: [17711] d_loss: 1.38629138, g_loss: 0.69306958\n",
      "Step: [17712] d_loss: 1.38628531, g_loss: 0.69387144\n",
      "Step: [17713] d_loss: 1.38628566, g_loss: 0.69314289\n",
      "Step: [17714] d_loss: 1.38628340, g_loss: 0.69286597\n",
      "Step: [17715] d_loss: 1.38629246, g_loss: 0.69319600\n",
      "Step: [17716] d_loss: 1.38629615, g_loss: 0.69320500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17717] d_loss: 1.38627839, g_loss: 0.69304937\n",
      "Step: [17718] d_loss: 1.38627994, g_loss: 0.69325042\n",
      "Step: [17719] d_loss: 1.38629270, g_loss: 0.69315684\n",
      "Step: [17720] d_loss: 1.38629901, g_loss: 0.69358230\n",
      "Step: [17721] d_loss: 1.38626933, g_loss: 0.69314861\n",
      "Step: [17722] d_loss: 1.38629222, g_loss: 0.69302779\n",
      "Step: [17723] d_loss: 1.38630176, g_loss: 0.69307256\n",
      "Step: [17724] d_loss: 1.38629735, g_loss: 0.69315088\n",
      "Step: [17725] d_loss: 1.38628864, g_loss: 0.69328266\n",
      "Step: [17726] d_loss: 1.38626409, g_loss: 0.69294715\n",
      "Step: [17727] d_loss: 1.38629055, g_loss: 0.69335341\n",
      "Step: [17728] d_loss: 1.38626981, g_loss: 0.69376701\n",
      "Step: [17729] d_loss: 1.38630772, g_loss: 0.69321495\n",
      "Step: [17730] d_loss: 1.38627291, g_loss: 0.69281888\n",
      "Step: [17731] d_loss: 1.38629222, g_loss: 0.69330502\n",
      "Step: [17732] d_loss: 1.38626909, g_loss: 0.69345427\n",
      "Step: [17733] d_loss: 1.38628316, g_loss: 0.69285840\n",
      "Step: [17734] d_loss: 1.38628972, g_loss: 0.69345820\n",
      "Step: [17735] d_loss: 1.38604236, g_loss: 0.69386607\n",
      "Step: [17736] d_loss: 1.38622904, g_loss: 0.69287860\n",
      "Step: [17737] d_loss: 1.38629723, g_loss: 0.69349062\n",
      "Step: [17738] d_loss: 1.38628852, g_loss: 0.69329488\n",
      "Step: [17739] d_loss: 1.38626492, g_loss: 0.69362181\n",
      "Step: [17740] d_loss: 1.38621593, g_loss: 0.69320846\n",
      "Step: [17741] d_loss: 1.38637769, g_loss: 0.69425648\n",
      "Step: [17742] d_loss: 1.38682711, g_loss: 0.69599575\n",
      "Step: [17743] d_loss: 1.38662136, g_loss: 0.69274694\n",
      "Step: [17744] d_loss: 1.38629532, g_loss: 0.69170117\n",
      "Step: [17745] d_loss: 1.38632488, g_loss: 0.69190252\n",
      "Step: [17746] d_loss: 1.38631058, g_loss: 0.69316834\n",
      "Step: [17747] d_loss: 1.38633537, g_loss: 0.69334030\n",
      "Step: [17748] d_loss: 1.38531375, g_loss: 0.69460690\n",
      "Step: [17749] d_loss: 1.38632011, g_loss: 0.69439888\n",
      "Step: [17750] d_loss: 1.38623261, g_loss: 0.69389749\n",
      "Step: [17751] d_loss: 1.38634777, g_loss: 0.69337875\n",
      "Step: [17752] d_loss: 1.38664639, g_loss: 0.69500923\n",
      "Step: [17753] d_loss: 1.38704848, g_loss: 0.69242001\n",
      "Step: [17754] d_loss: 1.38850880, g_loss: 0.69628370\n",
      "Step: [17755] d_loss: 1.38966072, g_loss: 0.69684887\n",
      "Step: [17756] d_loss: 1.38948750, g_loss: 0.69099885\n",
      "Step: [17757] d_loss: 1.38846111, g_loss: 0.68646377\n",
      "Step: [17758] d_loss: 1.38748252, g_loss: 0.69163758\n",
      "Step: [17759] d_loss: 1.38682199, g_loss: 0.69676793\n",
      "Step: [17760] d_loss: 1.38645339, g_loss: 0.69829214\n",
      "Step: [17761] d_loss: 1.38632941, g_loss: 0.69559842\n",
      "Step: [17762] d_loss: 1.38626492, g_loss: 0.69146001\n",
      "Step: [17763] d_loss: 1.38628161, g_loss: 0.69080853\n",
      "Step: [17764] d_loss: 1.38627982, g_loss: 0.69204992\n",
      "Step: [17765] d_loss: 1.38630641, g_loss: 0.69380319\n",
      "Step: [17766] d_loss: 1.38624835, g_loss: 0.69412315\n",
      "Step: [17767] d_loss: 1.38629687, g_loss: 0.69349784\n",
      "Step: [17768] d_loss: 1.38628769, g_loss: 0.69291234\n",
      "Step: [17769] d_loss: 1.38629353, g_loss: 0.69286978\n",
      "Step: [17770] d_loss: 1.38856769, g_loss: 0.69027889\n",
      "Step: [17771] d_loss: 1.38632202, g_loss: 0.69407707\n",
      "Step: [17772] d_loss: 1.38634503, g_loss: 0.69375128\n",
      "Step: [17773] d_loss: 1.38631463, g_loss: 0.69492978\n",
      "Step: [17774] d_loss: 1.38632059, g_loss: 0.69352758\n",
      "Step: [17775] d_loss: 1.38631809, g_loss: 0.69285303\n",
      "Step: [17776] d_loss: 1.38628292, g_loss: 0.69271863\n",
      "Step: [17777] d_loss: 1.38623333, g_loss: 0.69299459\n",
      "Step: [17778] d_loss: 1.38815546, g_loss: 0.69162226\n",
      "Step: [17779] d_loss: 1.38624597, g_loss: 0.69384074\n",
      "Step: [17780] d_loss: 1.38623428, g_loss: 0.69325578\n",
      "Step: [17781] d_loss: 1.39108026, g_loss: 0.69164801\n",
      "Step: [17782] d_loss: 1.38642955, g_loss: 0.69282818\n",
      "Step: [17783] d_loss: 1.38628972, g_loss: 0.69305867\n",
      "Step: [17784] d_loss: 1.38628197, g_loss: 0.69321507\n",
      "Step: [17785] d_loss: 1.38637054, g_loss: 0.69263363\n",
      "Step: [17786] d_loss: 1.38634455, g_loss: 0.69363576\n",
      "Step: [17787] d_loss: 1.38646817, g_loss: 0.69338727\n",
      "Step: [17788] d_loss: 1.38697517, g_loss: 0.69488525\n",
      "Step: [17789] d_loss: 1.38821769, g_loss: 0.69880569\n",
      "Step: [17790] d_loss: 1.38887370, g_loss: 0.69201326\n",
      "Step: [17791] d_loss: 1.38822484, g_loss: 0.69141388\n",
      "Step: [17792] d_loss: 1.38723719, g_loss: 0.69117868\n",
      "Step: [17793] d_loss: 1.38654530, g_loss: 0.69247401\n",
      "Step: [17794] d_loss: 1.38631344, g_loss: 0.69352937\n",
      "Step: [17795] d_loss: 1.38625216, g_loss: 0.69360220\n",
      "Step: [17796] d_loss: 1.38631654, g_loss: 0.69361210\n",
      "Step: [17797] d_loss: 1.38632464, g_loss: 0.69292450\n",
      "Step: [17798] d_loss: 1.38629878, g_loss: 0.69246483\n",
      "Step: [17799] d_loss: 1.38627088, g_loss: 0.69343859\n",
      "Step: [17800] d_loss: 1.38628531, g_loss: 0.69347155\n",
      "Step: [17801] d_loss: 1.38630629, g_loss: 0.69363582\n",
      "Step: [17802] d_loss: 1.38631368, g_loss: 0.69310445\n",
      "Step: [17803] d_loss: 1.38637674, g_loss: 0.69422913\n",
      "Step: [17804] d_loss: 1.38636494, g_loss: 0.69385141\n",
      "Step: [17805] d_loss: 1.38640618, g_loss: 0.69371033\n",
      "Step: [17806] d_loss: 1.38630760, g_loss: 0.69294095\n",
      "Step: [17807] d_loss: 1.38628197, g_loss: 0.69243461\n",
      "Step: [17808] d_loss: 1.38626623, g_loss: 0.69298542\n",
      "Step: [17809] d_loss: 1.38633323, g_loss: 0.69330525\n",
      "Step: [17810] d_loss: 1.38629699, g_loss: 0.69366205\n",
      "Step: [17811] d_loss: 1.38629460, g_loss: 0.69317812\n",
      "Step: [17812] d_loss: 1.38629174, g_loss: 0.69353962\n",
      "Step: [17813] d_loss: 1.38629436, g_loss: 0.69311190\n",
      "Step: [17814] d_loss: 1.38635659, g_loss: 0.69381893\n",
      "Step: [17815] d_loss: 1.38630247, g_loss: 0.69301212\n",
      "Step: [17816] d_loss: 1.38630021, g_loss: 0.69297552\n",
      "Step: [17817] d_loss: 1.38628638, g_loss: 0.69313496\n",
      "Step: [17818] d_loss: 1.38628459, g_loss: 0.69316882\n",
      "Step: [17819] d_loss: 1.38629937, g_loss: 0.69334316\n",
      "Step: [17820] d_loss: 1.38628864, g_loss: 0.69332892\n",
      "Step: [17821] d_loss: 1.38627529, g_loss: 0.69304633\n",
      "Step: [17822] d_loss: 1.38628602, g_loss: 0.69326317\n",
      "Step: [17823] d_loss: 1.38628435, g_loss: 0.69322288\n",
      "Step: [17824] d_loss: 1.38628542, g_loss: 0.69297832\n",
      "Step: [17825] d_loss: 1.38627779, g_loss: 0.69329953\n",
      "Step: [17826] d_loss: 1.38629556, g_loss: 0.69310081\n",
      "Step: [17827] d_loss: 1.38628614, g_loss: 0.69349420\n",
      "Step: [17828] d_loss: 1.38626862, g_loss: 0.69321430\n",
      "Step: [17829] d_loss: 1.38627601, g_loss: 0.69322574\n",
      "Step: [17830] d_loss: 1.38628399, g_loss: 0.69329405\n",
      "Step: [17831] d_loss: 1.38616455, g_loss: 0.69410193\n",
      "Step: [17832] d_loss: 1.38626993, g_loss: 0.69337124\n",
      "Step: [17833] d_loss: 1.38629043, g_loss: 0.69294655\n",
      "Step: [17834] d_loss: 1.38627863, g_loss: 0.69305027\n",
      "Step: [17835] d_loss: 1.38626170, g_loss: 0.69320726\n",
      "Step: [17836] d_loss: 1.38626516, g_loss: 0.69318169\n",
      "Step: [17837] d_loss: 1.38626468, g_loss: 0.69309467\n",
      "Step: [17838] d_loss: 1.38628983, g_loss: 0.69324064\n",
      "Step: [17839] d_loss: 1.38628578, g_loss: 0.69322741\n",
      "Step: [17840] d_loss: 1.38630128, g_loss: 0.69299102\n",
      "Step: [17841] d_loss: 1.38626623, g_loss: 0.69352239\n",
      "Step: [17842] d_loss: 1.38626909, g_loss: 0.69354582\n",
      "Step: [17843] d_loss: 1.38626599, g_loss: 0.69331896\n",
      "Step: [17844] d_loss: 1.38625097, g_loss: 0.69287449\n",
      "Step: [17845] d_loss: 1.38628840, g_loss: 0.69308859\n",
      "Step: [17846] d_loss: 1.38626337, g_loss: 0.69341069\n",
      "Step: [17847] d_loss: 1.38629901, g_loss: 0.69327343\n",
      "Step: [17848] d_loss: 1.38627839, g_loss: 0.69312847\n",
      "Step: [17849] d_loss: 1.38628101, g_loss: 0.69303465\n",
      "Step: [17850] d_loss: 1.38637304, g_loss: 0.69342011\n",
      "Step: [17851] d_loss: 1.38628697, g_loss: 0.69302499\n",
      "Step: [17852] d_loss: 1.38627934, g_loss: 0.69304907\n",
      "Step: [17853] d_loss: 1.38627374, g_loss: 0.69326591\n",
      "Step: [17854] d_loss: 1.38627625, g_loss: 0.69335651\n",
      "Step: [17855] d_loss: 1.38626730, g_loss: 0.69333065\n",
      "Step: [17856] d_loss: 1.38628125, g_loss: 0.69311535\n",
      "Step: [17857] d_loss: 1.38626337, g_loss: 0.69319338\n",
      "Step: [17858] d_loss: 1.38628423, g_loss: 0.69319832\n",
      "Step: [17859] d_loss: 1.38626456, g_loss: 0.69317281\n",
      "Step: [17860] d_loss: 1.38628840, g_loss: 0.69305146\n",
      "Step: [17861] d_loss: 1.38630962, g_loss: 0.69335169\n",
      "Step: [17862] d_loss: 1.38627076, g_loss: 0.69327390\n",
      "Step: [17863] d_loss: 1.38628507, g_loss: 0.69346279\n",
      "Step: [17864] d_loss: 1.38618612, g_loss: 0.69313920\n",
      "Step: [17865] d_loss: 1.38626456, g_loss: 0.69299251\n",
      "Step: [17866] d_loss: 1.38631213, g_loss: 0.69313073\n",
      "Step: [17867] d_loss: 1.38624990, g_loss: 0.69338369\n",
      "Step: [17868] d_loss: 1.38618159, g_loss: 0.69327736\n",
      "Step: [17869] d_loss: 1.38618159, g_loss: 0.69323742\n",
      "Step: [17870] d_loss: 1.38627005, g_loss: 0.69302636\n",
      "Step: [17871] d_loss: 1.38622665, g_loss: 0.69309634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [17872] d_loss: 1.38627851, g_loss: 0.69311607\n",
      "Step: [17873] d_loss: 1.38627326, g_loss: 0.69325614\n",
      "Step: [17874] d_loss: 1.38628793, g_loss: 0.69324750\n",
      "Step: [17875] d_loss: 1.38626575, g_loss: 0.69316816\n",
      "Step: [17876] d_loss: 1.38624513, g_loss: 0.69315249\n",
      "Step: [17877] d_loss: 1.38625491, g_loss: 0.69311708\n",
      "Step: [17878] d_loss: 1.38620365, g_loss: 0.69303006\n",
      "Step: [17879] d_loss: 1.38632858, g_loss: 0.69320297\n",
      "Step: [17880] d_loss: 1.38620937, g_loss: 0.69259632\n",
      "Step: [17881] d_loss: 1.38625538, g_loss: 0.69403601\n",
      "Step: [17882] d_loss: 1.38629603, g_loss: 0.69388843\n",
      "Step: [17883] d_loss: 1.38626838, g_loss: 0.69302636\n",
      "Step: [17884] d_loss: 1.38624883, g_loss: 0.69253790\n",
      "Step: [17885] d_loss: 1.38627505, g_loss: 0.69297898\n",
      "Step: [17886] d_loss: 1.38620722, g_loss: 0.69291002\n",
      "Step: [17887] d_loss: 1.38627517, g_loss: 0.69305551\n",
      "Step: [17888] d_loss: 1.38626468, g_loss: 0.69340479\n",
      "Step: [17889] d_loss: 1.38625455, g_loss: 0.69336432\n",
      "Step: [17890] d_loss: 1.38616538, g_loss: 0.69345099\n",
      "Step: [17891] d_loss: 1.38627100, g_loss: 0.69322443\n",
      "Step: [17892] d_loss: 1.38621449, g_loss: 0.69304073\n",
      "Step: [17893] d_loss: 1.38626301, g_loss: 0.69325221\n",
      "Step: [17894] d_loss: 1.38626647, g_loss: 0.69304180\n",
      "Step: [17895] d_loss: 1.38623095, g_loss: 0.69316339\n",
      "Step: [17896] d_loss: 1.38627052, g_loss: 0.69365817\n",
      "Step: [17897] d_loss: 1.38624167, g_loss: 0.69501328\n",
      "Step: [17898] d_loss: 1.38618696, g_loss: 0.69234139\n",
      "Step: [17899] d_loss: 1.38625789, g_loss: 0.69238603\n",
      "Step: [17900] d_loss: 1.38622355, g_loss: 0.69264638\n",
      "Step: [17901] d_loss: 1.38620603, g_loss: 0.69301045\n",
      "Step: [17902] d_loss: 1.38621497, g_loss: 0.69355166\n",
      "Step: [17903] d_loss: 1.38632798, g_loss: 0.69307721\n",
      "Step: [17904] d_loss: 1.38626111, g_loss: 0.69318998\n",
      "Step: [17905] d_loss: 1.38632441, g_loss: 0.69316679\n",
      "Step: [17906] d_loss: 1.38622737, g_loss: 0.69304597\n",
      "Step: [17907] d_loss: 1.38622391, g_loss: 0.69335258\n",
      "Step: [17908] d_loss: 1.38622785, g_loss: 0.69363838\n",
      "Step: [17909] d_loss: 1.38608003, g_loss: 0.69356930\n",
      "Step: [17910] d_loss: 1.38652432, g_loss: 0.69336903\n",
      "Step: [17911] d_loss: 1.38616812, g_loss: 0.69301945\n",
      "Step: [17912] d_loss: 1.38621747, g_loss: 0.69294655\n",
      "Step: [17913] d_loss: 1.38629472, g_loss: 0.69325817\n",
      "Step: [17914] d_loss: 1.38622820, g_loss: 0.69346321\n",
      "Step: [17915] d_loss: 1.38619030, g_loss: 0.69319487\n",
      "Step: [17916] d_loss: 1.38633800, g_loss: 0.69322169\n",
      "Step: [17917] d_loss: 1.38626075, g_loss: 0.69317734\n",
      "Step: [17918] d_loss: 1.38628602, g_loss: 0.69319862\n",
      "Step: [17919] d_loss: 1.38616276, g_loss: 0.69314373\n",
      "Step: [17920] d_loss: 1.38619256, g_loss: 0.69438457\n",
      "Step: [17921] d_loss: 1.38666224, g_loss: 0.69229454\n",
      "Step: [17922] d_loss: 1.38668096, g_loss: 0.69194400\n",
      "Step: [17923] d_loss: 1.38670683, g_loss: 0.69255692\n",
      "Step: [17924] d_loss: 1.38670075, g_loss: 0.69470394\n",
      "Step: [17925] d_loss: 1.38684082, g_loss: 0.69408059\n",
      "Step: [17926] d_loss: 1.38684833, g_loss: 0.69579017\n",
      "Step: [17927] d_loss: 1.38688016, g_loss: 0.69581532\n",
      "Step: [17928] d_loss: 1.38676500, g_loss: 0.69535428\n",
      "Step: [17929] d_loss: 1.38660932, g_loss: 0.69212753\n",
      "Step: [17930] d_loss: 1.38649607, g_loss: 0.69146150\n",
      "Step: [17931] d_loss: 1.38632202, g_loss: 0.69135755\n",
      "Step: [17932] d_loss: 1.38636756, g_loss: 0.69246447\n",
      "Step: [17933] d_loss: 1.38632321, g_loss: 0.69362533\n",
      "Step: [17934] d_loss: 1.38622475, g_loss: 0.69360936\n",
      "Step: [17935] d_loss: 1.38636005, g_loss: 0.69369411\n",
      "Step: [17936] d_loss: 1.38668180, g_loss: 0.69271588\n",
      "Step: [17937] d_loss: 1.38703036, g_loss: 0.69050974\n",
      "Step: [17938] d_loss: 1.38707757, g_loss: 0.68999553\n",
      "Step: [17939] d_loss: 1.38685322, g_loss: 0.69223428\n",
      "Step: [17940] d_loss: 1.38666105, g_loss: 0.69565856\n",
      "Step: [17941] d_loss: 1.38648021, g_loss: 0.69444340\n",
      "Step: [17942] d_loss: 1.38640118, g_loss: 0.69234610\n",
      "Step: [17943] d_loss: 1.38635659, g_loss: 0.69170541\n",
      "Step: [17944] d_loss: 1.38631821, g_loss: 0.69291615\n",
      "Step: [17945] d_loss: 1.38628578, g_loss: 0.69267190\n",
      "Step: [17946] d_loss: 1.38629913, g_loss: 0.69293523\n",
      "Step: [17947] d_loss: 1.38629365, g_loss: 0.69474763\n",
      "Step: [17948] d_loss: 1.38630402, g_loss: 0.69363081\n",
      "Step: [17949] d_loss: 1.38629544, g_loss: 0.69261873\n",
      "Step: [17950] d_loss: 1.38626480, g_loss: 0.69290435\n",
      "Step: [17951] d_loss: 1.38625717, g_loss: 0.69184911\n",
      "Step: [17952] d_loss: 1.38644719, g_loss: 0.69320077\n",
      "Step: [17953] d_loss: 1.38630724, g_loss: 0.69468689\n",
      "Step: [17954] d_loss: 1.38646495, g_loss: 0.69187760\n",
      "Step: [17955] d_loss: 1.38657963, g_loss: 0.69169152\n",
      "Step: [17956] d_loss: 1.38661265, g_loss: 0.69635183\n",
      "Step: [17957] d_loss: 1.38662851, g_loss: 0.69525772\n",
      "Step: [17958] d_loss: 1.38658559, g_loss: 0.68973625\n",
      "Step: [17959] d_loss: 1.38649154, g_loss: 0.69175726\n",
      "Step: [17960] d_loss: 1.38634062, g_loss: 0.69205344\n",
      "Step: [17961] d_loss: 1.38632655, g_loss: 0.69333017\n",
      "Step: [17962] d_loss: 1.38625860, g_loss: 0.69360125\n",
      "Step: [17963] d_loss: 1.38631678, g_loss: 0.69313776\n",
      "Step: [17964] d_loss: 1.38620710, g_loss: 0.69345820\n",
      "Step: [17965] d_loss: 1.38627064, g_loss: 0.69289690\n",
      "Step: [17966] d_loss: 1.38631678, g_loss: 0.69328129\n",
      "Step: [17967] d_loss: 1.38627470, g_loss: 0.69389254\n",
      "Step: [17968] d_loss: 1.38629961, g_loss: 0.69274586\n",
      "Step: [17969] d_loss: 1.38630450, g_loss: 0.69306421\n",
      "Step: [17970] d_loss: 1.38637030, g_loss: 0.69325924\n",
      "Step: [17971] d_loss: 1.38625741, g_loss: 0.69263232\n",
      "Step: [17972] d_loss: 1.38627601, g_loss: 0.69322664\n",
      "Step: [17973] d_loss: 1.38626826, g_loss: 0.69346309\n",
      "Step: [17974] d_loss: 1.38635194, g_loss: 0.69355142\n",
      "Step: [17975] d_loss: 1.38627505, g_loss: 0.69303161\n",
      "Step: [17976] d_loss: 1.38625908, g_loss: 0.69305360\n",
      "Step: [17977] d_loss: 1.38628125, g_loss: 0.69308907\n",
      "Step: [17978] d_loss: 1.38627481, g_loss: 0.69315517\n",
      "Step: [17979] d_loss: 1.38627243, g_loss: 0.69365013\n",
      "Step: [17980] d_loss: 1.38626707, g_loss: 0.69319671\n",
      "Step: [17981] d_loss: 1.38629937, g_loss: 0.69323164\n",
      "Step: [17982] d_loss: 1.38624668, g_loss: 0.69316107\n",
      "Step: [17983] d_loss: 1.38624001, g_loss: 0.69368303\n",
      "Step: [17984] d_loss: 1.38629746, g_loss: 0.69391924\n",
      "Step: [17985] d_loss: 1.38625264, g_loss: 0.69288886\n",
      "Step: [17986] d_loss: 1.38624167, g_loss: 0.69276518\n",
      "Step: [17987] d_loss: 1.38630223, g_loss: 0.69285828\n",
      "Step: [17988] d_loss: 1.38619423, g_loss: 0.69332445\n",
      "Step: [17989] d_loss: 1.38622928, g_loss: 0.69398451\n",
      "Step: [17990] d_loss: 1.38622808, g_loss: 0.69375283\n",
      "Step: [17991] d_loss: 1.38627779, g_loss: 0.69323379\n",
      "Step: [17992] d_loss: 1.38633323, g_loss: 0.69291604\n",
      "Step: [17993] d_loss: 1.38634837, g_loss: 0.69239378\n",
      "Step: [17994] d_loss: 1.38629067, g_loss: 0.69371533\n",
      "Step: [17995] d_loss: 1.38603711, g_loss: 0.69233668\n",
      "Step: [17996] d_loss: 1.38624644, g_loss: 0.69226813\n",
      "Step: [17997] d_loss: 1.38623142, g_loss: 0.69586581\n",
      "Step: [17998] d_loss: 1.38623118, g_loss: 0.69350922\n",
      "Step: [17999] d_loss: 1.38623285, g_loss: 0.69320679\n",
      "Step: [18000] d_loss: 1.38626730, g_loss: 0.69300044\n",
      "Step: [18001] d_loss: 1.38631868, g_loss: 0.69245601\n",
      "Step: [18002] d_loss: 1.38645124, g_loss: 0.69355428\n",
      "Step: [18003] d_loss: 1.38712680, g_loss: 0.69676059\n",
      "Step: [18004] d_loss: 1.38772786, g_loss: 0.69237012\n",
      "Step: [18005] d_loss: 1.38774216, g_loss: 0.69145238\n",
      "Step: [18006] d_loss: 1.38737059, g_loss: 0.69241858\n",
      "Step: [18007] d_loss: 1.38706172, g_loss: 0.69612831\n",
      "Step: [18008] d_loss: 1.38675284, g_loss: 0.69488692\n",
      "Step: [18009] d_loss: 1.38652062, g_loss: 0.69232023\n",
      "Step: [18010] d_loss: 1.38634610, g_loss: 0.69170833\n",
      "Step: [18011] d_loss: 1.38630009, g_loss: 0.69195378\n",
      "Step: [18012] d_loss: 1.38625789, g_loss: 0.69519788\n",
      "Step: [18013] d_loss: 1.38648438, g_loss: 0.69283164\n",
      "Step: [18014] d_loss: 1.38757122, g_loss: 0.69310111\n",
      "Step: [18015] d_loss: 1.38818383, g_loss: 0.69077909\n",
      "Step: [18016] d_loss: 1.38792443, g_loss: 0.69486374\n",
      "Step: [18017] d_loss: 1.38730431, g_loss: 0.69592285\n",
      "Step: [18018] d_loss: 1.38678896, g_loss: 0.69308794\n",
      "Step: [18019] d_loss: 1.38642955, g_loss: 0.69184983\n",
      "Step: [18020] d_loss: 1.38629520, g_loss: 0.69160527\n",
      "Step: [18021] d_loss: 1.38625932, g_loss: 0.69313544\n",
      "Step: [18022] d_loss: 1.38634002, g_loss: 0.69257379\n",
      "Step: [18023] d_loss: 1.38635302, g_loss: 0.69466907\n",
      "Step: [18024] d_loss: 1.38635731, g_loss: 0.69380665\n",
      "Step: [18025] d_loss: 1.38629162, g_loss: 0.69457769\n",
      "Step: [18026] d_loss: 1.38633990, g_loss: 0.69289064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18027] d_loss: 1.38629222, g_loss: 0.69391394\n",
      "Step: [18028] d_loss: 1.38631320, g_loss: 0.69327676\n",
      "Step: [18029] d_loss: 1.38630629, g_loss: 0.69305670\n",
      "Step: [18030] d_loss: 1.38629365, g_loss: 0.69189847\n",
      "Step: [18031] d_loss: 1.38636875, g_loss: 0.69333357\n",
      "Step: [18032] d_loss: 1.38626552, g_loss: 0.69440663\n",
      "Step: [18033] d_loss: 1.38630438, g_loss: 0.69477582\n",
      "Step: [18034] d_loss: 1.38636410, g_loss: 0.69299847\n",
      "Step: [18035] d_loss: 1.38631392, g_loss: 0.69316673\n",
      "Step: [18036] d_loss: 1.38631392, g_loss: 0.69286186\n",
      "Step: [18037] d_loss: 1.38629150, g_loss: 0.69365799\n",
      "Step: [18038] d_loss: 1.38634634, g_loss: 0.69349843\n",
      "Step: [18039] d_loss: 1.38626528, g_loss: 0.69244188\n",
      "Step: [18040] d_loss: 1.38631392, g_loss: 0.69298208\n",
      "Step: [18041] d_loss: 1.38630354, g_loss: 0.69403505\n",
      "Step: [18042] d_loss: 1.38628256, g_loss: 0.69323814\n",
      "Step: [18043] d_loss: 1.38628697, g_loss: 0.69276291\n",
      "Step: [18044] d_loss: 1.38628840, g_loss: 0.69312626\n",
      "Step: [18045] d_loss: 1.38629889, g_loss: 0.69342303\n",
      "Step: [18046] d_loss: 1.38626802, g_loss: 0.69317162\n",
      "Step: [18047] d_loss: 1.38624418, g_loss: 0.69315904\n",
      "Step: [18048] d_loss: 1.38626885, g_loss: 0.69299436\n",
      "Step: [18049] d_loss: 1.38629436, g_loss: 0.69322252\n",
      "Step: [18050] d_loss: 1.38628471, g_loss: 0.69376719\n",
      "Step: [18051] d_loss: 1.38628006, g_loss: 0.69318557\n",
      "Step: [18052] d_loss: 1.38623571, g_loss: 0.69400096\n",
      "Step: [18053] d_loss: 1.38631964, g_loss: 0.69326627\n",
      "Step: [18054] d_loss: 1.38636327, g_loss: 0.69552040\n",
      "Step: [18055] d_loss: 1.38633573, g_loss: 0.69375664\n",
      "Step: [18056] d_loss: 1.38630104, g_loss: 0.69294631\n",
      "Step: [18057] d_loss: 1.38628840, g_loss: 0.69242287\n",
      "Step: [18058] d_loss: 1.38629699, g_loss: 0.69247711\n",
      "Step: [18059] d_loss: 1.38628960, g_loss: 0.69301456\n",
      "Step: [18060] d_loss: 1.38626897, g_loss: 0.69310939\n",
      "Step: [18061] d_loss: 1.38626635, g_loss: 0.69371158\n",
      "Step: [18062] d_loss: 1.38626778, g_loss: 0.69467187\n",
      "Step: [18063] d_loss: 1.38629210, g_loss: 0.69306052\n",
      "Step: [18064] d_loss: 1.38632894, g_loss: 0.69344735\n",
      "Step: [18065] d_loss: 1.38629949, g_loss: 0.69302905\n",
      "Step: [18066] d_loss: 1.38634074, g_loss: 0.69413805\n",
      "Step: [18067] d_loss: 1.38630819, g_loss: 0.69320256\n",
      "Step: [18068] d_loss: 1.38629484, g_loss: 0.69350326\n",
      "Step: [18069] d_loss: 1.38626194, g_loss: 0.69305420\n",
      "Step: [18070] d_loss: 1.38627744, g_loss: 0.69243330\n",
      "Step: [18071] d_loss: 1.38626695, g_loss: 0.69326854\n",
      "Step: [18072] d_loss: 1.38625801, g_loss: 0.69325054\n",
      "Step: [18073] d_loss: 1.38625622, g_loss: 0.69342840\n",
      "Step: [18074] d_loss: 1.38630772, g_loss: 0.69344401\n",
      "Step: [18075] d_loss: 1.38621306, g_loss: 0.69268692\n",
      "Step: [18076] d_loss: 1.38639045, g_loss: 0.69338918\n",
      "Step: [18077] d_loss: 1.38624263, g_loss: 0.69306922\n",
      "Step: [18078] d_loss: 1.38626242, g_loss: 0.69296056\n",
      "Step: [18079] d_loss: 1.38625610, g_loss: 0.69367325\n",
      "Step: [18080] d_loss: 1.38628268, g_loss: 0.69431567\n",
      "Step: [18081] d_loss: 1.38630080, g_loss: 0.69323182\n",
      "Step: [18082] d_loss: 1.38628840, g_loss: 0.69264472\n",
      "Step: [18083] d_loss: 1.38624835, g_loss: 0.69300634\n",
      "Step: [18084] d_loss: 1.38625443, g_loss: 0.69351417\n",
      "Step: [18085] d_loss: 1.38626075, g_loss: 0.69357532\n",
      "Step: [18086] d_loss: 1.38627613, g_loss: 0.69408321\n",
      "Step: [18087] d_loss: 1.38624299, g_loss: 0.69292015\n",
      "Step: [18088] d_loss: 1.38629556, g_loss: 0.69262242\n",
      "Step: [18089] d_loss: 1.38627589, g_loss: 0.69363946\n",
      "Step: [18090] d_loss: 1.38628507, g_loss: 0.69324875\n",
      "Step: [18091] d_loss: 1.38626647, g_loss: 0.69254208\n",
      "Step: [18092] d_loss: 1.38626814, g_loss: 0.69351274\n",
      "Step: [18093] d_loss: 1.38625050, g_loss: 0.69310462\n",
      "Step: [18094] d_loss: 1.38628936, g_loss: 0.69325936\n",
      "Step: [18095] d_loss: 1.38632607, g_loss: 0.69335365\n",
      "Step: [18096] d_loss: 1.38629746, g_loss: 0.69333816\n",
      "Step: [18097] d_loss: 1.38632858, g_loss: 0.69291282\n",
      "Step: [18098] d_loss: 1.38634086, g_loss: 0.69316530\n",
      "Step: [18099] d_loss: 1.38629937, g_loss: 0.69340253\n",
      "Step: [18100] d_loss: 1.38631141, g_loss: 0.69347197\n",
      "Step: [18101] d_loss: 1.38627374, g_loss: 0.69314170\n",
      "Step: [18102] d_loss: 1.38628101, g_loss: 0.69331193\n",
      "Step: [18103] d_loss: 1.38624477, g_loss: 0.69319230\n",
      "Step: [18104] d_loss: 1.38626730, g_loss: 0.69390678\n",
      "Step: [18105] d_loss: 1.38621557, g_loss: 0.69314289\n",
      "Step: [18106] d_loss: 1.38637066, g_loss: 0.69338512\n",
      "Step: [18107] d_loss: 1.38640261, g_loss: 0.69304562\n",
      "Step: [18108] d_loss: 1.38657713, g_loss: 0.69463354\n",
      "Step: [18109] d_loss: 1.38661373, g_loss: 0.69594860\n",
      "Step: [18110] d_loss: 1.38664722, g_loss: 0.69671935\n",
      "Step: [18111] d_loss: 1.38665831, g_loss: 0.69599205\n",
      "Step: [18112] d_loss: 1.38662100, g_loss: 0.69402194\n",
      "Step: [18113] d_loss: 1.38647437, g_loss: 0.69131017\n",
      "Step: [18114] d_loss: 1.38644123, g_loss: 0.69193393\n",
      "Step: [18115] d_loss: 1.38635433, g_loss: 0.69453406\n",
      "Step: [18116] d_loss: 1.38631272, g_loss: 0.69475615\n",
      "Step: [18117] d_loss: 1.38625050, g_loss: 0.69355601\n",
      "Step: [18118] d_loss: 1.38631868, g_loss: 0.69320852\n",
      "Step: [18119] d_loss: 1.38624787, g_loss: 0.69299847\n",
      "Step: [18120] d_loss: 1.38629699, g_loss: 0.69274008\n",
      "Step: [18121] d_loss: 1.38624716, g_loss: 0.69296825\n",
      "Step: [18122] d_loss: 1.38625431, g_loss: 0.69322151\n",
      "Step: [18123] d_loss: 1.38631892, g_loss: 0.69311357\n",
      "Step: [18124] d_loss: 1.38629878, g_loss: 0.69311583\n",
      "Step: [18125] d_loss: 1.38625073, g_loss: 0.69315290\n",
      "Step: [18126] d_loss: 1.38627255, g_loss: 0.69295156\n",
      "Step: [18127] d_loss: 1.38627505, g_loss: 0.69285393\n",
      "Step: [18128] d_loss: 1.38651252, g_loss: 0.69424361\n",
      "Step: [18129] d_loss: 1.38666153, g_loss: 0.69803685\n",
      "Step: [18130] d_loss: 1.38805521, g_loss: 0.69906914\n",
      "Step: [18131] d_loss: 1.38796711, g_loss: 0.69325221\n",
      "Step: [18132] d_loss: 1.38741612, g_loss: 0.68738008\n",
      "Step: [18133] d_loss: 1.38702619, g_loss: 0.69069374\n",
      "Step: [18134] d_loss: 1.38797522, g_loss: 0.69362235\n",
      "Step: [18135] d_loss: 1.39007425, g_loss: 0.70003200\n",
      "Step: [18136] d_loss: 1.39027286, g_loss: 0.69508243\n",
      "Step: [18137] d_loss: 1.38849962, g_loss: 0.68878198\n",
      "Step: [18138] d_loss: 1.38696265, g_loss: 0.69463325\n",
      "Step: [18139] d_loss: 1.38636279, g_loss: 0.69633031\n",
      "Step: [18140] d_loss: 1.38629186, g_loss: 0.69466460\n",
      "Step: [18141] d_loss: 1.38636625, g_loss: 0.69223857\n",
      "Step: [18142] d_loss: 1.38636076, g_loss: 0.69152969\n",
      "Step: [18143] d_loss: 1.38632083, g_loss: 0.69327110\n",
      "Step: [18144] d_loss: 1.38636351, g_loss: 0.69378304\n",
      "Step: [18145] d_loss: 1.38625956, g_loss: 0.69424158\n",
      "Step: [18146] d_loss: 1.38634086, g_loss: 0.69303542\n",
      "Step: [18147] d_loss: 1.38626409, g_loss: 0.69219887\n",
      "Step: [18148] d_loss: 1.38624334, g_loss: 0.69260299\n",
      "Step: [18149] d_loss: 1.38626695, g_loss: 0.69354439\n",
      "Step: [18150] d_loss: 1.38624692, g_loss: 0.69352221\n",
      "Step: [18151] d_loss: 1.38624382, g_loss: 0.69357193\n",
      "Step: [18152] d_loss: 1.38628602, g_loss: 0.69301081\n",
      "Step: [18153] d_loss: 1.38639319, g_loss: 0.69331521\n",
      "Step: [18154] d_loss: 1.38662910, g_loss: 0.69372404\n",
      "Step: [18155] d_loss: 1.38662648, g_loss: 0.69309926\n",
      "Step: [18156] d_loss: 1.38643980, g_loss: 0.69353080\n",
      "Step: [18157] d_loss: 1.38626099, g_loss: 0.69397330\n",
      "Step: [18158] d_loss: 1.38641846, g_loss: 0.69411790\n",
      "Step: [18159] d_loss: 1.38657093, g_loss: 0.69313264\n",
      "Step: [18160] d_loss: 1.38687754, g_loss: 0.69372118\n",
      "Step: [18161] d_loss: 1.38639915, g_loss: 0.69419605\n",
      "Step: [18162] d_loss: 1.38630068, g_loss: 0.69339693\n",
      "Step: [18163] d_loss: 1.38625288, g_loss: 0.69239414\n",
      "Step: [18164] d_loss: 1.38631403, g_loss: 0.69199914\n",
      "Step: [18165] d_loss: 1.38630617, g_loss: 0.69294393\n",
      "Step: [18166] d_loss: 1.38628864, g_loss: 0.69348657\n",
      "Step: [18167] d_loss: 1.38627362, g_loss: 0.69359076\n",
      "Step: [18168] d_loss: 1.38626814, g_loss: 0.69357336\n",
      "Step: [18169] d_loss: 1.38632774, g_loss: 0.69299048\n",
      "Step: [18170] d_loss: 1.38627362, g_loss: 0.69343400\n",
      "Step: [18171] d_loss: 1.38631177, g_loss: 0.69291937\n",
      "Step: [18172] d_loss: 1.38631678, g_loss: 0.69322073\n",
      "Step: [18173] d_loss: 1.38628709, g_loss: 0.69311142\n",
      "Step: [18174] d_loss: 1.38629878, g_loss: 0.69274962\n",
      "Step: [18175] d_loss: 1.38628626, g_loss: 0.69310552\n",
      "Step: [18176] d_loss: 1.38629293, g_loss: 0.69252276\n",
      "Step: [18177] d_loss: 1.38627470, g_loss: 0.69323421\n",
      "Step: [18178] d_loss: 1.38627541, g_loss: 0.69248134\n",
      "Step: [18179] d_loss: 1.38629770, g_loss: 0.69296002\n",
      "Step: [18180] d_loss: 1.38629508, g_loss: 0.69398904\n",
      "Step: [18181] d_loss: 1.38627839, g_loss: 0.69295466\n",
      "Step: [18182] d_loss: 1.38630557, g_loss: 0.69278389\n",
      "Step: [18183] d_loss: 1.38627338, g_loss: 0.69317710\n",
      "Step: [18184] d_loss: 1.38628435, g_loss: 0.69268000\n",
      "Step: [18185] d_loss: 1.38628697, g_loss: 0.69308364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18186] d_loss: 1.38632679, g_loss: 0.69347811\n",
      "Step: [18187] d_loss: 1.38625252, g_loss: 0.69346917\n",
      "Step: [18188] d_loss: 1.38634801, g_loss: 0.69369066\n",
      "Step: [18189] d_loss: 1.38630676, g_loss: 0.69309592\n",
      "Step: [18190] d_loss: 1.38631582, g_loss: 0.69272953\n",
      "Step: [18191] d_loss: 1.38627148, g_loss: 0.69314057\n",
      "Step: [18192] d_loss: 1.38644624, g_loss: 0.69251502\n",
      "Step: [18193] d_loss: 1.38660848, g_loss: 0.69234729\n",
      "Step: [18194] d_loss: 1.38660538, g_loss: 0.69034803\n",
      "Step: [18195] d_loss: 1.38652253, g_loss: 0.69212347\n",
      "Step: [18196] d_loss: 1.38641024, g_loss: 0.69384414\n",
      "Step: [18197] d_loss: 1.38631630, g_loss: 0.69412696\n",
      "Step: [18198] d_loss: 1.38631880, g_loss: 0.69319147\n",
      "Step: [18199] d_loss: 1.38630891, g_loss: 0.69274688\n",
      "Step: [18200] d_loss: 1.38628185, g_loss: 0.69245237\n",
      "Step: [18201] d_loss: 1.38630748, g_loss: 0.69295996\n",
      "Step: [18202] d_loss: 1.38631022, g_loss: 0.69309497\n",
      "Step: [18203] d_loss: 1.38638115, g_loss: 0.69313538\n",
      "Step: [18204] d_loss: 1.38628912, g_loss: 0.69343936\n",
      "Step: [18205] d_loss: 1.38630295, g_loss: 0.69319892\n",
      "Step: [18206] d_loss: 1.38629007, g_loss: 0.69292969\n",
      "Step: [18207] d_loss: 1.38627207, g_loss: 0.69304770\n",
      "Step: [18208] d_loss: 1.38624620, g_loss: 0.69309103\n",
      "Step: [18209] d_loss: 1.38628578, g_loss: 0.69309855\n",
      "Step: [18210] d_loss: 1.38630128, g_loss: 0.69307590\n",
      "Step: [18211] d_loss: 1.38623667, g_loss: 0.69333088\n",
      "Step: [18212] d_loss: 1.38627863, g_loss: 0.69314396\n",
      "Step: [18213] d_loss: 1.38628304, g_loss: 0.69304866\n",
      "Step: [18214] d_loss: 1.38627625, g_loss: 0.69319153\n",
      "Step: [18215] d_loss: 1.38629007, g_loss: 0.69323117\n",
      "Step: [18216] d_loss: 1.38632274, g_loss: 0.69314885\n",
      "Step: [18217] d_loss: 1.38626301, g_loss: 0.69315267\n",
      "Step: [18218] d_loss: 1.38627410, g_loss: 0.69307691\n",
      "Step: [18219] d_loss: 1.38629115, g_loss: 0.69317311\n",
      "Step: [18220] d_loss: 1.38627839, g_loss: 0.69324988\n",
      "Step: [18221] d_loss: 1.38626480, g_loss: 0.69327784\n",
      "Step: [18222] d_loss: 1.38627076, g_loss: 0.69325984\n",
      "Step: [18223] d_loss: 1.38631344, g_loss: 0.69284385\n",
      "Step: [18224] d_loss: 1.38632441, g_loss: 0.69358951\n",
      "Step: [18225] d_loss: 1.38645148, g_loss: 0.69274271\n",
      "Step: [18226] d_loss: 1.38683724, g_loss: 0.69507062\n",
      "Step: [18227] d_loss: 1.38693631, g_loss: 0.69386375\n",
      "Step: [18228] d_loss: 1.38678133, g_loss: 0.69363809\n",
      "Step: [18229] d_loss: 1.38658679, g_loss: 0.69297159\n",
      "Step: [18230] d_loss: 1.38507366, g_loss: 0.69624841\n",
      "Step: [18231] d_loss: 1.38635445, g_loss: 0.69412982\n",
      "Step: [18232] d_loss: 1.38635480, g_loss: 0.69335413\n",
      "Step: [18233] d_loss: 1.38640130, g_loss: 0.69186360\n",
      "Step: [18234] d_loss: 1.38633299, g_loss: 0.69248754\n",
      "Step: [18235] d_loss: 1.38723910, g_loss: 0.69153750\n",
      "Step: [18236] d_loss: 1.38698053, g_loss: 0.69237244\n",
      "Step: [18237] d_loss: 1.38630128, g_loss: 0.69387865\n",
      "Step: [18238] d_loss: 1.38627696, g_loss: 0.69294286\n",
      "Step: [18239] d_loss: 1.38630044, g_loss: 0.69154060\n",
      "Step: [18240] d_loss: 1.38639474, g_loss: 0.69393355\n",
      "Step: [18241] d_loss: 1.38655853, g_loss: 0.69387448\n",
      "Step: [18242] d_loss: 1.38671649, g_loss: 0.69144666\n",
      "Step: [18243] d_loss: 1.38661993, g_loss: 0.69124126\n",
      "Step: [18244] d_loss: 1.38646841, g_loss: 0.69316804\n",
      "Step: [18245] d_loss: 1.38640523, g_loss: 0.69247556\n",
      "Step: [18246] d_loss: 1.38637424, g_loss: 0.69348347\n",
      "Step: [18247] d_loss: 1.38630664, g_loss: 0.69358587\n",
      "Step: [18248] d_loss: 1.38630235, g_loss: 0.69314766\n",
      "Step: [18249] d_loss: 1.38629568, g_loss: 0.69285166\n",
      "Step: [18250] d_loss: 1.38629532, g_loss: 0.69268119\n",
      "Step: [18251] d_loss: 1.38629997, g_loss: 0.69294870\n",
      "Step: [18252] d_loss: 1.38626432, g_loss: 0.69289994\n",
      "Step: [18253] d_loss: 1.38631010, g_loss: 0.69292367\n",
      "Step: [18254] d_loss: 1.38635194, g_loss: 0.69370544\n",
      "Step: [18255] d_loss: 1.38650656, g_loss: 0.69370711\n",
      "Step: [18256] d_loss: 1.38668585, g_loss: 0.69484234\n",
      "Step: [18257] d_loss: 1.38659072, g_loss: 0.69360089\n",
      "Step: [18258] d_loss: 1.38650799, g_loss: 0.69394821\n",
      "Step: [18259] d_loss: 1.38636172, g_loss: 0.69360209\n",
      "Step: [18260] d_loss: 1.38630986, g_loss: 0.69280511\n",
      "Step: [18261] d_loss: 1.38631725, g_loss: 0.69274008\n",
      "Step: [18262] d_loss: 1.38632488, g_loss: 0.69330287\n",
      "Step: [18263] d_loss: 1.38635063, g_loss: 0.69399488\n",
      "Step: [18264] d_loss: 1.38628697, g_loss: 0.69406307\n",
      "Step: [18265] d_loss: 1.38628900, g_loss: 0.69376069\n",
      "Step: [18266] d_loss: 1.38626766, g_loss: 0.69338417\n",
      "Step: [18267] d_loss: 1.38631070, g_loss: 0.69305027\n",
      "Step: [18268] d_loss: 1.38629794, g_loss: 0.69282496\n",
      "Step: [18269] d_loss: 1.38630104, g_loss: 0.69304025\n",
      "Step: [18270] d_loss: 1.38625097, g_loss: 0.69315320\n",
      "Step: [18271] d_loss: 1.38627338, g_loss: 0.69336623\n",
      "Step: [18272] d_loss: 1.38627732, g_loss: 0.69320881\n",
      "Step: [18273] d_loss: 1.38625956, g_loss: 0.69322002\n",
      "Step: [18274] d_loss: 1.38627148, g_loss: 0.69318521\n",
      "Step: [18275] d_loss: 1.38627958, g_loss: 0.69316006\n",
      "Step: [18276] d_loss: 1.38632727, g_loss: 0.69305837\n",
      "Step: [18277] d_loss: 1.38626373, g_loss: 0.69329381\n",
      "Step: [18278] d_loss: 1.38629997, g_loss: 0.69298673\n",
      "Step: [18279] d_loss: 1.38635886, g_loss: 0.69282877\n",
      "Step: [18280] d_loss: 1.38632417, g_loss: 0.69277608\n",
      "Step: [18281] d_loss: 1.38630235, g_loss: 0.69320101\n",
      "Step: [18282] d_loss: 1.38630116, g_loss: 0.69344199\n",
      "Step: [18283] d_loss: 1.38626885, g_loss: 0.69314921\n",
      "Step: [18284] d_loss: 1.38627291, g_loss: 0.69300628\n",
      "Step: [18285] d_loss: 1.38628769, g_loss: 0.69310546\n",
      "Step: [18286] d_loss: 1.38631272, g_loss: 0.69325483\n",
      "Step: [18287] d_loss: 1.38624680, g_loss: 0.69320023\n",
      "Step: [18288] d_loss: 1.38627148, g_loss: 0.69314384\n",
      "Step: [18289] d_loss: 1.38632667, g_loss: 0.69300675\n",
      "Step: [18290] d_loss: 1.38628340, g_loss: 0.69308990\n",
      "Step: [18291] d_loss: 1.38631475, g_loss: 0.69318789\n",
      "Step: [18292] d_loss: 1.38625789, g_loss: 0.69306177\n",
      "Step: [18293] d_loss: 1.38624072, g_loss: 0.69400758\n",
      "Step: [18294] d_loss: 1.38645744, g_loss: 0.69299805\n",
      "Step: [18295] d_loss: 1.38636208, g_loss: 0.69234741\n",
      "Step: [18296] d_loss: 1.38639402, g_loss: 0.69278771\n",
      "Step: [18297] d_loss: 1.38641071, g_loss: 0.69269103\n",
      "Step: [18298] d_loss: 1.38635826, g_loss: 0.69208032\n",
      "Step: [18299] d_loss: 1.38631105, g_loss: 0.69184345\n",
      "Step: [18300] d_loss: 1.38629973, g_loss: 0.69273651\n",
      "Step: [18301] d_loss: 1.38627970, g_loss: 0.69357371\n",
      "Step: [18302] d_loss: 1.38627970, g_loss: 0.69371003\n",
      "Step: [18303] d_loss: 1.38627768, g_loss: 0.69333011\n",
      "Step: [18304] d_loss: 1.38624966, g_loss: 0.69303197\n",
      "Step: [18305] d_loss: 1.38629746, g_loss: 0.69296789\n",
      "Step: [18306] d_loss: 1.38623834, g_loss: 0.69309390\n",
      "Step: [18307] d_loss: 1.38627064, g_loss: 0.69318461\n",
      "Step: [18308] d_loss: 1.38628840, g_loss: 0.69332069\n",
      "Step: [18309] d_loss: 1.38626719, g_loss: 0.69320935\n",
      "Step: [18310] d_loss: 1.38628745, g_loss: 0.69332910\n",
      "Step: [18311] d_loss: 1.38627911, g_loss: 0.69328058\n",
      "Step: [18312] d_loss: 1.38626862, g_loss: 0.69297040\n",
      "Step: [18313] d_loss: 1.38624191, g_loss: 0.69287980\n",
      "Step: [18314] d_loss: 1.38624299, g_loss: 0.69316733\n",
      "Step: [18315] d_loss: 1.38628733, g_loss: 0.69327831\n",
      "Step: [18316] d_loss: 1.38628507, g_loss: 0.69322717\n",
      "Step: [18317] d_loss: 1.38626838, g_loss: 0.69313627\n",
      "Step: [18318] d_loss: 1.38627315, g_loss: 0.69311833\n",
      "Step: [18319] d_loss: 1.38625073, g_loss: 0.69315368\n",
      "Step: [18320] d_loss: 1.38623977, g_loss: 0.69328713\n",
      "Step: [18321] d_loss: 1.38625932, g_loss: 0.69300485\n",
      "Step: [18322] d_loss: 1.38639522, g_loss: 0.69335753\n",
      "Step: [18323] d_loss: 1.38689160, g_loss: 0.69473141\n",
      "Step: [18324] d_loss: 1.38694811, g_loss: 0.69685811\n",
      "Step: [18325] d_loss: 1.38680863, g_loss: 0.69478434\n",
      "Step: [18326] d_loss: 1.38662720, g_loss: 0.69444418\n",
      "Step: [18327] d_loss: 1.38640046, g_loss: 0.69431901\n",
      "Step: [18328] d_loss: 1.38631606, g_loss: 0.69304371\n",
      "Step: [18329] d_loss: 1.38627481, g_loss: 0.69312894\n",
      "Step: [18330] d_loss: 1.38625193, g_loss: 0.69288731\n",
      "Step: [18331] d_loss: 1.38625944, g_loss: 0.69233203\n",
      "Step: [18332] d_loss: 1.38627136, g_loss: 0.69254237\n",
      "Step: [18333] d_loss: 1.38634431, g_loss: 0.69223273\n",
      "Step: [18334] d_loss: 1.38661480, g_loss: 0.69311309\n",
      "Step: [18335] d_loss: 1.38701427, g_loss: 0.69253993\n",
      "Step: [18336] d_loss: 1.38696623, g_loss: 0.69158900\n",
      "Step: [18337] d_loss: 1.38673759, g_loss: 0.69169688\n",
      "Step: [18338] d_loss: 1.38650417, g_loss: 0.69169259\n",
      "Step: [18339] d_loss: 1.38639319, g_loss: 0.69335330\n",
      "Step: [18340] d_loss: 1.38624668, g_loss: 0.69456828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18341] d_loss: 1.38629258, g_loss: 0.69396186\n",
      "Step: [18342] d_loss: 1.38626194, g_loss: 0.69330657\n",
      "Step: [18343] d_loss: 1.38625240, g_loss: 0.69297445\n",
      "Step: [18344] d_loss: 1.38625872, g_loss: 0.69291472\n",
      "Step: [18345] d_loss: 1.38628042, g_loss: 0.69296879\n",
      "Step: [18346] d_loss: 1.38633049, g_loss: 0.69317138\n",
      "Step: [18347] d_loss: 1.38634753, g_loss: 0.69338346\n",
      "Step: [18348] d_loss: 1.38623667, g_loss: 0.69337583\n",
      "Step: [18349] d_loss: 1.38625455, g_loss: 0.69316220\n",
      "Step: [18350] d_loss: 1.38625908, g_loss: 0.69307995\n",
      "Step: [18351] d_loss: 1.38625073, g_loss: 0.69315827\n",
      "Step: [18352] d_loss: 1.38618672, g_loss: 0.69374382\n",
      "Step: [18353] d_loss: 1.38631546, g_loss: 0.69296753\n",
      "Step: [18354] d_loss: 1.38630736, g_loss: 0.69326258\n",
      "Step: [18355] d_loss: 1.38627625, g_loss: 0.69298947\n",
      "Step: [18356] d_loss: 1.38626170, g_loss: 0.69320130\n",
      "Step: [18357] d_loss: 1.38634503, g_loss: 0.69410408\n",
      "Step: [18358] d_loss: 1.38653231, g_loss: 0.69405127\n",
      "Step: [18359] d_loss: 1.38654661, g_loss: 0.69372213\n",
      "Step: [18360] d_loss: 1.38645768, g_loss: 0.69397104\n",
      "Step: [18361] d_loss: 1.38674855, g_loss: 0.69334400\n",
      "Step: [18362] d_loss: 1.38639951, g_loss: 0.69487816\n",
      "Step: [18363] d_loss: 1.38628960, g_loss: 0.69346011\n",
      "Step: [18364] d_loss: 1.38625908, g_loss: 0.69245803\n",
      "Step: [18365] d_loss: 1.38630939, g_loss: 0.69253469\n",
      "Step: [18366] d_loss: 1.38625741, g_loss: 0.69305730\n",
      "Step: [18367] d_loss: 1.38632452, g_loss: 0.69301426\n",
      "Step: [18368] d_loss: 1.38633633, g_loss: 0.69325978\n",
      "Step: [18369] d_loss: 1.38620615, g_loss: 0.69457150\n",
      "Step: [18370] d_loss: 1.38624084, g_loss: 0.69277632\n",
      "Step: [18371] d_loss: 1.38629532, g_loss: 0.69292521\n",
      "Step: [18372] d_loss: 1.38625073, g_loss: 0.69219553\n",
      "Step: [18373] d_loss: 1.38631678, g_loss: 0.69245601\n",
      "Step: [18374] d_loss: 1.38630939, g_loss: 0.69425201\n",
      "Step: [18375] d_loss: 1.38629687, g_loss: 0.69434589\n",
      "Step: [18376] d_loss: 1.38630438, g_loss: 0.69340938\n",
      "Step: [18377] d_loss: 1.38626623, g_loss: 0.69302571\n",
      "Step: [18378] d_loss: 1.38627326, g_loss: 0.69242239\n",
      "Step: [18379] d_loss: 1.38626480, g_loss: 0.69274879\n",
      "Step: [18380] d_loss: 1.38632131, g_loss: 0.69312632\n",
      "Step: [18381] d_loss: 1.38620734, g_loss: 0.69341040\n",
      "Step: [18382] d_loss: 1.38631606, g_loss: 0.69273639\n",
      "Step: [18383] d_loss: 1.38630366, g_loss: 0.69296622\n",
      "Step: [18384] d_loss: 1.38636577, g_loss: 0.69271469\n",
      "Step: [18385] d_loss: 1.38619208, g_loss: 0.69202745\n",
      "Step: [18386] d_loss: 1.38660002, g_loss: 0.69169390\n",
      "Step: [18387] d_loss: 1.38709450, g_loss: 0.69506931\n",
      "Step: [18388] d_loss: 1.38696647, g_loss: 0.69605207\n",
      "Step: [18389] d_loss: 1.38677883, g_loss: 0.69345796\n",
      "Step: [18390] d_loss: 1.38659167, g_loss: 0.69173241\n",
      "Step: [18391] d_loss: 1.38645434, g_loss: 0.69165790\n",
      "Step: [18392] d_loss: 1.38638616, g_loss: 0.69433862\n",
      "Step: [18393] d_loss: 1.38630962, g_loss: 0.69340193\n",
      "Step: [18394] d_loss: 1.38632381, g_loss: 0.69183731\n",
      "Step: [18395] d_loss: 1.38631570, g_loss: 0.69256610\n",
      "Step: [18396] d_loss: 1.38629484, g_loss: 0.69341594\n",
      "Step: [18397] d_loss: 1.38630080, g_loss: 0.69366091\n",
      "Step: [18398] d_loss: 1.38626337, g_loss: 0.69347382\n",
      "Step: [18399] d_loss: 1.38633001, g_loss: 0.69276249\n",
      "Step: [18400] d_loss: 1.38625979, g_loss: 0.69284689\n",
      "Step: [18401] d_loss: 1.38628578, g_loss: 0.69343656\n",
      "Step: [18402] d_loss: 1.38628697, g_loss: 0.69294482\n",
      "Step: [18403] d_loss: 1.38629377, g_loss: 0.69311965\n",
      "Step: [18404] d_loss: 1.38628983, g_loss: 0.69362921\n",
      "Step: [18405] d_loss: 1.38637483, g_loss: 0.69317818\n",
      "Step: [18406] d_loss: 1.38657999, g_loss: 0.69445646\n",
      "Step: [18407] d_loss: 1.38660002, g_loss: 0.69525295\n",
      "Step: [18408] d_loss: 1.38651204, g_loss: 0.69511944\n",
      "Step: [18409] d_loss: 1.38641143, g_loss: 0.69356376\n",
      "Step: [18410] d_loss: 1.38634455, g_loss: 0.69150865\n",
      "Step: [18411] d_loss: 1.38629627, g_loss: 0.69253111\n",
      "Step: [18412] d_loss: 1.38630915, g_loss: 0.69383520\n",
      "Step: [18413] d_loss: 1.38610888, g_loss: 0.69280291\n",
      "Step: [18414] d_loss: 1.38628459, g_loss: 0.69244945\n",
      "Step: [18415] d_loss: 1.38660133, g_loss: 0.69081366\n",
      "Step: [18416] d_loss: 1.38682151, g_loss: 0.69057488\n",
      "Step: [18417] d_loss: 1.38678205, g_loss: 0.69009531\n",
      "Step: [18418] d_loss: 1.38661575, g_loss: 0.69203961\n",
      "Step: [18419] d_loss: 1.38643479, g_loss: 0.69257182\n",
      "Step: [18420] d_loss: 1.38634109, g_loss: 0.69283152\n",
      "Step: [18421] d_loss: 1.38630795, g_loss: 0.69307625\n",
      "Step: [18422] d_loss: 1.38625145, g_loss: 0.69305217\n",
      "Step: [18423] d_loss: 1.38626492, g_loss: 0.69318861\n",
      "Step: [18424] d_loss: 1.38619530, g_loss: 0.69334471\n",
      "Step: [18425] d_loss: 1.38627315, g_loss: 0.69335145\n",
      "Step: [18426] d_loss: 1.38628173, g_loss: 0.69331336\n",
      "Step: [18427] d_loss: 1.38624430, g_loss: 0.69320178\n",
      "Step: [18428] d_loss: 1.38625824, g_loss: 0.69310111\n",
      "Step: [18429] d_loss: 1.38625836, g_loss: 0.69309729\n",
      "Step: [18430] d_loss: 1.38619113, g_loss: 0.69323027\n",
      "Step: [18431] d_loss: 1.38626099, g_loss: 0.69308400\n",
      "Step: [18432] d_loss: 1.38616550, g_loss: 0.69305843\n",
      "Step: [18433] d_loss: 1.38632417, g_loss: 0.69319528\n",
      "Step: [18434] d_loss: 1.38629508, g_loss: 0.69323277\n",
      "Step: [18435] d_loss: 1.38626575, g_loss: 0.69317222\n",
      "Step: [18436] d_loss: 1.38624525, g_loss: 0.69319594\n",
      "Step: [18437] d_loss: 1.38632464, g_loss: 0.69290555\n",
      "Step: [18438] d_loss: 1.38634968, g_loss: 0.69302803\n",
      "Step: [18439] d_loss: 1.38632584, g_loss: 0.69312477\n",
      "Step: [18440] d_loss: 1.38629591, g_loss: 0.69324619\n",
      "Step: [18441] d_loss: 1.38630927, g_loss: 0.69321668\n",
      "Step: [18442] d_loss: 1.38625693, g_loss: 0.69329220\n",
      "Step: [18443] d_loss: 1.38630557, g_loss: 0.69322973\n",
      "Step: [18444] d_loss: 1.38628411, g_loss: 0.69315648\n",
      "Step: [18445] d_loss: 1.38624549, g_loss: 0.69313747\n",
      "Step: [18446] d_loss: 1.38626814, g_loss: 0.69330031\n",
      "Step: [18447] d_loss: 1.38628292, g_loss: 0.69327945\n",
      "Step: [18448] d_loss: 1.38632619, g_loss: 0.69305456\n",
      "Step: [18449] d_loss: 1.38628173, g_loss: 0.69308364\n",
      "Step: [18450] d_loss: 1.38628793, g_loss: 0.69316351\n",
      "Step: [18451] d_loss: 1.38629246, g_loss: 0.69333673\n",
      "Step: [18452] d_loss: 1.38631177, g_loss: 0.69315726\n",
      "Step: [18453] d_loss: 1.38630593, g_loss: 0.69292748\n",
      "Step: [18454] d_loss: 1.38629270, g_loss: 0.69317853\n",
      "Step: [18455] d_loss: 1.38625979, g_loss: 0.69311321\n",
      "Step: [18456] d_loss: 1.38626635, g_loss: 0.69315648\n",
      "Step: [18457] d_loss: 1.38628316, g_loss: 0.69332379\n",
      "Step: [18458] d_loss: 1.38627791, g_loss: 0.69320208\n",
      "Step: [18459] d_loss: 1.38627434, g_loss: 0.69313979\n",
      "Step: [18460] d_loss: 1.38637733, g_loss: 0.69308424\n",
      "Step: [18461] d_loss: 1.38627696, g_loss: 0.69303799\n",
      "Step: [18462] d_loss: 1.38623178, g_loss: 0.69287848\n",
      "Step: [18463] d_loss: 1.38628268, g_loss: 0.69347763\n",
      "Step: [18464] d_loss: 1.38627827, g_loss: 0.69436181\n",
      "Step: [18465] d_loss: 1.38645494, g_loss: 0.69344342\n",
      "Step: [18466] d_loss: 1.38645387, g_loss: 0.69304836\n",
      "Step: [18467] d_loss: 1.38640761, g_loss: 0.69473696\n",
      "Step: [18468] d_loss: 1.38639653, g_loss: 0.69514149\n",
      "Step: [18469] d_loss: 1.38634884, g_loss: 0.69350517\n",
      "Step: [18470] d_loss: 1.38639605, g_loss: 0.69344020\n",
      "Step: [18471] d_loss: 1.38635528, g_loss: 0.69302070\n",
      "Step: [18472] d_loss: 1.38628983, g_loss: 0.69293880\n",
      "Step: [18473] d_loss: 1.38631356, g_loss: 0.69267666\n",
      "Step: [18474] d_loss: 1.38624537, g_loss: 0.69317073\n",
      "Step: [18475] d_loss: 1.38630176, g_loss: 0.69307482\n",
      "Step: [18476] d_loss: 1.38625646, g_loss: 0.69302881\n",
      "Step: [18477] d_loss: 1.38625669, g_loss: 0.69300151\n",
      "Step: [18478] d_loss: 1.38627350, g_loss: 0.69323242\n",
      "Step: [18479] d_loss: 1.38630700, g_loss: 0.69323838\n",
      "Step: [18480] d_loss: 1.38627982, g_loss: 0.69321418\n",
      "Step: [18481] d_loss: 1.38630605, g_loss: 0.69313484\n",
      "Step: [18482] d_loss: 1.38624001, g_loss: 0.69325250\n",
      "Step: [18483] d_loss: 1.38640761, g_loss: 0.69453967\n",
      "Step: [18484] d_loss: 1.38670194, g_loss: 0.69448853\n",
      "Step: [18485] d_loss: 1.38662314, g_loss: 0.69279635\n",
      "Step: [18486] d_loss: 1.38656914, g_loss: 0.69378757\n",
      "Step: [18487] d_loss: 1.38655448, g_loss: 0.69299817\n",
      "Step: [18488] d_loss: 1.38645792, g_loss: 0.69331199\n",
      "Step: [18489] d_loss: 1.38644433, g_loss: 0.69196117\n",
      "Step: [18490] d_loss: 1.38638306, g_loss: 0.69188654\n",
      "Step: [18491] d_loss: 1.38634777, g_loss: 0.69278783\n",
      "Step: [18492] d_loss: 1.38633060, g_loss: 0.69397861\n",
      "Step: [18493] d_loss: 1.38627148, g_loss: 0.69472992\n",
      "Step: [18494] d_loss: 1.38627577, g_loss: 0.69289291\n",
      "Step: [18495] d_loss: 1.38631761, g_loss: 0.69241267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18496] d_loss: 1.38631701, g_loss: 0.69332898\n",
      "Step: [18497] d_loss: 1.38627434, g_loss: 0.69331825\n",
      "Step: [18498] d_loss: 1.38638711, g_loss: 0.69444579\n",
      "Step: [18499] d_loss: 1.38672900, g_loss: 0.69640851\n",
      "Step: [18500] d_loss: 1.38682723, g_loss: 0.69696933\n",
      "Step: [18501] d_loss: 1.38662589, g_loss: 0.69340283\n",
      "Step: [18502] d_loss: 1.38700175, g_loss: 0.68978137\n",
      "Step: [18503] d_loss: 1.38695014, g_loss: 0.69136107\n",
      "Step: [18504] d_loss: 1.38721180, g_loss: 0.69500548\n",
      "Step: [18505] d_loss: 1.38664103, g_loss: 0.69830692\n",
      "Step: [18506] d_loss: 1.38642383, g_loss: 0.69508255\n",
      "Step: [18507] d_loss: 1.38631284, g_loss: 0.69370866\n",
      "Step: [18508] d_loss: 1.38667011, g_loss: 0.69321275\n",
      "Step: [18509] d_loss: 1.38656008, g_loss: 0.69218391\n",
      "Step: [18510] d_loss: 1.38698411, g_loss: 0.69138801\n",
      "Step: [18511] d_loss: 1.38750696, g_loss: 0.68951774\n",
      "Step: [18512] d_loss: 1.38747382, g_loss: 0.69349271\n",
      "Step: [18513] d_loss: 1.38772094, g_loss: 0.69101566\n",
      "Step: [18514] d_loss: 1.38692451, g_loss: 0.69142628\n",
      "Step: [18515] d_loss: 1.38666677, g_loss: 0.69191074\n",
      "Step: [18516] d_loss: 1.38642752, g_loss: 0.69357938\n",
      "Step: [18517] d_loss: 1.38634431, g_loss: 0.69329327\n",
      "Step: [18518] d_loss: 1.38625741, g_loss: 0.69389486\n",
      "Step: [18519] d_loss: 1.38633418, g_loss: 0.69385254\n",
      "Step: [18520] d_loss: 1.38652909, g_loss: 0.69630921\n",
      "Step: [18521] d_loss: 1.38650119, g_loss: 0.69506770\n",
      "Step: [18522] d_loss: 1.38641953, g_loss: 0.69361174\n",
      "Step: [18523] d_loss: 1.38636339, g_loss: 0.69119871\n",
      "Step: [18524] d_loss: 1.38629103, g_loss: 0.69177943\n",
      "Step: [18525] d_loss: 1.38626707, g_loss: 0.69314289\n",
      "Step: [18526] d_loss: 1.38630986, g_loss: 0.69391870\n",
      "Step: [18527] d_loss: 1.38630390, g_loss: 0.69371080\n",
      "Step: [18528] d_loss: 1.38628197, g_loss: 0.69312513\n",
      "Step: [18529] d_loss: 1.38629293, g_loss: 0.69252574\n",
      "Step: [18530] d_loss: 1.38628602, g_loss: 0.69295228\n",
      "Step: [18531] d_loss: 1.38631821, g_loss: 0.69304657\n",
      "Step: [18532] d_loss: 1.38627219, g_loss: 0.69344091\n",
      "Step: [18533] d_loss: 1.38624895, g_loss: 0.69324815\n",
      "Step: [18534] d_loss: 1.38627338, g_loss: 0.69297528\n",
      "Step: [18535] d_loss: 1.38631511, g_loss: 0.69280654\n",
      "Step: [18536] d_loss: 1.38624954, g_loss: 0.69301057\n",
      "Step: [18537] d_loss: 1.38628078, g_loss: 0.69347358\n",
      "Step: [18538] d_loss: 1.38627291, g_loss: 0.69317859\n",
      "Step: [18539] d_loss: 1.38626945, g_loss: 0.69289869\n",
      "Step: [18540] d_loss: 1.38631582, g_loss: 0.69338799\n",
      "Step: [18541] d_loss: 1.38628101, g_loss: 0.69298685\n",
      "Step: [18542] d_loss: 1.38626242, g_loss: 0.69295263\n",
      "Step: [18543] d_loss: 1.38636613, g_loss: 0.69254816\n",
      "Step: [18544] d_loss: 1.38629174, g_loss: 0.69341356\n",
      "Step: [18545] d_loss: 1.38629031, g_loss: 0.69316208\n",
      "Step: [18546] d_loss: 1.38629413, g_loss: 0.69318718\n",
      "Step: [18547] d_loss: 1.38628793, g_loss: 0.69296050\n",
      "Step: [18548] d_loss: 1.38628531, g_loss: 0.69290388\n",
      "Step: [18549] d_loss: 1.38630509, g_loss: 0.69315845\n",
      "Step: [18550] d_loss: 1.38628268, g_loss: 0.69299579\n",
      "Step: [18551] d_loss: 1.38627470, g_loss: 0.69312847\n",
      "Step: [18552] d_loss: 1.38626122, g_loss: 0.69327223\n",
      "Step: [18553] d_loss: 1.38628209, g_loss: 0.69339132\n",
      "Step: [18554] d_loss: 1.38624310, g_loss: 0.69318712\n",
      "Step: [18555] d_loss: 1.38627064, g_loss: 0.69316190\n",
      "Step: [18556] d_loss: 1.38628364, g_loss: 0.69217801\n",
      "Step: [18557] d_loss: 1.38617098, g_loss: 0.69378954\n",
      "Step: [18558] d_loss: 1.38634419, g_loss: 0.69320941\n",
      "Step: [18559] d_loss: 1.38654041, g_loss: 0.69219482\n",
      "Step: [18560] d_loss: 1.38654089, g_loss: 0.69457233\n",
      "Step: [18561] d_loss: 1.38666344, g_loss: 0.69477707\n",
      "Step: [18562] d_loss: 1.38681340, g_loss: 0.68747294\n",
      "Step: [18563] d_loss: 1.38680887, g_loss: 0.68873525\n",
      "Step: [18564] d_loss: 1.38669395, g_loss: 0.69295269\n",
      "Step: [18565] d_loss: 1.38654256, g_loss: 0.69388711\n",
      "Step: [18566] d_loss: 1.38641560, g_loss: 0.69480127\n",
      "Step: [18567] d_loss: 1.38635576, g_loss: 0.69350320\n",
      "Step: [18568] d_loss: 1.38632011, g_loss: 0.69286251\n",
      "Step: [18569] d_loss: 1.38627613, g_loss: 0.69262469\n",
      "Step: [18570] d_loss: 1.38623893, g_loss: 0.69317055\n",
      "Step: [18571] d_loss: 1.38647473, g_loss: 0.69343615\n",
      "Step: [18572] d_loss: 1.38665974, g_loss: 0.69496453\n",
      "Step: [18573] d_loss: 1.38678229, g_loss: 0.69299793\n",
      "Step: [18574] d_loss: 1.38666213, g_loss: 0.69098741\n",
      "Step: [18575] d_loss: 1.38651192, g_loss: 0.69136727\n",
      "Step: [18576] d_loss: 1.38642776, g_loss: 0.69311637\n",
      "Step: [18577] d_loss: 1.38639009, g_loss: 0.69396520\n",
      "Step: [18578] d_loss: 1.38632536, g_loss: 0.69345015\n",
      "Step: [18579] d_loss: 1.38629103, g_loss: 0.69299805\n",
      "Step: [18580] d_loss: 1.38631594, g_loss: 0.69275564\n",
      "Step: [18581] d_loss: 1.38629711, g_loss: 0.69306231\n",
      "Step: [18582] d_loss: 1.38630962, g_loss: 0.69332325\n",
      "Step: [18583] d_loss: 1.38630247, g_loss: 0.69318938\n",
      "Step: [18584] d_loss: 1.38629937, g_loss: 0.69335139\n",
      "Step: [18585] d_loss: 1.38626111, g_loss: 0.69324076\n",
      "Step: [18586] d_loss: 1.38631225, g_loss: 0.69298828\n",
      "Step: [18587] d_loss: 1.38627434, g_loss: 0.69289410\n",
      "Step: [18588] d_loss: 1.38631260, g_loss: 0.69334090\n",
      "Step: [18589] d_loss: 1.38628924, g_loss: 0.69380581\n",
      "Step: [18590] d_loss: 1.38626945, g_loss: 0.69332373\n",
      "Step: [18591] d_loss: 1.38634038, g_loss: 0.69355541\n",
      "Step: [18592] d_loss: 1.38631094, g_loss: 0.69304192\n",
      "Step: [18593] d_loss: 1.38627625, g_loss: 0.69321179\n",
      "Step: [18594] d_loss: 1.38626289, g_loss: 0.69323242\n",
      "Step: [18595] d_loss: 1.38627779, g_loss: 0.69363344\n",
      "Step: [18596] d_loss: 1.38632226, g_loss: 0.69323802\n",
      "Step: [18597] d_loss: 1.38631940, g_loss: 0.69298601\n",
      "Step: [18598] d_loss: 1.38627231, g_loss: 0.69303507\n",
      "Step: [18599] d_loss: 1.38632584, g_loss: 0.69344139\n",
      "Step: [18600] d_loss: 1.38628149, g_loss: 0.69341564\n",
      "Step: [18601] d_loss: 1.38628387, g_loss: 0.69362855\n",
      "Step: [18602] d_loss: 1.38655007, g_loss: 0.69296587\n",
      "Step: [18603] d_loss: 1.38669109, g_loss: 0.69204664\n",
      "Step: [18604] d_loss: 1.38675714, g_loss: 0.69270492\n",
      "Step: [18605] d_loss: 1.38668656, g_loss: 0.69429719\n",
      "Step: [18606] d_loss: 1.38660216, g_loss: 0.69450873\n",
      "Step: [18607] d_loss: 1.38651371, g_loss: 0.69481623\n",
      "Step: [18608] d_loss: 1.38641262, g_loss: 0.69311303\n",
      "Step: [18609] d_loss: 1.38631403, g_loss: 0.69160157\n",
      "Step: [18610] d_loss: 1.38627696, g_loss: 0.69216239\n",
      "Step: [18611] d_loss: 1.38630247, g_loss: 0.69286072\n",
      "Step: [18612] d_loss: 1.38631678, g_loss: 0.69401360\n",
      "Step: [18613] d_loss: 1.38627911, g_loss: 0.69393253\n",
      "Step: [18614] d_loss: 1.38631296, g_loss: 0.69284320\n",
      "Step: [18615] d_loss: 1.38627362, g_loss: 0.69298369\n",
      "Step: [18616] d_loss: 1.38630652, g_loss: 0.69282556\n",
      "Step: [18617] d_loss: 1.38629079, g_loss: 0.69374484\n",
      "Step: [18618] d_loss: 1.38625669, g_loss: 0.69331163\n",
      "Step: [18619] d_loss: 1.38630033, g_loss: 0.69386417\n",
      "Step: [18620] d_loss: 1.38628125, g_loss: 0.69322598\n",
      "Step: [18621] d_loss: 1.38628054, g_loss: 0.69324458\n",
      "Step: [18622] d_loss: 1.38602829, g_loss: 0.69318044\n",
      "Step: [18623] d_loss: 1.38627076, g_loss: 0.69288206\n",
      "Step: [18624] d_loss: 1.38629889, g_loss: 0.69319475\n",
      "Step: [18625] d_loss: 1.38628507, g_loss: 0.69394362\n",
      "Step: [18626] d_loss: 1.38632727, g_loss: 0.69377029\n",
      "Step: [18627] d_loss: 1.38629425, g_loss: 0.69303107\n",
      "Step: [18628] d_loss: 1.38627970, g_loss: 0.69290811\n",
      "Step: [18629] d_loss: 1.38629484, g_loss: 0.69359928\n",
      "Step: [18630] d_loss: 1.38632286, g_loss: 0.69540405\n",
      "Step: [18631] d_loss: 1.38622475, g_loss: 0.69577134\n",
      "Step: [18632] d_loss: 1.38626766, g_loss: 0.69438004\n",
      "Step: [18633] d_loss: 1.38625145, g_loss: 0.69323057\n",
      "Step: [18634] d_loss: 1.38624370, g_loss: 0.69238043\n",
      "Step: [18635] d_loss: 1.38625526, g_loss: 0.69266123\n",
      "Step: [18636] d_loss: 1.38626063, g_loss: 0.69335037\n",
      "Step: [18637] d_loss: 1.38626623, g_loss: 0.69390339\n",
      "Step: [18638] d_loss: 1.38630104, g_loss: 0.69354844\n",
      "Step: [18639] d_loss: 1.38624084, g_loss: 0.69422269\n",
      "Step: [18640] d_loss: 1.38651872, g_loss: 0.69304854\n",
      "Step: [18641] d_loss: 1.38736951, g_loss: 0.69117904\n",
      "Step: [18642] d_loss: 1.38749743, g_loss: 0.69355464\n",
      "Step: [18643] d_loss: 1.38701224, g_loss: 0.69568080\n",
      "Step: [18644] d_loss: 1.38759327, g_loss: 0.69467509\n",
      "Step: [18645] d_loss: 1.38892233, g_loss: 0.69172049\n",
      "Step: [18646] d_loss: 1.38962328, g_loss: 0.69063294\n",
      "Step: [18647] d_loss: 1.38858569, g_loss: 0.69231415\n",
      "Step: [18648] d_loss: 1.38728058, g_loss: 0.69596696\n",
      "Step: [18649] d_loss: 1.38650644, g_loss: 0.69500244\n",
      "Step: [18650] d_loss: 1.38624644, g_loss: 0.69284642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18651] d_loss: 1.38625336, g_loss: 0.69223028\n",
      "Step: [18652] d_loss: 1.38633013, g_loss: 0.69320905\n",
      "Step: [18653] d_loss: 1.38635397, g_loss: 0.69457138\n",
      "Step: [18654] d_loss: 1.38626671, g_loss: 0.69447064\n",
      "Step: [18655] d_loss: 1.38623202, g_loss: 0.69412154\n",
      "Step: [18656] d_loss: 1.38628221, g_loss: 0.69319904\n",
      "Step: [18657] d_loss: 1.38622069, g_loss: 0.69276023\n",
      "Step: [18658] d_loss: 1.38623810, g_loss: 0.69301391\n",
      "Step: [18659] d_loss: 1.38622165, g_loss: 0.69306219\n",
      "Step: [18660] d_loss: 1.38621044, g_loss: 0.69341099\n",
      "Step: [18661] d_loss: 1.38621235, g_loss: 0.69355202\n",
      "Step: [18662] d_loss: 1.38623047, g_loss: 0.69354951\n",
      "Step: [18663] d_loss: 1.38647842, g_loss: 0.69267356\n",
      "Step: [18664] d_loss: 1.38628745, g_loss: 0.69251454\n",
      "Step: [18665] d_loss: 1.38628221, g_loss: 0.69329643\n",
      "Step: [18666] d_loss: 1.38633358, g_loss: 0.69398338\n",
      "Step: [18667] d_loss: 1.38660944, g_loss: 0.69244450\n",
      "Step: [18668] d_loss: 1.38649166, g_loss: 0.69088870\n",
      "Step: [18669] d_loss: 1.38645458, g_loss: 0.69135958\n",
      "Step: [18670] d_loss: 1.38636410, g_loss: 0.69255495\n",
      "Step: [18671] d_loss: 1.38621426, g_loss: 0.69431013\n",
      "Step: [18672] d_loss: 1.38624668, g_loss: 0.69431394\n",
      "Step: [18673] d_loss: 1.38633084, g_loss: 0.69379044\n",
      "Step: [18674] d_loss: 1.38621962, g_loss: 0.69396985\n",
      "Step: [18675] d_loss: 1.38628161, g_loss: 0.69294667\n",
      "Step: [18676] d_loss: 1.38672781, g_loss: 0.69586426\n",
      "Step: [18677] d_loss: 1.38708341, g_loss: 0.69411922\n",
      "Step: [18678] d_loss: 1.38700342, g_loss: 0.69326985\n",
      "Step: [18679] d_loss: 1.38666391, g_loss: 0.69578433\n",
      "Step: [18680] d_loss: 1.38647747, g_loss: 0.69589329\n",
      "Step: [18681] d_loss: 1.38633931, g_loss: 0.69411540\n",
      "Step: [18682] d_loss: 1.38635778, g_loss: 0.69238359\n",
      "Step: [18683] d_loss: 1.38630450, g_loss: 0.69225413\n",
      "Step: [18684] d_loss: 1.38624287, g_loss: 0.69251919\n",
      "Step: [18685] d_loss: 1.38629556, g_loss: 0.69313884\n",
      "Step: [18686] d_loss: 1.38622379, g_loss: 0.69368929\n",
      "Step: [18687] d_loss: 1.38624072, g_loss: 0.69349194\n",
      "Step: [18688] d_loss: 1.38631821, g_loss: 0.69314557\n",
      "Step: [18689] d_loss: 1.38621342, g_loss: 0.69300610\n",
      "Step: [18690] d_loss: 1.38629746, g_loss: 0.69317079\n",
      "Step: [18691] d_loss: 1.38633990, g_loss: 0.69325465\n",
      "Step: [18692] d_loss: 1.38629889, g_loss: 0.69307709\n",
      "Step: [18693] d_loss: 1.38630676, g_loss: 0.69280279\n",
      "Step: [18694] d_loss: 1.38629150, g_loss: 0.69299889\n",
      "Step: [18695] d_loss: 1.38635552, g_loss: 0.69312465\n",
      "Step: [18696] d_loss: 1.38626885, g_loss: 0.69293940\n",
      "Step: [18697] d_loss: 1.38632369, g_loss: 0.69300687\n",
      "Step: [18698] d_loss: 1.38623130, g_loss: 0.69329989\n",
      "Step: [18699] d_loss: 1.38629735, g_loss: 0.69370723\n",
      "Step: [18700] d_loss: 1.38631225, g_loss: 0.69376564\n",
      "Step: [18701] d_loss: 1.38634026, g_loss: 0.69315112\n",
      "Step: [18702] d_loss: 1.38629246, g_loss: 0.69299835\n",
      "Step: [18703] d_loss: 1.38638258, g_loss: 0.69287121\n",
      "Step: [18704] d_loss: 1.38628483, g_loss: 0.69309723\n",
      "Step: [18705] d_loss: 1.38627315, g_loss: 0.69335616\n",
      "Step: [18706] d_loss: 1.38626456, g_loss: 0.69333398\n",
      "Step: [18707] d_loss: 1.38626742, g_loss: 0.69313562\n",
      "Step: [18708] d_loss: 1.38626003, g_loss: 0.69295597\n",
      "Step: [18709] d_loss: 1.38626063, g_loss: 0.69316626\n",
      "Step: [18710] d_loss: 1.38634300, g_loss: 0.69288212\n",
      "Step: [18711] d_loss: 1.38623857, g_loss: 0.69240540\n",
      "Step: [18712] d_loss: 1.38638759, g_loss: 0.69529390\n",
      "Step: [18713] d_loss: 1.38666904, g_loss: 0.69539022\n",
      "Step: [18714] d_loss: 1.38638937, g_loss: 0.69388902\n",
      "Step: [18715] d_loss: 1.38630974, g_loss: 0.69269001\n",
      "Step: [18716] d_loss: 1.38654327, g_loss: 0.69296545\n",
      "Step: [18717] d_loss: 1.38634109, g_loss: 0.69351482\n",
      "Step: [18718] d_loss: 1.38637352, g_loss: 0.69257540\n",
      "Step: [18719] d_loss: 1.38635802, g_loss: 0.69279730\n",
      "Step: [18720] d_loss: 1.38644350, g_loss: 0.69310051\n",
      "Step: [18721] d_loss: 1.38652778, g_loss: 0.69458485\n",
      "Step: [18722] d_loss: 1.38649940, g_loss: 0.69485074\n",
      "Step: [18723] d_loss: 1.38655448, g_loss: 0.69409090\n",
      "Step: [18724] d_loss: 1.38636184, g_loss: 0.69283932\n",
      "Step: [18725] d_loss: 1.38630772, g_loss: 0.69253635\n",
      "Step: [18726] d_loss: 1.38633156, g_loss: 0.69233871\n",
      "Step: [18727] d_loss: 1.38626671, g_loss: 0.69318384\n",
      "Step: [18728] d_loss: 1.38631725, g_loss: 0.69320756\n",
      "Step: [18729] d_loss: 1.38622975, g_loss: 0.69350523\n",
      "Step: [18730] d_loss: 1.38625824, g_loss: 0.69329518\n",
      "Step: [18731] d_loss: 1.38627601, g_loss: 0.69276726\n",
      "Step: [18732] d_loss: 1.38624990, g_loss: 0.69262850\n",
      "Step: [18733] d_loss: 1.38630319, g_loss: 0.69337213\n",
      "Step: [18734] d_loss: 1.38627434, g_loss: 0.69328117\n",
      "Step: [18735] d_loss: 1.38622022, g_loss: 0.69332433\n",
      "Step: [18736] d_loss: 1.38633251, g_loss: 0.69305408\n",
      "Step: [18737] d_loss: 1.38628387, g_loss: 0.69325125\n",
      "Step: [18738] d_loss: 1.38624334, g_loss: 0.69306624\n",
      "Step: [18739] d_loss: 1.38622665, g_loss: 0.69307506\n",
      "Step: [18740] d_loss: 1.38627899, g_loss: 0.69332433\n",
      "Step: [18741] d_loss: 1.38627982, g_loss: 0.69319904\n",
      "Step: [18742] d_loss: 1.38626838, g_loss: 0.69314200\n",
      "Step: [18743] d_loss: 1.38627481, g_loss: 0.69315934\n",
      "Step: [18744] d_loss: 1.38628078, g_loss: 0.69299638\n",
      "Step: [18745] d_loss: 1.38629961, g_loss: 0.69305217\n",
      "Step: [18746] d_loss: 1.38627434, g_loss: 0.69310766\n",
      "Step: [18747] d_loss: 1.38628364, g_loss: 0.69319665\n",
      "Step: [18748] d_loss: 1.38626456, g_loss: 0.69317609\n",
      "Step: [18749] d_loss: 1.38629103, g_loss: 0.69319546\n",
      "Step: [18750] d_loss: 1.38629496, g_loss: 0.69314080\n",
      "Step: [18751] d_loss: 1.38628221, g_loss: 0.69314039\n",
      "Step: [18752] d_loss: 1.38627934, g_loss: 0.69322395\n",
      "Step: [18753] d_loss: 1.38627243, g_loss: 0.69321847\n",
      "Step: [18754] d_loss: 1.38633215, g_loss: 0.69311100\n",
      "Step: [18755] d_loss: 1.38628089, g_loss: 0.69286764\n",
      "Step: [18756] d_loss: 1.38628364, g_loss: 0.69311440\n",
      "Step: [18757] d_loss: 1.38630795, g_loss: 0.69271445\n",
      "Step: [18758] d_loss: 1.38628006, g_loss: 0.69317079\n",
      "Step: [18759] d_loss: 1.38635015, g_loss: 0.69355530\n",
      "Step: [18760] d_loss: 1.38628364, g_loss: 0.69344246\n",
      "Step: [18761] d_loss: 1.38631427, g_loss: 0.69306850\n",
      "Step: [18762] d_loss: 1.38630486, g_loss: 0.69305241\n",
      "Step: [18763] d_loss: 1.38635242, g_loss: 0.69308746\n",
      "Step: [18764] d_loss: 1.38633847, g_loss: 0.69319046\n",
      "Step: [18765] d_loss: 1.38631952, g_loss: 0.69323653\n",
      "Step: [18766] d_loss: 1.38633990, g_loss: 0.69320166\n",
      "Step: [18767] d_loss: 1.38630712, g_loss: 0.69299513\n",
      "Step: [18768] d_loss: 1.38628578, g_loss: 0.69295764\n",
      "Step: [18769] d_loss: 1.38630176, g_loss: 0.69315475\n",
      "Step: [18770] d_loss: 1.38630664, g_loss: 0.69296193\n",
      "Step: [18771] d_loss: 1.38632703, g_loss: 0.69309640\n",
      "Step: [18772] d_loss: 1.38629997, g_loss: 0.69330698\n",
      "Step: [18773] d_loss: 1.38633764, g_loss: 0.69326651\n",
      "Step: [18774] d_loss: 1.38631248, g_loss: 0.69310677\n",
      "Step: [18775] d_loss: 1.38630772, g_loss: 0.69290113\n",
      "Step: [18776] d_loss: 1.38628709, g_loss: 0.69308484\n",
      "Step: [18777] d_loss: 1.38629246, g_loss: 0.69299656\n",
      "Step: [18778] d_loss: 1.38632989, g_loss: 0.69320953\n",
      "Step: [18779] d_loss: 1.38629460, g_loss: 0.69333148\n",
      "Step: [18780] d_loss: 1.38634849, g_loss: 0.69310880\n",
      "Step: [18781] d_loss: 1.38633239, g_loss: 0.69305658\n",
      "Step: [18782] d_loss: 1.38635457, g_loss: 0.69308698\n",
      "Step: [18783] d_loss: 1.38629436, g_loss: 0.69319713\n",
      "Step: [18784] d_loss: 1.38627911, g_loss: 0.69316858\n",
      "Step: [18785] d_loss: 1.38627934, g_loss: 0.69310170\n",
      "Step: [18786] d_loss: 1.38631916, g_loss: 0.69304609\n",
      "Step: [18787] d_loss: 1.38629532, g_loss: 0.69305182\n",
      "Step: [18788] d_loss: 1.38628459, g_loss: 0.69304192\n",
      "Step: [18789] d_loss: 1.38625026, g_loss: 0.69319034\n",
      "Step: [18790] d_loss: 1.38631225, g_loss: 0.69293261\n",
      "Step: [18791] d_loss: 1.38627148, g_loss: 0.69317353\n",
      "Step: [18792] d_loss: 1.38629460, g_loss: 0.69337428\n",
      "Step: [18793] d_loss: 1.38630056, g_loss: 0.69327474\n",
      "Step: [18794] d_loss: 1.38627052, g_loss: 0.69316232\n",
      "Step: [18795] d_loss: 1.38631165, g_loss: 0.69310492\n",
      "Step: [18796] d_loss: 1.38629484, g_loss: 0.69302732\n",
      "Step: [18797] d_loss: 1.38630152, g_loss: 0.69312537\n",
      "Step: [18798] d_loss: 1.38633204, g_loss: 0.69317633\n",
      "Step: [18799] d_loss: 1.38628578, g_loss: 0.69322062\n",
      "Step: [18800] d_loss: 1.38627160, g_loss: 0.69221205\n",
      "Step: [18801] d_loss: 1.38647890, g_loss: 0.69404000\n",
      "Step: [18802] d_loss: 1.38708138, g_loss: 0.69541961\n",
      "Step: [18803] d_loss: 1.38778627, g_loss: 0.69992375\n",
      "Step: [18804] d_loss: 1.38787198, g_loss: 0.69171000\n",
      "Step: [18805] d_loss: 1.38751924, g_loss: 0.68820632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18806] d_loss: 1.38713026, g_loss: 0.69076300\n",
      "Step: [18807] d_loss: 1.38673139, g_loss: 0.69175863\n",
      "Step: [18808] d_loss: 1.38645935, g_loss: 0.69400281\n",
      "Step: [18809] d_loss: 1.38636696, g_loss: 0.69400716\n",
      "Step: [18810] d_loss: 1.38632786, g_loss: 0.69354647\n",
      "Step: [18811] d_loss: 1.38629425, g_loss: 0.69293082\n",
      "Step: [18812] d_loss: 1.38631403, g_loss: 0.69249439\n",
      "Step: [18813] d_loss: 1.38629866, g_loss: 0.69300628\n",
      "Step: [18814] d_loss: 1.38629222, g_loss: 0.69315660\n",
      "Step: [18815] d_loss: 1.38629293, g_loss: 0.69330305\n",
      "Step: [18816] d_loss: 1.38630247, g_loss: 0.69301641\n",
      "Step: [18817] d_loss: 1.38631654, g_loss: 0.69322056\n",
      "Step: [18818] d_loss: 1.38627291, g_loss: 0.69350642\n",
      "Step: [18819] d_loss: 1.38627946, g_loss: 0.69321936\n",
      "Step: [18820] d_loss: 1.38628042, g_loss: 0.69367862\n",
      "Step: [18821] d_loss: 1.38651538, g_loss: 0.69254673\n",
      "Step: [18822] d_loss: 1.38639641, g_loss: 0.69205892\n",
      "Step: [18823] d_loss: 1.38646436, g_loss: 0.69342428\n",
      "Step: [18824] d_loss: 1.38647413, g_loss: 0.69438630\n",
      "Step: [18825] d_loss: 1.38642120, g_loss: 0.69511658\n",
      "Step: [18826] d_loss: 1.38637269, g_loss: 0.69392121\n",
      "Step: [18827] d_loss: 1.38632596, g_loss: 0.69202459\n",
      "Step: [18828] d_loss: 1.38630939, g_loss: 0.69179159\n",
      "Step: [18829] d_loss: 1.38629985, g_loss: 0.69258684\n",
      "Step: [18830] d_loss: 1.38629687, g_loss: 0.69313931\n",
      "Step: [18831] d_loss: 1.38629460, g_loss: 0.69343436\n",
      "Step: [18832] d_loss: 1.38628018, g_loss: 0.69344920\n",
      "Step: [18833] d_loss: 1.38629413, g_loss: 0.69308692\n",
      "Step: [18834] d_loss: 1.38627958, g_loss: 0.69285595\n",
      "Step: [18835] d_loss: 1.38631594, g_loss: 0.69302356\n",
      "Step: [18836] d_loss: 1.38627744, g_loss: 0.69319952\n",
      "Step: [18837] d_loss: 1.38627982, g_loss: 0.69328278\n",
      "Step: [18838] d_loss: 1.38625526, g_loss: 0.69333786\n",
      "Step: [18839] d_loss: 1.38627815, g_loss: 0.69318622\n",
      "Step: [18840] d_loss: 1.38626146, g_loss: 0.69306177\n",
      "Step: [18841] d_loss: 1.38628674, g_loss: 0.69310087\n",
      "Step: [18842] d_loss: 1.38625312, g_loss: 0.69321442\n",
      "Step: [18843] d_loss: 1.38628948, g_loss: 0.69318008\n",
      "Step: [18844] d_loss: 1.38623238, g_loss: 0.69306946\n",
      "Step: [18845] d_loss: 1.38657248, g_loss: 0.69331944\n",
      "Step: [18846] d_loss: 1.38740635, g_loss: 0.69476402\n",
      "Step: [18847] d_loss: 1.38815355, g_loss: 0.69119066\n",
      "Step: [18848] d_loss: 1.38797581, g_loss: 0.69018424\n",
      "Step: [18849] d_loss: 1.38731289, g_loss: 0.69197130\n",
      "Step: [18850] d_loss: 1.38680005, g_loss: 0.69581962\n",
      "Step: [18851] d_loss: 1.38643384, g_loss: 0.69647032\n",
      "Step: [18852] d_loss: 1.38632119, g_loss: 0.69338173\n",
      "Step: [18853] d_loss: 1.38627553, g_loss: 0.69228935\n",
      "Step: [18854] d_loss: 1.38627625, g_loss: 0.69184923\n",
      "Step: [18855] d_loss: 1.38630295, g_loss: 0.69243783\n",
      "Step: [18856] d_loss: 1.38626456, g_loss: 0.69385660\n",
      "Step: [18857] d_loss: 1.38628888, g_loss: 0.69440311\n",
      "Step: [18858] d_loss: 1.38628888, g_loss: 0.69356221\n",
      "Step: [18859] d_loss: 1.38626266, g_loss: 0.69278377\n",
      "Step: [18860] d_loss: 1.38626516, g_loss: 0.69302940\n",
      "Step: [18861] d_loss: 1.38624775, g_loss: 0.69299626\n",
      "Step: [18862] d_loss: 1.38628531, g_loss: 0.69363707\n",
      "Step: [18863] d_loss: 1.38633740, g_loss: 0.69386601\n",
      "Step: [18864] d_loss: 1.38643479, g_loss: 0.69487572\n",
      "Step: [18865] d_loss: 1.38626409, g_loss: 0.69321901\n",
      "Step: [18866] d_loss: 1.38631070, g_loss: 0.69280851\n",
      "Step: [18867] d_loss: 1.38632321, g_loss: 0.69320822\n",
      "Step: [18868] d_loss: 1.38629866, g_loss: 0.69338030\n",
      "Step: [18869] d_loss: 1.38632464, g_loss: 0.69267464\n",
      "Step: [18870] d_loss: 1.38624418, g_loss: 0.69280732\n",
      "Step: [18871] d_loss: 1.38652408, g_loss: 0.69366354\n",
      "Step: [18872] d_loss: 1.38676810, g_loss: 0.69229317\n",
      "Step: [18873] d_loss: 1.38691163, g_loss: 0.69541073\n",
      "Step: [18874] d_loss: 1.38673568, g_loss: 0.69168651\n",
      "Step: [18875] d_loss: 1.38655639, g_loss: 0.69142020\n",
      "Step: [18876] d_loss: 1.38636351, g_loss: 0.69336337\n",
      "Step: [18877] d_loss: 1.38631725, g_loss: 0.69394404\n",
      "Step: [18878] d_loss: 1.38631654, g_loss: 0.69438106\n",
      "Step: [18879] d_loss: 1.38622737, g_loss: 0.69387519\n",
      "Step: [18880] d_loss: 1.38629627, g_loss: 0.69260609\n",
      "Step: [18881] d_loss: 1.38626409, g_loss: 0.69310397\n",
      "Step: [18882] d_loss: 1.38627219, g_loss: 0.69310707\n",
      "Step: [18883] d_loss: 1.38628554, g_loss: 0.69293094\n",
      "Step: [18884] d_loss: 1.38631809, g_loss: 0.69347906\n",
      "Step: [18885] d_loss: 1.38627422, g_loss: 0.69345045\n",
      "Step: [18886] d_loss: 1.38630772, g_loss: 0.69308585\n",
      "Step: [18887] d_loss: 1.38629305, g_loss: 0.69289774\n",
      "Step: [18888] d_loss: 1.38625193, g_loss: 0.69358087\n",
      "Step: [18889] d_loss: 1.38631082, g_loss: 0.69308734\n",
      "Step: [18890] d_loss: 1.38630271, g_loss: 0.69311643\n",
      "Step: [18891] d_loss: 1.38631976, g_loss: 0.69264185\n",
      "Step: [18892] d_loss: 1.38627326, g_loss: 0.69352973\n",
      "Step: [18893] d_loss: 1.38630486, g_loss: 0.69322544\n",
      "Step: [18894] d_loss: 1.38626146, g_loss: 0.69334888\n",
      "Step: [18895] d_loss: 1.38623393, g_loss: 0.69364995\n",
      "Step: [18896] d_loss: 1.38625097, g_loss: 0.69382298\n",
      "Step: [18897] d_loss: 1.38635123, g_loss: 0.69304556\n",
      "Step: [18898] d_loss: 1.38630223, g_loss: 0.69362080\n",
      "Step: [18899] d_loss: 1.38633728, g_loss: 0.69309384\n",
      "Step: [18900] d_loss: 1.38630426, g_loss: 0.69291133\n",
      "Step: [18901] d_loss: 1.38641810, g_loss: 0.69297630\n",
      "Step: [18902] d_loss: 1.38632345, g_loss: 0.69288898\n",
      "Step: [18903] d_loss: 1.38628173, g_loss: 0.69314075\n",
      "Step: [18904] d_loss: 1.38626707, g_loss: 0.69330430\n",
      "Step: [18905] d_loss: 1.38624573, g_loss: 0.69333416\n",
      "Step: [18906] d_loss: 1.38629401, g_loss: 0.69345295\n",
      "Step: [18907] d_loss: 1.38630474, g_loss: 0.69309038\n",
      "Step: [18908] d_loss: 1.38626969, g_loss: 0.69341558\n",
      "Step: [18909] d_loss: 1.38630962, g_loss: 0.69339216\n",
      "Step: [18910] d_loss: 1.38630891, g_loss: 0.69303799\n",
      "Step: [18911] d_loss: 1.38624907, g_loss: 0.69313478\n",
      "Step: [18912] d_loss: 1.38635361, g_loss: 0.69352734\n",
      "Step: [18913] d_loss: 1.38632286, g_loss: 0.69316125\n",
      "Step: [18914] d_loss: 1.38635778, g_loss: 0.69316578\n",
      "Step: [18915] d_loss: 1.38629806, g_loss: 0.69285488\n",
      "Step: [18916] d_loss: 1.38631880, g_loss: 0.69318342\n",
      "Step: [18917] d_loss: 1.38628387, g_loss: 0.69335198\n",
      "Step: [18918] d_loss: 1.38628590, g_loss: 0.69315016\n",
      "Step: [18919] d_loss: 1.38631105, g_loss: 0.69300187\n",
      "Step: [18920] d_loss: 1.38629878, g_loss: 0.69336081\n",
      "Step: [18921] d_loss: 1.38633335, g_loss: 0.69331205\n",
      "Step: [18922] d_loss: 1.38626814, g_loss: 0.69311970\n",
      "Step: [18923] d_loss: 1.38625658, g_loss: 0.69286346\n",
      "Step: [18924] d_loss: 1.38641167, g_loss: 0.69356763\n",
      "Step: [18925] d_loss: 1.38667846, g_loss: 0.69258964\n",
      "Step: [18926] d_loss: 1.38676095, g_loss: 0.69431198\n",
      "Step: [18927] d_loss: 1.38670778, g_loss: 0.69295478\n",
      "Step: [18928] d_loss: 1.38654172, g_loss: 0.69149804\n",
      "Step: [18929] d_loss: 1.38636446, g_loss: 0.69206607\n",
      "Step: [18930] d_loss: 1.38638473, g_loss: 0.69416451\n",
      "Step: [18931] d_loss: 1.38636923, g_loss: 0.69392139\n",
      "Step: [18932] d_loss: 1.38645005, g_loss: 0.69326830\n",
      "Step: [18933] d_loss: 1.38654876, g_loss: 0.69148886\n",
      "Step: [18934] d_loss: 1.38653731, g_loss: 0.69103467\n",
      "Step: [18935] d_loss: 1.38643742, g_loss: 0.69131255\n",
      "Step: [18936] d_loss: 1.38633919, g_loss: 0.69293833\n",
      "Step: [18937] d_loss: 1.38630581, g_loss: 0.69384372\n",
      "Step: [18938] d_loss: 1.38623822, g_loss: 0.69384187\n",
      "Step: [18939] d_loss: 1.38626504, g_loss: 0.69400144\n",
      "Step: [18940] d_loss: 1.38625073, g_loss: 0.69323754\n",
      "Step: [18941] d_loss: 1.38623631, g_loss: 0.69319409\n",
      "Step: [18942] d_loss: 1.38628912, g_loss: 0.69270229\n",
      "Step: [18943] d_loss: 1.38628125, g_loss: 0.69366789\n",
      "Step: [18944] d_loss: 1.38631213, g_loss: 0.69399416\n",
      "Step: [18945] d_loss: 1.38617110, g_loss: 0.69266528\n",
      "Step: [18946] d_loss: 1.38628101, g_loss: 0.69402045\n",
      "Step: [18947] d_loss: 1.38664854, g_loss: 0.69649124\n",
      "Step: [18948] d_loss: 1.38767600, g_loss: 0.69374859\n",
      "Step: [18949] d_loss: 1.38985968, g_loss: 0.69602698\n",
      "Step: [18950] d_loss: 1.39014089, g_loss: 0.70592892\n",
      "Step: [18951] d_loss: 1.38891864, g_loss: 0.69340813\n",
      "Step: [18952] d_loss: 1.38770342, g_loss: 0.68654990\n",
      "Step: [18953] d_loss: 1.38661671, g_loss: 0.68623739\n",
      "Step: [18954] d_loss: 1.38639092, g_loss: 0.69249576\n",
      "Step: [18955] d_loss: 1.38627720, g_loss: 0.69671166\n",
      "Step: [18956] d_loss: 1.38630772, g_loss: 0.69634908\n",
      "Step: [18957] d_loss: 1.38629651, g_loss: 0.69410813\n",
      "Step: [18958] d_loss: 1.38630128, g_loss: 0.69179082\n",
      "Step: [18959] d_loss: 1.38629127, g_loss: 0.69198543\n",
      "Step: [18960] d_loss: 1.38625360, g_loss: 0.69247490\n",
      "Step: [18961] d_loss: 1.38630795, g_loss: 0.69381785\n",
      "Step: [18962] d_loss: 1.38629246, g_loss: 0.69503093\n",
      "Step: [18963] d_loss: 1.38626921, g_loss: 0.69428372\n",
      "Step: [18964] d_loss: 1.38628078, g_loss: 0.69272327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [18965] d_loss: 1.38626051, g_loss: 0.69248164\n",
      "Step: [18966] d_loss: 1.38628435, g_loss: 0.69281679\n",
      "Step: [18967] d_loss: 1.38627458, g_loss: 0.69351321\n",
      "Step: [18968] d_loss: 1.38626790, g_loss: 0.69327235\n",
      "Step: [18969] d_loss: 1.38629305, g_loss: 0.69333768\n",
      "Step: [18970] d_loss: 1.38623726, g_loss: 0.69331300\n",
      "Step: [18971] d_loss: 1.38626206, g_loss: 0.69344562\n",
      "Step: [18972] d_loss: 1.38622546, g_loss: 0.69290388\n",
      "Step: [18973] d_loss: 1.38627124, g_loss: 0.69301438\n",
      "Step: [18974] d_loss: 1.38635504, g_loss: 0.69332576\n",
      "Step: [18975] d_loss: 1.38629353, g_loss: 0.69320226\n",
      "Step: [18976] d_loss: 1.38630605, g_loss: 0.69308907\n",
      "Step: [18977] d_loss: 1.38632512, g_loss: 0.69310057\n",
      "Step: [18978] d_loss: 1.38621330, g_loss: 0.69311750\n",
      "Step: [18979] d_loss: 1.38624740, g_loss: 0.69318014\n",
      "Step: [18980] d_loss: 1.38627422, g_loss: 0.69319177\n",
      "Step: [18981] d_loss: 1.38627100, g_loss: 0.69323039\n",
      "Step: [18982] d_loss: 1.38629127, g_loss: 0.69331729\n",
      "Step: [18983] d_loss: 1.38623023, g_loss: 0.69323897\n",
      "Step: [18984] d_loss: 1.38637495, g_loss: 0.69307905\n",
      "Step: [18985] d_loss: 1.38622200, g_loss: 0.69301456\n",
      "Step: [18986] d_loss: 1.38629246, g_loss: 0.69320011\n",
      "Step: [18987] d_loss: 1.38620567, g_loss: 0.69332457\n",
      "Step: [18988] d_loss: 1.38621926, g_loss: 0.69306076\n",
      "Step: [18989] d_loss: 1.38631821, g_loss: 0.69316280\n",
      "Step: [18990] d_loss: 1.38627195, g_loss: 0.69313836\n",
      "Step: [18991] d_loss: 1.38624573, g_loss: 0.69327593\n",
      "Step: [18992] d_loss: 1.38627148, g_loss: 0.69339013\n",
      "Step: [18993] d_loss: 1.38627970, g_loss: 0.69307721\n",
      "Step: [18994] d_loss: 1.38631463, g_loss: 0.69342309\n",
      "Step: [18995] d_loss: 1.38642430, g_loss: 0.69417787\n",
      "Step: [18996] d_loss: 1.38666856, g_loss: 0.69522893\n",
      "Step: [18997] d_loss: 1.38657475, g_loss: 0.69465983\n",
      "Step: [18998] d_loss: 1.38651621, g_loss: 0.69454563\n",
      "Step: [18999] d_loss: 1.38634706, g_loss: 0.69367051\n",
      "Step: [19000] d_loss: 1.38628888, g_loss: 0.69262099\n",
      "Step: [19001] d_loss: 1.38630414, g_loss: 0.69240725\n",
      "Step: [19002] d_loss: 1.38629353, g_loss: 0.69293535\n",
      "Step: [19003] d_loss: 1.38631511, g_loss: 0.69331956\n",
      "Step: [19004] d_loss: 1.38629723, g_loss: 0.69316256\n",
      "Step: [19005] d_loss: 1.38634789, g_loss: 0.69320190\n",
      "Step: [19006] d_loss: 1.38630486, g_loss: 0.69316351\n",
      "Step: [19007] d_loss: 1.38626409, g_loss: 0.69318533\n",
      "Step: [19008] d_loss: 1.38652468, g_loss: 0.69380331\n",
      "Step: [19009] d_loss: 1.38628960, g_loss: 0.69330299\n",
      "Step: [19010] d_loss: 1.38637376, g_loss: 0.69291919\n",
      "Step: [19011] d_loss: 1.38633358, g_loss: 0.69274998\n",
      "Step: [19012] d_loss: 1.38634813, g_loss: 0.69327796\n",
      "Step: [19013] d_loss: 1.38628352, g_loss: 0.69356275\n",
      "Step: [19014] d_loss: 1.38633990, g_loss: 0.69335210\n",
      "Step: [19015] d_loss: 1.38631701, g_loss: 0.69303292\n",
      "Step: [19016] d_loss: 1.38628089, g_loss: 0.69297647\n",
      "Step: [19017] d_loss: 1.38732195, g_loss: 0.69291764\n",
      "Step: [19018] d_loss: 1.38630211, g_loss: 0.69467485\n",
      "Step: [19019] d_loss: 1.38629210, g_loss: 0.69362819\n",
      "Step: [19020] d_loss: 1.38619757, g_loss: 0.69256836\n",
      "Step: [19021] d_loss: 1.38630247, g_loss: 0.69280517\n",
      "Step: [19022] d_loss: 1.38647056, g_loss: 0.69440168\n",
      "Step: [19023] d_loss: 1.38635254, g_loss: 0.69368184\n",
      "Step: [19024] d_loss: 1.38654196, g_loss: 0.69250262\n",
      "Step: [19025] d_loss: 1.38676667, g_loss: 0.69066012\n",
      "Step: [19026] d_loss: 1.38666296, g_loss: 0.69000018\n",
      "Step: [19027] d_loss: 1.38646972, g_loss: 0.69155610\n",
      "Step: [19028] d_loss: 1.38631856, g_loss: 0.69289768\n",
      "Step: [19029] d_loss: 1.38625968, g_loss: 0.69607359\n",
      "Step: [19030] d_loss: 1.38624930, g_loss: 0.69574499\n",
      "Step: [19031] d_loss: 1.38639975, g_loss: 0.69123232\n",
      "Step: [19032] d_loss: 1.38642502, g_loss: 0.69126189\n",
      "Step: [19033] d_loss: 1.38633168, g_loss: 0.69336700\n",
      "Step: [19034] d_loss: 1.38653874, g_loss: 0.69436735\n",
      "Step: [19035] d_loss: 1.38671505, g_loss: 0.69568765\n",
      "Step: [19036] d_loss: 1.38662481, g_loss: 0.69429791\n",
      "Step: [19037] d_loss: 1.38642812, g_loss: 0.69228733\n",
      "Step: [19038] d_loss: 1.38637352, g_loss: 0.69167829\n",
      "Step: [19039] d_loss: 1.38630378, g_loss: 0.69163007\n",
      "Step: [19040] d_loss: 1.38625288, g_loss: 0.69432175\n",
      "Step: [19041] d_loss: 1.38632083, g_loss: 0.69508672\n",
      "Step: [19042] d_loss: 1.38626361, g_loss: 0.69385505\n",
      "Step: [19043] d_loss: 1.38631856, g_loss: 0.69247699\n",
      "Step: [19044] d_loss: 1.38629174, g_loss: 0.69275570\n",
      "Step: [19045] d_loss: 1.38628936, g_loss: 0.69277143\n",
      "Step: [19046] d_loss: 1.38631320, g_loss: 0.69320583\n",
      "Step: [19047] d_loss: 1.38626575, g_loss: 0.69328266\n",
      "Step: [19048] d_loss: 1.38626790, g_loss: 0.69378251\n",
      "Step: [19049] d_loss: 1.38623977, g_loss: 0.69345218\n",
      "Step: [19050] d_loss: 1.38653266, g_loss: 0.69408131\n",
      "Step: [19051] d_loss: 1.38657498, g_loss: 0.69217885\n",
      "Step: [19052] d_loss: 1.38659739, g_loss: 0.69171149\n",
      "Step: [19053] d_loss: 1.38650799, g_loss: 0.69375694\n",
      "Step: [19054] d_loss: 1.38690054, g_loss: 0.69697189\n",
      "Step: [19055] d_loss: 1.38811600, g_loss: 0.69502592\n",
      "Step: [19056] d_loss: 1.38870287, g_loss: 0.68477881\n",
      "Step: [19057] d_loss: 1.38768268, g_loss: 0.68464422\n",
      "Step: [19058] d_loss: 1.38683033, g_loss: 0.68723595\n",
      "Step: [19059] d_loss: 1.38634300, g_loss: 0.69590116\n",
      "Step: [19060] d_loss: 1.38721240, g_loss: 0.70044112\n",
      "Step: [19061] d_loss: 1.38799417, g_loss: 0.70028633\n",
      "Step: [19062] d_loss: 1.38748527, g_loss: 0.69730568\n",
      "Step: [19063] d_loss: 1.38662100, g_loss: 0.69666082\n",
      "Step: [19064] d_loss: 1.38627207, g_loss: 0.69361037\n",
      "Step: [19065] d_loss: 1.38627386, g_loss: 0.69203043\n",
      "Step: [19066] d_loss: 1.38607931, g_loss: 0.69198477\n",
      "Step: [19067] d_loss: 1.38652444, g_loss: 0.69165111\n",
      "Step: [19068] d_loss: 1.38677335, g_loss: 0.69327450\n",
      "Step: [19069] d_loss: 1.38662183, g_loss: 0.69378638\n",
      "Step: [19070] d_loss: 1.38642085, g_loss: 0.69612867\n",
      "Step: [19071] d_loss: 1.38618016, g_loss: 0.69546473\n",
      "Step: [19072] d_loss: 1.38650250, g_loss: 0.69323444\n",
      "Step: [19073] d_loss: 1.38660944, g_loss: 0.68693984\n",
      "Step: [19074] d_loss: 1.38676977, g_loss: 0.68842983\n",
      "Step: [19075] d_loss: 1.38685369, g_loss: 0.69230098\n",
      "Step: [19076] d_loss: 1.38644934, g_loss: 0.69735754\n",
      "Step: [19077] d_loss: 1.38626647, g_loss: 0.69888401\n",
      "Step: [19078] d_loss: 1.38625181, g_loss: 0.69601929\n",
      "Step: [19079] d_loss: 1.38636601, g_loss: 0.69365931\n",
      "Step: [19080] d_loss: 1.38617921, g_loss: 0.69124436\n",
      "Step: [19081] d_loss: 1.38618731, g_loss: 0.69258404\n",
      "Step: [19082] d_loss: 1.38640952, g_loss: 0.69369888\n",
      "Step: [19083] d_loss: 1.38664103, g_loss: 0.69473743\n",
      "Step: [19084] d_loss: 1.38708687, g_loss: 0.69397950\n",
      "Step: [19085] d_loss: 1.38735175, g_loss: 0.69652539\n",
      "Step: [19086] d_loss: 1.38723850, g_loss: 0.69730437\n",
      "Step: [19087] d_loss: 1.38664293, g_loss: 0.69466865\n",
      "Step: [19088] d_loss: 1.38634801, g_loss: 0.69214857\n",
      "Step: [19089] d_loss: 1.38629460, g_loss: 0.69095397\n",
      "Step: [19090] d_loss: 1.38649154, g_loss: 0.69266444\n",
      "Step: [19091] d_loss: 1.38626528, g_loss: 0.69350070\n",
      "Step: [19092] d_loss: 1.38624680, g_loss: 0.69335592\n",
      "Step: [19093] d_loss: 1.38614774, g_loss: 0.69367135\n",
      "Step: [19094] d_loss: 1.38625312, g_loss: 0.69351995\n",
      "Step: [19095] d_loss: 1.38624275, g_loss: 0.69303197\n",
      "Step: [19096] d_loss: 1.38615537, g_loss: 0.69328701\n",
      "Step: [19097] d_loss: 1.38624430, g_loss: 0.69337285\n",
      "Step: [19098] d_loss: 1.38621008, g_loss: 0.69340008\n",
      "Step: [19099] d_loss: 1.38639069, g_loss: 0.69295776\n",
      "Step: [19100] d_loss: 1.38632464, g_loss: 0.69289058\n",
      "Step: [19101] d_loss: 1.38626051, g_loss: 0.69453537\n",
      "Step: [19102] d_loss: 1.38654506, g_loss: 0.69339287\n",
      "Step: [19103] d_loss: 1.38648200, g_loss: 0.69480479\n",
      "Step: [19104] d_loss: 1.38628316, g_loss: 0.69367379\n",
      "Step: [19105] d_loss: 1.38641357, g_loss: 0.69376969\n",
      "Step: [19106] d_loss: 1.38647056, g_loss: 0.69329429\n",
      "Step: [19107] d_loss: 1.38642383, g_loss: 0.69287598\n",
      "Step: [19108] d_loss: 1.38681459, g_loss: 0.69199896\n",
      "Step: [19109] d_loss: 1.38629937, g_loss: 0.69264424\n",
      "Step: [19110] d_loss: 1.38624310, g_loss: 0.69336623\n",
      "Step: [19111] d_loss: 1.38620985, g_loss: 0.69314182\n",
      "Step: [19112] d_loss: 1.38616824, g_loss: 0.69274062\n",
      "Step: [19113] d_loss: 1.38644803, g_loss: 0.69338727\n",
      "Step: [19114] d_loss: 1.38664091, g_loss: 0.69096947\n",
      "Step: [19115] d_loss: 1.38682604, g_loss: 0.69398016\n",
      "Step: [19116] d_loss: 1.38642597, g_loss: 0.69481915\n",
      "Step: [19117] d_loss: 1.38629901, g_loss: 0.69439054\n",
      "Step: [19118] d_loss: 1.38661861, g_loss: 0.69189036\n",
      "Step: [19119] d_loss: 1.38739252, g_loss: 0.69251722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19120] d_loss: 1.38667655, g_loss: 0.69251311\n",
      "Step: [19121] d_loss: 1.38631248, g_loss: 0.69273788\n",
      "Step: [19122] d_loss: 1.38630044, g_loss: 0.69327700\n",
      "Step: [19123] d_loss: 1.38635790, g_loss: 0.69266760\n",
      "Step: [19124] d_loss: 1.38632786, g_loss: 0.69353354\n",
      "Step: [19125] d_loss: 1.38630307, g_loss: 0.69356430\n",
      "Step: [19126] d_loss: 1.38628888, g_loss: 0.69302922\n",
      "Step: [19127] d_loss: 1.38650119, g_loss: 0.69386923\n",
      "Step: [19128] d_loss: 1.38627172, g_loss: 0.69325459\n",
      "Step: [19129] d_loss: 1.38627315, g_loss: 0.69257867\n",
      "Step: [19130] d_loss: 1.38627589, g_loss: 0.69271612\n",
      "Step: [19131] d_loss: 1.38629961, g_loss: 0.69326627\n",
      "Step: [19132] d_loss: 1.38634682, g_loss: 0.69340730\n",
      "Step: [19133] d_loss: 1.38624823, g_loss: 0.69281101\n",
      "Step: [19134] d_loss: 1.38628709, g_loss: 0.69268429\n",
      "Step: [19135] d_loss: 1.38643026, g_loss: 0.69384032\n",
      "Step: [19136] d_loss: 1.38629365, g_loss: 0.69323206\n",
      "Step: [19137] d_loss: 1.38628459, g_loss: 0.69245887\n",
      "Step: [19138] d_loss: 1.38633800, g_loss: 0.69355309\n",
      "Step: [19139] d_loss: 1.38626873, g_loss: 0.69401360\n",
      "Step: [19140] d_loss: 1.38625050, g_loss: 0.69345379\n",
      "Step: [19141] d_loss: 1.38629556, g_loss: 0.69321030\n",
      "Step: [19142] d_loss: 1.38622355, g_loss: 0.69279844\n",
      "Step: [19143] d_loss: 1.38626099, g_loss: 0.69285965\n",
      "Step: [19144] d_loss: 1.38627887, g_loss: 0.69357288\n",
      "Step: [19145] d_loss: 1.38638735, g_loss: 0.69290197\n",
      "Step: [19146] d_loss: 1.38628840, g_loss: 0.69280553\n",
      "Step: [19147] d_loss: 1.38629591, g_loss: 0.69349825\n",
      "Step: [19148] d_loss: 1.38629007, g_loss: 0.69363654\n",
      "Step: [19149] d_loss: 1.38626158, g_loss: 0.69324410\n",
      "Step: [19150] d_loss: 1.38630855, g_loss: 0.69308126\n",
      "Step: [19151] d_loss: 1.38630700, g_loss: 0.69293678\n",
      "Step: [19152] d_loss: 1.38628006, g_loss: 0.69294840\n",
      "Step: [19153] d_loss: 1.38627410, g_loss: 0.69316614\n",
      "Step: [19154] d_loss: 1.38625860, g_loss: 0.69340348\n",
      "Step: [19155] d_loss: 1.38625729, g_loss: 0.69336915\n",
      "Step: [19156] d_loss: 1.38627958, g_loss: 0.69314229\n",
      "Step: [19157] d_loss: 1.38630092, g_loss: 0.69300807\n",
      "Step: [19158] d_loss: 1.38623357, g_loss: 0.69307047\n",
      "Step: [19159] d_loss: 1.38624907, g_loss: 0.69276077\n",
      "Step: [19160] d_loss: 1.38635671, g_loss: 0.69396460\n",
      "Step: [19161] d_loss: 1.38630188, g_loss: 0.69360524\n",
      "Step: [19162] d_loss: 1.38633609, g_loss: 0.69334412\n",
      "Step: [19163] d_loss: 1.38630474, g_loss: 0.69280958\n",
      "Step: [19164] d_loss: 1.38625264, g_loss: 0.69340348\n",
      "Step: [19165] d_loss: 1.38625634, g_loss: 0.69326127\n",
      "Step: [19166] d_loss: 1.38629937, g_loss: 0.69330072\n",
      "Step: [19167] d_loss: 1.38626885, g_loss: 0.69354630\n",
      "Step: [19168] d_loss: 1.38633323, g_loss: 0.69362020\n",
      "Step: [19169] d_loss: 1.38629413, g_loss: 0.69393831\n",
      "Step: [19170] d_loss: 1.38627243, g_loss: 0.69358826\n",
      "Step: [19171] d_loss: 1.38631845, g_loss: 0.69322568\n",
      "Step: [19172] d_loss: 1.38632953, g_loss: 0.69300348\n",
      "Step: [19173] d_loss: 1.38631988, g_loss: 0.69291878\n",
      "Step: [19174] d_loss: 1.38658428, g_loss: 0.69314408\n",
      "Step: [19175] d_loss: 1.38696373, g_loss: 0.69417202\n",
      "Step: [19176] d_loss: 1.38722312, g_loss: 0.69643354\n",
      "Step: [19177] d_loss: 1.38694847, g_loss: 0.69568837\n",
      "Step: [19178] d_loss: 1.38648999, g_loss: 0.69379216\n",
      "Step: [19179] d_loss: 1.38632226, g_loss: 0.69270313\n",
      "Step: [19180] d_loss: 1.38628519, g_loss: 0.69262481\n",
      "Step: [19181] d_loss: 1.38627505, g_loss: 0.69301510\n",
      "Step: [19182] d_loss: 1.38625002, g_loss: 0.69348979\n",
      "Step: [19183] d_loss: 1.38644111, g_loss: 0.69293845\n",
      "Step: [19184] d_loss: 1.38634706, g_loss: 0.69322884\n",
      "Step: [19185] d_loss: 1.38655710, g_loss: 0.69208878\n",
      "Step: [19186] d_loss: 1.38654470, g_loss: 0.69257766\n",
      "Step: [19187] d_loss: 1.38646019, g_loss: 0.69273496\n",
      "Step: [19188] d_loss: 1.38638425, g_loss: 0.69349933\n",
      "Step: [19189] d_loss: 1.38625968, g_loss: 0.69402742\n",
      "Step: [19190] d_loss: 1.38625789, g_loss: 0.69314861\n",
      "Step: [19191] d_loss: 1.38625193, g_loss: 0.69258130\n",
      "Step: [19192] d_loss: 1.38630986, g_loss: 0.69170475\n",
      "Step: [19193] d_loss: 1.38631940, g_loss: 0.69302505\n",
      "Step: [19194] d_loss: 1.38644540, g_loss: 0.69257575\n",
      "Step: [19195] d_loss: 1.38644528, g_loss: 0.69199806\n",
      "Step: [19196] d_loss: 1.38639176, g_loss: 0.69287884\n",
      "Step: [19197] d_loss: 1.38642263, g_loss: 0.69495761\n",
      "Step: [19198] d_loss: 1.38642073, g_loss: 0.69368076\n",
      "Step: [19199] d_loss: 1.38641024, g_loss: 0.69274855\n",
      "Step: [19200] d_loss: 1.38642597, g_loss: 0.69253421\n",
      "Step: [19201] d_loss: 1.38636148, g_loss: 0.69346964\n",
      "Step: [19202] d_loss: 1.38634539, g_loss: 0.69339943\n",
      "Step: [19203] d_loss: 1.38718772, g_loss: 0.69355190\n",
      "Step: [19204] d_loss: 1.38627422, g_loss: 0.69390869\n",
      "Step: [19205] d_loss: 1.38629043, g_loss: 0.69307488\n",
      "Step: [19206] d_loss: 1.38625288, g_loss: 0.69270855\n",
      "Step: [19207] d_loss: 1.38629997, g_loss: 0.69312656\n",
      "Step: [19208] d_loss: 1.38628888, g_loss: 0.69314194\n",
      "Step: [19209] d_loss: 1.38628817, g_loss: 0.69317400\n",
      "Step: [19210] d_loss: 1.38629997, g_loss: 0.69311029\n",
      "Step: [19211] d_loss: 1.38630962, g_loss: 0.69293952\n",
      "Step: [19212] d_loss: 1.38630199, g_loss: 0.69333184\n",
      "Step: [19213] d_loss: 1.38629794, g_loss: 0.69326049\n",
      "Step: [19214] d_loss: 1.38628209, g_loss: 0.69294786\n",
      "Step: [19215] d_loss: 1.38628066, g_loss: 0.69285792\n",
      "Step: [19216] d_loss: 1.38650227, g_loss: 0.69258118\n",
      "Step: [19217] d_loss: 1.38627911, g_loss: 0.69341451\n",
      "Step: [19218] d_loss: 1.38630736, g_loss: 0.69305724\n",
      "Step: [19219] d_loss: 1.38630986, g_loss: 0.69316930\n",
      "Step: [19220] d_loss: 1.38630950, g_loss: 0.69290912\n",
      "Step: [19221] d_loss: 1.38630366, g_loss: 0.69305301\n",
      "Step: [19222] d_loss: 1.38629186, g_loss: 0.69260889\n",
      "Step: [19223] d_loss: 1.38633144, g_loss: 0.69316196\n",
      "Step: [19224] d_loss: 1.38629568, g_loss: 0.69350183\n",
      "Step: [19225] d_loss: 1.38634443, g_loss: 0.69320011\n",
      "Step: [19226] d_loss: 1.38630700, g_loss: 0.69308293\n",
      "Step: [19227] d_loss: 1.38628531, g_loss: 0.69283301\n",
      "Step: [19228] d_loss: 1.38629138, g_loss: 0.69305873\n",
      "Step: [19229] d_loss: 1.38628745, g_loss: 0.69299066\n",
      "Step: [19230] d_loss: 1.38632643, g_loss: 0.69311643\n",
      "Step: [19231] d_loss: 1.38629425, g_loss: 0.69296598\n",
      "Step: [19232] d_loss: 1.38628554, g_loss: 0.69314396\n",
      "Step: [19233] d_loss: 1.38629270, g_loss: 0.69327003\n",
      "Step: [19234] d_loss: 1.38626373, g_loss: 0.69302177\n",
      "Step: [19235] d_loss: 1.38630414, g_loss: 0.69328958\n",
      "Step: [19236] d_loss: 1.38629174, g_loss: 0.69328290\n",
      "Step: [19237] d_loss: 1.38627636, g_loss: 0.69302732\n",
      "Step: [19238] d_loss: 1.38629913, g_loss: 0.69298792\n",
      "Step: [19239] d_loss: 1.38627613, g_loss: 0.69307739\n",
      "Step: [19240] d_loss: 1.38630474, g_loss: 0.69323218\n",
      "Step: [19241] d_loss: 1.38628745, g_loss: 0.69322044\n",
      "Step: [19242] d_loss: 1.38627148, g_loss: 0.69316006\n",
      "Step: [19243] d_loss: 1.38628542, g_loss: 0.69310421\n",
      "Step: [19244] d_loss: 1.38628948, g_loss: 0.69306767\n",
      "Step: [19245] d_loss: 1.38629055, g_loss: 0.69311392\n",
      "Step: [19246] d_loss: 1.38628006, g_loss: 0.69316602\n",
      "Step: [19247] d_loss: 1.38627672, g_loss: 0.69321519\n",
      "Step: [19248] d_loss: 1.38632250, g_loss: 0.69323373\n",
      "Step: [19249] d_loss: 1.38631797, g_loss: 0.69333494\n",
      "Step: [19250] d_loss: 1.38636541, g_loss: 0.69360876\n",
      "Step: [19251] d_loss: 1.38638771, g_loss: 0.69364315\n",
      "Step: [19252] d_loss: 1.38631725, g_loss: 0.69318175\n",
      "Step: [19253] d_loss: 1.38631582, g_loss: 0.69318020\n",
      "Step: [19254] d_loss: 1.38629985, g_loss: 0.69310236\n",
      "Step: [19255] d_loss: 1.38623047, g_loss: 0.69287169\n",
      "Step: [19256] d_loss: 1.38629341, g_loss: 0.69330484\n",
      "Step: [19257] d_loss: 1.38629019, g_loss: 0.69301379\n",
      "Step: [19258] d_loss: 1.38627744, g_loss: 0.69305766\n",
      "Step: [19259] d_loss: 1.38626528, g_loss: 0.69327116\n",
      "Step: [19260] d_loss: 1.38629866, g_loss: 0.69322979\n",
      "Step: [19261] d_loss: 1.38627565, g_loss: 0.69317770\n",
      "Step: [19262] d_loss: 1.38625717, g_loss: 0.69315261\n",
      "Step: [19263] d_loss: 1.38629270, g_loss: 0.69315004\n",
      "Step: [19264] d_loss: 1.38626719, g_loss: 0.69319153\n",
      "Step: [19265] d_loss: 1.38625276, g_loss: 0.69287264\n",
      "Step: [19266] d_loss: 1.38628221, g_loss: 0.69313604\n",
      "Step: [19267] d_loss: 1.38628280, g_loss: 0.69343841\n",
      "Step: [19268] d_loss: 1.38629317, g_loss: 0.69320607\n",
      "Step: [19269] d_loss: 1.38629413, g_loss: 0.69330680\n",
      "Step: [19270] d_loss: 1.38628185, g_loss: 0.69307542\n",
      "Step: [19271] d_loss: 1.38631618, g_loss: 0.69294226\n",
      "Step: [19272] d_loss: 1.38629055, g_loss: 0.69335103\n",
      "Step: [19273] d_loss: 1.38627815, g_loss: 0.69298124\n",
      "Step: [19274] d_loss: 1.38629043, g_loss: 0.69296962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19275] d_loss: 1.38629878, g_loss: 0.69297326\n",
      "Step: [19276] d_loss: 1.38628674, g_loss: 0.69344181\n",
      "Step: [19277] d_loss: 1.38628697, g_loss: 0.69302464\n",
      "Step: [19278] d_loss: 1.38628972, g_loss: 0.69324899\n",
      "Step: [19279] d_loss: 1.38624728, g_loss: 0.69298893\n",
      "Step: [19280] d_loss: 1.38629460, g_loss: 0.69293314\n",
      "Step: [19281] d_loss: 1.38628411, g_loss: 0.69343436\n",
      "Step: [19282] d_loss: 1.38628149, g_loss: 0.69327259\n",
      "Step: [19283] d_loss: 1.38634205, g_loss: 0.69340163\n",
      "Step: [19284] d_loss: 1.38623142, g_loss: 0.69336963\n",
      "Step: [19285] d_loss: 1.38628173, g_loss: 0.69317949\n",
      "Step: [19286] d_loss: 1.38629842, g_loss: 0.69300961\n",
      "Step: [19287] d_loss: 1.38630867, g_loss: 0.69362777\n",
      "Step: [19288] d_loss: 1.38629413, g_loss: 0.69353437\n",
      "Step: [19289] d_loss: 1.38618386, g_loss: 0.69365323\n",
      "Step: [19290] d_loss: 1.38623857, g_loss: 0.69313198\n",
      "Step: [19291] d_loss: 1.38623023, g_loss: 0.69267398\n",
      "Step: [19292] d_loss: 1.38621509, g_loss: 0.69307709\n",
      "Step: [19293] d_loss: 1.38637984, g_loss: 0.69353569\n",
      "Step: [19294] d_loss: 1.38633621, g_loss: 0.69312370\n",
      "Step: [19295] d_loss: 1.38628602, g_loss: 0.69248390\n",
      "Step: [19296] d_loss: 1.38633370, g_loss: 0.69274873\n",
      "Step: [19297] d_loss: 1.38634670, g_loss: 0.69291085\n",
      "Step: [19298] d_loss: 1.38644028, g_loss: 0.69344324\n",
      "Step: [19299] d_loss: 1.38624144, g_loss: 0.69384331\n",
      "Step: [19300] d_loss: 1.38624191, g_loss: 0.69367814\n",
      "Step: [19301] d_loss: 1.38627791, g_loss: 0.69352603\n",
      "Step: [19302] d_loss: 1.38623750, g_loss: 0.69319677\n",
      "Step: [19303] d_loss: 1.38625741, g_loss: 0.69208533\n",
      "Step: [19304] d_loss: 1.38631082, g_loss: 0.69309998\n",
      "Step: [19305] d_loss: 1.38653755, g_loss: 0.69336313\n",
      "Step: [19306] d_loss: 1.38660061, g_loss: 0.69403237\n",
      "Step: [19307] d_loss: 1.38659477, g_loss: 0.69404960\n",
      "Step: [19308] d_loss: 1.38641059, g_loss: 0.69376153\n",
      "Step: [19309] d_loss: 1.38652086, g_loss: 0.69347262\n",
      "Step: [19310] d_loss: 1.38643098, g_loss: 0.69323599\n",
      "Step: [19311] d_loss: 1.38629842, g_loss: 0.69313741\n",
      "Step: [19312] d_loss: 1.38630104, g_loss: 0.69310713\n",
      "Step: [19313] d_loss: 1.38626349, g_loss: 0.69315374\n",
      "Step: [19314] d_loss: 1.38639271, g_loss: 0.69294429\n",
      "Step: [19315] d_loss: 1.38640952, g_loss: 0.69274652\n",
      "Step: [19316] d_loss: 1.38639474, g_loss: 0.69328618\n",
      "Step: [19317] d_loss: 1.38620508, g_loss: 0.69302362\n",
      "Step: [19318] d_loss: 1.38628340, g_loss: 0.69309515\n",
      "Step: [19319] d_loss: 1.38635123, g_loss: 0.69293702\n",
      "Step: [19320] d_loss: 1.38627315, g_loss: 0.69327956\n",
      "Step: [19321] d_loss: 1.38628697, g_loss: 0.69328815\n",
      "Step: [19322] d_loss: 1.38626766, g_loss: 0.69307464\n",
      "Step: [19323] d_loss: 1.38623333, g_loss: 0.69353259\n",
      "Step: [19324] d_loss: 1.38625026, g_loss: 0.69239569\n",
      "Step: [19325] d_loss: 1.38594234, g_loss: 0.69147366\n",
      "Step: [19326] d_loss: 1.38631868, g_loss: 0.69278252\n",
      "Step: [19327] d_loss: 1.38637078, g_loss: 0.69322610\n",
      "Step: [19328] d_loss: 1.38632333, g_loss: 0.69402122\n",
      "Step: [19329] d_loss: 1.38633072, g_loss: 0.69313693\n",
      "Step: [19330] d_loss: 1.38628733, g_loss: 0.69263685\n",
      "Step: [19331] d_loss: 1.38616860, g_loss: 0.69231153\n",
      "Step: [19332] d_loss: 1.38628554, g_loss: 0.69318318\n",
      "Step: [19333] d_loss: 1.38630748, g_loss: 0.69314367\n",
      "Step: [19334] d_loss: 1.38627100, g_loss: 0.69328761\n",
      "Step: [19335] d_loss: 1.38635635, g_loss: 0.69289064\n",
      "Step: [19336] d_loss: 1.38626671, g_loss: 0.69303989\n",
      "Step: [19337] d_loss: 1.38640952, g_loss: 0.69397306\n",
      "Step: [19338] d_loss: 1.38655400, g_loss: 0.69252563\n",
      "Step: [19339] d_loss: 1.38655186, g_loss: 0.69240886\n",
      "Step: [19340] d_loss: 1.38647950, g_loss: 0.69193560\n",
      "Step: [19341] d_loss: 1.38645101, g_loss: 0.69305778\n",
      "Step: [19342] d_loss: 1.38641953, g_loss: 0.69305080\n",
      "Step: [19343] d_loss: 1.38643432, g_loss: 0.69341910\n",
      "Step: [19344] d_loss: 1.38637137, g_loss: 0.69323933\n",
      "Step: [19345] d_loss: 1.38631546, g_loss: 0.69311988\n",
      "Step: [19346] d_loss: 1.38628650, g_loss: 0.69328809\n",
      "Step: [19347] d_loss: 1.38634360, g_loss: 0.69319689\n",
      "Step: [19348] d_loss: 1.38628078, g_loss: 0.69296843\n",
      "Step: [19349] d_loss: 1.38632143, g_loss: 0.69316828\n",
      "Step: [19350] d_loss: 1.38630986, g_loss: 0.69336307\n",
      "Step: [19351] d_loss: 1.38630915, g_loss: 0.69323635\n",
      "Step: [19352] d_loss: 1.38627696, g_loss: 0.69306636\n",
      "Step: [19353] d_loss: 1.38624799, g_loss: 0.69303089\n",
      "Step: [19354] d_loss: 1.38633490, g_loss: 0.69288415\n",
      "Step: [19355] d_loss: 1.38625407, g_loss: 0.69325519\n",
      "Step: [19356] d_loss: 1.38629675, g_loss: 0.69342005\n",
      "Step: [19357] d_loss: 1.38631535, g_loss: 0.69301635\n",
      "Step: [19358] d_loss: 1.38631129, g_loss: 0.69307119\n",
      "Step: [19359] d_loss: 1.38624048, g_loss: 0.69308323\n",
      "Step: [19360] d_loss: 1.38631439, g_loss: 0.69311535\n",
      "Step: [19361] d_loss: 1.38628817, g_loss: 0.69312119\n",
      "Step: [19362] d_loss: 1.38626993, g_loss: 0.69330144\n",
      "Step: [19363] d_loss: 1.38629079, g_loss: 0.69336593\n",
      "Step: [19364] d_loss: 1.38629746, g_loss: 0.69316161\n",
      "Step: [19365] d_loss: 1.38629436, g_loss: 0.69303775\n",
      "Step: [19366] d_loss: 1.38640976, g_loss: 0.69306791\n",
      "Step: [19367] d_loss: 1.38626027, g_loss: 0.69319677\n",
      "Step: [19368] d_loss: 1.38628531, g_loss: 0.69316596\n",
      "Step: [19369] d_loss: 1.38634372, g_loss: 0.69332111\n",
      "Step: [19370] d_loss: 1.38627005, g_loss: 0.69318819\n",
      "Step: [19371] d_loss: 1.38628268, g_loss: 0.69313502\n",
      "Step: [19372] d_loss: 1.38631606, g_loss: 0.69355631\n",
      "Step: [19373] d_loss: 1.38656020, g_loss: 0.69289136\n",
      "Step: [19374] d_loss: 1.38697481, g_loss: 0.69571447\n",
      "Step: [19375] d_loss: 1.38722181, g_loss: 0.69672287\n",
      "Step: [19376] d_loss: 1.38709927, g_loss: 0.69426775\n",
      "Step: [19377] d_loss: 1.38701141, g_loss: 0.69294292\n",
      "Step: [19378] d_loss: 1.38681030, g_loss: 0.69278979\n",
      "Step: [19379] d_loss: 1.38658047, g_loss: 0.69244766\n",
      "Step: [19380] d_loss: 1.38650990, g_loss: 0.69330776\n",
      "Step: [19381] d_loss: 1.38640594, g_loss: 0.69269568\n",
      "Step: [19382] d_loss: 1.38642836, g_loss: 0.69335645\n",
      "Step: [19383] d_loss: 1.38630581, g_loss: 0.69350475\n",
      "Step: [19384] d_loss: 1.38631642, g_loss: 0.69223166\n",
      "Step: [19385] d_loss: 1.38679779, g_loss: 0.69179595\n",
      "Step: [19386] d_loss: 1.38633823, g_loss: 0.69233918\n",
      "Step: [19387] d_loss: 1.38627994, g_loss: 0.69482923\n",
      "Step: [19388] d_loss: 1.38629174, g_loss: 0.69473672\n",
      "Step: [19389] d_loss: 1.38632822, g_loss: 0.69410646\n",
      "Step: [19390] d_loss: 1.38620687, g_loss: 0.69353735\n",
      "Step: [19391] d_loss: 1.38633776, g_loss: 0.69294506\n",
      "Step: [19392] d_loss: 1.38629937, g_loss: 0.69192582\n",
      "Step: [19393] d_loss: 1.38661075, g_loss: 0.69387627\n",
      "Step: [19394] d_loss: 1.38674021, g_loss: 0.69489819\n",
      "Step: [19395] d_loss: 1.38672686, g_loss: 0.69516873\n",
      "Step: [19396] d_loss: 1.38666141, g_loss: 0.69362348\n",
      "Step: [19397] d_loss: 1.38649082, g_loss: 0.69241512\n",
      "Step: [19398] d_loss: 1.38674521, g_loss: 0.69043338\n",
      "Step: [19399] d_loss: 1.38635075, g_loss: 0.69179308\n",
      "Step: [19400] d_loss: 1.38632643, g_loss: 0.69352317\n",
      "Step: [19401] d_loss: 1.38633633, g_loss: 0.69447207\n",
      "Step: [19402] d_loss: 1.38662946, g_loss: 0.69308937\n",
      "Step: [19403] d_loss: 1.38684106, g_loss: 0.68977576\n",
      "Step: [19404] d_loss: 1.38729632, g_loss: 0.69396448\n",
      "Step: [19405] d_loss: 1.38745260, g_loss: 0.69672394\n",
      "Step: [19406] d_loss: 1.38737869, g_loss: 0.69772589\n",
      "Step: [19407] d_loss: 1.38697851, g_loss: 0.69123614\n",
      "Step: [19408] d_loss: 1.38672113, g_loss: 0.69256186\n",
      "Step: [19409] d_loss: 1.38648534, g_loss: 0.69309020\n",
      "Step: [19410] d_loss: 1.38646555, g_loss: 0.69347990\n",
      "Step: [19411] d_loss: 1.38670218, g_loss: 0.69479531\n",
      "Step: [19412] d_loss: 1.38694692, g_loss: 0.69390178\n",
      "Step: [19413] d_loss: 1.38673496, g_loss: 0.69364721\n",
      "Step: [19414] d_loss: 1.38654470, g_loss: 0.69628334\n",
      "Step: [19415] d_loss: 1.38638949, g_loss: 0.69554871\n",
      "Step: [19416] d_loss: 1.38643026, g_loss: 0.69463617\n",
      "Step: [19417] d_loss: 1.38703704, g_loss: 0.69517469\n",
      "Step: [19418] d_loss: 1.38731956, g_loss: 0.69229341\n",
      "Step: [19419] d_loss: 1.38714957, g_loss: 0.68822575\n",
      "Step: [19420] d_loss: 1.38672400, g_loss: 0.68933654\n",
      "Step: [19421] d_loss: 1.38647842, g_loss: 0.69264400\n",
      "Step: [19422] d_loss: 1.38627553, g_loss: 0.69515479\n",
      "Step: [19423] d_loss: 1.38629150, g_loss: 0.69476175\n",
      "Step: [19424] d_loss: 1.38638282, g_loss: 0.69166636\n",
      "Step: [19425] d_loss: 1.38664150, g_loss: 0.69183588\n",
      "Step: [19426] d_loss: 1.38680065, g_loss: 0.69371915\n",
      "Step: [19427] d_loss: 1.38650680, g_loss: 0.69580615\n",
      "Step: [19428] d_loss: 1.38635850, g_loss: 0.69356441\n",
      "Step: [19429] d_loss: 1.38641810, g_loss: 0.69238079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19430] d_loss: 1.38651586, g_loss: 0.69288361\n",
      "Step: [19431] d_loss: 1.38666725, g_loss: 0.69181287\n",
      "Step: [19432] d_loss: 1.38624966, g_loss: 0.69182634\n",
      "Step: [19433] d_loss: 1.38629842, g_loss: 0.69408238\n",
      "Step: [19434] d_loss: 1.38629568, g_loss: 0.69457626\n",
      "Step: [19435] d_loss: 1.38624072, g_loss: 0.69380128\n",
      "Step: [19436] d_loss: 1.38629270, g_loss: 0.69291568\n",
      "Step: [19437] d_loss: 1.38623548, g_loss: 0.69264913\n",
      "Step: [19438] d_loss: 1.38624549, g_loss: 0.69257361\n",
      "Step: [19439] d_loss: 1.38640833, g_loss: 0.69248760\n",
      "Step: [19440] d_loss: 1.38635623, g_loss: 0.69230473\n",
      "Step: [19441] d_loss: 1.38645101, g_loss: 0.69248629\n",
      "Step: [19442] d_loss: 1.38662362, g_loss: 0.69374979\n",
      "Step: [19443] d_loss: 1.38655615, g_loss: 0.69446278\n",
      "Step: [19444] d_loss: 1.38655412, g_loss: 0.69476914\n",
      "Step: [19445] d_loss: 1.38636887, g_loss: 0.69432408\n",
      "Step: [19446] d_loss: 1.38633156, g_loss: 0.69292903\n",
      "Step: [19447] d_loss: 1.38627446, g_loss: 0.69176948\n",
      "Step: [19448] d_loss: 1.38634074, g_loss: 0.69209713\n",
      "Step: [19449] d_loss: 1.38627815, g_loss: 0.69318968\n",
      "Step: [19450] d_loss: 1.38626528, g_loss: 0.69362003\n",
      "Step: [19451] d_loss: 1.38619447, g_loss: 0.69250840\n",
      "Step: [19452] d_loss: 1.38632703, g_loss: 0.69293976\n",
      "Step: [19453] d_loss: 1.38620305, g_loss: 0.69363260\n",
      "Step: [19454] d_loss: 1.38632941, g_loss: 0.69214863\n",
      "Step: [19455] d_loss: 1.38649094, g_loss: 0.69169396\n",
      "Step: [19456] d_loss: 1.38644707, g_loss: 0.69320977\n",
      "Step: [19457] d_loss: 1.38639498, g_loss: 0.69427818\n",
      "Step: [19458] d_loss: 1.38633227, g_loss: 0.69337654\n",
      "Step: [19459] d_loss: 1.38623297, g_loss: 0.69277090\n",
      "Step: [19460] d_loss: 1.38623238, g_loss: 0.69273680\n",
      "Step: [19461] d_loss: 1.38627911, g_loss: 0.69312197\n",
      "Step: [19462] d_loss: 1.38620901, g_loss: 0.69356596\n",
      "Step: [19463] d_loss: 1.38626218, g_loss: 0.69328493\n",
      "Step: [19464] d_loss: 1.38621247, g_loss: 0.69310361\n",
      "Step: [19465] d_loss: 1.38632119, g_loss: 0.69310856\n",
      "Step: [19466] d_loss: 1.38628042, g_loss: 0.69318217\n",
      "Step: [19467] d_loss: 1.38628912, g_loss: 0.69337404\n",
      "Step: [19468] d_loss: 1.38627541, g_loss: 0.69327462\n",
      "Step: [19469] d_loss: 1.38625097, g_loss: 0.69320226\n",
      "Step: [19470] d_loss: 1.38626027, g_loss: 0.69324046\n",
      "Step: [19471] d_loss: 1.38625419, g_loss: 0.69328809\n",
      "Step: [19472] d_loss: 1.38623476, g_loss: 0.69297385\n",
      "Step: [19473] d_loss: 1.38659561, g_loss: 0.69465727\n",
      "Step: [19474] d_loss: 1.38708436, g_loss: 0.69796419\n",
      "Step: [19475] d_loss: 1.38713312, g_loss: 0.69513893\n",
      "Step: [19476] d_loss: 1.38682175, g_loss: 0.69133329\n",
      "Step: [19477] d_loss: 1.38653016, g_loss: 0.69197118\n",
      "Step: [19478] d_loss: 1.38635266, g_loss: 0.69473124\n",
      "Step: [19479] d_loss: 1.38629889, g_loss: 0.69472706\n",
      "Step: [19480] d_loss: 1.38627768, g_loss: 0.69385195\n",
      "Step: [19481] d_loss: 1.38625097, g_loss: 0.69254190\n",
      "Step: [19482] d_loss: 1.38627577, g_loss: 0.69229007\n",
      "Step: [19483] d_loss: 1.38628292, g_loss: 0.69294363\n",
      "Step: [19484] d_loss: 1.38627088, g_loss: 0.69314718\n",
      "Step: [19485] d_loss: 1.38630033, g_loss: 0.69335032\n",
      "Step: [19486] d_loss: 1.38672471, g_loss: 0.69314742\n",
      "Step: [19487] d_loss: 1.38683212, g_loss: 0.69564581\n",
      "Step: [19488] d_loss: 1.38675869, g_loss: 0.69541836\n",
      "Step: [19489] d_loss: 1.38652658, g_loss: 0.69321913\n",
      "Step: [19490] d_loss: 1.38647616, g_loss: 0.69029915\n",
      "Step: [19491] d_loss: 1.38638496, g_loss: 0.68967342\n",
      "Step: [19492] d_loss: 1.38639462, g_loss: 0.69206834\n",
      "Step: [19493] d_loss: 1.38633406, g_loss: 0.69366837\n",
      "Step: [19494] d_loss: 1.38629484, g_loss: 0.69416428\n",
      "Step: [19495] d_loss: 1.38628948, g_loss: 0.69397789\n",
      "Step: [19496] d_loss: 1.38629198, g_loss: 0.69340336\n",
      "Step: [19497] d_loss: 1.38632143, g_loss: 0.69304192\n",
      "Step: [19498] d_loss: 1.38633180, g_loss: 0.69257969\n",
      "Step: [19499] d_loss: 1.38631213, g_loss: 0.69276023\n",
      "Step: [19500] d_loss: 1.38630533, g_loss: 0.69324684\n",
      "Step: [19501] d_loss: 1.38629866, g_loss: 0.69334924\n",
      "Step: [19502] d_loss: 1.38630903, g_loss: 0.69287312\n",
      "Step: [19503] d_loss: 1.38631344, g_loss: 0.69309378\n",
      "Step: [19504] d_loss: 1.38631010, g_loss: 0.69279850\n",
      "Step: [19505] d_loss: 1.38631737, g_loss: 0.69317442\n",
      "Step: [19506] d_loss: 1.38627839, g_loss: 0.69335413\n",
      "Step: [19507] d_loss: 1.38631678, g_loss: 0.69283926\n",
      "Step: [19508] d_loss: 1.38628161, g_loss: 0.69320291\n",
      "Step: [19509] d_loss: 1.38634872, g_loss: 0.69322610\n",
      "Step: [19510] d_loss: 1.38633060, g_loss: 0.69313592\n",
      "Step: [19511] d_loss: 1.38631392, g_loss: 0.69286150\n",
      "Step: [19512] d_loss: 1.38632345, g_loss: 0.69330192\n",
      "Step: [19513] d_loss: 1.38631153, g_loss: 0.69324172\n",
      "Step: [19514] d_loss: 1.38631439, g_loss: 0.69296694\n",
      "Step: [19515] d_loss: 1.38632751, g_loss: 0.69315410\n",
      "Step: [19516] d_loss: 1.38630033, g_loss: 0.69314331\n",
      "Step: [19517] d_loss: 1.38630378, g_loss: 0.69313431\n",
      "Step: [19518] d_loss: 1.38631213, g_loss: 0.69264376\n",
      "Step: [19519] d_loss: 1.38629758, g_loss: 0.69299906\n",
      "Step: [19520] d_loss: 1.38630557, g_loss: 0.69340700\n",
      "Step: [19521] d_loss: 1.38631606, g_loss: 0.69320834\n",
      "Step: [19522] d_loss: 1.38630915, g_loss: 0.69300079\n",
      "Step: [19523] d_loss: 1.38630939, g_loss: 0.69284177\n",
      "Step: [19524] d_loss: 1.38633621, g_loss: 0.69303060\n",
      "Step: [19525] d_loss: 1.38628197, g_loss: 0.69339716\n",
      "Step: [19526] d_loss: 1.38627076, g_loss: 0.69315219\n",
      "Step: [19527] d_loss: 1.38630581, g_loss: 0.69255674\n",
      "Step: [19528] d_loss: 1.38631022, g_loss: 0.69306976\n",
      "Step: [19529] d_loss: 1.38628268, g_loss: 0.69348598\n",
      "Step: [19530] d_loss: 1.38627768, g_loss: 0.69315529\n",
      "Step: [19531] d_loss: 1.38628578, g_loss: 0.69350350\n",
      "Step: [19532] d_loss: 1.38626850, g_loss: 0.69295144\n",
      "Step: [19533] d_loss: 1.38631237, g_loss: 0.69303972\n",
      "Step: [19534] d_loss: 1.38631845, g_loss: 0.69290036\n",
      "Step: [19535] d_loss: 1.38630271, g_loss: 0.69306356\n",
      "Step: [19536] d_loss: 1.38627708, g_loss: 0.69320196\n",
      "Step: [19537] d_loss: 1.38626301, g_loss: 0.69300961\n",
      "Step: [19538] d_loss: 1.38674653, g_loss: 0.69218105\n",
      "Step: [19539] d_loss: 1.38844490, g_loss: 0.69460016\n",
      "Step: [19540] d_loss: 1.38940132, g_loss: 0.69103241\n",
      "Step: [19541] d_loss: 1.38860250, g_loss: 0.69077259\n",
      "Step: [19542] d_loss: 1.38731110, g_loss: 0.69003540\n",
      "Step: [19543] d_loss: 1.38652146, g_loss: 0.69002247\n",
      "Step: [19544] d_loss: 1.38631225, g_loss: 0.69403279\n",
      "Step: [19545] d_loss: 1.38641405, g_loss: 0.69471514\n",
      "Step: [19546] d_loss: 1.38647556, g_loss: 0.69547284\n",
      "Step: [19547] d_loss: 1.38638723, g_loss: 0.69399738\n",
      "Step: [19548] d_loss: 1.38636398, g_loss: 0.69230336\n",
      "Step: [19549] d_loss: 1.38634372, g_loss: 0.69247913\n",
      "Step: [19550] d_loss: 1.38632619, g_loss: 0.69378257\n",
      "Step: [19551] d_loss: 1.38632596, g_loss: 0.69369304\n",
      "Step: [19552] d_loss: 1.38621891, g_loss: 0.69457543\n",
      "Step: [19553] d_loss: 1.38630092, g_loss: 0.69337702\n",
      "Step: [19554] d_loss: 1.38632965, g_loss: 0.69259489\n",
      "Step: [19555] d_loss: 1.38629520, g_loss: 0.69327646\n",
      "Step: [19556] d_loss: 1.38631320, g_loss: 0.69287932\n",
      "Step: [19557] d_loss: 1.38629580, g_loss: 0.69335419\n",
      "Step: [19558] d_loss: 1.38628364, g_loss: 0.69339615\n",
      "Step: [19559] d_loss: 1.38630521, g_loss: 0.69332683\n",
      "Step: [19560] d_loss: 1.38627732, g_loss: 0.69303787\n",
      "Step: [19561] d_loss: 1.38629580, g_loss: 0.69287515\n",
      "Step: [19562] d_loss: 1.38627410, g_loss: 0.69315588\n",
      "Step: [19563] d_loss: 1.38627911, g_loss: 0.69327629\n",
      "Step: [19564] d_loss: 1.38630629, g_loss: 0.69375473\n",
      "Step: [19565] d_loss: 1.38629007, g_loss: 0.69327581\n",
      "Step: [19566] d_loss: 1.38627183, g_loss: 0.69321513\n",
      "Step: [19567] d_loss: 1.38628697, g_loss: 0.69320238\n",
      "Step: [19568] d_loss: 1.38625026, g_loss: 0.69416976\n",
      "Step: [19569] d_loss: 1.38629007, g_loss: 0.69328660\n",
      "Step: [19570] d_loss: 1.38629031, g_loss: 0.69354284\n",
      "Step: [19571] d_loss: 1.38630164, g_loss: 0.69330871\n",
      "Step: [19572] d_loss: 1.38629580, g_loss: 0.69276327\n",
      "Step: [19573] d_loss: 1.38625777, g_loss: 0.69310361\n",
      "Step: [19574] d_loss: 1.38631463, g_loss: 0.69367075\n",
      "Step: [19575] d_loss: 1.38629198, g_loss: 0.69328529\n",
      "Step: [19576] d_loss: 1.38627207, g_loss: 0.69297850\n",
      "Step: [19577] d_loss: 1.38628614, g_loss: 0.69345206\n",
      "Step: [19578] d_loss: 1.38631642, g_loss: 0.69317013\n",
      "Step: [19579] d_loss: 1.38628256, g_loss: 0.69287717\n",
      "Step: [19580] d_loss: 1.38627839, g_loss: 0.69307870\n",
      "Step: [19581] d_loss: 1.38625133, g_loss: 0.69317412\n",
      "Step: [19582] d_loss: 1.38628352, g_loss: 0.69306046\n",
      "Step: [19583] d_loss: 1.38626146, g_loss: 0.69318789\n",
      "Step: [19584] d_loss: 1.38626003, g_loss: 0.69320542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19585] d_loss: 1.38623154, g_loss: 0.69375432\n",
      "Step: [19586] d_loss: 1.38628912, g_loss: 0.69327396\n",
      "Step: [19587] d_loss: 1.38624644, g_loss: 0.69287318\n",
      "Step: [19588] d_loss: 1.38621211, g_loss: 0.69305360\n",
      "Step: [19589] d_loss: 1.38631535, g_loss: 0.69318449\n",
      "Step: [19590] d_loss: 1.38627768, g_loss: 0.69323516\n",
      "Step: [19591] d_loss: 1.38632154, g_loss: 0.69305730\n",
      "Step: [19592] d_loss: 1.38627672, g_loss: 0.69314170\n",
      "Step: [19593] d_loss: 1.38626957, g_loss: 0.69334805\n",
      "Step: [19594] d_loss: 1.38630331, g_loss: 0.69311053\n",
      "Step: [19595] d_loss: 1.38628078, g_loss: 0.69337010\n",
      "Step: [19596] d_loss: 1.38628733, g_loss: 0.69327855\n",
      "Step: [19597] d_loss: 1.38624883, g_loss: 0.69301987\n",
      "Step: [19598] d_loss: 1.38626850, g_loss: 0.69306874\n",
      "Step: [19599] d_loss: 1.38629317, g_loss: 0.69310880\n",
      "Step: [19600] d_loss: 1.38630104, g_loss: 0.69337171\n",
      "Step: [19601] d_loss: 1.38627839, g_loss: 0.69326097\n",
      "Step: [19602] d_loss: 1.38631153, g_loss: 0.69328046\n",
      "Step: [19603] d_loss: 1.38627768, g_loss: 0.69300234\n",
      "Step: [19604] d_loss: 1.38632154, g_loss: 0.69364476\n",
      "Step: [19605] d_loss: 1.38629234, g_loss: 0.69333971\n",
      "Step: [19606] d_loss: 1.38628531, g_loss: 0.69285381\n",
      "Step: [19607] d_loss: 1.38625979, g_loss: 0.69325030\n",
      "Step: [19608] d_loss: 1.38634777, g_loss: 0.69325137\n",
      "Step: [19609] d_loss: 1.38658142, g_loss: 0.69472593\n",
      "Step: [19610] d_loss: 1.38664162, g_loss: 0.69552732\n",
      "Step: [19611] d_loss: 1.38659894, g_loss: 0.69565618\n",
      "Step: [19612] d_loss: 1.38634920, g_loss: 0.69408393\n",
      "Step: [19613] d_loss: 1.38630247, g_loss: 0.69231868\n",
      "Step: [19614] d_loss: 1.38639224, g_loss: 0.69048250\n",
      "Step: [19615] d_loss: 1.38636601, g_loss: 0.69162858\n",
      "Step: [19616] d_loss: 1.38631082, g_loss: 0.69321978\n",
      "Step: [19617] d_loss: 1.38637376, g_loss: 0.69377792\n",
      "Step: [19618] d_loss: 1.38629627, g_loss: 0.69311166\n",
      "Step: [19619] d_loss: 1.38628852, g_loss: 0.69289047\n",
      "Step: [19620] d_loss: 1.38630903, g_loss: 0.69280517\n",
      "Step: [19621] d_loss: 1.38628197, g_loss: 0.69367135\n",
      "Step: [19622] d_loss: 1.38636827, g_loss: 0.69284570\n",
      "Step: [19623] d_loss: 1.38647699, g_loss: 0.69379711\n",
      "Step: [19624] d_loss: 1.38653314, g_loss: 0.69438219\n",
      "Step: [19625] d_loss: 1.38648224, g_loss: 0.69366145\n",
      "Step: [19626] d_loss: 1.38638067, g_loss: 0.69221580\n",
      "Step: [19627] d_loss: 1.38634551, g_loss: 0.69177049\n",
      "Step: [19628] d_loss: 1.38628006, g_loss: 0.69240862\n",
      "Step: [19629] d_loss: 1.38627028, g_loss: 0.69371116\n",
      "Step: [19630] d_loss: 1.38627791, g_loss: 0.69362146\n",
      "Step: [19631] d_loss: 1.38627672, g_loss: 0.69360977\n",
      "Step: [19632] d_loss: 1.38626218, g_loss: 0.69343686\n",
      "Step: [19633] d_loss: 1.38630319, g_loss: 0.69270653\n",
      "Step: [19634] d_loss: 1.38630533, g_loss: 0.69220006\n",
      "Step: [19635] d_loss: 1.38629830, g_loss: 0.69460958\n",
      "Step: [19636] d_loss: 1.38749731, g_loss: 0.69411147\n",
      "Step: [19637] d_loss: 1.38911831, g_loss: 0.69272035\n",
      "Step: [19638] d_loss: 1.38900828, g_loss: 0.69124341\n",
      "Step: [19639] d_loss: 1.38897705, g_loss: 0.69724095\n",
      "Step: [19640] d_loss: 1.38838780, g_loss: 0.69254178\n",
      "Step: [19641] d_loss: 1.38717294, g_loss: 0.69095230\n",
      "Step: [19642] d_loss: 1.38647795, g_loss: 0.69121844\n",
      "Step: [19643] d_loss: 1.38625848, g_loss: 0.69369602\n",
      "Step: [19644] d_loss: 1.38626432, g_loss: 0.69440460\n",
      "Step: [19645] d_loss: 1.38634431, g_loss: 0.69411784\n",
      "Step: [19646] d_loss: 1.38633275, g_loss: 0.69343650\n",
      "Step: [19647] d_loss: 1.38630986, g_loss: 0.69380647\n",
      "Step: [19648] d_loss: 1.38626182, g_loss: 0.69339955\n",
      "Step: [19649] d_loss: 1.38630676, g_loss: 0.69229460\n",
      "Step: [19650] d_loss: 1.38622344, g_loss: 0.69310725\n",
      "Step: [19651] d_loss: 1.38630259, g_loss: 0.69298857\n",
      "Step: [19652] d_loss: 1.38663340, g_loss: 0.69319427\n",
      "Step: [19653] d_loss: 1.38763118, g_loss: 0.69157404\n",
      "Step: [19654] d_loss: 1.38879442, g_loss: 0.69038856\n",
      "Step: [19655] d_loss: 1.38855028, g_loss: 0.69373226\n",
      "Step: [19656] d_loss: 1.38726056, g_loss: 0.69313049\n",
      "Step: [19657] d_loss: 1.38647962, g_loss: 0.69411886\n",
      "Step: [19658] d_loss: 1.38631117, g_loss: 0.69426584\n",
      "Step: [19659] d_loss: 1.38630188, g_loss: 0.69376117\n",
      "Step: [19660] d_loss: 1.38631034, g_loss: 0.69212019\n",
      "Step: [19661] d_loss: 1.38623190, g_loss: 0.69281644\n",
      "Step: [19662] d_loss: 1.38627064, g_loss: 0.69334739\n",
      "Step: [19663] d_loss: 1.38631189, g_loss: 0.69364506\n",
      "Step: [19664] d_loss: 1.38627410, g_loss: 0.69313955\n",
      "Step: [19665] d_loss: 1.38629985, g_loss: 0.69285786\n",
      "Step: [19666] d_loss: 1.38629842, g_loss: 0.69353002\n",
      "Step: [19667] d_loss: 1.38638246, g_loss: 0.69293439\n",
      "Step: [19668] d_loss: 1.38631916, g_loss: 0.69282377\n",
      "Step: [19669] d_loss: 1.38631153, g_loss: 0.69354570\n",
      "Step: [19670] d_loss: 1.38636947, g_loss: 0.69356203\n",
      "Step: [19671] d_loss: 1.38637924, g_loss: 0.69296890\n",
      "Step: [19672] d_loss: 1.38595808, g_loss: 0.69593573\n",
      "Step: [19673] d_loss: 1.38632357, g_loss: 0.69277573\n",
      "Step: [19674] d_loss: 1.38634479, g_loss: 0.69391751\n",
      "Step: [19675] d_loss: 1.38633883, g_loss: 0.69347119\n",
      "Step: [19676] d_loss: 1.38631451, g_loss: 0.69347709\n",
      "Step: [19677] d_loss: 1.38632631, g_loss: 0.69312584\n",
      "Step: [19678] d_loss: 1.38627040, g_loss: 0.69324541\n",
      "Step: [19679] d_loss: 1.38633943, g_loss: 0.69382739\n",
      "Step: [19680] d_loss: 1.38632572, g_loss: 0.69326049\n",
      "Step: [19681] d_loss: 1.38629079, g_loss: 0.69222355\n",
      "Step: [19682] d_loss: 1.38634443, g_loss: 0.69364870\n",
      "Step: [19683] d_loss: 1.38637280, g_loss: 0.69300747\n",
      "Step: [19684] d_loss: 1.38637996, g_loss: 0.69322425\n",
      "Step: [19685] d_loss: 1.38631201, g_loss: 0.69423395\n",
      "Step: [19686] d_loss: 1.38634384, g_loss: 0.69412541\n",
      "Step: [19687] d_loss: 1.38632262, g_loss: 0.69331181\n",
      "Step: [19688] d_loss: 1.38626242, g_loss: 0.69253808\n",
      "Step: [19689] d_loss: 1.38630235, g_loss: 0.69282568\n",
      "Step: [19690] d_loss: 1.38635671, g_loss: 0.69362044\n",
      "Step: [19691] d_loss: 1.38628924, g_loss: 0.69314837\n",
      "Step: [19692] d_loss: 1.38631153, g_loss: 0.69335222\n",
      "Step: [19693] d_loss: 1.38635015, g_loss: 0.69316638\n",
      "Step: [19694] d_loss: 1.38628531, g_loss: 0.69279599\n",
      "Step: [19695] d_loss: 1.38626981, g_loss: 0.69319272\n",
      "Step: [19696] d_loss: 1.38629055, g_loss: 0.69360101\n",
      "Step: [19697] d_loss: 1.38627005, g_loss: 0.69313353\n",
      "Step: [19698] d_loss: 1.38630033, g_loss: 0.69330126\n",
      "Step: [19699] d_loss: 1.38627911, g_loss: 0.69304204\n",
      "Step: [19700] d_loss: 1.38628435, g_loss: 0.69307429\n",
      "Step: [19701] d_loss: 1.38628721, g_loss: 0.69301105\n",
      "Step: [19702] d_loss: 1.38631749, g_loss: 0.69340682\n",
      "Step: [19703] d_loss: 1.38626432, g_loss: 0.69369757\n",
      "Step: [19704] d_loss: 1.38632393, g_loss: 0.69343603\n",
      "Step: [19705] d_loss: 1.38627124, g_loss: 0.69286603\n",
      "Step: [19706] d_loss: 1.38626182, g_loss: 0.69308478\n",
      "Step: [19707] d_loss: 1.38629484, g_loss: 0.69384551\n",
      "Step: [19708] d_loss: 1.38629222, g_loss: 0.69329393\n",
      "Step: [19709] d_loss: 1.38626862, g_loss: 0.69309509\n",
      "Step: [19710] d_loss: 1.38625765, g_loss: 0.69332600\n",
      "Step: [19711] d_loss: 1.38626957, g_loss: 0.69382876\n",
      "Step: [19712] d_loss: 1.38623881, g_loss: 0.69340277\n",
      "Step: [19713] d_loss: 1.38627791, g_loss: 0.69274116\n",
      "Step: [19714] d_loss: 1.38627517, g_loss: 0.69302893\n",
      "Step: [19715] d_loss: 1.38628411, g_loss: 0.69317114\n",
      "Step: [19716] d_loss: 1.38624549, g_loss: 0.69327277\n",
      "Step: [19717] d_loss: 1.38624144, g_loss: 0.69313306\n",
      "Step: [19718] d_loss: 1.38645148, g_loss: 0.69403815\n",
      "Step: [19719] d_loss: 1.38630128, g_loss: 0.69364971\n",
      "Step: [19720] d_loss: 1.38634813, g_loss: 0.69317341\n",
      "Step: [19721] d_loss: 1.38630438, g_loss: 0.69308269\n",
      "Step: [19722] d_loss: 1.38633919, g_loss: 0.69317365\n",
      "Step: [19723] d_loss: 1.38630939, g_loss: 0.69353604\n",
      "Step: [19724] d_loss: 1.38625193, g_loss: 0.69319099\n",
      "Step: [19725] d_loss: 1.38626206, g_loss: 0.69284582\n",
      "Step: [19726] d_loss: 1.38630939, g_loss: 0.69349480\n",
      "Step: [19727] d_loss: 1.38630128, g_loss: 0.69319087\n",
      "Step: [19728] d_loss: 1.38627458, g_loss: 0.69318831\n",
      "Step: [19729] d_loss: 1.38625658, g_loss: 0.69324964\n",
      "Step: [19730] d_loss: 1.38624501, g_loss: 0.69322079\n",
      "Step: [19731] d_loss: 1.38630652, g_loss: 0.69346309\n",
      "Step: [19732] d_loss: 1.38630390, g_loss: 0.69293660\n",
      "Step: [19733] d_loss: 1.38636374, g_loss: 0.69305497\n",
      "Step: [19734] d_loss: 1.38704467, g_loss: 0.69574273\n",
      "Step: [19735] d_loss: 1.38748729, g_loss: 0.69417566\n",
      "Step: [19736] d_loss: 1.38719726, g_loss: 0.69464862\n",
      "Step: [19737] d_loss: 1.38669741, g_loss: 0.69497204\n",
      "Step: [19738] d_loss: 1.38636279, g_loss: 0.69427150\n",
      "Step: [19739] d_loss: 1.38626707, g_loss: 0.69316411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19740] d_loss: 1.38629949, g_loss: 0.69251019\n",
      "Step: [19741] d_loss: 1.38630164, g_loss: 0.69244277\n",
      "Step: [19742] d_loss: 1.38626575, g_loss: 0.69286323\n",
      "Step: [19743] d_loss: 1.38632345, g_loss: 0.69310772\n",
      "Step: [19744] d_loss: 1.38633168, g_loss: 0.69335783\n",
      "Step: [19745] d_loss: 1.38630843, g_loss: 0.69316638\n",
      "Step: [19746] d_loss: 1.38632178, g_loss: 0.69322199\n",
      "Step: [19747] d_loss: 1.38628459, g_loss: 0.69317985\n",
      "Step: [19748] d_loss: 1.38632452, g_loss: 0.69320059\n",
      "Step: [19749] d_loss: 1.38628244, g_loss: 0.69347900\n",
      "Step: [19750] d_loss: 1.38628769, g_loss: 0.69347143\n",
      "Step: [19751] d_loss: 1.38629639, g_loss: 0.69273740\n",
      "Step: [19752] d_loss: 1.38628161, g_loss: 0.69276851\n",
      "Step: [19753] d_loss: 1.38627136, g_loss: 0.69299120\n",
      "Step: [19754] d_loss: 1.38624656, g_loss: 0.69323766\n",
      "Step: [19755] d_loss: 1.38631153, g_loss: 0.69325459\n",
      "Step: [19756] d_loss: 1.38624716, g_loss: 0.69334602\n",
      "Step: [19757] d_loss: 1.38625908, g_loss: 0.69316041\n",
      "Step: [19758] d_loss: 1.38626015, g_loss: 0.69325423\n",
      "Step: [19759] d_loss: 1.38630795, g_loss: 0.69314402\n",
      "Step: [19760] d_loss: 1.38628340, g_loss: 0.69319719\n",
      "Step: [19761] d_loss: 1.38630915, g_loss: 0.69332200\n",
      "Step: [19762] d_loss: 1.38626695, g_loss: 0.69335437\n",
      "Step: [19763] d_loss: 1.38633227, g_loss: 0.69350910\n",
      "Step: [19764] d_loss: 1.38625741, g_loss: 0.69271731\n",
      "Step: [19765] d_loss: 1.38626528, g_loss: 0.69327158\n",
      "Step: [19766] d_loss: 1.38645184, g_loss: 0.69380665\n",
      "Step: [19767] d_loss: 1.38633490, g_loss: 0.69358164\n",
      "Step: [19768] d_loss: 1.38658500, g_loss: 0.69266576\n",
      "Step: [19769] d_loss: 1.45032227, g_loss: 0.72774601\n",
      "Step: [19770] d_loss: 1.38799024, g_loss: 0.69150585\n",
      "Step: [19771] d_loss: 1.38791406, g_loss: 0.69458377\n",
      "Step: [19772] d_loss: 1.38767636, g_loss: 0.69425905\n",
      "Step: [19773] d_loss: 1.38681197, g_loss: 0.69434011\n",
      "Step: [19774] d_loss: 1.38657856, g_loss: 0.69219887\n",
      "Step: [19775] d_loss: 1.38639128, g_loss: 0.69125760\n",
      "Step: [19776] d_loss: 1.38638735, g_loss: 0.69041288\n",
      "Step: [19777] d_loss: 1.38634062, g_loss: 0.69308853\n",
      "Step: [19778] d_loss: 1.38631570, g_loss: 0.69399792\n",
      "Step: [19779] d_loss: 1.38621652, g_loss: 0.69445860\n",
      "Step: [19780] d_loss: 1.38636613, g_loss: 0.69343829\n",
      "Step: [19781] d_loss: 1.38632977, g_loss: 0.69155777\n",
      "Step: [19782] d_loss: 1.38631320, g_loss: 0.69216597\n",
      "Step: [19783] d_loss: 1.38631296, g_loss: 0.69334263\n",
      "Step: [19784] d_loss: 1.38637650, g_loss: 0.69356382\n",
      "Step: [19785] d_loss: 1.38630426, g_loss: 0.69317961\n",
      "Step: [19786] d_loss: 1.38631010, g_loss: 0.69254518\n",
      "Step: [19787] d_loss: 1.38620687, g_loss: 0.69283575\n",
      "Step: [19788] d_loss: 1.38632703, g_loss: 0.69357735\n",
      "Step: [19789] d_loss: 1.38634598, g_loss: 0.69284904\n",
      "Step: [19790] d_loss: 1.38625216, g_loss: 0.69347435\n",
      "Step: [19791] d_loss: 1.38636708, g_loss: 0.69325519\n",
      "Step: [19792] d_loss: 1.38457990, g_loss: 0.68977034\n",
      "Step: [19793] d_loss: 1.38644850, g_loss: 0.69454205\n",
      "Step: [19794] d_loss: 1.38633060, g_loss: 0.69264042\n",
      "Step: [19795] d_loss: 1.38636971, g_loss: 0.69197762\n",
      "Step: [19796] d_loss: 1.38625133, g_loss: 0.69275135\n",
      "Step: [19797] d_loss: 1.38644660, g_loss: 0.69263566\n",
      "Step: [19798] d_loss: 1.38634527, g_loss: 0.69291741\n",
      "Step: [19799] d_loss: 1.38625717, g_loss: 0.69407982\n",
      "Step: [19800] d_loss: 1.38637733, g_loss: 0.69491398\n",
      "Step: [19801] d_loss: 1.38633740, g_loss: 0.69396293\n",
      "Step: [19802] d_loss: 1.38635409, g_loss: 0.69242364\n",
      "Step: [19803] d_loss: 1.38633084, g_loss: 0.69224262\n",
      "Step: [19804] d_loss: 1.38629007, g_loss: 0.69316196\n",
      "Step: [19805] d_loss: 1.38639522, g_loss: 0.69442403\n",
      "Step: [19806] d_loss: 1.38626778, g_loss: 0.69378126\n",
      "Step: [19807] d_loss: 1.38646507, g_loss: 0.69344598\n",
      "Step: [19808] d_loss: 1.38637376, g_loss: 0.69290602\n",
      "Step: [19809] d_loss: 1.38635492, g_loss: 0.69279134\n",
      "Step: [19810] d_loss: 1.38628829, g_loss: 0.69311696\n",
      "Step: [19811] d_loss: 1.38634765, g_loss: 0.69370514\n",
      "Step: [19812] d_loss: 1.38633180, g_loss: 0.69328719\n",
      "Step: [19813] d_loss: 1.38630414, g_loss: 0.69334722\n",
      "Step: [19814] d_loss: 1.38631010, g_loss: 0.69315684\n",
      "Step: [19815] d_loss: 1.38653874, g_loss: 0.69358873\n",
      "Step: [19816] d_loss: 1.38637233, g_loss: 0.69293022\n",
      "Step: [19817] d_loss: 1.38648081, g_loss: 0.69399011\n",
      "Step: [19818] d_loss: 1.38636506, g_loss: 0.69329691\n",
      "Step: [19819] d_loss: 1.38633370, g_loss: 0.69340724\n",
      "Step: [19820] d_loss: 1.38622165, g_loss: 0.69348943\n",
      "Step: [19821] d_loss: 1.38629639, g_loss: 0.69312960\n",
      "Step: [19822] d_loss: 1.38633919, g_loss: 0.69333708\n",
      "Step: [19823] d_loss: 1.38627923, g_loss: 0.69282687\n",
      "Step: [19824] d_loss: 1.38638473, g_loss: 0.69307590\n",
      "Step: [19825] d_loss: 1.38621783, g_loss: 0.69349289\n",
      "Step: [19826] d_loss: 1.38634658, g_loss: 0.69383651\n",
      "Step: [19827] d_loss: 1.38623333, g_loss: 0.69377673\n",
      "Step: [19828] d_loss: 1.38634479, g_loss: 0.69307125\n",
      "Step: [19829] d_loss: 1.38627052, g_loss: 0.69265985\n",
      "Step: [19830] d_loss: 1.38630450, g_loss: 0.69183576\n",
      "Step: [19831] d_loss: 1.38629484, g_loss: 0.69268531\n",
      "Step: [19832] d_loss: 1.38638115, g_loss: 0.69165730\n",
      "Step: [19833] d_loss: 1.38645303, g_loss: 0.69146669\n",
      "Step: [19834] d_loss: 1.38631296, g_loss: 0.69243848\n",
      "Step: [19835] d_loss: 1.38625538, g_loss: 0.69471627\n",
      "Step: [19836] d_loss: 1.38575053, g_loss: 0.69153476\n",
      "Step: [19837] d_loss: 1.38648939, g_loss: 0.69750845\n",
      "Step: [19838] d_loss: 1.38640201, g_loss: 0.69340438\n",
      "Step: [19839] d_loss: 1.38649011, g_loss: 0.69125664\n",
      "Step: [19840] d_loss: 1.38632202, g_loss: 0.69141507\n",
      "Step: [19841] d_loss: 1.38621485, g_loss: 0.69295985\n",
      "Step: [19842] d_loss: 1.38622999, g_loss: 0.69411290\n",
      "Step: [19843] d_loss: 1.38628316, g_loss: 0.69381344\n",
      "Step: [19844] d_loss: 1.38618445, g_loss: 0.69322884\n",
      "Step: [19845] d_loss: 1.38630152, g_loss: 0.69278908\n",
      "Step: [19846] d_loss: 1.38626790, g_loss: 0.69284713\n",
      "Step: [19847] d_loss: 1.38623381, g_loss: 0.69309926\n",
      "Step: [19848] d_loss: 1.38634348, g_loss: 0.69349945\n",
      "Step: [19849] d_loss: 1.38627088, g_loss: 0.69300270\n",
      "Step: [19850] d_loss: 1.38636923, g_loss: 0.69297576\n",
      "Step: [19851] d_loss: 1.38626289, g_loss: 0.69293094\n",
      "Step: [19852] d_loss: 1.38630915, g_loss: 0.69311982\n",
      "Step: [19853] d_loss: 1.38630962, g_loss: 0.69338435\n",
      "Step: [19854] d_loss: 1.38640380, g_loss: 0.69342184\n",
      "Step: [19855] d_loss: 1.38643062, g_loss: 0.69338632\n",
      "Step: [19856] d_loss: 1.38635063, g_loss: 0.69299781\n",
      "Step: [19857] d_loss: 1.38635993, g_loss: 0.69286764\n",
      "Step: [19858] d_loss: 1.38628471, g_loss: 0.69317025\n",
      "Step: [19859] d_loss: 1.38631439, g_loss: 0.69337893\n",
      "Step: [19860] d_loss: 1.38625121, g_loss: 0.69356477\n",
      "Step: [19861] d_loss: 1.38630271, g_loss: 0.69359910\n",
      "Step: [19862] d_loss: 1.38510501, g_loss: 0.69405508\n",
      "Step: [19863] d_loss: 1.38643444, g_loss: 0.69531870\n",
      "Step: [19864] d_loss: 1.38671684, g_loss: 0.69337213\n",
      "Step: [19865] d_loss: 1.38645792, g_loss: 0.69294101\n",
      "Step: [19866] d_loss: 1.38640273, g_loss: 0.69354510\n",
      "Step: [19867] d_loss: 1.38630462, g_loss: 0.69405651\n",
      "Step: [19868] d_loss: 1.38626444, g_loss: 0.69370008\n",
      "Step: [19869] d_loss: 1.38634276, g_loss: 0.69333690\n",
      "Step: [19870] d_loss: 1.38619280, g_loss: 0.69374818\n",
      "Step: [19871] d_loss: 1.38630795, g_loss: 0.69270051\n",
      "Step: [19872] d_loss: 1.38619947, g_loss: 0.69332623\n",
      "Step: [19873] d_loss: 1.38651359, g_loss: 0.69320166\n",
      "Step: [19874] d_loss: 1.38669491, g_loss: 0.69162220\n",
      "Step: [19875] d_loss: 1.38690138, g_loss: 0.69315040\n",
      "Step: [19876] d_loss: 1.38672805, g_loss: 0.69385576\n",
      "Step: [19877] d_loss: 1.38661790, g_loss: 0.69058216\n",
      "Step: [19878] d_loss: 1.38645101, g_loss: 0.69219410\n",
      "Step: [19879] d_loss: 1.38633347, g_loss: 0.69403046\n",
      "Step: [19880] d_loss: 1.38610327, g_loss: 0.69432747\n",
      "Step: [19881] d_loss: 1.38622928, g_loss: 0.69403130\n",
      "Step: [19882] d_loss: 1.38630772, g_loss: 0.69319707\n",
      "Step: [19883] d_loss: 1.38633418, g_loss: 0.69256628\n",
      "Step: [19884] d_loss: 1.38628864, g_loss: 0.69266337\n",
      "Step: [19885] d_loss: 1.38823140, g_loss: 0.69281936\n",
      "Step: [19886] d_loss: 1.38630438, g_loss: 0.69414067\n",
      "Step: [19887] d_loss: 1.38624346, g_loss: 0.69314861\n",
      "Step: [19888] d_loss: 1.38634729, g_loss: 0.69255686\n",
      "Step: [19889] d_loss: 1.38629746, g_loss: 0.69301993\n",
      "Step: [19890] d_loss: 1.38654256, g_loss: 0.69471711\n",
      "Step: [19891] d_loss: 1.38632321, g_loss: 0.69396627\n",
      "Step: [19892] d_loss: 1.38687170, g_loss: 0.69190681\n",
      "Step: [19893] d_loss: 1.38628411, g_loss: 0.69214785\n",
      "Step: [19894] d_loss: 1.38628173, g_loss: 0.69287425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [19895] d_loss: 1.38630438, g_loss: 0.69389606\n",
      "Step: [19896] d_loss: 1.38625741, g_loss: 0.69383383\n",
      "Step: [19897] d_loss: 1.38615596, g_loss: 0.69323850\n",
      "Step: [19898] d_loss: 1.38623989, g_loss: 0.69279826\n",
      "Step: [19899] d_loss: 1.38623178, g_loss: 0.69295561\n",
      "Step: [19900] d_loss: 1.38620043, g_loss: 0.69327110\n",
      "Step: [19901] d_loss: 1.38623333, g_loss: 0.69342208\n",
      "Step: [19902] d_loss: 1.38624835, g_loss: 0.69328427\n",
      "Step: [19903] d_loss: 1.38632786, g_loss: 0.69305396\n",
      "Step: [19904] d_loss: 1.38622665, g_loss: 0.69341266\n",
      "Step: [19905] d_loss: 1.38618326, g_loss: 0.69244879\n",
      "Step: [19906] d_loss: 1.38627601, g_loss: 0.69319981\n",
      "Step: [19907] d_loss: 1.38625550, g_loss: 0.69330513\n",
      "Step: [19908] d_loss: 1.38626695, g_loss: 0.69275618\n",
      "Step: [19909] d_loss: 1.38631892, g_loss: 0.69250709\n",
      "Step: [19910] d_loss: 1.38619614, g_loss: 0.69306231\n",
      "Step: [19911] d_loss: 1.38622856, g_loss: 0.69308197\n",
      "Step: [19912] d_loss: 1.38620043, g_loss: 0.69429404\n",
      "Step: [19913] d_loss: 1.38630676, g_loss: 0.69462913\n",
      "Step: [19914] d_loss: 1.38652277, g_loss: 0.69469464\n",
      "Step: [19915] d_loss: 1.38670850, g_loss: 0.69348681\n",
      "Step: [19916] d_loss: 1.38670301, g_loss: 0.69217503\n",
      "Step: [19917] d_loss: 1.38664246, g_loss: 0.69198334\n",
      "Step: [19918] d_loss: 1.38653874, g_loss: 0.69237834\n",
      "Step: [19919] d_loss: 1.38638222, g_loss: 0.69258541\n",
      "Step: [19920] d_loss: 1.38683820, g_loss: 0.69531351\n",
      "Step: [19921] d_loss: 1.38625371, g_loss: 0.69399810\n",
      "Step: [19922] d_loss: 1.38629007, g_loss: 0.69314420\n",
      "Step: [19923] d_loss: 1.38631511, g_loss: 0.69255936\n",
      "Step: [19924] d_loss: 1.38622868, g_loss: 0.69276941\n",
      "Step: [19925] d_loss: 1.38630295, g_loss: 0.69318533\n",
      "Step: [19926] d_loss: 1.38629532, g_loss: 0.69296807\n",
      "Step: [19927] d_loss: 1.38631773, g_loss: 0.69381678\n",
      "Step: [19928] d_loss: 1.38629329, g_loss: 0.69386476\n",
      "Step: [19929] d_loss: 1.38626564, g_loss: 0.69388545\n",
      "Step: [19930] d_loss: 1.38634741, g_loss: 0.69472468\n",
      "Step: [19931] d_loss: 1.38634467, g_loss: 0.69401580\n",
      "Step: [19932] d_loss: 1.38641071, g_loss: 0.69278699\n",
      "Step: [19933] d_loss: 1.38634968, g_loss: 0.69090074\n",
      "Step: [19934] d_loss: 1.38628030, g_loss: 0.69156837\n",
      "Step: [19935] d_loss: 1.38614845, g_loss: 0.69317597\n",
      "Step: [19936] d_loss: 1.38625884, g_loss: 0.69431961\n",
      "Step: [19937] d_loss: 1.38624263, g_loss: 0.69403338\n",
      "Step: [19938] d_loss: 1.38618541, g_loss: 0.69371849\n",
      "Step: [19939] d_loss: 1.38617551, g_loss: 0.69402319\n",
      "Step: [19940] d_loss: 1.38617182, g_loss: 0.69193196\n",
      "Step: [19941] d_loss: 1.38642550, g_loss: 0.69296336\n",
      "Step: [19942] d_loss: 1.38656211, g_loss: 0.69571650\n",
      "Step: [19943] d_loss: 1.38683033, g_loss: 0.69639176\n",
      "Step: [19944] d_loss: 1.38686323, g_loss: 0.69351923\n",
      "Step: [19945] d_loss: 1.38668656, g_loss: 0.69023585\n",
      "Step: [19946] d_loss: 1.38664675, g_loss: 0.69093114\n",
      "Step: [19947] d_loss: 1.38648570, g_loss: 0.69263911\n",
      "Step: [19948] d_loss: 1.38641715, g_loss: 0.69394267\n",
      "Step: [19949] d_loss: 1.38636029, g_loss: 0.69353747\n",
      "Step: [19950] d_loss: 1.38632250, g_loss: 0.69319910\n",
      "Step: [19951] d_loss: 1.38636744, g_loss: 0.69304717\n",
      "Step: [19952] d_loss: 1.38633907, g_loss: 0.69297206\n",
      "Step: [19953] d_loss: 1.38622880, g_loss: 0.69341576\n",
      "Step: [19954] d_loss: 1.38652456, g_loss: 0.69328994\n",
      "Step: [19955] d_loss: 1.38628399, g_loss: 0.69329083\n",
      "Step: [19956] d_loss: 1.38627815, g_loss: 0.69337744\n",
      "Step: [19957] d_loss: 1.38633442, g_loss: 0.69356644\n",
      "Step: [19958] d_loss: 1.38632095, g_loss: 0.69352752\n",
      "Step: [19959] d_loss: 1.38629735, g_loss: 0.69348288\n",
      "Step: [19960] d_loss: 1.38633132, g_loss: 0.69420797\n",
      "Step: [19961] d_loss: 1.38631141, g_loss: 0.69380736\n",
      "Step: [19962] d_loss: 1.38641262, g_loss: 0.69242424\n",
      "Step: [19963] d_loss: 1.38621986, g_loss: 0.69351935\n",
      "Step: [19964] d_loss: 1.38636625, g_loss: 0.69283926\n",
      "Step: [19965] d_loss: 1.38639045, g_loss: 0.69322646\n",
      "Step: [19966] d_loss: 1.38635302, g_loss: 0.69513142\n",
      "Step: [19967] d_loss: 1.38646722, g_loss: 0.69476855\n",
      "Step: [19968] d_loss: 1.38657618, g_loss: 0.69380522\n",
      "Step: [19969] d_loss: 1.38669872, g_loss: 0.69461530\n",
      "Step: [19970] d_loss: 1.38663638, g_loss: 0.69346851\n",
      "Step: [19971] d_loss: 1.38653922, g_loss: 0.69339341\n",
      "Step: [19972] d_loss: 1.38652396, g_loss: 0.69183099\n",
      "Step: [19973] d_loss: 1.38641262, g_loss: 0.69133282\n",
      "Step: [19974] d_loss: 1.38636494, g_loss: 0.69472110\n",
      "Step: [19975] d_loss: 1.38631749, g_loss: 0.69483507\n",
      "Step: [19976] d_loss: 1.38629520, g_loss: 0.69358420\n",
      "Step: [19977] d_loss: 1.38628149, g_loss: 0.69340521\n",
      "Step: [19978] d_loss: 1.38632548, g_loss: 0.69285089\n",
      "Step: [19979] d_loss: 1.38629997, g_loss: 0.69299519\n",
      "Step: [19980] d_loss: 1.38616323, g_loss: 0.69284081\n",
      "Step: [19981] d_loss: 1.38625407, g_loss: 0.69326127\n",
      "Step: [19982] d_loss: 1.38630891, g_loss: 0.69351095\n",
      "Step: [19983] d_loss: 1.38629937, g_loss: 0.69322211\n",
      "Step: [19984] d_loss: 1.38627064, g_loss: 0.69317615\n",
      "Step: [19985] d_loss: 1.38608813, g_loss: 0.69400615\n",
      "Step: [19986] d_loss: 1.38634670, g_loss: 0.69259453\n",
      "Step: [19987] d_loss: 1.38627303, g_loss: 0.69335389\n",
      "Step: [19988] d_loss: 1.38634467, g_loss: 0.69319618\n",
      "Step: [19989] d_loss: 1.38771725, g_loss: 0.69157302\n",
      "Step: [19990] d_loss: 1.38628733, g_loss: 0.69401342\n",
      "Step: [19991] d_loss: 1.38624358, g_loss: 0.69322491\n",
      "Step: [19992] d_loss: 1.38625801, g_loss: 0.69272673\n",
      "Step: [19993] d_loss: 1.38627458, g_loss: 0.69286895\n",
      "Step: [19994] d_loss: 1.38820815, g_loss: 0.69207567\n",
      "Step: [19995] d_loss: 1.38630438, g_loss: 0.69415534\n",
      "Step: [19996] d_loss: 1.38626957, g_loss: 0.69332838\n",
      "Step: [19997] d_loss: 1.38626015, g_loss: 0.69320965\n",
      "Step: [19998] d_loss: 1.38628197, g_loss: 0.69315046\n",
      "Step: [19999] d_loss: 1.38627732, g_loss: 0.69355792\n",
      "Step: [20000] d_loss: 1.38627172, g_loss: 0.69335186\n",
      "Step: [20001] d_loss: 1.38624883, g_loss: 0.69338119\n",
      "Step: [20002] d_loss: 1.38627720, g_loss: 0.69336063\n",
      "Step: [20003] d_loss: 1.38629103, g_loss: 0.69323623\n",
      "Step: [20004] d_loss: 1.38887811, g_loss: 0.69175386\n",
      "Step: [20005] d_loss: 1.38629270, g_loss: 0.69507939\n",
      "Step: [20006] d_loss: 1.38628793, g_loss: 0.69316059\n",
      "Step: [20007] d_loss: 1.38627028, g_loss: 0.69166648\n",
      "Step: [20008] d_loss: 1.38626802, g_loss: 0.69216573\n",
      "Step: [20009] d_loss: 1.38626862, g_loss: 0.69320661\n",
      "Step: [20010] d_loss: 1.38628936, g_loss: 0.69353700\n",
      "Step: [20011] d_loss: 1.38625050, g_loss: 0.69331574\n",
      "Step: [20012] d_loss: 1.38629198, g_loss: 0.69295007\n",
      "Step: [20013] d_loss: 1.38627708, g_loss: 0.69317269\n",
      "Step: [20014] d_loss: 1.38626790, g_loss: 0.69296741\n",
      "Step: [20015] d_loss: 1.38635230, g_loss: 0.69266856\n",
      "Step: [20016] d_loss: 1.38626850, g_loss: 0.69341922\n",
      "Step: [20017] d_loss: 1.38628125, g_loss: 0.69312757\n",
      "Step: [20018] d_loss: 1.38635683, g_loss: 0.69249988\n",
      "Step: [20019] d_loss: 1.38626361, g_loss: 0.69320035\n",
      "Step: [20020] d_loss: 1.38622737, g_loss: 0.69353819\n",
      "Step: [20021] d_loss: 1.38628888, g_loss: 0.69304109\n",
      "Step: [20022] d_loss: 1.38626492, g_loss: 0.69314766\n",
      "Step: [20023] d_loss: 1.38631535, g_loss: 0.69307780\n",
      "Step: [20024] d_loss: 1.38630033, g_loss: 0.69322395\n",
      "Step: [20025] d_loss: 1.38627791, g_loss: 0.69278455\n",
      "Step: [20026] d_loss: 1.38632703, g_loss: 0.69335264\n",
      "Step: [20027] d_loss: 1.38621664, g_loss: 0.69307303\n",
      "Step: [20028] d_loss: 1.38628078, g_loss: 0.69317633\n",
      "Step: [20029] d_loss: 1.38620329, g_loss: 0.69289041\n",
      "Step: [20030] d_loss: 1.38616586, g_loss: 0.69377583\n",
      "Step: [20031] d_loss: 1.38623774, g_loss: 0.69324708\n",
      "Step: [20032] d_loss: 1.38629675, g_loss: 0.69311583\n",
      "Step: [20033] d_loss: 1.38827658, g_loss: 0.69186592\n",
      "Step: [20034] d_loss: 1.38625956, g_loss: 0.69284952\n",
      "Step: [20035] d_loss: 1.38627958, g_loss: 0.69361007\n",
      "Step: [20036] d_loss: 1.38622153, g_loss: 0.69358975\n",
      "Step: [20037] d_loss: 1.38625252, g_loss: 0.69320023\n",
      "Step: [20038] d_loss: 1.38626242, g_loss: 0.69327575\n",
      "Step: [20039] d_loss: 1.38626432, g_loss: 0.69330126\n",
      "Step: [20040] d_loss: 1.38626313, g_loss: 0.69322181\n",
      "Step: [20041] d_loss: 1.38628292, g_loss: 0.69274974\n",
      "Step: [20042] d_loss: 1.38629758, g_loss: 0.69378293\n",
      "Step: [20043] d_loss: 1.38623822, g_loss: 0.69325280\n",
      "Step: [20044] d_loss: 1.38629091, g_loss: 0.69280750\n",
      "Step: [20045] d_loss: 1.38629198, g_loss: 0.69279897\n",
      "Step: [20046] d_loss: 1.38629103, g_loss: 0.69293886\n",
      "Step: [20047] d_loss: 1.38627136, g_loss: 0.69273090\n",
      "Step: [20048] d_loss: 1.38626003, g_loss: 0.69317651\n",
      "Step: [20049] d_loss: 1.38820136, g_loss: 0.69303834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20050] d_loss: 1.38624573, g_loss: 0.69321740\n",
      "Step: [20051] d_loss: 1.38627970, g_loss: 0.69321555\n",
      "Step: [20052] d_loss: 1.38627553, g_loss: 0.69328815\n",
      "Step: [20053] d_loss: 1.38630128, g_loss: 0.69283521\n",
      "Step: [20054] d_loss: 1.38629174, g_loss: 0.69256127\n",
      "Step: [20055] d_loss: 1.38629866, g_loss: 0.69294876\n",
      "Step: [20056] d_loss: 1.38627481, g_loss: 0.69322056\n",
      "Step: [20057] d_loss: 1.38629150, g_loss: 0.69350207\n",
      "Step: [20058] d_loss: 1.38627613, g_loss: 0.69310212\n",
      "Step: [20059] d_loss: 1.38617873, g_loss: 0.69354904\n",
      "Step: [20060] d_loss: 1.38630438, g_loss: 0.69346094\n",
      "Step: [20061] d_loss: 1.38629448, g_loss: 0.69358647\n",
      "Step: [20062] d_loss: 1.38632917, g_loss: 0.69325525\n",
      "Step: [20063] d_loss: 1.38634920, g_loss: 0.69349486\n",
      "Step: [20064] d_loss: 1.38640332, g_loss: 0.69261867\n",
      "Step: [20065] d_loss: 1.38640881, g_loss: 0.69321012\n",
      "Step: [20066] d_loss: 1.38641262, g_loss: 0.69385123\n",
      "Step: [20067] d_loss: 1.38645625, g_loss: 0.69489181\n",
      "Step: [20068] d_loss: 1.38651896, g_loss: 0.69426155\n",
      "Step: [20069] d_loss: 1.38654351, g_loss: 0.69380379\n",
      "Step: [20070] d_loss: 1.38659704, g_loss: 0.69241261\n",
      "Step: [20071] d_loss: 1.38660395, g_loss: 0.69219536\n",
      "Step: [20072] d_loss: 1.38657498, g_loss: 0.69197869\n",
      "Step: [20073] d_loss: 1.38655293, g_loss: 0.69282264\n",
      "Step: [20074] d_loss: 1.38648641, g_loss: 0.69223690\n",
      "Step: [20075] d_loss: 1.38647342, g_loss: 0.69263506\n",
      "Step: [20076] d_loss: 1.38640261, g_loss: 0.69312358\n",
      "Step: [20077] d_loss: 1.38639712, g_loss: 0.69368517\n",
      "Step: [20078] d_loss: 1.38638866, g_loss: 0.69342154\n",
      "Step: [20079] d_loss: 1.38635552, g_loss: 0.69422042\n",
      "Step: [20080] d_loss: 1.38642347, g_loss: 0.69470137\n",
      "Step: [20081] d_loss: 1.38650155, g_loss: 0.69368958\n",
      "Step: [20082] d_loss: 1.38907886, g_loss: 0.69189614\n",
      "Step: [20083] d_loss: 1.38648391, g_loss: 0.69342005\n",
      "Step: [20084] d_loss: 1.38643670, g_loss: 0.69222945\n",
      "Step: [20085] d_loss: 1.38648200, g_loss: 0.69236344\n",
      "Step: [20086] d_loss: 1.38645124, g_loss: 0.69314098\n",
      "Step: [20087] d_loss: 1.38641262, g_loss: 0.69349658\n",
      "Step: [20088] d_loss: 1.38639879, g_loss: 0.69365585\n",
      "Step: [20089] d_loss: 1.38638985, g_loss: 0.69357550\n",
      "Step: [20090] d_loss: 1.38637114, g_loss: 0.69280684\n",
      "Step: [20091] d_loss: 1.38635349, g_loss: 0.69292420\n",
      "Step: [20092] d_loss: 1.38635957, g_loss: 0.69304526\n",
      "Step: [20093] d_loss: 1.38619542, g_loss: 0.69401228\n",
      "Step: [20094] d_loss: 1.38644838, g_loss: 0.69735414\n",
      "Step: [20095] d_loss: 1.38655639, g_loss: 0.69418848\n",
      "Step: [20096] d_loss: 1.38645709, g_loss: 0.69103378\n",
      "Step: [20097] d_loss: 1.38648653, g_loss: 0.69057822\n",
      "Step: [20098] d_loss: 1.38743854, g_loss: 0.69281608\n",
      "Step: [20099] d_loss: 1.38629055, g_loss: 0.69254661\n",
      "Step: [20100] d_loss: 1.38629031, g_loss: 0.69411987\n",
      "Step: [20101] d_loss: 1.38633585, g_loss: 0.69421387\n",
      "Step: [20102] d_loss: 1.38639545, g_loss: 0.69427061\n",
      "Step: [20103] d_loss: 1.38644862, g_loss: 0.69291723\n",
      "Step: [20104] d_loss: 1.38657737, g_loss: 0.69364578\n",
      "Step: [20105] d_loss: 1.38653314, g_loss: 0.69283962\n",
      "Step: [20106] d_loss: 1.38666320, g_loss: 0.69499111\n",
      "Step: [20107] d_loss: 1.38667870, g_loss: 0.69435489\n",
      "Step: [20108] d_loss: 1.38669229, g_loss: 0.69530904\n",
      "Step: [20109] d_loss: 1.38679981, g_loss: 0.69598979\n",
      "Step: [20110] d_loss: 1.38682437, g_loss: 0.69448316\n",
      "Step: [20111] d_loss: 1.38679707, g_loss: 0.69237876\n",
      "Step: [20112] d_loss: 1.38670349, g_loss: 0.69292498\n",
      "Step: [20113] d_loss: 1.38662386, g_loss: 0.69209850\n",
      "Step: [20114] d_loss: 1.38652468, g_loss: 0.69362116\n",
      "Step: [20115] d_loss: 1.38646555, g_loss: 0.69463646\n",
      "Step: [20116] d_loss: 1.38641143, g_loss: 0.69324666\n",
      "Step: [20117] d_loss: 1.38638783, g_loss: 0.69279504\n",
      "Step: [20118] d_loss: 1.38637137, g_loss: 0.69239444\n",
      "Step: [20119] d_loss: 1.38631535, g_loss: 0.69263279\n",
      "Step: [20120] d_loss: 1.38629889, g_loss: 0.69349623\n",
      "Step: [20121] d_loss: 1.38631678, g_loss: 0.69398260\n",
      "Step: [20122] d_loss: 1.38628829, g_loss: 0.69361711\n",
      "Step: [20123] d_loss: 1.38631177, g_loss: 0.69338679\n",
      "Step: [20124] d_loss: 1.38631034, g_loss: 0.69311446\n",
      "Step: [20125] d_loss: 1.38631082, g_loss: 0.69286364\n",
      "Step: [20126] d_loss: 1.38721132, g_loss: 0.69374001\n",
      "Step: [20127] d_loss: 1.38651443, g_loss: 0.69283581\n",
      "Step: [20128] d_loss: 1.38634050, g_loss: 0.69278127\n",
      "Step: [20129] d_loss: 1.38633609, g_loss: 0.69405091\n",
      "Step: [20130] d_loss: 1.38635671, g_loss: 0.69360566\n",
      "Step: [20131] d_loss: 1.38633394, g_loss: 0.69300747\n",
      "Step: [20132] d_loss: 1.38645673, g_loss: 0.69348085\n",
      "Step: [20133] d_loss: 1.38623774, g_loss: 0.69338667\n",
      "Step: [20134] d_loss: 1.38632381, g_loss: 0.69285202\n",
      "Step: [20135] d_loss: 1.38634419, g_loss: 0.69303393\n",
      "Step: [20136] d_loss: 1.38632989, g_loss: 0.69254231\n",
      "Step: [20137] d_loss: 1.38632262, g_loss: 0.69336355\n",
      "Step: [20138] d_loss: 1.38632584, g_loss: 0.69334656\n",
      "Step: [20139] d_loss: 1.38631248, g_loss: 0.69289708\n",
      "Step: [20140] d_loss: 1.38630402, g_loss: 0.69328654\n",
      "Step: [20141] d_loss: 1.38626146, g_loss: 0.69292486\n",
      "Step: [20142] d_loss: 1.38628852, g_loss: 0.69312888\n",
      "Step: [20143] d_loss: 1.38629270, g_loss: 0.69275963\n",
      "Step: [20144] d_loss: 1.38628078, g_loss: 0.69285297\n",
      "Step: [20145] d_loss: 1.38627994, g_loss: 0.69336641\n",
      "Step: [20146] d_loss: 1.38640273, g_loss: 0.69637382\n",
      "Step: [20147] d_loss: 1.38631678, g_loss: 0.69226176\n",
      "Step: [20148] d_loss: 1.38638651, g_loss: 0.69308889\n",
      "Step: [20149] d_loss: 1.38638115, g_loss: 0.69346142\n",
      "Step: [20150] d_loss: 1.38639939, g_loss: 0.69408554\n",
      "Step: [20151] d_loss: 1.38637578, g_loss: 0.69321036\n",
      "Step: [20152] d_loss: 1.38637900, g_loss: 0.69089890\n",
      "Step: [20153] d_loss: 1.38632679, g_loss: 0.69115555\n",
      "Step: [20154] d_loss: 1.38631439, g_loss: 0.69312161\n",
      "Step: [20155] d_loss: 1.38625669, g_loss: 0.69328892\n",
      "Step: [20156] d_loss: 1.38628197, g_loss: 0.69348067\n",
      "Step: [20157] d_loss: 1.38626420, g_loss: 0.69302881\n",
      "Step: [20158] d_loss: 1.38626313, g_loss: 0.69304252\n",
      "Step: [20159] d_loss: 1.38624763, g_loss: 0.69309843\n",
      "Step: [20160] d_loss: 1.38620603, g_loss: 0.69320464\n",
      "Step: [20161] d_loss: 1.38624203, g_loss: 0.69350302\n",
      "Step: [20162] d_loss: 1.38627553, g_loss: 0.69272757\n",
      "Step: [20163] d_loss: 1.38630438, g_loss: 0.69288832\n",
      "Step: [20164] d_loss: 1.38626087, g_loss: 0.69344366\n",
      "Step: [20165] d_loss: 1.38628399, g_loss: 0.69337344\n",
      "Step: [20166] d_loss: 1.38630939, g_loss: 0.69379675\n",
      "Step: [20167] d_loss: 1.38631427, g_loss: 0.69335133\n",
      "Step: [20168] d_loss: 1.38638401, g_loss: 0.69332337\n",
      "Step: [20169] d_loss: 1.38634050, g_loss: 0.69375420\n",
      "Step: [20170] d_loss: 1.38635886, g_loss: 0.69293660\n",
      "Step: [20171] d_loss: 1.38635898, g_loss: 0.69337332\n",
      "Step: [20172] d_loss: 1.38641810, g_loss: 0.69380331\n",
      "Step: [20173] d_loss: 1.38641572, g_loss: 0.69202423\n",
      "Step: [20174] d_loss: 1.38642740, g_loss: 0.69149345\n",
      "Step: [20175] d_loss: 1.38642859, g_loss: 0.69303775\n",
      "Step: [20176] d_loss: 1.38637066, g_loss: 0.69286394\n",
      "Step: [20177] d_loss: 1.38635230, g_loss: 0.69270182\n",
      "Step: [20178] d_loss: 1.38632774, g_loss: 0.69300508\n",
      "Step: [20179] d_loss: 1.38632870, g_loss: 0.69361484\n",
      "Step: [20180] d_loss: 1.38634789, g_loss: 0.69332111\n",
      "Step: [20181] d_loss: 1.38629341, g_loss: 0.69347286\n",
      "Step: [20182] d_loss: 1.38630629, g_loss: 0.69332665\n",
      "Step: [20183] d_loss: 1.38633919, g_loss: 0.69326901\n",
      "Step: [20184] d_loss: 1.38636971, g_loss: 0.69280481\n",
      "Step: [20185] d_loss: 1.38629162, g_loss: 0.69209647\n",
      "Step: [20186] d_loss: 1.38633192, g_loss: 0.69283730\n",
      "Step: [20187] d_loss: 1.38628054, g_loss: 0.69405955\n",
      "Step: [20188] d_loss: 1.38632011, g_loss: 0.69403791\n",
      "Step: [20189] d_loss: 1.38635051, g_loss: 0.69395697\n",
      "Step: [20190] d_loss: 1.38637400, g_loss: 0.69315434\n",
      "Step: [20191] d_loss: 1.38641465, g_loss: 0.69359076\n",
      "Step: [20192] d_loss: 1.38641679, g_loss: 0.69307053\n",
      "Step: [20193] d_loss: 1.38639140, g_loss: 0.69209576\n",
      "Step: [20194] d_loss: 1.38638496, g_loss: 0.69297421\n",
      "Step: [20195] d_loss: 1.38637972, g_loss: 0.69257319\n",
      "Step: [20196] d_loss: 1.38632298, g_loss: 0.69218343\n",
      "Step: [20197] d_loss: 1.38631320, g_loss: 0.69213992\n",
      "Step: [20198] d_loss: 1.38627851, g_loss: 0.69309866\n",
      "Step: [20199] d_loss: 1.38628674, g_loss: 0.69350827\n",
      "Step: [20200] d_loss: 1.38626814, g_loss: 0.69271368\n",
      "Step: [20201] d_loss: 1.38635409, g_loss: 0.69389319\n",
      "Step: [20202] d_loss: 1.38631344, g_loss: 0.69380569\n",
      "Step: [20203] d_loss: 1.38645101, g_loss: 0.69309199\n",
      "Step: [20204] d_loss: 1.38657570, g_loss: 0.69194734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20205] d_loss: 1.38656449, g_loss: 0.69117618\n",
      "Step: [20206] d_loss: 1.38659489, g_loss: 0.69172585\n",
      "Step: [20207] d_loss: 1.38655996, g_loss: 0.69426912\n",
      "Step: [20208] d_loss: 1.38663542, g_loss: 0.69656146\n",
      "Step: [20209] d_loss: 1.38672471, g_loss: 0.69447660\n",
      "Step: [20210] d_loss: 1.38672590, g_loss: 0.69561923\n",
      "Step: [20211] d_loss: 1.38691866, g_loss: 0.69287264\n",
      "Step: [20212] d_loss: 1.38702703, g_loss: 0.69369483\n",
      "Step: [20213] d_loss: 1.38701797, g_loss: 0.69331241\n",
      "Step: [20214] d_loss: 1.38698959, g_loss: 0.69480598\n",
      "Step: [20215] d_loss: 1.38760376, g_loss: 0.69563901\n",
      "Step: [20216] d_loss: 1.38822663, g_loss: 0.69350666\n",
      "Step: [20217] d_loss: 1.38822103, g_loss: 0.69545281\n",
      "Step: [20218] d_loss: 1.38784909, g_loss: 0.69408035\n",
      "Step: [20219] d_loss: 1.38730907, g_loss: 0.69161463\n",
      "Step: [20220] d_loss: 1.38691926, g_loss: 0.69442606\n",
      "Step: [20221] d_loss: 1.38668454, g_loss: 0.69686711\n",
      "Step: [20222] d_loss: 1.39085519, g_loss: 0.80683380\n",
      "Step: [20223] d_loss: 1.38826632, g_loss: 0.68039143\n",
      "Step: [20224] d_loss: 1.39060259, g_loss: 0.69135308\n",
      "Step: [20225] d_loss: 1.38791418, g_loss: 0.69579977\n",
      "Step: [20226] d_loss: 1.38735652, g_loss: 0.69885409\n",
      "Step: [20227] d_loss: 1.38696039, g_loss: 0.69599968\n",
      "Step: [20228] d_loss: 1.38664079, g_loss: 0.69255763\n",
      "Step: [20229] d_loss: 1.38646603, g_loss: 0.69277442\n",
      "Step: [20230] d_loss: 1.38642716, g_loss: 0.69379222\n",
      "Step: [20231] d_loss: 1.40244651, g_loss: 0.69705504\n",
      "Step: [20232] d_loss: 1.38641262, g_loss: 0.69442141\n",
      "Step: [20233] d_loss: 1.38657618, g_loss: 0.69463503\n",
      "Step: [20234] d_loss: 1.38653445, g_loss: 0.69301426\n",
      "Step: [20235] d_loss: 1.38648152, g_loss: 0.69282305\n",
      "Step: [20236] d_loss: 1.38639712, g_loss: 0.69243777\n",
      "Step: [20237] d_loss: 1.38631201, g_loss: 0.69315517\n",
      "Step: [20238] d_loss: 1.38610125, g_loss: 0.69187737\n",
      "Step: [20239] d_loss: 1.38635099, g_loss: 0.69159913\n",
      "Step: [20240] d_loss: 1.38633370, g_loss: 0.69534546\n",
      "Step: [20241] d_loss: 1.38636279, g_loss: 0.69517398\n",
      "Step: [20242] d_loss: 1.38634896, g_loss: 0.69438177\n",
      "Step: [20243] d_loss: 1.38633859, g_loss: 0.69304526\n",
      "Step: [20244] d_loss: 1.38630438, g_loss: 0.69215047\n",
      "Step: [20245] d_loss: 1.38631999, g_loss: 0.69336748\n",
      "Step: [20246] d_loss: 1.38631940, g_loss: 0.69270110\n",
      "Step: [20247] d_loss: 1.38631201, g_loss: 0.69321531\n",
      "Step: [20248] d_loss: 1.38628745, g_loss: 0.69344336\n",
      "Step: [20249] d_loss: 1.38630462, g_loss: 0.69405431\n",
      "Step: [20250] d_loss: 1.38613892, g_loss: 0.69375092\n",
      "Step: [20251] d_loss: 1.38631129, g_loss: 0.69225216\n",
      "Step: [20252] d_loss: 1.38630342, g_loss: 0.69326890\n",
      "Step: [20253] d_loss: 1.38629365, g_loss: 0.69431251\n",
      "Step: [20254] d_loss: 1.38629162, g_loss: 0.69389606\n",
      "Step: [20255] d_loss: 1.38629794, g_loss: 0.69394296\n",
      "Step: [20256] d_loss: 1.38630188, g_loss: 0.69301867\n",
      "Step: [20257] d_loss: 1.38630319, g_loss: 0.69278586\n",
      "Step: [20258] d_loss: 1.38628840, g_loss: 0.69308144\n",
      "Step: [20259] d_loss: 1.38627374, g_loss: 0.69319761\n",
      "Step: [20260] d_loss: 1.38629913, g_loss: 0.69352657\n",
      "Step: [20261] d_loss: 1.38627696, g_loss: 0.69330543\n",
      "Step: [20262] d_loss: 1.38625479, g_loss: 0.69310850\n",
      "Step: [20263] d_loss: 1.38623857, g_loss: 0.69319099\n",
      "Step: [20264] d_loss: 1.38618541, g_loss: 0.69322789\n",
      "Step: [20265] d_loss: 1.38671136, g_loss: 0.69464433\n",
      "Step: [20266] d_loss: 1.38761902, g_loss: 0.69402301\n",
      "Step: [20267] d_loss: 1.38773584, g_loss: 0.69255006\n",
      "Step: [20268] d_loss: 1.38725543, g_loss: 0.69131857\n",
      "Step: [20269] d_loss: 1.38674116, g_loss: 0.69231969\n",
      "Step: [20270] d_loss: 1.38642049, g_loss: 0.69251388\n",
      "Step: [20271] d_loss: 1.38632679, g_loss: 0.69268942\n",
      "Step: [20272] d_loss: 1.38629603, g_loss: 0.69294977\n",
      "Step: [20273] d_loss: 1.38627362, g_loss: 0.69470358\n",
      "Step: [20274] d_loss: 1.38630247, g_loss: 0.69445515\n",
      "Step: [20275] d_loss: 1.38632119, g_loss: 0.69343817\n",
      "Step: [20276] d_loss: 1.38628078, g_loss: 0.69193542\n",
      "Step: [20277] d_loss: 1.38627672, g_loss: 0.69279146\n",
      "Step: [20278] d_loss: 1.38627863, g_loss: 0.69360900\n",
      "Step: [20279] d_loss: 1.38627589, g_loss: 0.69331777\n",
      "Step: [20280] d_loss: 1.38626885, g_loss: 0.69332129\n",
      "Step: [20281] d_loss: 1.38626766, g_loss: 0.69358754\n",
      "Step: [20282] d_loss: 1.38626945, g_loss: 0.69301337\n",
      "Step: [20283] d_loss: 1.38633227, g_loss: 0.69372004\n",
      "Step: [20284] d_loss: 1.38627982, g_loss: 0.69324958\n",
      "Step: [20285] d_loss: 1.38628399, g_loss: 0.69328761\n",
      "Step: [20286] d_loss: 1.38628936, g_loss: 0.69333321\n",
      "Step: [20287] d_loss: 1.38625813, g_loss: 0.69276297\n",
      "Step: [20288] d_loss: 1.38626957, g_loss: 0.69326103\n",
      "Step: [20289] d_loss: 1.38625526, g_loss: 0.69364882\n",
      "Step: [20290] d_loss: 1.38655567, g_loss: 0.69375265\n",
      "Step: [20291] d_loss: 1.38656044, g_loss: 0.69210523\n",
      "Step: [20292] d_loss: 1.38649642, g_loss: 0.69051045\n",
      "Step: [20293] d_loss: 1.38639879, g_loss: 0.69237077\n",
      "Step: [20294] d_loss: 1.38629293, g_loss: 0.69366723\n",
      "Step: [20295] d_loss: 1.38628340, g_loss: 0.69402766\n",
      "Step: [20296] d_loss: 1.38626933, g_loss: 0.69389027\n",
      "Step: [20297] d_loss: 1.38626695, g_loss: 0.69332170\n",
      "Step: [20298] d_loss: 1.38626754, g_loss: 0.69283688\n",
      "Step: [20299] d_loss: 1.38626492, g_loss: 0.69328523\n",
      "Step: [20300] d_loss: 1.38626361, g_loss: 0.69307303\n",
      "Step: [20301] d_loss: 1.38630962, g_loss: 0.69389468\n",
      "Step: [20302] d_loss: 1.38627899, g_loss: 0.69306678\n",
      "Step: [20303] d_loss: 1.38627291, g_loss: 0.69263101\n",
      "Step: [20304] d_loss: 1.38627028, g_loss: 0.69331992\n",
      "Step: [20305] d_loss: 1.38625216, g_loss: 0.69326872\n",
      "Step: [20306] d_loss: 1.38623834, g_loss: 0.69333369\n",
      "Step: [20307] d_loss: 1.38624024, g_loss: 0.69338042\n",
      "Step: [20308] d_loss: 1.38626313, g_loss: 0.69343746\n",
      "Step: [20309] d_loss: 1.38678336, g_loss: 0.69518298\n",
      "Step: [20310] d_loss: 1.38629532, g_loss: 0.69083887\n",
      "Step: [20311] d_loss: 1.38630533, g_loss: 0.69444692\n",
      "Step: [20312] d_loss: 1.38632286, g_loss: 0.69496095\n",
      "Step: [20313] d_loss: 1.38633156, g_loss: 0.69404083\n",
      "Step: [20314] d_loss: 1.38629794, g_loss: 0.69301116\n",
      "Step: [20315] d_loss: 1.38630795, g_loss: 0.69367349\n",
      "Step: [20316] d_loss: 1.38628721, g_loss: 0.69297826\n",
      "Step: [20317] d_loss: 1.38631284, g_loss: 0.69379401\n",
      "Step: [20318] d_loss: 1.38629627, g_loss: 0.69314313\n",
      "Step: [20319] d_loss: 1.38627255, g_loss: 0.69287288\n",
      "Step: [20320] d_loss: 1.38622272, g_loss: 0.69389784\n",
      "Step: [20321] d_loss: 1.38626361, g_loss: 0.69433630\n",
      "Step: [20322] d_loss: 1.38630009, g_loss: 0.69351226\n",
      "Step: [20323] d_loss: 1.38624918, g_loss: 0.69267058\n",
      "Step: [20324] d_loss: 1.38625264, g_loss: 0.69283074\n",
      "Step: [20325] d_loss: 1.38626111, g_loss: 0.69329870\n",
      "Step: [20326] d_loss: 1.38623738, g_loss: 0.69330502\n",
      "Step: [20327] d_loss: 1.38626313, g_loss: 0.69361293\n",
      "Step: [20328] d_loss: 1.38626397, g_loss: 0.69329435\n",
      "Step: [20329] d_loss: 1.38622236, g_loss: 0.69283897\n",
      "Step: [20330] d_loss: 1.38622856, g_loss: 0.69321287\n",
      "Step: [20331] d_loss: 1.38622367, g_loss: 0.69337976\n",
      "Step: [20332] d_loss: 1.38627660, g_loss: 0.69353408\n",
      "Step: [20333] d_loss: 1.38623083, g_loss: 0.69309443\n",
      "Step: [20334] d_loss: 1.38626194, g_loss: 0.69352812\n",
      "Step: [20335] d_loss: 1.38594472, g_loss: 0.69373947\n",
      "Step: [20336] d_loss: 1.38621354, g_loss: 0.69229072\n",
      "Step: [20337] d_loss: 1.38622212, g_loss: 0.69322687\n",
      "Step: [20338] d_loss: 1.38620365, g_loss: 0.69408345\n",
      "Step: [20339] d_loss: 1.38610554, g_loss: 0.69372618\n",
      "Step: [20340] d_loss: 1.38643718, g_loss: 0.69318604\n",
      "Step: [20341] d_loss: 1.38517678, g_loss: 0.69777691\n",
      "Step: [20342] d_loss: 1.38632429, g_loss: 0.69143254\n",
      "Step: [20343] d_loss: 1.38632202, g_loss: 0.69202280\n",
      "Step: [20344] d_loss: 1.38628531, g_loss: 0.69403601\n",
      "Step: [20345] d_loss: 1.38629222, g_loss: 0.69430000\n",
      "Step: [20346] d_loss: 1.38628268, g_loss: 0.69365883\n",
      "Step: [20347] d_loss: 1.38627410, g_loss: 0.69259787\n",
      "Step: [20348] d_loss: 1.38628554, g_loss: 0.69251353\n",
      "Step: [20349] d_loss: 1.38627207, g_loss: 0.69305521\n",
      "Step: [20350] d_loss: 1.38628101, g_loss: 0.69352156\n",
      "Step: [20351] d_loss: 1.38625455, g_loss: 0.69352722\n",
      "Step: [20352] d_loss: 1.38623691, g_loss: 0.69387031\n",
      "Step: [20353] d_loss: 1.38623297, g_loss: 0.69338667\n",
      "Step: [20354] d_loss: 1.38628602, g_loss: 0.69280076\n",
      "Step: [20355] d_loss: 1.38633919, g_loss: 0.69238991\n",
      "Step: [20356] d_loss: 1.38639855, g_loss: 0.69219881\n",
      "Step: [20357] d_loss: 1.38640189, g_loss: 0.69343460\n",
      "Step: [20358] d_loss: 1.38638163, g_loss: 0.69428635\n",
      "Step: [20359] d_loss: 1.38657582, g_loss: 0.69414341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20360] d_loss: 1.38697314, g_loss: 0.69470251\n",
      "Step: [20361] d_loss: 1.38726056, g_loss: 0.69432187\n",
      "Step: [20362] d_loss: 1.38715768, g_loss: 0.69528264\n",
      "Step: [20363] d_loss: 1.38684607, g_loss: 0.69391346\n",
      "Step: [20364] d_loss: 1.38711667, g_loss: 0.69053286\n",
      "Step: [20365] d_loss: 1.38724029, g_loss: 0.68930638\n",
      "Step: [20366] d_loss: 1.38694847, g_loss: 0.69029284\n",
      "Step: [20367] d_loss: 1.38665318, g_loss: 0.69405979\n",
      "Step: [20368] d_loss: 1.38648820, g_loss: 0.69526207\n",
      "Step: [20369] d_loss: 1.38637209, g_loss: 0.69180739\n",
      "Step: [20370] d_loss: 1.38629508, g_loss: 0.69127637\n",
      "Step: [20371] d_loss: 1.38629127, g_loss: 0.69306040\n",
      "Step: [20372] d_loss: 1.38628006, g_loss: 0.69212943\n",
      "Step: [20373] d_loss: 1.38626289, g_loss: 0.69272488\n",
      "Step: [20374] d_loss: 1.38625062, g_loss: 0.69393909\n",
      "Step: [20375] d_loss: 1.38626349, g_loss: 0.69398761\n",
      "Step: [20376] d_loss: 1.38627207, g_loss: 0.69368380\n",
      "Step: [20377] d_loss: 1.38624883, g_loss: 0.69303024\n",
      "Step: [20378] d_loss: 1.38631380, g_loss: 0.69313276\n",
      "Step: [20379] d_loss: 1.38634729, g_loss: 0.69240844\n",
      "Step: [20380] d_loss: 1.38640904, g_loss: 0.69307327\n",
      "Step: [20381] d_loss: 1.38640618, g_loss: 0.69316483\n",
      "Step: [20382] d_loss: 1.38637948, g_loss: 0.69393492\n",
      "Step: [20383] d_loss: 1.38632965, g_loss: 0.69331557\n",
      "Step: [20384] d_loss: 1.38628614, g_loss: 0.69308126\n",
      "Step: [20385] d_loss: 1.38629699, g_loss: 0.69313610\n",
      "Step: [20386] d_loss: 1.38626516, g_loss: 0.69316500\n",
      "Step: [20387] d_loss: 1.38627696, g_loss: 0.69349146\n",
      "Step: [20388] d_loss: 1.38627172, g_loss: 0.69325948\n",
      "Step: [20389] d_loss: 1.38627124, g_loss: 0.69312048\n",
      "Step: [20390] d_loss: 1.38629270, g_loss: 0.69299030\n",
      "Step: [20391] d_loss: 1.38626885, g_loss: 0.69322890\n",
      "Step: [20392] d_loss: 1.38629127, g_loss: 0.69376022\n",
      "Step: [20393] d_loss: 1.38631654, g_loss: 0.69320768\n",
      "Step: [20394] d_loss: 1.38630140, g_loss: 0.69314408\n",
      "Step: [20395] d_loss: 1.38629937, g_loss: 0.69303709\n",
      "Step: [20396] d_loss: 1.38632894, g_loss: 0.69359165\n",
      "Step: [20397] d_loss: 1.38632226, g_loss: 0.69315910\n",
      "Step: [20398] d_loss: 1.38631248, g_loss: 0.69308543\n",
      "Step: [20399] d_loss: 1.38630164, g_loss: 0.69300044\n",
      "Step: [20400] d_loss: 1.38629532, g_loss: 0.69304317\n",
      "Step: [20401] d_loss: 1.38630140, g_loss: 0.69342351\n",
      "Step: [20402] d_loss: 1.38631725, g_loss: 0.69327044\n",
      "Step: [20403] d_loss: 1.38630354, g_loss: 0.69312185\n",
      "Step: [20404] d_loss: 1.38608968, g_loss: 0.69415283\n",
      "Step: [20405] d_loss: 1.38630748, g_loss: 0.69240332\n",
      "Step: [20406] d_loss: 1.38632798, g_loss: 0.69364983\n",
      "Step: [20407] d_loss: 1.38634133, g_loss: 0.69367558\n",
      "Step: [20408] d_loss: 1.38634682, g_loss: 0.69186610\n",
      "Step: [20409] d_loss: 1.38632298, g_loss: 0.69217896\n",
      "Step: [20410] d_loss: 1.38631654, g_loss: 0.69257486\n",
      "Step: [20411] d_loss: 1.38630104, g_loss: 0.69379044\n",
      "Step: [20412] d_loss: 1.38632643, g_loss: 0.69465035\n",
      "Step: [20413] d_loss: 1.38632894, g_loss: 0.69365704\n",
      "Step: [20414] d_loss: 1.38632703, g_loss: 0.69241261\n",
      "Step: [20415] d_loss: 1.38630724, g_loss: 0.69241017\n",
      "Step: [20416] d_loss: 1.38629913, g_loss: 0.69283628\n",
      "Step: [20417] d_loss: 1.38630986, g_loss: 0.69319177\n",
      "Step: [20418] d_loss: 1.38631701, g_loss: 0.69393396\n",
      "Step: [20419] d_loss: 1.38631129, g_loss: 0.69401538\n",
      "Step: [20420] d_loss: 1.38632476, g_loss: 0.69423658\n",
      "Step: [20421] d_loss: 1.38634861, g_loss: 0.69396907\n",
      "Step: [20422] d_loss: 1.38634539, g_loss: 0.69194233\n",
      "Step: [20423] d_loss: 1.38634384, g_loss: 0.69280678\n",
      "Step: [20424] d_loss: 1.38633895, g_loss: 0.69404316\n",
      "Step: [20425] d_loss: 1.38635743, g_loss: 0.69489056\n",
      "Step: [20426] d_loss: 1.38638043, g_loss: 0.69390523\n",
      "Step: [20427] d_loss: 1.38636148, g_loss: 0.69284940\n",
      "Step: [20428] d_loss: 1.38634741, g_loss: 0.69243187\n",
      "Step: [20429] d_loss: 1.38646448, g_loss: 0.69297183\n",
      "Step: [20430] d_loss: 1.38630426, g_loss: 0.69243562\n",
      "Step: [20431] d_loss: 1.38631463, g_loss: 0.69399136\n",
      "Step: [20432] d_loss: 1.38635254, g_loss: 0.69342434\n",
      "Step: [20433] d_loss: 1.38633120, g_loss: 0.69317985\n",
      "Step: [20434] d_loss: 1.38631332, g_loss: 0.69292819\n",
      "Step: [20435] d_loss: 1.38630581, g_loss: 0.69313920\n",
      "Step: [20436] d_loss: 1.38630104, g_loss: 0.69323778\n",
      "Step: [20437] d_loss: 1.38630724, g_loss: 0.69358909\n",
      "Step: [20438] d_loss: 1.38631177, g_loss: 0.69394708\n",
      "Step: [20439] d_loss: 1.38631999, g_loss: 0.69364744\n",
      "Step: [20440] d_loss: 1.38632834, g_loss: 0.69301760\n",
      "Step: [20441] d_loss: 1.38633049, g_loss: 0.69321084\n",
      "Step: [20442] d_loss: 1.38633752, g_loss: 0.69306314\n",
      "Step: [20443] d_loss: 1.38632512, g_loss: 0.69353306\n",
      "Step: [20444] d_loss: 1.38632274, g_loss: 0.69334579\n",
      "Step: [20445] d_loss: 1.38631833, g_loss: 0.69354498\n",
      "Step: [20446] d_loss: 1.38632870, g_loss: 0.69373822\n",
      "Step: [20447] d_loss: 1.38633502, g_loss: 0.69347233\n",
      "Step: [20448] d_loss: 1.38633549, g_loss: 0.69367337\n",
      "Step: [20449] d_loss: 1.38636351, g_loss: 0.69348431\n",
      "Step: [20450] d_loss: 1.38637280, g_loss: 0.69273019\n",
      "Step: [20451] d_loss: 1.38636851, g_loss: 0.69284880\n",
      "Step: [20452] d_loss: 1.38637257, g_loss: 0.69274920\n",
      "Step: [20453] d_loss: 1.38633966, g_loss: 0.69294798\n",
      "Step: [20454] d_loss: 1.38632083, g_loss: 0.69255328\n",
      "Step: [20455] d_loss: 1.38630593, g_loss: 0.69297743\n",
      "Step: [20456] d_loss: 1.38631427, g_loss: 0.69298387\n",
      "Step: [20457] d_loss: 1.38629723, g_loss: 0.69324851\n",
      "Step: [20458] d_loss: 1.38630152, g_loss: 0.69369924\n",
      "Step: [20459] d_loss: 1.38629746, g_loss: 0.69265389\n",
      "Step: [20460] d_loss: 1.38630295, g_loss: 0.69319701\n",
      "Step: [20461] d_loss: 1.38631296, g_loss: 0.69340819\n",
      "Step: [20462] d_loss: 1.38631177, g_loss: 0.69316339\n",
      "Step: [20463] d_loss: 1.38630319, g_loss: 0.69299048\n",
      "Step: [20464] d_loss: 1.38629973, g_loss: 0.69294930\n",
      "Step: [20465] d_loss: 1.38630033, g_loss: 0.69317186\n",
      "Step: [20466] d_loss: 1.38629484, g_loss: 0.69323152\n",
      "Step: [20467] d_loss: 1.38631201, g_loss: 0.69331443\n",
      "Step: [20468] d_loss: 1.38629866, g_loss: 0.69384730\n",
      "Step: [20469] d_loss: 1.38630199, g_loss: 0.69297057\n",
      "Step: [20470] d_loss: 1.38631296, g_loss: 0.69242340\n",
      "Step: [20471] d_loss: 1.38629580, g_loss: 0.69256091\n",
      "Step: [20472] d_loss: 1.38629925, g_loss: 0.69305599\n",
      "Step: [20473] d_loss: 1.38629913, g_loss: 0.69318759\n",
      "Step: [20474] d_loss: 1.38628149, g_loss: 0.69330400\n",
      "Step: [20475] d_loss: 1.38749015, g_loss: 0.69611681\n",
      "Step: [20476] d_loss: 1.38629985, g_loss: 0.69262564\n",
      "Step: [20477] d_loss: 1.38631201, g_loss: 0.69334310\n",
      "Step: [20478] d_loss: 1.38632774, g_loss: 0.69407296\n",
      "Step: [20479] d_loss: 1.38634586, g_loss: 0.69262159\n",
      "Step: [20480] d_loss: 1.38634610, g_loss: 0.69206184\n",
      "Step: [20481] d_loss: 1.38620901, g_loss: 0.69370437\n",
      "Step: [20482] d_loss: 1.38637030, g_loss: 0.69311178\n",
      "Step: [20483] d_loss: 1.38637853, g_loss: 0.69374806\n",
      "Step: [20484] d_loss: 1.38637078, g_loss: 0.69305420\n",
      "Step: [20485] d_loss: 1.38638544, g_loss: 0.69365609\n",
      "Step: [20486] d_loss: 1.38636684, g_loss: 0.69283313\n",
      "Step: [20487] d_loss: 1.38636661, g_loss: 0.69231367\n",
      "Step: [20488] d_loss: 1.38634694, g_loss: 0.69263625\n",
      "Step: [20489] d_loss: 1.38633287, g_loss: 0.69273955\n",
      "Step: [20490] d_loss: 1.38631821, g_loss: 0.69271767\n",
      "Step: [20491] d_loss: 1.38631248, g_loss: 0.69333059\n",
      "Step: [20492] d_loss: 1.38629913, g_loss: 0.69326574\n",
      "Step: [20493] d_loss: 1.38630247, g_loss: 0.69300807\n",
      "Step: [20494] d_loss: 1.38629496, g_loss: 0.69286144\n",
      "Step: [20495] d_loss: 1.38628840, g_loss: 0.69302952\n",
      "Step: [20496] d_loss: 1.38628769, g_loss: 0.69320017\n",
      "Step: [20497] d_loss: 1.38629806, g_loss: 0.69327658\n",
      "Step: [20498] d_loss: 1.38629472, g_loss: 0.69317365\n",
      "Step: [20499] d_loss: 1.38628602, g_loss: 0.69299853\n",
      "Step: [20500] d_loss: 1.38629103, g_loss: 0.69310057\n",
      "Step: [20501] d_loss: 1.38629401, g_loss: 0.69340944\n",
      "Step: [20502] d_loss: 1.38629699, g_loss: 0.69345737\n",
      "Step: [20503] d_loss: 1.38629937, g_loss: 0.69315982\n",
      "Step: [20504] d_loss: 1.38630366, g_loss: 0.69290179\n",
      "Step: [20505] d_loss: 1.38630319, g_loss: 0.69302976\n",
      "Step: [20506] d_loss: 1.38630056, g_loss: 0.69289923\n",
      "Step: [20507] d_loss: 1.38629448, g_loss: 0.69276160\n",
      "Step: [20508] d_loss: 1.38629401, g_loss: 0.69302940\n",
      "Step: [20509] d_loss: 1.38628113, g_loss: 0.69330204\n",
      "Step: [20510] d_loss: 1.38630092, g_loss: 0.69355285\n",
      "Step: [20511] d_loss: 1.38633335, g_loss: 0.69394839\n",
      "Step: [20512] d_loss: 1.38635767, g_loss: 0.69439077\n",
      "Step: [20513] d_loss: 1.38638008, g_loss: 0.69410640\n",
      "Step: [20514] d_loss: 1.38635254, g_loss: 0.69296467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20515] d_loss: 1.38640010, g_loss: 0.69222009\n",
      "Step: [20516] d_loss: 1.38639820, g_loss: 0.69243211\n",
      "Step: [20517] d_loss: 1.38640165, g_loss: 0.69379473\n",
      "Step: [20518] d_loss: 1.38639390, g_loss: 0.69336933\n",
      "Step: [20519] d_loss: 1.38639331, g_loss: 0.69349509\n",
      "Step: [20520] d_loss: 1.38639176, g_loss: 0.69314384\n",
      "Step: [20521] d_loss: 1.38674188, g_loss: 0.69438112\n",
      "Step: [20522] d_loss: 1.38644266, g_loss: 0.69336993\n",
      "Step: [20523] d_loss: 1.38649011, g_loss: 0.69413692\n",
      "Step: [20524] d_loss: 1.38651776, g_loss: 0.69298184\n",
      "Step: [20525] d_loss: 1.38653982, g_loss: 0.69311810\n",
      "Step: [20526] d_loss: 1.38655710, g_loss: 0.69303566\n",
      "Step: [20527] d_loss: 1.38655376, g_loss: 0.69418991\n",
      "Step: [20528] d_loss: 1.38656652, g_loss: 0.69362998\n",
      "Step: [20529] d_loss: 1.38655543, g_loss: 0.69102854\n",
      "Step: [20530] d_loss: 1.38655162, g_loss: 0.69266474\n",
      "Step: [20531] d_loss: 1.38652873, g_loss: 0.69410896\n",
      "Step: [20532] d_loss: 1.38652039, g_loss: 0.69353354\n",
      "Step: [20533] d_loss: 1.38650513, g_loss: 0.69442958\n",
      "Step: [20534] d_loss: 1.38650548, g_loss: 0.69338143\n",
      "Step: [20535] d_loss: 1.38650775, g_loss: 0.69362617\n",
      "Step: [20536] d_loss: 1.38650799, g_loss: 0.69247085\n",
      "Step: [20537] d_loss: 1.38648081, g_loss: 0.69302595\n",
      "Step: [20538] d_loss: 1.38646412, g_loss: 0.69332373\n",
      "Step: [20539] d_loss: 1.38643527, g_loss: 0.69412029\n",
      "Step: [20540] d_loss: 1.38679278, g_loss: 0.69294018\n",
      "Step: [20541] d_loss: 1.38637590, g_loss: 0.69240069\n",
      "Step: [20542] d_loss: 1.38671076, g_loss: 0.69274503\n",
      "Step: [20543] d_loss: 1.38702321, g_loss: 0.68344325\n",
      "Step: [20544] d_loss: 1.38718629, g_loss: 0.68910170\n",
      "Step: [20545] d_loss: 1.38711834, g_loss: 0.69599378\n",
      "Step: [20546] d_loss: 1.38707960, g_loss: 0.69915473\n",
      "Step: [20547] d_loss: 1.38695598, g_loss: 0.69687986\n",
      "Step: [20548] d_loss: 1.38677359, g_loss: 0.69326991\n",
      "Step: [20549] d_loss: 1.38764179, g_loss: 0.69393957\n",
      "Step: [20550] d_loss: 1.38844109, g_loss: 0.69150889\n",
      "Step: [20551] d_loss: 1.38834953, g_loss: 0.69429314\n",
      "Step: [20552] d_loss: 1.38763332, g_loss: 0.69199359\n",
      "Step: [20553] d_loss: 1.38680840, g_loss: 0.68710661\n",
      "Step: [20554] d_loss: 1.38635182, g_loss: 0.69015640\n",
      "Step: [20555] d_loss: 1.38633060, g_loss: 0.69414258\n",
      "Step: [20556] d_loss: 1.38642895, g_loss: 0.69577223\n",
      "Step: [20557] d_loss: 1.38647938, g_loss: 0.69487107\n",
      "Step: [20558] d_loss: 1.38644099, g_loss: 0.69361025\n",
      "Step: [20559] d_loss: 1.38637316, g_loss: 0.69228667\n",
      "Step: [20560] d_loss: 1.38631678, g_loss: 0.69232917\n",
      "Step: [20561] d_loss: 1.38629723, g_loss: 0.69304234\n",
      "Step: [20562] d_loss: 1.38629162, g_loss: 0.69350493\n",
      "Step: [20563] d_loss: 1.38629818, g_loss: 0.69341743\n",
      "Step: [20564] d_loss: 1.38629675, g_loss: 0.69308484\n",
      "Step: [20565] d_loss: 1.38629150, g_loss: 0.69264877\n",
      "Step: [20566] d_loss: 1.38628078, g_loss: 0.69305122\n",
      "Step: [20567] d_loss: 1.38629675, g_loss: 0.69296050\n",
      "Step: [20568] d_loss: 1.38630390, g_loss: 0.69303268\n",
      "Step: [20569] d_loss: 1.38629687, g_loss: 0.69364953\n",
      "Step: [20570] d_loss: 1.38629413, g_loss: 0.69400191\n",
      "Step: [20571] d_loss: 1.38631868, g_loss: 0.69333112\n",
      "Step: [20572] d_loss: 1.38631773, g_loss: 0.69336987\n",
      "Step: [20573] d_loss: 1.38632226, g_loss: 0.69339144\n",
      "Step: [20574] d_loss: 1.38631773, g_loss: 0.69305015\n",
      "Step: [20575] d_loss: 1.38630128, g_loss: 0.69278753\n",
      "Step: [20576] d_loss: 1.38629127, g_loss: 0.69290006\n",
      "Step: [20577] d_loss: 1.38629436, g_loss: 0.69342899\n",
      "Step: [20578] d_loss: 1.38632369, g_loss: 0.69302213\n",
      "Step: [20579] d_loss: 1.38630319, g_loss: 0.69280899\n",
      "Step: [20580] d_loss: 1.38630199, g_loss: 0.69344974\n",
      "Step: [20581] d_loss: 1.38630044, g_loss: 0.69321436\n",
      "Step: [20582] d_loss: 1.38628662, g_loss: 0.69321513\n",
      "Step: [20583] d_loss: 1.38628232, g_loss: 0.69335675\n",
      "Step: [20584] d_loss: 1.38629556, g_loss: 0.69324523\n",
      "Step: [20585] d_loss: 1.38629770, g_loss: 0.69341010\n",
      "Step: [20586] d_loss: 1.38629806, g_loss: 0.69310194\n",
      "Step: [20587] d_loss: 1.38629460, g_loss: 0.69305575\n",
      "Step: [20588] d_loss: 1.38628662, g_loss: 0.69305491\n",
      "Step: [20589] d_loss: 1.38628912, g_loss: 0.69317496\n",
      "Step: [20590] d_loss: 1.38627362, g_loss: 0.69319952\n",
      "Step: [20591] d_loss: 1.38629556, g_loss: 0.69437194\n",
      "Step: [20592] d_loss: 1.38630438, g_loss: 0.69237792\n",
      "Step: [20593] d_loss: 1.38631535, g_loss: 0.69319105\n",
      "Step: [20594] d_loss: 1.38632452, g_loss: 0.69463706\n",
      "Step: [20595] d_loss: 1.38633752, g_loss: 0.69461811\n",
      "Step: [20596] d_loss: 1.38635373, g_loss: 0.69372463\n",
      "Step: [20597] d_loss: 1.38633084, g_loss: 0.69054437\n",
      "Step: [20598] d_loss: 1.38630164, g_loss: 0.69100058\n",
      "Step: [20599] d_loss: 1.38621354, g_loss: 0.69307518\n",
      "Step: [20600] d_loss: 1.38629341, g_loss: 0.69250894\n",
      "Step: [20601] d_loss: 1.38630772, g_loss: 0.69371665\n",
      "Step: [20602] d_loss: 1.38631034, g_loss: 0.69369984\n",
      "Step: [20603] d_loss: 1.38629913, g_loss: 0.69259655\n",
      "Step: [20604] d_loss: 1.38628829, g_loss: 0.69270682\n",
      "Step: [20605] d_loss: 1.38628757, g_loss: 0.69314122\n",
      "Step: [20606] d_loss: 1.38628888, g_loss: 0.69331110\n",
      "Step: [20607] d_loss: 1.38627744, g_loss: 0.69335532\n",
      "Step: [20608] d_loss: 1.38628507, g_loss: 0.69347721\n",
      "Step: [20609] d_loss: 1.38629055, g_loss: 0.69339705\n",
      "Step: [20610] d_loss: 1.38631558, g_loss: 0.69389135\n",
      "Step: [20611] d_loss: 1.38633728, g_loss: 0.69398063\n",
      "Step: [20612] d_loss: 1.38637245, g_loss: 0.69350529\n",
      "Step: [20613] d_loss: 1.38636351, g_loss: 0.69367838\n",
      "Step: [20614] d_loss: 1.38637257, g_loss: 0.69358277\n",
      "Step: [20615] d_loss: 1.38635564, g_loss: 0.69332319\n",
      "Step: [20616] d_loss: 1.38634253, g_loss: 0.69288737\n",
      "Step: [20617] d_loss: 1.38630784, g_loss: 0.69182533\n",
      "Step: [20618] d_loss: 1.38628316, g_loss: 0.69201750\n",
      "Step: [20619] d_loss: 1.38628030, g_loss: 0.69284552\n",
      "Step: [20620] d_loss: 1.38627839, g_loss: 0.69352674\n",
      "Step: [20621] d_loss: 1.38628209, g_loss: 0.69347680\n",
      "Step: [20622] d_loss: 1.38627923, g_loss: 0.69317251\n",
      "Step: [20623] d_loss: 1.38628197, g_loss: 0.69311184\n",
      "Step: [20624] d_loss: 1.38627708, g_loss: 0.69297475\n",
      "Step: [20625] d_loss: 1.38626838, g_loss: 0.69299406\n",
      "Step: [20626] d_loss: 1.38628435, g_loss: 0.69355011\n",
      "Step: [20627] d_loss: 1.38638139, g_loss: 0.69322622\n",
      "Step: [20628] d_loss: 1.38642931, g_loss: 0.69236755\n",
      "Step: [20629] d_loss: 1.38675880, g_loss: 0.69189894\n",
      "Step: [20630] d_loss: 1.38682401, g_loss: 0.69199234\n",
      "Step: [20631] d_loss: 1.38678646, g_loss: 0.69494176\n",
      "Step: [20632] d_loss: 1.38673830, g_loss: 0.69428527\n",
      "Step: [20633] d_loss: 1.38665426, g_loss: 0.69481832\n",
      "Step: [20634] d_loss: 1.38656449, g_loss: 0.69322133\n",
      "Step: [20635] d_loss: 1.38647342, g_loss: 0.69318873\n",
      "Step: [20636] d_loss: 1.38642073, g_loss: 0.69408977\n",
      "Step: [20637] d_loss: 1.38637519, g_loss: 0.69330579\n",
      "Step: [20638] d_loss: 1.38633144, g_loss: 0.69256687\n",
      "Step: [20639] d_loss: 1.38629508, g_loss: 0.69287896\n",
      "Step: [20640] d_loss: 1.38628173, g_loss: 0.69298941\n",
      "Step: [20641] d_loss: 1.38630533, g_loss: 0.69328499\n",
      "Step: [20642] d_loss: 1.38627338, g_loss: 0.69333482\n",
      "Step: [20643] d_loss: 1.38627505, g_loss: 0.69332254\n",
      "Step: [20644] d_loss: 1.38627410, g_loss: 0.69336343\n",
      "Step: [20645] d_loss: 1.38628292, g_loss: 0.69331616\n",
      "Step: [20646] d_loss: 1.38630271, g_loss: 0.69315553\n",
      "Step: [20647] d_loss: 1.38631010, g_loss: 0.69237304\n",
      "Step: [20648] d_loss: 1.38628018, g_loss: 0.69236982\n",
      "Step: [20649] d_loss: 1.38627696, g_loss: 0.69306171\n",
      "Step: [20650] d_loss: 1.38627505, g_loss: 0.69345164\n",
      "Step: [20651] d_loss: 1.38627911, g_loss: 0.69374400\n",
      "Step: [20652] d_loss: 1.38629985, g_loss: 0.69362545\n",
      "Step: [20653] d_loss: 1.38632393, g_loss: 0.69382602\n",
      "Step: [20654] d_loss: 1.38634324, g_loss: 0.69287211\n",
      "Step: [20655] d_loss: 1.38635194, g_loss: 0.69364262\n",
      "Step: [20656] d_loss: 1.38635015, g_loss: 0.69405973\n",
      "Step: [20657] d_loss: 1.38698196, g_loss: 0.69621587\n",
      "Step: [20658] d_loss: 1.38628626, g_loss: 0.69228852\n",
      "Step: [20659] d_loss: 1.38629317, g_loss: 0.69284344\n",
      "Step: [20660] d_loss: 1.38628781, g_loss: 0.69336689\n",
      "Step: [20661] d_loss: 1.38627028, g_loss: 0.69351566\n",
      "Step: [20662] d_loss: 1.38628054, g_loss: 0.69341862\n",
      "Step: [20663] d_loss: 1.38627756, g_loss: 0.69319534\n",
      "Step: [20664] d_loss: 1.38627946, g_loss: 0.69298589\n",
      "Step: [20665] d_loss: 1.38616180, g_loss: 0.69309652\n",
      "Step: [20666] d_loss: 1.38628149, g_loss: 0.69266170\n",
      "Step: [20667] d_loss: 1.38628578, g_loss: 0.69309866\n",
      "Step: [20668] d_loss: 1.38627934, g_loss: 0.69300181\n",
      "Step: [20669] d_loss: 1.38627195, g_loss: 0.69317973\n",
      "Step: [20670] d_loss: 1.38627768, g_loss: 0.69301707\n",
      "Step: [20671] d_loss: 1.38567913, g_loss: 0.69452548\n",
      "Step: [20672] d_loss: 1.38627934, g_loss: 0.69283992\n",
      "Step: [20673] d_loss: 1.38627970, g_loss: 0.69306618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20674] d_loss: 1.38628471, g_loss: 0.69290251\n",
      "Step: [20675] d_loss: 1.38625944, g_loss: 0.69316399\n",
      "Step: [20676] d_loss: 1.38625526, g_loss: 0.69301927\n",
      "Step: [20677] d_loss: 1.38626420, g_loss: 0.69326645\n",
      "Step: [20678] d_loss: 1.38627326, g_loss: 0.69321299\n",
      "Step: [20679] d_loss: 1.38627076, g_loss: 0.69366992\n",
      "Step: [20680] d_loss: 1.38630319, g_loss: 0.69226372\n",
      "Step: [20681] d_loss: 1.38636255, g_loss: 0.69301522\n",
      "Step: [20682] d_loss: 1.38632560, g_loss: 0.69336975\n",
      "Step: [20683] d_loss: 1.38637853, g_loss: 0.69312835\n",
      "Step: [20684] d_loss: 1.38638091, g_loss: 0.69271863\n",
      "Step: [20685] d_loss: 1.38635707, g_loss: 0.69278127\n",
      "Step: [20686] d_loss: 1.38632083, g_loss: 0.69297701\n",
      "Step: [20687] d_loss: 1.38629806, g_loss: 0.69343865\n",
      "Step: [20688] d_loss: 1.38628697, g_loss: 0.69278854\n",
      "Step: [20689] d_loss: 1.38626611, g_loss: 0.69349164\n",
      "Step: [20690] d_loss: 1.38628089, g_loss: 0.69354975\n",
      "Step: [20691] d_loss: 1.38628221, g_loss: 0.69392461\n",
      "Step: [20692] d_loss: 1.38633108, g_loss: 0.69374645\n",
      "Step: [20693] d_loss: 1.38634539, g_loss: 0.69285142\n",
      "Step: [20694] d_loss: 1.38628435, g_loss: 0.69247353\n",
      "Step: [20695] d_loss: 1.38625157, g_loss: 0.69325411\n",
      "Step: [20696] d_loss: 1.38625729, g_loss: 0.69332981\n",
      "Step: [20697] d_loss: 1.38627088, g_loss: 0.69287765\n",
      "Step: [20698] d_loss: 1.38610303, g_loss: 0.69487983\n",
      "Step: [20699] d_loss: 1.38628554, g_loss: 0.69336104\n",
      "Step: [20700] d_loss: 1.38630760, g_loss: 0.69275868\n",
      "Step: [20701] d_loss: 1.38631606, g_loss: 0.69271028\n",
      "Step: [20702] d_loss: 1.38630807, g_loss: 0.69282317\n",
      "Step: [20703] d_loss: 1.38628674, g_loss: 0.69379455\n",
      "Step: [20704] d_loss: 1.38629878, g_loss: 0.69319201\n",
      "Step: [20705] d_loss: 1.38628924, g_loss: 0.69289273\n",
      "Step: [20706] d_loss: 1.38630843, g_loss: 0.69316626\n",
      "Step: [20707] d_loss: 1.38628697, g_loss: 0.69323391\n",
      "Step: [20708] d_loss: 1.38627529, g_loss: 0.69310701\n",
      "Step: [20709] d_loss: 1.38625431, g_loss: 0.69318676\n",
      "Step: [20710] d_loss: 1.38605261, g_loss: 0.69513744\n",
      "Step: [20711] d_loss: 1.38634133, g_loss: 0.69370693\n",
      "Step: [20712] d_loss: 1.38644171, g_loss: 0.69403064\n",
      "Step: [20713] d_loss: 1.38651860, g_loss: 0.69419569\n",
      "Step: [20714] d_loss: 1.38655972, g_loss: 0.69406986\n",
      "Step: [20715] d_loss: 1.38600683, g_loss: 0.69459665\n",
      "Step: [20716] d_loss: 1.38639855, g_loss: 0.68957436\n",
      "Step: [20717] d_loss: 1.38635159, g_loss: 0.69270420\n",
      "Step: [20718] d_loss: 1.40854180, g_loss: 0.69766170\n",
      "Step: [20719] d_loss: 1.38655782, g_loss: 0.69336176\n",
      "Step: [20720] d_loss: 1.38659334, g_loss: 0.69349253\n",
      "Step: [20721] d_loss: 1.38685989, g_loss: 0.69167250\n",
      "Step: [20722] d_loss: 1.38642478, g_loss: 0.69164348\n",
      "Step: [20723] d_loss: 1.38641858, g_loss: 0.69328415\n",
      "Step: [20724] d_loss: 1.38635683, g_loss: 0.69392085\n",
      "Step: [20725] d_loss: 1.38632393, g_loss: 0.69376111\n",
      "Step: [20726] d_loss: 1.38636684, g_loss: 0.69316268\n",
      "Step: [20727] d_loss: 1.38634014, g_loss: 0.69313699\n",
      "Step: [20728] d_loss: 1.38631511, g_loss: 0.69298911\n",
      "Step: [20729] d_loss: 1.38635051, g_loss: 0.69232130\n",
      "Step: [20730] d_loss: 1.38630533, g_loss: 0.69281107\n",
      "Step: [20731] d_loss: 1.38631558, g_loss: 0.69317704\n",
      "Step: [20732] d_loss: 1.38598216, g_loss: 0.69481897\n",
      "Step: [20733] d_loss: 1.38631129, g_loss: 0.69288337\n",
      "Step: [20734] d_loss: 1.38698983, g_loss: 0.69440502\n",
      "Step: [20735] d_loss: 1.38632643, g_loss: 0.69295114\n",
      "Step: [20736] d_loss: 1.38629079, g_loss: 0.69500977\n",
      "Step: [20737] d_loss: 1.38632345, g_loss: 0.69353354\n",
      "Step: [20738] d_loss: 1.38645720, g_loss: 0.69244373\n",
      "Step: [20739] d_loss: 1.38668084, g_loss: 0.69383180\n",
      "Step: [20740] d_loss: 1.38684082, g_loss: 0.69355780\n",
      "Step: [20741] d_loss: 1.38685441, g_loss: 0.69164789\n",
      "Step: [20742] d_loss: 1.38680720, g_loss: 0.69094074\n",
      "Step: [20743] d_loss: 1.38670301, g_loss: 0.68932521\n",
      "Step: [20744] d_loss: 1.38652301, g_loss: 0.69087517\n",
      "Step: [20745] d_loss: 1.38642383, g_loss: 0.69280565\n",
      "Step: [20746] d_loss: 1.38633895, g_loss: 0.69425827\n",
      "Step: [20747] d_loss: 1.38631415, g_loss: 0.69474030\n",
      "Step: [20748] d_loss: 1.38631833, g_loss: 0.69376707\n",
      "Step: [20749] d_loss: 1.38627505, g_loss: 0.69379675\n",
      "Step: [20750] d_loss: 1.38631034, g_loss: 0.69220233\n",
      "Step: [20751] d_loss: 1.38629985, g_loss: 0.69221443\n",
      "Step: [20752] d_loss: 1.38631105, g_loss: 0.69339120\n",
      "Step: [20753] d_loss: 1.38629019, g_loss: 0.69414741\n",
      "Step: [20754] d_loss: 1.38630915, g_loss: 0.69388723\n",
      "Step: [20755] d_loss: 1.38628602, g_loss: 0.69332492\n",
      "Step: [20756] d_loss: 1.38629365, g_loss: 0.69354069\n",
      "Step: [20757] d_loss: 1.38627398, g_loss: 0.69356751\n",
      "Step: [20758] d_loss: 1.38642037, g_loss: 0.69156289\n",
      "Step: [20759] d_loss: 1.38650250, g_loss: 0.69269180\n",
      "Step: [20760] d_loss: 1.38667262, g_loss: 0.69423741\n",
      "Step: [20761] d_loss: 1.38674664, g_loss: 0.69438314\n",
      "Step: [20762] d_loss: 1.38654208, g_loss: 0.69234800\n",
      "Step: [20763] d_loss: 1.38662553, g_loss: 0.68977606\n",
      "Step: [20764] d_loss: 1.38653064, g_loss: 0.69190264\n",
      "Step: [20765] d_loss: 1.38643885, g_loss: 0.69431055\n",
      "Step: [20766] d_loss: 1.38736153, g_loss: 0.69712830\n",
      "Step: [20767] d_loss: 1.38631773, g_loss: 0.69204628\n",
      "Step: [20768] d_loss: 1.38633180, g_loss: 0.69265598\n",
      "Step: [20769] d_loss: 1.38632154, g_loss: 0.69353211\n",
      "Step: [20770] d_loss: 1.38634372, g_loss: 0.69445598\n",
      "Step: [20771] d_loss: 1.38633060, g_loss: 0.69424874\n",
      "Step: [20772] d_loss: 1.38645029, g_loss: 0.69614631\n",
      "Step: [20773] d_loss: 1.38667941, g_loss: 0.69329774\n",
      "Step: [20774] d_loss: 1.38698077, g_loss: 0.69251305\n",
      "Step: [20775] d_loss: 1.38692629, g_loss: 0.69291359\n",
      "Step: [20776] d_loss: 1.38683808, g_loss: 0.69406444\n",
      "Step: [20777] d_loss: 1.38670075, g_loss: 0.69834012\n",
      "Step: [20778] d_loss: 1.38658381, g_loss: 0.69664419\n",
      "Step: [20779] d_loss: 1.38699317, g_loss: 0.69381857\n",
      "Step: [20780] d_loss: 1.38644385, g_loss: 0.68727720\n",
      "Step: [20781] d_loss: 1.38639259, g_loss: 0.68868840\n",
      "Step: [20782] d_loss: 1.38643718, g_loss: 0.69217986\n",
      "Step: [20783] d_loss: 1.38635540, g_loss: 0.69258189\n",
      "Step: [20784] d_loss: 1.38633072, g_loss: 0.69338179\n",
      "Step: [20785] d_loss: 1.38630152, g_loss: 0.69334543\n",
      "Step: [20786] d_loss: 1.38631964, g_loss: 0.69473600\n",
      "Step: [20787] d_loss: 1.38629913, g_loss: 0.69326246\n",
      "Step: [20788] d_loss: 1.38629973, g_loss: 0.69344789\n",
      "Step: [20789] d_loss: 1.38632298, g_loss: 0.69333601\n",
      "Step: [20790] d_loss: 1.38639307, g_loss: 0.69413435\n",
      "Step: [20791] d_loss: 1.38632727, g_loss: 0.69325066\n",
      "Step: [20792] d_loss: 1.38633394, g_loss: 0.69335788\n",
      "Step: [20793] d_loss: 1.38633764, g_loss: 0.69340611\n",
      "Step: [20794] d_loss: 1.38644886, g_loss: 0.69439107\n",
      "Step: [20795] d_loss: 1.38648367, g_loss: 0.69337606\n",
      "Step: [20796] d_loss: 1.38634241, g_loss: 0.69129574\n",
      "Step: [20797] d_loss: 1.38639224, g_loss: 0.69738990\n",
      "Step: [20798] d_loss: 1.38643444, g_loss: 0.69550818\n",
      "Step: [20799] d_loss: 1.38645804, g_loss: 0.69401395\n",
      "Step: [20800] d_loss: 1.38644886, g_loss: 0.69333464\n",
      "Step: [20801] d_loss: 1.38640440, g_loss: 0.69246078\n",
      "Step: [20802] d_loss: 1.38638794, g_loss: 0.69219875\n",
      "Step: [20803] d_loss: 1.38636208, g_loss: 0.69207269\n",
      "Step: [20804] d_loss: 1.38634062, g_loss: 0.69315749\n",
      "Step: [20805] d_loss: 1.38631558, g_loss: 0.69362843\n",
      "Step: [20806] d_loss: 1.38630295, g_loss: 0.69354248\n",
      "Step: [20807] d_loss: 1.38630402, g_loss: 0.69345975\n",
      "Step: [20808] d_loss: 1.38629937, g_loss: 0.69261563\n",
      "Step: [20809] d_loss: 1.38629937, g_loss: 0.69220257\n",
      "Step: [20810] d_loss: 1.38627768, g_loss: 0.69272888\n",
      "Step: [20811] d_loss: 1.38629532, g_loss: 0.69351387\n",
      "Step: [20812] d_loss: 1.38629627, g_loss: 0.69343495\n",
      "Step: [20813] d_loss: 1.38628173, g_loss: 0.69340521\n",
      "Step: [20814] d_loss: 1.38629568, g_loss: 0.69295108\n",
      "Step: [20815] d_loss: 1.38628840, g_loss: 0.69281745\n",
      "Step: [20816] d_loss: 1.38629746, g_loss: 0.69311082\n",
      "Step: [20817] d_loss: 1.38628805, g_loss: 0.69330496\n",
      "Step: [20818] d_loss: 1.38629484, g_loss: 0.69331944\n",
      "Step: [20819] d_loss: 1.38629055, g_loss: 0.69291639\n",
      "Step: [20820] d_loss: 1.38629854, g_loss: 0.69286752\n",
      "Step: [20821] d_loss: 1.38629568, g_loss: 0.69284517\n",
      "Step: [20822] d_loss: 1.38628387, g_loss: 0.69315314\n",
      "Step: [20823] d_loss: 1.38629222, g_loss: 0.69339335\n",
      "Step: [20824] d_loss: 1.38687921, g_loss: 0.69590145\n",
      "Step: [20825] d_loss: 1.38630104, g_loss: 0.69215465\n",
      "Step: [20826] d_loss: 1.38630366, g_loss: 0.69269347\n",
      "Step: [20827] d_loss: 1.38629067, g_loss: 0.69308555\n",
      "Step: [20828] d_loss: 1.38630199, g_loss: 0.69249165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20829] d_loss: 1.38628352, g_loss: 0.69215643\n",
      "Step: [20830] d_loss: 1.38630247, g_loss: 0.69284606\n",
      "Step: [20831] d_loss: 1.38625038, g_loss: 0.69355416\n",
      "Step: [20832] d_loss: 1.38638854, g_loss: 0.69434988\n",
      "Step: [20833] d_loss: 1.38697338, g_loss: 0.69181061\n",
      "Step: [20834] d_loss: 1.38789976, g_loss: 0.69627285\n",
      "Step: [20835] d_loss: 1.38806546, g_loss: 0.69330442\n",
      "Step: [20836] d_loss: 1.38808727, g_loss: 0.68796688\n",
      "Step: [20837] d_loss: 1.38763440, g_loss: 0.68772596\n",
      "Step: [20838] d_loss: 1.38714206, g_loss: 0.69342113\n",
      "Step: [20839] d_loss: 1.38693023, g_loss: 0.69825298\n",
      "Step: [20840] d_loss: 1.38674235, g_loss: 0.69857234\n",
      "Step: [20841] d_loss: 1.38654172, g_loss: 0.69523621\n",
      "Step: [20842] d_loss: 1.38638484, g_loss: 0.69392514\n",
      "Step: [20843] d_loss: 1.38632321, g_loss: 0.69287831\n",
      "Step: [20844] d_loss: 1.38630772, g_loss: 0.69323605\n",
      "Step: [20845] d_loss: 1.38630247, g_loss: 0.69391841\n",
      "Step: [20846] d_loss: 1.38629305, g_loss: 0.69309998\n",
      "Step: [20847] d_loss: 1.38629699, g_loss: 0.69289100\n",
      "Step: [20848] d_loss: 1.38631940, g_loss: 0.69290888\n",
      "Step: [20849] d_loss: 1.38629901, g_loss: 0.69343787\n",
      "Step: [20850] d_loss: 1.38629639, g_loss: 0.69269419\n",
      "Step: [20851] d_loss: 1.38627648, g_loss: 0.69313216\n",
      "Step: [20852] d_loss: 1.38628888, g_loss: 0.69348645\n",
      "Step: [20853] d_loss: 1.38629353, g_loss: 0.69350362\n",
      "Step: [20854] d_loss: 1.38629246, g_loss: 0.69320983\n",
      "Step: [20855] d_loss: 1.38629150, g_loss: 0.69300592\n",
      "Step: [20856] d_loss: 1.38629794, g_loss: 0.69281828\n",
      "Step: [20857] d_loss: 1.38673937, g_loss: 0.69254208\n",
      "Step: [20858] d_loss: 1.38629401, g_loss: 0.69205981\n",
      "Step: [20859] d_loss: 1.38627410, g_loss: 0.69293725\n",
      "Step: [20860] d_loss: 1.38632703, g_loss: 0.69343495\n",
      "Step: [20861] d_loss: 1.38628721, g_loss: 0.69332141\n",
      "Step: [20862] d_loss: 1.38629246, g_loss: 0.69333065\n",
      "Step: [20863] d_loss: 1.38630009, g_loss: 0.69319379\n",
      "Step: [20864] d_loss: 1.38628829, g_loss: 0.69321573\n",
      "Step: [20865] d_loss: 1.38629794, g_loss: 0.69328845\n",
      "Step: [20866] d_loss: 1.38629603, g_loss: 0.69314253\n",
      "Step: [20867] d_loss: 1.38629115, g_loss: 0.69297516\n",
      "Step: [20868] d_loss: 1.38630843, g_loss: 0.69315660\n",
      "Step: [20869] d_loss: 1.38629091, g_loss: 0.69321191\n",
      "Step: [20870] d_loss: 1.38628650, g_loss: 0.69329333\n",
      "Step: [20871] d_loss: 1.38628602, g_loss: 0.69323313\n",
      "Step: [20872] d_loss: 1.38627601, g_loss: 0.69307995\n",
      "Step: [20873] d_loss: 1.38629603, g_loss: 0.69288266\n",
      "Step: [20874] d_loss: 1.38628614, g_loss: 0.69300807\n",
      "Step: [20875] d_loss: 1.38628483, g_loss: 0.69329458\n",
      "Step: [20876] d_loss: 1.38628352, g_loss: 0.69333804\n",
      "Step: [20877] d_loss: 1.38628983, g_loss: 0.69325912\n",
      "Step: [20878] d_loss: 1.38665485, g_loss: 0.69564968\n",
      "Step: [20879] d_loss: 1.38628006, g_loss: 0.69224513\n",
      "Step: [20880] d_loss: 1.38629222, g_loss: 0.69322824\n",
      "Step: [20881] d_loss: 1.38629222, g_loss: 0.69363761\n",
      "Step: [20882] d_loss: 1.38629532, g_loss: 0.69355631\n",
      "Step: [20883] d_loss: 1.38629055, g_loss: 0.69318819\n",
      "Step: [20884] d_loss: 1.38628983, g_loss: 0.69286424\n",
      "Step: [20885] d_loss: 1.38628280, g_loss: 0.69294280\n",
      "Step: [20886] d_loss: 1.38628626, g_loss: 0.69314033\n",
      "Step: [20887] d_loss: 1.38627934, g_loss: 0.69327140\n",
      "Step: [20888] d_loss: 1.38627744, g_loss: 0.69317186\n",
      "Step: [20889] d_loss: 1.38617253, g_loss: 0.69345897\n",
      "Step: [20890] d_loss: 1.38628626, g_loss: 0.69257790\n",
      "Step: [20891] d_loss: 1.38629496, g_loss: 0.69322616\n",
      "Step: [20892] d_loss: 1.38629353, g_loss: 0.69341242\n",
      "Step: [20893] d_loss: 1.38626027, g_loss: 0.69329387\n",
      "Step: [20894] d_loss: 1.38629210, g_loss: 0.69308352\n",
      "Step: [20895] d_loss: 1.38628554, g_loss: 0.69317818\n",
      "Step: [20896] d_loss: 1.38643885, g_loss: 0.69219816\n",
      "Step: [20897] d_loss: 1.38628519, g_loss: 0.69265187\n",
      "Step: [20898] d_loss: 1.38647413, g_loss: 0.69391996\n",
      "Step: [20899] d_loss: 1.38628483, g_loss: 0.69336617\n",
      "Step: [20900] d_loss: 1.38670301, g_loss: 0.69389731\n",
      "Step: [20901] d_loss: 1.38631392, g_loss: 0.69342393\n",
      "Step: [20902] d_loss: 1.38640249, g_loss: 0.69221628\n",
      "Step: [20903] d_loss: 1.38648295, g_loss: 0.69177663\n",
      "Step: [20904] d_loss: 1.38649905, g_loss: 0.69357824\n",
      "Step: [20905] d_loss: 1.38647223, g_loss: 0.69239020\n",
      "Step: [20906] d_loss: 1.38642812, g_loss: 0.69221711\n",
      "Step: [20907] d_loss: 1.38638103, g_loss: 0.69101751\n",
      "Step: [20908] d_loss: 1.38636243, g_loss: 0.69381392\n",
      "Step: [20909] d_loss: 1.38633370, g_loss: 0.69477820\n",
      "Step: [20910] d_loss: 1.38632536, g_loss: 0.69400120\n",
      "Step: [20911] d_loss: 1.38681674, g_loss: 0.69379818\n",
      "Step: [20912] d_loss: 1.38631546, g_loss: 0.69280505\n",
      "Step: [20913] d_loss: 1.38664997, g_loss: 0.69394052\n",
      "Step: [20914] d_loss: 1.38632166, g_loss: 0.69377816\n",
      "Step: [20915] d_loss: 1.38643277, g_loss: 0.69384068\n",
      "Step: [20916] d_loss: 1.38631606, g_loss: 0.69309735\n",
      "Step: [20917] d_loss: 1.38632083, g_loss: 0.69257087\n",
      "Step: [20918] d_loss: 1.38631678, g_loss: 0.69231439\n",
      "Step: [20919] d_loss: 1.38630414, g_loss: 0.69298118\n",
      "Step: [20920] d_loss: 1.38630533, g_loss: 0.69370395\n",
      "Step: [20921] d_loss: 1.38629770, g_loss: 0.69360518\n",
      "Step: [20922] d_loss: 1.38629866, g_loss: 0.69332510\n",
      "Step: [20923] d_loss: 1.38629150, g_loss: 0.69297314\n",
      "Step: [20924] d_loss: 1.38628697, g_loss: 0.69300508\n",
      "Step: [20925] d_loss: 1.38629425, g_loss: 0.69315320\n",
      "Step: [20926] d_loss: 1.38628459, g_loss: 0.69302464\n",
      "Step: [20927] d_loss: 1.38628924, g_loss: 0.69295871\n",
      "Step: [20928] d_loss: 1.38628519, g_loss: 0.69288653\n",
      "Step: [20929] d_loss: 1.38713527, g_loss: 0.69445169\n",
      "Step: [20930] d_loss: 1.38629365, g_loss: 0.69282889\n",
      "Step: [20931] d_loss: 1.38630188, g_loss: 0.69325995\n",
      "Step: [20932] d_loss: 1.38629580, g_loss: 0.69400680\n",
      "Step: [20933] d_loss: 1.38629961, g_loss: 0.69334942\n",
      "Step: [20934] d_loss: 1.38630986, g_loss: 0.69361770\n",
      "Step: [20935] d_loss: 1.38630152, g_loss: 0.69262058\n",
      "Step: [20936] d_loss: 1.38630033, g_loss: 0.69337499\n",
      "Step: [20937] d_loss: 1.38629675, g_loss: 0.69316328\n",
      "Step: [20938] d_loss: 1.38630605, g_loss: 0.69392484\n",
      "Step: [20939] d_loss: 1.38629663, g_loss: 0.69361925\n",
      "Step: [20940] d_loss: 1.38627481, g_loss: 0.69307065\n",
      "Step: [20941] d_loss: 1.38628769, g_loss: 0.69297630\n",
      "Step: [20942] d_loss: 1.38629234, g_loss: 0.69274980\n",
      "Step: [20943] d_loss: 1.38629270, g_loss: 0.69312859\n",
      "Step: [20944] d_loss: 1.38628972, g_loss: 0.69338632\n",
      "Step: [20945] d_loss: 1.38629222, g_loss: 0.69287914\n",
      "Step: [20946] d_loss: 1.38633251, g_loss: 0.69264132\n",
      "Step: [20947] d_loss: 1.38628960, g_loss: 0.69273442\n",
      "Step: [20948] d_loss: 1.38626361, g_loss: 0.69331586\n",
      "Step: [20949] d_loss: 1.38628006, g_loss: 0.69353926\n",
      "Step: [20950] d_loss: 1.38626242, g_loss: 0.69353127\n",
      "Step: [20951] d_loss: 1.38628638, g_loss: 0.69323629\n",
      "Step: [20952] d_loss: 1.38628972, g_loss: 0.69289261\n",
      "Step: [20953] d_loss: 1.38626719, g_loss: 0.69278473\n",
      "Step: [20954] d_loss: 1.38628578, g_loss: 0.69307935\n",
      "Step: [20955] d_loss: 1.38628602, g_loss: 0.69332814\n",
      "Step: [20956] d_loss: 1.38628030, g_loss: 0.69323719\n",
      "Step: [20957] d_loss: 1.38628936, g_loss: 0.69324362\n",
      "Step: [20958] d_loss: 1.38628197, g_loss: 0.69316083\n",
      "Step: [20959] d_loss: 1.38625956, g_loss: 0.69306612\n",
      "Step: [20960] d_loss: 1.38628817, g_loss: 0.69319320\n",
      "Step: [20961] d_loss: 1.38621664, g_loss: 0.69327259\n",
      "Step: [20962] d_loss: 1.38629675, g_loss: 0.69338596\n",
      "Step: [20963] d_loss: 1.38627553, g_loss: 0.69325423\n",
      "Step: [20964] d_loss: 1.38654602, g_loss: 0.69328493\n",
      "Step: [20965] d_loss: 1.38627458, g_loss: 0.69275182\n",
      "Step: [20966] d_loss: 1.38628483, g_loss: 0.69352555\n",
      "Step: [20967] d_loss: 1.38624465, g_loss: 0.69360113\n",
      "Step: [20968] d_loss: 1.38619888, g_loss: 0.69337392\n",
      "Step: [20969] d_loss: 1.38629770, g_loss: 0.69258821\n",
      "Step: [20970] d_loss: 1.38624620, g_loss: 0.69294661\n",
      "Step: [20971] d_loss: 1.38625491, g_loss: 0.69289696\n",
      "Step: [20972] d_loss: 1.38624096, g_loss: 0.69294381\n",
      "Step: [20973] d_loss: 1.38626277, g_loss: 0.69285876\n",
      "Step: [20974] d_loss: 1.38622832, g_loss: 0.69321948\n",
      "Step: [20975] d_loss: 1.38623643, g_loss: 0.69328797\n",
      "Step: [20976] d_loss: 1.38625097, g_loss: 0.69323540\n",
      "Step: [20977] d_loss: 1.38619792, g_loss: 0.69340986\n",
      "Step: [20978] d_loss: 1.38625741, g_loss: 0.69313216\n",
      "Step: [20979] d_loss: 1.38622522, g_loss: 0.69313496\n",
      "Step: [20980] d_loss: 1.38623178, g_loss: 0.69320881\n",
      "Step: [20981] d_loss: 1.38642466, g_loss: 0.69345057\n",
      "Step: [20982] d_loss: 1.38621819, g_loss: 0.69301498\n",
      "Step: [20983] d_loss: 1.38638628, g_loss: 0.69373626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [20984] d_loss: 1.38621581, g_loss: 0.69312727\n",
      "Step: [20985] d_loss: 1.38632512, g_loss: 0.69358850\n",
      "Step: [20986] d_loss: 1.38627553, g_loss: 0.69313067\n",
      "Step: [20987] d_loss: 1.38625669, g_loss: 0.69304508\n",
      "Step: [20988] d_loss: 1.38632798, g_loss: 0.69311881\n",
      "Step: [20989] d_loss: 1.38627017, g_loss: 0.69216764\n",
      "Step: [20990] d_loss: 1.38623297, g_loss: 0.69338965\n",
      "Step: [20991] d_loss: 1.38623023, g_loss: 0.69370914\n",
      "Step: [20992] d_loss: 1.38630128, g_loss: 0.69298625\n",
      "Step: [20993] d_loss: 1.38633502, g_loss: 0.69322145\n",
      "Step: [20994] d_loss: 1.38628316, g_loss: 0.69332272\n",
      "Step: [20995] d_loss: 1.38628197, g_loss: 0.69312185\n",
      "Step: [20996] d_loss: 1.38630271, g_loss: 0.69311166\n",
      "Step: [20997] d_loss: 1.38638854, g_loss: 0.69297665\n",
      "Step: [20998] d_loss: 1.38649344, g_loss: 0.69459206\n",
      "Step: [20999] d_loss: 1.38667202, g_loss: 0.69444865\n",
      "Step: [21000] d_loss: 1.38680506, g_loss: 0.69034731\n",
      "Step: [21001] d_loss: 1.38685966, g_loss: 0.69063252\n",
      "Step: [21002] d_loss: 1.38692737, g_loss: 0.69108117\n",
      "Step: [21003] d_loss: 1.38695467, g_loss: 0.69210935\n",
      "Step: [21004] d_loss: 1.38689470, g_loss: 0.69300771\n",
      "Step: [21005] d_loss: 1.38684523, g_loss: 0.69291759\n",
      "Step: [21006] d_loss: 1.38678229, g_loss: 0.69343567\n",
      "Step: [21007] d_loss: 1.38678312, g_loss: 0.69610620\n",
      "Step: [21008] d_loss: 1.38680625, g_loss: 0.69600159\n",
      "Step: [21009] d_loss: 1.38761854, g_loss: 0.69507980\n",
      "Step: [21010] d_loss: 1.38673997, g_loss: 0.68970674\n",
      "Step: [21011] d_loss: 1.38661373, g_loss: 0.69383538\n",
      "Step: [21012] d_loss: 1.38666809, g_loss: 0.69613904\n",
      "Step: [21013] d_loss: 1.38662744, g_loss: 0.69438314\n",
      "Step: [21014] d_loss: 1.38667488, g_loss: 0.69466060\n",
      "Step: [21015] d_loss: 1.38654256, g_loss: 0.69245756\n",
      "Step: [21016] d_loss: 1.38647497, g_loss: 0.69396210\n",
      "Step: [21017] d_loss: 1.38649011, g_loss: 0.69300705\n",
      "Step: [21018] d_loss: 1.38641882, g_loss: 0.68947542\n",
      "Step: [21019] d_loss: 1.38630521, g_loss: 0.69184303\n",
      "Step: [21020] d_loss: 1.38633776, g_loss: 0.69429004\n",
      "Step: [21021] d_loss: 1.38637578, g_loss: 0.69463134\n",
      "Step: [21022] d_loss: 1.38702035, g_loss: 0.69420522\n",
      "Step: [21023] d_loss: 1.38634157, g_loss: 0.69273680\n",
      "Step: [21024] d_loss: 1.38636017, g_loss: 0.69253528\n",
      "Step: [21025] d_loss: 1.38633370, g_loss: 0.69334829\n",
      "Step: [21026] d_loss: 1.38628531, g_loss: 0.69440103\n",
      "Step: [21027] d_loss: 1.38629055, g_loss: 0.69397300\n",
      "Step: [21028] d_loss: 1.38632369, g_loss: 0.69338512\n",
      "Step: [21029] d_loss: 1.38631845, g_loss: 0.69266081\n",
      "Step: [21030] d_loss: 1.38626659, g_loss: 0.69246972\n",
      "Step: [21031] d_loss: 1.38632452, g_loss: 0.69478607\n",
      "Step: [21032] d_loss: 1.38631845, g_loss: 0.69409257\n",
      "Step: [21033] d_loss: 1.38633895, g_loss: 0.69304174\n",
      "Step: [21034] d_loss: 1.38631701, g_loss: 0.69272816\n",
      "Step: [21035] d_loss: 1.38628960, g_loss: 0.69304454\n",
      "Step: [21036] d_loss: 1.38630915, g_loss: 0.69374299\n",
      "Step: [21037] d_loss: 1.38632011, g_loss: 0.69350344\n",
      "Step: [21038] d_loss: 1.38631344, g_loss: 0.69204921\n",
      "Step: [21039] d_loss: 1.38629460, g_loss: 0.69418752\n",
      "Step: [21040] d_loss: 1.38631892, g_loss: 0.69476181\n",
      "Step: [21041] d_loss: 1.38623750, g_loss: 0.69513309\n",
      "Step: [21042] d_loss: 1.38628829, g_loss: 0.69218206\n",
      "Step: [21043] d_loss: 1.38634098, g_loss: 0.69270933\n",
      "Step: [21044] d_loss: 1.38632584, g_loss: 0.69335359\n",
      "Step: [21045] d_loss: 1.38634992, g_loss: 0.69381064\n",
      "Step: [21046] d_loss: 1.38630676, g_loss: 0.69343519\n",
      "Step: [21047] d_loss: 1.38631129, g_loss: 0.69300604\n",
      "Step: [21048] d_loss: 1.38636708, g_loss: 0.69250160\n",
      "Step: [21049] d_loss: 1.38635051, g_loss: 0.69270200\n",
      "Step: [21050] d_loss: 1.43257356, g_loss: 0.68175232\n",
      "Step: [21051] d_loss: 1.38788486, g_loss: 0.69044960\n",
      "Step: [21052] d_loss: 1.39058781, g_loss: 0.69758165\n",
      "Step: [21053] d_loss: 1.39134192, g_loss: 0.70100754\n",
      "Step: [21054] d_loss: 1.39004982, g_loss: 0.69745082\n",
      "Step: [21055] d_loss: 1.38824642, g_loss: 0.69731879\n",
      "Step: [21056] d_loss: 1.38698626, g_loss: 0.69669414\n",
      "Step: [21057] d_loss: 1.38643944, g_loss: 0.69501507\n",
      "Step: [21058] d_loss: 1.38630652, g_loss: 0.69221926\n",
      "Step: [21059] d_loss: 1.38632333, g_loss: 0.69235861\n",
      "Step: [21060] d_loss: 1.38631988, g_loss: 0.69293243\n",
      "Step: [21061] d_loss: 1.38630664, g_loss: 0.69351906\n",
      "Step: [21062] d_loss: 1.38630223, g_loss: 0.69296819\n",
      "Step: [21063] d_loss: 1.38626456, g_loss: 0.69387949\n",
      "Step: [21064] d_loss: 1.38625312, g_loss: 0.69274241\n",
      "Step: [21065] d_loss: 1.38630605, g_loss: 0.69290709\n",
      "Step: [21066] d_loss: 1.38612127, g_loss: 0.69210607\n",
      "Step: [21067] d_loss: 1.38628507, g_loss: 0.69306785\n",
      "Step: [21068] d_loss: 1.38630891, g_loss: 0.69318897\n",
      "Step: [21069] d_loss: 1.38637996, g_loss: 0.69405532\n",
      "Step: [21070] d_loss: 1.38670802, g_loss: 0.69184381\n",
      "Step: [21071] d_loss: 1.38630843, g_loss: 0.69291067\n",
      "Step: [21072] d_loss: 1.38616467, g_loss: 0.69314092\n",
      "Step: [21073] d_loss: 1.38681579, g_loss: 0.69249445\n",
      "Step: [21074] d_loss: 1.38609922, g_loss: 0.69411123\n",
      "Step: [21075] d_loss: 1.38628125, g_loss: 0.69306290\n",
      "Step: [21076] d_loss: 1.38595843, g_loss: 0.69476986\n",
      "Step: [21077] d_loss: 1.38624263, g_loss: 0.69405282\n",
      "Step: [21078] d_loss: 1.38628125, g_loss: 0.69333595\n",
      "Step: [21079] d_loss: 1.38643050, g_loss: 0.69264722\n",
      "Step: [21080] d_loss: 1.38629746, g_loss: 0.69277012\n",
      "Step: [21081] d_loss: 1.38640690, g_loss: 0.69354701\n",
      "Step: [21082] d_loss: 1.38634431, g_loss: 0.69368708\n",
      "Step: [21083] d_loss: 1.38631487, g_loss: 0.69295764\n",
      "Step: [21084] d_loss: 1.38661718, g_loss: 0.69314492\n",
      "Step: [21085] d_loss: 1.38627756, g_loss: 0.69217098\n",
      "Step: [21086] d_loss: 1.38630342, g_loss: 0.69368923\n",
      "Step: [21087] d_loss: 1.38626146, g_loss: 0.69371188\n",
      "Step: [21088] d_loss: 1.38636601, g_loss: 0.69350827\n",
      "Step: [21089] d_loss: 1.38644266, g_loss: 0.69263345\n",
      "Step: [21090] d_loss: 1.38641655, g_loss: 0.69284165\n",
      "Step: [21091] d_loss: 1.38611388, g_loss: 0.69519138\n",
      "Step: [21092] d_loss: 1.38654757, g_loss: 0.69337827\n",
      "Step: [21093] d_loss: 1.38629830, g_loss: 0.69361997\n",
      "Step: [21094] d_loss: 1.38640690, g_loss: 0.69190395\n",
      "Step: [21095] d_loss: 1.38644099, g_loss: 0.69257373\n",
      "Step: [21096] d_loss: 1.38641834, g_loss: 0.69355446\n",
      "Step: [21097] d_loss: 1.38628173, g_loss: 0.69348347\n",
      "Step: [21098] d_loss: 1.38631058, g_loss: 0.69324076\n",
      "Step: [21099] d_loss: 1.38629854, g_loss: 0.69320232\n",
      "Step: [21100] d_loss: 1.38630009, g_loss: 0.69317466\n",
      "Step: [21101] d_loss: 1.38629270, g_loss: 0.69332302\n",
      "Step: [21102] d_loss: 1.38630533, g_loss: 0.69330370\n",
      "Step: [21103] d_loss: 1.38630140, g_loss: 0.69279867\n",
      "Step: [21104] d_loss: 1.38684261, g_loss: 0.69149840\n",
      "Step: [21105] d_loss: 1.38629556, g_loss: 0.69273984\n",
      "Step: [21106] d_loss: 1.38630509, g_loss: 0.69341505\n",
      "Step: [21107] d_loss: 1.38630867, g_loss: 0.69364822\n",
      "Step: [21108] d_loss: 1.38630986, g_loss: 0.69391382\n",
      "Step: [21109] d_loss: 1.38631046, g_loss: 0.69330621\n",
      "Step: [21110] d_loss: 1.38661623, g_loss: 0.69308531\n",
      "Step: [21111] d_loss: 1.38630462, g_loss: 0.69323671\n",
      "Step: [21112] d_loss: 1.38617778, g_loss: 0.69371796\n",
      "Step: [21113] d_loss: 1.38631797, g_loss: 0.69339347\n",
      "Step: [21114] d_loss: 1.38627529, g_loss: 0.69381422\n",
      "Step: [21115] d_loss: 1.38632011, g_loss: 0.69341433\n",
      "Step: [21116] d_loss: 1.38636613, g_loss: 0.69344157\n",
      "Step: [21117] d_loss: 1.38632584, g_loss: 0.69354630\n",
      "Step: [21118] d_loss: 1.38632774, g_loss: 0.69314349\n",
      "Step: [21119] d_loss: 1.38632309, g_loss: 0.69311130\n",
      "Step: [21120] d_loss: 1.38631117, g_loss: 0.69307739\n",
      "Step: [21121] d_loss: 1.38629949, g_loss: 0.69315195\n",
      "Step: [21122] d_loss: 1.38629413, g_loss: 0.69306105\n",
      "Step: [21123] d_loss: 1.38629985, g_loss: 0.69323611\n",
      "Step: [21124] d_loss: 1.38629675, g_loss: 0.69326305\n",
      "Step: [21125] d_loss: 1.38631141, g_loss: 0.69351119\n",
      "Step: [21126] d_loss: 1.38630962, g_loss: 0.69322491\n",
      "Step: [21127] d_loss: 1.38632512, g_loss: 0.69349504\n",
      "Step: [21128] d_loss: 1.38632059, g_loss: 0.69327188\n",
      "Step: [21129] d_loss: 1.38644981, g_loss: 0.69251609\n",
      "Step: [21130] d_loss: 1.38629699, g_loss: 0.69276965\n",
      "Step: [21131] d_loss: 1.38629341, g_loss: 0.69304490\n",
      "Step: [21132] d_loss: 1.38629866, g_loss: 0.69323540\n",
      "Step: [21133] d_loss: 1.38629580, g_loss: 0.69333041\n",
      "Step: [21134] d_loss: 1.38629735, g_loss: 0.69331074\n",
      "Step: [21135] d_loss: 1.38612866, g_loss: 0.69242972\n",
      "Step: [21136] d_loss: 1.38630188, g_loss: 0.69331110\n",
      "Step: [21137] d_loss: 1.38630462, g_loss: 0.69328070\n",
      "Step: [21138] d_loss: 1.38629878, g_loss: 0.69256425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21139] d_loss: 1.38629436, g_loss: 0.69307065\n",
      "Step: [21140] d_loss: 1.38629770, g_loss: 0.69314718\n",
      "Step: [21141] d_loss: 1.38629115, g_loss: 0.69327950\n",
      "Step: [21142] d_loss: 1.38629580, g_loss: 0.69320083\n",
      "Step: [21143] d_loss: 1.38629770, g_loss: 0.69307113\n",
      "Step: [21144] d_loss: 1.38629150, g_loss: 0.69301951\n",
      "Step: [21145] d_loss: 1.38639581, g_loss: 0.69329107\n",
      "Step: [21146] d_loss: 1.38632214, g_loss: 0.69356036\n",
      "Step: [21147] d_loss: 1.38619530, g_loss: 0.69396329\n",
      "Step: [21148] d_loss: 1.38633454, g_loss: 0.69424736\n",
      "Step: [21149] d_loss: 1.38637793, g_loss: 0.69353044\n",
      "Step: [21150] d_loss: 1.38624156, g_loss: 0.69321167\n",
      "Step: [21151] d_loss: 1.38635015, g_loss: 0.69061381\n",
      "Step: [21152] d_loss: 1.38632298, g_loss: 0.69138098\n",
      "Step: [21153] d_loss: 1.38630438, g_loss: 0.69335139\n",
      "Step: [21154] d_loss: 1.38630104, g_loss: 0.69411010\n",
      "Step: [21155] d_loss: 1.38631749, g_loss: 0.69397247\n",
      "Step: [21156] d_loss: 1.38631594, g_loss: 0.69318259\n",
      "Step: [21157] d_loss: 1.38631010, g_loss: 0.69301826\n",
      "Step: [21158] d_loss: 1.38631487, g_loss: 0.69317520\n",
      "Step: [21159] d_loss: 1.38631153, g_loss: 0.69305080\n",
      "Step: [21160] d_loss: 1.38630033, g_loss: 0.69319779\n",
      "Step: [21161] d_loss: 1.38630533, g_loss: 0.69333333\n",
      "Step: [21162] d_loss: 1.38630450, g_loss: 0.69300365\n",
      "Step: [21163] d_loss: 1.38630462, g_loss: 0.69296241\n",
      "Step: [21164] d_loss: 1.38631880, g_loss: 0.69322067\n",
      "Step: [21165] d_loss: 1.38629758, g_loss: 0.69347948\n",
      "Step: [21166] d_loss: 1.38630629, g_loss: 0.69320595\n",
      "Step: [21167] d_loss: 1.38630688, g_loss: 0.69309872\n",
      "Step: [21168] d_loss: 1.38630724, g_loss: 0.69302815\n",
      "Step: [21169] d_loss: 1.38652706, g_loss: 0.69233358\n",
      "Step: [21170] d_loss: 1.38629830, g_loss: 0.69316614\n",
      "Step: [21171] d_loss: 1.38631368, g_loss: 0.69327807\n",
      "Step: [21172] d_loss: 1.38633418, g_loss: 0.69317931\n",
      "Step: [21173] d_loss: 1.38632429, g_loss: 0.69320273\n",
      "Step: [21174] d_loss: 1.38630795, g_loss: 0.69269228\n",
      "Step: [21175] d_loss: 1.38628817, g_loss: 0.69297540\n",
      "Step: [21176] d_loss: 1.38632369, g_loss: 0.69325590\n",
      "Step: [21177] d_loss: 1.38603973, g_loss: 0.69332474\n",
      "Step: [21178] d_loss: 1.38630962, g_loss: 0.69336164\n",
      "Step: [21179] d_loss: 1.38630033, g_loss: 0.69330335\n",
      "Step: [21180] d_loss: 1.38630855, g_loss: 0.69303310\n",
      "Step: [21181] d_loss: 1.38630605, g_loss: 0.69307363\n",
      "Step: [21182] d_loss: 1.38630366, g_loss: 0.69323599\n",
      "Step: [21183] d_loss: 1.38629961, g_loss: 0.69317675\n",
      "Step: [21184] d_loss: 1.38629556, g_loss: 0.69322908\n",
      "Step: [21185] d_loss: 1.38630474, g_loss: 0.69353127\n",
      "Step: [21186] d_loss: 1.38631558, g_loss: 0.69296837\n",
      "Step: [21187] d_loss: 1.38631773, g_loss: 0.69298875\n",
      "Step: [21188] d_loss: 1.38644648, g_loss: 0.69314575\n",
      "Step: [21189] d_loss: 1.38634765, g_loss: 0.69442290\n",
      "Step: [21190] d_loss: 1.38637602, g_loss: 0.69385767\n",
      "Step: [21191] d_loss: 1.38639641, g_loss: 0.69402069\n",
      "Step: [21192] d_loss: 1.38641191, g_loss: 0.69216776\n",
      "Step: [21193] d_loss: 1.38637495, g_loss: 0.69160533\n",
      "Step: [21194] d_loss: 1.38633943, g_loss: 0.69244444\n",
      "Step: [21195] d_loss: 1.38631320, g_loss: 0.69338787\n",
      "Step: [21196] d_loss: 1.38630509, g_loss: 0.69374663\n",
      "Step: [21197] d_loss: 1.38642454, g_loss: 0.69333410\n",
      "Step: [21198] d_loss: 1.38630748, g_loss: 0.69316041\n",
      "Step: [21199] d_loss: 1.38630772, g_loss: 0.69259918\n",
      "Step: [21200] d_loss: 1.38631666, g_loss: 0.69286168\n",
      "Step: [21201] d_loss: 1.38630688, g_loss: 0.69338971\n",
      "Step: [21202] d_loss: 1.38634717, g_loss: 0.69368732\n",
      "Step: [21203] d_loss: 1.38632405, g_loss: 0.69192696\n",
      "Step: [21204] d_loss: 1.38657403, g_loss: 0.69182634\n",
      "Step: [21205] d_loss: 1.38629937, g_loss: 0.69263506\n",
      "Step: [21206] d_loss: 1.38629794, g_loss: 0.69363582\n",
      "Step: [21207] d_loss: 1.38629997, g_loss: 0.69362754\n",
      "Step: [21208] d_loss: 1.38629413, g_loss: 0.69328207\n",
      "Step: [21209] d_loss: 1.38647771, g_loss: 0.69281310\n",
      "Step: [21210] d_loss: 1.38629711, g_loss: 0.69298553\n",
      "Step: [21211] d_loss: 1.38630748, g_loss: 0.69300848\n",
      "Step: [21212] d_loss: 1.38632977, g_loss: 0.69426596\n",
      "Step: [21213] d_loss: 1.38635623, g_loss: 0.69370866\n",
      "Step: [21214] d_loss: 1.38641250, g_loss: 0.69619191\n",
      "Step: [21215] d_loss: 1.38647008, g_loss: 0.69473666\n",
      "Step: [21216] d_loss: 1.38654613, g_loss: 0.69264758\n",
      "Step: [21217] d_loss: 1.38646638, g_loss: 0.68949330\n",
      "Step: [21218] d_loss: 1.38642240, g_loss: 0.69204819\n",
      "Step: [21219] d_loss: 1.38640416, g_loss: 0.69436771\n",
      "Step: [21220] d_loss: 1.38637710, g_loss: 0.69431090\n",
      "Step: [21221] d_loss: 1.38639092, g_loss: 0.69446874\n",
      "Step: [21222] d_loss: 1.38640845, g_loss: 0.69422770\n",
      "Step: [21223] d_loss: 1.38640523, g_loss: 0.69170153\n",
      "Step: [21224] d_loss: 1.38641357, g_loss: 0.69397146\n",
      "Step: [21225] d_loss: 1.38642716, g_loss: 0.69445276\n",
      "Step: [21226] d_loss: 1.38651454, g_loss: 0.69311571\n",
      "Step: [21227] d_loss: 1.38642144, g_loss: 0.69202280\n",
      "Step: [21228] d_loss: 1.38639688, g_loss: 0.69337910\n",
      "Step: [21229] d_loss: 1.38639820, g_loss: 0.69396174\n",
      "Step: [21230] d_loss: 1.38648295, g_loss: 0.69272840\n",
      "Step: [21231] d_loss: 1.38638496, g_loss: 0.69319963\n",
      "Step: [21232] d_loss: 1.38635898, g_loss: 0.69345850\n",
      "Step: [21233] d_loss: 1.38647568, g_loss: 0.69354904\n",
      "Step: [21234] d_loss: 1.38636923, g_loss: 0.69388759\n",
      "Step: [21235] d_loss: 1.38637114, g_loss: 0.69362146\n",
      "Step: [21236] d_loss: 1.38637185, g_loss: 0.69294000\n",
      "Step: [21237] d_loss: 1.38635659, g_loss: 0.69232708\n",
      "Step: [21238] d_loss: 1.38633776, g_loss: 0.69222403\n",
      "Step: [21239] d_loss: 1.38632762, g_loss: 0.69318932\n",
      "Step: [21240] d_loss: 1.38630128, g_loss: 0.69348705\n",
      "Step: [21241] d_loss: 1.38629794, g_loss: 0.69327796\n",
      "Step: [21242] d_loss: 1.38631082, g_loss: 0.69299483\n",
      "Step: [21243] d_loss: 1.38630009, g_loss: 0.69298422\n",
      "Step: [21244] d_loss: 1.38629425, g_loss: 0.69301206\n",
      "Step: [21245] d_loss: 1.38629389, g_loss: 0.69301271\n",
      "Step: [21246] d_loss: 1.38630271, g_loss: 0.69360149\n",
      "Step: [21247] d_loss: 1.38630986, g_loss: 0.69314355\n",
      "Step: [21248] d_loss: 1.38631928, g_loss: 0.69312215\n",
      "Step: [21249] d_loss: 1.38632607, g_loss: 0.69306523\n",
      "Step: [21250] d_loss: 1.38632846, g_loss: 0.69300532\n",
      "Step: [21251] d_loss: 1.38632059, g_loss: 0.69246602\n",
      "Step: [21252] d_loss: 1.38630021, g_loss: 0.69289362\n",
      "Step: [21253] d_loss: 1.38630295, g_loss: 0.69297695\n",
      "Step: [21254] d_loss: 1.38629770, g_loss: 0.69309914\n",
      "Step: [21255] d_loss: 1.38629293, g_loss: 0.69344860\n",
      "Step: [21256] d_loss: 1.38629544, g_loss: 0.69331706\n",
      "Step: [21257] d_loss: 1.38632464, g_loss: 0.69338107\n",
      "Step: [21258] d_loss: 1.38632095, g_loss: 0.69303596\n",
      "Step: [21259] d_loss: 1.38631463, g_loss: 0.69256479\n",
      "Step: [21260] d_loss: 1.38640296, g_loss: 0.69309688\n",
      "Step: [21261] d_loss: 1.38629222, g_loss: 0.69283879\n",
      "Step: [21262] d_loss: 1.38630843, g_loss: 0.69314396\n",
      "Step: [21263] d_loss: 1.38630438, g_loss: 0.69310975\n",
      "Step: [21264] d_loss: 1.38629913, g_loss: 0.69289714\n",
      "Step: [21265] d_loss: 1.38629830, g_loss: 0.69326532\n",
      "Step: [21266] d_loss: 1.38631773, g_loss: 0.69343543\n",
      "Step: [21267] d_loss: 1.38631272, g_loss: 0.69362116\n",
      "Step: [21268] d_loss: 1.38633275, g_loss: 0.69354236\n",
      "Step: [21269] d_loss: 1.38636041, g_loss: 0.69426751\n",
      "Step: [21270] d_loss: 1.38614511, g_loss: 0.69429851\n",
      "Step: [21271] d_loss: 1.38647866, g_loss: 0.69379067\n",
      "Step: [21272] d_loss: 1.38651419, g_loss: 0.69359696\n",
      "Step: [21273] d_loss: 1.38653445, g_loss: 0.69273913\n",
      "Step: [21274] d_loss: 1.38657832, g_loss: 0.69348419\n",
      "Step: [21275] d_loss: 1.38657999, g_loss: 0.69376743\n",
      "Step: [21276] d_loss: 1.38658905, g_loss: 0.69408488\n",
      "Step: [21277] d_loss: 1.38660336, g_loss: 0.69404995\n",
      "Step: [21278] d_loss: 1.38659859, g_loss: 0.69210577\n",
      "Step: [21279] d_loss: 1.38656116, g_loss: 0.69132841\n",
      "Step: [21280] d_loss: 1.38652611, g_loss: 0.69226301\n",
      "Step: [21281] d_loss: 1.38648796, g_loss: 0.69336784\n",
      "Step: [21282] d_loss: 1.38644385, g_loss: 0.69433403\n",
      "Step: [21283] d_loss: 1.38638687, g_loss: 0.69318795\n",
      "Step: [21284] d_loss: 1.38641596, g_loss: 0.69213653\n",
      "Step: [21285] d_loss: 1.38640213, g_loss: 0.69327897\n",
      "Step: [21286] d_loss: 1.38639903, g_loss: 0.69348466\n",
      "Step: [21287] d_loss: 1.38640666, g_loss: 0.69442350\n",
      "Step: [21288] d_loss: 1.38652706, g_loss: 0.69284540\n",
      "Step: [21289] d_loss: 1.38661361, g_loss: 0.69192672\n",
      "Step: [21290] d_loss: 1.38641500, g_loss: 0.69181406\n",
      "Step: [21291] d_loss: 1.38638783, g_loss: 0.69263464\n",
      "Step: [21292] d_loss: 1.38635075, g_loss: 0.69337869\n",
      "Step: [21293] d_loss: 1.38633668, g_loss: 0.69417334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21294] d_loss: 1.38633513, g_loss: 0.69348919\n",
      "Step: [21295] d_loss: 1.38634193, g_loss: 0.69328082\n",
      "Step: [21296] d_loss: 1.38634300, g_loss: 0.69290173\n",
      "Step: [21297] d_loss: 1.38635969, g_loss: 0.69443154\n",
      "Step: [21298] d_loss: 1.38637567, g_loss: 0.69296324\n",
      "Step: [21299] d_loss: 1.38631737, g_loss: 0.69188011\n",
      "Step: [21300] d_loss: 1.38635969, g_loss: 0.69263744\n",
      "Step: [21301] d_loss: 1.38634753, g_loss: 0.69388270\n",
      "Step: [21302] d_loss: 1.38634658, g_loss: 0.69341898\n",
      "Step: [21303] d_loss: 1.38627172, g_loss: 0.69226277\n",
      "Step: [21304] d_loss: 1.38631320, g_loss: 0.69235533\n",
      "Step: [21305] d_loss: 1.38630509, g_loss: 0.69324684\n",
      "Step: [21306] d_loss: 1.38619828, g_loss: 0.69362760\n",
      "Step: [21307] d_loss: 1.38630950, g_loss: 0.69396126\n",
      "Step: [21308] d_loss: 1.38632417, g_loss: 0.69256401\n",
      "Step: [21309] d_loss: 1.38632274, g_loss: 0.69278443\n",
      "Step: [21310] d_loss: 1.38633704, g_loss: 0.69353485\n",
      "Step: [21311] d_loss: 1.38621783, g_loss: 0.69375420\n",
      "Step: [21312] d_loss: 1.38632226, g_loss: 0.69157654\n",
      "Step: [21313] d_loss: 1.38630748, g_loss: 0.69262958\n",
      "Step: [21314] d_loss: 1.38624811, g_loss: 0.69335473\n",
      "Step: [21315] d_loss: 1.38631248, g_loss: 0.69383442\n",
      "Step: [21316] d_loss: 1.38632536, g_loss: 0.69237447\n",
      "Step: [21317] d_loss: 1.38631523, g_loss: 0.69264394\n",
      "Step: [21318] d_loss: 1.38632274, g_loss: 0.69328785\n",
      "Step: [21319] d_loss: 1.38634372, g_loss: 0.69510669\n",
      "Step: [21320] d_loss: 1.38639081, g_loss: 0.69393957\n",
      "Step: [21321] d_loss: 1.38642514, g_loss: 0.69318169\n",
      "Step: [21322] d_loss: 1.38643670, g_loss: 0.69161463\n",
      "Step: [21323] d_loss: 1.38639927, g_loss: 0.69168425\n",
      "Step: [21324] d_loss: 1.38638401, g_loss: 0.69310766\n",
      "Step: [21325] d_loss: 1.38639879, g_loss: 0.69419086\n",
      "Step: [21326] d_loss: 1.38640201, g_loss: 0.69370240\n",
      "Step: [21327] d_loss: 1.38640642, g_loss: 0.69320601\n",
      "Step: [21328] d_loss: 1.38639474, g_loss: 0.69147813\n",
      "Step: [21329] d_loss: 1.38634837, g_loss: 0.69184887\n",
      "Step: [21330] d_loss: 1.38633108, g_loss: 0.69312114\n",
      "Step: [21331] d_loss: 1.38634086, g_loss: 0.69462371\n",
      "Step: [21332] d_loss: 1.38635361, g_loss: 0.69461882\n",
      "Step: [21333] d_loss: 1.38638902, g_loss: 0.69331169\n",
      "Step: [21334] d_loss: 1.38641000, g_loss: 0.69271624\n",
      "Step: [21335] d_loss: 1.38639331, g_loss: 0.69228268\n",
      "Step: [21336] d_loss: 1.38638806, g_loss: 0.69255090\n",
      "Step: [21337] d_loss: 1.38637471, g_loss: 0.69319165\n",
      "Step: [21338] d_loss: 1.38635516, g_loss: 0.69328231\n",
      "Step: [21339] d_loss: 1.38634801, g_loss: 0.69296336\n",
      "Step: [21340] d_loss: 1.38632536, g_loss: 0.69301820\n",
      "Step: [21341] d_loss: 1.38638628, g_loss: 0.69283640\n",
      "Step: [21342] d_loss: 1.38629746, g_loss: 0.69276392\n",
      "Step: [21343] d_loss: 1.38671255, g_loss: 0.69305778\n",
      "Step: [21344] d_loss: 1.38634729, g_loss: 0.69337082\n",
      "Step: [21345] d_loss: 1.38648534, g_loss: 0.69481367\n",
      "Step: [21346] d_loss: 1.38656902, g_loss: 0.69521403\n",
      "Step: [21347] d_loss: 1.38669550, g_loss: 0.69336414\n",
      "Step: [21348] d_loss: 1.38676107, g_loss: 0.69293082\n",
      "Step: [21349] d_loss: 1.38682079, g_loss: 0.69344813\n",
      "Step: [21350] d_loss: 1.38684440, g_loss: 0.69412482\n",
      "Step: [21351] d_loss: 1.38683939, g_loss: 0.69245559\n",
      "Step: [21352] d_loss: 1.38681567, g_loss: 0.69283640\n",
      "Step: [21353] d_loss: 1.38676000, g_loss: 0.69307327\n",
      "Step: [21354] d_loss: 1.38673782, g_loss: 0.69551480\n",
      "Step: [21355] d_loss: 1.38672090, g_loss: 0.69442236\n",
      "Step: [21356] d_loss: 1.38670921, g_loss: 0.69561332\n",
      "Step: [21357] d_loss: 1.38671315, g_loss: 0.69446719\n",
      "Step: [21358] d_loss: 1.38664031, g_loss: 0.69196194\n",
      "Step: [21359] d_loss: 1.38662338, g_loss: 0.69262242\n",
      "Step: [21360] d_loss: 1.38658190, g_loss: 0.69450188\n",
      "Step: [21361] d_loss: 1.38656116, g_loss: 0.69455594\n",
      "Step: [21362] d_loss: 1.38639450, g_loss: 0.69276440\n",
      "Step: [21363] d_loss: 1.38644266, g_loss: 0.69090664\n",
      "Step: [21364] d_loss: 1.38639903, g_loss: 0.69372332\n",
      "Step: [21365] d_loss: 1.38636839, g_loss: 0.69366759\n",
      "Step: [21366] d_loss: 1.38636887, g_loss: 0.69434214\n",
      "Step: [21367] d_loss: 1.38631129, g_loss: 0.69414246\n",
      "Step: [21368] d_loss: 1.38639164, g_loss: 0.69381380\n",
      "Step: [21369] d_loss: 1.38640380, g_loss: 0.69312561\n",
      "Step: [21370] d_loss: 1.38640666, g_loss: 0.69419968\n",
      "Step: [21371] d_loss: 1.38640630, g_loss: 0.69398928\n",
      "Step: [21372] d_loss: 1.38642120, g_loss: 0.69365406\n",
      "Step: [21373] d_loss: 1.38638365, g_loss: 0.69340980\n",
      "Step: [21374] d_loss: 1.38641357, g_loss: 0.69420946\n",
      "Step: [21375] d_loss: 1.38636291, g_loss: 0.69364583\n",
      "Step: [21376] d_loss: 1.38639939, g_loss: 0.69455534\n",
      "Step: [21377] d_loss: 1.38638806, g_loss: 0.69291896\n",
      "Step: [21378] d_loss: 1.38636327, g_loss: 0.69209081\n",
      "Step: [21379] d_loss: 1.38634586, g_loss: 0.69219935\n",
      "Step: [21380] d_loss: 1.38631964, g_loss: 0.69260353\n",
      "Step: [21381] d_loss: 1.38630605, g_loss: 0.69323230\n",
      "Step: [21382] d_loss: 1.38628519, g_loss: 0.69295180\n",
      "Step: [21383] d_loss: 1.38628328, g_loss: 0.69329751\n",
      "Step: [21384] d_loss: 1.38614881, g_loss: 0.69407064\n",
      "Step: [21385] d_loss: 1.38629997, g_loss: 0.69341028\n",
      "Step: [21386] d_loss: 1.38625920, g_loss: 0.69293678\n",
      "Step: [21387] d_loss: 1.38631177, g_loss: 0.69346851\n",
      "Step: [21388] d_loss: 1.38640046, g_loss: 0.69360030\n",
      "Step: [21389] d_loss: 1.38632882, g_loss: 0.69344139\n",
      "Step: [21390] d_loss: 1.38633943, g_loss: 0.69229662\n",
      "Step: [21391] d_loss: 1.38633728, g_loss: 0.69290006\n",
      "Step: [21392] d_loss: 1.38633692, g_loss: 0.69334483\n",
      "Step: [21393] d_loss: 1.38635552, g_loss: 0.69293529\n",
      "Step: [21394] d_loss: 1.38632059, g_loss: 0.69281894\n",
      "Step: [21395] d_loss: 1.38629699, g_loss: 0.69263744\n",
      "Step: [21396] d_loss: 1.38628852, g_loss: 0.69317019\n",
      "Step: [21397] d_loss: 1.38634300, g_loss: 0.69334674\n",
      "Step: [21398] d_loss: 1.38648510, g_loss: 0.69367158\n",
      "Step: [21399] d_loss: 1.38670027, g_loss: 0.69571143\n",
      "Step: [21400] d_loss: 1.38680935, g_loss: 0.69193852\n",
      "Step: [21401] d_loss: 1.38681090, g_loss: 0.69023931\n",
      "Step: [21402] d_loss: 1.38673484, g_loss: 0.69224334\n",
      "Step: [21403] d_loss: 1.38665795, g_loss: 0.69456005\n",
      "Step: [21404] d_loss: 1.38660574, g_loss: 0.69557083\n",
      "Step: [21405] d_loss: 1.38656712, g_loss: 0.69475782\n",
      "Step: [21406] d_loss: 1.38652587, g_loss: 0.69310880\n",
      "Step: [21407] d_loss: 1.38647461, g_loss: 0.69289207\n",
      "Step: [21408] d_loss: 1.38643456, g_loss: 0.69312131\n",
      "Step: [21409] d_loss: 1.38626528, g_loss: 0.69309217\n",
      "Step: [21410] d_loss: 1.38638139, g_loss: 0.69334960\n",
      "Step: [21411] d_loss: 1.38636446, g_loss: 0.69308197\n",
      "Step: [21412] d_loss: 1.38622689, g_loss: 0.69351876\n",
      "Step: [21413] d_loss: 1.38635874, g_loss: 0.69445699\n",
      "Step: [21414] d_loss: 1.38634479, g_loss: 0.69287467\n",
      "Step: [21415] d_loss: 1.38633776, g_loss: 0.69285083\n",
      "Step: [21416] d_loss: 1.38631845, g_loss: 0.69296998\n",
      "Step: [21417] d_loss: 1.38632607, g_loss: 0.69406271\n",
      "Step: [21418] d_loss: 1.38630438, g_loss: 0.69345069\n",
      "Step: [21419] d_loss: 1.38631749, g_loss: 0.69345176\n",
      "Step: [21420] d_loss: 1.38644755, g_loss: 0.69336599\n",
      "Step: [21421] d_loss: 1.38632417, g_loss: 0.69293588\n",
      "Step: [21422] d_loss: 1.38633633, g_loss: 0.69167566\n",
      "Step: [21423] d_loss: 1.38641334, g_loss: 0.69304472\n",
      "Step: [21424] d_loss: 1.38630486, g_loss: 0.69358033\n",
      "Step: [21425] d_loss: 1.38629413, g_loss: 0.69346625\n",
      "Step: [21426] d_loss: 1.38629961, g_loss: 0.69341826\n",
      "Step: [21427] d_loss: 1.38624954, g_loss: 0.69367611\n",
      "Step: [21428] d_loss: 1.38629889, g_loss: 0.69338328\n",
      "Step: [21429] d_loss: 1.38629568, g_loss: 0.69296527\n",
      "Step: [21430] d_loss: 1.38629484, g_loss: 0.69273210\n",
      "Step: [21431] d_loss: 1.38629341, g_loss: 0.69306564\n",
      "Step: [21432] d_loss: 1.38629866, g_loss: 0.69318479\n",
      "Step: [21433] d_loss: 1.38629174, g_loss: 0.69314623\n",
      "Step: [21434] d_loss: 1.38629389, g_loss: 0.69328332\n",
      "Step: [21435] d_loss: 1.38605642, g_loss: 0.69328558\n",
      "Step: [21436] d_loss: 1.38642502, g_loss: 0.69400525\n",
      "Step: [21437] d_loss: 1.38629723, g_loss: 0.69373560\n",
      "Step: [21438] d_loss: 1.38629627, g_loss: 0.69285476\n",
      "Step: [21439] d_loss: 1.38629949, g_loss: 0.69145095\n",
      "Step: [21440] d_loss: 1.38629818, g_loss: 0.69213206\n",
      "Step: [21441] d_loss: 1.38621664, g_loss: 0.69359940\n",
      "Step: [21442] d_loss: 1.38636351, g_loss: 0.69315612\n",
      "Step: [21443] d_loss: 1.38630867, g_loss: 0.69381648\n",
      "Step: [21444] d_loss: 1.38620567, g_loss: 0.69254535\n",
      "Step: [21445] d_loss: 1.38629889, g_loss: 0.69288749\n",
      "Step: [21446] d_loss: 1.38629913, g_loss: 0.69335377\n",
      "Step: [21447] d_loss: 1.38630164, g_loss: 0.69282484\n",
      "Step: [21448] d_loss: 1.38631296, g_loss: 0.69365048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21449] d_loss: 1.38630271, g_loss: 0.69232500\n",
      "Step: [21450] d_loss: 1.38631392, g_loss: 0.69222414\n",
      "Step: [21451] d_loss: 1.38630390, g_loss: 0.69312465\n",
      "Step: [21452] d_loss: 1.38632178, g_loss: 0.69395256\n",
      "Step: [21453] d_loss: 1.38631153, g_loss: 0.69373035\n",
      "Step: [21454] d_loss: 1.38632083, g_loss: 0.69343913\n",
      "Step: [21455] d_loss: 1.38632131, g_loss: 0.69252038\n",
      "Step: [21456] d_loss: 1.38631463, g_loss: 0.69242966\n",
      "Step: [21457] d_loss: 1.38629222, g_loss: 0.69269884\n",
      "Step: [21458] d_loss: 1.38623750, g_loss: 0.69322658\n",
      "Step: [21459] d_loss: 1.38632989, g_loss: 0.69277608\n",
      "Step: [21460] d_loss: 1.38632846, g_loss: 0.69273055\n",
      "Step: [21461] d_loss: 1.38632119, g_loss: 0.69260812\n",
      "Step: [21462] d_loss: 1.38657594, g_loss: 0.69319946\n",
      "Step: [21463] d_loss: 1.38630128, g_loss: 0.69322050\n",
      "Step: [21464] d_loss: 1.38628817, g_loss: 0.69323081\n",
      "Step: [21465] d_loss: 1.38629031, g_loss: 0.69354182\n",
      "Step: [21466] d_loss: 1.38629687, g_loss: 0.69273537\n",
      "Step: [21467] d_loss: 1.38630533, g_loss: 0.69320542\n",
      "Step: [21468] d_loss: 1.38635039, g_loss: 0.69294453\n",
      "Step: [21469] d_loss: 1.38630629, g_loss: 0.69333494\n",
      "Step: [21470] d_loss: 1.38642025, g_loss: 0.69368613\n",
      "Step: [21471] d_loss: 1.38636768, g_loss: 0.69429410\n",
      "Step: [21472] d_loss: 1.38637781, g_loss: 0.69178164\n",
      "Step: [21473] d_loss: 1.38639641, g_loss: 0.69423878\n",
      "Step: [21474] d_loss: 1.38644910, g_loss: 0.69425982\n",
      "Step: [21475] d_loss: 1.38649297, g_loss: 0.69524485\n",
      "Step: [21476] d_loss: 1.38651085, g_loss: 0.69408977\n",
      "Step: [21477] d_loss: 1.38647628, g_loss: 0.69229460\n",
      "Step: [21478] d_loss: 1.38654566, g_loss: 0.69147277\n",
      "Step: [21479] d_loss: 1.38654327, g_loss: 0.69163680\n",
      "Step: [21480] d_loss: 1.38652778, g_loss: 0.69261253\n",
      "Step: [21481] d_loss: 1.38650584, g_loss: 0.69369304\n",
      "Step: [21482] d_loss: 1.38645029, g_loss: 0.69419241\n",
      "Step: [21483] d_loss: 1.38652492, g_loss: 0.69580090\n",
      "Step: [21484] d_loss: 1.38652265, g_loss: 0.69355661\n",
      "Step: [21485] d_loss: 1.38651371, g_loss: 0.69302630\n",
      "Step: [21486] d_loss: 1.38650692, g_loss: 0.69322562\n",
      "Step: [21487] d_loss: 1.38632226, g_loss: 0.69337010\n",
      "Step: [21488] d_loss: 1.38649166, g_loss: 0.69309175\n",
      "Step: [21489] d_loss: 1.38646877, g_loss: 0.69561303\n",
      "Step: [21490] d_loss: 1.38645911, g_loss: 0.69246423\n",
      "Step: [21491] d_loss: 1.38645113, g_loss: 0.69264257\n",
      "Step: [21492] d_loss: 1.38644314, g_loss: 0.69173092\n",
      "Step: [21493] d_loss: 1.38637161, g_loss: 0.69068891\n",
      "Step: [21494] d_loss: 1.38638389, g_loss: 0.69298089\n",
      "Step: [21495] d_loss: 1.38636398, g_loss: 0.69380295\n",
      "Step: [21496] d_loss: 1.38635802, g_loss: 0.69398737\n",
      "Step: [21497] d_loss: 1.38649893, g_loss: 0.69286549\n",
      "Step: [21498] d_loss: 1.38627398, g_loss: 0.69267905\n",
      "Step: [21499] d_loss: 1.38632464, g_loss: 0.69336909\n",
      "Step: [21500] d_loss: 1.38631821, g_loss: 0.69334120\n",
      "Step: [21501] d_loss: 1.38633299, g_loss: 0.69378942\n",
      "Step: [21502] d_loss: 1.38619590, g_loss: 0.69306219\n",
      "Step: [21503] d_loss: 1.38635314, g_loss: 0.69400632\n",
      "Step: [21504] d_loss: 1.38634777, g_loss: 0.69329798\n",
      "Step: [21505] d_loss: 1.38637161, g_loss: 0.69326854\n",
      "Step: [21506] d_loss: 1.38637996, g_loss: 0.69330263\n",
      "Step: [21507] d_loss: 1.38638759, g_loss: 0.69481277\n",
      "Step: [21508] d_loss: 1.38638425, g_loss: 0.69324082\n",
      "Step: [21509] d_loss: 1.38638008, g_loss: 0.69385600\n",
      "Step: [21510] d_loss: 1.38638031, g_loss: 0.69265199\n",
      "Step: [21511] d_loss: 1.38638020, g_loss: 0.69308817\n",
      "Step: [21512] d_loss: 1.38637412, g_loss: 0.69287872\n",
      "Step: [21513] d_loss: 1.38637137, g_loss: 0.69277811\n",
      "Step: [21514] d_loss: 1.38636947, g_loss: 0.69324172\n",
      "Step: [21515] d_loss: 1.38636315, g_loss: 0.69439209\n",
      "Step: [21516] d_loss: 1.38636279, g_loss: 0.69249821\n",
      "Step: [21517] d_loss: 1.38635826, g_loss: 0.69240469\n",
      "Step: [21518] d_loss: 1.38635790, g_loss: 0.69290334\n",
      "Step: [21519] d_loss: 1.38635433, g_loss: 0.69301742\n",
      "Step: [21520] d_loss: 1.38634694, g_loss: 0.69317698\n",
      "Step: [21521] d_loss: 1.38625252, g_loss: 0.69183874\n",
      "Step: [21522] d_loss: 1.38632345, g_loss: 0.69223696\n",
      "Step: [21523] d_loss: 1.38631570, g_loss: 0.69304657\n",
      "Step: [21524] d_loss: 1.38630521, g_loss: 0.69253111\n",
      "Step: [21525] d_loss: 1.38631034, g_loss: 0.69254148\n",
      "Step: [21526] d_loss: 1.38625503, g_loss: 0.69323957\n",
      "Step: [21527] d_loss: 1.38631582, g_loss: 0.69446087\n",
      "Step: [21528] d_loss: 1.38631916, g_loss: 0.69296759\n",
      "Step: [21529] d_loss: 1.38632190, g_loss: 0.69309056\n",
      "Step: [21530] d_loss: 1.38632751, g_loss: 0.69279891\n",
      "Step: [21531] d_loss: 1.38632596, g_loss: 0.69305450\n",
      "Step: [21532] d_loss: 1.38632274, g_loss: 0.69260424\n",
      "Step: [21533] d_loss: 1.38632035, g_loss: 0.69233024\n",
      "Step: [21534] d_loss: 1.38631344, g_loss: 0.69326198\n",
      "Step: [21535] d_loss: 1.38631272, g_loss: 0.69379294\n",
      "Step: [21536] d_loss: 1.38630486, g_loss: 0.69340444\n",
      "Step: [21537] d_loss: 1.38638401, g_loss: 0.69378495\n",
      "Step: [21538] d_loss: 1.38629770, g_loss: 0.69332081\n",
      "Step: [21539] d_loss: 1.38629937, g_loss: 0.69335985\n",
      "Step: [21540] d_loss: 1.38629270, g_loss: 0.69260705\n",
      "Step: [21541] d_loss: 1.38629079, g_loss: 0.69317311\n",
      "Step: [21542] d_loss: 1.38629484, g_loss: 0.69343203\n",
      "Step: [21543] d_loss: 1.38629532, g_loss: 0.69303387\n",
      "Step: [21544] d_loss: 1.38630974, g_loss: 0.69254279\n",
      "Step: [21545] d_loss: 1.38628972, g_loss: 0.69315326\n",
      "Step: [21546] d_loss: 1.38631320, g_loss: 0.69241261\n",
      "Step: [21547] d_loss: 1.38629413, g_loss: 0.69318128\n",
      "Step: [21548] d_loss: 1.38628876, g_loss: 0.69285816\n",
      "Step: [21549] d_loss: 1.38629282, g_loss: 0.69312590\n",
      "Step: [21550] d_loss: 1.38619280, g_loss: 0.69298220\n",
      "Step: [21551] d_loss: 1.38629639, g_loss: 0.69362122\n",
      "Step: [21552] d_loss: 1.38630712, g_loss: 0.69281244\n",
      "Step: [21553] d_loss: 1.38630128, g_loss: 0.69245088\n",
      "Step: [21554] d_loss: 1.38630629, g_loss: 0.69295794\n",
      "Step: [21555] d_loss: 1.38630223, g_loss: 0.69291806\n",
      "Step: [21556] d_loss: 1.38651621, g_loss: 0.69422674\n",
      "Step: [21557] d_loss: 1.38631117, g_loss: 0.69399691\n",
      "Step: [21558] d_loss: 1.38631666, g_loss: 0.69291508\n",
      "Step: [21559] d_loss: 1.38633275, g_loss: 0.69215465\n",
      "Step: [21560] d_loss: 1.38628983, g_loss: 0.69311702\n",
      "Step: [21561] d_loss: 1.38637757, g_loss: 0.69559562\n",
      "Step: [21562] d_loss: 1.38641214, g_loss: 0.69298548\n",
      "Step: [21563] d_loss: 1.38634992, g_loss: 0.69142509\n",
      "Step: [21564] d_loss: 1.38643241, g_loss: 0.69273472\n",
      "Step: [21565] d_loss: 1.38642526, g_loss: 0.69275320\n",
      "Step: [21566] d_loss: 1.38648367, g_loss: 0.69304204\n",
      "Step: [21567] d_loss: 1.38652587, g_loss: 0.69935858\n",
      "Step: [21568] d_loss: 1.38657355, g_loss: 0.69618940\n",
      "Step: [21569] d_loss: 1.38667250, g_loss: 0.69373381\n",
      "Step: [21570] d_loss: 1.38673949, g_loss: 0.69256115\n",
      "Step: [21571] d_loss: 1.38680696, g_loss: 0.69672817\n",
      "Step: [21572] d_loss: 1.38688850, g_loss: 0.69351101\n",
      "Step: [21573] d_loss: 1.38697231, g_loss: 0.69040096\n",
      "Step: [21574] d_loss: 1.38698339, g_loss: 0.69162142\n",
      "Step: [21575] d_loss: 1.38699389, g_loss: 0.69766462\n",
      "Step: [21576] d_loss: 1.38690019, g_loss: 0.69721675\n",
      "Step: [21577] d_loss: 1.38715065, g_loss: 0.69475591\n",
      "Step: [21578] d_loss: 1.38704228, g_loss: 0.69169503\n",
      "Step: [21579] d_loss: 1.38729358, g_loss: 0.68860364\n",
      "Step: [21580] d_loss: 1.38690364, g_loss: 0.69099003\n",
      "Step: [21581] d_loss: 1.38685715, g_loss: 0.69561088\n",
      "Step: [21582] d_loss: 1.38677990, g_loss: 0.69602990\n",
      "Step: [21583] d_loss: 1.38669610, g_loss: 0.69470489\n",
      "Step: [21584] d_loss: 1.38659096, g_loss: 0.69290471\n",
      "Step: [21585] d_loss: 1.38651335, g_loss: 0.69227171\n",
      "Step: [21586] d_loss: 1.38653982, g_loss: 0.69356489\n",
      "Step: [21587] d_loss: 1.38642836, g_loss: 0.69429523\n",
      "Step: [21588] d_loss: 1.38640261, g_loss: 0.69383186\n",
      "Step: [21589] d_loss: 1.38625467, g_loss: 0.69257659\n",
      "Step: [21590] d_loss: 1.38633573, g_loss: 0.69297993\n",
      "Step: [21591] d_loss: 1.38634253, g_loss: 0.69401962\n",
      "Step: [21592] d_loss: 1.38630486, g_loss: 0.69177604\n",
      "Step: [21593] d_loss: 1.38631630, g_loss: 0.69281554\n",
      "Step: [21594] d_loss: 1.38629603, g_loss: 0.69347513\n",
      "Step: [21595] d_loss: 1.38629270, g_loss: 0.69336945\n",
      "Step: [21596] d_loss: 1.38628411, g_loss: 0.69356728\n",
      "Step: [21597] d_loss: 1.38628113, g_loss: 0.69360256\n",
      "Step: [21598] d_loss: 1.38629448, g_loss: 0.69346815\n",
      "Step: [21599] d_loss: 1.38628364, g_loss: 0.69288433\n",
      "Step: [21600] d_loss: 1.38627958, g_loss: 0.69251776\n",
      "Step: [21601] d_loss: 1.38628328, g_loss: 0.69269246\n",
      "Step: [21602] d_loss: 1.38627481, g_loss: 0.69309711\n",
      "Step: [21603] d_loss: 1.38630617, g_loss: 0.69382268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21604] d_loss: 1.38630295, g_loss: 0.69351697\n",
      "Step: [21605] d_loss: 1.38628340, g_loss: 0.69246054\n",
      "Step: [21606] d_loss: 1.38640952, g_loss: 0.69103140\n",
      "Step: [21607] d_loss: 1.38625515, g_loss: 0.69275618\n",
      "Step: [21608] d_loss: 1.38629889, g_loss: 0.69441152\n",
      "Step: [21609] d_loss: 1.38628435, g_loss: 0.69319248\n",
      "Step: [21610] d_loss: 1.38692617, g_loss: 0.69292748\n",
      "Step: [21611] d_loss: 1.38629127, g_loss: 0.69294602\n",
      "Step: [21612] d_loss: 1.38630795, g_loss: 0.69298261\n",
      "Step: [21613] d_loss: 1.38630867, g_loss: 0.69209486\n",
      "Step: [21614] d_loss: 1.38628507, g_loss: 0.69270706\n",
      "Step: [21615] d_loss: 1.38629580, g_loss: 0.69365996\n",
      "Step: [21616] d_loss: 1.38630366, g_loss: 0.69281161\n",
      "Step: [21617] d_loss: 1.38628805, g_loss: 0.69157094\n",
      "Step: [21618] d_loss: 1.38678885, g_loss: 0.69330865\n",
      "Step: [21619] d_loss: 1.38629770, g_loss: 0.69388318\n",
      "Step: [21620] d_loss: 1.38629627, g_loss: 0.69364119\n",
      "Step: [21621] d_loss: 1.38631344, g_loss: 0.69287896\n",
      "Step: [21622] d_loss: 1.38636351, g_loss: 0.69256198\n",
      "Step: [21623] d_loss: 1.38634050, g_loss: 0.69274116\n",
      "Step: [21624] d_loss: 1.38634801, g_loss: 0.69359970\n",
      "Step: [21625] d_loss: 1.38635182, g_loss: 0.69269818\n",
      "Step: [21626] d_loss: 1.38634539, g_loss: 0.69286656\n",
      "Step: [21627] d_loss: 1.38634753, g_loss: 0.69333780\n",
      "Step: [21628] d_loss: 1.38633657, g_loss: 0.69389343\n",
      "Step: [21629] d_loss: 1.38636088, g_loss: 0.69350374\n",
      "Step: [21630] d_loss: 1.38635468, g_loss: 0.69324380\n",
      "Step: [21631] d_loss: 1.38634872, g_loss: 0.69292617\n",
      "Step: [21632] d_loss: 1.38635373, g_loss: 0.69367647\n",
      "Step: [21633] d_loss: 1.38635588, g_loss: 0.69206178\n",
      "Step: [21634] d_loss: 1.38675177, g_loss: 0.69106126\n",
      "Step: [21635] d_loss: 1.38631034, g_loss: 0.69220233\n",
      "Step: [21636] d_loss: 1.38632989, g_loss: 0.69391239\n",
      "Step: [21637] d_loss: 1.38633084, g_loss: 0.69390124\n",
      "Step: [21638] d_loss: 1.38623130, g_loss: 0.69323862\n",
      "Step: [21639] d_loss: 1.38632941, g_loss: 0.69327235\n",
      "Step: [21640] d_loss: 1.38635027, g_loss: 0.69346917\n",
      "Step: [21641] d_loss: 1.38634479, g_loss: 0.69125342\n",
      "Step: [21642] d_loss: 1.38642728, g_loss: 0.69095087\n",
      "Step: [21643] d_loss: 1.38631845, g_loss: 0.69259566\n",
      "Step: [21644] d_loss: 1.38631594, g_loss: 0.69478410\n",
      "Step: [21645] d_loss: 1.38629806, g_loss: 0.69378340\n",
      "Step: [21646] d_loss: 1.38626194, g_loss: 0.69221294\n",
      "Step: [21647] d_loss: 1.38630128, g_loss: 0.69274163\n",
      "Step: [21648] d_loss: 1.38628531, g_loss: 0.69249904\n",
      "Step: [21649] d_loss: 1.38629699, g_loss: 0.69319487\n",
      "Step: [21650] d_loss: 1.38629699, g_loss: 0.69267488\n",
      "Step: [21651] d_loss: 1.38636565, g_loss: 0.69445735\n",
      "Step: [21652] d_loss: 1.38697040, g_loss: 0.69556922\n",
      "Step: [21653] d_loss: 1.38781464, g_loss: 0.69033039\n",
      "Step: [21654] d_loss: 1.38835347, g_loss: 0.69333130\n",
      "Step: [21655] d_loss: 1.38847983, g_loss: 0.69609904\n",
      "Step: [21656] d_loss: 1.38826442, g_loss: 0.69288236\n",
      "Step: [21657] d_loss: 1.38773370, g_loss: 0.69118613\n",
      "Step: [21658] d_loss: 1.38726091, g_loss: 0.69541323\n",
      "Step: [21659] d_loss: 1.38689041, g_loss: 0.69226527\n",
      "Step: [21660] d_loss: 1.38662195, g_loss: 0.69325638\n",
      "Step: [21661] d_loss: 1.38646150, g_loss: 0.69388056\n",
      "Step: [21662] d_loss: 1.38634562, g_loss: 0.69298416\n",
      "Step: [21663] d_loss: 1.38632202, g_loss: 0.69215417\n",
      "Step: [21664] d_loss: 1.38630342, g_loss: 0.69228733\n",
      "Step: [21665] d_loss: 1.38670635, g_loss: 0.69243389\n",
      "Step: [21666] d_loss: 1.38647437, g_loss: 0.69389355\n",
      "Step: [21667] d_loss: 1.38654280, g_loss: 0.69710928\n",
      "Step: [21668] d_loss: 1.38663542, g_loss: 0.69161928\n",
      "Step: [21669] d_loss: 1.38659489, g_loss: 0.68978775\n",
      "Step: [21670] d_loss: 1.38664341, g_loss: 0.69262910\n",
      "Step: [21671] d_loss: 1.38649309, g_loss: 0.69428515\n",
      "Step: [21672] d_loss: 1.38645434, g_loss: 0.69399101\n",
      "Step: [21673] d_loss: 1.38648701, g_loss: 0.69243026\n",
      "Step: [21674] d_loss: 1.38633657, g_loss: 0.69249439\n",
      "Step: [21675] d_loss: 1.38631606, g_loss: 0.69347274\n",
      "Step: [21676] d_loss: 1.38631415, g_loss: 0.69352889\n",
      "Step: [21677] d_loss: 1.38630843, g_loss: 0.69343638\n",
      "Step: [21678] d_loss: 1.38629866, g_loss: 0.69307017\n",
      "Step: [21679] d_loss: 1.38622904, g_loss: 0.69279426\n",
      "Step: [21680] d_loss: 1.38624239, g_loss: 0.69398379\n",
      "Step: [21681] d_loss: 1.38630247, g_loss: 0.69304127\n",
      "Step: [21682] d_loss: 1.38630819, g_loss: 0.69328809\n",
      "Step: [21683] d_loss: 1.38630080, g_loss: 0.69330704\n",
      "Step: [21684] d_loss: 1.38630319, g_loss: 0.69311702\n",
      "Step: [21685] d_loss: 1.38629818, g_loss: 0.69301450\n",
      "Step: [21686] d_loss: 1.38629436, g_loss: 0.69302094\n",
      "Step: [21687] d_loss: 1.38630056, g_loss: 0.69291943\n",
      "Step: [21688] d_loss: 1.38630533, g_loss: 0.69321311\n",
      "Step: [21689] d_loss: 1.38607919, g_loss: 0.69382119\n",
      "Step: [21690] d_loss: 1.38631892, g_loss: 0.69319308\n",
      "Step: [21691] d_loss: 1.38631487, g_loss: 0.69227624\n",
      "Step: [21692] d_loss: 1.38630652, g_loss: 0.69234782\n",
      "Step: [21693] d_loss: 1.38630247, g_loss: 0.69293666\n",
      "Step: [21694] d_loss: 1.38630378, g_loss: 0.69349456\n",
      "Step: [21695] d_loss: 1.38630962, g_loss: 0.69351989\n",
      "Step: [21696] d_loss: 1.38630390, g_loss: 0.69321370\n",
      "Step: [21697] d_loss: 1.38631177, g_loss: 0.69327474\n",
      "Step: [21698] d_loss: 1.38632250, g_loss: 0.69338310\n",
      "Step: [21699] d_loss: 1.38621855, g_loss: 0.69322604\n",
      "Step: [21700] d_loss: 1.38630629, g_loss: 0.69267142\n",
      "Step: [21701] d_loss: 1.38629258, g_loss: 0.69292498\n",
      "Step: [21702] d_loss: 1.38633573, g_loss: 0.69338167\n",
      "Step: [21703] d_loss: 1.38662374, g_loss: 0.69445288\n",
      "Step: [21704] d_loss: 1.38698804, g_loss: 0.69262630\n",
      "Step: [21705] d_loss: 1.38703156, g_loss: 0.69378877\n",
      "Step: [21706] d_loss: 1.38692272, g_loss: 0.69320750\n",
      "Step: [21707] d_loss: 1.38675189, g_loss: 0.69179469\n",
      "Step: [21708] d_loss: 1.38657880, g_loss: 0.69251323\n",
      "Step: [21709] d_loss: 1.38625407, g_loss: 0.69468236\n",
      "Step: [21710] d_loss: 1.38642418, g_loss: 0.69438386\n",
      "Step: [21711] d_loss: 1.38638353, g_loss: 0.69294345\n",
      "Step: [21712] d_loss: 1.38616920, g_loss: 0.69210136\n",
      "Step: [21713] d_loss: 1.38625503, g_loss: 0.69195998\n",
      "Step: [21714] d_loss: 1.38656986, g_loss: 0.69592935\n",
      "Step: [21715] d_loss: 1.38680196, g_loss: 0.69658369\n",
      "Step: [21716] d_loss: 1.38685739, g_loss: 0.69499850\n",
      "Step: [21717] d_loss: 1.38730884, g_loss: 0.69175673\n",
      "Step: [21718] d_loss: 1.38769531, g_loss: 0.68948764\n",
      "Step: [21719] d_loss: 1.38713026, g_loss: 0.69057226\n",
      "Step: [21720] d_loss: 1.38657892, g_loss: 0.69174230\n",
      "Step: [21721] d_loss: 1.38620293, g_loss: 0.69381726\n",
      "Step: [21722] d_loss: 1.38632953, g_loss: 0.69528949\n",
      "Step: [21723] d_loss: 1.38633227, g_loss: 0.69463062\n",
      "Step: [21724] d_loss: 1.38634002, g_loss: 0.69385540\n",
      "Step: [21725] d_loss: 1.38633215, g_loss: 0.69150627\n",
      "Step: [21726] d_loss: 1.38632143, g_loss: 0.69174778\n",
      "Step: [21727] d_loss: 1.38630807, g_loss: 0.69323802\n",
      "Step: [21728] d_loss: 1.38630009, g_loss: 0.69365036\n",
      "Step: [21729] d_loss: 1.38632023, g_loss: 0.69334799\n",
      "Step: [21730] d_loss: 1.38603115, g_loss: 0.69428456\n",
      "Step: [21731] d_loss: 1.38632643, g_loss: 0.69415414\n",
      "Step: [21732] d_loss: 1.38635480, g_loss: 0.69365478\n",
      "Step: [21733] d_loss: 1.38638258, g_loss: 0.69276869\n",
      "Step: [21734] d_loss: 1.38636923, g_loss: 0.69310594\n",
      "Step: [21735] d_loss: 1.38634968, g_loss: 0.69391191\n",
      "Step: [21736] d_loss: 1.38635898, g_loss: 0.69403303\n",
      "Step: [21737] d_loss: 1.38635623, g_loss: 0.69334406\n",
      "Step: [21738] d_loss: 1.38633287, g_loss: 0.69320977\n",
      "Step: [21739] d_loss: 1.38631964, g_loss: 0.69327962\n",
      "Step: [21740] d_loss: 1.38631523, g_loss: 0.69236428\n",
      "Step: [21741] d_loss: 1.38634801, g_loss: 0.69211119\n",
      "Step: [21742] d_loss: 1.38630176, g_loss: 0.69286692\n",
      "Step: [21743] d_loss: 1.38630104, g_loss: 0.69346708\n",
      "Step: [21744] d_loss: 1.38630080, g_loss: 0.69339174\n",
      "Step: [21745] d_loss: 1.38629532, g_loss: 0.69330108\n",
      "Step: [21746] d_loss: 1.38629460, g_loss: 0.69312531\n",
      "Step: [21747] d_loss: 1.38629603, g_loss: 0.69318950\n",
      "Step: [21748] d_loss: 1.38630199, g_loss: 0.69321764\n",
      "Step: [21749] d_loss: 1.38629842, g_loss: 0.69314486\n",
      "Step: [21750] d_loss: 1.38630700, g_loss: 0.69316804\n",
      "Step: [21751] d_loss: 1.38630164, g_loss: 0.69323552\n",
      "Step: [21752] d_loss: 1.38630652, g_loss: 0.69353318\n",
      "Step: [21753] d_loss: 1.38631344, g_loss: 0.69326752\n",
      "Step: [21754] d_loss: 1.38630986, g_loss: 0.69338822\n",
      "Step: [21755] d_loss: 1.38629746, g_loss: 0.69347727\n",
      "Step: [21756] d_loss: 1.38633895, g_loss: 0.69327843\n",
      "Step: [21757] d_loss: 1.38633823, g_loss: 0.69336343\n",
      "Step: [21758] d_loss: 1.38628983, g_loss: 0.69278818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21759] d_loss: 1.38630652, g_loss: 0.69300508\n",
      "Step: [21760] d_loss: 1.38630199, g_loss: 0.69324428\n",
      "Step: [21761] d_loss: 1.38632250, g_loss: 0.69333959\n",
      "Step: [21762] d_loss: 1.38631046, g_loss: 0.69336122\n",
      "Step: [21763] d_loss: 1.38630188, g_loss: 0.69315320\n",
      "Step: [21764] d_loss: 1.38630462, g_loss: 0.69293749\n",
      "Step: [21765] d_loss: 1.38630605, g_loss: 0.69313097\n",
      "Step: [21766] d_loss: 1.38630199, g_loss: 0.69341904\n",
      "Step: [21767] d_loss: 1.38631678, g_loss: 0.69339520\n",
      "Step: [21768] d_loss: 1.38631082, g_loss: 0.69369406\n",
      "Step: [21769] d_loss: 1.38632345, g_loss: 0.69338059\n",
      "Step: [21770] d_loss: 1.38634431, g_loss: 0.69299388\n",
      "Step: [21771] d_loss: 1.38632917, g_loss: 0.69326895\n",
      "Step: [21772] d_loss: 1.38632631, g_loss: 0.69365501\n",
      "Step: [21773] d_loss: 1.38631237, g_loss: 0.69344687\n",
      "Step: [21774] d_loss: 1.38628507, g_loss: 0.69262391\n",
      "Step: [21775] d_loss: 1.38631928, g_loss: 0.69282836\n",
      "Step: [21776] d_loss: 1.38630319, g_loss: 0.69283724\n",
      "Step: [21777] d_loss: 1.38629818, g_loss: 0.69330812\n",
      "Step: [21778] d_loss: 1.38632536, g_loss: 0.69321531\n",
      "Step: [21779] d_loss: 1.38629055, g_loss: 0.69325221\n",
      "Step: [21780] d_loss: 1.38629723, g_loss: 0.69328433\n",
      "Step: [21781] d_loss: 1.38630867, g_loss: 0.69288296\n",
      "Step: [21782] d_loss: 1.38630271, g_loss: 0.69279456\n",
      "Step: [21783] d_loss: 1.38642406, g_loss: 0.69387370\n",
      "Step: [21784] d_loss: 1.38658845, g_loss: 0.69592738\n",
      "Step: [21785] d_loss: 1.38670778, g_loss: 0.69579625\n",
      "Step: [21786] d_loss: 1.38672757, g_loss: 0.69316411\n",
      "Step: [21787] d_loss: 1.38671517, g_loss: 0.69212735\n",
      "Step: [21788] d_loss: 1.38668239, g_loss: 0.69418311\n",
      "Step: [21789] d_loss: 1.38664031, g_loss: 0.69514251\n",
      "Step: [21790] d_loss: 1.38658869, g_loss: 0.69455457\n",
      "Step: [21791] d_loss: 1.38653386, g_loss: 0.69392192\n",
      "Step: [21792] d_loss: 1.38647842, g_loss: 0.69230378\n",
      "Step: [21793] d_loss: 1.38641751, g_loss: 0.69267559\n",
      "Step: [21794] d_loss: 1.38638854, g_loss: 0.69310665\n",
      "Step: [21795] d_loss: 1.38593614, g_loss: 0.69410789\n",
      "Step: [21796] d_loss: 1.38601530, g_loss: 0.69307303\n",
      "Step: [21797] d_loss: 1.38634551, g_loss: 0.69250429\n",
      "Step: [21798] d_loss: 1.38628614, g_loss: 0.69209933\n",
      "Step: [21799] d_loss: 1.38630581, g_loss: 0.69280469\n",
      "Step: [21800] d_loss: 1.38630223, g_loss: 0.69336236\n",
      "Step: [21801] d_loss: 1.38634622, g_loss: 0.69328201\n",
      "Step: [21802] d_loss: 1.38629854, g_loss: 0.69349051\n",
      "Step: [21803] d_loss: 1.38630021, g_loss: 0.69335544\n",
      "Step: [21804] d_loss: 1.38630009, g_loss: 0.69319212\n",
      "Step: [21805] d_loss: 1.38638282, g_loss: 0.69267380\n",
      "Step: [21806] d_loss: 1.38630903, g_loss: 0.69313508\n",
      "Step: [21807] d_loss: 1.38631141, g_loss: 0.69328719\n",
      "Step: [21808] d_loss: 1.38630235, g_loss: 0.69309211\n",
      "Step: [21809] d_loss: 1.38629699, g_loss: 0.69297987\n",
      "Step: [21810] d_loss: 1.38629913, g_loss: 0.69286680\n",
      "Step: [21811] d_loss: 1.38629460, g_loss: 0.69323343\n",
      "Step: [21812] d_loss: 1.38630962, g_loss: 0.69316268\n",
      "Step: [21813] d_loss: 1.38629889, g_loss: 0.69284511\n",
      "Step: [21814] d_loss: 1.38628888, g_loss: 0.69305569\n",
      "Step: [21815] d_loss: 1.38629067, g_loss: 0.69322872\n",
      "Step: [21816] d_loss: 1.38629150, g_loss: 0.69323575\n",
      "Step: [21817] d_loss: 1.38629532, g_loss: 0.69323057\n",
      "Step: [21818] d_loss: 1.38638020, g_loss: 0.69342303\n",
      "Step: [21819] d_loss: 1.38629389, g_loss: 0.69286788\n",
      "Step: [21820] d_loss: 1.38629234, g_loss: 0.69308275\n",
      "Step: [21821] d_loss: 1.38629651, g_loss: 0.69323283\n",
      "Step: [21822] d_loss: 1.38619900, g_loss: 0.69302833\n",
      "Step: [21823] d_loss: 1.38630795, g_loss: 0.69362271\n",
      "Step: [21824] d_loss: 1.38621497, g_loss: 0.69364762\n",
      "Step: [21825] d_loss: 1.38634849, g_loss: 0.69421417\n",
      "Step: [21826] d_loss: 1.38635385, g_loss: 0.69359022\n",
      "Step: [21827] d_loss: 1.38639855, g_loss: 0.69242561\n",
      "Step: [21828] d_loss: 1.38731122, g_loss: 0.69232750\n",
      "Step: [21829] d_loss: 1.38634431, g_loss: 0.69265544\n",
      "Step: [21830] d_loss: 1.38647687, g_loss: 0.69349045\n",
      "Step: [21831] d_loss: 1.38645685, g_loss: 0.69285846\n",
      "Step: [21832] d_loss: 1.38638413, g_loss: 0.69336712\n",
      "Step: [21833] d_loss: 1.38638639, g_loss: 0.69273484\n",
      "Step: [21834] d_loss: 1.38653278, g_loss: 0.69318742\n",
      "Step: [21835] d_loss: 1.38638306, g_loss: 0.69373733\n",
      "Step: [21836] d_loss: 1.38637877, g_loss: 0.69392627\n",
      "Step: [21837] d_loss: 1.38638699, g_loss: 0.69347471\n",
      "Step: [21838] d_loss: 1.38637197, g_loss: 0.69320703\n",
      "Step: [21839] d_loss: 1.38636184, g_loss: 0.69337773\n",
      "Step: [21840] d_loss: 1.38632190, g_loss: 0.69366920\n",
      "Step: [21841] d_loss: 1.38635325, g_loss: 0.69385445\n",
      "Step: [21842] d_loss: 1.38663375, g_loss: 0.69252604\n",
      "Step: [21843] d_loss: 1.38633680, g_loss: 0.69412279\n",
      "Step: [21844] d_loss: 1.38631296, g_loss: 0.69309133\n",
      "Step: [21845] d_loss: 1.38630652, g_loss: 0.69253415\n",
      "Step: [21846] d_loss: 1.38635850, g_loss: 0.69221306\n",
      "Step: [21847] d_loss: 1.38667107, g_loss: 0.69354928\n",
      "Step: [21848] d_loss: 1.38692546, g_loss: 0.69291419\n",
      "Step: [21849] d_loss: 1.38719344, g_loss: 0.69511342\n",
      "Step: [21850] d_loss: 1.38709784, g_loss: 0.69550365\n",
      "Step: [21851] d_loss: 1.38704884, g_loss: 0.69426310\n",
      "Step: [21852] d_loss: 1.38693762, g_loss: 0.69295967\n",
      "Step: [21853] d_loss: 1.38680935, g_loss: 0.69326299\n",
      "Step: [21854] d_loss: 1.38669300, g_loss: 0.69230270\n",
      "Step: [21855] d_loss: 1.38658881, g_loss: 0.68915027\n",
      "Step: [21856] d_loss: 1.38651848, g_loss: 0.69150662\n",
      "Step: [21857] d_loss: 1.38644862, g_loss: 0.69349587\n",
      "Step: [21858] d_loss: 1.38638973, g_loss: 0.69487184\n",
      "Step: [21859] d_loss: 1.38635874, g_loss: 0.69407129\n",
      "Step: [21860] d_loss: 1.38633215, g_loss: 0.69295669\n",
      "Step: [21861] d_loss: 1.38630939, g_loss: 0.69248903\n",
      "Step: [21862] d_loss: 1.38631272, g_loss: 0.69228923\n",
      "Step: [21863] d_loss: 1.38633585, g_loss: 0.69278806\n",
      "Step: [21864] d_loss: 1.38629770, g_loss: 0.69394934\n",
      "Step: [21865] d_loss: 1.38629866, g_loss: 0.69361955\n",
      "Step: [21866] d_loss: 1.38629317, g_loss: 0.69355631\n",
      "Step: [21867] d_loss: 1.38628876, g_loss: 0.69292331\n",
      "Step: [21868] d_loss: 1.38628674, g_loss: 0.69276780\n",
      "Step: [21869] d_loss: 1.38628721, g_loss: 0.69288427\n",
      "Step: [21870] d_loss: 1.38628638, g_loss: 0.69350493\n",
      "Step: [21871] d_loss: 1.38628125, g_loss: 0.69328350\n",
      "Step: [21872] d_loss: 1.38629413, g_loss: 0.69316888\n",
      "Step: [21873] d_loss: 1.38627124, g_loss: 0.69309497\n",
      "Step: [21874] d_loss: 1.38628507, g_loss: 0.69333994\n",
      "Step: [21875] d_loss: 1.38628101, g_loss: 0.69265634\n",
      "Step: [21876] d_loss: 1.38628566, g_loss: 0.69310689\n",
      "Step: [21877] d_loss: 1.38628316, g_loss: 0.69313109\n",
      "Step: [21878] d_loss: 1.38628578, g_loss: 0.69304180\n",
      "Step: [21879] d_loss: 1.38628674, g_loss: 0.69319797\n",
      "Step: [21880] d_loss: 1.38629186, g_loss: 0.69316208\n",
      "Step: [21881] d_loss: 1.38628983, g_loss: 0.69318545\n",
      "Step: [21882] d_loss: 1.38628435, g_loss: 0.69271600\n",
      "Step: [21883] d_loss: 1.38628435, g_loss: 0.69329584\n",
      "Step: [21884] d_loss: 1.38627696, g_loss: 0.69314575\n",
      "Step: [21885] d_loss: 1.38622761, g_loss: 0.69338083\n",
      "Step: [21886] d_loss: 1.38657641, g_loss: 0.69270891\n",
      "Step: [21887] d_loss: 1.38628221, g_loss: 0.69330263\n",
      "Step: [21888] d_loss: 1.38628936, g_loss: 0.69272923\n",
      "Step: [21889] d_loss: 1.38629043, g_loss: 0.69229418\n",
      "Step: [21890] d_loss: 1.38629460, g_loss: 0.69288540\n",
      "Step: [21891] d_loss: 1.38629341, g_loss: 0.69321018\n",
      "Step: [21892] d_loss: 1.38629484, g_loss: 0.69344652\n",
      "Step: [21893] d_loss: 1.38628590, g_loss: 0.69310474\n",
      "Step: [21894] d_loss: 1.38629115, g_loss: 0.69310528\n",
      "Step: [21895] d_loss: 1.38628793, g_loss: 0.69306636\n",
      "Step: [21896] d_loss: 1.38629103, g_loss: 0.69303167\n",
      "Step: [21897] d_loss: 1.38628888, g_loss: 0.69324368\n",
      "Step: [21898] d_loss: 1.38628519, g_loss: 0.69320852\n",
      "Step: [21899] d_loss: 1.38628674, g_loss: 0.69298410\n",
      "Step: [21900] d_loss: 1.38628888, g_loss: 0.69326639\n",
      "Step: [21901] d_loss: 1.38628578, g_loss: 0.69327891\n",
      "Step: [21902] d_loss: 1.38629198, g_loss: 0.69313359\n",
      "Step: [21903] d_loss: 1.38646245, g_loss: 0.69289207\n",
      "Step: [21904] d_loss: 1.38629246, g_loss: 0.69337368\n",
      "Step: [21905] d_loss: 1.38628697, g_loss: 0.69290233\n",
      "Step: [21906] d_loss: 1.38629460, g_loss: 0.69272411\n",
      "Step: [21907] d_loss: 1.38629556, g_loss: 0.69310409\n",
      "Step: [21908] d_loss: 1.38628769, g_loss: 0.69368970\n",
      "Step: [21909] d_loss: 1.38630021, g_loss: 0.69300115\n",
      "Step: [21910] d_loss: 1.38681293, g_loss: 0.69178265\n",
      "Step: [21911] d_loss: 1.38640845, g_loss: 0.69271290\n",
      "Step: [21912] d_loss: 1.38629222, g_loss: 0.69349134\n",
      "Step: [21913] d_loss: 1.38628507, g_loss: 0.69321078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [21914] d_loss: 1.38628983, g_loss: 0.69280869\n",
      "Step: [21915] d_loss: 1.38629651, g_loss: 0.69301039\n",
      "Step: [21916] d_loss: 1.38629341, g_loss: 0.69301736\n",
      "Step: [21917] d_loss: 1.38627088, g_loss: 0.69332331\n",
      "Step: [21918] d_loss: 1.38630843, g_loss: 0.69334823\n",
      "Step: [21919] d_loss: 1.38628960, g_loss: 0.69338971\n",
      "Step: [21920] d_loss: 1.38628840, g_loss: 0.69307053\n",
      "Step: [21921] d_loss: 1.38629222, g_loss: 0.69289672\n",
      "Step: [21922] d_loss: 1.38629007, g_loss: 0.69289172\n",
      "Step: [21923] d_loss: 1.38628197, g_loss: 0.69314718\n",
      "Step: [21924] d_loss: 1.38632870, g_loss: 0.69316232\n",
      "Step: [21925] d_loss: 1.38629043, g_loss: 0.69326627\n",
      "Step: [21926] d_loss: 1.38628531, g_loss: 0.69290799\n",
      "Step: [21927] d_loss: 1.38625705, g_loss: 0.69284606\n",
      "Step: [21928] d_loss: 1.38628566, g_loss: 0.69329131\n",
      "Step: [21929] d_loss: 1.38629782, g_loss: 0.69332731\n",
      "Step: [21930] d_loss: 1.38629234, g_loss: 0.69344848\n",
      "Step: [21931] d_loss: 1.38629913, g_loss: 0.69315100\n",
      "Step: [21932] d_loss: 1.38629508, g_loss: 0.69270587\n",
      "Step: [21933] d_loss: 1.38630009, g_loss: 0.69280887\n",
      "Step: [21934] d_loss: 1.38662124, g_loss: 0.69219434\n",
      "Step: [21935] d_loss: 1.38629186, g_loss: 0.69315588\n",
      "Step: [21936] d_loss: 1.38628244, g_loss: 0.69319737\n",
      "Step: [21937] d_loss: 1.38629007, g_loss: 0.69317073\n",
      "Step: [21938] d_loss: 1.38629770, g_loss: 0.69295716\n",
      "Step: [21939] d_loss: 1.38668370, g_loss: 0.69331300\n",
      "Step: [21940] d_loss: 1.38629496, g_loss: 0.69326568\n",
      "Step: [21941] d_loss: 1.38629925, g_loss: 0.69321966\n",
      "Step: [21942] d_loss: 1.38630176, g_loss: 0.69336033\n",
      "Step: [21943] d_loss: 1.38630903, g_loss: 0.69298983\n",
      "Step: [21944] d_loss: 1.38627541, g_loss: 0.69212919\n",
      "Step: [21945] d_loss: 1.38626146, g_loss: 0.69302601\n",
      "Step: [21946] d_loss: 1.38631070, g_loss: 0.69396293\n",
      "Step: [21947] d_loss: 1.38631892, g_loss: 0.69266260\n",
      "Step: [21948] d_loss: 1.38632917, g_loss: 0.69241381\n",
      "Step: [21949] d_loss: 1.38633084, g_loss: 0.69304001\n",
      "Step: [21950] d_loss: 1.38633204, g_loss: 0.69293684\n",
      "Step: [21951] d_loss: 1.38644338, g_loss: 0.69407940\n",
      "Step: [21952] d_loss: 1.38629341, g_loss: 0.69371217\n",
      "Step: [21953] d_loss: 1.38632846, g_loss: 0.69266188\n",
      "Step: [21954] d_loss: 1.38682306, g_loss: 0.69322133\n",
      "Step: [21955] d_loss: 1.38769162, g_loss: 0.68674272\n",
      "Step: [21956] d_loss: 1.38847947, g_loss: 0.69646776\n",
      "Step: [21957] d_loss: 1.38901293, g_loss: 0.69790161\n",
      "Step: [21958] d_loss: 1.38900971, g_loss: 0.69312620\n",
      "Step: [21959] d_loss: 1.38830805, g_loss: 0.68309021\n",
      "Step: [21960] d_loss: 1.38752961, g_loss: 0.69092005\n",
      "Step: [21961] d_loss: 1.38696754, g_loss: 0.69741869\n",
      "Step: [21962] d_loss: 1.38672829, g_loss: 0.69635886\n",
      "Step: [21963] d_loss: 1.38654912, g_loss: 0.69421852\n",
      "Step: [21964] d_loss: 1.38638568, g_loss: 0.69183964\n",
      "Step: [21965] d_loss: 1.38629055, g_loss: 0.69191003\n",
      "Step: [21966] d_loss: 1.38633251, g_loss: 0.69275725\n",
      "Step: [21967] d_loss: 1.38630724, g_loss: 0.69347805\n",
      "Step: [21968] d_loss: 1.38629246, g_loss: 0.69368821\n",
      "Step: [21969] d_loss: 1.38630438, g_loss: 0.69355947\n",
      "Step: [21970] d_loss: 1.38629627, g_loss: 0.69286036\n",
      "Step: [21971] d_loss: 1.38621128, g_loss: 0.69376469\n",
      "Step: [21972] d_loss: 1.38635755, g_loss: 0.69255441\n",
      "Step: [21973] d_loss: 1.38635635, g_loss: 0.69281507\n",
      "Step: [21974] d_loss: 1.38634300, g_loss: 0.69357407\n",
      "Step: [21975] d_loss: 1.38632619, g_loss: 0.69328380\n",
      "Step: [21976] d_loss: 1.38631022, g_loss: 0.69328332\n",
      "Step: [21977] d_loss: 1.38631022, g_loss: 0.69338369\n",
      "Step: [21978] d_loss: 1.38623869, g_loss: 0.69430929\n",
      "Step: [21979] d_loss: 1.38629818, g_loss: 0.69241041\n",
      "Step: [21980] d_loss: 1.38630819, g_loss: 0.69336933\n",
      "Step: [21981] d_loss: 1.38633513, g_loss: 0.69301778\n",
      "Step: [21982] d_loss: 1.38631880, g_loss: 0.69299436\n",
      "Step: [21983] d_loss: 1.38628912, g_loss: 0.69286203\n",
      "Step: [21984] d_loss: 1.38629329, g_loss: 0.69289416\n",
      "Step: [21985] d_loss: 1.38631630, g_loss: 0.69360912\n",
      "Step: [21986] d_loss: 1.38614833, g_loss: 0.69377100\n",
      "Step: [21987] d_loss: 1.38635743, g_loss: 0.69268161\n",
      "Step: [21988] d_loss: 1.38610494, g_loss: 0.69453526\n",
      "Step: [21989] d_loss: 1.38630033, g_loss: 0.69250178\n",
      "Step: [21990] d_loss: 1.38630450, g_loss: 0.69334906\n",
      "Step: [21991] d_loss: 1.38630700, g_loss: 0.69360119\n",
      "Step: [21992] d_loss: 1.38633013, g_loss: 0.69376588\n",
      "Step: [21993] d_loss: 1.38636160, g_loss: 0.69383919\n",
      "Step: [21994] d_loss: 1.38639224, g_loss: 0.69307542\n",
      "Step: [21995] d_loss: 1.38637328, g_loss: 0.69215822\n",
      "Step: [21996] d_loss: 1.38632810, g_loss: 0.69212276\n",
      "Step: [21997] d_loss: 1.38643646, g_loss: 0.69393039\n",
      "Step: [21998] d_loss: 1.38645232, g_loss: 0.69412893\n",
      "Step: [21999] d_loss: 1.38662732, g_loss: 0.69232404\n",
      "Step: [22000] d_loss: 1.38663173, g_loss: 0.69226742\n",
      "Step: [22001] d_loss: 1.38660491, g_loss: 0.69436395\n",
      "Step: [22002] d_loss: 1.38656688, g_loss: 0.69348919\n",
      "Step: [22003] d_loss: 1.38648891, g_loss: 0.69262516\n",
      "Step: [22004] d_loss: 1.38641107, g_loss: 0.69249558\n",
      "Step: [22005] d_loss: 1.38636875, g_loss: 0.69357765\n",
      "Step: [22006] d_loss: 1.38648224, g_loss: 0.69479275\n",
      "Step: [22007] d_loss: 1.38648248, g_loss: 0.69580710\n",
      "Step: [22008] d_loss: 1.38658822, g_loss: 0.69521719\n",
      "Step: [22009] d_loss: 1.38665366, g_loss: 0.69453192\n",
      "Step: [22010] d_loss: 1.38665044, g_loss: 0.69322044\n",
      "Step: [22011] d_loss: 1.38633180, g_loss: 0.69303024\n",
      "Step: [22012] d_loss: 1.38641894, g_loss: 0.69116199\n",
      "Step: [22013] d_loss: 1.38636398, g_loss: 0.69564021\n",
      "Step: [22014] d_loss: 1.38636780, g_loss: 0.69545871\n",
      "Step: [22015] d_loss: 1.38637209, g_loss: 0.69394487\n",
      "Step: [22016] d_loss: 1.38635015, g_loss: 0.69147426\n",
      "Step: [22017] d_loss: 1.38633263, g_loss: 0.69128323\n",
      "Step: [22018] d_loss: 1.38626337, g_loss: 0.69267809\n",
      "Step: [22019] d_loss: 1.38631010, g_loss: 0.69315362\n",
      "Step: [22020] d_loss: 1.38630438, g_loss: 0.69339889\n",
      "Step: [22021] d_loss: 1.38613009, g_loss: 0.69413888\n",
      "Step: [22022] d_loss: 1.38633049, g_loss: 0.69222921\n",
      "Step: [22023] d_loss: 1.38631511, g_loss: 0.69257450\n",
      "Step: [22024] d_loss: 1.38630462, g_loss: 0.69296968\n",
      "Step: [22025] d_loss: 1.38629913, g_loss: 0.69339085\n",
      "Step: [22026] d_loss: 1.38629782, g_loss: 0.69349658\n",
      "Step: [22027] d_loss: 1.38629854, g_loss: 0.69285667\n",
      "Step: [22028] d_loss: 1.38629639, g_loss: 0.69263232\n",
      "Step: [22029] d_loss: 1.38630128, g_loss: 0.69303489\n",
      "Step: [22030] d_loss: 1.38633728, g_loss: 0.69332600\n",
      "Step: [22031] d_loss: 1.38633204, g_loss: 0.69359487\n",
      "Step: [22032] d_loss: 1.38635683, g_loss: 0.69309127\n",
      "Step: [22033] d_loss: 1.38634753, g_loss: 0.69227636\n",
      "Step: [22034] d_loss: 1.38633001, g_loss: 0.69455934\n",
      "Step: [22035] d_loss: 1.38637900, g_loss: 0.69385660\n",
      "Step: [22036] d_loss: 1.38641715, g_loss: 0.69334978\n",
      "Step: [22037] d_loss: 1.38642418, g_loss: 0.69363081\n",
      "Step: [22038] d_loss: 1.38644719, g_loss: 0.69559121\n",
      "Step: [22039] d_loss: 1.38641572, g_loss: 0.69428152\n",
      "Step: [22040] d_loss: 1.38642609, g_loss: 0.68968779\n",
      "Step: [22041] d_loss: 1.38636708, g_loss: 0.69143283\n",
      "Step: [22042] d_loss: 1.38631892, g_loss: 0.69374287\n",
      "Step: [22043] d_loss: 1.38629913, g_loss: 0.69408053\n",
      "Step: [22044] d_loss: 1.38598502, g_loss: 0.69601136\n",
      "Step: [22045] d_loss: 1.38630641, g_loss: 0.69331896\n",
      "Step: [22046] d_loss: 1.38631999, g_loss: 0.69257510\n",
      "Step: [22047] d_loss: 1.38627124, g_loss: 0.69192356\n",
      "Step: [22048] d_loss: 1.38629389, g_loss: 0.69256645\n",
      "Step: [22049] d_loss: 1.38629436, g_loss: 0.69360375\n",
      "Step: [22050] d_loss: 1.38630056, g_loss: 0.69365573\n",
      "Step: [22051] d_loss: 1.38613200, g_loss: 0.69447613\n",
      "Step: [22052] d_loss: 1.38630009, g_loss: 0.69273090\n",
      "Step: [22053] d_loss: 1.38630962, g_loss: 0.69313997\n",
      "Step: [22054] d_loss: 1.38632727, g_loss: 0.69326937\n",
      "Step: [22055] d_loss: 1.38634658, g_loss: 0.69277990\n",
      "Step: [22056] d_loss: 1.38633418, g_loss: 0.69257319\n",
      "Step: [22057] d_loss: 1.38632119, g_loss: 0.69323504\n",
      "Step: [22058] d_loss: 1.38632464, g_loss: 0.69374478\n",
      "Step: [22059] d_loss: 1.38633251, g_loss: 0.69347459\n",
      "Step: [22060] d_loss: 1.38630676, g_loss: 0.69270271\n",
      "Step: [22061] d_loss: 1.38642454, g_loss: 0.69401109\n",
      "Step: [22062] d_loss: 1.38636458, g_loss: 0.69458604\n",
      "Step: [22063] d_loss: 1.38641286, g_loss: 0.69244254\n",
      "Step: [22064] d_loss: 1.38640523, g_loss: 0.69090199\n",
      "Step: [22065] d_loss: 1.38636422, g_loss: 0.69208622\n",
      "Step: [22066] d_loss: 1.38634276, g_loss: 0.69368112\n",
      "Step: [22067] d_loss: 1.38633001, g_loss: 0.69411838\n",
      "Step: [22068] d_loss: 1.38629246, g_loss: 0.69372308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22069] d_loss: 1.38629830, g_loss: 0.69220543\n",
      "Step: [22070] d_loss: 1.38628829, g_loss: 0.69285542\n",
      "Step: [22071] d_loss: 1.38629878, g_loss: 0.69316816\n",
      "Step: [22072] d_loss: 1.38629210, g_loss: 0.69333148\n",
      "Step: [22073] d_loss: 1.38629293, g_loss: 0.69310844\n",
      "Step: [22074] d_loss: 1.38623583, g_loss: 0.69324994\n",
      "Step: [22075] d_loss: 1.38629341, g_loss: 0.69294870\n",
      "Step: [22076] d_loss: 1.38634932, g_loss: 0.69291133\n",
      "Step: [22077] d_loss: 1.38632298, g_loss: 0.69457710\n",
      "Step: [22078] d_loss: 1.38667417, g_loss: 0.69653273\n",
      "Step: [22079] d_loss: 1.38690424, g_loss: 0.69437784\n",
      "Step: [22080] d_loss: 1.38737941, g_loss: 0.69688010\n",
      "Step: [22081] d_loss: 1.38737154, g_loss: 0.69386685\n",
      "Step: [22082] d_loss: 1.38726747, g_loss: 0.69233471\n",
      "Step: [22083] d_loss: 1.38715887, g_loss: 0.69351918\n",
      "Step: [22084] d_loss: 1.38698971, g_loss: 0.69346112\n",
      "Step: [22085] d_loss: 1.38673627, g_loss: 0.69108206\n",
      "Step: [22086] d_loss: 1.38661218, g_loss: 0.69412917\n",
      "Step: [22087] d_loss: 1.38639867, g_loss: 0.69292182\n",
      "Step: [22088] d_loss: 1.38643575, g_loss: 0.69319111\n",
      "Step: [22089] d_loss: 1.38645816, g_loss: 0.69210243\n",
      "Step: [22090] d_loss: 1.38633943, g_loss: 0.69322455\n",
      "Step: [22091] d_loss: 1.38664424, g_loss: 0.69407523\n",
      "Step: [22092] d_loss: 1.38687468, g_loss: 0.69636428\n",
      "Step: [22093] d_loss: 1.38694429, g_loss: 0.69769484\n",
      "Step: [22094] d_loss: 1.38695168, g_loss: 0.70149231\n",
      "Step: [22095] d_loss: 1.38659644, g_loss: 0.69468510\n",
      "Step: [22096] d_loss: 1.38642120, g_loss: 0.69282550\n",
      "Step: [22097] d_loss: 1.38632655, g_loss: 0.69123995\n",
      "Step: [22098] d_loss: 1.38624597, g_loss: 0.69087267\n",
      "Step: [22099] d_loss: 1.38629949, g_loss: 0.69353145\n",
      "Step: [22100] d_loss: 1.38630855, g_loss: 0.69386339\n",
      "Step: [22101] d_loss: 1.38631916, g_loss: 0.69316608\n",
      "Step: [22102] d_loss: 1.38631451, g_loss: 0.69352227\n",
      "Step: [22103] d_loss: 1.38631129, g_loss: 0.69274998\n",
      "Step: [22104] d_loss: 1.38630033, g_loss: 0.69290328\n",
      "Step: [22105] d_loss: 1.38630450, g_loss: 0.69340968\n",
      "Step: [22106] d_loss: 1.38629746, g_loss: 0.69206679\n",
      "Step: [22107] d_loss: 1.38629746, g_loss: 0.69255501\n",
      "Step: [22108] d_loss: 1.38629365, g_loss: 0.69306749\n",
      "Step: [22109] d_loss: 1.38629270, g_loss: 0.69410276\n",
      "Step: [22110] d_loss: 1.38629723, g_loss: 0.69304395\n",
      "Step: [22111] d_loss: 1.38628817, g_loss: 0.69262922\n",
      "Step: [22112] d_loss: 1.38619208, g_loss: 0.69308633\n",
      "Step: [22113] d_loss: 1.38629770, g_loss: 0.69332570\n",
      "Step: [22114] d_loss: 1.38632154, g_loss: 0.69284028\n",
      "Step: [22115] d_loss: 1.38629603, g_loss: 0.69326073\n",
      "Step: [22116] d_loss: 1.38629842, g_loss: 0.69342268\n",
      "Step: [22117] d_loss: 1.38630390, g_loss: 0.69194973\n",
      "Step: [22118] d_loss: 1.38628697, g_loss: 0.69283962\n",
      "Step: [22119] d_loss: 1.38629282, g_loss: 0.69348156\n",
      "Step: [22120] d_loss: 1.38629651, g_loss: 0.69334257\n",
      "Step: [22121] d_loss: 1.38629031, g_loss: 0.69298589\n",
      "Step: [22122] d_loss: 1.38629878, g_loss: 0.69316268\n",
      "Step: [22123] d_loss: 1.38628721, g_loss: 0.69310272\n",
      "Step: [22124] d_loss: 1.38628697, g_loss: 0.69329661\n",
      "Step: [22125] d_loss: 1.38650429, g_loss: 0.69298041\n",
      "Step: [22126] d_loss: 1.38629448, g_loss: 0.69351047\n",
      "Step: [22127] d_loss: 1.38628983, g_loss: 0.69273567\n",
      "Step: [22128] d_loss: 1.38629889, g_loss: 0.69338238\n",
      "Step: [22129] d_loss: 1.38629222, g_loss: 0.69312394\n",
      "Step: [22130] d_loss: 1.38628960, g_loss: 0.69303000\n",
      "Step: [22131] d_loss: 1.38628650, g_loss: 0.69334096\n",
      "Step: [22132] d_loss: 1.38631201, g_loss: 0.69236481\n",
      "Step: [22133] d_loss: 1.38629031, g_loss: 0.69306868\n",
      "Step: [22134] d_loss: 1.38628674, g_loss: 0.69294763\n",
      "Step: [22135] d_loss: 1.38617539, g_loss: 0.69313109\n",
      "Step: [22136] d_loss: 1.38629794, g_loss: 0.69363809\n",
      "Step: [22137] d_loss: 1.38627887, g_loss: 0.69259346\n",
      "Step: [22138] d_loss: 1.38641453, g_loss: 0.69306040\n",
      "Step: [22139] d_loss: 1.38671875, g_loss: 0.69227290\n",
      "Step: [22140] d_loss: 1.38687611, g_loss: 0.69058937\n",
      "Step: [22141] d_loss: 1.38688707, g_loss: 0.69216514\n",
      "Step: [22142] d_loss: 1.38678980, g_loss: 0.69177604\n",
      "Step: [22143] d_loss: 1.38665533, g_loss: 0.69225407\n",
      "Step: [22144] d_loss: 1.38654459, g_loss: 0.69342518\n",
      "Step: [22145] d_loss: 1.38646650, g_loss: 0.69510144\n",
      "Step: [22146] d_loss: 1.38647759, g_loss: 0.69463402\n",
      "Step: [22147] d_loss: 1.38644838, g_loss: 0.69319314\n",
      "Step: [22148] d_loss: 1.38639152, g_loss: 0.69132471\n",
      "Step: [22149] d_loss: 1.38636243, g_loss: 0.69210625\n",
      "Step: [22150] d_loss: 1.38662219, g_loss: 0.69224477\n",
      "Step: [22151] d_loss: 1.38642275, g_loss: 0.69271982\n",
      "Step: [22152] d_loss: 1.38631117, g_loss: 0.69310939\n",
      "Step: [22153] d_loss: 1.38629556, g_loss: 0.69327790\n",
      "Step: [22154] d_loss: 1.38629174, g_loss: 0.69286740\n",
      "Step: [22155] d_loss: 1.38629222, g_loss: 0.69319189\n",
      "Step: [22156] d_loss: 1.38628542, g_loss: 0.69315302\n",
      "Step: [22157] d_loss: 1.38685834, g_loss: 0.69289058\n",
      "Step: [22158] d_loss: 1.38629687, g_loss: 0.69284695\n",
      "Step: [22159] d_loss: 1.38629580, g_loss: 0.69317412\n",
      "Step: [22160] d_loss: 1.38628650, g_loss: 0.69329560\n",
      "Step: [22161] d_loss: 1.38632989, g_loss: 0.69321597\n",
      "Step: [22162] d_loss: 1.38629580, g_loss: 0.69325030\n",
      "Step: [22163] d_loss: 1.38629520, g_loss: 0.69301742\n",
      "Step: [22164] d_loss: 1.38629484, g_loss: 0.69303805\n",
      "Step: [22165] d_loss: 1.38629353, g_loss: 0.69320077\n",
      "Step: [22166] d_loss: 1.38628960, g_loss: 0.69301069\n",
      "Step: [22167] d_loss: 1.38629401, g_loss: 0.69322407\n",
      "Step: [22168] d_loss: 1.38629866, g_loss: 0.69323623\n",
      "Step: [22169] d_loss: 1.38629782, g_loss: 0.69319892\n",
      "Step: [22170] d_loss: 1.38630664, g_loss: 0.69330889\n",
      "Step: [22171] d_loss: 1.38630593, g_loss: 0.69306350\n",
      "Step: [22172] d_loss: 1.38630223, g_loss: 0.69329274\n",
      "Step: [22173] d_loss: 1.38631129, g_loss: 0.69305134\n",
      "Step: [22174] d_loss: 1.38628662, g_loss: 0.69331902\n",
      "Step: [22175] d_loss: 1.38631678, g_loss: 0.69356143\n",
      "Step: [22176] d_loss: 1.38648534, g_loss: 0.69359922\n",
      "Step: [22177] d_loss: 1.38635516, g_loss: 0.69361091\n",
      "Step: [22178] d_loss: 1.38637006, g_loss: 0.69365966\n",
      "Step: [22179] d_loss: 1.38637829, g_loss: 0.69256592\n",
      "Step: [22180] d_loss: 1.38637924, g_loss: 0.69293892\n",
      "Step: [22181] d_loss: 1.38636088, g_loss: 0.69253677\n",
      "Step: [22182] d_loss: 1.38634706, g_loss: 0.69228786\n",
      "Step: [22183] d_loss: 1.38632560, g_loss: 0.69309574\n",
      "Step: [22184] d_loss: 1.38631761, g_loss: 0.69350493\n",
      "Step: [22185] d_loss: 1.38631058, g_loss: 0.69321883\n",
      "Step: [22186] d_loss: 1.38659787, g_loss: 0.69368649\n",
      "Step: [22187] d_loss: 1.38634098, g_loss: 0.69457757\n",
      "Step: [22188] d_loss: 1.38631642, g_loss: 0.69417751\n",
      "Step: [22189] d_loss: 1.38653708, g_loss: 0.69324756\n",
      "Step: [22190] d_loss: 1.38647377, g_loss: 0.69327843\n",
      "Step: [22191] d_loss: 1.38631845, g_loss: 0.69180506\n",
      "Step: [22192] d_loss: 1.38643253, g_loss: 0.69210166\n",
      "Step: [22193] d_loss: 1.38640964, g_loss: 0.69333744\n",
      "Step: [22194] d_loss: 1.38638067, g_loss: 0.69242251\n",
      "Step: [22195] d_loss: 1.38637829, g_loss: 0.69154811\n",
      "Step: [22196] d_loss: 1.38633060, g_loss: 0.69258207\n",
      "Step: [22197] d_loss: 1.38632417, g_loss: 0.69366771\n",
      "Step: [22198] d_loss: 1.38632202, g_loss: 0.69254923\n",
      "Step: [22199] d_loss: 1.38630295, g_loss: 0.69215429\n",
      "Step: [22200] d_loss: 1.38646007, g_loss: 0.69290793\n",
      "Step: [22201] d_loss: 1.38631344, g_loss: 0.69329500\n",
      "Step: [22202] d_loss: 1.38632739, g_loss: 0.69378328\n",
      "Step: [22203] d_loss: 1.38647962, g_loss: 0.69290912\n",
      "Step: [22204] d_loss: 1.38631868, g_loss: 0.69294465\n",
      "Step: [22205] d_loss: 1.38628650, g_loss: 0.69279355\n",
      "Step: [22206] d_loss: 1.38630545, g_loss: 0.69320154\n",
      "Step: [22207] d_loss: 1.38631070, g_loss: 0.69339705\n",
      "Step: [22208] d_loss: 1.38630700, g_loss: 0.69364893\n",
      "Step: [22209] d_loss: 1.38632488, g_loss: 0.69352221\n",
      "Step: [22210] d_loss: 1.38633370, g_loss: 0.69342148\n",
      "Step: [22211] d_loss: 1.38632357, g_loss: 0.69272888\n",
      "Step: [22212] d_loss: 1.38633585, g_loss: 0.69284958\n",
      "Step: [22213] d_loss: 1.38633764, g_loss: 0.69360024\n",
      "Step: [22214] d_loss: 1.38633704, g_loss: 0.69316685\n",
      "Step: [22215] d_loss: 1.38632488, g_loss: 0.69266242\n",
      "Step: [22216] d_loss: 1.38631821, g_loss: 0.69296706\n",
      "Step: [22217] d_loss: 1.38624501, g_loss: 0.69288409\n",
      "Step: [22218] d_loss: 1.38615239, g_loss: 0.69285119\n",
      "Step: [22219] d_loss: 1.38625860, g_loss: 0.69267559\n",
      "Step: [22220] d_loss: 1.38629508, g_loss: 0.69304520\n",
      "Step: [22221] d_loss: 1.38630486, g_loss: 0.69361866\n",
      "Step: [22222] d_loss: 1.38635421, g_loss: 0.69441026\n",
      "Step: [22223] d_loss: 1.38657033, g_loss: 0.69286048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22224] d_loss: 1.38676882, g_loss: 0.69349444\n",
      "Step: [22225] d_loss: 1.38687527, g_loss: 0.69199795\n",
      "Step: [22226] d_loss: 1.38693964, g_loss: 0.69407928\n",
      "Step: [22227] d_loss: 1.38696980, g_loss: 0.69233060\n",
      "Step: [22228] d_loss: 1.38690257, g_loss: 0.68885326\n",
      "Step: [22229] d_loss: 1.38674581, g_loss: 0.68983388\n",
      "Step: [22230] d_loss: 1.38655376, g_loss: 0.69381660\n",
      "Step: [22231] d_loss: 1.38667083, g_loss: 0.69376773\n",
      "Step: [22232] d_loss: 1.38665485, g_loss: 0.69385874\n",
      "Step: [22233] d_loss: 1.38658452, g_loss: 0.69249535\n",
      "Step: [22234] d_loss: 1.38644695, g_loss: 0.69023889\n",
      "Step: [22235] d_loss: 1.38636208, g_loss: 0.69391686\n",
      "Step: [22236] d_loss: 1.38633490, g_loss: 0.69590384\n",
      "Step: [22237] d_loss: 1.38638818, g_loss: 0.69451869\n",
      "Step: [22238] d_loss: 1.38643169, g_loss: 0.69427311\n",
      "Step: [22239] d_loss: 1.38647509, g_loss: 0.69255996\n",
      "Step: [22240] d_loss: 1.38645482, g_loss: 0.69156599\n",
      "Step: [22241] d_loss: 1.38637841, g_loss: 0.69206131\n",
      "Step: [22242] d_loss: 1.38632429, g_loss: 0.69251460\n",
      "Step: [22243] d_loss: 1.38628769, g_loss: 0.69396341\n",
      "Step: [22244] d_loss: 1.38647759, g_loss: 0.69436133\n",
      "Step: [22245] d_loss: 1.38627565, g_loss: 0.69263542\n",
      "Step: [22246] d_loss: 1.38643718, g_loss: 0.69264930\n",
      "Step: [22247] d_loss: 1.38741827, g_loss: 0.69541252\n",
      "Step: [22248] d_loss: 1.38863623, g_loss: 0.69277292\n",
      "Step: [22249] d_loss: 1.38869047, g_loss: 0.68539488\n",
      "Step: [22250] d_loss: 1.38802207, g_loss: 0.69108367\n",
      "Step: [22251] d_loss: 1.38735580, g_loss: 0.69738019\n",
      "Step: [22252] d_loss: 1.38696921, g_loss: 0.69609427\n",
      "Step: [22253] d_loss: 1.38667250, g_loss: 0.69523019\n",
      "Step: [22254] d_loss: 1.38657212, g_loss: 0.69446015\n",
      "Step: [22255] d_loss: 1.38630724, g_loss: 0.69036740\n",
      "Step: [22256] d_loss: 1.38644707, g_loss: 0.69162667\n",
      "Step: [22257] d_loss: 1.38714564, g_loss: 0.69659615\n",
      "Step: [22258] d_loss: 1.38802803, g_loss: 0.69744474\n",
      "Step: [22259] d_loss: 1.38802457, g_loss: 0.69571066\n",
      "Step: [22260] d_loss: 1.38746786, g_loss: 0.69715333\n",
      "Step: [22261] d_loss: 1.38693357, g_loss: 0.69523072\n",
      "Step: [22262] d_loss: 1.38653660, g_loss: 0.69258988\n",
      "Step: [22263] d_loss: 1.38633072, g_loss: 0.69070876\n",
      "Step: [22264] d_loss: 1.38631082, g_loss: 0.69227743\n",
      "Step: [22265] d_loss: 1.38635993, g_loss: 0.69275248\n",
      "Step: [22266] d_loss: 1.38635635, g_loss: 0.69346881\n",
      "Step: [22267] d_loss: 1.38634288, g_loss: 0.69381440\n",
      "Step: [22268] d_loss: 1.38632512, g_loss: 0.69346058\n",
      "Step: [22269] d_loss: 1.38630068, g_loss: 0.69310451\n",
      "Step: [22270] d_loss: 1.38629270, g_loss: 0.69270325\n",
      "Step: [22271] d_loss: 1.38629830, g_loss: 0.69289851\n",
      "Step: [22272] d_loss: 1.38629079, g_loss: 0.69340611\n",
      "Step: [22273] d_loss: 1.38629735, g_loss: 0.69324136\n",
      "Step: [22274] d_loss: 1.38629913, g_loss: 0.69291788\n",
      "Step: [22275] d_loss: 1.38629508, g_loss: 0.69378990\n",
      "Step: [22276] d_loss: 1.38630593, g_loss: 0.69325656\n",
      "Step: [22277] d_loss: 1.38631737, g_loss: 0.69300258\n",
      "Step: [22278] d_loss: 1.38630676, g_loss: 0.69291401\n",
      "Step: [22279] d_loss: 1.38629651, g_loss: 0.69311893\n",
      "Step: [22280] d_loss: 1.38630211, g_loss: 0.69318026\n",
      "Step: [22281] d_loss: 1.38630033, g_loss: 0.69326675\n",
      "Step: [22282] d_loss: 1.38631105, g_loss: 0.69349182\n",
      "Step: [22283] d_loss: 1.38632369, g_loss: 0.69330353\n",
      "Step: [22284] d_loss: 1.38628125, g_loss: 0.69295752\n",
      "Step: [22285] d_loss: 1.38629115, g_loss: 0.69300485\n",
      "Step: [22286] d_loss: 1.38610041, g_loss: 0.69425297\n",
      "Step: [22287] d_loss: 1.38628864, g_loss: 0.69289553\n",
      "Step: [22288] d_loss: 1.38637280, g_loss: 0.69331539\n",
      "Step: [22289] d_loss: 1.38674486, g_loss: 0.69660008\n",
      "Step: [22290] d_loss: 1.38736224, g_loss: 0.69544268\n",
      "Step: [22291] d_loss: 1.38734031, g_loss: 0.69180858\n",
      "Step: [22292] d_loss: 1.38684309, g_loss: 0.69100827\n",
      "Step: [22293] d_loss: 1.38648391, g_loss: 0.69194227\n",
      "Step: [22294] d_loss: 1.38668740, g_loss: 0.69232845\n",
      "Step: [22295] d_loss: 1.38735723, g_loss: 0.69510204\n",
      "Step: [22296] d_loss: 1.38801885, g_loss: 0.69605124\n",
      "Step: [22297] d_loss: 1.38773334, g_loss: 0.69687140\n",
      "Step: [22298] d_loss: 1.38700354, g_loss: 0.69326264\n",
      "Step: [22299] d_loss: 1.38647318, g_loss: 0.69016773\n",
      "Step: [22300] d_loss: 1.38630772, g_loss: 0.69145352\n",
      "Step: [22301] d_loss: 1.38629353, g_loss: 0.69272113\n",
      "Step: [22302] d_loss: 1.38631797, g_loss: 0.69319063\n",
      "Step: [22303] d_loss: 1.38628817, g_loss: 0.69387853\n",
      "Step: [22304] d_loss: 1.38630772, g_loss: 0.69353545\n",
      "Step: [22305] d_loss: 1.38631654, g_loss: 0.69289219\n",
      "Step: [22306] d_loss: 1.38630438, g_loss: 0.69268584\n",
      "Step: [22307] d_loss: 1.38629961, g_loss: 0.69313020\n",
      "Step: [22308] d_loss: 1.38632822, g_loss: 0.69379437\n",
      "Step: [22309] d_loss: 1.38634253, g_loss: 0.69301593\n",
      "Step: [22310] d_loss: 1.38634324, g_loss: 0.69258547\n",
      "Step: [22311] d_loss: 1.38609993, g_loss: 0.69376719\n",
      "Step: [22312] d_loss: 1.38629389, g_loss: 0.69307542\n",
      "Step: [22313] d_loss: 1.38630199, g_loss: 0.69234633\n",
      "Step: [22314] d_loss: 1.38629413, g_loss: 0.69317520\n",
      "Step: [22315] d_loss: 1.38629365, g_loss: 0.69342583\n",
      "Step: [22316] d_loss: 1.38630557, g_loss: 0.69388449\n",
      "Step: [22317] d_loss: 1.38631999, g_loss: 0.69331014\n",
      "Step: [22318] d_loss: 1.38630664, g_loss: 0.69240463\n",
      "Step: [22319] d_loss: 1.38627756, g_loss: 0.69276357\n",
      "Step: [22320] d_loss: 1.38633227, g_loss: 0.69292909\n",
      "Step: [22321] d_loss: 1.38629925, g_loss: 0.69272244\n",
      "Step: [22322] d_loss: 1.38629210, g_loss: 0.69350547\n",
      "Step: [22323] d_loss: 1.38629985, g_loss: 0.69364792\n",
      "Step: [22324] d_loss: 1.38626039, g_loss: 0.69443196\n",
      "Step: [22325] d_loss: 1.38635659, g_loss: 0.69353545\n",
      "Step: [22326] d_loss: 1.38635325, g_loss: 0.69269687\n",
      "Step: [22327] d_loss: 1.38632715, g_loss: 0.69323105\n",
      "Step: [22328] d_loss: 1.38630831, g_loss: 0.69395059\n",
      "Step: [22329] d_loss: 1.38638592, g_loss: 0.69366884\n",
      "Step: [22330] d_loss: 1.38628483, g_loss: 0.69245368\n",
      "Step: [22331] d_loss: 1.38628256, g_loss: 0.69270468\n",
      "Step: [22332] d_loss: 1.38628983, g_loss: 0.69310331\n",
      "Step: [22333] d_loss: 1.38630581, g_loss: 0.69371557\n",
      "Step: [22334] d_loss: 1.38628125, g_loss: 0.69361979\n",
      "Step: [22335] d_loss: 1.38637042, g_loss: 0.69322866\n",
      "Step: [22336] d_loss: 1.38631105, g_loss: 0.69342399\n",
      "Step: [22337] d_loss: 1.38635945, g_loss: 0.69426239\n",
      "Step: [22338] d_loss: 1.38644814, g_loss: 0.69324911\n",
      "Step: [22339] d_loss: 1.38634121, g_loss: 0.69248998\n",
      "Step: [22340] d_loss: 1.38636327, g_loss: 0.69279903\n",
      "Step: [22341] d_loss: 1.38628340, g_loss: 0.69247174\n",
      "Step: [22342] d_loss: 1.38629746, g_loss: 0.69338274\n",
      "Step: [22343] d_loss: 1.38642192, g_loss: 0.69438541\n",
      "Step: [22344] d_loss: 1.38637352, g_loss: 0.69468856\n",
      "Step: [22345] d_loss: 1.38655424, g_loss: 0.69560802\n",
      "Step: [22346] d_loss: 1.38653445, g_loss: 0.69439143\n",
      "Step: [22347] d_loss: 1.38647699, g_loss: 0.69165266\n",
      "Step: [22348] d_loss: 1.38639069, g_loss: 0.69175947\n",
      "Step: [22349] d_loss: 1.38632441, g_loss: 0.69289684\n",
      "Step: [22350] d_loss: 1.38630795, g_loss: 0.69450963\n",
      "Step: [22351] d_loss: 1.38646698, g_loss: 0.69486594\n",
      "Step: [22352] d_loss: 1.38635182, g_loss: 0.69394743\n",
      "Step: [22353] d_loss: 1.38634813, g_loss: 0.69383591\n",
      "Step: [22354] d_loss: 1.38635242, g_loss: 0.69279099\n",
      "Step: [22355] d_loss: 1.38632190, g_loss: 0.69321704\n",
      "Step: [22356] d_loss: 1.38631451, g_loss: 0.69293541\n",
      "Step: [22357] d_loss: 1.38630235, g_loss: 0.69311309\n",
      "Step: [22358] d_loss: 1.38630438, g_loss: 0.69322056\n",
      "Step: [22359] d_loss: 1.38633084, g_loss: 0.69317532\n",
      "Step: [22360] d_loss: 1.38641942, g_loss: 0.69398630\n",
      "Step: [22361] d_loss: 1.38629401, g_loss: 0.69328177\n",
      "Step: [22362] d_loss: 1.38637173, g_loss: 0.69272327\n",
      "Step: [22363] d_loss: 1.38618529, g_loss: 0.69282132\n",
      "Step: [22364] d_loss: 1.38630342, g_loss: 0.69278914\n",
      "Step: [22365] d_loss: 1.38629842, g_loss: 0.69327223\n",
      "Step: [22366] d_loss: 1.38613355, g_loss: 0.69362402\n",
      "Step: [22367] d_loss: 1.38630283, g_loss: 0.69316220\n",
      "Step: [22368] d_loss: 1.38606858, g_loss: 0.69499058\n",
      "Step: [22369] d_loss: 1.38634288, g_loss: 0.69325441\n",
      "Step: [22370] d_loss: 1.38636386, g_loss: 0.69350684\n",
      "Step: [22371] d_loss: 1.38636076, g_loss: 0.69388151\n",
      "Step: [22372] d_loss: 1.38634646, g_loss: 0.69288719\n",
      "Step: [22373] d_loss: 1.38632023, g_loss: 0.69336003\n",
      "Step: [22374] d_loss: 1.38631320, g_loss: 0.69334787\n",
      "Step: [22375] d_loss: 1.38631940, g_loss: 0.69345462\n",
      "Step: [22376] d_loss: 1.38632071, g_loss: 0.69347620\n",
      "Step: [22377] d_loss: 1.38630784, g_loss: 0.69321704\n",
      "Step: [22378] d_loss: 1.38630009, g_loss: 0.69305098\n",
      "Step: [22379] d_loss: 1.38629413, g_loss: 0.69294178\n",
      "Step: [22380] d_loss: 1.38630033, g_loss: 0.69313985\n",
      "Step: [22381] d_loss: 1.38627839, g_loss: 0.69334155\n",
      "Step: [22382] d_loss: 1.38629723, g_loss: 0.69315720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22383] d_loss: 1.38631952, g_loss: 0.69313979\n",
      "Step: [22384] d_loss: 1.38629103, g_loss: 0.69315076\n",
      "Step: [22385] d_loss: 1.38629341, g_loss: 0.69338387\n",
      "Step: [22386] d_loss: 1.38630581, g_loss: 0.69277138\n",
      "Step: [22387] d_loss: 1.38586152, g_loss: 0.69370699\n",
      "Step: [22388] d_loss: 1.38630223, g_loss: 0.69306326\n",
      "Step: [22389] d_loss: 1.38633013, g_loss: 0.69421518\n",
      "Step: [22390] d_loss: 1.38632655, g_loss: 0.69378114\n",
      "Step: [22391] d_loss: 1.38631225, g_loss: 0.69260007\n",
      "Step: [22392] d_loss: 1.38645625, g_loss: 0.69185698\n",
      "Step: [22393] d_loss: 1.38655674, g_loss: 0.69315577\n",
      "Step: [22394] d_loss: 1.38657570, g_loss: 0.69369006\n",
      "Step: [22395] d_loss: 1.38640761, g_loss: 0.69506776\n",
      "Step: [22396] d_loss: 1.38644719, g_loss: 0.69384021\n",
      "Step: [22397] d_loss: 1.38670123, g_loss: 0.69372535\n",
      "Step: [22398] d_loss: 1.38640809, g_loss: 0.69498438\n",
      "Step: [22399] d_loss: 1.38638330, g_loss: 0.69386399\n",
      "Step: [22400] d_loss: 1.38635921, g_loss: 0.69261944\n",
      "Step: [22401] d_loss: 1.38633156, g_loss: 0.69263065\n",
      "Step: [22402] d_loss: 1.38629782, g_loss: 0.69278657\n",
      "Step: [22403] d_loss: 1.38629723, g_loss: 0.69322413\n",
      "Step: [22404] d_loss: 1.38629544, g_loss: 0.69348657\n",
      "Step: [22405] d_loss: 1.38630021, g_loss: 0.69343364\n",
      "Step: [22406] d_loss: 1.38629341, g_loss: 0.69295955\n",
      "Step: [22407] d_loss: 1.38629603, g_loss: 0.69295859\n",
      "Step: [22408] d_loss: 1.38629055, g_loss: 0.69290805\n",
      "Step: [22409] d_loss: 1.38630044, g_loss: 0.69344896\n",
      "Step: [22410] d_loss: 1.38628519, g_loss: 0.69319105\n",
      "Step: [22411] d_loss: 1.38629293, g_loss: 0.69331646\n",
      "Step: [22412] d_loss: 1.38631630, g_loss: 0.69316691\n",
      "Step: [22413] d_loss: 1.38630819, g_loss: 0.69360018\n",
      "Step: [22414] d_loss: 1.38631952, g_loss: 0.69357133\n",
      "Step: [22415] d_loss: 1.38630629, g_loss: 0.69281101\n",
      "Step: [22416] d_loss: 1.38629758, g_loss: 0.69304079\n",
      "Step: [22417] d_loss: 1.38629282, g_loss: 0.69326568\n",
      "Step: [22418] d_loss: 1.38627064, g_loss: 0.69312638\n",
      "Step: [22419] d_loss: 1.38629150, g_loss: 0.69362849\n",
      "Step: [22420] d_loss: 1.38607144, g_loss: 0.69380558\n",
      "Step: [22421] d_loss: 1.38628972, g_loss: 0.69330877\n",
      "Step: [22422] d_loss: 1.38632774, g_loss: 0.69231081\n",
      "Step: [22423] d_loss: 1.38631320, g_loss: 0.69219893\n",
      "Step: [22424] d_loss: 1.38629770, g_loss: 0.69316518\n",
      "Step: [22425] d_loss: 1.38628697, g_loss: 0.69343209\n",
      "Step: [22426] d_loss: 1.38629889, g_loss: 0.69373000\n",
      "Step: [22427] d_loss: 1.38629758, g_loss: 0.69311488\n",
      "Step: [22428] d_loss: 1.38629305, g_loss: 0.69291818\n",
      "Step: [22429] d_loss: 1.38627398, g_loss: 0.69352025\n",
      "Step: [22430] d_loss: 1.38628447, g_loss: 0.69393790\n",
      "Step: [22431] d_loss: 1.38632917, g_loss: 0.69276500\n",
      "Step: [22432] d_loss: 1.38628960, g_loss: 0.69302207\n",
      "Step: [22433] d_loss: 1.38629997, g_loss: 0.69313711\n",
      "Step: [22434] d_loss: 1.38629818, g_loss: 0.69292676\n",
      "Step: [22435] d_loss: 1.38630056, g_loss: 0.69341218\n",
      "Step: [22436] d_loss: 1.38630867, g_loss: 0.69351488\n",
      "Step: [22437] d_loss: 1.38631105, g_loss: 0.69327807\n",
      "Step: [22438] d_loss: 1.38629961, g_loss: 0.69319409\n",
      "Step: [22439] d_loss: 1.38629198, g_loss: 0.69274592\n",
      "Step: [22440] d_loss: 1.38626051, g_loss: 0.69298017\n",
      "Step: [22441] d_loss: 1.38629723, g_loss: 0.69327176\n",
      "Step: [22442] d_loss: 1.38628888, g_loss: 0.69323790\n",
      "Step: [22443] d_loss: 1.38627267, g_loss: 0.69279420\n",
      "Step: [22444] d_loss: 1.38628721, g_loss: 0.69291651\n",
      "Step: [22445] d_loss: 1.38622952, g_loss: 0.69343162\n",
      "Step: [22446] d_loss: 1.38626719, g_loss: 0.69343448\n",
      "Step: [22447] d_loss: 1.38628316, g_loss: 0.69367474\n",
      "Step: [22448] d_loss: 1.38626885, g_loss: 0.69326866\n",
      "Step: [22449] d_loss: 1.38625979, g_loss: 0.69253016\n",
      "Step: [22450] d_loss: 1.38624835, g_loss: 0.69313002\n",
      "Step: [22451] d_loss: 1.38628221, g_loss: 0.69266003\n",
      "Step: [22452] d_loss: 1.38626766, g_loss: 0.69308817\n",
      "Step: [22453] d_loss: 1.38618445, g_loss: 0.69315326\n",
      "Step: [22454] d_loss: 1.38620794, g_loss: 0.69313002\n",
      "Step: [22455] d_loss: 1.38630211, g_loss: 0.69336569\n",
      "Step: [22456] d_loss: 1.38629270, g_loss: 0.69275910\n",
      "Step: [22457] d_loss: 1.38630331, g_loss: 0.69301736\n",
      "Step: [22458] d_loss: 1.38627410, g_loss: 0.69353354\n",
      "Step: [22459] d_loss: 1.38627005, g_loss: 0.69340360\n",
      "Step: [22460] d_loss: 1.38625646, g_loss: 0.69327366\n",
      "Step: [22461] d_loss: 1.38626134, g_loss: 0.69307888\n",
      "Step: [22462] d_loss: 1.38633156, g_loss: 0.69222265\n",
      "Step: [22463] d_loss: 1.38650072, g_loss: 0.69200647\n",
      "Step: [22464] d_loss: 1.38699150, g_loss: 0.68988496\n",
      "Step: [22465] d_loss: 1.38716578, g_loss: 0.69099605\n",
      "Step: [22466] d_loss: 1.38707423, g_loss: 0.69267380\n",
      "Step: [22467] d_loss: 1.38687158, g_loss: 0.69117439\n",
      "Step: [22468] d_loss: 1.38654065, g_loss: 0.69127464\n",
      "Step: [22469] d_loss: 1.38638675, g_loss: 0.69317693\n",
      "Step: [22470] d_loss: 1.38634253, g_loss: 0.69448262\n",
      "Step: [22471] d_loss: 1.38631868, g_loss: 0.69412583\n",
      "Step: [22472] d_loss: 1.38631392, g_loss: 0.69339144\n",
      "Step: [22473] d_loss: 1.38630915, g_loss: 0.69268036\n",
      "Step: [22474] d_loss: 1.38630557, g_loss: 0.69242811\n",
      "Step: [22475] d_loss: 1.38627231, g_loss: 0.69372272\n",
      "Step: [22476] d_loss: 1.38643849, g_loss: 0.69375348\n",
      "Step: [22477] d_loss: 1.38666391, g_loss: 0.69232726\n",
      "Step: [22478] d_loss: 1.38670421, g_loss: 0.69485736\n",
      "Step: [22479] d_loss: 1.38690233, g_loss: 0.69387597\n",
      "Step: [22480] d_loss: 1.38689232, g_loss: 0.69491047\n",
      "Step: [22481] d_loss: 1.38679934, g_loss: 0.69234157\n",
      "Step: [22482] d_loss: 1.38664985, g_loss: 0.69141430\n",
      "Step: [22483] d_loss: 1.38649356, g_loss: 0.69218373\n",
      "Step: [22484] d_loss: 1.38638568, g_loss: 0.69403172\n",
      "Step: [22485] d_loss: 1.38635242, g_loss: 0.69360548\n",
      "Step: [22486] d_loss: 1.38632226, g_loss: 0.69290704\n",
      "Step: [22487] d_loss: 1.38629973, g_loss: 0.69269824\n",
      "Step: [22488] d_loss: 1.38628817, g_loss: 0.69280380\n",
      "Step: [22489] d_loss: 1.38636315, g_loss: 0.69311470\n",
      "Step: [22490] d_loss: 1.38677800, g_loss: 0.69127560\n",
      "Step: [22491] d_loss: 1.38680649, g_loss: 0.69319820\n",
      "Step: [22492] d_loss: 1.38734198, g_loss: 0.69267952\n",
      "Step: [22493] d_loss: 1.38654995, g_loss: 0.69121933\n",
      "Step: [22494] d_loss: 1.38639796, g_loss: 0.69313645\n",
      "Step: [22495] d_loss: 1.38633704, g_loss: 0.69434130\n",
      "Step: [22496] d_loss: 1.38628149, g_loss: 0.69412661\n",
      "Step: [22497] d_loss: 1.38630676, g_loss: 0.69295096\n",
      "Step: [22498] d_loss: 1.38648844, g_loss: 0.69316983\n",
      "Step: [22499] d_loss: 1.38649154, g_loss: 0.69204116\n",
      "Step: [22500] d_loss: 1.38672721, g_loss: 0.69330931\n",
      "Step: [22501] d_loss: 1.38660073, g_loss: 0.69577503\n",
      "Step: [22502] d_loss: 1.38667703, g_loss: 0.69695473\n",
      "Step: [22503] d_loss: 1.38669229, g_loss: 0.69441825\n",
      "Step: [22504] d_loss: 1.38657546, g_loss: 0.69027120\n",
      "Step: [22505] d_loss: 1.38653851, g_loss: 0.68977487\n",
      "Step: [22506] d_loss: 1.38642311, g_loss: 0.69106829\n",
      "Step: [22507] d_loss: 1.38628602, g_loss: 0.69237071\n",
      "Step: [22508] d_loss: 1.38628638, g_loss: 0.69341099\n",
      "Step: [22509] d_loss: 1.38629055, g_loss: 0.69409847\n",
      "Step: [22510] d_loss: 1.38640785, g_loss: 0.69520175\n",
      "Step: [22511] d_loss: 1.38632989, g_loss: 0.69320875\n",
      "Step: [22512] d_loss: 1.38638091, g_loss: 0.69367921\n",
      "Step: [22513] d_loss: 1.38638830, g_loss: 0.69260049\n",
      "Step: [22514] d_loss: 1.38639593, g_loss: 0.69330472\n",
      "Step: [22515] d_loss: 1.38640487, g_loss: 0.69437450\n",
      "Step: [22516] d_loss: 1.38640642, g_loss: 0.69446450\n",
      "Step: [22517] d_loss: 1.38640964, g_loss: 0.69416225\n",
      "Step: [22518] d_loss: 1.38642693, g_loss: 0.69390327\n",
      "Step: [22519] d_loss: 1.38638973, g_loss: 0.69249058\n",
      "Step: [22520] d_loss: 1.38634062, g_loss: 0.69271517\n",
      "Step: [22521] d_loss: 1.38633180, g_loss: 0.69296587\n",
      "Step: [22522] d_loss: 1.38630593, g_loss: 0.69285601\n",
      "Step: [22523] d_loss: 1.38627243, g_loss: 0.69310105\n",
      "Step: [22524] d_loss: 1.38631713, g_loss: 0.69267559\n",
      "Step: [22525] d_loss: 1.38634276, g_loss: 0.69355690\n",
      "Step: [22526] d_loss: 1.38635969, g_loss: 0.69366610\n",
      "Step: [22527] d_loss: 1.38635159, g_loss: 0.69292516\n",
      "Step: [22528] d_loss: 1.38632524, g_loss: 0.69275868\n",
      "Step: [22529] d_loss: 1.38631630, g_loss: 0.69332123\n",
      "Step: [22530] d_loss: 1.38634634, g_loss: 0.69513595\n",
      "Step: [22531] d_loss: 1.38640428, g_loss: 0.69606823\n",
      "Step: [22532] d_loss: 1.38631046, g_loss: 0.69446266\n",
      "Step: [22533] d_loss: 1.38642454, g_loss: 0.69260597\n",
      "Step: [22534] d_loss: 1.38634658, g_loss: 0.69187701\n",
      "Step: [22535] d_loss: 1.38636613, g_loss: 0.69291127\n",
      "Step: [22536] d_loss: 1.38639188, g_loss: 0.69454348\n",
      "Step: [22537] d_loss: 1.38641167, g_loss: 0.69504249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22538] d_loss: 1.38643014, g_loss: 0.69222313\n",
      "Step: [22539] d_loss: 1.38640797, g_loss: 0.69221091\n",
      "Step: [22540] d_loss: 1.38639498, g_loss: 0.69328314\n",
      "Step: [22541] d_loss: 1.38637507, g_loss: 0.69377357\n",
      "Step: [22542] d_loss: 1.38635898, g_loss: 0.69399023\n",
      "Step: [22543] d_loss: 1.38635111, g_loss: 0.69331431\n",
      "Step: [22544] d_loss: 1.38633919, g_loss: 0.69302076\n",
      "Step: [22545] d_loss: 1.38633573, g_loss: 0.69325829\n",
      "Step: [22546] d_loss: 1.38632274, g_loss: 0.69280529\n",
      "Step: [22547] d_loss: 1.38630462, g_loss: 0.69251007\n",
      "Step: [22548] d_loss: 1.38629580, g_loss: 0.69294059\n",
      "Step: [22549] d_loss: 1.38629580, g_loss: 0.69336879\n",
      "Step: [22550] d_loss: 1.38629460, g_loss: 0.69333923\n",
      "Step: [22551] d_loss: 1.38629651, g_loss: 0.69321758\n",
      "Step: [22552] d_loss: 1.38629794, g_loss: 0.69305456\n",
      "Step: [22553] d_loss: 1.38630438, g_loss: 0.69283664\n",
      "Step: [22554] d_loss: 1.38630676, g_loss: 0.69321251\n",
      "Step: [22555] d_loss: 1.38629770, g_loss: 0.69318962\n",
      "Step: [22556] d_loss: 1.38629317, g_loss: 0.69305187\n",
      "Step: [22557] d_loss: 1.38629854, g_loss: 0.69326913\n",
      "Step: [22558] d_loss: 1.38631988, g_loss: 0.69293427\n",
      "Step: [22559] d_loss: 1.38631439, g_loss: 0.69271785\n",
      "Step: [22560] d_loss: 1.38630104, g_loss: 0.69336748\n",
      "Step: [22561] d_loss: 1.38662386, g_loss: 0.69445711\n",
      "Step: [22562] d_loss: 1.38718891, g_loss: 0.69560921\n",
      "Step: [22563] d_loss: 1.38743806, g_loss: 0.69271362\n",
      "Step: [22564] d_loss: 1.38729811, g_loss: 0.69026929\n",
      "Step: [22565] d_loss: 1.38701034, g_loss: 0.69213772\n",
      "Step: [22566] d_loss: 1.38672066, g_loss: 0.69359583\n",
      "Step: [22567] d_loss: 1.38650906, g_loss: 0.69342613\n",
      "Step: [22568] d_loss: 1.38637769, g_loss: 0.69291371\n",
      "Step: [22569] d_loss: 1.38631976, g_loss: 0.69301724\n",
      "Step: [22570] d_loss: 1.38629997, g_loss: 0.69312686\n",
      "Step: [22571] d_loss: 1.38629580, g_loss: 0.69324380\n",
      "Step: [22572] d_loss: 1.38629067, g_loss: 0.69322348\n",
      "Step: [22573] d_loss: 1.38629746, g_loss: 0.69307923\n",
      "Step: [22574] d_loss: 1.38666523, g_loss: 0.69398314\n",
      "Step: [22575] d_loss: 1.38632166, g_loss: 0.69333982\n",
      "Step: [22576] d_loss: 1.38636100, g_loss: 0.69413537\n",
      "Step: [22577] d_loss: 1.38643909, g_loss: 0.69681382\n",
      "Step: [22578] d_loss: 1.38645566, g_loss: 0.69431472\n",
      "Step: [22579] d_loss: 1.38642955, g_loss: 0.69203067\n",
      "Step: [22580] d_loss: 1.38639784, g_loss: 0.69259506\n",
      "Step: [22581] d_loss: 1.38637590, g_loss: 0.69350487\n",
      "Step: [22582] d_loss: 1.38635635, g_loss: 0.69356763\n",
      "Step: [22583] d_loss: 1.38633132, g_loss: 0.69277173\n",
      "Step: [22584] d_loss: 1.38630807, g_loss: 0.69285417\n",
      "Step: [22585] d_loss: 1.38628995, g_loss: 0.69306123\n",
      "Step: [22586] d_loss: 1.38629782, g_loss: 0.69326353\n",
      "Step: [22587] d_loss: 1.38629365, g_loss: 0.69329935\n",
      "Step: [22588] d_loss: 1.38629067, g_loss: 0.69319868\n",
      "Step: [22589] d_loss: 1.38628924, g_loss: 0.69305396\n",
      "Step: [22590] d_loss: 1.38629627, g_loss: 0.69311655\n",
      "Step: [22591] d_loss: 1.38629484, g_loss: 0.69317305\n",
      "Step: [22592] d_loss: 1.38628983, g_loss: 0.69344878\n",
      "Step: [22593] d_loss: 1.38632011, g_loss: 0.69312346\n",
      "Step: [22594] d_loss: 1.38632655, g_loss: 0.69320762\n",
      "Step: [22595] d_loss: 1.38645065, g_loss: 0.69392914\n",
      "Step: [22596] d_loss: 1.38655138, g_loss: 0.69558740\n",
      "Step: [22597] d_loss: 1.38661826, g_loss: 0.69587427\n",
      "Step: [22598] d_loss: 1.38661730, g_loss: 0.69401979\n",
      "Step: [22599] d_loss: 1.38657236, g_loss: 0.69316894\n",
      "Step: [22600] d_loss: 1.38650250, g_loss: 0.69158131\n",
      "Step: [22601] d_loss: 1.38643599, g_loss: 0.69283664\n",
      "Step: [22602] d_loss: 1.38639772, g_loss: 0.69400239\n",
      "Step: [22603] d_loss: 1.38638020, g_loss: 0.69448125\n",
      "Step: [22604] d_loss: 1.38636923, g_loss: 0.69444227\n",
      "Step: [22605] d_loss: 1.38643003, g_loss: 0.69406533\n",
      "Step: [22606] d_loss: 1.38636971, g_loss: 0.69430095\n",
      "Step: [22607] d_loss: 1.38634849, g_loss: 0.69340533\n",
      "Step: [22608] d_loss: 1.38635564, g_loss: 0.69331998\n",
      "Step: [22609] d_loss: 1.38635182, g_loss: 0.69328058\n",
      "Step: [22610] d_loss: 1.38633871, g_loss: 0.69354463\n",
      "Step: [22611] d_loss: 1.38633895, g_loss: 0.69292331\n",
      "Step: [22612] d_loss: 1.38631845, g_loss: 0.69291419\n",
      "Step: [22613] d_loss: 1.38630939, g_loss: 0.69298971\n",
      "Step: [22614] d_loss: 1.38631070, g_loss: 0.69249380\n",
      "Step: [22615] d_loss: 1.38630033, g_loss: 0.69327193\n",
      "Step: [22616] d_loss: 1.38634598, g_loss: 0.69368255\n",
      "Step: [22617] d_loss: 1.38629138, g_loss: 0.69274575\n",
      "Step: [22618] d_loss: 1.38629353, g_loss: 0.69298685\n",
      "Step: [22619] d_loss: 1.38618207, g_loss: 0.69253135\n",
      "Step: [22620] d_loss: 1.38626468, g_loss: 0.69299769\n",
      "Step: [22621] d_loss: 1.38629365, g_loss: 0.69339818\n",
      "Step: [22622] d_loss: 1.38629949, g_loss: 0.69341952\n",
      "Step: [22623] d_loss: 1.38629436, g_loss: 0.69336843\n",
      "Step: [22624] d_loss: 1.38629055, g_loss: 0.69319212\n",
      "Step: [22625] d_loss: 1.38627362, g_loss: 0.69285566\n",
      "Step: [22626] d_loss: 1.38632512, g_loss: 0.69288063\n",
      "Step: [22627] d_loss: 1.38629770, g_loss: 0.69322240\n",
      "Step: [22628] d_loss: 1.38629723, g_loss: 0.69309431\n",
      "Step: [22629] d_loss: 1.38669157, g_loss: 0.69333565\n",
      "Step: [22630] d_loss: 1.38629794, g_loss: 0.69351697\n",
      "Step: [22631] d_loss: 1.38629651, g_loss: 0.69301724\n",
      "Step: [22632] d_loss: 1.38630533, g_loss: 0.69323957\n",
      "Step: [22633] d_loss: 1.38630033, g_loss: 0.69321996\n",
      "Step: [22634] d_loss: 1.38630486, g_loss: 0.69330651\n",
      "Step: [22635] d_loss: 1.38629460, g_loss: 0.69285822\n",
      "Step: [22636] d_loss: 1.38625336, g_loss: 0.69229746\n",
      "Step: [22637] d_loss: 1.38629913, g_loss: 0.69287908\n",
      "Step: [22638] d_loss: 1.38629842, g_loss: 0.69299835\n",
      "Step: [22639] d_loss: 1.38628757, g_loss: 0.69335878\n",
      "Step: [22640] d_loss: 1.38629436, g_loss: 0.69345653\n",
      "Step: [22641] d_loss: 1.38629293, g_loss: 0.69291747\n",
      "Step: [22642] d_loss: 1.38629055, g_loss: 0.69340134\n",
      "Step: [22643] d_loss: 1.38629448, g_loss: 0.69273198\n",
      "Step: [22644] d_loss: 1.38639367, g_loss: 0.69374073\n",
      "Step: [22645] d_loss: 1.38631272, g_loss: 0.69336438\n",
      "Step: [22646] d_loss: 1.38632333, g_loss: 0.69297230\n",
      "Step: [22647] d_loss: 1.38631892, g_loss: 0.69259357\n",
      "Step: [22648] d_loss: 1.38631785, g_loss: 0.69307005\n",
      "Step: [22649] d_loss: 1.38631999, g_loss: 0.69372022\n",
      "Step: [22650] d_loss: 1.38630795, g_loss: 0.69345915\n",
      "Step: [22651] d_loss: 1.38631511, g_loss: 0.69295108\n",
      "Step: [22652] d_loss: 1.38631368, g_loss: 0.69244730\n",
      "Step: [22653] d_loss: 1.38630915, g_loss: 0.69241595\n",
      "Step: [22654] d_loss: 1.38636506, g_loss: 0.69373280\n",
      "Step: [22655] d_loss: 1.38667071, g_loss: 0.69684899\n",
      "Step: [22656] d_loss: 1.38708162, g_loss: 0.69655102\n",
      "Step: [22657] d_loss: 1.38713300, g_loss: 0.69680595\n",
      "Step: [22658] d_loss: 1.38721752, g_loss: 0.69515634\n",
      "Step: [22659] d_loss: 1.38699138, g_loss: 0.69500059\n",
      "Step: [22660] d_loss: 1.38685429, g_loss: 0.69604790\n",
      "Step: [22661] d_loss: 1.38666117, g_loss: 0.68922299\n",
      "Step: [22662] d_loss: 1.38657403, g_loss: 0.68790352\n",
      "Step: [22663] d_loss: 1.38651848, g_loss: 0.69216740\n",
      "Step: [22664] d_loss: 1.38644099, g_loss: 0.69644183\n",
      "Step: [22665] d_loss: 1.38639653, g_loss: 0.69524121\n",
      "Step: [22666] d_loss: 1.38635302, g_loss: 0.69275320\n",
      "Step: [22667] d_loss: 1.38634801, g_loss: 0.69284260\n",
      "Step: [22668] d_loss: 1.38632274, g_loss: 0.69292569\n",
      "Step: [22669] d_loss: 1.38632309, g_loss: 0.69309860\n",
      "Step: [22670] d_loss: 1.38630438, g_loss: 0.69374275\n",
      "Step: [22671] d_loss: 1.38629079, g_loss: 0.69245571\n",
      "Step: [22672] d_loss: 1.38630271, g_loss: 0.69246930\n",
      "Step: [22673] d_loss: 1.38629305, g_loss: 0.69312835\n",
      "Step: [22674] d_loss: 1.38629997, g_loss: 0.69336414\n",
      "Step: [22675] d_loss: 1.38603854, g_loss: 0.69154853\n",
      "Step: [22676] d_loss: 1.38632309, g_loss: 0.69331366\n",
      "Step: [22677] d_loss: 1.38632727, g_loss: 0.69390261\n",
      "Step: [22678] d_loss: 1.38610721, g_loss: 0.69348204\n",
      "Step: [22679] d_loss: 1.38631082, g_loss: 0.69379008\n",
      "Step: [22680] d_loss: 1.38629603, g_loss: 0.69229442\n",
      "Step: [22681] d_loss: 1.38622189, g_loss: 0.69266808\n",
      "Step: [22682] d_loss: 1.38628733, g_loss: 0.69352269\n",
      "Step: [22683] d_loss: 1.38629866, g_loss: 0.69300926\n",
      "Step: [22684] d_loss: 1.38624954, g_loss: 0.69278240\n",
      "Step: [22685] d_loss: 1.38629627, g_loss: 0.69332004\n",
      "Step: [22686] d_loss: 1.38630664, g_loss: 0.69172835\n",
      "Step: [22687] d_loss: 1.38630676, g_loss: 0.69300473\n",
      "Step: [22688] d_loss: 1.38630366, g_loss: 0.69358802\n",
      "Step: [22689] d_loss: 1.38630533, g_loss: 0.69353235\n",
      "Step: [22690] d_loss: 1.38680315, g_loss: 0.69295043\n",
      "Step: [22691] d_loss: 1.38628757, g_loss: 0.69368500\n",
      "Step: [22692] d_loss: 1.38627100, g_loss: 0.69289875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22693] d_loss: 1.38630748, g_loss: 0.69270515\n",
      "Step: [22694] d_loss: 1.38631320, g_loss: 0.69189429\n",
      "Step: [22695] d_loss: 1.38631701, g_loss: 0.69310105\n",
      "Step: [22696] d_loss: 1.38631797, g_loss: 0.69393098\n",
      "Step: [22697] d_loss: 1.38630867, g_loss: 0.69343507\n",
      "Step: [22698] d_loss: 1.38630450, g_loss: 0.69383162\n",
      "Step: [22699] d_loss: 1.38627434, g_loss: 0.69237643\n",
      "Step: [22700] d_loss: 1.38628781, g_loss: 0.69327164\n",
      "Step: [22701] d_loss: 1.38629496, g_loss: 0.69322389\n",
      "Step: [22702] d_loss: 1.38628626, g_loss: 0.69266534\n",
      "Step: [22703] d_loss: 1.38625264, g_loss: 0.69206762\n",
      "Step: [22704] d_loss: 1.38629103, g_loss: 0.69354272\n",
      "Step: [22705] d_loss: 1.38628793, g_loss: 0.69359016\n",
      "Step: [22706] d_loss: 1.38628805, g_loss: 0.69216812\n",
      "Step: [22707] d_loss: 1.38628650, g_loss: 0.69328862\n",
      "Step: [22708] d_loss: 1.38630271, g_loss: 0.69336009\n",
      "Step: [22709] d_loss: 1.38629043, g_loss: 0.69230199\n",
      "Step: [22710] d_loss: 1.38628817, g_loss: 0.69319570\n",
      "Step: [22711] d_loss: 1.38624108, g_loss: 0.69291604\n",
      "Step: [22712] d_loss: 1.38630044, g_loss: 0.69371051\n",
      "Step: [22713] d_loss: 1.38627815, g_loss: 0.69302052\n",
      "Step: [22714] d_loss: 1.38630378, g_loss: 0.69159794\n",
      "Step: [22715] d_loss: 1.38631773, g_loss: 0.69207090\n",
      "Step: [22716] d_loss: 1.38632905, g_loss: 0.69350219\n",
      "Step: [22717] d_loss: 1.38618577, g_loss: 0.69345158\n",
      "Step: [22718] d_loss: 1.38633633, g_loss: 0.69310820\n",
      "Step: [22719] d_loss: 1.38635063, g_loss: 0.69238544\n",
      "Step: [22720] d_loss: 1.38632715, g_loss: 0.69298708\n",
      "Step: [22721] d_loss: 1.38636184, g_loss: 0.69374043\n",
      "Step: [22722] d_loss: 1.38636935, g_loss: 0.69323349\n",
      "Step: [22723] d_loss: 1.38698876, g_loss: 0.69107419\n",
      "Step: [22724] d_loss: 1.38636339, g_loss: 0.69205153\n",
      "Step: [22725] d_loss: 1.38635731, g_loss: 0.69331872\n",
      "Step: [22726] d_loss: 1.38635612, g_loss: 0.69388729\n",
      "Step: [22727] d_loss: 1.38635373, g_loss: 0.69323838\n",
      "Step: [22728] d_loss: 1.38635373, g_loss: 0.69297427\n",
      "Step: [22729] d_loss: 1.38635588, g_loss: 0.69222677\n",
      "Step: [22730] d_loss: 1.38636279, g_loss: 0.69327044\n",
      "Step: [22731] d_loss: 1.38635540, g_loss: 0.69358039\n",
      "Step: [22732] d_loss: 1.38627243, g_loss: 0.69333446\n",
      "Step: [22733] d_loss: 1.38688624, g_loss: 0.69448501\n",
      "Step: [22734] d_loss: 1.38794947, g_loss: 0.69344485\n",
      "Step: [22735] d_loss: 1.38852119, g_loss: 0.68565023\n",
      "Step: [22736] d_loss: 1.38859522, g_loss: 0.68948984\n",
      "Step: [22737] d_loss: 1.38816547, g_loss: 0.69482279\n",
      "Step: [22738] d_loss: 1.38757241, g_loss: 0.69650543\n",
      "Step: [22739] d_loss: 1.38705647, g_loss: 0.69665456\n",
      "Step: [22740] d_loss: 1.38668132, g_loss: 0.69295710\n",
      "Step: [22741] d_loss: 1.38642454, g_loss: 0.69267732\n",
      "Step: [22742] d_loss: 1.38637888, g_loss: 0.69276673\n",
      "Step: [22743] d_loss: 1.38632512, g_loss: 0.69317710\n",
      "Step: [22744] d_loss: 1.38630354, g_loss: 0.69321167\n",
      "Step: [22745] d_loss: 1.38627875, g_loss: 0.69338828\n",
      "Step: [22746] d_loss: 1.38627684, g_loss: 0.69326460\n",
      "Step: [22747] d_loss: 1.38627315, g_loss: 0.69316149\n",
      "Step: [22748] d_loss: 1.38628411, g_loss: 0.69309735\n",
      "Step: [22749] d_loss: 1.38669252, g_loss: 0.69255954\n",
      "Step: [22750] d_loss: 1.38637257, g_loss: 0.69314671\n",
      "Step: [22751] d_loss: 1.38629413, g_loss: 0.69300294\n",
      "Step: [22752] d_loss: 1.38628900, g_loss: 0.69309169\n",
      "Step: [22753] d_loss: 1.38627672, g_loss: 0.69317377\n",
      "Step: [22754] d_loss: 1.38627267, g_loss: 0.69325525\n",
      "Step: [22755] d_loss: 1.38626361, g_loss: 0.69329262\n",
      "Step: [22756] d_loss: 1.38624036, g_loss: 0.69366229\n",
      "Step: [22757] d_loss: 1.38636184, g_loss: 0.69213051\n",
      "Step: [22758] d_loss: 1.38645983, g_loss: 0.69194555\n",
      "Step: [22759] d_loss: 1.38642991, g_loss: 0.69252700\n",
      "Step: [22760] d_loss: 1.38638079, g_loss: 0.69364071\n",
      "Step: [22761] d_loss: 1.38634253, g_loss: 0.69408268\n",
      "Step: [22762] d_loss: 1.38631713, g_loss: 0.69414896\n",
      "Step: [22763] d_loss: 1.38631785, g_loss: 0.69387865\n",
      "Step: [22764] d_loss: 1.38630772, g_loss: 0.69311190\n",
      "Step: [22765] d_loss: 1.38630033, g_loss: 0.69310701\n",
      "Step: [22766] d_loss: 1.38629842, g_loss: 0.69326353\n",
      "Step: [22767] d_loss: 1.38630056, g_loss: 0.69376212\n",
      "Step: [22768] d_loss: 1.38630199, g_loss: 0.69342995\n",
      "Step: [22769] d_loss: 1.38683915, g_loss: 0.69206583\n",
      "Step: [22770] d_loss: 1.38629079, g_loss: 0.69257736\n",
      "Step: [22771] d_loss: 1.38629460, g_loss: 0.69290715\n",
      "Step: [22772] d_loss: 1.38629389, g_loss: 0.69347250\n",
      "Step: [22773] d_loss: 1.38629997, g_loss: 0.69335324\n",
      "Step: [22774] d_loss: 1.38630080, g_loss: 0.69304889\n",
      "Step: [22775] d_loss: 1.38622046, g_loss: 0.69354868\n",
      "Step: [22776] d_loss: 1.38629150, g_loss: 0.69331539\n",
      "Step: [22777] d_loss: 1.38629651, g_loss: 0.69330609\n",
      "Step: [22778] d_loss: 1.38629103, g_loss: 0.69315052\n",
      "Step: [22779] d_loss: 1.38629878, g_loss: 0.69299001\n",
      "Step: [22780] d_loss: 1.38629365, g_loss: 0.69286299\n",
      "Step: [22781] d_loss: 1.38629746, g_loss: 0.69310081\n",
      "Step: [22782] d_loss: 1.38629270, g_loss: 0.69324601\n",
      "Step: [22783] d_loss: 1.38629448, g_loss: 0.69325715\n",
      "Step: [22784] d_loss: 1.38629293, g_loss: 0.69313425\n",
      "Step: [22785] d_loss: 1.38629305, g_loss: 0.69306350\n",
      "Step: [22786] d_loss: 1.38629127, g_loss: 0.69315386\n",
      "Step: [22787] d_loss: 1.38629115, g_loss: 0.69321692\n",
      "Step: [22788] d_loss: 1.38693500, g_loss: 0.69176686\n",
      "Step: [22789] d_loss: 1.38629758, g_loss: 0.69300151\n",
      "Step: [22790] d_loss: 1.38629770, g_loss: 0.69304848\n",
      "Step: [22791] d_loss: 1.38629627, g_loss: 0.69300461\n",
      "Step: [22792] d_loss: 1.38629675, g_loss: 0.69323689\n",
      "Step: [22793] d_loss: 1.38630307, g_loss: 0.69326091\n",
      "Step: [22794] d_loss: 1.38629317, g_loss: 0.69323170\n",
      "Step: [22795] d_loss: 1.38629842, g_loss: 0.69308156\n",
      "Step: [22796] d_loss: 1.38678575, g_loss: 0.69254267\n",
      "Step: [22797] d_loss: 1.38629150, g_loss: 0.69317770\n",
      "Step: [22798] d_loss: 1.38629913, g_loss: 0.69323146\n",
      "Step: [22799] d_loss: 1.38629842, g_loss: 0.69342905\n",
      "Step: [22800] d_loss: 1.38630748, g_loss: 0.69300199\n",
      "Step: [22801] d_loss: 1.38630390, g_loss: 0.69263172\n",
      "Step: [22802] d_loss: 1.38630033, g_loss: 0.69285601\n",
      "Step: [22803] d_loss: 1.38629723, g_loss: 0.69303071\n",
      "Step: [22804] d_loss: 1.38653958, g_loss: 0.69297779\n",
      "Step: [22805] d_loss: 1.38624644, g_loss: 0.69332731\n",
      "Step: [22806] d_loss: 1.38629580, g_loss: 0.69331497\n",
      "Step: [22807] d_loss: 1.38626921, g_loss: 0.69292182\n",
      "Step: [22808] d_loss: 1.38626313, g_loss: 0.69305670\n",
      "Step: [22809] d_loss: 1.38629961, g_loss: 0.69311088\n",
      "Step: [22810] d_loss: 1.38629699, g_loss: 0.69312710\n",
      "Step: [22811] d_loss: 1.38621616, g_loss: 0.69389355\n",
      "Step: [22812] d_loss: 1.38630033, g_loss: 0.69339335\n",
      "Step: [22813] d_loss: 1.38627136, g_loss: 0.69348919\n",
      "Step: [22814] d_loss: 1.38631463, g_loss: 0.69313085\n",
      "Step: [22815] d_loss: 1.38631344, g_loss: 0.69330478\n",
      "Step: [22816] d_loss: 1.38632452, g_loss: 0.69301349\n",
      "Step: [22817] d_loss: 1.38632011, g_loss: 0.69335747\n",
      "Step: [22818] d_loss: 1.38632894, g_loss: 0.69313723\n",
      "Step: [22819] d_loss: 1.38632166, g_loss: 0.69299340\n",
      "Step: [22820] d_loss: 1.38631535, g_loss: 0.69357598\n",
      "Step: [22821] d_loss: 1.38631737, g_loss: 0.69300652\n",
      "Step: [22822] d_loss: 1.38626122, g_loss: 0.69229859\n",
      "Step: [22823] d_loss: 1.38632286, g_loss: 0.69299185\n",
      "Step: [22824] d_loss: 1.38630903, g_loss: 0.69360602\n",
      "Step: [22825] d_loss: 1.38630867, g_loss: 0.69311118\n",
      "Step: [22826] d_loss: 1.38627648, g_loss: 0.69257355\n",
      "Step: [22827] d_loss: 1.38628626, g_loss: 0.69266093\n",
      "Step: [22828] d_loss: 1.38633418, g_loss: 0.69403374\n",
      "Step: [22829] d_loss: 1.38643599, g_loss: 0.69469666\n",
      "Step: [22830] d_loss: 1.38674998, g_loss: 0.69627762\n",
      "Step: [22831] d_loss: 1.38695312, g_loss: 0.69670868\n",
      "Step: [22832] d_loss: 1.38705730, g_loss: 0.69404268\n",
      "Step: [22833] d_loss: 1.38704431, g_loss: 0.69453615\n",
      "Step: [22834] d_loss: 1.38699317, g_loss: 0.69255292\n",
      "Step: [22835] d_loss: 1.38692689, g_loss: 0.69300258\n",
      "Step: [22836] d_loss: 1.38682103, g_loss: 0.69619489\n",
      "Step: [22837] d_loss: 1.38662195, g_loss: 0.69667172\n",
      "Step: [22838] d_loss: 1.38658953, g_loss: 0.69531393\n",
      "Step: [22839] d_loss: 1.38647652, g_loss: 0.69360602\n",
      "Step: [22840] d_loss: 1.38638806, g_loss: 0.69204772\n",
      "Step: [22841] d_loss: 1.38637471, g_loss: 0.69270706\n",
      "Step: [22842] d_loss: 1.38631189, g_loss: 0.69129264\n",
      "Step: [22843] d_loss: 1.38656640, g_loss: 0.69469595\n",
      "Step: [22844] d_loss: 1.38681698, g_loss: 0.69704717\n",
      "Step: [22845] d_loss: 1.38708687, g_loss: 0.68898535\n",
      "Step: [22846] d_loss: 1.38718688, g_loss: 0.68884766\n",
      "Step: [22847] d_loss: 1.38709950, g_loss: 0.69387704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [22848] d_loss: 1.38692403, g_loss: 0.69529653\n",
      "Step: [22849] d_loss: 1.38673353, g_loss: 0.69560182\n",
      "Step: [22850] d_loss: 1.38656604, g_loss: 0.69359237\n",
      "Step: [22851] d_loss: 1.38643885, g_loss: 0.69281983\n",
      "Step: [22852] d_loss: 1.38634849, g_loss: 0.69157499\n",
      "Step: [22853] d_loss: 1.38637984, g_loss: 0.69298685\n",
      "Step: [22854] d_loss: 1.38632035, g_loss: 0.69348472\n",
      "Step: [22855] d_loss: 1.38608134, g_loss: 0.69294059\n",
      "Step: [22856] d_loss: 1.38611603, g_loss: 0.69354749\n",
      "Step: [22857] d_loss: 1.38632679, g_loss: 0.69366425\n",
      "Step: [22858] d_loss: 1.38631916, g_loss: 0.69193763\n",
      "Step: [22859] d_loss: 1.38632393, g_loss: 0.69251156\n",
      "Step: [22860] d_loss: 1.38631535, g_loss: 0.69266438\n",
      "Step: [22861] d_loss: 1.38631558, g_loss: 0.69339186\n",
      "Step: [22862] d_loss: 1.38607824, g_loss: 0.69267440\n",
      "Step: [22863] d_loss: 1.38629985, g_loss: 0.69316345\n",
      "Step: [22864] d_loss: 1.38633084, g_loss: 0.69376570\n",
      "Step: [22865] d_loss: 1.38630176, g_loss: 0.69339901\n",
      "Step: [22866] d_loss: 1.38630772, g_loss: 0.69294190\n",
      "Step: [22867] d_loss: 1.38628125, g_loss: 0.69280452\n",
      "Step: [22868] d_loss: 1.38630331, g_loss: 0.69330585\n",
      "Step: [22869] d_loss: 1.38629937, g_loss: 0.69288230\n",
      "Step: [22870] d_loss: 1.38629925, g_loss: 0.69299793\n",
      "Step: [22871] d_loss: 1.38629019, g_loss: 0.69298553\n",
      "Step: [22872] d_loss: 1.38619590, g_loss: 0.69301343\n",
      "Step: [22873] d_loss: 1.38629317, g_loss: 0.69368058\n",
      "Step: [22874] d_loss: 1.38629508, g_loss: 0.69267422\n",
      "Step: [22875] d_loss: 1.38629293, g_loss: 0.69300628\n",
      "Step: [22876] d_loss: 1.38630569, g_loss: 0.69275558\n",
      "Step: [22877] d_loss: 1.38628709, g_loss: 0.69286990\n",
      "Step: [22878] d_loss: 1.38630319, g_loss: 0.69353426\n",
      "Step: [22879] d_loss: 1.38629961, g_loss: 0.69341016\n",
      "Step: [22880] d_loss: 1.38631511, g_loss: 0.69270909\n",
      "Step: [22881] d_loss: 1.38630533, g_loss: 0.69266725\n",
      "Step: [22882] d_loss: 1.38629603, g_loss: 0.69266009\n",
      "Step: [22883] d_loss: 1.38630414, g_loss: 0.69324011\n",
      "Step: [22884] d_loss: 1.38629985, g_loss: 0.69240081\n",
      "Step: [22885] d_loss: 1.38630474, g_loss: 0.69346768\n",
      "Step: [22886] d_loss: 1.38631940, g_loss: 0.69349498\n",
      "Step: [22887] d_loss: 1.38628542, g_loss: 0.69308984\n",
      "Step: [22888] d_loss: 1.38629866, g_loss: 0.69342852\n",
      "Step: [22889] d_loss: 1.38628316, g_loss: 0.69260252\n",
      "Step: [22890] d_loss: 1.38628864, g_loss: 0.69296789\n",
      "Step: [22891] d_loss: 1.38629532, g_loss: 0.69323134\n",
      "Step: [22892] d_loss: 1.38586044, g_loss: 0.69270092\n",
      "Step: [22893] d_loss: 1.38631368, g_loss: 0.69358522\n",
      "Step: [22894] d_loss: 1.38629603, g_loss: 0.69302964\n",
      "Step: [22895] d_loss: 1.38629651, g_loss: 0.69325298\n",
      "Step: [22896] d_loss: 1.38630152, g_loss: 0.69303751\n",
      "Step: [22897] d_loss: 1.38628459, g_loss: 0.69301170\n",
      "Step: [22898] d_loss: 1.38629675, g_loss: 0.69326514\n",
      "Step: [22899] d_loss: 1.38630009, g_loss: 0.69309282\n",
      "Step: [22900] d_loss: 1.38578093, g_loss: 0.69338703\n",
      "Step: [22901] d_loss: 1.38629317, g_loss: 0.69326770\n",
      "Step: [22902] d_loss: 1.38645971, g_loss: 0.69219226\n",
      "Step: [22903] d_loss: 1.38630891, g_loss: 0.69291490\n",
      "Step: [22904] d_loss: 1.38629985, g_loss: 0.69372594\n",
      "Step: [22905] d_loss: 1.38628435, g_loss: 0.69348371\n",
      "Step: [22906] d_loss: 1.38631725, g_loss: 0.69290537\n",
      "Step: [22907] d_loss: 1.38630426, g_loss: 0.69311863\n",
      "Step: [22908] d_loss: 1.38642609, g_loss: 0.69223702\n",
      "Step: [22909] d_loss: 1.38630295, g_loss: 0.69233698\n",
      "Step: [22910] d_loss: 1.38627076, g_loss: 0.69174528\n",
      "Step: [22911] d_loss: 1.38631654, g_loss: 0.69286931\n",
      "Step: [22912] d_loss: 1.38630080, g_loss: 0.69314849\n",
      "Step: [22913] d_loss: 1.38628542, g_loss: 0.69343328\n",
      "Step: [22914] d_loss: 1.38632727, g_loss: 0.69336033\n",
      "Step: [22915] d_loss: 1.38629341, g_loss: 0.69365644\n",
      "Step: [22916] d_loss: 1.38629949, g_loss: 0.69289607\n",
      "Step: [22917] d_loss: 1.38629818, g_loss: 0.69291639\n",
      "Step: [22918] d_loss: 1.38629723, g_loss: 0.69306207\n",
      "Step: [22919] d_loss: 1.38629723, g_loss: 0.69329804\n",
      "Step: [22920] d_loss: 1.38630462, g_loss: 0.69335580\n",
      "Step: [22921] d_loss: 1.38629723, g_loss: 0.69333398\n",
      "Step: [22922] d_loss: 1.38628459, g_loss: 0.69340509\n",
      "Step: [22923] d_loss: 1.38629842, g_loss: 0.69298458\n",
      "Step: [22924] d_loss: 1.38630199, g_loss: 0.69304323\n",
      "Step: [22925] d_loss: 1.38629079, g_loss: 0.69315797\n",
      "Step: [22926] d_loss: 1.38627768, g_loss: 0.69275957\n",
      "Step: [22927] d_loss: 1.38630426, g_loss: 0.69346166\n",
      "Step: [22928] d_loss: 1.38630235, g_loss: 0.69337368\n",
      "Step: [22929] d_loss: 1.38629377, g_loss: 0.69280648\n",
      "Step: [22930] d_loss: 1.38628924, g_loss: 0.69242477\n",
      "Step: [22931] d_loss: 1.38628078, g_loss: 0.69338769\n",
      "Step: [22932] d_loss: 1.38627398, g_loss: 0.69329160\n",
      "Step: [22933] d_loss: 1.38629913, g_loss: 0.69316804\n",
      "Step: [22934] d_loss: 1.38660717, g_loss: 0.69305599\n",
      "Step: [22935] d_loss: 1.38629436, g_loss: 0.69315946\n",
      "Step: [22936] d_loss: 1.38629019, g_loss: 0.69316483\n",
      "Step: [22937] d_loss: 1.38629305, g_loss: 0.69289917\n",
      "Step: [22938] d_loss: 1.38629436, g_loss: 0.69305325\n",
      "Step: [22939] d_loss: 1.38694680, g_loss: 0.69319642\n",
      "Step: [22940] d_loss: 1.38630509, g_loss: 0.69327599\n",
      "Step: [22941] d_loss: 1.38628244, g_loss: 0.69290137\n",
      "Step: [22942] d_loss: 1.38628912, g_loss: 0.69306695\n",
      "Step: [22943] d_loss: 1.38628829, g_loss: 0.69314784\n",
      "Step: [22944] d_loss: 1.38632393, g_loss: 0.69312841\n",
      "Step: [22945] d_loss: 1.38708389, g_loss: 0.69484639\n",
      "Step: [22946] d_loss: 1.38628650, g_loss: 0.69284779\n",
      "Step: [22947] d_loss: 1.38628423, g_loss: 0.69270611\n",
      "Step: [22948] d_loss: 1.38628650, g_loss: 0.69305652\n",
      "Step: [22949] d_loss: 1.38628685, g_loss: 0.69335699\n",
      "Step: [22950] d_loss: 1.38629019, g_loss: 0.69312668\n",
      "Step: [22951] d_loss: 1.38628936, g_loss: 0.69330990\n",
      "Step: [22952] d_loss: 1.38689709, g_loss: 0.69445550\n",
      "Step: [22953] d_loss: 1.38629782, g_loss: 0.69335353\n",
      "Step: [22954] d_loss: 1.38641536, g_loss: 0.69334346\n",
      "Step: [22955] d_loss: 1.38630176, g_loss: 0.69339097\n",
      "Step: [22956] d_loss: 1.38629568, g_loss: 0.69323456\n",
      "Step: [22957] d_loss: 1.38629961, g_loss: 0.69319260\n",
      "Step: [22958] d_loss: 1.38630664, g_loss: 0.69287896\n",
      "Step: [22959] d_loss: 1.38630462, g_loss: 0.69255567\n",
      "Step: [22960] d_loss: 1.38630688, g_loss: 0.69293129\n",
      "Step: [22961] d_loss: 1.38630974, g_loss: 0.69284403\n",
      "Step: [22962] d_loss: 1.38631201, g_loss: 0.69334841\n",
      "Step: [22963] d_loss: 1.38631570, g_loss: 0.69305074\n",
      "Step: [22964] d_loss: 1.38631868, g_loss: 0.69348860\n",
      "Step: [22965] d_loss: 1.38633227, g_loss: 0.69373024\n",
      "Step: [22966] d_loss: 1.38633275, g_loss: 0.69359750\n",
      "Step: [22967] d_loss: 1.38633823, g_loss: 0.69269860\n",
      "Step: [22968] d_loss: 1.38634920, g_loss: 0.69319856\n",
      "Step: [22969] d_loss: 1.38636100, g_loss: 0.69404519\n",
      "Step: [22970] d_loss: 1.38636696, g_loss: 0.69414771\n",
      "Step: [22971] d_loss: 1.38637400, g_loss: 0.69299090\n",
      "Step: [22972] d_loss: 1.38630188, g_loss: 0.69272929\n",
      "Step: [22973] d_loss: 1.38634610, g_loss: 0.69335604\n",
      "Step: [22974] d_loss: 1.38643312, g_loss: 0.69270933\n",
      "Step: [22975] d_loss: 1.38652849, g_loss: 0.69421947\n",
      "Step: [22976] d_loss: 1.38657212, g_loss: 0.69523156\n",
      "Step: [22977] d_loss: 1.38661242, g_loss: 0.69408494\n",
      "Step: [22978] d_loss: 1.38662958, g_loss: 0.69427377\n",
      "Step: [22979] d_loss: 1.38663077, g_loss: 0.69446135\n",
      "Step: [22980] d_loss: 1.38664913, g_loss: 0.69285429\n",
      "Step: [22981] d_loss: 1.38668239, g_loss: 0.69364542\n",
      "Step: [22982] d_loss: 1.38670301, g_loss: 0.69542038\n",
      "Step: [22983] d_loss: 1.38668931, g_loss: 0.69524992\n",
      "Step: [22984] d_loss: 1.38668966, g_loss: 0.69252622\n",
      "Step: [22985] d_loss: 1.38672280, g_loss: 0.69268203\n",
      "Step: [22986] d_loss: 1.38676357, g_loss: 0.69318259\n",
      "Step: [22987] d_loss: 1.38674641, g_loss: 0.69509494\n",
      "Step: [22988] d_loss: 1.38678062, g_loss: 0.69301188\n",
      "Step: [22989] d_loss: 1.38686109, g_loss: 0.69501936\n",
      "Step: [22990] d_loss: 1.38682222, g_loss: 0.69464165\n",
      "Step: [22991] d_loss: 1.38675642, g_loss: 0.69382292\n",
      "Step: [22992] d_loss: 1.38671231, g_loss: 0.69144082\n",
      "Step: [22993] d_loss: 1.38668442, g_loss: 0.69331247\n",
      "Step: [22994] d_loss: 1.38666952, g_loss: 0.69198298\n",
      "Step: [22995] d_loss: 1.38669348, g_loss: 0.69103813\n",
      "Step: [22996] d_loss: 1.38671851, g_loss: 0.69278508\n",
      "Step: [22997] d_loss: 1.38668847, g_loss: 0.69269580\n",
      "Step: [22998] d_loss: 1.38663554, g_loss: 0.69469583\n",
      "Step: [22999] d_loss: 1.38655591, g_loss: 0.69517618\n",
      "Step: [23000] d_loss: 1.38648796, g_loss: 0.69444764\n",
      "Step: [23001] d_loss: 1.38640821, g_loss: 0.69332302\n",
      "Step: [23002] d_loss: 1.38635075, g_loss: 0.69365925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23003] d_loss: 1.38630259, g_loss: 0.69243658\n",
      "Step: [23004] d_loss: 1.38629317, g_loss: 0.69367635\n",
      "Step: [23005] d_loss: 1.38629460, g_loss: 0.69323844\n",
      "Step: [23006] d_loss: 1.38632560, g_loss: 0.69167644\n",
      "Step: [23007] d_loss: 1.38669670, g_loss: 0.69234550\n",
      "Step: [23008] d_loss: 1.38629794, g_loss: 0.69401884\n",
      "Step: [23009] d_loss: 1.38630736, g_loss: 0.69213116\n",
      "Step: [23010] d_loss: 1.38631165, g_loss: 0.69311130\n",
      "Step: [23011] d_loss: 1.38632727, g_loss: 0.69302374\n",
      "Step: [23012] d_loss: 1.38633347, g_loss: 0.69325638\n",
      "Step: [23013] d_loss: 1.38632846, g_loss: 0.69295770\n",
      "Step: [23014] d_loss: 1.38632917, g_loss: 0.69345915\n",
      "Step: [23015] d_loss: 1.38632166, g_loss: 0.69316065\n",
      "Step: [23016] d_loss: 1.38632751, g_loss: 0.69275653\n",
      "Step: [23017] d_loss: 1.38633919, g_loss: 0.69200408\n",
      "Step: [23018] d_loss: 1.38634253, g_loss: 0.69316649\n",
      "Step: [23019] d_loss: 1.38633358, g_loss: 0.69468784\n",
      "Step: [23020] d_loss: 1.38630366, g_loss: 0.69321650\n",
      "Step: [23021] d_loss: 1.38630080, g_loss: 0.69276047\n",
      "Step: [23022] d_loss: 1.38629556, g_loss: 0.69287926\n",
      "Step: [23023] d_loss: 1.38628006, g_loss: 0.69241810\n",
      "Step: [23024] d_loss: 1.38630390, g_loss: 0.69353020\n",
      "Step: [23025] d_loss: 1.38631177, g_loss: 0.69248384\n",
      "Step: [23026] d_loss: 1.38634419, g_loss: 0.69127935\n",
      "Step: [23027] d_loss: 1.38638914, g_loss: 0.69205302\n",
      "Step: [23028] d_loss: 1.38641405, g_loss: 0.69378144\n",
      "Step: [23029] d_loss: 1.38642430, g_loss: 0.69353282\n",
      "Step: [23030] d_loss: 1.38642991, g_loss: 0.69399548\n",
      "Step: [23031] d_loss: 1.38640666, g_loss: 0.69426703\n",
      "Step: [23032] d_loss: 1.38635921, g_loss: 0.69323331\n",
      "Step: [23033] d_loss: 1.38633180, g_loss: 0.69359696\n",
      "Step: [23034] d_loss: 1.38630235, g_loss: 0.69308412\n",
      "Step: [23035] d_loss: 1.38631427, g_loss: 0.69242120\n",
      "Step: [23036] d_loss: 1.38629603, g_loss: 0.69344175\n",
      "Step: [23037] d_loss: 1.38629580, g_loss: 0.69349533\n",
      "Step: [23038] d_loss: 1.38629150, g_loss: 0.69189298\n",
      "Step: [23039] d_loss: 1.38628793, g_loss: 0.69316089\n",
      "Step: [23040] d_loss: 1.38630819, g_loss: 0.69387066\n",
      "Step: [23041] d_loss: 1.38628709, g_loss: 0.69284493\n",
      "Step: [23042] d_loss: 1.38628697, g_loss: 0.69357407\n",
      "Step: [23043] d_loss: 1.38628888, g_loss: 0.69290209\n",
      "Step: [23044] d_loss: 1.38628244, g_loss: 0.69232655\n",
      "Step: [23045] d_loss: 1.38627768, g_loss: 0.69302177\n",
      "Step: [23046] d_loss: 1.38627648, g_loss: 0.69399685\n",
      "Step: [23047] d_loss: 1.38627338, g_loss: 0.69346511\n",
      "Step: [23048] d_loss: 1.38626802, g_loss: 0.69249153\n",
      "Step: [23049] d_loss: 1.38627005, g_loss: 0.69289368\n",
      "Step: [23050] d_loss: 1.38628268, g_loss: 0.69205606\n",
      "Step: [23051] d_loss: 1.38632488, g_loss: 0.69445282\n",
      "Step: [23052] d_loss: 1.38629055, g_loss: 0.69347394\n",
      "Step: [23053] d_loss: 1.38629651, g_loss: 0.69281757\n",
      "Step: [23054] d_loss: 1.38629222, g_loss: 0.69271499\n",
      "Step: [23055] d_loss: 1.38619065, g_loss: 0.69255573\n",
      "Step: [23056] d_loss: 1.38628769, g_loss: 0.69293791\n",
      "Step: [23057] d_loss: 1.38629234, g_loss: 0.69323492\n",
      "Step: [23058] d_loss: 1.38620424, g_loss: 0.69389910\n",
      "Step: [23059] d_loss: 1.38629317, g_loss: 0.69332480\n",
      "Step: [23060] d_loss: 1.38628936, g_loss: 0.69255459\n",
      "Step: [23061] d_loss: 1.38629651, g_loss: 0.69262469\n",
      "Step: [23062] d_loss: 1.38628995, g_loss: 0.69332683\n",
      "Step: [23063] d_loss: 1.38629591, g_loss: 0.69289416\n",
      "Step: [23064] d_loss: 1.38629842, g_loss: 0.69340289\n",
      "Step: [23065] d_loss: 1.38629889, g_loss: 0.69358778\n",
      "Step: [23066] d_loss: 1.38629842, g_loss: 0.69309723\n",
      "Step: [23067] d_loss: 1.38629413, g_loss: 0.69290352\n",
      "Step: [23068] d_loss: 1.38629782, g_loss: 0.69328755\n",
      "Step: [23069] d_loss: 1.38629830, g_loss: 0.69321203\n",
      "Step: [23070] d_loss: 1.38629127, g_loss: 0.69290084\n",
      "Step: [23071] d_loss: 1.38629138, g_loss: 0.69318932\n",
      "Step: [23072] d_loss: 1.38629997, g_loss: 0.69271481\n",
      "Step: [23073] d_loss: 1.38630760, g_loss: 0.69317174\n",
      "Step: [23074] d_loss: 1.38631105, g_loss: 0.69381404\n",
      "Step: [23075] d_loss: 1.38630605, g_loss: 0.69282651\n",
      "Step: [23076] d_loss: 1.38631368, g_loss: 0.69267207\n",
      "Step: [23077] d_loss: 1.38631392, g_loss: 0.69331211\n",
      "Step: [23078] d_loss: 1.38612306, g_loss: 0.69332504\n",
      "Step: [23079] d_loss: 1.38634586, g_loss: 0.69279617\n",
      "Step: [23080] d_loss: 1.38637793, g_loss: 0.69159180\n",
      "Step: [23081] d_loss: 1.38642001, g_loss: 0.69295716\n",
      "Step: [23082] d_loss: 1.38645172, g_loss: 0.69195962\n",
      "Step: [23083] d_loss: 1.38651514, g_loss: 0.69330627\n",
      "Step: [23084] d_loss: 1.38656080, g_loss: 0.69384146\n",
      "Step: [23085] d_loss: 1.38659620, g_loss: 0.69460857\n",
      "Step: [23086] d_loss: 1.38664484, g_loss: 0.69214678\n",
      "Step: [23087] d_loss: 1.38687837, g_loss: 0.69518715\n",
      "Step: [23088] d_loss: 1.38674426, g_loss: 0.69832170\n",
      "Step: [23089] d_loss: 1.38666236, g_loss: 0.69628620\n",
      "Step: [23090] d_loss: 1.38660264, g_loss: 0.69335288\n",
      "Step: [23091] d_loss: 1.38658404, g_loss: 0.69136691\n",
      "Step: [23092] d_loss: 1.38657928, g_loss: 0.69507933\n",
      "Step: [23093] d_loss: 1.38653100, g_loss: 0.69545710\n",
      "Step: [23094] d_loss: 1.38691080, g_loss: 0.69196826\n",
      "Step: [23095] d_loss: 1.38643956, g_loss: 0.69462919\n",
      "Step: [23096] d_loss: 1.38664865, g_loss: 0.69481844\n",
      "Step: [23097] d_loss: 1.38666821, g_loss: 0.68987101\n",
      "Step: [23098] d_loss: 1.38675416, g_loss: 0.68734562\n",
      "Step: [23099] d_loss: 1.38687181, g_loss: 0.69217825\n",
      "Step: [23100] d_loss: 1.38690710, g_loss: 0.69350302\n",
      "Step: [23101] d_loss: 1.38692856, g_loss: 0.69562101\n",
      "Step: [23102] d_loss: 1.38661432, g_loss: 0.69439781\n",
      "Step: [23103] d_loss: 1.38695276, g_loss: 0.69337016\n",
      "Step: [23104] d_loss: 1.38711214, g_loss: 0.69253814\n",
      "Step: [23105] d_loss: 1.38688445, g_loss: 0.68820697\n",
      "Step: [23106] d_loss: 1.38679588, g_loss: 0.69126451\n",
      "Step: [23107] d_loss: 1.38667107, g_loss: 0.69431174\n",
      "Step: [23108] d_loss: 1.38639820, g_loss: 0.69338965\n",
      "Step: [23109] d_loss: 1.38650370, g_loss: 0.69095528\n",
      "Step: [23110] d_loss: 1.38644850, g_loss: 0.69298160\n",
      "Step: [23111] d_loss: 1.38640881, g_loss: 0.69332105\n",
      "Step: [23112] d_loss: 1.38636863, g_loss: 0.69311059\n",
      "Step: [23113] d_loss: 1.38635719, g_loss: 0.69365561\n",
      "Step: [23114] d_loss: 1.38633847, g_loss: 0.69380367\n",
      "Step: [23115] d_loss: 1.38633847, g_loss: 0.69375944\n",
      "Step: [23116] d_loss: 1.38633466, g_loss: 0.69316739\n",
      "Step: [23117] d_loss: 1.38632417, g_loss: 0.69306028\n",
      "Step: [23118] d_loss: 1.38632035, g_loss: 0.69245863\n",
      "Step: [23119] d_loss: 1.38630843, g_loss: 0.69278413\n",
      "Step: [23120] d_loss: 1.38630438, g_loss: 0.69302738\n",
      "Step: [23121] d_loss: 1.38629711, g_loss: 0.69304383\n",
      "Step: [23122] d_loss: 1.38630176, g_loss: 0.69308150\n",
      "Step: [23123] d_loss: 1.38620806, g_loss: 0.69381624\n",
      "Step: [23124] d_loss: 1.38629842, g_loss: 0.69333529\n",
      "Step: [23125] d_loss: 1.38629937, g_loss: 0.69335359\n",
      "Step: [23126] d_loss: 1.38630319, g_loss: 0.69303238\n",
      "Step: [23127] d_loss: 1.38628578, g_loss: 0.69321156\n",
      "Step: [23128] d_loss: 1.38629198, g_loss: 0.69311380\n",
      "Step: [23129] d_loss: 1.38629532, g_loss: 0.69296718\n",
      "Step: [23130] d_loss: 1.38592684, g_loss: 0.69572246\n",
      "Step: [23131] d_loss: 1.38629317, g_loss: 0.69295275\n",
      "Step: [23132] d_loss: 1.38648701, g_loss: 0.69595599\n",
      "Step: [23133] d_loss: 1.38631701, g_loss: 0.69209015\n",
      "Step: [23134] d_loss: 1.38631928, g_loss: 0.69312149\n",
      "Step: [23135] d_loss: 1.38634765, g_loss: 0.69443315\n",
      "Step: [23136] d_loss: 1.38634634, g_loss: 0.69292963\n",
      "Step: [23137] d_loss: 1.38634443, g_loss: 0.69276857\n",
      "Step: [23138] d_loss: 1.38632607, g_loss: 0.69337720\n",
      "Step: [23139] d_loss: 1.38631821, g_loss: 0.69303560\n",
      "Step: [23140] d_loss: 1.38620090, g_loss: 0.69437170\n",
      "Step: [23141] d_loss: 1.38633955, g_loss: 0.69360089\n",
      "Step: [23142] d_loss: 1.38634312, g_loss: 0.69339663\n",
      "Step: [23143] d_loss: 1.38633037, g_loss: 0.69290406\n",
      "Step: [23144] d_loss: 1.38630915, g_loss: 0.69354057\n",
      "Step: [23145] d_loss: 1.38633227, g_loss: 0.69339609\n",
      "Step: [23146] d_loss: 1.38633406, g_loss: 0.69280767\n",
      "Step: [23147] d_loss: 1.38644052, g_loss: 0.69209433\n",
      "Step: [23148] d_loss: 1.38634157, g_loss: 0.69216341\n",
      "Step: [23149] d_loss: 1.38633597, g_loss: 0.69280297\n",
      "Step: [23150] d_loss: 1.38633025, g_loss: 0.69351447\n",
      "Step: [23151] d_loss: 1.38632834, g_loss: 0.69274592\n",
      "Step: [23152] d_loss: 1.38632917, g_loss: 0.69281542\n",
      "Step: [23153] d_loss: 1.38633370, g_loss: 0.69315135\n",
      "Step: [23154] d_loss: 1.38705361, g_loss: 0.69397986\n",
      "Step: [23155] d_loss: 1.38633835, g_loss: 0.69367063\n",
      "Step: [23156] d_loss: 1.38633990, g_loss: 0.69228292\n",
      "Step: [23157] d_loss: 1.38634992, g_loss: 0.69176930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23158] d_loss: 1.38634479, g_loss: 0.69292653\n",
      "Step: [23159] d_loss: 1.38633263, g_loss: 0.69284558\n",
      "Step: [23160] d_loss: 1.38634300, g_loss: 0.69350803\n",
      "Step: [23161] d_loss: 1.38633561, g_loss: 0.69338721\n",
      "Step: [23162] d_loss: 1.38633108, g_loss: 0.69350231\n",
      "Step: [23163] d_loss: 1.38633442, g_loss: 0.69329214\n",
      "Step: [23164] d_loss: 1.38633931, g_loss: 0.69413304\n",
      "Step: [23165] d_loss: 1.38633275, g_loss: 0.69387925\n",
      "Step: [23166] d_loss: 1.38702524, g_loss: 0.69330859\n",
      "Step: [23167] d_loss: 1.38632607, g_loss: 0.69219780\n",
      "Step: [23168] d_loss: 1.38632548, g_loss: 0.69278932\n",
      "Step: [23169] d_loss: 1.38632202, g_loss: 0.69283986\n",
      "Step: [23170] d_loss: 1.38632584, g_loss: 0.69345820\n",
      "Step: [23171] d_loss: 1.38632238, g_loss: 0.69340640\n",
      "Step: [23172] d_loss: 1.38632607, g_loss: 0.69196391\n",
      "Step: [23173] d_loss: 1.38633275, g_loss: 0.69173682\n",
      "Step: [23174] d_loss: 1.38633084, g_loss: 0.69292450\n",
      "Step: [23175] d_loss: 1.38633204, g_loss: 0.69313931\n",
      "Step: [23176] d_loss: 1.38633013, g_loss: 0.69380897\n",
      "Step: [23177] d_loss: 1.38633585, g_loss: 0.69371116\n",
      "Step: [23178] d_loss: 1.38631320, g_loss: 0.69387466\n",
      "Step: [23179] d_loss: 1.38632917, g_loss: 0.69327593\n",
      "Step: [23180] d_loss: 1.38632417, g_loss: 0.69342864\n",
      "Step: [23181] d_loss: 1.38630974, g_loss: 0.69282001\n",
      "Step: [23182] d_loss: 1.38633966, g_loss: 0.69309169\n",
      "Step: [23183] d_loss: 1.38634169, g_loss: 0.69362843\n",
      "Step: [23184] d_loss: 1.38633871, g_loss: 0.69387388\n",
      "Step: [23185] d_loss: 1.38639843, g_loss: 0.69292772\n",
      "Step: [23186] d_loss: 1.38635433, g_loss: 0.69285810\n",
      "Step: [23187] d_loss: 1.38627625, g_loss: 0.69216532\n",
      "Step: [23188] d_loss: 1.38636756, g_loss: 0.69201636\n",
      "Step: [23189] d_loss: 1.38633502, g_loss: 0.69214404\n",
      "Step: [23190] d_loss: 1.38636959, g_loss: 0.69304311\n",
      "Step: [23191] d_loss: 1.38636446, g_loss: 0.69389391\n",
      "Step: [23192] d_loss: 1.38636827, g_loss: 0.69201314\n",
      "Step: [23193] d_loss: 1.38636696, g_loss: 0.69213796\n",
      "Step: [23194] d_loss: 1.38637292, g_loss: 0.69334638\n",
      "Step: [23195] d_loss: 1.38637877, g_loss: 0.69512469\n",
      "Step: [23196] d_loss: 1.38639116, g_loss: 0.69506717\n",
      "Step: [23197] d_loss: 1.38639081, g_loss: 0.69304830\n",
      "Step: [23198] d_loss: 1.38640773, g_loss: 0.69271183\n",
      "Step: [23199] d_loss: 1.38641644, g_loss: 0.69233876\n",
      "Step: [23200] d_loss: 1.38642585, g_loss: 0.69371730\n",
      "Step: [23201] d_loss: 1.38643408, g_loss: 0.69510901\n",
      "Step: [23202] d_loss: 1.38644540, g_loss: 0.69394231\n",
      "Step: [23203] d_loss: 1.38645029, g_loss: 0.69286966\n",
      "Step: [23204] d_loss: 1.38644779, g_loss: 0.69298732\n",
      "Step: [23205] d_loss: 1.38640952, g_loss: 0.69184709\n",
      "Step: [23206] d_loss: 1.38646531, g_loss: 0.69289434\n",
      "Step: [23207] d_loss: 1.38647366, g_loss: 0.69400471\n",
      "Step: [23208] d_loss: 1.38648915, g_loss: 0.69553548\n",
      "Step: [23209] d_loss: 1.38649249, g_loss: 0.69385719\n",
      "Step: [23210] d_loss: 1.38651013, g_loss: 0.69211745\n",
      "Step: [23211] d_loss: 1.38652492, g_loss: 0.69182909\n",
      "Step: [23212] d_loss: 1.38652277, g_loss: 0.69392812\n",
      "Step: [23213] d_loss: 1.38653553, g_loss: 0.69254363\n",
      "Step: [23214] d_loss: 1.38656926, g_loss: 0.69373107\n",
      "Step: [23215] d_loss: 1.38660538, g_loss: 0.69389558\n",
      "Step: [23216] d_loss: 1.38659596, g_loss: 0.69492090\n",
      "Step: [23217] d_loss: 1.38657570, g_loss: 0.69603640\n",
      "Step: [23218] d_loss: 1.38651419, g_loss: 0.69481766\n",
      "Step: [23219] d_loss: 1.38646030, g_loss: 0.69358182\n",
      "Step: [23220] d_loss: 1.38640285, g_loss: 0.69340277\n",
      "Step: [23221] d_loss: 1.38637137, g_loss: 0.69323051\n",
      "Step: [23222] d_loss: 1.38628435, g_loss: 0.69232434\n",
      "Step: [23223] d_loss: 1.38629830, g_loss: 0.69324887\n",
      "Step: [23224] d_loss: 1.38631928, g_loss: 0.69324738\n",
      "Step: [23225] d_loss: 1.38628626, g_loss: 0.69126165\n",
      "Step: [23226] d_loss: 1.38635325, g_loss: 0.69258040\n",
      "Step: [23227] d_loss: 1.38638639, g_loss: 0.69210941\n",
      "Step: [23228] d_loss: 1.38639772, g_loss: 0.69255400\n",
      "Step: [23229] d_loss: 1.38638878, g_loss: 0.69209677\n",
      "Step: [23230] d_loss: 1.38643503, g_loss: 0.69318008\n",
      "Step: [23231] d_loss: 1.38644063, g_loss: 0.69387245\n",
      "Step: [23232] d_loss: 1.38642037, g_loss: 0.69464290\n",
      "Step: [23233] d_loss: 1.38641834, g_loss: 0.69409746\n",
      "Step: [23234] d_loss: 1.38640702, g_loss: 0.69164968\n",
      "Step: [23235] d_loss: 1.38640237, g_loss: 0.69295150\n",
      "Step: [23236] d_loss: 1.38640034, g_loss: 0.69359076\n",
      "Step: [23237] d_loss: 1.38631666, g_loss: 0.69306654\n",
      "Step: [23238] d_loss: 1.38628805, g_loss: 0.69305319\n",
      "Step: [23239] d_loss: 1.38629234, g_loss: 0.69310236\n",
      "Step: [23240] d_loss: 1.38629484, g_loss: 0.69329977\n",
      "Step: [23241] d_loss: 1.38628888, g_loss: 0.69333398\n",
      "Step: [23242] d_loss: 1.38628960, g_loss: 0.69313240\n",
      "Step: [23243] d_loss: 1.38629532, g_loss: 0.69311595\n",
      "Step: [23244] d_loss: 1.38629293, g_loss: 0.69320118\n",
      "Step: [23245] d_loss: 1.38629532, g_loss: 0.69307578\n",
      "Step: [23246] d_loss: 1.38629091, g_loss: 0.69301593\n",
      "Step: [23247] d_loss: 1.38629687, g_loss: 0.69323415\n",
      "Step: [23248] d_loss: 1.38629019, g_loss: 0.69337952\n",
      "Step: [23249] d_loss: 1.38629079, g_loss: 0.69328380\n",
      "Step: [23250] d_loss: 1.38628972, g_loss: 0.69317353\n",
      "Step: [23251] d_loss: 1.38628316, g_loss: 0.69310653\n",
      "Step: [23252] d_loss: 1.38624895, g_loss: 0.69287241\n",
      "Step: [23253] d_loss: 1.38610113, g_loss: 0.69320786\n",
      "Step: [23254] d_loss: 1.38628650, g_loss: 0.69331497\n",
      "Step: [23255] d_loss: 1.38628674, g_loss: 0.69306690\n",
      "Step: [23256] d_loss: 1.38635421, g_loss: 0.69155121\n",
      "Step: [23257] d_loss: 1.38624251, g_loss: 0.69345880\n",
      "Step: [23258] d_loss: 1.38627815, g_loss: 0.69292837\n",
      "Step: [23259] d_loss: 1.38643169, g_loss: 0.69441772\n",
      "Step: [23260] d_loss: 1.38647008, g_loss: 0.69285822\n",
      "Step: [23261] d_loss: 1.38657272, g_loss: 0.69234890\n",
      "Step: [23262] d_loss: 1.38662696, g_loss: 0.69136405\n",
      "Step: [23263] d_loss: 1.38639414, g_loss: 0.69210070\n",
      "Step: [23264] d_loss: 1.38651109, g_loss: 0.69289696\n",
      "Step: [23265] d_loss: 1.38630533, g_loss: 0.69525647\n",
      "Step: [23266] d_loss: 1.38633192, g_loss: 0.69520319\n",
      "Step: [23267] d_loss: 1.38634300, g_loss: 0.69317830\n",
      "Step: [23268] d_loss: 1.38637280, g_loss: 0.69246930\n",
      "Step: [23269] d_loss: 1.38669038, g_loss: 0.68944383\n",
      "Step: [23270] d_loss: 1.38650131, g_loss: 0.69197094\n",
      "Step: [23271] d_loss: 1.38656616, g_loss: 0.69386864\n",
      "Step: [23272] d_loss: 1.38662159, g_loss: 0.69618690\n",
      "Step: [23273] d_loss: 1.38652945, g_loss: 0.69272280\n",
      "Step: [23274] d_loss: 1.38687086, g_loss: 0.69410843\n",
      "Step: [23275] d_loss: 1.38699555, g_loss: 0.69416332\n",
      "Step: [23276] d_loss: 1.38719118, g_loss: 0.69702715\n",
      "Step: [23277] d_loss: 1.38716185, g_loss: 0.69599628\n",
      "Step: [23278] d_loss: 1.38714743, g_loss: 0.69727385\n",
      "Step: [23279] d_loss: 1.38702214, g_loss: 0.69470841\n",
      "Step: [23280] d_loss: 1.38692474, g_loss: 0.69248748\n",
      "Step: [23281] d_loss: 1.38686740, g_loss: 0.69228554\n",
      "Step: [23282] d_loss: 1.38680053, g_loss: 0.69451588\n",
      "Step: [23283] d_loss: 1.38696051, g_loss: 0.69258153\n",
      "Step: [23284] d_loss: 1.38679147, g_loss: 0.69294310\n",
      "Step: [23285] d_loss: 1.38680935, g_loss: 0.69128394\n",
      "Step: [23286] d_loss: 1.38683319, g_loss: 0.69158745\n",
      "Step: [23287] d_loss: 1.38678205, g_loss: 0.68822169\n",
      "Step: [23288] d_loss: 1.38683033, g_loss: 0.69199646\n",
      "Step: [23289] d_loss: 1.38674378, g_loss: 0.69244564\n",
      "Step: [23290] d_loss: 1.38663650, g_loss: 0.69079721\n",
      "Step: [23291] d_loss: 1.38648784, g_loss: 0.69231594\n",
      "Step: [23292] d_loss: 1.38638639, g_loss: 0.69272220\n",
      "Step: [23293] d_loss: 1.38634074, g_loss: 0.69376075\n",
      "Step: [23294] d_loss: 1.38632786, g_loss: 0.69406366\n",
      "Step: [23295] d_loss: 1.38628340, g_loss: 0.69389433\n",
      "Step: [23296] d_loss: 1.38629746, g_loss: 0.69214463\n",
      "Step: [23297] d_loss: 1.38630223, g_loss: 0.69389361\n",
      "Step: [23298] d_loss: 1.38630581, g_loss: 0.69449145\n",
      "Step: [23299] d_loss: 1.38630772, g_loss: 0.69379056\n",
      "Step: [23300] d_loss: 1.38630831, g_loss: 0.69500637\n",
      "Step: [23301] d_loss: 1.38634157, g_loss: 0.69340324\n",
      "Step: [23302] d_loss: 1.38634586, g_loss: 0.69178617\n",
      "Step: [23303] d_loss: 1.38634467, g_loss: 0.69336009\n",
      "Step: [23304] d_loss: 1.38634539, g_loss: 0.69464719\n",
      "Step: [23305] d_loss: 1.38633263, g_loss: 0.69348842\n",
      "Step: [23306] d_loss: 1.38650680, g_loss: 0.69356918\n",
      "Step: [23307] d_loss: 1.38634253, g_loss: 0.69296533\n",
      "Step: [23308] d_loss: 1.38633442, g_loss: 0.69303840\n",
      "Step: [23309] d_loss: 1.38631880, g_loss: 0.69309139\n",
      "Step: [23310] d_loss: 1.38630533, g_loss: 0.69275522\n",
      "Step: [23311] d_loss: 1.38630128, g_loss: 0.69310325\n",
      "Step: [23312] d_loss: 1.38629401, g_loss: 0.69347697\n",
      "Step: [23313] d_loss: 1.38627231, g_loss: 0.69324028\n",
      "Step: [23314] d_loss: 1.38649654, g_loss: 0.69480455\n",
      "Step: [23315] d_loss: 1.38774550, g_loss: 0.69565195\n",
      "Step: [23316] d_loss: 1.38914132, g_loss: 0.69798785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23317] d_loss: 1.38938522, g_loss: 0.69108772\n",
      "Step: [23318] d_loss: 1.38854635, g_loss: 0.68908447\n",
      "Step: [23319] d_loss: 1.38761222, g_loss: 0.69508862\n",
      "Step: [23320] d_loss: 1.38688767, g_loss: 0.69665039\n",
      "Step: [23321] d_loss: 1.38649762, g_loss: 0.69419819\n",
      "Step: [23322] d_loss: 1.38651180, g_loss: 0.69393587\n",
      "Step: [23323] d_loss: 1.38632882, g_loss: 0.69293690\n",
      "Step: [23324] d_loss: 1.38686585, g_loss: 0.69333547\n",
      "Step: [23325] d_loss: 1.38628662, g_loss: 0.69350660\n",
      "Step: [23326] d_loss: 1.38630509, g_loss: 0.69218445\n",
      "Step: [23327] d_loss: 1.38629699, g_loss: 0.69312823\n",
      "Step: [23328] d_loss: 1.38629413, g_loss: 0.69432008\n",
      "Step: [23329] d_loss: 1.38621688, g_loss: 0.69539940\n",
      "Step: [23330] d_loss: 1.38632786, g_loss: 0.69376588\n",
      "Step: [23331] d_loss: 1.38632619, g_loss: 0.69299626\n",
      "Step: [23332] d_loss: 1.38632107, g_loss: 0.69270825\n",
      "Step: [23333] d_loss: 1.38630366, g_loss: 0.69234550\n",
      "Step: [23334] d_loss: 1.38630307, g_loss: 0.69449300\n",
      "Step: [23335] d_loss: 1.38629484, g_loss: 0.69384229\n",
      "Step: [23336] d_loss: 1.38631248, g_loss: 0.69233119\n",
      "Step: [23337] d_loss: 1.38630533, g_loss: 0.69331646\n",
      "Step: [23338] d_loss: 1.38629532, g_loss: 0.69355023\n",
      "Step: [23339] d_loss: 1.38630140, g_loss: 0.69341946\n",
      "Step: [23340] d_loss: 1.38630617, g_loss: 0.69385445\n",
      "Step: [23341] d_loss: 1.38627636, g_loss: 0.69330561\n",
      "Step: [23342] d_loss: 1.38630366, g_loss: 0.69298321\n",
      "Step: [23343] d_loss: 1.38628745, g_loss: 0.69291568\n",
      "Step: [23344] d_loss: 1.38629341, g_loss: 0.69302964\n",
      "Step: [23345] d_loss: 1.38627553, g_loss: 0.69336903\n",
      "Step: [23346] d_loss: 1.38629627, g_loss: 0.69310582\n",
      "Step: [23347] d_loss: 1.38629436, g_loss: 0.69300854\n",
      "Step: [23348] d_loss: 1.38628674, g_loss: 0.69328403\n",
      "Step: [23349] d_loss: 1.38629460, g_loss: 0.69330519\n",
      "Step: [23350] d_loss: 1.38629603, g_loss: 0.69323474\n",
      "Step: [23351] d_loss: 1.38628578, g_loss: 0.69314623\n",
      "Step: [23352] d_loss: 1.38629138, g_loss: 0.69297779\n",
      "Step: [23353] d_loss: 1.38628364, g_loss: 0.69324499\n",
      "Step: [23354] d_loss: 1.38629222, g_loss: 0.69312984\n",
      "Step: [23355] d_loss: 1.38629425, g_loss: 0.69321311\n",
      "Step: [23356] d_loss: 1.38628507, g_loss: 0.69324183\n",
      "Step: [23357] d_loss: 1.38628125, g_loss: 0.69310069\n",
      "Step: [23358] d_loss: 1.38630497, g_loss: 0.69313478\n",
      "Step: [23359] d_loss: 1.38629270, g_loss: 0.69304454\n",
      "Step: [23360] d_loss: 1.38629317, g_loss: 0.69382250\n",
      "Step: [23361] d_loss: 1.38630271, g_loss: 0.69345152\n",
      "Step: [23362] d_loss: 1.38630629, g_loss: 0.69389874\n",
      "Step: [23363] d_loss: 1.38630712, g_loss: 0.69307089\n",
      "Step: [23364] d_loss: 1.38631082, g_loss: 0.69308406\n",
      "Step: [23365] d_loss: 1.38630009, g_loss: 0.69290888\n",
      "Step: [23366] d_loss: 1.38630271, g_loss: 0.69310892\n",
      "Step: [23367] d_loss: 1.38629484, g_loss: 0.69317144\n",
      "Step: [23368] d_loss: 1.38630366, g_loss: 0.69357312\n",
      "Step: [23369] d_loss: 1.38628531, g_loss: 0.69332141\n",
      "Step: [23370] d_loss: 1.38628578, g_loss: 0.69320017\n",
      "Step: [23371] d_loss: 1.38628244, g_loss: 0.69302702\n",
      "Step: [23372] d_loss: 1.38628674, g_loss: 0.69292921\n",
      "Step: [23373] d_loss: 1.38630199, g_loss: 0.69330847\n",
      "Step: [23374] d_loss: 1.38629282, g_loss: 0.69315159\n",
      "Step: [23375] d_loss: 1.38629663, g_loss: 0.69321835\n",
      "Step: [23376] d_loss: 1.38628745, g_loss: 0.69318968\n",
      "Step: [23377] d_loss: 1.38628697, g_loss: 0.69360960\n",
      "Step: [23378] d_loss: 1.38629246, g_loss: 0.69334602\n",
      "Step: [23379] d_loss: 1.38629699, g_loss: 0.69283348\n",
      "Step: [23380] d_loss: 1.38631177, g_loss: 0.69309461\n",
      "Step: [23381] d_loss: 1.38628721, g_loss: 0.69306654\n",
      "Step: [23382] d_loss: 1.38629305, g_loss: 0.69298089\n",
      "Step: [23383] d_loss: 1.38631654, g_loss: 0.69319367\n",
      "Step: [23384] d_loss: 1.38629496, g_loss: 0.69319880\n",
      "Step: [23385] d_loss: 1.38628864, g_loss: 0.69309485\n",
      "Step: [23386] d_loss: 1.38628078, g_loss: 0.69324249\n",
      "Step: [23387] d_loss: 1.38630235, g_loss: 0.69309676\n",
      "Step: [23388] d_loss: 1.38629937, g_loss: 0.69328880\n",
      "Step: [23389] d_loss: 1.38628805, g_loss: 0.69315881\n",
      "Step: [23390] d_loss: 1.38629293, g_loss: 0.69303834\n",
      "Step: [23391] d_loss: 1.38628590, g_loss: 0.69329929\n",
      "Step: [23392] d_loss: 1.38628983, g_loss: 0.69327813\n",
      "Step: [23393] d_loss: 1.38629150, g_loss: 0.69356370\n",
      "Step: [23394] d_loss: 1.38630462, g_loss: 0.69360638\n",
      "Step: [23395] d_loss: 1.38630950, g_loss: 0.69285393\n",
      "Step: [23396] d_loss: 1.38630629, g_loss: 0.69291717\n",
      "Step: [23397] d_loss: 1.38629675, g_loss: 0.69318312\n",
      "Step: [23398] d_loss: 1.38630176, g_loss: 0.69427556\n",
      "Step: [23399] d_loss: 1.38629103, g_loss: 0.69473338\n",
      "Step: [23400] d_loss: 1.38631356, g_loss: 0.69360900\n",
      "Step: [23401] d_loss: 1.38631916, g_loss: 0.69163978\n",
      "Step: [23402] d_loss: 1.38629925, g_loss: 0.69405568\n",
      "Step: [23403] d_loss: 1.38632238, g_loss: 0.69331872\n",
      "Step: [23404] d_loss: 1.38629806, g_loss: 0.69309455\n",
      "Step: [23405] d_loss: 1.38629389, g_loss: 0.69285756\n",
      "Step: [23406] d_loss: 1.38628840, g_loss: 0.69311726\n",
      "Step: [23407] d_loss: 1.38629436, g_loss: 0.69343698\n",
      "Step: [23408] d_loss: 1.38627458, g_loss: 0.69390011\n",
      "Step: [23409] d_loss: 1.38629389, g_loss: 0.69341576\n",
      "Step: [23410] d_loss: 1.38629031, g_loss: 0.69302684\n",
      "Step: [23411] d_loss: 1.38626957, g_loss: 0.69284570\n",
      "Step: [23412] d_loss: 1.38629282, g_loss: 0.69279408\n",
      "Step: [23413] d_loss: 1.38625622, g_loss: 0.69360018\n",
      "Step: [23414] d_loss: 1.38628399, g_loss: 0.69340730\n",
      "Step: [23415] d_loss: 1.38644254, g_loss: 0.69267863\n",
      "Step: [23416] d_loss: 1.38677418, g_loss: 0.69249344\n",
      "Step: [23417] d_loss: 1.38696313, g_loss: 0.69136560\n",
      "Step: [23418] d_loss: 1.38699913, g_loss: 0.69397020\n",
      "Step: [23419] d_loss: 1.38693953, g_loss: 0.69293094\n",
      "Step: [23420] d_loss: 1.38683784, g_loss: 0.69454598\n",
      "Step: [23421] d_loss: 1.38674855, g_loss: 0.69260156\n",
      "Step: [23422] d_loss: 1.38660967, g_loss: 0.69382763\n",
      "Step: [23423] d_loss: 1.38655114, g_loss: 0.69319654\n",
      "Step: [23424] d_loss: 1.38647795, g_loss: 0.69413584\n",
      "Step: [23425] d_loss: 1.38641930, g_loss: 0.69321221\n",
      "Step: [23426] d_loss: 1.38637578, g_loss: 0.69220942\n",
      "Step: [23427] d_loss: 1.38719380, g_loss: 0.69185853\n",
      "Step: [23428] d_loss: 1.38631630, g_loss: 0.69299555\n",
      "Step: [23429] d_loss: 1.38629425, g_loss: 0.69317079\n",
      "Step: [23430] d_loss: 1.38630068, g_loss: 0.69326103\n",
      "Step: [23431] d_loss: 1.38629031, g_loss: 0.69332850\n",
      "Step: [23432] d_loss: 1.38630199, g_loss: 0.69402862\n",
      "Step: [23433] d_loss: 1.38630450, g_loss: 0.69320405\n",
      "Step: [23434] d_loss: 1.38628936, g_loss: 0.69432950\n",
      "Step: [23435] d_loss: 1.38632059, g_loss: 0.69306254\n",
      "Step: [23436] d_loss: 1.38631165, g_loss: 0.69273788\n",
      "Step: [23437] d_loss: 1.38630199, g_loss: 0.69274110\n",
      "Step: [23438] d_loss: 1.38627410, g_loss: 0.69310665\n",
      "Step: [23439] d_loss: 1.38628614, g_loss: 0.69372523\n",
      "Step: [23440] d_loss: 1.38629329, g_loss: 0.69328487\n",
      "Step: [23441] d_loss: 1.38628531, g_loss: 0.69279373\n",
      "Step: [23442] d_loss: 1.38628829, g_loss: 0.69321620\n",
      "Step: [23443] d_loss: 1.38629222, g_loss: 0.69326258\n",
      "Step: [23444] d_loss: 1.38629138, g_loss: 0.69327682\n",
      "Step: [23445] d_loss: 1.38629174, g_loss: 0.69332290\n",
      "Step: [23446] d_loss: 1.38629472, g_loss: 0.69302398\n",
      "Step: [23447] d_loss: 1.38629568, g_loss: 0.69313139\n",
      "Step: [23448] d_loss: 1.38629508, g_loss: 0.69329417\n",
      "Step: [23449] d_loss: 1.38629091, g_loss: 0.69346100\n",
      "Step: [23450] d_loss: 1.38629520, g_loss: 0.69311053\n",
      "Step: [23451] d_loss: 1.38629234, g_loss: 0.69314075\n",
      "Step: [23452] d_loss: 1.38628721, g_loss: 0.69292355\n",
      "Step: [23453] d_loss: 1.38629329, g_loss: 0.69298279\n",
      "Step: [23454] d_loss: 1.38628709, g_loss: 0.69366336\n",
      "Step: [23455] d_loss: 1.38629150, g_loss: 0.69337177\n",
      "Step: [23456] d_loss: 1.38628602, g_loss: 0.69323808\n",
      "Step: [23457] d_loss: 1.38629889, g_loss: 0.69322574\n",
      "Step: [23458] d_loss: 1.38630152, g_loss: 0.69348657\n",
      "Step: [23459] d_loss: 1.38629508, g_loss: 0.69314325\n",
      "Step: [23460] d_loss: 1.38630223, g_loss: 0.69242287\n",
      "Step: [23461] d_loss: 1.38618982, g_loss: 0.69288296\n",
      "Step: [23462] d_loss: 1.38629091, g_loss: 0.69313490\n",
      "Step: [23463] d_loss: 1.38629675, g_loss: 0.69324994\n",
      "Step: [23464] d_loss: 1.38629532, g_loss: 0.69345117\n",
      "Step: [23465] d_loss: 1.38629842, g_loss: 0.69262707\n",
      "Step: [23466] d_loss: 1.38629985, g_loss: 0.69339782\n",
      "Step: [23467] d_loss: 1.38632441, g_loss: 0.69398344\n",
      "Step: [23468] d_loss: 1.38629913, g_loss: 0.69400012\n",
      "Step: [23469] d_loss: 1.38630509, g_loss: 0.69376171\n",
      "Step: [23470] d_loss: 1.38630414, g_loss: 0.69275236\n",
      "Step: [23471] d_loss: 1.38631165, g_loss: 0.69287574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23472] d_loss: 1.38629591, g_loss: 0.69277149\n",
      "Step: [23473] d_loss: 1.38629818, g_loss: 0.69294417\n",
      "Step: [23474] d_loss: 1.38630390, g_loss: 0.69319427\n",
      "Step: [23475] d_loss: 1.38629770, g_loss: 0.69317591\n",
      "Step: [23476] d_loss: 1.38628948, g_loss: 0.69321132\n",
      "Step: [23477] d_loss: 1.38629258, g_loss: 0.69290322\n",
      "Step: [23478] d_loss: 1.38638628, g_loss: 0.69329166\n",
      "Step: [23479] d_loss: 1.38629472, g_loss: 0.69315988\n",
      "Step: [23480] d_loss: 1.38628972, g_loss: 0.69342566\n",
      "Step: [23481] d_loss: 1.38629127, g_loss: 0.69309551\n",
      "Step: [23482] d_loss: 1.38629866, g_loss: 0.69276017\n",
      "Step: [23483] d_loss: 1.38629270, g_loss: 0.69308007\n",
      "Step: [23484] d_loss: 1.38629246, g_loss: 0.69320762\n",
      "Step: [23485] d_loss: 1.38631940, g_loss: 0.69325858\n",
      "Step: [23486] d_loss: 1.38628578, g_loss: 0.69306082\n",
      "Step: [23487] d_loss: 1.38628721, g_loss: 0.69305742\n",
      "Step: [23488] d_loss: 1.38629484, g_loss: 0.69311649\n",
      "Step: [23489] d_loss: 1.38673830, g_loss: 0.69317245\n",
      "Step: [23490] d_loss: 1.38628387, g_loss: 0.69376665\n",
      "Step: [23491] d_loss: 1.38629055, g_loss: 0.69309145\n",
      "Step: [23492] d_loss: 1.38628674, g_loss: 0.69295180\n",
      "Step: [23493] d_loss: 1.38628888, g_loss: 0.69280535\n",
      "Step: [23494] d_loss: 1.38628578, g_loss: 0.69309402\n",
      "Step: [23495] d_loss: 1.38628531, g_loss: 0.69350374\n",
      "Step: [23496] d_loss: 1.38627481, g_loss: 0.69359279\n",
      "Step: [23497] d_loss: 1.38626409, g_loss: 0.69331336\n",
      "Step: [23498] d_loss: 1.38628936, g_loss: 0.69297242\n",
      "Step: [23499] d_loss: 1.38628411, g_loss: 0.69312453\n",
      "Step: [23500] d_loss: 1.38628781, g_loss: 0.69308156\n",
      "Step: [23501] d_loss: 1.38629591, g_loss: 0.69325137\n",
      "Step: [23502] d_loss: 1.38625836, g_loss: 0.69248879\n",
      "Step: [23503] d_loss: 1.38629365, g_loss: 0.69326526\n",
      "Step: [23504] d_loss: 1.38673615, g_loss: 0.69354463\n",
      "Step: [23505] d_loss: 1.38629365, g_loss: 0.69349158\n",
      "Step: [23506] d_loss: 1.38629436, g_loss: 0.69307041\n",
      "Step: [23507] d_loss: 1.38629973, g_loss: 0.69316101\n",
      "Step: [23508] d_loss: 1.38630223, g_loss: 0.69300520\n",
      "Step: [23509] d_loss: 1.38626552, g_loss: 0.69339156\n",
      "Step: [23510] d_loss: 1.38627553, g_loss: 0.69273543\n",
      "Step: [23511] d_loss: 1.38630533, g_loss: 0.69366407\n",
      "Step: [23512] d_loss: 1.38626027, g_loss: 0.69370770\n",
      "Step: [23513] d_loss: 1.38629150, g_loss: 0.69244802\n",
      "Step: [23514] d_loss: 1.38629007, g_loss: 0.69369709\n",
      "Step: [23515] d_loss: 1.38624215, g_loss: 0.69346941\n",
      "Step: [23516] d_loss: 1.38630605, g_loss: 0.69306087\n",
      "Step: [23517] d_loss: 1.38628292, g_loss: 0.69320631\n",
      "Step: [23518] d_loss: 1.38627291, g_loss: 0.69373000\n",
      "Step: [23519] d_loss: 1.38624656, g_loss: 0.69222593\n",
      "Step: [23520] d_loss: 1.38630760, g_loss: 0.69227815\n",
      "Step: [23521] d_loss: 1.38632917, g_loss: 0.69412261\n",
      "Step: [23522] d_loss: 1.38634777, g_loss: 0.69365251\n",
      "Step: [23523] d_loss: 1.38645864, g_loss: 0.69551766\n",
      "Step: [23524] d_loss: 1.38646555, g_loss: 0.69497693\n",
      "Step: [23525] d_loss: 1.38648033, g_loss: 0.69540936\n",
      "Step: [23526] d_loss: 1.38656485, g_loss: 0.69236487\n",
      "Step: [23527] d_loss: 1.38676214, g_loss: 0.69244045\n",
      "Step: [23528] d_loss: 1.38726342, g_loss: 0.69241226\n",
      "Step: [23529] d_loss: 1.38989830, g_loss: 0.69886988\n",
      "Step: [23530] d_loss: 1.39521515, g_loss: 0.69376183\n",
      "Step: [23531] d_loss: 1.39671826, g_loss: 0.69968575\n",
      "Step: [23532] d_loss: 1.39139795, g_loss: 0.69997752\n",
      "Step: [23533] d_loss: 1.38688111, g_loss: 0.69532871\n",
      "Step: [23534] d_loss: 1.38631582, g_loss: 0.69182014\n",
      "Step: [23535] d_loss: 1.38647187, g_loss: 0.69046724\n",
      "Step: [23536] d_loss: 1.38649821, g_loss: 0.69145167\n",
      "Step: [23537] d_loss: 1.38714755, g_loss: 0.69265342\n",
      "Step: [23538] d_loss: 1.38631415, g_loss: 0.69275570\n",
      "Step: [23539] d_loss: 1.38629484, g_loss: 0.69331956\n",
      "Step: [23540] d_loss: 1.38630152, g_loss: 0.69345593\n",
      "Step: [23541] d_loss: 1.38632870, g_loss: 0.69359934\n",
      "Step: [23542] d_loss: 1.38630652, g_loss: 0.69343185\n",
      "Step: [23543] d_loss: 1.38630140, g_loss: 0.69287658\n",
      "Step: [23544] d_loss: 1.38629591, g_loss: 0.69258535\n",
      "Step: [23545] d_loss: 1.38629508, g_loss: 0.69281000\n",
      "Step: [23546] d_loss: 1.38629246, g_loss: 0.69321811\n",
      "Step: [23547] d_loss: 1.38629472, g_loss: 0.69329882\n",
      "Step: [23548] d_loss: 1.38629580, g_loss: 0.69316810\n",
      "Step: [23549] d_loss: 1.38629234, g_loss: 0.69320571\n",
      "Step: [23550] d_loss: 1.38629317, g_loss: 0.69304931\n",
      "Step: [23551] d_loss: 1.38629186, g_loss: 0.69292915\n",
      "Step: [23552] d_loss: 1.38629270, g_loss: 0.69310200\n",
      "Step: [23553] d_loss: 1.38629317, g_loss: 0.69322550\n",
      "Step: [23554] d_loss: 1.38629258, g_loss: 0.69316262\n",
      "Step: [23555] d_loss: 1.38629293, g_loss: 0.69306529\n",
      "Step: [23556] d_loss: 1.38629186, g_loss: 0.69300210\n",
      "Step: [23557] d_loss: 1.38628924, g_loss: 0.69308722\n",
      "Step: [23558] d_loss: 1.38629317, g_loss: 0.69322729\n",
      "Step: [23559] d_loss: 1.38629079, g_loss: 0.69319129\n",
      "Step: [23560] d_loss: 1.38629067, g_loss: 0.69315410\n",
      "Step: [23561] d_loss: 1.38629198, g_loss: 0.69306225\n",
      "Step: [23562] d_loss: 1.38629436, g_loss: 0.69321191\n",
      "Step: [23563] d_loss: 1.38629448, g_loss: 0.69309103\n",
      "Step: [23564] d_loss: 1.38629317, g_loss: 0.69297081\n",
      "Step: [23565] d_loss: 1.38629341, g_loss: 0.69310236\n",
      "Step: [23566] d_loss: 1.38629401, g_loss: 0.69323683\n",
      "Step: [23567] d_loss: 1.38629293, g_loss: 0.69311374\n",
      "Step: [23568] d_loss: 1.38628948, g_loss: 0.69314355\n",
      "Step: [23569] d_loss: 1.38629138, g_loss: 0.69311816\n",
      "Step: [23570] d_loss: 1.38629091, g_loss: 0.69300389\n",
      "Step: [23571] d_loss: 1.38629198, g_loss: 0.69309443\n",
      "Step: [23572] d_loss: 1.38629341, g_loss: 0.69321764\n",
      "Step: [23573] d_loss: 1.38630462, g_loss: 0.69304019\n",
      "Step: [23574] d_loss: 1.38629317, g_loss: 0.69324470\n",
      "Step: [23575] d_loss: 1.38629389, g_loss: 0.69319493\n",
      "Step: [23576] d_loss: 1.38629317, g_loss: 0.69320720\n",
      "Step: [23577] d_loss: 1.38629460, g_loss: 0.69309533\n",
      "Step: [23578] d_loss: 1.38629341, g_loss: 0.69296467\n",
      "Step: [23579] d_loss: 1.38629258, g_loss: 0.69309312\n",
      "Step: [23580] d_loss: 1.38629103, g_loss: 0.69321442\n",
      "Step: [23581] d_loss: 1.38629174, g_loss: 0.69321424\n",
      "Step: [23582] d_loss: 1.38629055, g_loss: 0.69312125\n",
      "Step: [23583] d_loss: 1.38629019, g_loss: 0.69303745\n",
      "Step: [23584] d_loss: 1.38629222, g_loss: 0.69310594\n",
      "Step: [23585] d_loss: 1.38629162, g_loss: 0.69346106\n",
      "Step: [23586] d_loss: 1.38629031, g_loss: 0.69324225\n",
      "Step: [23587] d_loss: 1.38629413, g_loss: 0.69321299\n",
      "Step: [23588] d_loss: 1.38629484, g_loss: 0.69316161\n",
      "Step: [23589] d_loss: 1.38629472, g_loss: 0.69315183\n",
      "Step: [23590] d_loss: 1.38629282, g_loss: 0.69325155\n",
      "Step: [23591] d_loss: 1.38629198, g_loss: 0.69306135\n",
      "Step: [23592] d_loss: 1.38629568, g_loss: 0.69293326\n",
      "Step: [23593] d_loss: 1.38629889, g_loss: 0.69298482\n",
      "Step: [23594] d_loss: 1.38629150, g_loss: 0.69329208\n",
      "Step: [23595] d_loss: 1.38629222, g_loss: 0.69285464\n",
      "Step: [23596] d_loss: 1.38628340, g_loss: 0.69266188\n",
      "Step: [23597] d_loss: 1.38629305, g_loss: 0.69284153\n",
      "Step: [23598] d_loss: 1.38629222, g_loss: 0.69321978\n",
      "Step: [23599] d_loss: 1.38629389, g_loss: 0.69342268\n",
      "Step: [23600] d_loss: 1.38629079, g_loss: 0.69324946\n",
      "Step: [23601] d_loss: 1.38629031, g_loss: 0.69313633\n",
      "Step: [23602] d_loss: 1.38629067, g_loss: 0.69306552\n",
      "Step: [23603] d_loss: 1.38626742, g_loss: 0.69310850\n",
      "Step: [23604] d_loss: 1.38629365, g_loss: 0.69300330\n",
      "Step: [23605] d_loss: 1.38629425, g_loss: 0.69318163\n",
      "Step: [23606] d_loss: 1.38629341, g_loss: 0.69312453\n",
      "Step: [23607] d_loss: 1.38628769, g_loss: 0.69319254\n",
      "Step: [23608] d_loss: 1.38629007, g_loss: 0.69317263\n",
      "Step: [23609] d_loss: 1.38629246, g_loss: 0.69314092\n",
      "Step: [23610] d_loss: 1.38622582, g_loss: 0.69378549\n",
      "Step: [23611] d_loss: 1.38629293, g_loss: 0.69299221\n",
      "Step: [23612] d_loss: 1.38629580, g_loss: 0.69314659\n",
      "Step: [23613] d_loss: 1.38626599, g_loss: 0.69304752\n",
      "Step: [23614] d_loss: 1.38629329, g_loss: 0.69303417\n",
      "Step: [23615] d_loss: 1.38629484, g_loss: 0.69313240\n",
      "Step: [23616] d_loss: 1.38627577, g_loss: 0.69312888\n",
      "Step: [23617] d_loss: 1.38629007, g_loss: 0.69318295\n",
      "Step: [23618] d_loss: 1.38628888, g_loss: 0.69316292\n",
      "Step: [23619] d_loss: 1.38628745, g_loss: 0.69338322\n",
      "Step: [23620] d_loss: 1.38648462, g_loss: 0.69324446\n",
      "Step: [23621] d_loss: 1.38695729, g_loss: 0.69603413\n",
      "Step: [23622] d_loss: 1.38712120, g_loss: 0.69913399\n",
      "Step: [23623] d_loss: 1.38691950, g_loss: 0.69534075\n",
      "Step: [23624] d_loss: 1.38662577, g_loss: 0.69133282\n",
      "Step: [23625] d_loss: 1.38645732, g_loss: 0.69171745\n",
      "Step: [23626] d_loss: 1.38633776, g_loss: 0.69320780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23627] d_loss: 1.38630831, g_loss: 0.69395185\n",
      "Step: [23628] d_loss: 1.38650036, g_loss: 0.69407499\n",
      "Step: [23629] d_loss: 1.38659739, g_loss: 0.69219398\n",
      "Step: [23630] d_loss: 1.38657308, g_loss: 0.69336522\n",
      "Step: [23631] d_loss: 1.38648856, g_loss: 0.69322062\n",
      "Step: [23632] d_loss: 1.38639927, g_loss: 0.69362718\n",
      "Step: [23633] d_loss: 1.38634276, g_loss: 0.69314063\n",
      "Step: [23634] d_loss: 1.38631248, g_loss: 0.69288939\n",
      "Step: [23635] d_loss: 1.38629913, g_loss: 0.69278991\n",
      "Step: [23636] d_loss: 1.38629556, g_loss: 0.69304365\n",
      "Step: [23637] d_loss: 1.38628817, g_loss: 0.69344270\n",
      "Step: [23638] d_loss: 1.38628292, g_loss: 0.69300711\n",
      "Step: [23639] d_loss: 1.38629222, g_loss: 0.69325757\n",
      "Step: [23640] d_loss: 1.38629293, g_loss: 0.69292104\n",
      "Step: [23641] d_loss: 1.38629150, g_loss: 0.69314098\n",
      "Step: [23642] d_loss: 1.38589644, g_loss: 0.69419056\n",
      "Step: [23643] d_loss: 1.38628352, g_loss: 0.69310999\n",
      "Step: [23644] d_loss: 1.38629365, g_loss: 0.69318020\n",
      "Step: [23645] d_loss: 1.38629317, g_loss: 0.69265276\n",
      "Step: [23646] d_loss: 1.38629484, g_loss: 0.69290257\n",
      "Step: [23647] d_loss: 1.38629675, g_loss: 0.69347036\n",
      "Step: [23648] d_loss: 1.38629341, g_loss: 0.69333100\n",
      "Step: [23649] d_loss: 1.38629317, g_loss: 0.69323963\n",
      "Step: [23650] d_loss: 1.38629174, g_loss: 0.69290900\n",
      "Step: [23651] d_loss: 1.38629115, g_loss: 0.69325620\n",
      "Step: [23652] d_loss: 1.38629842, g_loss: 0.69275469\n",
      "Step: [23653] d_loss: 1.38629746, g_loss: 0.69316924\n",
      "Step: [23654] d_loss: 1.38628221, g_loss: 0.69297045\n",
      "Step: [23655] d_loss: 1.38626027, g_loss: 0.69369823\n",
      "Step: [23656] d_loss: 1.38641286, g_loss: 0.69355804\n",
      "Step: [23657] d_loss: 1.38663983, g_loss: 0.69431829\n",
      "Step: [23658] d_loss: 1.38671565, g_loss: 0.69285482\n",
      "Step: [23659] d_loss: 1.38666189, g_loss: 0.69234884\n",
      "Step: [23660] d_loss: 1.38655198, g_loss: 0.69234991\n",
      "Step: [23661] d_loss: 1.38645363, g_loss: 0.69371057\n",
      "Step: [23662] d_loss: 1.38638186, g_loss: 0.69297838\n",
      "Step: [23663] d_loss: 1.38634324, g_loss: 0.69321686\n",
      "Step: [23664] d_loss: 1.38631737, g_loss: 0.69284952\n",
      "Step: [23665] d_loss: 1.38630366, g_loss: 0.69310760\n",
      "Step: [23666] d_loss: 1.38630557, g_loss: 0.69277835\n",
      "Step: [23667] d_loss: 1.38629389, g_loss: 0.69308996\n",
      "Step: [23668] d_loss: 1.38629401, g_loss: 0.69299632\n",
      "Step: [23669] d_loss: 1.38629305, g_loss: 0.69316673\n",
      "Step: [23670] d_loss: 1.38629067, g_loss: 0.69327652\n",
      "Step: [23671] d_loss: 1.38629353, g_loss: 0.69321477\n",
      "Step: [23672] d_loss: 1.38616872, g_loss: 0.69355297\n",
      "Step: [23673] d_loss: 1.38629246, g_loss: 0.69309127\n",
      "Step: [23674] d_loss: 1.38628936, g_loss: 0.69311202\n",
      "Step: [23675] d_loss: 1.38628864, g_loss: 0.69313711\n",
      "Step: [23676] d_loss: 1.38629234, g_loss: 0.69316638\n",
      "Step: [23677] d_loss: 1.38628805, g_loss: 0.69320327\n",
      "Step: [23678] d_loss: 1.38629234, g_loss: 0.69317710\n",
      "Step: [23679] d_loss: 1.38628852, g_loss: 0.69311738\n",
      "Step: [23680] d_loss: 1.38628793, g_loss: 0.69306576\n",
      "Step: [23681] d_loss: 1.38628864, g_loss: 0.69313455\n",
      "Step: [23682] d_loss: 1.38628948, g_loss: 0.69313407\n",
      "Step: [23683] d_loss: 1.38628864, g_loss: 0.69316202\n",
      "Step: [23684] d_loss: 1.38628626, g_loss: 0.69307375\n",
      "Step: [23685] d_loss: 1.38629234, g_loss: 0.69310248\n",
      "Step: [23686] d_loss: 1.38628983, g_loss: 0.69298959\n",
      "Step: [23687] d_loss: 1.38630068, g_loss: 0.69313192\n",
      "Step: [23688] d_loss: 1.38629043, g_loss: 0.69326115\n",
      "Step: [23689] d_loss: 1.38628936, g_loss: 0.69319975\n",
      "Step: [23690] d_loss: 1.38629127, g_loss: 0.69314796\n",
      "Step: [23691] d_loss: 1.38628840, g_loss: 0.69316024\n",
      "Step: [23692] d_loss: 1.38628662, g_loss: 0.69307750\n",
      "Step: [23693] d_loss: 1.38629413, g_loss: 0.69310510\n",
      "Step: [23694] d_loss: 1.38628650, g_loss: 0.69318324\n",
      "Step: [23695] d_loss: 1.38628936, g_loss: 0.69311345\n",
      "Step: [23696] d_loss: 1.38629150, g_loss: 0.69315958\n",
      "Step: [23697] d_loss: 1.38630867, g_loss: 0.69304550\n",
      "Step: [23698] d_loss: 1.38629341, g_loss: 0.69323862\n",
      "Step: [23699] d_loss: 1.38629007, g_loss: 0.69316071\n",
      "Step: [23700] d_loss: 1.38628912, g_loss: 0.69310349\n",
      "Step: [23701] d_loss: 1.38628173, g_loss: 0.69309831\n",
      "Step: [23702] d_loss: 1.38628817, g_loss: 0.69317442\n",
      "Step: [23703] d_loss: 1.38629222, g_loss: 0.69315636\n",
      "Step: [23704] d_loss: 1.38629830, g_loss: 0.69306773\n",
      "Step: [23705] d_loss: 1.38629222, g_loss: 0.69324064\n",
      "Step: [23706] d_loss: 1.38628769, g_loss: 0.69317096\n",
      "Step: [23707] d_loss: 1.38629460, g_loss: 0.69307905\n",
      "Step: [23708] d_loss: 1.38629365, g_loss: 0.69311672\n",
      "Step: [23709] d_loss: 1.38628650, g_loss: 0.69319022\n",
      "Step: [23710] d_loss: 1.38629282, g_loss: 0.69314766\n",
      "Step: [23711] d_loss: 1.38629031, g_loss: 0.69314027\n",
      "Step: [23712] d_loss: 1.38628423, g_loss: 0.69304514\n",
      "Step: [23713] d_loss: 1.38620400, g_loss: 0.69345236\n",
      "Step: [23714] d_loss: 1.38629436, g_loss: 0.69331610\n",
      "Step: [23715] d_loss: 1.38629174, g_loss: 0.69283831\n",
      "Step: [23716] d_loss: 1.38628721, g_loss: 0.69261980\n",
      "Step: [23717] d_loss: 1.38629961, g_loss: 0.69297898\n",
      "Step: [23718] d_loss: 1.38629198, g_loss: 0.69331813\n",
      "Step: [23719] d_loss: 1.38628960, g_loss: 0.69330782\n",
      "Step: [23720] d_loss: 1.38629079, g_loss: 0.69311482\n",
      "Step: [23721] d_loss: 1.38629150, g_loss: 0.69319749\n",
      "Step: [23722] d_loss: 1.38629055, g_loss: 0.69309568\n",
      "Step: [23723] d_loss: 1.38629448, g_loss: 0.69321507\n",
      "Step: [23724] d_loss: 1.38629007, g_loss: 0.69318277\n",
      "Step: [23725] d_loss: 1.38628840, g_loss: 0.69283354\n",
      "Step: [23726] d_loss: 1.38629317, g_loss: 0.69293809\n",
      "Step: [23727] d_loss: 1.38631403, g_loss: 0.69316673\n",
      "Step: [23728] d_loss: 1.38629198, g_loss: 0.69339472\n",
      "Step: [23729] d_loss: 1.38629270, g_loss: 0.69338876\n",
      "Step: [23730] d_loss: 1.38630366, g_loss: 0.69288552\n",
      "Step: [23731] d_loss: 1.38628912, g_loss: 0.69278812\n",
      "Step: [23732] d_loss: 1.38629389, g_loss: 0.69315726\n",
      "Step: [23733] d_loss: 1.38628399, g_loss: 0.69345576\n",
      "Step: [23734] d_loss: 1.38629341, g_loss: 0.69353420\n",
      "Step: [23735] d_loss: 1.38628602, g_loss: 0.69327259\n",
      "Step: [23736] d_loss: 1.38629103, g_loss: 0.69303882\n",
      "Step: [23737] d_loss: 1.38628507, g_loss: 0.69309908\n",
      "Step: [23738] d_loss: 1.38628936, g_loss: 0.69316620\n",
      "Step: [23739] d_loss: 1.38628697, g_loss: 0.69326848\n",
      "Step: [23740] d_loss: 1.38628876, g_loss: 0.69323546\n",
      "Step: [23741] d_loss: 1.38629222, g_loss: 0.69322366\n",
      "Step: [23742] d_loss: 1.38629436, g_loss: 0.69317383\n",
      "Step: [23743] d_loss: 1.38629103, g_loss: 0.69308817\n",
      "Step: [23744] d_loss: 1.38629043, g_loss: 0.69312984\n",
      "Step: [23745] d_loss: 1.38628817, g_loss: 0.69319433\n",
      "Step: [23746] d_loss: 1.38629150, g_loss: 0.69327569\n",
      "Step: [23747] d_loss: 1.38628614, g_loss: 0.69316566\n",
      "Step: [23748] d_loss: 1.38628757, g_loss: 0.69313216\n",
      "Step: [23749] d_loss: 1.38628817, g_loss: 0.69306237\n",
      "Step: [23750] d_loss: 1.38628900, g_loss: 0.69315177\n",
      "Step: [23751] d_loss: 1.38629007, g_loss: 0.69314742\n",
      "Step: [23752] d_loss: 1.38628674, g_loss: 0.69316280\n",
      "Step: [23753] d_loss: 1.38627958, g_loss: 0.69311833\n",
      "Step: [23754] d_loss: 1.38628674, g_loss: 0.69319755\n",
      "Step: [23755] d_loss: 1.38629293, g_loss: 0.69319141\n",
      "Step: [23756] d_loss: 1.38628662, g_loss: 0.69317818\n",
      "Step: [23757] d_loss: 1.38628602, g_loss: 0.69320709\n",
      "Step: [23758] d_loss: 1.38628864, g_loss: 0.69310570\n",
      "Step: [23759] d_loss: 1.38628566, g_loss: 0.69314492\n",
      "Step: [23760] d_loss: 1.38630414, g_loss: 0.69302547\n",
      "Step: [23761] d_loss: 1.38628972, g_loss: 0.69331759\n",
      "Step: [23762] d_loss: 1.38628840, g_loss: 0.69308674\n",
      "Step: [23763] d_loss: 1.38629246, g_loss: 0.69307321\n",
      "Step: [23764] d_loss: 1.38628864, g_loss: 0.69308871\n",
      "Step: [23765] d_loss: 1.38628125, g_loss: 0.69318843\n",
      "Step: [23766] d_loss: 1.38628936, g_loss: 0.69309294\n",
      "Step: [23767] d_loss: 1.38629127, g_loss: 0.69319057\n",
      "Step: [23768] d_loss: 1.38628829, g_loss: 0.69333029\n",
      "Step: [23769] d_loss: 1.38628221, g_loss: 0.69320548\n",
      "Step: [23770] d_loss: 1.38628292, g_loss: 0.69292587\n",
      "Step: [23771] d_loss: 1.38628840, g_loss: 0.69323301\n",
      "Step: [23772] d_loss: 1.38628781, g_loss: 0.69328207\n",
      "Step: [23773] d_loss: 1.38628888, g_loss: 0.69309592\n",
      "Step: [23774] d_loss: 1.38628840, g_loss: 0.69306028\n",
      "Step: [23775] d_loss: 1.38603091, g_loss: 0.69423944\n",
      "Step: [23776] d_loss: 1.38628411, g_loss: 0.69324076\n",
      "Step: [23777] d_loss: 1.38629198, g_loss: 0.69318771\n",
      "Step: [23778] d_loss: 1.38628852, g_loss: 0.69311482\n",
      "Step: [23779] d_loss: 1.38629079, g_loss: 0.69315624\n",
      "Step: [23780] d_loss: 1.38628900, g_loss: 0.69316477\n",
      "Step: [23781] d_loss: 1.38628995, g_loss: 0.69316733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23782] d_loss: 1.38629198, g_loss: 0.69309074\n",
      "Step: [23783] d_loss: 1.38628912, g_loss: 0.69310075\n",
      "Step: [23784] d_loss: 1.38628888, g_loss: 0.69299817\n",
      "Step: [23785] d_loss: 1.38629174, g_loss: 0.69321215\n",
      "Step: [23786] d_loss: 1.38628936, g_loss: 0.69319963\n",
      "Step: [23787] d_loss: 1.38633132, g_loss: 0.69317937\n",
      "Step: [23788] d_loss: 1.38629711, g_loss: 0.69336396\n",
      "Step: [23789] d_loss: 1.38629508, g_loss: 0.69315147\n",
      "Step: [23790] d_loss: 1.38630116, g_loss: 0.69304448\n",
      "Step: [23791] d_loss: 1.38629663, g_loss: 0.69340086\n",
      "Step: [23792] d_loss: 1.38631797, g_loss: 0.69292533\n",
      "Step: [23793] d_loss: 1.38631487, g_loss: 0.69348186\n",
      "Step: [23794] d_loss: 1.38632894, g_loss: 0.69362223\n",
      "Step: [23795] d_loss: 1.38633943, g_loss: 0.69274634\n",
      "Step: [23796] d_loss: 1.38634682, g_loss: 0.69173503\n",
      "Step: [23797] d_loss: 1.38636863, g_loss: 0.69268471\n",
      "Step: [23798] d_loss: 1.38639808, g_loss: 0.69296992\n",
      "Step: [23799] d_loss: 1.38642633, g_loss: 0.69309354\n",
      "Step: [23800] d_loss: 1.38638067, g_loss: 0.69274604\n",
      "Step: [23801] d_loss: 1.38646531, g_loss: 0.69362020\n",
      "Step: [23802] d_loss: 1.38671970, g_loss: 0.69554698\n",
      "Step: [23803] d_loss: 1.38693953, g_loss: 0.69449890\n",
      "Step: [23804] d_loss: 1.38713455, g_loss: 0.69162923\n",
      "Step: [23805] d_loss: 1.38734794, g_loss: 0.69298089\n",
      "Step: [23806] d_loss: 1.38753581, g_loss: 0.69595480\n",
      "Step: [23807] d_loss: 1.38762808, g_loss: 0.69865155\n",
      "Step: [23808] d_loss: 1.38761830, g_loss: 0.69853544\n",
      "Step: [23809] d_loss: 1.38696218, g_loss: 0.69827676\n",
      "Step: [23810] d_loss: 1.38704896, g_loss: 0.69621968\n",
      "Step: [23811] d_loss: 1.38678312, g_loss: 0.69183743\n",
      "Step: [23812] d_loss: 1.38666785, g_loss: 0.68849385\n",
      "Step: [23813] d_loss: 1.38664317, g_loss: 0.69064647\n",
      "Step: [23814] d_loss: 1.38803315, g_loss: 0.69137019\n",
      "Step: [23815] d_loss: 1.38658285, g_loss: 0.69334352\n",
      "Step: [23816] d_loss: 1.38655353, g_loss: 0.69307196\n",
      "Step: [23817] d_loss: 1.38653564, g_loss: 0.69420153\n",
      "Step: [23818] d_loss: 1.38665271, g_loss: 0.69605285\n",
      "Step: [23819] d_loss: 1.38644862, g_loss: 0.69143313\n",
      "Step: [23820] d_loss: 1.38640177, g_loss: 0.69245839\n",
      "Step: [23821] d_loss: 1.38637209, g_loss: 0.69394624\n",
      "Step: [23822] d_loss: 1.38634205, g_loss: 0.69417173\n",
      "Step: [23823] d_loss: 1.38632464, g_loss: 0.69346535\n",
      "Step: [23824] d_loss: 1.38630712, g_loss: 0.69304347\n",
      "Step: [23825] d_loss: 1.38630104, g_loss: 0.69318855\n",
      "Step: [23826] d_loss: 1.38631475, g_loss: 0.69208264\n",
      "Step: [23827] d_loss: 1.38631165, g_loss: 0.69276905\n",
      "Step: [23828] d_loss: 1.38629699, g_loss: 0.69362992\n",
      "Step: [23829] d_loss: 1.38629758, g_loss: 0.69336724\n",
      "Step: [23830] d_loss: 1.38629556, g_loss: 0.69326353\n",
      "Step: [23831] d_loss: 1.38628769, g_loss: 0.69278842\n",
      "Step: [23832] d_loss: 1.38629460, g_loss: 0.69243985\n",
      "Step: [23833] d_loss: 1.38629401, g_loss: 0.69322717\n",
      "Step: [23834] d_loss: 1.38630056, g_loss: 0.69279659\n",
      "Step: [23835] d_loss: 1.38630295, g_loss: 0.69328558\n",
      "Step: [23836] d_loss: 1.38630891, g_loss: 0.69325185\n",
      "Step: [23837] d_loss: 1.38630629, g_loss: 0.69334382\n",
      "Step: [23838] d_loss: 1.38630652, g_loss: 0.69318587\n",
      "Step: [23839] d_loss: 1.38630247, g_loss: 0.69333541\n",
      "Step: [23840] d_loss: 1.38630319, g_loss: 0.69306540\n",
      "Step: [23841] d_loss: 1.38630104, g_loss: 0.69301784\n",
      "Step: [23842] d_loss: 1.38630319, g_loss: 0.69271767\n",
      "Step: [23843] d_loss: 1.38633442, g_loss: 0.69335473\n",
      "Step: [23844] d_loss: 1.38599253, g_loss: 0.69429302\n",
      "Step: [23845] d_loss: 1.38631654, g_loss: 0.69324172\n",
      "Step: [23846] d_loss: 1.38632119, g_loss: 0.69267380\n",
      "Step: [23847] d_loss: 1.38632393, g_loss: 0.69285178\n",
      "Step: [23848] d_loss: 1.38632607, g_loss: 0.69240355\n",
      "Step: [23849] d_loss: 1.38633227, g_loss: 0.69335300\n",
      "Step: [23850] d_loss: 1.38633513, g_loss: 0.69408613\n",
      "Step: [23851] d_loss: 1.38633204, g_loss: 0.69328308\n",
      "Step: [23852] d_loss: 1.38630652, g_loss: 0.69364083\n",
      "Step: [23853] d_loss: 1.38630188, g_loss: 0.69333601\n",
      "Step: [23854] d_loss: 1.38630092, g_loss: 0.69272548\n",
      "Step: [23855] d_loss: 1.38629186, g_loss: 0.69305736\n",
      "Step: [23856] d_loss: 1.38630092, g_loss: 0.69333315\n",
      "Step: [23857] d_loss: 1.38630056, g_loss: 0.69320083\n",
      "Step: [23858] d_loss: 1.38629818, g_loss: 0.69316721\n",
      "Step: [23859] d_loss: 1.38629794, g_loss: 0.69247001\n",
      "Step: [23860] d_loss: 1.38629639, g_loss: 0.69334024\n",
      "Step: [23861] d_loss: 1.38631463, g_loss: 0.69268799\n",
      "Step: [23862] d_loss: 1.38629746, g_loss: 0.69342601\n",
      "Step: [23863] d_loss: 1.38631237, g_loss: 0.69297624\n",
      "Step: [23864] d_loss: 1.38630795, g_loss: 0.69424033\n",
      "Step: [23865] d_loss: 1.38633895, g_loss: 0.69366378\n",
      "Step: [23866] d_loss: 1.38629627, g_loss: 0.69267297\n",
      "Step: [23867] d_loss: 1.38630319, g_loss: 0.69312209\n",
      "Step: [23868] d_loss: 1.38631725, g_loss: 0.69307673\n",
      "Step: [23869] d_loss: 1.38631821, g_loss: 0.69343311\n",
      "Step: [23870] d_loss: 1.38631809, g_loss: 0.69319701\n",
      "Step: [23871] d_loss: 1.38632846, g_loss: 0.69274855\n",
      "Step: [23872] d_loss: 1.38634408, g_loss: 0.69232106\n",
      "Step: [23873] d_loss: 1.38636029, g_loss: 0.69328916\n",
      "Step: [23874] d_loss: 1.38636351, g_loss: 0.69265139\n",
      "Step: [23875] d_loss: 1.38638115, g_loss: 0.69354534\n",
      "Step: [23876] d_loss: 1.38639975, g_loss: 0.69199842\n",
      "Step: [23877] d_loss: 1.38644171, g_loss: 0.69199395\n",
      "Step: [23878] d_loss: 1.38646615, g_loss: 0.69346189\n",
      "Step: [23879] d_loss: 1.38648534, g_loss: 0.69429433\n",
      "Step: [23880] d_loss: 1.38649631, g_loss: 0.69319284\n",
      "Step: [23881] d_loss: 1.38650751, g_loss: 0.69264168\n",
      "Step: [23882] d_loss: 1.38652289, g_loss: 0.69141924\n",
      "Step: [23883] d_loss: 1.38653648, g_loss: 0.69235545\n",
      "Step: [23884] d_loss: 1.38654327, g_loss: 0.69315743\n",
      "Step: [23885] d_loss: 1.38653541, g_loss: 0.69409275\n",
      "Step: [23886] d_loss: 1.38652289, g_loss: 0.69448280\n",
      "Step: [23887] d_loss: 1.38649750, g_loss: 0.69435769\n",
      "Step: [23888] d_loss: 1.38648546, g_loss: 0.69254601\n",
      "Step: [23889] d_loss: 1.38647985, g_loss: 0.69328177\n",
      "Step: [23890] d_loss: 1.38646770, g_loss: 0.69537473\n",
      "Step: [23891] d_loss: 1.38626313, g_loss: 0.69542444\n",
      "Step: [23892] d_loss: 1.38641238, g_loss: 0.69426799\n",
      "Step: [23893] d_loss: 1.38639951, g_loss: 0.69381267\n",
      "Step: [23894] d_loss: 1.38639271, g_loss: 0.69301438\n",
      "Step: [23895] d_loss: 1.38637090, g_loss: 0.69219834\n",
      "Step: [23896] d_loss: 1.38637638, g_loss: 0.69152415\n",
      "Step: [23897] d_loss: 1.38637686, g_loss: 0.69307095\n",
      "Step: [23898] d_loss: 1.38637638, g_loss: 0.69432992\n",
      "Step: [23899] d_loss: 1.38635206, g_loss: 0.69430208\n",
      "Step: [23900] d_loss: 1.38633728, g_loss: 0.69378018\n",
      "Step: [23901] d_loss: 1.38634145, g_loss: 0.69331717\n",
      "Step: [23902] d_loss: 1.38632953, g_loss: 0.69352663\n",
      "Step: [23903] d_loss: 1.38632119, g_loss: 0.69258547\n",
      "Step: [23904] d_loss: 1.38631797, g_loss: 0.69246721\n",
      "Step: [23905] d_loss: 1.38631845, g_loss: 0.69310373\n",
      "Step: [23906] d_loss: 1.38630486, g_loss: 0.69409418\n",
      "Step: [23907] d_loss: 1.38629973, g_loss: 0.69331032\n",
      "Step: [23908] d_loss: 1.38628876, g_loss: 0.69340611\n",
      "Step: [23909] d_loss: 1.38628817, g_loss: 0.69294918\n",
      "Step: [23910] d_loss: 1.38629460, g_loss: 0.69272071\n",
      "Step: [23911] d_loss: 1.38628888, g_loss: 0.69324327\n",
      "Step: [23912] d_loss: 1.38628960, g_loss: 0.69310308\n",
      "Step: [23913] d_loss: 1.38629162, g_loss: 0.69310606\n",
      "Step: [23914] d_loss: 1.38629174, g_loss: 0.69343710\n",
      "Step: [23915] d_loss: 1.38628316, g_loss: 0.69288570\n",
      "Step: [23916] d_loss: 1.38627028, g_loss: 0.69298506\n",
      "Step: [23917] d_loss: 1.38635182, g_loss: 0.69360590\n",
      "Step: [23918] d_loss: 1.38659585, g_loss: 0.69268191\n",
      "Step: [23919] d_loss: 1.38679397, g_loss: 0.69492358\n",
      "Step: [23920] d_loss: 1.38693833, g_loss: 0.69359767\n",
      "Step: [23921] d_loss: 1.38711095, g_loss: 0.68993658\n",
      "Step: [23922] d_loss: 1.38726664, g_loss: 0.68791211\n",
      "Step: [23923] d_loss: 1.38734210, g_loss: 0.69158447\n",
      "Step: [23924] d_loss: 1.38726592, g_loss: 0.69561303\n",
      "Step: [23925] d_loss: 1.38715553, g_loss: 0.69568622\n",
      "Step: [23926] d_loss: 1.38703012, g_loss: 0.69241095\n",
      "Step: [23927] d_loss: 1.38690507, g_loss: 0.69497865\n",
      "Step: [23928] d_loss: 1.38683605, g_loss: 0.69277167\n",
      "Step: [23929] d_loss: 1.38672805, g_loss: 0.69102544\n",
      "Step: [23930] d_loss: 1.38665438, g_loss: 0.69214785\n",
      "Step: [23931] d_loss: 1.38656044, g_loss: 0.69362926\n",
      "Step: [23932] d_loss: 1.38648987, g_loss: 0.69240320\n",
      "Step: [23933] d_loss: 1.38643920, g_loss: 0.69321901\n",
      "Step: [23934] d_loss: 1.38639760, g_loss: 0.69242966\n",
      "Step: [23935] d_loss: 1.38637197, g_loss: 0.69326895\n",
      "Step: [23936] d_loss: 1.38634551, g_loss: 0.69304031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [23937] d_loss: 1.38632655, g_loss: 0.69274038\n",
      "Step: [23938] d_loss: 1.38631797, g_loss: 0.69314975\n",
      "Step: [23939] d_loss: 1.38630915, g_loss: 0.69334543\n",
      "Step: [23940] d_loss: 1.38629866, g_loss: 0.69366139\n",
      "Step: [23941] d_loss: 1.38629961, g_loss: 0.69329858\n",
      "Step: [23942] d_loss: 1.38629603, g_loss: 0.69305027\n",
      "Step: [23943] d_loss: 1.38629675, g_loss: 0.69301438\n",
      "Step: [23944] d_loss: 1.38629103, g_loss: 0.69297868\n",
      "Step: [23945] d_loss: 1.38600934, g_loss: 0.69468415\n",
      "Step: [23946] d_loss: 1.38629413, g_loss: 0.69327664\n",
      "Step: [23947] d_loss: 1.38628733, g_loss: 0.69314778\n",
      "Step: [23948] d_loss: 1.38629663, g_loss: 0.69313550\n",
      "Step: [23949] d_loss: 1.38628793, g_loss: 0.69307810\n",
      "Step: [23950] d_loss: 1.38628900, g_loss: 0.69317985\n",
      "Step: [23951] d_loss: 1.38629758, g_loss: 0.69309902\n",
      "Step: [23952] d_loss: 1.38628721, g_loss: 0.69304746\n",
      "Step: [23953] d_loss: 1.38628709, g_loss: 0.69311523\n",
      "Step: [23954] d_loss: 1.38629043, g_loss: 0.69321322\n",
      "Step: [23955] d_loss: 1.38633990, g_loss: 0.69258630\n",
      "Step: [23956] d_loss: 1.38628709, g_loss: 0.69327176\n",
      "Step: [23957] d_loss: 1.38629580, g_loss: 0.69304192\n",
      "Step: [23958] d_loss: 1.38629818, g_loss: 0.69314599\n",
      "Step: [23959] d_loss: 1.38628244, g_loss: 0.69307876\n",
      "Step: [23960] d_loss: 1.38628697, g_loss: 0.69308519\n",
      "Step: [23961] d_loss: 1.38629019, g_loss: 0.69310749\n",
      "Step: [23962] d_loss: 1.38629246, g_loss: 0.69287181\n",
      "Step: [23963] d_loss: 1.38628280, g_loss: 0.69318044\n",
      "Step: [23964] d_loss: 1.38629258, g_loss: 0.69334233\n",
      "Step: [23965] d_loss: 1.38629019, g_loss: 0.69327021\n",
      "Step: [23966] d_loss: 1.38629711, g_loss: 0.69314730\n",
      "Step: [23967] d_loss: 1.38628983, g_loss: 0.69307417\n",
      "Step: [23968] d_loss: 1.38643610, g_loss: 0.69420034\n",
      "Step: [23969] d_loss: 1.38629055, g_loss: 0.69285041\n",
      "Step: [23970] d_loss: 1.38630080, g_loss: 0.69331765\n",
      "Step: [23971] d_loss: 1.38630366, g_loss: 0.69352961\n",
      "Step: [23972] d_loss: 1.38629365, g_loss: 0.69352090\n",
      "Step: [23973] d_loss: 1.38639712, g_loss: 0.69393426\n",
      "Step: [23974] d_loss: 1.38729811, g_loss: 0.69179887\n",
      "Step: [23975] d_loss: 1.38820481, g_loss: 0.69415772\n",
      "Step: [23976] d_loss: 1.38874340, g_loss: 0.69188708\n",
      "Step: [23977] d_loss: 1.38833332, g_loss: 0.69806147\n",
      "Step: [23978] d_loss: 1.38810468, g_loss: 0.69311070\n",
      "Step: [23979] d_loss: 1.38753724, g_loss: 0.69074082\n",
      "Step: [23980] d_loss: 1.38700855, g_loss: 0.69310057\n",
      "Step: [23981] d_loss: 1.38662541, g_loss: 0.69525409\n",
      "Step: [23982] d_loss: 1.38643098, g_loss: 0.69434619\n",
      "Step: [23983] d_loss: 1.38633180, g_loss: 0.69311422\n",
      "Step: [23984] d_loss: 1.38629639, g_loss: 0.69245875\n",
      "Step: [23985] d_loss: 1.38629317, g_loss: 0.69272876\n",
      "Step: [23986] d_loss: 1.38628793, g_loss: 0.69316292\n",
      "Step: [23987] d_loss: 1.38629484, g_loss: 0.69341874\n",
      "Step: [23988] d_loss: 1.38629019, g_loss: 0.69327688\n",
      "Step: [23989] d_loss: 1.38629615, g_loss: 0.69318426\n",
      "Step: [23990] d_loss: 1.38629651, g_loss: 0.69285679\n",
      "Step: [23991] d_loss: 1.38628459, g_loss: 0.69293809\n",
      "Step: [23992] d_loss: 1.38629675, g_loss: 0.69307077\n",
      "Step: [23993] d_loss: 1.38629222, g_loss: 0.69323742\n",
      "Step: [23994] d_loss: 1.38631201, g_loss: 0.69283307\n",
      "Step: [23995] d_loss: 1.38629222, g_loss: 0.69319063\n",
      "Step: [23996] d_loss: 1.38629222, g_loss: 0.69325376\n",
      "Step: [23997] d_loss: 1.38629341, g_loss: 0.69324052\n",
      "Step: [23998] d_loss: 1.38628268, g_loss: 0.69315505\n",
      "Step: [23999] d_loss: 1.38641596, g_loss: 0.69321632\n",
      "Step: [24000] d_loss: 1.38630998, g_loss: 0.69302583\n",
      "Step: [24001] d_loss: 1.38618159, g_loss: 0.69268620\n",
      "Step: [24002] d_loss: 1.38629794, g_loss: 0.69352245\n",
      "Step: [24003] d_loss: 1.38629365, g_loss: 0.69326705\n",
      "Step: [24004] d_loss: 1.38653469, g_loss: 0.69466829\n",
      "Step: [24005] d_loss: 1.38633120, g_loss: 0.69276273\n",
      "Step: [24006] d_loss: 1.38630366, g_loss: 0.69297528\n",
      "Step: [24007] d_loss: 1.38629544, g_loss: 0.69318640\n",
      "Step: [24008] d_loss: 1.38628745, g_loss: 0.69330680\n",
      "Step: [24009] d_loss: 1.38629580, g_loss: 0.69323760\n",
      "Step: [24010] d_loss: 1.38629556, g_loss: 0.69316161\n",
      "Step: [24011] d_loss: 1.38629496, g_loss: 0.69314766\n",
      "Step: [24012] d_loss: 1.38625479, g_loss: 0.69245243\n",
      "Step: [24013] d_loss: 1.38629317, g_loss: 0.69326735\n",
      "Step: [24014] d_loss: 1.38628983, g_loss: 0.69319856\n",
      "Step: [24015] d_loss: 1.38628864, g_loss: 0.69314635\n",
      "Step: [24016] d_loss: 1.38629210, g_loss: 0.69317394\n",
      "Step: [24017] d_loss: 1.38629556, g_loss: 0.69316304\n",
      "Step: [24018] d_loss: 1.38680947, g_loss: 0.69309700\n",
      "Step: [24019] d_loss: 1.38628507, g_loss: 0.69277608\n",
      "Step: [24020] d_loss: 1.38630140, g_loss: 0.69300628\n",
      "Step: [24021] d_loss: 1.38634109, g_loss: 0.69262373\n",
      "Step: [24022] d_loss: 1.38628232, g_loss: 0.69365859\n",
      "Step: [24023] d_loss: 1.38631725, g_loss: 0.69347918\n",
      "Step: [24024] d_loss: 1.38637221, g_loss: 0.69265771\n",
      "Step: [24025] d_loss: 1.38638496, g_loss: 0.69289017\n",
      "Step: [24026] d_loss: 1.38637567, g_loss: 0.69390011\n",
      "Step: [24027] d_loss: 1.38635039, g_loss: 0.69315928\n",
      "Step: [24028] d_loss: 1.38654685, g_loss: 0.69473946\n",
      "Step: [24029] d_loss: 1.38633871, g_loss: 0.69365370\n",
      "Step: [24030] d_loss: 1.38632679, g_loss: 0.69335920\n",
      "Step: [24031] d_loss: 1.38631749, g_loss: 0.69284403\n",
      "Step: [24032] d_loss: 1.38630795, g_loss: 0.69307578\n",
      "Step: [24033] d_loss: 1.38629830, g_loss: 0.69302046\n",
      "Step: [24034] d_loss: 1.38630402, g_loss: 0.69276524\n",
      "Step: [24035] d_loss: 1.38630033, g_loss: 0.69286370\n",
      "Step: [24036] d_loss: 1.38629854, g_loss: 0.69319755\n",
      "Step: [24037] d_loss: 1.38629699, g_loss: 0.69319874\n",
      "Step: [24038] d_loss: 1.38629436, g_loss: 0.69319236\n",
      "Step: [24039] d_loss: 1.38610959, g_loss: 0.69371915\n",
      "Step: [24040] d_loss: 1.38629091, g_loss: 0.69328731\n",
      "Step: [24041] d_loss: 1.38628709, g_loss: 0.69415677\n",
      "Step: [24042] d_loss: 1.38642001, g_loss: 0.69352674\n",
      "Step: [24043] d_loss: 1.38663685, g_loss: 0.69330478\n",
      "Step: [24044] d_loss: 1.38672209, g_loss: 0.69446528\n",
      "Step: [24045] d_loss: 1.38667870, g_loss: 0.69457126\n",
      "Step: [24046] d_loss: 1.38656819, g_loss: 0.69477928\n",
      "Step: [24047] d_loss: 1.38646126, g_loss: 0.69317257\n",
      "Step: [24048] d_loss: 1.38642502, g_loss: 0.69244087\n",
      "Step: [24049] d_loss: 1.38637519, g_loss: 0.69372582\n",
      "Step: [24050] d_loss: 1.38634515, g_loss: 0.69355142\n",
      "Step: [24051] d_loss: 1.38630486, g_loss: 0.69260871\n",
      "Step: [24052] d_loss: 1.38632321, g_loss: 0.69305098\n",
      "Step: [24053] d_loss: 1.38630819, g_loss: 0.69288540\n",
      "Step: [24054] d_loss: 1.38627481, g_loss: 0.69315237\n",
      "Step: [24055] d_loss: 1.38630223, g_loss: 0.69361818\n",
      "Step: [24056] d_loss: 1.38628864, g_loss: 0.69319385\n",
      "Step: [24057] d_loss: 1.38628125, g_loss: 0.69293451\n",
      "Step: [24058] d_loss: 1.38626647, g_loss: 0.69309932\n",
      "Step: [24059] d_loss: 1.38627636, g_loss: 0.69321233\n",
      "Step: [24060] d_loss: 1.38629985, g_loss: 0.69248438\n",
      "Step: [24061] d_loss: 1.38630867, g_loss: 0.69229096\n",
      "Step: [24062] d_loss: 1.38631260, g_loss: 0.69290602\n",
      "Step: [24063] d_loss: 1.38631737, g_loss: 0.69287646\n",
      "Step: [24064] d_loss: 1.38631272, g_loss: 0.69346958\n",
      "Step: [24065] d_loss: 1.38631868, g_loss: 0.69342011\n",
      "Step: [24066] d_loss: 1.38631439, g_loss: 0.69327897\n",
      "Step: [24067] d_loss: 1.38630581, g_loss: 0.69305563\n",
      "Step: [24068] d_loss: 1.38631344, g_loss: 0.69271189\n",
      "Step: [24069] d_loss: 1.38630319, g_loss: 0.69284838\n",
      "Step: [24070] d_loss: 1.38627923, g_loss: 0.69315040\n",
      "Step: [24071] d_loss: 1.38630342, g_loss: 0.69369686\n",
      "Step: [24072] d_loss: 1.38628793, g_loss: 0.69323742\n",
      "Step: [24073] d_loss: 1.38633847, g_loss: 0.69319594\n",
      "Step: [24074] d_loss: 1.38629758, g_loss: 0.69342387\n",
      "Step: [24075] d_loss: 1.38627315, g_loss: 0.69292456\n",
      "Step: [24076] d_loss: 1.38629425, g_loss: 0.69312549\n",
      "Step: [24077] d_loss: 1.38629854, g_loss: 0.69295931\n",
      "Step: [24078] d_loss: 1.38630033, g_loss: 0.69274908\n",
      "Step: [24079] d_loss: 1.38630223, g_loss: 0.69280827\n",
      "Step: [24080] d_loss: 1.38631403, g_loss: 0.69298947\n",
      "Step: [24081] d_loss: 1.38630152, g_loss: 0.69290841\n",
      "Step: [24082] d_loss: 1.38630819, g_loss: 0.69315130\n",
      "Step: [24083] d_loss: 1.38644242, g_loss: 0.69188178\n",
      "Step: [24084] d_loss: 1.38630700, g_loss: 0.69326413\n",
      "Step: [24085] d_loss: 1.38630033, g_loss: 0.69315368\n",
      "Step: [24086] d_loss: 1.38632011, g_loss: 0.69282478\n",
      "Step: [24087] d_loss: 1.38631332, g_loss: 0.69377881\n",
      "Step: [24088] d_loss: 1.38630462, g_loss: 0.69312507\n",
      "Step: [24089] d_loss: 1.38630509, g_loss: 0.69270039\n",
      "Step: [24090] d_loss: 1.38631296, g_loss: 0.69308543\n",
      "Step: [24091] d_loss: 1.38632894, g_loss: 0.69229829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24092] d_loss: 1.38631070, g_loss: 0.69297081\n",
      "Step: [24093] d_loss: 1.38630509, g_loss: 0.69275999\n",
      "Step: [24094] d_loss: 1.38668633, g_loss: 0.69350481\n",
      "Step: [24095] d_loss: 1.38640571, g_loss: 0.69322777\n",
      "Step: [24096] d_loss: 1.38656759, g_loss: 0.69225156\n",
      "Step: [24097] d_loss: 1.38666534, g_loss: 0.69164109\n",
      "Step: [24098] d_loss: 1.38668525, g_loss: 0.69397295\n",
      "Step: [24099] d_loss: 1.38668287, g_loss: 0.69501835\n",
      "Step: [24100] d_loss: 1.38663816, g_loss: 0.69440198\n",
      "Step: [24101] d_loss: 1.38658309, g_loss: 0.69444132\n",
      "Step: [24102] d_loss: 1.38650537, g_loss: 0.69365197\n",
      "Step: [24103] d_loss: 1.38645923, g_loss: 0.69208157\n",
      "Step: [24104] d_loss: 1.38642812, g_loss: 0.69088972\n",
      "Step: [24105] d_loss: 1.38636851, g_loss: 0.69110000\n",
      "Step: [24106] d_loss: 1.38642299, g_loss: 0.69258177\n",
      "Step: [24107] d_loss: 1.38635361, g_loss: 0.69372547\n",
      "Step: [24108] d_loss: 1.38634551, g_loss: 0.69397378\n",
      "Step: [24109] d_loss: 1.38633871, g_loss: 0.69302821\n",
      "Step: [24110] d_loss: 1.38633633, g_loss: 0.69274497\n",
      "Step: [24111] d_loss: 1.38632739, g_loss: 0.69307977\n",
      "Step: [24112] d_loss: 1.38631821, g_loss: 0.69359887\n",
      "Step: [24113] d_loss: 1.38631320, g_loss: 0.69338095\n",
      "Step: [24114] d_loss: 1.38631678, g_loss: 0.69316769\n",
      "Step: [24115] d_loss: 1.38630986, g_loss: 0.69303799\n",
      "Step: [24116] d_loss: 1.38629436, g_loss: 0.69309878\n",
      "Step: [24117] d_loss: 1.38632596, g_loss: 0.69357049\n",
      "Step: [24118] d_loss: 1.38646841, g_loss: 0.69230056\n",
      "Step: [24119] d_loss: 1.38660753, g_loss: 0.69313288\n",
      "Step: [24120] d_loss: 1.38665164, g_loss: 0.69231188\n",
      "Step: [24121] d_loss: 1.38664818, g_loss: 0.69390190\n",
      "Step: [24122] d_loss: 1.38662148, g_loss: 0.69530475\n",
      "Step: [24123] d_loss: 1.38656950, g_loss: 0.69502747\n",
      "Step: [24124] d_loss: 1.38651383, g_loss: 0.69335365\n",
      "Step: [24125] d_loss: 1.38645172, g_loss: 0.69336331\n",
      "Step: [24126] d_loss: 1.38639593, g_loss: 0.69172502\n",
      "Step: [24127] d_loss: 1.38638628, g_loss: 0.69082963\n",
      "Step: [24128] d_loss: 1.38636291, g_loss: 0.69150621\n",
      "Step: [24129] d_loss: 1.38633335, g_loss: 0.69289148\n",
      "Step: [24130] d_loss: 1.38630652, g_loss: 0.69385207\n",
      "Step: [24131] d_loss: 1.38630283, g_loss: 0.69351983\n",
      "Step: [24132] d_loss: 1.38627338, g_loss: 0.69275719\n",
      "Step: [24133] d_loss: 1.38629222, g_loss: 0.69288945\n",
      "Step: [24134] d_loss: 1.38629758, g_loss: 0.69303656\n",
      "Step: [24135] d_loss: 1.38629496, g_loss: 0.69319385\n",
      "Step: [24136] d_loss: 1.38629115, g_loss: 0.69318402\n",
      "Step: [24137] d_loss: 1.38629150, g_loss: 0.69306624\n",
      "Step: [24138] d_loss: 1.38627434, g_loss: 0.69325072\n",
      "Step: [24139] d_loss: 1.38630486, g_loss: 0.69325900\n",
      "Step: [24140] d_loss: 1.38629997, g_loss: 0.69297993\n",
      "Step: [24141] d_loss: 1.38630176, g_loss: 0.69296265\n",
      "Step: [24142] d_loss: 1.38631141, g_loss: 0.69333643\n",
      "Step: [24143] d_loss: 1.38630295, g_loss: 0.69333917\n",
      "Step: [24144] d_loss: 1.38630974, g_loss: 0.69306415\n",
      "Step: [24145] d_loss: 1.38630819, g_loss: 0.69287217\n",
      "Step: [24146] d_loss: 1.38632154, g_loss: 0.69300151\n",
      "Step: [24147] d_loss: 1.38630474, g_loss: 0.69345510\n",
      "Step: [24148] d_loss: 1.38630712, g_loss: 0.69359827\n",
      "Step: [24149] d_loss: 1.38630915, g_loss: 0.69311810\n",
      "Step: [24150] d_loss: 1.38631058, g_loss: 0.69297814\n",
      "Step: [24151] d_loss: 1.38631213, g_loss: 0.69299901\n",
      "Step: [24152] d_loss: 1.38630927, g_loss: 0.69321406\n",
      "Step: [24153] d_loss: 1.38631010, g_loss: 0.69333243\n",
      "Step: [24154] d_loss: 1.38630795, g_loss: 0.69318622\n",
      "Step: [24155] d_loss: 1.38628590, g_loss: 0.69285703\n",
      "Step: [24156] d_loss: 1.38629150, g_loss: 0.69264042\n",
      "Step: [24157] d_loss: 1.38632703, g_loss: 0.69269109\n",
      "Step: [24158] d_loss: 1.38679910, g_loss: 0.69241613\n",
      "Step: [24159] d_loss: 1.38744807, g_loss: 0.69560611\n",
      "Step: [24160] d_loss: 1.38772559, g_loss: 0.69201851\n",
      "Step: [24161] d_loss: 1.38775992, g_loss: 0.69145882\n",
      "Step: [24162] d_loss: 1.38748431, g_loss: 0.69077075\n",
      "Step: [24163] d_loss: 1.38712811, g_loss: 0.69253230\n",
      "Step: [24164] d_loss: 1.38682818, g_loss: 0.69500339\n",
      "Step: [24165] d_loss: 1.38661432, g_loss: 0.69683087\n",
      "Step: [24166] d_loss: 1.38651705, g_loss: 0.69458002\n",
      "Step: [24167] d_loss: 1.38643134, g_loss: 0.69333661\n",
      "Step: [24168] d_loss: 1.38637614, g_loss: 0.69212675\n",
      "Step: [24169] d_loss: 1.38671243, g_loss: 0.69443226\n",
      "Step: [24170] d_loss: 1.38635564, g_loss: 0.69274855\n",
      "Step: [24171] d_loss: 1.38634586, g_loss: 0.69288999\n",
      "Step: [24172] d_loss: 1.38633204, g_loss: 0.69306147\n",
      "Step: [24173] d_loss: 1.38630664, g_loss: 0.69318420\n",
      "Step: [24174] d_loss: 1.38630116, g_loss: 0.69312835\n",
      "Step: [24175] d_loss: 1.38629830, g_loss: 0.69295430\n",
      "Step: [24176] d_loss: 1.38629174, g_loss: 0.69319445\n",
      "Step: [24177] d_loss: 1.38629425, g_loss: 0.69341624\n",
      "Step: [24178] d_loss: 1.38632834, g_loss: 0.69318587\n",
      "Step: [24179] d_loss: 1.38628662, g_loss: 0.69297546\n",
      "Step: [24180] d_loss: 1.38629222, g_loss: 0.69304699\n",
      "Step: [24181] d_loss: 1.38629508, g_loss: 0.69311988\n",
      "Step: [24182] d_loss: 1.38628983, g_loss: 0.69323361\n",
      "Step: [24183] d_loss: 1.38629484, g_loss: 0.69331908\n",
      "Step: [24184] d_loss: 1.38628876, g_loss: 0.69332957\n",
      "Step: [24185] d_loss: 1.38625669, g_loss: 0.69287229\n",
      "Step: [24186] d_loss: 1.38629234, g_loss: 0.69300032\n",
      "Step: [24187] d_loss: 1.38624418, g_loss: 0.69310099\n",
      "Step: [24188] d_loss: 1.38628149, g_loss: 0.69314396\n",
      "Step: [24189] d_loss: 1.38626850, g_loss: 0.69258451\n",
      "Step: [24190] d_loss: 1.38627815, g_loss: 0.69311696\n",
      "Step: [24191] d_loss: 1.38626099, g_loss: 0.69292110\n",
      "Step: [24192] d_loss: 1.38647306, g_loss: 0.69374263\n",
      "Step: [24193] d_loss: 1.38696194, g_loss: 0.69573390\n",
      "Step: [24194] d_loss: 1.38759804, g_loss: 0.69467354\n",
      "Step: [24195] d_loss: 1.38770199, g_loss: 0.69383985\n",
      "Step: [24196] d_loss: 1.38742959, g_loss: 0.69108111\n",
      "Step: [24197] d_loss: 1.38741410, g_loss: 0.69489610\n",
      "Step: [24198] d_loss: 1.38684189, g_loss: 0.69310558\n",
      "Step: [24199] d_loss: 1.38665164, g_loss: 0.69364923\n",
      "Step: [24200] d_loss: 1.38649237, g_loss: 0.69276720\n",
      "Step: [24201] d_loss: 1.38638186, g_loss: 0.69238043\n",
      "Step: [24202] d_loss: 1.38632262, g_loss: 0.69337797\n",
      "Step: [24203] d_loss: 1.38630164, g_loss: 0.69373691\n",
      "Step: [24204] d_loss: 1.38629484, g_loss: 0.69341314\n",
      "Step: [24205] d_loss: 1.38629353, g_loss: 0.69318068\n",
      "Step: [24206] d_loss: 1.38624382, g_loss: 0.69289666\n",
      "Step: [24207] d_loss: 1.38629174, g_loss: 0.69311869\n",
      "Step: [24208] d_loss: 1.38629246, g_loss: 0.69338083\n",
      "Step: [24209] d_loss: 1.38630545, g_loss: 0.69319445\n",
      "Step: [24210] d_loss: 1.38629985, g_loss: 0.69314724\n",
      "Step: [24211] d_loss: 1.38629639, g_loss: 0.69321769\n",
      "Step: [24212] d_loss: 1.38629484, g_loss: 0.69341105\n",
      "Step: [24213] d_loss: 1.38629031, g_loss: 0.69319481\n",
      "Step: [24214] d_loss: 1.38629675, g_loss: 0.69312602\n",
      "Step: [24215] d_loss: 1.38629305, g_loss: 0.69313586\n",
      "Step: [24216] d_loss: 1.38629246, g_loss: 0.69309115\n",
      "Step: [24217] d_loss: 1.38629436, g_loss: 0.69316709\n",
      "Step: [24218] d_loss: 1.38628936, g_loss: 0.69308448\n",
      "Step: [24219] d_loss: 1.38629532, g_loss: 0.69308722\n",
      "Step: [24220] d_loss: 1.38629508, g_loss: 0.69322807\n",
      "Step: [24221] d_loss: 1.38628960, g_loss: 0.69321203\n",
      "Step: [24222] d_loss: 1.38628960, g_loss: 0.69317198\n",
      "Step: [24223] d_loss: 1.38629484, g_loss: 0.69311583\n",
      "Step: [24224] d_loss: 1.38634157, g_loss: 0.69395107\n",
      "Step: [24225] d_loss: 1.38629270, g_loss: 0.69290566\n",
      "Step: [24226] d_loss: 1.38629723, g_loss: 0.69309270\n",
      "Step: [24227] d_loss: 1.38629627, g_loss: 0.69339138\n",
      "Step: [24228] d_loss: 1.38629842, g_loss: 0.69337291\n",
      "Step: [24229] d_loss: 1.38629711, g_loss: 0.69319737\n",
      "Step: [24230] d_loss: 1.38629043, g_loss: 0.69309676\n",
      "Step: [24231] d_loss: 1.38629425, g_loss: 0.69302344\n",
      "Step: [24232] d_loss: 1.38629532, g_loss: 0.69343066\n",
      "Step: [24233] d_loss: 1.38632464, g_loss: 0.69328916\n",
      "Step: [24234] d_loss: 1.38629365, g_loss: 0.69337177\n",
      "Step: [24235] d_loss: 1.38628864, g_loss: 0.69324064\n",
      "Step: [24236] d_loss: 1.38629973, g_loss: 0.69313258\n",
      "Step: [24237] d_loss: 1.38659000, g_loss: 0.69365335\n",
      "Step: [24238] d_loss: 1.38629603, g_loss: 0.69280314\n",
      "Step: [24239] d_loss: 1.38627923, g_loss: 0.69354719\n",
      "Step: [24240] d_loss: 1.38630795, g_loss: 0.69356394\n",
      "Step: [24241] d_loss: 1.38630867, g_loss: 0.69269645\n",
      "Step: [24242] d_loss: 1.38630295, g_loss: 0.69279033\n",
      "Step: [24243] d_loss: 1.38629663, g_loss: 0.69297242\n",
      "Step: [24244] d_loss: 1.38629484, g_loss: 0.69352555\n",
      "Step: [24245] d_loss: 1.38629663, g_loss: 0.69359589\n",
      "Step: [24246] d_loss: 1.38628638, g_loss: 0.69317329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24247] d_loss: 1.38629305, g_loss: 0.69288099\n",
      "Step: [24248] d_loss: 1.38628948, g_loss: 0.69298923\n",
      "Step: [24249] d_loss: 1.38629079, g_loss: 0.69316167\n",
      "Step: [24250] d_loss: 1.38629270, g_loss: 0.69328463\n",
      "Step: [24251] d_loss: 1.38626993, g_loss: 0.69338620\n",
      "Step: [24252] d_loss: 1.38628399, g_loss: 0.69315219\n",
      "Step: [24253] d_loss: 1.38628983, g_loss: 0.69299555\n",
      "Step: [24254] d_loss: 1.38629460, g_loss: 0.69305402\n",
      "Step: [24255] d_loss: 1.38628745, g_loss: 0.69313180\n",
      "Step: [24256] d_loss: 1.38629270, g_loss: 0.69318151\n",
      "Step: [24257] d_loss: 1.38629198, g_loss: 0.69318110\n",
      "Step: [24258] d_loss: 1.38629222, g_loss: 0.69315434\n",
      "Step: [24259] d_loss: 1.38629293, g_loss: 0.69304276\n",
      "Step: [24260] d_loss: 1.38629079, g_loss: 0.69315791\n",
      "Step: [24261] d_loss: 1.38628983, g_loss: 0.69313729\n",
      "Step: [24262] d_loss: 1.38627529, g_loss: 0.69360197\n",
      "Step: [24263] d_loss: 1.38641632, g_loss: 0.69421953\n",
      "Step: [24264] d_loss: 1.38686562, g_loss: 0.69175714\n",
      "Step: [24265] d_loss: 1.38707900, g_loss: 0.69238740\n",
      "Step: [24266] d_loss: 1.38709581, g_loss: 0.69280797\n",
      "Step: [24267] d_loss: 1.38695502, g_loss: 0.69200826\n",
      "Step: [24268] d_loss: 1.38675833, g_loss: 0.69200087\n",
      "Step: [24269] d_loss: 1.38658857, g_loss: 0.69300163\n",
      "Step: [24270] d_loss: 1.38646984, g_loss: 0.69414413\n",
      "Step: [24271] d_loss: 1.38639784, g_loss: 0.69368207\n",
      "Step: [24272] d_loss: 1.38634658, g_loss: 0.69318390\n",
      "Step: [24273] d_loss: 1.38632488, g_loss: 0.69301701\n",
      "Step: [24274] d_loss: 1.38631368, g_loss: 0.69328368\n",
      "Step: [24275] d_loss: 1.38630211, g_loss: 0.69361818\n",
      "Step: [24276] d_loss: 1.38628089, g_loss: 0.69321620\n",
      "Step: [24277] d_loss: 1.38629818, g_loss: 0.69275475\n",
      "Step: [24278] d_loss: 1.38629198, g_loss: 0.69310510\n",
      "Step: [24279] d_loss: 1.38629317, g_loss: 0.69339430\n",
      "Step: [24280] d_loss: 1.38628840, g_loss: 0.69347584\n",
      "Step: [24281] d_loss: 1.38629258, g_loss: 0.69308269\n",
      "Step: [24282] d_loss: 1.38629162, g_loss: 0.69300765\n",
      "Step: [24283] d_loss: 1.38628983, g_loss: 0.69339216\n",
      "Step: [24284] d_loss: 1.38629794, g_loss: 0.69368273\n",
      "Step: [24285] d_loss: 1.38630366, g_loss: 0.69317782\n",
      "Step: [24286] d_loss: 1.38629484, g_loss: 0.69253111\n",
      "Step: [24287] d_loss: 1.38628554, g_loss: 0.69301599\n",
      "Step: [24288] d_loss: 1.38628912, g_loss: 0.69330984\n",
      "Step: [24289] d_loss: 1.38629723, g_loss: 0.69376588\n",
      "Step: [24290] d_loss: 1.38630402, g_loss: 0.69379413\n",
      "Step: [24291] d_loss: 1.38630009, g_loss: 0.69418019\n",
      "Step: [24292] d_loss: 1.38632524, g_loss: 0.69307357\n",
      "Step: [24293] d_loss: 1.38633204, g_loss: 0.69303286\n",
      "Step: [24294] d_loss: 1.38632488, g_loss: 0.69284338\n",
      "Step: [24295] d_loss: 1.38631248, g_loss: 0.69361615\n",
      "Step: [24296] d_loss: 1.38632107, g_loss: 0.69349068\n",
      "Step: [24297] d_loss: 1.38629699, g_loss: 0.69396418\n",
      "Step: [24298] d_loss: 1.38632464, g_loss: 0.69342875\n",
      "Step: [24299] d_loss: 1.38632441, g_loss: 0.69318646\n",
      "Step: [24300] d_loss: 1.38631535, g_loss: 0.69276851\n",
      "Step: [24301] d_loss: 1.38631964, g_loss: 0.69295311\n",
      "Step: [24302] d_loss: 1.38629937, g_loss: 0.69313133\n",
      "Step: [24303] d_loss: 1.38626945, g_loss: 0.69434768\n",
      "Step: [24304] d_loss: 1.38638604, g_loss: 0.69274747\n",
      "Step: [24305] d_loss: 1.38632858, g_loss: 0.69331598\n",
      "Step: [24306] d_loss: 1.38633835, g_loss: 0.69364321\n",
      "Step: [24307] d_loss: 1.38633978, g_loss: 0.69377434\n",
      "Step: [24308] d_loss: 1.38634789, g_loss: 0.69293082\n",
      "Step: [24309] d_loss: 1.38631856, g_loss: 0.69323790\n",
      "Step: [24310] d_loss: 1.38633108, g_loss: 0.69356334\n",
      "Step: [24311] d_loss: 1.38632607, g_loss: 0.69394588\n",
      "Step: [24312] d_loss: 1.38632917, g_loss: 0.69342196\n",
      "Step: [24313] d_loss: 1.38632953, g_loss: 0.69313347\n",
      "Step: [24314] d_loss: 1.38631868, g_loss: 0.69315362\n",
      "Step: [24315] d_loss: 1.38630617, g_loss: 0.69359016\n",
      "Step: [24316] d_loss: 1.38632178, g_loss: 0.69404185\n",
      "Step: [24317] d_loss: 1.38631856, g_loss: 0.69333994\n",
      "Step: [24318] d_loss: 1.38630986, g_loss: 0.69301260\n",
      "Step: [24319] d_loss: 1.38630629, g_loss: 0.69285202\n",
      "Step: [24320] d_loss: 1.38630128, g_loss: 0.69313967\n",
      "Step: [24321] d_loss: 1.38630474, g_loss: 0.69344568\n",
      "Step: [24322] d_loss: 1.38629794, g_loss: 0.69383240\n",
      "Step: [24323] d_loss: 1.38628471, g_loss: 0.69344032\n",
      "Step: [24324] d_loss: 1.38629591, g_loss: 0.69331837\n",
      "Step: [24325] d_loss: 1.38619041, g_loss: 0.69301468\n",
      "Step: [24326] d_loss: 1.38628721, g_loss: 0.69339573\n",
      "Step: [24327] d_loss: 1.38629675, g_loss: 0.69307399\n",
      "Step: [24328] d_loss: 1.38629866, g_loss: 0.69290614\n",
      "Step: [24329] d_loss: 1.38629615, g_loss: 0.69322050\n",
      "Step: [24330] d_loss: 1.38628840, g_loss: 0.69314480\n",
      "Step: [24331] d_loss: 1.38628101, g_loss: 0.69300634\n",
      "Step: [24332] d_loss: 1.38629365, g_loss: 0.69320816\n",
      "Step: [24333] d_loss: 1.38632393, g_loss: 0.69381833\n",
      "Step: [24334] d_loss: 1.38628983, g_loss: 0.69324791\n",
      "Step: [24335] d_loss: 1.38629115, g_loss: 0.69310075\n",
      "Step: [24336] d_loss: 1.38628983, g_loss: 0.69270402\n",
      "Step: [24337] d_loss: 1.38619602, g_loss: 0.69461334\n",
      "Step: [24338] d_loss: 1.38627744, g_loss: 0.69298482\n",
      "Step: [24339] d_loss: 1.38630545, g_loss: 0.69339514\n",
      "Step: [24340] d_loss: 1.38627493, g_loss: 0.69294429\n",
      "Step: [24341] d_loss: 1.38631415, g_loss: 0.69321322\n",
      "Step: [24342] d_loss: 1.38629699, g_loss: 0.69297636\n",
      "Step: [24343] d_loss: 1.38631034, g_loss: 0.69289398\n",
      "Step: [24344] d_loss: 1.38629651, g_loss: 0.69311213\n",
      "Step: [24345] d_loss: 1.38630521, g_loss: 0.69452685\n",
      "Step: [24346] d_loss: 1.38631105, g_loss: 0.69333369\n",
      "Step: [24347] d_loss: 1.38639474, g_loss: 0.69337523\n",
      "Step: [24348] d_loss: 1.38665986, g_loss: 0.69398457\n",
      "Step: [24349] d_loss: 1.38688493, g_loss: 0.69583392\n",
      "Step: [24350] d_loss: 1.38706136, g_loss: 0.69751507\n",
      "Step: [24351] d_loss: 1.38702321, g_loss: 0.69387686\n",
      "Step: [24352] d_loss: 1.38682294, g_loss: 0.68944108\n",
      "Step: [24353] d_loss: 1.38693595, g_loss: 0.69011074\n",
      "Step: [24354] d_loss: 1.38679385, g_loss: 0.69048083\n",
      "Step: [24355] d_loss: 1.38675213, g_loss: 0.69351768\n",
      "Step: [24356] d_loss: 1.38665092, g_loss: 0.69359940\n",
      "Step: [24357] d_loss: 1.38658309, g_loss: 0.69411516\n",
      "Step: [24358] d_loss: 1.38652945, g_loss: 0.69254500\n",
      "Step: [24359] d_loss: 1.38646019, g_loss: 0.69386017\n",
      "Step: [24360] d_loss: 1.38646352, g_loss: 0.69378662\n",
      "Step: [24361] d_loss: 1.38640594, g_loss: 0.69412345\n",
      "Step: [24362] d_loss: 1.38639903, g_loss: 0.69347453\n",
      "Step: [24363] d_loss: 1.38638198, g_loss: 0.69279158\n",
      "Step: [24364] d_loss: 1.38625681, g_loss: 0.69168526\n",
      "Step: [24365] d_loss: 1.38635707, g_loss: 0.69281906\n",
      "Step: [24366] d_loss: 1.38632452, g_loss: 0.69327974\n",
      "Step: [24367] d_loss: 1.38631558, g_loss: 0.69318873\n",
      "Step: [24368] d_loss: 1.38631654, g_loss: 0.69359481\n",
      "Step: [24369] d_loss: 1.38634014, g_loss: 0.69374764\n",
      "Step: [24370] d_loss: 1.38659275, g_loss: 0.69437885\n",
      "Step: [24371] d_loss: 1.38674784, g_loss: 0.69564271\n",
      "Step: [24372] d_loss: 1.38686037, g_loss: 0.69374448\n",
      "Step: [24373] d_loss: 1.38682258, g_loss: 0.69316554\n",
      "Step: [24374] d_loss: 1.38694823, g_loss: 0.69095045\n",
      "Step: [24375] d_loss: 1.38665462, g_loss: 0.69128162\n",
      "Step: [24376] d_loss: 1.38655806, g_loss: 0.69527799\n",
      "Step: [24377] d_loss: 1.38638854, g_loss: 0.69625032\n",
      "Step: [24378] d_loss: 1.38640738, g_loss: 0.69499016\n",
      "Step: [24379] d_loss: 1.38640618, g_loss: 0.69209254\n",
      "Step: [24380] d_loss: 1.38637435, g_loss: 0.69044292\n",
      "Step: [24381] d_loss: 1.38605011, g_loss: 0.69284064\n",
      "Step: [24382] d_loss: 1.38635373, g_loss: 0.69566506\n",
      "Step: [24383] d_loss: 1.38620770, g_loss: 0.69460231\n",
      "Step: [24384] d_loss: 1.38628507, g_loss: 0.69474912\n",
      "Step: [24385] d_loss: 1.38644981, g_loss: 0.69338334\n",
      "Step: [24386] d_loss: 1.38674855, g_loss: 0.69092870\n",
      "Step: [24387] d_loss: 1.38691604, g_loss: 0.69342649\n",
      "Step: [24388] d_loss: 1.38687658, g_loss: 0.69500804\n",
      "Step: [24389] d_loss: 1.38723171, g_loss: 0.69630373\n",
      "Step: [24390] d_loss: 1.38777363, g_loss: 0.69096190\n",
      "Step: [24391] d_loss: 1.38794816, g_loss: 0.69394958\n",
      "Step: [24392] d_loss: 1.38810837, g_loss: 0.69561112\n",
      "Step: [24393] d_loss: 1.38782477, g_loss: 0.69223684\n",
      "Step: [24394] d_loss: 1.38725960, g_loss: 0.68927944\n",
      "Step: [24395] d_loss: 1.38680077, g_loss: 0.69208395\n",
      "Step: [24396] d_loss: 1.38647676, g_loss: 0.69264245\n",
      "Step: [24397] d_loss: 1.38636494, g_loss: 0.69347680\n",
      "Step: [24398] d_loss: 1.38629985, g_loss: 0.69304097\n",
      "Step: [24399] d_loss: 1.38629127, g_loss: 0.69313216\n",
      "Step: [24400] d_loss: 1.38626802, g_loss: 0.69316226\n",
      "Step: [24401] d_loss: 1.38629401, g_loss: 0.69327790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24402] d_loss: 1.38628054, g_loss: 0.69306773\n",
      "Step: [24403] d_loss: 1.38628566, g_loss: 0.69318390\n",
      "Step: [24404] d_loss: 1.38629782, g_loss: 0.69292283\n",
      "Step: [24405] d_loss: 1.38626397, g_loss: 0.69321382\n",
      "Step: [24406] d_loss: 1.38625550, g_loss: 0.69335055\n",
      "Step: [24407] d_loss: 1.38639235, g_loss: 0.69301140\n",
      "Step: [24408] d_loss: 1.38628578, g_loss: 0.69310600\n",
      "Step: [24409] d_loss: 1.38627934, g_loss: 0.69281846\n",
      "Step: [24410] d_loss: 1.38628566, g_loss: 0.69288689\n",
      "Step: [24411] d_loss: 1.38628531, g_loss: 0.69320095\n",
      "Step: [24412] d_loss: 1.38628948, g_loss: 0.69318861\n",
      "Step: [24413] d_loss: 1.38628221, g_loss: 0.69299871\n",
      "Step: [24414] d_loss: 1.38624001, g_loss: 0.69280493\n",
      "Step: [24415] d_loss: 1.38629472, g_loss: 0.69333994\n",
      "Step: [24416] d_loss: 1.38628376, g_loss: 0.69320089\n",
      "Step: [24417] d_loss: 1.38626742, g_loss: 0.69273710\n",
      "Step: [24418] d_loss: 1.38636148, g_loss: 0.69352573\n",
      "Step: [24419] d_loss: 1.38650846, g_loss: 0.69624114\n",
      "Step: [24420] d_loss: 1.38654280, g_loss: 0.69352621\n",
      "Step: [24421] d_loss: 1.38690507, g_loss: 0.69159412\n",
      "Step: [24422] d_loss: 1.38642347, g_loss: 0.69105083\n",
      "Step: [24423] d_loss: 1.38668466, g_loss: 0.69257498\n",
      "Step: [24424] d_loss: 1.38630104, g_loss: 0.69272959\n",
      "Step: [24425] d_loss: 1.38627851, g_loss: 0.69376767\n",
      "Step: [24426] d_loss: 1.38629150, g_loss: 0.69382763\n",
      "Step: [24427] d_loss: 1.38625431, g_loss: 0.69353211\n",
      "Step: [24428] d_loss: 1.38629520, g_loss: 0.69335198\n",
      "Step: [24429] d_loss: 1.38628304, g_loss: 0.69299167\n",
      "Step: [24430] d_loss: 1.38624191, g_loss: 0.69310164\n",
      "Step: [24431] d_loss: 1.38628519, g_loss: 0.69319677\n",
      "Step: [24432] d_loss: 1.38626051, g_loss: 0.69323128\n",
      "Step: [24433] d_loss: 1.38628364, g_loss: 0.69322610\n",
      "Step: [24434] d_loss: 1.38630772, g_loss: 0.69288135\n",
      "Step: [24435] d_loss: 1.38631475, g_loss: 0.69326830\n",
      "Step: [24436] d_loss: 1.38634944, g_loss: 0.69395214\n",
      "Step: [24437] d_loss: 1.38636374, g_loss: 0.69347936\n",
      "Step: [24438] d_loss: 1.38633454, g_loss: 0.69361788\n",
      "Step: [24439] d_loss: 1.38634801, g_loss: 0.69424832\n",
      "Step: [24440] d_loss: 1.38632083, g_loss: 0.69346833\n",
      "Step: [24441] d_loss: 1.38629985, g_loss: 0.69277823\n",
      "Step: [24442] d_loss: 1.38625860, g_loss: 0.69290912\n",
      "Step: [24443] d_loss: 1.38621449, g_loss: 0.69333446\n",
      "Step: [24444] d_loss: 1.38640523, g_loss: 0.69222856\n",
      "Step: [24445] d_loss: 1.38703775, g_loss: 0.68944061\n",
      "Step: [24446] d_loss: 1.38760185, g_loss: 0.69193292\n",
      "Step: [24447] d_loss: 1.38769984, g_loss: 0.68907273\n",
      "Step: [24448] d_loss: 1.38729572, g_loss: 0.69176710\n",
      "Step: [24449] d_loss: 1.38685095, g_loss: 0.69294846\n",
      "Step: [24450] d_loss: 1.38655996, g_loss: 0.69430739\n",
      "Step: [24451] d_loss: 1.38639045, g_loss: 0.69405448\n",
      "Step: [24452] d_loss: 1.38629651, g_loss: 0.69388497\n",
      "Step: [24453] d_loss: 1.38629138, g_loss: 0.69300580\n",
      "Step: [24454] d_loss: 1.38629448, g_loss: 0.69256175\n",
      "Step: [24455] d_loss: 1.38628721, g_loss: 0.69313645\n",
      "Step: [24456] d_loss: 1.38630199, g_loss: 0.69335020\n",
      "Step: [24457] d_loss: 1.38630271, g_loss: 0.69399250\n",
      "Step: [24458] d_loss: 1.38624263, g_loss: 0.69311649\n",
      "Step: [24459] d_loss: 1.38628888, g_loss: 0.69306892\n",
      "Step: [24460] d_loss: 1.38631463, g_loss: 0.69421148\n",
      "Step: [24461] d_loss: 1.38632870, g_loss: 0.69349396\n",
      "Step: [24462] d_loss: 1.38627100, g_loss: 0.69261932\n",
      "Step: [24463] d_loss: 1.38627982, g_loss: 0.69205701\n",
      "Step: [24464] d_loss: 1.38628745, g_loss: 0.69353426\n",
      "Step: [24465] d_loss: 1.38627267, g_loss: 0.69392467\n",
      "Step: [24466] d_loss: 1.38627350, g_loss: 0.69340277\n",
      "Step: [24467] d_loss: 1.38627911, g_loss: 0.69280970\n",
      "Step: [24468] d_loss: 1.38629842, g_loss: 0.69295144\n",
      "Step: [24469] d_loss: 1.38667190, g_loss: 0.69491130\n",
      "Step: [24470] d_loss: 1.38629365, g_loss: 0.69302440\n",
      "Step: [24471] d_loss: 1.38631511, g_loss: 0.69330549\n",
      "Step: [24472] d_loss: 1.38630617, g_loss: 0.69305146\n",
      "Step: [24473] d_loss: 1.38629639, g_loss: 0.69326568\n",
      "Step: [24474] d_loss: 1.38631034, g_loss: 0.69309986\n",
      "Step: [24475] d_loss: 1.38630307, g_loss: 0.69282871\n",
      "Step: [24476] d_loss: 1.38629317, g_loss: 0.69323874\n",
      "Step: [24477] d_loss: 1.38628006, g_loss: 0.69330430\n",
      "Step: [24478] d_loss: 1.38629460, g_loss: 0.69314367\n",
      "Step: [24479] d_loss: 1.38628864, g_loss: 0.69331729\n",
      "Step: [24480] d_loss: 1.38623488, g_loss: 0.69318378\n",
      "Step: [24481] d_loss: 1.38630378, g_loss: 0.69358593\n",
      "Step: [24482] d_loss: 1.38623476, g_loss: 0.69328499\n",
      "Step: [24483] d_loss: 1.38624215, g_loss: 0.69264007\n",
      "Step: [24484] d_loss: 1.38626540, g_loss: 0.69307005\n",
      "Step: [24485] d_loss: 1.38628912, g_loss: 0.69348377\n",
      "Step: [24486] d_loss: 1.38629806, g_loss: 0.69363207\n",
      "Step: [24487] d_loss: 1.38628042, g_loss: 0.69322211\n",
      "Step: [24488] d_loss: 1.38627183, g_loss: 0.69307309\n",
      "Step: [24489] d_loss: 1.38627434, g_loss: 0.69296879\n",
      "Step: [24490] d_loss: 1.38621223, g_loss: 0.69334149\n",
      "Step: [24491] d_loss: 1.38630176, g_loss: 0.69338500\n",
      "Step: [24492] d_loss: 1.38628829, g_loss: 0.69377160\n",
      "Step: [24493] d_loss: 1.38629746, g_loss: 0.69324434\n",
      "Step: [24494] d_loss: 1.38632822, g_loss: 0.69262898\n",
      "Step: [24495] d_loss: 1.38731003, g_loss: 0.69322777\n",
      "Step: [24496] d_loss: 1.38627827, g_loss: 0.69194531\n",
      "Step: [24497] d_loss: 1.38630414, g_loss: 0.69322479\n",
      "Step: [24498] d_loss: 1.38631058, g_loss: 0.69359392\n",
      "Step: [24499] d_loss: 1.38630664, g_loss: 0.69332266\n",
      "Step: [24500] d_loss: 1.38623667, g_loss: 0.69283104\n",
      "Step: [24501] d_loss: 1.38628399, g_loss: 0.69224167\n",
      "Step: [24502] d_loss: 1.38640881, g_loss: 0.69355887\n",
      "Step: [24503] d_loss: 1.38629878, g_loss: 0.69350457\n",
      "Step: [24504] d_loss: 1.38629699, g_loss: 0.69339812\n",
      "Step: [24505] d_loss: 1.38630700, g_loss: 0.69340831\n",
      "Step: [24506] d_loss: 1.38610291, g_loss: 0.69454300\n",
      "Step: [24507] d_loss: 1.38629568, g_loss: 0.69328552\n",
      "Step: [24508] d_loss: 1.38632548, g_loss: 0.69137090\n",
      "Step: [24509] d_loss: 1.38630724, g_loss: 0.69323665\n",
      "Step: [24510] d_loss: 1.38627076, g_loss: 0.69420832\n",
      "Step: [24511] d_loss: 1.38602769, g_loss: 0.69415128\n",
      "Step: [24512] d_loss: 1.38621318, g_loss: 0.69311160\n",
      "Step: [24513] d_loss: 1.38625979, g_loss: 0.69295096\n",
      "Step: [24514] d_loss: 1.38627446, g_loss: 0.69311619\n",
      "Step: [24515] d_loss: 1.38629365, g_loss: 0.69363081\n",
      "Step: [24516] d_loss: 1.38629019, g_loss: 0.69328994\n",
      "Step: [24517] d_loss: 1.38629079, g_loss: 0.69314933\n",
      "Step: [24518] d_loss: 1.38629723, g_loss: 0.69301182\n",
      "Step: [24519] d_loss: 1.38624239, g_loss: 0.69263750\n",
      "Step: [24520] d_loss: 1.38626993, g_loss: 0.69302708\n",
      "Step: [24521] d_loss: 1.38629079, g_loss: 0.69333446\n",
      "Step: [24522] d_loss: 1.38628995, g_loss: 0.69331527\n",
      "Step: [24523] d_loss: 1.38628328, g_loss: 0.69300306\n",
      "Step: [24524] d_loss: 1.38629258, g_loss: 0.69297260\n",
      "Step: [24525] d_loss: 1.38626623, g_loss: 0.69306952\n",
      "Step: [24526] d_loss: 1.38632154, g_loss: 0.69316471\n",
      "Step: [24527] d_loss: 1.38627684, g_loss: 0.69336319\n",
      "Step: [24528] d_loss: 1.38628578, g_loss: 0.69312704\n",
      "Step: [24529] d_loss: 1.38627005, g_loss: 0.69301701\n",
      "Step: [24530] d_loss: 1.38625789, g_loss: 0.69288504\n",
      "Step: [24531] d_loss: 1.38622904, g_loss: 0.69365919\n",
      "Step: [24532] d_loss: 1.38644838, g_loss: 0.69426131\n",
      "Step: [24533] d_loss: 1.38630056, g_loss: 0.69372708\n",
      "Step: [24534] d_loss: 1.38614035, g_loss: 0.69342339\n",
      "Step: [24535] d_loss: 1.38635182, g_loss: 0.69271016\n",
      "Step: [24536] d_loss: 1.38635743, g_loss: 0.69301111\n",
      "Step: [24537] d_loss: 1.38636136, g_loss: 0.69363010\n",
      "Step: [24538] d_loss: 1.38636184, g_loss: 0.69392973\n",
      "Step: [24539] d_loss: 1.38636589, g_loss: 0.69431287\n",
      "Step: [24540] d_loss: 1.38652229, g_loss: 0.69253898\n",
      "Step: [24541] d_loss: 1.38694715, g_loss: 0.69446111\n",
      "Step: [24542] d_loss: 1.38702774, g_loss: 0.69634360\n",
      "Step: [24543] d_loss: 1.38697517, g_loss: 0.69730228\n",
      "Step: [24544] d_loss: 1.38696611, g_loss: 0.69558835\n",
      "Step: [24545] d_loss: 1.38720751, g_loss: 0.69486606\n",
      "Step: [24546] d_loss: 1.38664460, g_loss: 0.69321978\n",
      "Step: [24547] d_loss: 1.38653576, g_loss: 0.69300032\n",
      "Step: [24548] d_loss: 1.38645148, g_loss: 0.69154882\n",
      "Step: [24549] d_loss: 1.38640428, g_loss: 0.69291133\n",
      "Step: [24550] d_loss: 1.38637364, g_loss: 0.69335288\n",
      "Step: [24551] d_loss: 1.38625479, g_loss: 0.69368446\n",
      "Step: [24552] d_loss: 1.38633084, g_loss: 0.69340080\n",
      "Step: [24553] d_loss: 1.38615406, g_loss: 0.69383144\n",
      "Step: [24554] d_loss: 1.38632739, g_loss: 0.69364208\n",
      "Step: [24555] d_loss: 1.38629758, g_loss: 0.69164741\n",
      "Step: [24556] d_loss: 1.38629246, g_loss: 0.69177687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24557] d_loss: 1.38628840, g_loss: 0.69228065\n",
      "Step: [24558] d_loss: 1.38627350, g_loss: 0.69399762\n",
      "Step: [24559] d_loss: 1.38628459, g_loss: 0.69343472\n",
      "Step: [24560] d_loss: 1.38627791, g_loss: 0.69313353\n",
      "Step: [24561] d_loss: 1.38617945, g_loss: 0.69285059\n",
      "Step: [24562] d_loss: 1.38629007, g_loss: 0.69361097\n",
      "Step: [24563] d_loss: 1.38629699, g_loss: 0.69317079\n",
      "Step: [24564] d_loss: 1.38629603, g_loss: 0.69320840\n",
      "Step: [24565] d_loss: 1.38629448, g_loss: 0.69329631\n",
      "Step: [24566] d_loss: 1.38626921, g_loss: 0.69310081\n",
      "Step: [24567] d_loss: 1.38701975, g_loss: 0.69262534\n",
      "Step: [24568] d_loss: 1.38628602, g_loss: 0.69326448\n",
      "Step: [24569] d_loss: 1.38629580, g_loss: 0.69305003\n",
      "Step: [24570] d_loss: 1.38630295, g_loss: 0.69322002\n",
      "Step: [24571] d_loss: 1.38629341, g_loss: 0.69320178\n",
      "Step: [24572] d_loss: 1.38629222, g_loss: 0.69298613\n",
      "Step: [24573] d_loss: 1.38628244, g_loss: 0.69319797\n",
      "Step: [24574] d_loss: 1.38632584, g_loss: 0.69321233\n",
      "Step: [24575] d_loss: 1.38627911, g_loss: 0.69325143\n",
      "Step: [24576] d_loss: 1.38629007, g_loss: 0.69278049\n",
      "Step: [24577] d_loss: 1.38626409, g_loss: 0.69293332\n",
      "Step: [24578] d_loss: 1.38673878, g_loss: 0.69311571\n",
      "Step: [24579] d_loss: 1.38647854, g_loss: 0.69434482\n",
      "Step: [24580] d_loss: 1.38628554, g_loss: 0.69380414\n",
      "Step: [24581] d_loss: 1.38629746, g_loss: 0.69288343\n",
      "Step: [24582] d_loss: 1.38628542, g_loss: 0.69243658\n",
      "Step: [24583] d_loss: 1.38627613, g_loss: 0.69294965\n",
      "Step: [24584] d_loss: 1.38624191, g_loss: 0.69304812\n",
      "Step: [24585] d_loss: 1.38626647, g_loss: 0.69348234\n",
      "Step: [24586] d_loss: 1.38629246, g_loss: 0.69308639\n",
      "Step: [24587] d_loss: 1.38627541, g_loss: 0.69251561\n",
      "Step: [24588] d_loss: 1.38625288, g_loss: 0.69306660\n",
      "Step: [24589] d_loss: 1.38626218, g_loss: 0.69327432\n",
      "Step: [24590] d_loss: 1.38623953, g_loss: 0.69405365\n",
      "Step: [24591] d_loss: 1.38630223, g_loss: 0.69328225\n",
      "Step: [24592] d_loss: 1.38628888, g_loss: 0.69228655\n",
      "Step: [24593] d_loss: 1.38629448, g_loss: 0.69239712\n",
      "Step: [24594] d_loss: 1.38627589, g_loss: 0.69287497\n",
      "Step: [24595] d_loss: 1.38628030, g_loss: 0.69383419\n",
      "Step: [24596] d_loss: 1.38631272, g_loss: 0.69284236\n",
      "Step: [24597] d_loss: 1.38626552, g_loss: 0.69330329\n",
      "Step: [24598] d_loss: 1.38629138, g_loss: 0.69344711\n",
      "Step: [24599] d_loss: 1.38624048, g_loss: 0.69318795\n",
      "Step: [24600] d_loss: 1.38629913, g_loss: 0.69330192\n",
      "Step: [24601] d_loss: 1.38650727, g_loss: 0.69274557\n",
      "Step: [24602] d_loss: 1.38647652, g_loss: 0.69288474\n",
      "Step: [24603] d_loss: 1.38679123, g_loss: 0.69565427\n",
      "Step: [24604] d_loss: 1.38699722, g_loss: 0.69781172\n",
      "Step: [24605] d_loss: 1.38708091, g_loss: 0.69950765\n",
      "Step: [24606] d_loss: 1.38683224, g_loss: 0.69446993\n",
      "Step: [24607] d_loss: 1.38664138, g_loss: 0.69169629\n",
      "Step: [24608] d_loss: 1.38653505, g_loss: 0.69211966\n",
      "Step: [24609] d_loss: 1.38646424, g_loss: 0.69275987\n",
      "Step: [24610] d_loss: 1.38645720, g_loss: 0.68952024\n",
      "Step: [24611] d_loss: 1.38663268, g_loss: 0.69132990\n",
      "Step: [24612] d_loss: 1.38694918, g_loss: 0.69322598\n",
      "Step: [24613] d_loss: 1.38721347, g_loss: 0.69165313\n",
      "Step: [24614] d_loss: 1.38725829, g_loss: 0.69479507\n",
      "Step: [24615] d_loss: 1.38712680, g_loss: 0.69426763\n",
      "Step: [24616] d_loss: 1.38695288, g_loss: 0.69360113\n",
      "Step: [24617] d_loss: 1.38734114, g_loss: 0.69228041\n",
      "Step: [24618] d_loss: 1.38661408, g_loss: 0.69081002\n",
      "Step: [24619] d_loss: 1.38649559, g_loss: 0.69638461\n",
      "Step: [24620] d_loss: 1.38644052, g_loss: 0.69406819\n",
      "Step: [24621] d_loss: 1.38638914, g_loss: 0.69140762\n",
      "Step: [24622] d_loss: 1.38637280, g_loss: 0.69226247\n",
      "Step: [24623] d_loss: 1.38635015, g_loss: 0.69288206\n",
      "Step: [24624] d_loss: 1.38635540, g_loss: 0.69363487\n",
      "Step: [24625] d_loss: 1.38630569, g_loss: 0.69391191\n",
      "Step: [24626] d_loss: 1.38631034, g_loss: 0.69320512\n",
      "Step: [24627] d_loss: 1.38630843, g_loss: 0.69282591\n",
      "Step: [24628] d_loss: 1.38607597, g_loss: 0.69310331\n",
      "Step: [24629] d_loss: 1.38628626, g_loss: 0.69285333\n",
      "Step: [24630] d_loss: 1.38629246, g_loss: 0.69330156\n",
      "Step: [24631] d_loss: 1.38622248, g_loss: 0.69267786\n",
      "Step: [24632] d_loss: 1.38629436, g_loss: 0.69314504\n",
      "Step: [24633] d_loss: 1.38625884, g_loss: 0.69253516\n",
      "Step: [24634] d_loss: 1.38628840, g_loss: 0.69309306\n",
      "Step: [24635] d_loss: 1.38628316, g_loss: 0.69341218\n",
      "Step: [24636] d_loss: 1.38627660, g_loss: 0.69317722\n",
      "Step: [24637] d_loss: 1.38627744, g_loss: 0.69299877\n",
      "Step: [24638] d_loss: 1.38676226, g_loss: 0.69324815\n",
      "Step: [24639] d_loss: 1.38628328, g_loss: 0.69294363\n",
      "Step: [24640] d_loss: 1.38629055, g_loss: 0.69305652\n",
      "Step: [24641] d_loss: 1.38629115, g_loss: 0.69358170\n",
      "Step: [24642] d_loss: 1.38626885, g_loss: 0.69309330\n",
      "Step: [24643] d_loss: 1.38627660, g_loss: 0.69306993\n",
      "Step: [24644] d_loss: 1.38646269, g_loss: 0.69033080\n",
      "Step: [24645] d_loss: 1.38624001, g_loss: 0.69312245\n",
      "Step: [24646] d_loss: 1.38635445, g_loss: 0.69403321\n",
      "Step: [24647] d_loss: 1.38645434, g_loss: 0.69383144\n",
      "Step: [24648] d_loss: 1.38657641, g_loss: 0.69572484\n",
      "Step: [24649] d_loss: 1.38669109, g_loss: 0.69517279\n",
      "Step: [24650] d_loss: 1.38668203, g_loss: 0.69482011\n",
      "Step: [24651] d_loss: 1.38656509, g_loss: 0.69458318\n",
      "Step: [24652] d_loss: 1.38645506, g_loss: 0.69387168\n",
      "Step: [24653] d_loss: 1.38638616, g_loss: 0.69359481\n",
      "Step: [24654] d_loss: 1.38633227, g_loss: 0.69311047\n",
      "Step: [24655] d_loss: 1.38629198, g_loss: 0.69250083\n",
      "Step: [24656] d_loss: 1.38623905, g_loss: 0.69324404\n",
      "Step: [24657] d_loss: 1.38629425, g_loss: 0.69369692\n",
      "Step: [24658] d_loss: 1.38628769, g_loss: 0.69287443\n",
      "Step: [24659] d_loss: 1.38628554, g_loss: 0.69288939\n",
      "Step: [24660] d_loss: 1.38627625, g_loss: 0.69285476\n",
      "Step: [24661] d_loss: 1.38629162, g_loss: 0.69340402\n",
      "Step: [24662] d_loss: 1.38628519, g_loss: 0.69329041\n",
      "Step: [24663] d_loss: 1.38627529, g_loss: 0.69301128\n",
      "Step: [24664] d_loss: 1.38628650, g_loss: 0.69334793\n",
      "Step: [24665] d_loss: 1.38628340, g_loss: 0.69291490\n",
      "Step: [24666] d_loss: 1.38628626, g_loss: 0.69251627\n",
      "Step: [24667] d_loss: 1.38629103, g_loss: 0.69322884\n",
      "Step: [24668] d_loss: 1.38629794, g_loss: 0.69337666\n",
      "Step: [24669] d_loss: 1.38630545, g_loss: 0.69327939\n",
      "Step: [24670] d_loss: 1.38627899, g_loss: 0.69332355\n",
      "Step: [24671] d_loss: 1.38627338, g_loss: 0.69287556\n",
      "Step: [24672] d_loss: 1.38628399, g_loss: 0.69320112\n",
      "Step: [24673] d_loss: 1.38628221, g_loss: 0.69335997\n",
      "Step: [24674] d_loss: 1.38631058, g_loss: 0.69243729\n",
      "Step: [24675] d_loss: 1.38637853, g_loss: 0.69302988\n",
      "Step: [24676] d_loss: 1.38670135, g_loss: 0.69470632\n",
      "Step: [24677] d_loss: 1.38698888, g_loss: 0.69301307\n",
      "Step: [24678] d_loss: 1.38731623, g_loss: 0.68942213\n",
      "Step: [24679] d_loss: 1.38729250, g_loss: 0.69101501\n",
      "Step: [24680] d_loss: 1.38717270, g_loss: 0.69262189\n",
      "Step: [24681] d_loss: 1.38700342, g_loss: 0.69394964\n",
      "Step: [24682] d_loss: 1.38680506, g_loss: 0.69347531\n",
      "Step: [24683] d_loss: 1.38641071, g_loss: 0.69645363\n",
      "Step: [24684] d_loss: 1.38653517, g_loss: 0.69433951\n",
      "Step: [24685] d_loss: 1.38713145, g_loss: 0.69520736\n",
      "Step: [24686] d_loss: 1.38644600, g_loss: 0.69353700\n",
      "Step: [24687] d_loss: 1.38638902, g_loss: 0.69046116\n",
      "Step: [24688] d_loss: 1.38634372, g_loss: 0.69051278\n",
      "Step: [24689] d_loss: 1.38634360, g_loss: 0.69256717\n",
      "Step: [24690] d_loss: 1.38630581, g_loss: 0.69381273\n",
      "Step: [24691] d_loss: 1.38629723, g_loss: 0.69433576\n",
      "Step: [24692] d_loss: 1.38628221, g_loss: 0.69358194\n",
      "Step: [24693] d_loss: 1.38636398, g_loss: 0.69266105\n",
      "Step: [24694] d_loss: 1.38677764, g_loss: 0.69362003\n",
      "Step: [24695] d_loss: 1.38697863, g_loss: 0.69342119\n",
      "Step: [24696] d_loss: 1.38694215, g_loss: 0.69305140\n",
      "Step: [24697] d_loss: 1.38675404, g_loss: 0.69152892\n",
      "Step: [24698] d_loss: 1.38658559, g_loss: 0.68971354\n",
      "Step: [24699] d_loss: 1.38637626, g_loss: 0.69257200\n",
      "Step: [24700] d_loss: 1.38630605, g_loss: 0.69399571\n",
      "Step: [24701] d_loss: 1.38659418, g_loss: 0.69376516\n",
      "Step: [24702] d_loss: 1.38725412, g_loss: 0.69782913\n",
      "Step: [24703] d_loss: 1.38765907, g_loss: 0.69655621\n",
      "Step: [24704] d_loss: 1.38756466, g_loss: 0.69441319\n",
      "Step: [24705] d_loss: 1.38716447, g_loss: 0.69112802\n",
      "Step: [24706] d_loss: 1.38675523, g_loss: 0.68894517\n",
      "Step: [24707] d_loss: 1.38648820, g_loss: 0.69373357\n",
      "Step: [24708] d_loss: 1.38633657, g_loss: 0.69484997\n",
      "Step: [24709] d_loss: 1.38629723, g_loss: 0.69393748\n",
      "Step: [24710] d_loss: 1.38627744, g_loss: 0.69220215\n",
      "Step: [24711] d_loss: 1.38626611, g_loss: 0.69165981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24712] d_loss: 1.38631654, g_loss: 0.69338399\n",
      "Step: [24713] d_loss: 1.38623154, g_loss: 0.69377315\n",
      "Step: [24714] d_loss: 1.38627887, g_loss: 0.69325280\n",
      "Step: [24715] d_loss: 1.38627744, g_loss: 0.69334149\n",
      "Step: [24716] d_loss: 1.38627493, g_loss: 0.69311309\n",
      "Step: [24717] d_loss: 1.38627839, g_loss: 0.69355631\n",
      "Step: [24718] d_loss: 1.38627923, g_loss: 0.69314265\n",
      "Step: [24719] d_loss: 1.38627887, g_loss: 0.69316596\n",
      "Step: [24720] d_loss: 1.38627315, g_loss: 0.69313276\n",
      "Step: [24721] d_loss: 1.38626695, g_loss: 0.69299692\n",
      "Step: [24722] d_loss: 1.38626766, g_loss: 0.69329441\n",
      "Step: [24723] d_loss: 1.38625681, g_loss: 0.69321644\n",
      "Step: [24724] d_loss: 1.38616991, g_loss: 0.69318163\n",
      "Step: [24725] d_loss: 1.38623655, g_loss: 0.69317067\n",
      "Step: [24726] d_loss: 1.38711405, g_loss: 0.69284445\n",
      "Step: [24727] d_loss: 1.38627648, g_loss: 0.69260472\n",
      "Step: [24728] d_loss: 1.38622212, g_loss: 0.69336855\n",
      "Step: [24729] d_loss: 1.38625240, g_loss: 0.69371533\n",
      "Step: [24730] d_loss: 1.38624144, g_loss: 0.69325113\n",
      "Step: [24731] d_loss: 1.38626146, g_loss: 0.69307637\n",
      "Step: [24732] d_loss: 1.38622439, g_loss: 0.69289595\n",
      "Step: [24733] d_loss: 1.38617873, g_loss: 0.69291985\n",
      "Step: [24734] d_loss: 1.38623309, g_loss: 0.69401115\n",
      "Step: [24735] d_loss: 1.38625145, g_loss: 0.69295353\n",
      "Step: [24736] d_loss: 1.38630033, g_loss: 0.69300145\n",
      "Step: [24737] d_loss: 1.38636518, g_loss: 0.69151461\n",
      "Step: [24738] d_loss: 1.38627303, g_loss: 0.69247943\n",
      "Step: [24739] d_loss: 1.38627458, g_loss: 0.69237709\n",
      "Step: [24740] d_loss: 1.38628244, g_loss: 0.69419384\n",
      "Step: [24741] d_loss: 1.38627267, g_loss: 0.69419289\n",
      "Step: [24742] d_loss: 1.38622355, g_loss: 0.69298077\n",
      "Step: [24743] d_loss: 1.38643134, g_loss: 0.69368201\n",
      "Step: [24744] d_loss: 1.38678908, g_loss: 0.69353902\n",
      "Step: [24745] d_loss: 1.38704848, g_loss: 0.69467199\n",
      "Step: [24746] d_loss: 1.38669336, g_loss: 0.69357997\n",
      "Step: [24747] d_loss: 1.38657093, g_loss: 0.69412011\n",
      "Step: [24748] d_loss: 1.38658714, g_loss: 0.69325984\n",
      "Step: [24749] d_loss: 1.38635528, g_loss: 0.69230813\n",
      "Step: [24750] d_loss: 1.38634551, g_loss: 0.69354379\n",
      "Step: [24751] d_loss: 1.38634181, g_loss: 0.69368654\n",
      "Step: [24752] d_loss: 1.38625991, g_loss: 0.69331825\n",
      "Step: [24753] d_loss: 1.38625503, g_loss: 0.69275713\n",
      "Step: [24754] d_loss: 1.38627911, g_loss: 0.69290221\n",
      "Step: [24755] d_loss: 1.38627124, g_loss: 0.69313550\n",
      "Step: [24756] d_loss: 1.38628101, g_loss: 0.69323939\n",
      "Step: [24757] d_loss: 1.38627529, g_loss: 0.69326556\n",
      "Step: [24758] d_loss: 1.38627160, g_loss: 0.69324952\n",
      "Step: [24759] d_loss: 1.38626695, g_loss: 0.69337487\n",
      "Step: [24760] d_loss: 1.38627791, g_loss: 0.69309849\n",
      "Step: [24761] d_loss: 1.38628149, g_loss: 0.69317055\n",
      "Step: [24762] d_loss: 1.38627100, g_loss: 0.69337821\n",
      "Step: [24763] d_loss: 1.38628078, g_loss: 0.69316781\n",
      "Step: [24764] d_loss: 1.38626480, g_loss: 0.69315171\n",
      "Step: [24765] d_loss: 1.38656092, g_loss: 0.69414026\n",
      "Step: [24766] d_loss: 1.38622069, g_loss: 0.69302022\n",
      "Step: [24767] d_loss: 1.38625646, g_loss: 0.69320434\n",
      "Step: [24768] d_loss: 1.38623071, g_loss: 0.69367021\n",
      "Step: [24769] d_loss: 1.38637447, g_loss: 0.69361520\n",
      "Step: [24770] d_loss: 1.38630068, g_loss: 0.69272304\n",
      "Step: [24771] d_loss: 1.38627875, g_loss: 0.69327140\n",
      "Step: [24772] d_loss: 1.38617039, g_loss: 0.69350988\n",
      "Step: [24773] d_loss: 1.38626933, g_loss: 0.69328731\n",
      "Step: [24774] d_loss: 1.38624668, g_loss: 0.69334626\n",
      "Step: [24775] d_loss: 1.38629961, g_loss: 0.69292748\n",
      "Step: [24776] d_loss: 1.38626957, g_loss: 0.69306731\n",
      "Step: [24777] d_loss: 1.38624358, g_loss: 0.69315422\n",
      "Step: [24778] d_loss: 1.38621950, g_loss: 0.69308305\n",
      "Step: [24779] d_loss: 1.38626003, g_loss: 0.69296008\n",
      "Step: [24780] d_loss: 1.38631999, g_loss: 0.69342732\n",
      "Step: [24781] d_loss: 1.38626695, g_loss: 0.69309473\n",
      "Step: [24782] d_loss: 1.38622820, g_loss: 0.69344670\n",
      "Step: [24783] d_loss: 1.38626051, g_loss: 0.69320893\n",
      "Step: [24784] d_loss: 1.38624585, g_loss: 0.69367635\n",
      "Step: [24785] d_loss: 1.38620615, g_loss: 0.69289112\n",
      "Step: [24786] d_loss: 1.38628244, g_loss: 0.69297838\n",
      "Step: [24787] d_loss: 1.38623857, g_loss: 0.69344419\n",
      "Step: [24788] d_loss: 1.38643146, g_loss: 0.69369686\n",
      "Step: [24789] d_loss: 1.38625503, g_loss: 0.69327414\n",
      "Step: [24790] d_loss: 1.38616931, g_loss: 0.69343334\n",
      "Step: [24791] d_loss: 1.38640976, g_loss: 0.69365382\n",
      "Step: [24792] d_loss: 1.38648736, g_loss: 0.69242549\n",
      "Step: [24793] d_loss: 1.38653326, g_loss: 0.69365072\n",
      "Step: [24794] d_loss: 1.38654065, g_loss: 0.69284195\n",
      "Step: [24795] d_loss: 1.38651037, g_loss: 0.69391859\n",
      "Step: [24796] d_loss: 1.38647354, g_loss: 0.69251841\n",
      "Step: [24797] d_loss: 1.38643479, g_loss: 0.69301391\n",
      "Step: [24798] d_loss: 1.38638675, g_loss: 0.69348335\n",
      "Step: [24799] d_loss: 1.38635695, g_loss: 0.69226569\n",
      "Step: [24800] d_loss: 1.38657618, g_loss: 0.69406140\n",
      "Step: [24801] d_loss: 1.38653588, g_loss: 0.69545388\n",
      "Step: [24802] d_loss: 1.38650560, g_loss: 0.69369531\n",
      "Step: [24803] d_loss: 1.38662791, g_loss: 0.69354802\n",
      "Step: [24804] d_loss: 1.38652050, g_loss: 0.69290376\n",
      "Step: [24805] d_loss: 1.38645899, g_loss: 0.69312572\n",
      "Step: [24806] d_loss: 1.38644314, g_loss: 0.69298446\n",
      "Step: [24807] d_loss: 1.38637614, g_loss: 0.69379306\n",
      "Step: [24808] d_loss: 1.38637280, g_loss: 0.69359815\n",
      "Step: [24809] d_loss: 1.38631773, g_loss: 0.69344044\n",
      "Step: [24810] d_loss: 1.38632703, g_loss: 0.69269091\n",
      "Step: [24811] d_loss: 1.38629830, g_loss: 0.69295424\n",
      "Step: [24812] d_loss: 1.38629031, g_loss: 0.69307184\n",
      "Step: [24813] d_loss: 1.38628066, g_loss: 0.69325721\n",
      "Step: [24814] d_loss: 1.38630652, g_loss: 0.69334245\n",
      "Step: [24815] d_loss: 1.38629270, g_loss: 0.69322777\n",
      "Step: [24816] d_loss: 1.38632035, g_loss: 0.69248366\n",
      "Step: [24817] d_loss: 1.38629317, g_loss: 0.69326925\n",
      "Step: [24818] d_loss: 1.38633204, g_loss: 0.69257534\n",
      "Step: [24819] d_loss: 1.38640177, g_loss: 0.69297415\n",
      "Step: [24820] d_loss: 1.38632584, g_loss: 0.69357371\n",
      "Step: [24821] d_loss: 1.38629615, g_loss: 0.69286650\n",
      "Step: [24822] d_loss: 1.38629293, g_loss: 0.69289547\n",
      "Step: [24823] d_loss: 1.38628101, g_loss: 0.69302535\n",
      "Step: [24824] d_loss: 1.38625848, g_loss: 0.69268322\n",
      "Step: [24825] d_loss: 1.38642240, g_loss: 0.69453394\n",
      "Step: [24826] d_loss: 1.38634205, g_loss: 0.69395161\n",
      "Step: [24827] d_loss: 1.38635623, g_loss: 0.69218683\n",
      "Step: [24828] d_loss: 1.38637066, g_loss: 0.69160002\n",
      "Step: [24829] d_loss: 1.38638258, g_loss: 0.69286597\n",
      "Step: [24830] d_loss: 1.38635945, g_loss: 0.69354773\n",
      "Step: [24831] d_loss: 1.38635445, g_loss: 0.69376379\n",
      "Step: [24832] d_loss: 1.38633132, g_loss: 0.69306588\n",
      "Step: [24833] d_loss: 1.38629234, g_loss: 0.69334698\n",
      "Step: [24834] d_loss: 1.38631773, g_loss: 0.69377971\n",
      "Step: [24835] d_loss: 1.38630974, g_loss: 0.69322067\n",
      "Step: [24836] d_loss: 1.38630772, g_loss: 0.69284010\n",
      "Step: [24837] d_loss: 1.38630533, g_loss: 0.69319785\n",
      "Step: [24838] d_loss: 1.38632286, g_loss: 0.69281983\n",
      "Step: [24839] d_loss: 1.38630581, g_loss: 0.69344306\n",
      "Step: [24840] d_loss: 1.38631594, g_loss: 0.69313490\n",
      "Step: [24841] d_loss: 1.38693106, g_loss: 0.69409931\n",
      "Step: [24842] d_loss: 1.38648987, g_loss: 0.69360501\n",
      "Step: [24843] d_loss: 1.38688755, g_loss: 0.69422495\n",
      "Step: [24844] d_loss: 1.38710463, g_loss: 0.69217587\n",
      "Step: [24845] d_loss: 1.38713634, g_loss: 0.69447953\n",
      "Step: [24846] d_loss: 1.38705838, g_loss: 0.69605500\n",
      "Step: [24847] d_loss: 1.38685489, g_loss: 0.69678569\n",
      "Step: [24848] d_loss: 1.38667119, g_loss: 0.69530398\n",
      "Step: [24849] d_loss: 1.38645852, g_loss: 0.69360840\n",
      "Step: [24850] d_loss: 1.38612270, g_loss: 0.69212645\n",
      "Step: [24851] d_loss: 1.38636971, g_loss: 0.69181532\n",
      "Step: [24852] d_loss: 1.38638973, g_loss: 0.69103324\n",
      "Step: [24853] d_loss: 1.38634181, g_loss: 0.69297504\n",
      "Step: [24854] d_loss: 1.38634837, g_loss: 0.69367397\n",
      "Step: [24855] d_loss: 1.38633275, g_loss: 0.69359303\n",
      "Step: [24856] d_loss: 1.38631117, g_loss: 0.69266325\n",
      "Step: [24857] d_loss: 1.38629723, g_loss: 0.69282115\n",
      "Step: [24858] d_loss: 1.38628519, g_loss: 0.69248927\n",
      "Step: [24859] d_loss: 1.38633537, g_loss: 0.69331187\n",
      "Step: [24860] d_loss: 1.38626695, g_loss: 0.69333673\n",
      "Step: [24861] d_loss: 1.38623476, g_loss: 0.69341886\n",
      "Step: [24862] d_loss: 1.38623154, g_loss: 0.69361156\n",
      "Step: [24863] d_loss: 1.38625765, g_loss: 0.69372773\n",
      "Step: [24864] d_loss: 1.38630199, g_loss: 0.69267851\n",
      "Step: [24865] d_loss: 1.38628769, g_loss: 0.69236803\n",
      "Step: [24866] d_loss: 1.38627958, g_loss: 0.69285226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [24867] d_loss: 1.38631105, g_loss: 0.69322729\n",
      "Step: [24868] d_loss: 1.38629055, g_loss: 0.69349188\n",
      "Step: [24869] d_loss: 1.38630080, g_loss: 0.69270146\n",
      "Step: [24870] d_loss: 1.38628769, g_loss: 0.69320983\n",
      "Step: [24871] d_loss: 1.38628137, g_loss: 0.69283783\n",
      "Step: [24872] d_loss: 1.38626826, g_loss: 0.69282866\n",
      "Step: [24873] d_loss: 1.38623023, g_loss: 0.69253051\n",
      "Step: [24874] d_loss: 1.38647318, g_loss: 0.69369483\n",
      "Step: [24875] d_loss: 1.38631368, g_loss: 0.69335246\n",
      "Step: [24876] d_loss: 1.38632023, g_loss: 0.69354296\n",
      "Step: [24877] d_loss: 1.38633537, g_loss: 0.69297218\n",
      "Step: [24878] d_loss: 1.38632119, g_loss: 0.69275647\n",
      "Step: [24879] d_loss: 1.38632083, g_loss: 0.69316852\n",
      "Step: [24880] d_loss: 1.38632047, g_loss: 0.69354558\n",
      "Step: [24881] d_loss: 1.38630605, g_loss: 0.69338751\n",
      "Step: [24882] d_loss: 1.38630223, g_loss: 0.69333529\n",
      "Step: [24883] d_loss: 1.38630366, g_loss: 0.69309896\n",
      "Step: [24884] d_loss: 1.38631523, g_loss: 0.69321108\n",
      "Step: [24885] d_loss: 1.38630557, g_loss: 0.69317377\n",
      "Step: [24886] d_loss: 1.38585722, g_loss: 0.69412005\n",
      "Step: [24887] d_loss: 1.38626814, g_loss: 0.69291914\n",
      "Step: [24888] d_loss: 1.38628769, g_loss: 0.69300872\n",
      "Step: [24889] d_loss: 1.38628006, g_loss: 0.69293082\n",
      "Step: [24890] d_loss: 1.38629270, g_loss: 0.69318730\n",
      "Step: [24891] d_loss: 1.38631773, g_loss: 0.69294906\n",
      "Step: [24892] d_loss: 1.38630986, g_loss: 0.69317514\n",
      "Step: [24893] d_loss: 1.38629580, g_loss: 0.69333333\n",
      "Step: [24894] d_loss: 1.38629234, g_loss: 0.69307518\n",
      "Step: [24895] d_loss: 1.38630462, g_loss: 0.69309807\n",
      "Step: [24896] d_loss: 1.38629079, g_loss: 0.69286966\n",
      "Step: [24897] d_loss: 1.38631535, g_loss: 0.69264013\n",
      "Step: [24898] d_loss: 1.38629663, g_loss: 0.69308782\n",
      "Step: [24899] d_loss: 1.38629746, g_loss: 0.69322163\n",
      "Step: [24900] d_loss: 1.38630092, g_loss: 0.69343722\n",
      "Step: [24901] d_loss: 1.38630152, g_loss: 0.69327849\n",
      "Step: [24902] d_loss: 1.38631010, g_loss: 0.69314539\n",
      "Step: [24903] d_loss: 1.38628173, g_loss: 0.69341552\n",
      "Step: [24904] d_loss: 1.38626814, g_loss: 0.69329166\n",
      "Step: [24905] d_loss: 1.38630545, g_loss: 0.69304788\n",
      "Step: [24906] d_loss: 1.38620734, g_loss: 0.69381320\n",
      "Step: [24907] d_loss: 1.38628519, g_loss: 0.69272804\n",
      "Step: [24908] d_loss: 1.38638639, g_loss: 0.69325614\n",
      "Step: [24909] d_loss: 1.38670218, g_loss: 0.69489843\n",
      "Step: [24910] d_loss: 1.38690782, g_loss: 0.69330084\n",
      "Step: [24911] d_loss: 1.38701367, g_loss: 0.69404054\n",
      "Step: [24912] d_loss: 1.38701904, g_loss: 0.69414425\n",
      "Step: [24913] d_loss: 1.38692594, g_loss: 0.69558048\n",
      "Step: [24914] d_loss: 1.38683367, g_loss: 0.69315344\n",
      "Step: [24915] d_loss: 1.38676071, g_loss: 0.69177866\n",
      "Step: [24916] d_loss: 1.38668323, g_loss: 0.69706130\n",
      "Step: [24917] d_loss: 1.38659561, g_loss: 0.69502074\n",
      "Step: [24918] d_loss: 1.38657045, g_loss: 0.69082171\n",
      "Step: [24919] d_loss: 1.38655210, g_loss: 0.69248760\n",
      "Step: [24920] d_loss: 1.38650584, g_loss: 0.69585156\n",
      "Step: [24921] d_loss: 1.38640809, g_loss: 0.69511116\n",
      "Step: [24922] d_loss: 1.38639545, g_loss: 0.69286168\n",
      "Step: [24923] d_loss: 1.38637495, g_loss: 0.68849432\n",
      "Step: [24924] d_loss: 1.38641667, g_loss: 0.68992293\n",
      "Step: [24925] d_loss: 1.38656354, g_loss: 0.69268644\n",
      "Step: [24926] d_loss: 1.38668489, g_loss: 0.69545430\n",
      "Step: [24927] d_loss: 1.38682985, g_loss: 0.69285488\n",
      "Step: [24928] d_loss: 1.38682485, g_loss: 0.68987346\n",
      "Step: [24929] d_loss: 1.38679504, g_loss: 0.69007421\n",
      "Step: [24930] d_loss: 1.38663602, g_loss: 0.69155157\n",
      "Step: [24931] d_loss: 1.38649845, g_loss: 0.69256508\n",
      "Step: [24932] d_loss: 1.38642013, g_loss: 0.69295943\n",
      "Step: [24933] d_loss: 1.38632762, g_loss: 0.69298071\n",
      "Step: [24934] d_loss: 1.38629746, g_loss: 0.69431686\n",
      "Step: [24935] d_loss: 1.38646889, g_loss: 0.69469237\n",
      "Step: [24936] d_loss: 1.38682032, g_loss: 0.69473773\n",
      "Step: [24937] d_loss: 1.38725984, g_loss: 0.69132829\n",
      "Step: [24938] d_loss: 1.38726878, g_loss: 0.69174135\n",
      "Step: [24939] d_loss: 1.38708317, g_loss: 0.69601524\n",
      "Step: [24940] d_loss: 1.38682795, g_loss: 0.69605315\n",
      "Step: [24941] d_loss: 1.38660347, g_loss: 0.69355083\n",
      "Step: [24942] d_loss: 1.38641977, g_loss: 0.69125617\n",
      "Step: [24943] d_loss: 1.38636851, g_loss: 0.69148844\n",
      "Step: [24944] d_loss: 1.38633609, g_loss: 0.69305861\n",
      "Step: [24945] d_loss: 1.38630629, g_loss: 0.69411731\n",
      "Step: [24946] d_loss: 1.38628352, g_loss: 0.69388884\n",
      "Step: [24947] d_loss: 1.38629651, g_loss: 0.69312668\n",
      "Step: [24948] d_loss: 1.38629079, g_loss: 0.69288170\n",
      "Step: [24949] d_loss: 1.38628983, g_loss: 0.69298851\n",
      "Step: [24950] d_loss: 1.38627911, g_loss: 0.69298768\n",
      "Step: [24951] d_loss: 1.38632178, g_loss: 0.69300938\n",
      "Step: [24952] d_loss: 1.38629031, g_loss: 0.69325602\n",
      "Step: [24953] d_loss: 1.38628983, g_loss: 0.69320202\n",
      "Step: [24954] d_loss: 1.38629317, g_loss: 0.69321203\n",
      "Step: [24955] d_loss: 1.38628244, g_loss: 0.69317305\n",
      "Step: [24956] d_loss: 1.38628626, g_loss: 0.69355536\n",
      "Step: [24957] d_loss: 1.38636446, g_loss: 0.69309765\n",
      "Step: [24958] d_loss: 1.38626003, g_loss: 0.69321799\n",
      "Step: [24959] d_loss: 1.38629460, g_loss: 0.69299030\n",
      "Step: [24960] d_loss: 1.38637519, g_loss: 0.69282216\n",
      "Step: [24961] d_loss: 1.38628912, g_loss: 0.69303280\n",
      "Step: [24962] d_loss: 1.38628888, g_loss: 0.69296753\n",
      "Step: [24963] d_loss: 1.38629651, g_loss: 0.69329780\n",
      "Step: [24964] d_loss: 1.38627815, g_loss: 0.69343954\n",
      "Step: [24965] d_loss: 1.38629842, g_loss: 0.69316196\n",
      "Step: [24966] d_loss: 1.38627315, g_loss: 0.69310945\n",
      "Step: [24967] d_loss: 1.38630366, g_loss: 0.69311649\n",
      "Step: [24968] d_loss: 1.38629246, g_loss: 0.69321764\n",
      "Step: [24969] d_loss: 1.38627601, g_loss: 0.69309384\n",
      "Step: [24970] d_loss: 1.38628125, g_loss: 0.69297850\n",
      "Step: [24971] d_loss: 1.38630366, g_loss: 0.69319856\n",
      "Step: [24972] d_loss: 1.38628507, g_loss: 0.69329560\n",
      "Step: [24973] d_loss: 1.38627172, g_loss: 0.69329178\n",
      "Step: [24974] d_loss: 1.38631701, g_loss: 0.69317031\n",
      "Step: [24975] d_loss: 1.38626528, g_loss: 0.69298792\n",
      "Step: [24976] d_loss: 1.38628733, g_loss: 0.69313550\n",
      "Step: [24977] d_loss: 1.38629627, g_loss: 0.69331646\n",
      "Step: [24978] d_loss: 1.38628924, g_loss: 0.69333291\n",
      "Step: [24979] d_loss: 1.38629186, g_loss: 0.69307780\n",
      "Step: [24980] d_loss: 1.38629115, g_loss: 0.69306076\n",
      "Step: [24981] d_loss: 1.38627338, g_loss: 0.69314754\n",
      "Step: [24982] d_loss: 1.38628471, g_loss: 0.69305861\n",
      "Step: [24983] d_loss: 1.38628829, g_loss: 0.69333267\n",
      "Step: [24984] d_loss: 1.38630176, g_loss: 0.69312871\n",
      "Step: [24985] d_loss: 1.38628292, g_loss: 0.69320542\n",
      "Step: [24986] d_loss: 1.38628185, g_loss: 0.69309616\n",
      "Step: [24987] d_loss: 1.38628757, g_loss: 0.69337380\n",
      "Step: [24988] d_loss: 1.38627195, g_loss: 0.69318169\n",
      "Step: [24989] d_loss: 1.38627756, g_loss: 0.69306374\n",
      "Step: [24990] d_loss: 1.38630342, g_loss: 0.69319218\n",
      "Step: [24991] d_loss: 1.38626289, g_loss: 0.69343114\n",
      "Step: [24992] d_loss: 1.38629365, g_loss: 0.69305789\n",
      "Step: [24993] d_loss: 1.38625634, g_loss: 0.69328594\n",
      "Step: [24994] d_loss: 1.38629675, g_loss: 0.69339299\n",
      "Step: [24995] d_loss: 1.38630033, g_loss: 0.69330025\n",
      "Step: [24996] d_loss: 1.38629866, g_loss: 0.69312149\n",
      "Step: [24997] d_loss: 1.38629746, g_loss: 0.69320786\n",
      "Step: [24998] d_loss: 1.38625360, g_loss: 0.69297481\n",
      "Step: [24999] d_loss: 1.38627028, g_loss: 0.69296473\n",
      "Step: [25000] d_loss: 1.38628507, g_loss: 0.69317639\n",
      "Step: [25001] d_loss: 1.38629091, g_loss: 0.69319403\n",
      "Step: [25002] d_loss: 1.38629305, g_loss: 0.69316125\n",
      "Step: [25003] d_loss: 1.38630211, g_loss: 0.69314623\n",
      "Step: [25004] d_loss: 1.38628781, g_loss: 0.69313514\n",
      "Step: [25005] d_loss: 1.38629544, g_loss: 0.69311666\n",
      "Step: [25006] d_loss: 1.38629925, g_loss: 0.69314897\n",
      "Step: [25007] d_loss: 1.38631594, g_loss: 0.69316155\n",
      "Step: [25008] d_loss: 1.38628757, g_loss: 0.69315797\n",
      "Step: [25009] d_loss: 1.38570023, g_loss: 0.69451189\n",
      "Step: [25010] d_loss: 1.38629782, g_loss: 0.69342554\n",
      "Step: [25011] d_loss: 1.38629389, g_loss: 0.69308722\n",
      "Step: [25012] d_loss: 1.38628399, g_loss: 0.69319957\n",
      "Step: [25013] d_loss: 1.38627839, g_loss: 0.69324422\n",
      "Step: [25014] d_loss: 1.38628554, g_loss: 0.69317091\n",
      "Step: [25015] d_loss: 1.38630331, g_loss: 0.69312245\n",
      "Step: [25016] d_loss: 1.38625836, g_loss: 0.69296610\n",
      "Step: [25017] d_loss: 1.38624859, g_loss: 0.69309783\n",
      "Step: [25018] d_loss: 1.38628805, g_loss: 0.69329435\n",
      "Step: [25019] d_loss: 1.38626671, g_loss: 0.69312847\n",
      "Step: [25020] d_loss: 1.38629079, g_loss: 0.69260168\n",
      "Step: [25021] d_loss: 1.38636196, g_loss: 0.69294626\n",
      "Step: [25022] d_loss: 1.38648701, g_loss: 0.69331044\n",
      "Step: [25023] d_loss: 1.38658381, g_loss: 0.69369292\n",
      "Step: [25024] d_loss: 1.38662720, g_loss: 0.69581628\n",
      "Step: [25025] d_loss: 1.38662577, g_loss: 0.69377428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25026] d_loss: 1.38661718, g_loss: 0.69397402\n",
      "Step: [25027] d_loss: 1.38716149, g_loss: 0.69300842\n",
      "Step: [25028] d_loss: 1.38647735, g_loss: 0.69045061\n",
      "Step: [25029] d_loss: 1.38645911, g_loss: 0.69326460\n",
      "Step: [25030] d_loss: 1.38638496, g_loss: 0.69417322\n",
      "Step: [25031] d_loss: 1.38637340, g_loss: 0.69376004\n",
      "Step: [25032] d_loss: 1.38634729, g_loss: 0.69384843\n",
      "Step: [25033] d_loss: 1.38631153, g_loss: 0.69314545\n",
      "Step: [25034] d_loss: 1.38632298, g_loss: 0.69225180\n",
      "Step: [25035] d_loss: 1.38632131, g_loss: 0.69224024\n",
      "Step: [25036] d_loss: 1.38632643, g_loss: 0.69314647\n",
      "Step: [25037] d_loss: 1.38629341, g_loss: 0.69447267\n",
      "Step: [25038] d_loss: 1.38632631, g_loss: 0.69444525\n",
      "Step: [25039] d_loss: 1.38627958, g_loss: 0.69294256\n",
      "Step: [25040] d_loss: 1.38631749, g_loss: 0.69264603\n",
      "Step: [25041] d_loss: 1.38630223, g_loss: 0.69277292\n",
      "Step: [25042] d_loss: 1.38628173, g_loss: 0.69294059\n",
      "Step: [25043] d_loss: 1.38629985, g_loss: 0.69336194\n",
      "Step: [25044] d_loss: 1.38628113, g_loss: 0.69359863\n",
      "Step: [25045] d_loss: 1.38628864, g_loss: 0.69344008\n",
      "Step: [25046] d_loss: 1.38633370, g_loss: 0.69297636\n",
      "Step: [25047] d_loss: 1.38627112, g_loss: 0.69275790\n",
      "Step: [25048] d_loss: 1.38631010, g_loss: 0.69298100\n",
      "Step: [25049] d_loss: 1.38625932, g_loss: 0.69307804\n",
      "Step: [25050] d_loss: 1.38629007, g_loss: 0.69320142\n",
      "Step: [25051] d_loss: 1.38632584, g_loss: 0.69314229\n",
      "Step: [25052] d_loss: 1.38627553, g_loss: 0.69317538\n",
      "Step: [25053] d_loss: 1.38625860, g_loss: 0.69314063\n",
      "Step: [25054] d_loss: 1.38627064, g_loss: 0.69313383\n",
      "Step: [25055] d_loss: 1.38631201, g_loss: 0.69318771\n",
      "Step: [25056] d_loss: 1.38629651, g_loss: 0.69354188\n",
      "Step: [25057] d_loss: 1.38627553, g_loss: 0.69315749\n",
      "Step: [25058] d_loss: 1.38629055, g_loss: 0.69320345\n",
      "Step: [25059] d_loss: 1.38629580, g_loss: 0.69312263\n",
      "Step: [25060] d_loss: 1.38620961, g_loss: 0.69332176\n",
      "Step: [25061] d_loss: 1.38629591, g_loss: 0.69327676\n",
      "Step: [25062] d_loss: 1.38627303, g_loss: 0.69324613\n",
      "Step: [25063] d_loss: 1.38626122, g_loss: 0.69302231\n",
      "Step: [25064] d_loss: 1.38627255, g_loss: 0.69300306\n",
      "Step: [25065] d_loss: 1.38628876, g_loss: 0.69292277\n",
      "Step: [25066] d_loss: 1.38629138, g_loss: 0.69302213\n",
      "Step: [25067] d_loss: 1.38625765, g_loss: 0.69331408\n",
      "Step: [25068] d_loss: 1.38626361, g_loss: 0.69332016\n",
      "Step: [25069] d_loss: 1.38623476, g_loss: 0.69335246\n",
      "Step: [25070] d_loss: 1.38629711, g_loss: 0.69311500\n",
      "Step: [25071] d_loss: 1.38636398, g_loss: 0.69374102\n",
      "Step: [25072] d_loss: 1.38634384, g_loss: 0.69233680\n",
      "Step: [25073] d_loss: 1.38626313, g_loss: 0.69284695\n",
      "Step: [25074] d_loss: 1.38532233, g_loss: 0.69519645\n",
      "Step: [25075] d_loss: 1.38639283, g_loss: 0.69332141\n",
      "Step: [25076] d_loss: 1.38648462, g_loss: 0.69147325\n",
      "Step: [25077] d_loss: 1.38650632, g_loss: 0.69078177\n",
      "Step: [25078] d_loss: 1.38657331, g_loss: 0.69148695\n",
      "Step: [25079] d_loss: 1.39051950, g_loss: 0.69157553\n",
      "Step: [25080] d_loss: 1.38647234, g_loss: 0.69220102\n",
      "Step: [25081] d_loss: 1.38645470, g_loss: 0.69319248\n",
      "Step: [25082] d_loss: 1.38643932, g_loss: 0.69383222\n",
      "Step: [25083] d_loss: 1.38643765, g_loss: 0.69341111\n",
      "Step: [25084] d_loss: 1.38643408, g_loss: 0.69379866\n",
      "Step: [25085] d_loss: 1.38644636, g_loss: 0.69268966\n",
      "Step: [25086] d_loss: 1.38645852, g_loss: 0.69330853\n",
      "Step: [25087] d_loss: 1.38648582, g_loss: 0.69480705\n",
      "Step: [25088] d_loss: 1.38651121, g_loss: 0.69663787\n",
      "Step: [25089] d_loss: 1.38661945, g_loss: 0.69411302\n",
      "Step: [25090] d_loss: 1.38663232, g_loss: 0.69418597\n",
      "Step: [25091] d_loss: 1.38669658, g_loss: 0.69409657\n",
      "Step: [25092] d_loss: 1.38669884, g_loss: 0.69604886\n",
      "Step: [25093] d_loss: 1.38674855, g_loss: 0.69615412\n",
      "Step: [25094] d_loss: 1.38673806, g_loss: 0.69237947\n",
      "Step: [25095] d_loss: 1.38669825, g_loss: 0.69028318\n",
      "Step: [25096] d_loss: 1.38668990, g_loss: 0.69346809\n",
      "Step: [25097] d_loss: 1.38668990, g_loss: 0.69673085\n",
      "Step: [25098] d_loss: 1.38664567, g_loss: 0.69767439\n",
      "Step: [25099] d_loss: 1.38661158, g_loss: 0.69491965\n",
      "Step: [25100] d_loss: 1.38651311, g_loss: 0.69340444\n",
      "Step: [25101] d_loss: 1.38646579, g_loss: 0.69358063\n",
      "Step: [25102] d_loss: 1.38634193, g_loss: 0.69412720\n",
      "Step: [25103] d_loss: 1.38630724, g_loss: 0.69370455\n",
      "Step: [25104] d_loss: 1.38630700, g_loss: 0.69295728\n",
      "Step: [25105] d_loss: 1.38630152, g_loss: 0.69215930\n",
      "Step: [25106] d_loss: 1.38626003, g_loss: 0.69287360\n",
      "Step: [25107] d_loss: 1.38623941, g_loss: 0.69406211\n",
      "Step: [25108] d_loss: 1.38626409, g_loss: 0.69383806\n",
      "Step: [25109] d_loss: 1.38652706, g_loss: 0.69140232\n",
      "Step: [25110] d_loss: 1.38770962, g_loss: 0.69307178\n",
      "Step: [25111] d_loss: 1.38908410, g_loss: 0.69166547\n",
      "Step: [25112] d_loss: 1.38939047, g_loss: 0.68281519\n",
      "Step: [25113] d_loss: 1.38896322, g_loss: 0.69352663\n",
      "Step: [25114] d_loss: 1.38797772, g_loss: 0.70133924\n",
      "Step: [25115] d_loss: 1.38732409, g_loss: 0.69717026\n",
      "Step: [25116] d_loss: 1.38684416, g_loss: 0.69205469\n",
      "Step: [25117] d_loss: 1.38651800, g_loss: 0.69032502\n",
      "Step: [25118] d_loss: 1.38635814, g_loss: 0.69218177\n",
      "Step: [25119] d_loss: 1.38629878, g_loss: 0.69340259\n",
      "Step: [25120] d_loss: 1.38628244, g_loss: 0.69389629\n",
      "Step: [25121] d_loss: 1.38627195, g_loss: 0.69367146\n",
      "Step: [25122] d_loss: 1.38627219, g_loss: 0.69231319\n",
      "Step: [25123] d_loss: 1.38633096, g_loss: 0.69391447\n",
      "Step: [25124] d_loss: 1.38635957, g_loss: 0.69476610\n",
      "Step: [25125] d_loss: 1.38661063, g_loss: 0.69235915\n",
      "Step: [25126] d_loss: 1.38666785, g_loss: 0.69387853\n",
      "Step: [25127] d_loss: 1.38662565, g_loss: 0.69313252\n",
      "Step: [25128] d_loss: 1.38651085, g_loss: 0.69375646\n",
      "Step: [25129] d_loss: 1.38638973, g_loss: 0.69422507\n",
      "Step: [25130] d_loss: 1.38631606, g_loss: 0.69348186\n",
      "Step: [25131] d_loss: 1.38629794, g_loss: 0.69273067\n",
      "Step: [25132] d_loss: 1.38627851, g_loss: 0.69278502\n",
      "Step: [25133] d_loss: 1.38626766, g_loss: 0.69311261\n",
      "Step: [25134] d_loss: 1.38628256, g_loss: 0.69347978\n",
      "Step: [25135] d_loss: 1.38641286, g_loss: 0.69348156\n",
      "Step: [25136] d_loss: 1.38674021, g_loss: 0.69158244\n",
      "Step: [25137] d_loss: 1.38679576, g_loss: 0.68989086\n",
      "Step: [25138] d_loss: 1.38664401, g_loss: 0.69136256\n",
      "Step: [25139] d_loss: 1.38671279, g_loss: 0.69484431\n",
      "Step: [25140] d_loss: 1.38669252, g_loss: 0.69569576\n",
      "Step: [25141] d_loss: 1.38753581, g_loss: 0.69755930\n",
      "Step: [25142] d_loss: 1.38790250, g_loss: 0.69625366\n",
      "Step: [25143] d_loss: 1.38729501, g_loss: 0.69709194\n",
      "Step: [25144] d_loss: 1.38673401, g_loss: 0.69545448\n",
      "Step: [25145] d_loss: 1.38631058, g_loss: 0.69343257\n",
      "Step: [25146] d_loss: 1.38629341, g_loss: 0.69324797\n",
      "Step: [25147] d_loss: 1.38629818, g_loss: 0.69251239\n",
      "Step: [25148] d_loss: 1.38629317, g_loss: 0.69240999\n",
      "Step: [25149] d_loss: 1.38628554, g_loss: 0.69343066\n",
      "Step: [25150] d_loss: 1.38628995, g_loss: 0.69337809\n",
      "Step: [25151] d_loss: 1.38628483, g_loss: 0.69302142\n",
      "Step: [25152] d_loss: 1.38630986, g_loss: 0.69307965\n",
      "Step: [25153] d_loss: 1.38630044, g_loss: 0.69380856\n",
      "Step: [25154] d_loss: 1.38630140, g_loss: 0.69243705\n",
      "Step: [25155] d_loss: 1.38631988, g_loss: 0.69275534\n",
      "Step: [25156] d_loss: 1.38633513, g_loss: 0.69229537\n",
      "Step: [25157] d_loss: 1.38632119, g_loss: 0.69311589\n",
      "Step: [25158] d_loss: 1.38629532, g_loss: 0.69338274\n",
      "Step: [25159] d_loss: 1.38629627, g_loss: 0.69333887\n",
      "Step: [25160] d_loss: 1.38629723, g_loss: 0.69341034\n",
      "Step: [25161] d_loss: 1.38629067, g_loss: 0.69336110\n",
      "Step: [25162] d_loss: 1.38629472, g_loss: 0.69300252\n",
      "Step: [25163] d_loss: 1.38628435, g_loss: 0.69258898\n",
      "Step: [25164] d_loss: 1.38629293, g_loss: 0.69288278\n",
      "Step: [25165] d_loss: 1.38627124, g_loss: 0.69333029\n",
      "Step: [25166] d_loss: 1.38628793, g_loss: 0.69343603\n",
      "Step: [25167] d_loss: 1.38628411, g_loss: 0.69331658\n",
      "Step: [25168] d_loss: 1.38627601, g_loss: 0.69305253\n",
      "Step: [25169] d_loss: 1.38629532, g_loss: 0.69194400\n",
      "Step: [25170] d_loss: 1.38630795, g_loss: 0.69283128\n",
      "Step: [25171] d_loss: 1.38631177, g_loss: 0.69273478\n",
      "Step: [25172] d_loss: 1.38634300, g_loss: 0.69297206\n",
      "Step: [25173] d_loss: 1.38628221, g_loss: 0.69369841\n",
      "Step: [25174] d_loss: 1.38630080, g_loss: 0.69362813\n",
      "Step: [25175] d_loss: 1.38633418, g_loss: 0.69226730\n",
      "Step: [25176] d_loss: 1.38630867, g_loss: 0.69270992\n",
      "Step: [25177] d_loss: 1.38630128, g_loss: 0.69395196\n",
      "Step: [25178] d_loss: 1.38627195, g_loss: 0.69347650\n",
      "Step: [25179] d_loss: 1.38629532, g_loss: 0.69333112\n",
      "Step: [25180] d_loss: 1.38628447, g_loss: 0.69288433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25181] d_loss: 1.38629627, g_loss: 0.69296467\n",
      "Step: [25182] d_loss: 1.38626790, g_loss: 0.69281137\n",
      "Step: [25183] d_loss: 1.38628340, g_loss: 0.69333196\n",
      "Step: [25184] d_loss: 1.38631094, g_loss: 0.69385445\n",
      "Step: [25185] d_loss: 1.38627815, g_loss: 0.69293296\n",
      "Step: [25186] d_loss: 1.38630319, g_loss: 0.69295871\n",
      "Step: [25187] d_loss: 1.38629854, g_loss: 0.69320893\n",
      "Step: [25188] d_loss: 1.38627553, g_loss: 0.69298738\n",
      "Step: [25189] d_loss: 1.38629746, g_loss: 0.69329107\n",
      "Step: [25190] d_loss: 1.38629019, g_loss: 0.69348359\n",
      "Step: [25191] d_loss: 1.38629556, g_loss: 0.69289345\n",
      "Step: [25192] d_loss: 1.38628137, g_loss: 0.69271982\n",
      "Step: [25193] d_loss: 1.38629675, g_loss: 0.69286263\n",
      "Step: [25194] d_loss: 1.38630486, g_loss: 0.69303149\n",
      "Step: [25195] d_loss: 1.38628674, g_loss: 0.69360477\n",
      "Step: [25196] d_loss: 1.38628960, g_loss: 0.69346929\n",
      "Step: [25197] d_loss: 1.38629317, g_loss: 0.69311082\n",
      "Step: [25198] d_loss: 1.38627720, g_loss: 0.69299483\n",
      "Step: [25199] d_loss: 1.38629293, g_loss: 0.69291133\n",
      "Step: [25200] d_loss: 1.38628995, g_loss: 0.69326907\n",
      "Step: [25201] d_loss: 1.38628769, g_loss: 0.69312704\n",
      "Step: [25202] d_loss: 1.38629198, g_loss: 0.69326425\n",
      "Step: [25203] d_loss: 1.38629532, g_loss: 0.69305706\n",
      "Step: [25204] d_loss: 1.38627648, g_loss: 0.69297338\n",
      "Step: [25205] d_loss: 1.38634682, g_loss: 0.69289708\n",
      "Step: [25206] d_loss: 1.38663888, g_loss: 0.69175971\n",
      "Step: [25207] d_loss: 1.38690352, g_loss: 0.69374692\n",
      "Step: [25208] d_loss: 1.38683867, g_loss: 0.69792533\n",
      "Step: [25209] d_loss: 1.38657105, g_loss: 0.69607908\n",
      "Step: [25210] d_loss: 1.38640046, g_loss: 0.69319725\n",
      "Step: [25211] d_loss: 1.38628888, g_loss: 0.69205940\n",
      "Step: [25212] d_loss: 1.38629365, g_loss: 0.69313753\n",
      "Step: [25213] d_loss: 1.38638258, g_loss: 0.69206297\n",
      "Step: [25214] d_loss: 1.38634825, g_loss: 0.69325840\n",
      "Step: [25215] d_loss: 1.38635695, g_loss: 0.69349718\n",
      "Step: [25216] d_loss: 1.38631690, g_loss: 0.69339550\n",
      "Step: [25217] d_loss: 1.38628864, g_loss: 0.69276500\n",
      "Step: [25218] d_loss: 1.38630462, g_loss: 0.69332087\n",
      "Step: [25219] d_loss: 1.38630509, g_loss: 0.69345343\n",
      "Step: [25220] d_loss: 1.38628399, g_loss: 0.69325876\n",
      "Step: [25221] d_loss: 1.38630867, g_loss: 0.69231796\n",
      "Step: [25222] d_loss: 1.38632083, g_loss: 0.69238365\n",
      "Step: [25223] d_loss: 1.38687444, g_loss: 0.69281876\n",
      "Step: [25224] d_loss: 1.38752484, g_loss: 0.69194531\n",
      "Step: [25225] d_loss: 1.38748145, g_loss: 0.69502717\n",
      "Step: [25226] d_loss: 1.38698268, g_loss: 0.69807124\n",
      "Step: [25227] d_loss: 1.38652432, g_loss: 0.69607139\n",
      "Step: [25228] d_loss: 1.38634622, g_loss: 0.69339395\n",
      "Step: [25229] d_loss: 1.38629079, g_loss: 0.69208205\n",
      "Step: [25230] d_loss: 1.38628232, g_loss: 0.69211125\n",
      "Step: [25231] d_loss: 1.38629591, g_loss: 0.69265020\n",
      "Step: [25232] d_loss: 1.38629377, g_loss: 0.69335318\n",
      "Step: [25233] d_loss: 1.38629270, g_loss: 0.69371712\n",
      "Step: [25234] d_loss: 1.38628554, g_loss: 0.69309217\n",
      "Step: [25235] d_loss: 1.38629317, g_loss: 0.69172102\n",
      "Step: [25236] d_loss: 1.38631570, g_loss: 0.69297600\n",
      "Step: [25237] d_loss: 1.38632870, g_loss: 0.69298363\n",
      "Step: [25238] d_loss: 1.38633156, g_loss: 0.69338238\n",
      "Step: [25239] d_loss: 1.38630128, g_loss: 0.69390857\n",
      "Step: [25240] d_loss: 1.38628316, g_loss: 0.69312310\n",
      "Step: [25241] d_loss: 1.38629532, g_loss: 0.69233811\n",
      "Step: [25242] d_loss: 1.38628125, g_loss: 0.69280785\n",
      "Step: [25243] d_loss: 1.38629663, g_loss: 0.69317222\n",
      "Step: [25244] d_loss: 1.38627708, g_loss: 0.69336748\n",
      "Step: [25245] d_loss: 1.38629234, g_loss: 0.69311577\n",
      "Step: [25246] d_loss: 1.38626897, g_loss: 0.69279253\n",
      "Step: [25247] d_loss: 1.38629615, g_loss: 0.69372344\n",
      "Step: [25248] d_loss: 1.38629818, g_loss: 0.69250989\n",
      "Step: [25249] d_loss: 1.38631988, g_loss: 0.69312298\n",
      "Step: [25250] d_loss: 1.38635087, g_loss: 0.69215316\n",
      "Step: [25251] d_loss: 1.38635230, g_loss: 0.69236761\n",
      "Step: [25252] d_loss: 1.38636732, g_loss: 0.69164348\n",
      "Step: [25253] d_loss: 1.38635969, g_loss: 0.69323581\n",
      "Step: [25254] d_loss: 1.38632691, g_loss: 0.69416416\n",
      "Step: [25255] d_loss: 1.38630891, g_loss: 0.69360965\n",
      "Step: [25256] d_loss: 1.38628519, g_loss: 0.69398642\n",
      "Step: [25257] d_loss: 1.38640940, g_loss: 0.69232261\n",
      "Step: [25258] d_loss: 1.38635063, g_loss: 0.69284105\n",
      "Step: [25259] d_loss: 1.38632214, g_loss: 0.69444126\n",
      "Step: [25260] d_loss: 1.38631344, g_loss: 0.69340116\n",
      "Step: [25261] d_loss: 1.38629961, g_loss: 0.69331229\n",
      "Step: [25262] d_loss: 1.38628531, g_loss: 0.69346154\n",
      "Step: [25263] d_loss: 1.38629365, g_loss: 0.69246376\n",
      "Step: [25264] d_loss: 1.38628805, g_loss: 0.69331038\n",
      "Step: [25265] d_loss: 1.38631535, g_loss: 0.69279933\n",
      "Step: [25266] d_loss: 1.38631940, g_loss: 0.69300389\n",
      "Step: [25267] d_loss: 1.38630056, g_loss: 0.69280100\n",
      "Step: [25268] d_loss: 1.38631678, g_loss: 0.69200414\n",
      "Step: [25269] d_loss: 1.38631606, g_loss: 0.69219059\n",
      "Step: [25270] d_loss: 1.38641429, g_loss: 0.69195229\n",
      "Step: [25271] d_loss: 1.38653302, g_loss: 0.69314820\n",
      "Step: [25272] d_loss: 1.38655031, g_loss: 0.69323099\n",
      "Step: [25273] d_loss: 1.38646185, g_loss: 0.69386053\n",
      "Step: [25274] d_loss: 1.38642371, g_loss: 0.69229126\n",
      "Step: [25275] d_loss: 1.38635421, g_loss: 0.69322765\n",
      "Step: [25276] d_loss: 1.38631332, g_loss: 0.69363457\n",
      "Step: [25277] d_loss: 1.38630891, g_loss: 0.69272840\n",
      "Step: [25278] d_loss: 1.38630462, g_loss: 0.69257492\n",
      "Step: [25279] d_loss: 1.38628411, g_loss: 0.69333524\n",
      "Step: [25280] d_loss: 1.38630176, g_loss: 0.69297695\n",
      "Step: [25281] d_loss: 1.38629711, g_loss: 0.69310832\n",
      "Step: [25282] d_loss: 1.38630724, g_loss: 0.69281447\n",
      "Step: [25283] d_loss: 1.38627613, g_loss: 0.69309855\n",
      "Step: [25284] d_loss: 1.38629842, g_loss: 0.69355160\n",
      "Step: [25285] d_loss: 1.38595176, g_loss: 0.69413286\n",
      "Step: [25286] d_loss: 1.38632023, g_loss: 0.69289380\n",
      "Step: [25287] d_loss: 1.38634133, g_loss: 0.69290549\n",
      "Step: [25288] d_loss: 1.38630462, g_loss: 0.69340950\n",
      "Step: [25289] d_loss: 1.38630414, g_loss: 0.69341135\n",
      "Step: [25290] d_loss: 1.38628888, g_loss: 0.69425356\n",
      "Step: [25291] d_loss: 1.38638616, g_loss: 0.69469255\n",
      "Step: [25292] d_loss: 1.38684559, g_loss: 0.69226313\n",
      "Step: [25293] d_loss: 1.38699400, g_loss: 0.69267243\n",
      "Step: [25294] d_loss: 1.38692284, g_loss: 0.69011337\n",
      "Step: [25295] d_loss: 1.38675332, g_loss: 0.69209862\n",
      "Step: [25296] d_loss: 1.38657260, g_loss: 0.69312012\n",
      "Step: [25297] d_loss: 1.38641453, g_loss: 0.69430602\n",
      "Step: [25298] d_loss: 1.38637185, g_loss: 0.69388020\n",
      "Step: [25299] d_loss: 1.38636518, g_loss: 0.69342244\n",
      "Step: [25300] d_loss: 1.38646650, g_loss: 0.69193351\n",
      "Step: [25301] d_loss: 1.38646042, g_loss: 0.69199294\n",
      "Step: [25302] d_loss: 1.38626790, g_loss: 0.69212848\n",
      "Step: [25303] d_loss: 1.38638544, g_loss: 0.69208831\n",
      "Step: [25304] d_loss: 1.38692641, g_loss: 0.69432163\n",
      "Step: [25305] d_loss: 1.38632429, g_loss: 0.69407594\n",
      "Step: [25306] d_loss: 1.38631630, g_loss: 0.69406688\n",
      "Step: [25307] d_loss: 1.38629770, g_loss: 0.69300908\n",
      "Step: [25308] d_loss: 1.38629782, g_loss: 0.69260800\n",
      "Step: [25309] d_loss: 1.38631058, g_loss: 0.69269431\n",
      "Step: [25310] d_loss: 1.38631487, g_loss: 0.69309926\n",
      "Step: [25311] d_loss: 1.38630414, g_loss: 0.69397712\n",
      "Step: [25312] d_loss: 1.38632715, g_loss: 0.69329762\n",
      "Step: [25313] d_loss: 1.38635087, g_loss: 0.69324011\n",
      "Step: [25314] d_loss: 1.38639760, g_loss: 0.69268119\n",
      "Step: [25315] d_loss: 1.38639736, g_loss: 0.69159120\n",
      "Step: [25316] d_loss: 1.38637757, g_loss: 0.69256139\n",
      "Step: [25317] d_loss: 1.38636017, g_loss: 0.69239545\n",
      "Step: [25318] d_loss: 1.38631964, g_loss: 0.69319081\n",
      "Step: [25319] d_loss: 1.38631177, g_loss: 0.69367945\n",
      "Step: [25320] d_loss: 1.38630342, g_loss: 0.69356012\n",
      "Step: [25321] d_loss: 1.38628495, g_loss: 0.69319540\n",
      "Step: [25322] d_loss: 1.38628674, g_loss: 0.69292432\n",
      "Step: [25323] d_loss: 1.38630271, g_loss: 0.69296539\n",
      "Step: [25324] d_loss: 1.38629055, g_loss: 0.69304562\n",
      "Step: [25325] d_loss: 1.38629818, g_loss: 0.69317901\n",
      "Step: [25326] d_loss: 1.38630354, g_loss: 0.69314492\n",
      "Step: [25327] d_loss: 1.38628936, g_loss: 0.69320625\n",
      "Step: [25328] d_loss: 1.38547349, g_loss: 0.69604671\n",
      "Step: [25329] d_loss: 1.38632047, g_loss: 0.69219196\n",
      "Step: [25330] d_loss: 1.38629854, g_loss: 0.69297600\n",
      "Step: [25331] d_loss: 1.38629639, g_loss: 0.69329035\n",
      "Step: [25332] d_loss: 1.38630927, g_loss: 0.69299746\n",
      "Step: [25333] d_loss: 1.38629663, g_loss: 0.69325560\n",
      "Step: [25334] d_loss: 1.38627481, g_loss: 0.69307208\n",
      "Step: [25335] d_loss: 1.38630080, g_loss: 0.69296503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25336] d_loss: 1.38628173, g_loss: 0.69282955\n",
      "Step: [25337] d_loss: 1.38629484, g_loss: 0.69313985\n",
      "Step: [25338] d_loss: 1.38620448, g_loss: 0.69272864\n",
      "Step: [25339] d_loss: 1.38630176, g_loss: 0.69264132\n",
      "Step: [25340] d_loss: 1.38645339, g_loss: 0.69391209\n",
      "Step: [25341] d_loss: 1.38648379, g_loss: 0.69255787\n",
      "Step: [25342] d_loss: 1.38651562, g_loss: 0.69353473\n",
      "Step: [25343] d_loss: 1.38648152, g_loss: 0.69392252\n",
      "Step: [25344] d_loss: 1.38637257, g_loss: 0.69436872\n",
      "Step: [25345] d_loss: 1.38635778, g_loss: 0.69328183\n",
      "Step: [25346] d_loss: 1.38630199, g_loss: 0.69321603\n",
      "Step: [25347] d_loss: 1.38631070, g_loss: 0.69303942\n",
      "Step: [25348] d_loss: 1.38616240, g_loss: 0.69272578\n",
      "Step: [25349] d_loss: 1.38628566, g_loss: 0.69339693\n",
      "Step: [25350] d_loss: 1.38629282, g_loss: 0.69317120\n",
      "Step: [25351] d_loss: 1.38626957, g_loss: 0.69295752\n",
      "Step: [25352] d_loss: 1.38626266, g_loss: 0.69294530\n",
      "Step: [25353] d_loss: 1.38633037, g_loss: 0.69318855\n",
      "Step: [25354] d_loss: 1.38629115, g_loss: 0.69348437\n",
      "Step: [25355] d_loss: 1.38631344, g_loss: 0.69325209\n",
      "Step: [25356] d_loss: 1.38632345, g_loss: 0.69332993\n",
      "Step: [25357] d_loss: 1.38631928, g_loss: 0.69347632\n",
      "Step: [25358] d_loss: 1.38630795, g_loss: 0.69306695\n",
      "Step: [25359] d_loss: 1.38626695, g_loss: 0.69283652\n",
      "Step: [25360] d_loss: 1.38629627, g_loss: 0.69303060\n",
      "Step: [25361] d_loss: 1.38628912, g_loss: 0.69313133\n",
      "Step: [25362] d_loss: 1.38633037, g_loss: 0.69325876\n",
      "Step: [25363] d_loss: 1.38612127, g_loss: 0.69308496\n",
      "Step: [25364] d_loss: 1.38630044, g_loss: 0.69279182\n",
      "Step: [25365] d_loss: 1.38628912, g_loss: 0.69287825\n",
      "Step: [25366] d_loss: 1.38622534, g_loss: 0.69336367\n",
      "Step: [25367] d_loss: 1.38628793, g_loss: 0.69368470\n",
      "Step: [25368] d_loss: 1.38631701, g_loss: 0.69367707\n",
      "Step: [25369] d_loss: 1.38638616, g_loss: 0.69273317\n",
      "Step: [25370] d_loss: 1.38665915, g_loss: 0.69189918\n",
      "Step: [25371] d_loss: 1.38677239, g_loss: 0.69079387\n",
      "Step: [25372] d_loss: 1.38667321, g_loss: 0.69106930\n",
      "Step: [25373] d_loss: 1.38655865, g_loss: 0.69318438\n",
      "Step: [25374] d_loss: 1.38643384, g_loss: 0.69448191\n",
      "Step: [25375] d_loss: 1.38639855, g_loss: 0.69409108\n",
      "Step: [25376] d_loss: 1.38631272, g_loss: 0.69382453\n",
      "Step: [25377] d_loss: 1.38632011, g_loss: 0.69288605\n",
      "Step: [25378] d_loss: 1.38628316, g_loss: 0.69388676\n",
      "Step: [25379] d_loss: 1.38631690, g_loss: 0.69312000\n",
      "Step: [25380] d_loss: 1.38632703, g_loss: 0.69319201\n",
      "Step: [25381] d_loss: 1.38630533, g_loss: 0.69411916\n",
      "Step: [25382] d_loss: 1.38642764, g_loss: 0.69495028\n",
      "Step: [25383] d_loss: 1.38684130, g_loss: 0.69294381\n",
      "Step: [25384] d_loss: 1.38662410, g_loss: 0.69460309\n",
      "Step: [25385] d_loss: 1.38635254, g_loss: 0.69339073\n",
      "Step: [25386] d_loss: 1.38640916, g_loss: 0.69218230\n",
      "Step: [25387] d_loss: 1.38652658, g_loss: 0.69227868\n",
      "Step: [25388] d_loss: 1.38659704, g_loss: 0.69336402\n",
      "Step: [25389] d_loss: 1.38655519, g_loss: 0.69451016\n",
      "Step: [25390] d_loss: 1.38643718, g_loss: 0.69263011\n",
      "Step: [25391] d_loss: 1.38646305, g_loss: 0.69078612\n",
      "Step: [25392] d_loss: 1.38651872, g_loss: 0.69074345\n",
      "Step: [25393] d_loss: 1.38636565, g_loss: 0.69245857\n",
      "Step: [25394] d_loss: 1.38632107, g_loss: 0.69364828\n",
      "Step: [25395] d_loss: 1.38632119, g_loss: 0.69416857\n",
      "Step: [25396] d_loss: 1.38628566, g_loss: 0.69353360\n",
      "Step: [25397] d_loss: 1.38629830, g_loss: 0.69259214\n",
      "Step: [25398] d_loss: 1.38627827, g_loss: 0.69226921\n",
      "Step: [25399] d_loss: 1.38631511, g_loss: 0.69291443\n",
      "Step: [25400] d_loss: 1.38626814, g_loss: 0.69254267\n",
      "Step: [25401] d_loss: 1.38666260, g_loss: 0.69020492\n",
      "Step: [25402] d_loss: 1.38663340, g_loss: 0.69083464\n",
      "Step: [25403] d_loss: 1.38660669, g_loss: 0.69267488\n",
      "Step: [25404] d_loss: 1.38653314, g_loss: 0.69457877\n",
      "Step: [25405] d_loss: 1.38644552, g_loss: 0.69583368\n",
      "Step: [25406] d_loss: 1.38637996, g_loss: 0.69437379\n",
      "Step: [25407] d_loss: 1.38634682, g_loss: 0.69257033\n",
      "Step: [25408] d_loss: 1.38631320, g_loss: 0.69255370\n",
      "Step: [25409] d_loss: 1.38629341, g_loss: 0.69295216\n",
      "Step: [25410] d_loss: 1.38629901, g_loss: 0.69348717\n",
      "Step: [25411] d_loss: 1.38632989, g_loss: 0.69363499\n",
      "Step: [25412] d_loss: 1.38629484, g_loss: 0.69303823\n",
      "Step: [25413] d_loss: 1.38629818, g_loss: 0.69297647\n",
      "Step: [25414] d_loss: 1.38629889, g_loss: 0.69319892\n",
      "Step: [25415] d_loss: 1.38633108, g_loss: 0.69328737\n",
      "Step: [25416] d_loss: 1.38627315, g_loss: 0.69321519\n",
      "Step: [25417] d_loss: 1.38632584, g_loss: 0.69330680\n",
      "Step: [25418] d_loss: 1.38630199, g_loss: 0.69398022\n",
      "Step: [25419] d_loss: 1.38632011, g_loss: 0.69338155\n",
      "Step: [25420] d_loss: 1.38632226, g_loss: 0.69334763\n",
      "Step: [25421] d_loss: 1.38631940, g_loss: 0.69282877\n",
      "Step: [25422] d_loss: 1.38631070, g_loss: 0.69302404\n",
      "Step: [25423] d_loss: 1.38630819, g_loss: 0.69300354\n",
      "Step: [25424] d_loss: 1.38629782, g_loss: 0.69306540\n",
      "Step: [25425] d_loss: 1.38629484, g_loss: 0.69342268\n",
      "Step: [25426] d_loss: 1.38630056, g_loss: 0.69342589\n",
      "Step: [25427] d_loss: 1.38640118, g_loss: 0.69327164\n",
      "Step: [25428] d_loss: 1.38631868, g_loss: 0.69293296\n",
      "Step: [25429] d_loss: 1.38627696, g_loss: 0.69376278\n",
      "Step: [25430] d_loss: 1.38636923, g_loss: 0.69349670\n",
      "Step: [25431] d_loss: 1.38633049, g_loss: 0.69358337\n",
      "Step: [25432] d_loss: 1.38635993, g_loss: 0.69376981\n",
      "Step: [25433] d_loss: 1.38632035, g_loss: 0.69214940\n",
      "Step: [25434] d_loss: 1.38640618, g_loss: 0.69224286\n",
      "Step: [25435] d_loss: 1.38630462, g_loss: 0.69260335\n",
      "Step: [25436] d_loss: 1.38632941, g_loss: 0.69331759\n",
      "Step: [25437] d_loss: 1.38631606, g_loss: 0.69355792\n",
      "Step: [25438] d_loss: 1.38630509, g_loss: 0.69353396\n",
      "Step: [25439] d_loss: 1.38631392, g_loss: 0.69309592\n",
      "Step: [25440] d_loss: 1.38639474, g_loss: 0.69319057\n",
      "Step: [25441] d_loss: 1.38631296, g_loss: 0.69297826\n",
      "Step: [25442] d_loss: 1.38626230, g_loss: 0.69313270\n",
      "Step: [25443] d_loss: 1.38626194, g_loss: 0.69318271\n",
      "Step: [25444] d_loss: 1.38628745, g_loss: 0.69308275\n",
      "Step: [25445] d_loss: 1.38636637, g_loss: 0.69292760\n",
      "Step: [25446] d_loss: 1.38667738, g_loss: 0.69407678\n",
      "Step: [25447] d_loss: 1.38683641, g_loss: 0.69281453\n",
      "Step: [25448] d_loss: 1.38698602, g_loss: 0.69363099\n",
      "Step: [25449] d_loss: 1.38696313, g_loss: 0.69416428\n",
      "Step: [25450] d_loss: 1.38685489, g_loss: 0.69344509\n",
      "Step: [25451] d_loss: 1.38673544, g_loss: 0.69195187\n",
      "Step: [25452] d_loss: 1.38654685, g_loss: 0.69156504\n",
      "Step: [25453] d_loss: 1.38643575, g_loss: 0.69310540\n",
      "Step: [25454] d_loss: 1.38637280, g_loss: 0.69286811\n",
      "Step: [25455] d_loss: 1.38640094, g_loss: 0.69269323\n",
      "Step: [25456] d_loss: 1.38630140, g_loss: 0.69233060\n",
      "Step: [25457] d_loss: 1.38628125, g_loss: 0.69340122\n",
      "Step: [25458] d_loss: 1.38628006, g_loss: 0.69337809\n",
      "Step: [25459] d_loss: 1.38627744, g_loss: 0.69310081\n",
      "Step: [25460] d_loss: 1.38639712, g_loss: 0.69300401\n",
      "Step: [25461] d_loss: 1.38627994, g_loss: 0.69285536\n",
      "Step: [25462] d_loss: 1.38629937, g_loss: 0.69309914\n",
      "Step: [25463] d_loss: 1.38628674, g_loss: 0.69310910\n",
      "Step: [25464] d_loss: 1.38629198, g_loss: 0.69327545\n",
      "Step: [25465] d_loss: 1.38634706, g_loss: 0.69360846\n",
      "Step: [25466] d_loss: 1.38640583, g_loss: 0.69340849\n",
      "Step: [25467] d_loss: 1.38627934, g_loss: 0.69282329\n",
      "Step: [25468] d_loss: 1.38627577, g_loss: 0.69340628\n",
      "Step: [25469] d_loss: 1.38641596, g_loss: 0.69355971\n",
      "Step: [25470] d_loss: 1.38676000, g_loss: 0.69396514\n",
      "Step: [25471] d_loss: 1.38698173, g_loss: 0.69610220\n",
      "Step: [25472] d_loss: 1.38697839, g_loss: 0.69504595\n",
      "Step: [25473] d_loss: 1.38687527, g_loss: 0.69472623\n",
      "Step: [25474] d_loss: 1.38671565, g_loss: 0.69236666\n",
      "Step: [25475] d_loss: 1.38656974, g_loss: 0.69117379\n",
      "Step: [25476] d_loss: 1.38661218, g_loss: 0.69104159\n",
      "Step: [25477] d_loss: 1.38642001, g_loss: 0.69159049\n",
      "Step: [25478] d_loss: 1.38640428, g_loss: 0.69343805\n",
      "Step: [25479] d_loss: 1.38651109, g_loss: 0.69444871\n",
      "Step: [25480] d_loss: 1.38653314, g_loss: 0.69440711\n",
      "Step: [25481] d_loss: 1.38646698, g_loss: 0.69387913\n",
      "Step: [25482] d_loss: 1.38642979, g_loss: 0.69245988\n",
      "Step: [25483] d_loss: 1.38642991, g_loss: 0.69286168\n",
      "Step: [25484] d_loss: 1.38641930, g_loss: 0.69313824\n",
      "Step: [25485] d_loss: 1.38645792, g_loss: 0.69497591\n",
      "Step: [25486] d_loss: 1.38636923, g_loss: 0.69461292\n",
      "Step: [25487] d_loss: 1.38634348, g_loss: 0.69194448\n",
      "Step: [25488] d_loss: 1.38639915, g_loss: 0.69138932\n",
      "Step: [25489] d_loss: 1.38633239, g_loss: 0.69228280\n",
      "Step: [25490] d_loss: 1.38631964, g_loss: 0.69297302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25491] d_loss: 1.38632607, g_loss: 0.69368863\n",
      "Step: [25492] d_loss: 1.38628721, g_loss: 0.69366336\n",
      "Step: [25493] d_loss: 1.38633478, g_loss: 0.69403589\n",
      "Step: [25494] d_loss: 1.38626051, g_loss: 0.69334352\n",
      "Step: [25495] d_loss: 1.38628149, g_loss: 0.69263697\n",
      "Step: [25496] d_loss: 1.38628864, g_loss: 0.69316757\n",
      "Step: [25497] d_loss: 1.38623834, g_loss: 0.69399661\n",
      "Step: [25498] d_loss: 1.38627410, g_loss: 0.69335675\n",
      "Step: [25499] d_loss: 1.38635159, g_loss: 0.69483787\n",
      "Step: [25500] d_loss: 1.38626707, g_loss: 0.69305909\n",
      "Step: [25501] d_loss: 1.38649201, g_loss: 0.69432354\n",
      "Step: [25502] d_loss: 1.38709283, g_loss: 0.69200730\n",
      "Step: [25503] d_loss: 1.38778996, g_loss: 0.69112051\n",
      "Step: [25504] d_loss: 1.38770723, g_loss: 0.69017565\n",
      "Step: [25505] d_loss: 1.38729012, g_loss: 0.69224465\n",
      "Step: [25506] d_loss: 1.38685560, g_loss: 0.69500434\n",
      "Step: [25507] d_loss: 1.38652790, g_loss: 0.69561791\n",
      "Step: [25508] d_loss: 1.38629043, g_loss: 0.69329244\n",
      "Step: [25509] d_loss: 1.38645172, g_loss: 0.69420886\n",
      "Step: [25510] d_loss: 1.38667917, g_loss: 0.69365954\n",
      "Step: [25511] d_loss: 1.38826978, g_loss: 0.69680667\n",
      "Step: [25512] d_loss: 1.38941264, g_loss: 0.69101989\n",
      "Step: [25513] d_loss: 1.38893771, g_loss: 0.69511139\n",
      "Step: [25514] d_loss: 1.38767087, g_loss: 0.69949406\n",
      "Step: [25515] d_loss: 1.38675451, g_loss: 0.69911110\n",
      "Step: [25516] d_loss: 1.38640666, g_loss: 0.69574702\n",
      "Step: [25517] d_loss: 1.38630199, g_loss: 0.69196141\n",
      "Step: [25518] d_loss: 1.38627648, g_loss: 0.69105351\n",
      "Step: [25519] d_loss: 1.38633633, g_loss: 0.69229919\n",
      "Step: [25520] d_loss: 1.38625646, g_loss: 0.69382644\n",
      "Step: [25521] d_loss: 1.38624454, g_loss: 0.69397044\n",
      "Step: [25522] d_loss: 1.38627493, g_loss: 0.69327521\n",
      "Step: [25523] d_loss: 1.38627028, g_loss: 0.69309837\n",
      "Step: [25524] d_loss: 1.38638794, g_loss: 0.69294369\n",
      "Step: [25525] d_loss: 1.38661218, g_loss: 0.69284236\n",
      "Step: [25526] d_loss: 1.38660324, g_loss: 0.69412148\n",
      "Step: [25527] d_loss: 1.38645864, g_loss: 0.69482780\n",
      "Step: [25528] d_loss: 1.38632989, g_loss: 0.69506133\n",
      "Step: [25529] d_loss: 1.38630009, g_loss: 0.69299126\n",
      "Step: [25530] d_loss: 1.38631868, g_loss: 0.69271696\n",
      "Step: [25531] d_loss: 1.38631177, g_loss: 0.69212908\n",
      "Step: [25532] d_loss: 1.38630974, g_loss: 0.69260418\n",
      "Step: [25533] d_loss: 1.38632417, g_loss: 0.69311708\n",
      "Step: [25534] d_loss: 1.38628626, g_loss: 0.69321859\n",
      "Step: [25535] d_loss: 1.38629425, g_loss: 0.69307375\n",
      "Step: [25536] d_loss: 1.38626933, g_loss: 0.69304514\n",
      "Step: [25537] d_loss: 1.38626790, g_loss: 0.69321156\n",
      "Step: [25538] d_loss: 1.38630009, g_loss: 0.69315660\n",
      "Step: [25539] d_loss: 1.38625515, g_loss: 0.69310689\n",
      "Step: [25540] d_loss: 1.38610458, g_loss: 0.69345295\n",
      "Step: [25541] d_loss: 1.38621199, g_loss: 0.69336069\n",
      "Step: [25542] d_loss: 1.38628662, g_loss: 0.69358325\n",
      "Step: [25543] d_loss: 1.38625455, g_loss: 0.69296205\n",
      "Step: [25544] d_loss: 1.38628221, g_loss: 0.69282383\n",
      "Step: [25545] d_loss: 1.38629913, g_loss: 0.69257218\n",
      "Step: [25546] d_loss: 1.38632309, g_loss: 0.69344652\n",
      "Step: [25547] d_loss: 1.38628030, g_loss: 0.69316757\n",
      "Step: [25548] d_loss: 1.38629556, g_loss: 0.69360399\n",
      "Step: [25549] d_loss: 1.38656926, g_loss: 0.69520581\n",
      "Step: [25550] d_loss: 1.38647079, g_loss: 0.69395399\n",
      "Step: [25551] d_loss: 1.38644361, g_loss: 0.69373810\n",
      "Step: [25552] d_loss: 1.38647628, g_loss: 0.69157344\n",
      "Step: [25553] d_loss: 1.38644457, g_loss: 0.69170010\n",
      "Step: [25554] d_loss: 1.38636518, g_loss: 0.69314504\n",
      "Step: [25555] d_loss: 1.38632154, g_loss: 0.69347507\n",
      "Step: [25556] d_loss: 1.38626337, g_loss: 0.69326669\n",
      "Step: [25557] d_loss: 1.38627231, g_loss: 0.69324726\n",
      "Step: [25558] d_loss: 1.38627195, g_loss: 0.69325662\n",
      "Step: [25559] d_loss: 1.38626659, g_loss: 0.69318599\n",
      "Step: [25560] d_loss: 1.38624763, g_loss: 0.69310367\n",
      "Step: [25561] d_loss: 1.38629055, g_loss: 0.69317120\n",
      "Step: [25562] d_loss: 1.38629246, g_loss: 0.69325125\n",
      "Step: [25563] d_loss: 1.38629186, g_loss: 0.69316286\n",
      "Step: [25564] d_loss: 1.38632274, g_loss: 0.69332349\n",
      "Step: [25565] d_loss: 1.38627791, g_loss: 0.69332325\n",
      "Step: [25566] d_loss: 1.38628626, g_loss: 0.69301742\n",
      "Step: [25567] d_loss: 1.38633859, g_loss: 0.69305193\n",
      "Step: [25568] d_loss: 1.38626969, g_loss: 0.69297397\n",
      "Step: [25569] d_loss: 1.38639951, g_loss: 0.69360435\n",
      "Step: [25570] d_loss: 1.38606620, g_loss: 0.69323885\n",
      "Step: [25571] d_loss: 1.38649452, g_loss: 0.69098133\n",
      "Step: [25572] d_loss: 1.38640189, g_loss: 0.69248366\n",
      "Step: [25573] d_loss: 1.38632226, g_loss: 0.69317257\n",
      "Step: [25574] d_loss: 1.38631999, g_loss: 0.69388527\n",
      "Step: [25575] d_loss: 1.38628101, g_loss: 0.69400698\n",
      "Step: [25576] d_loss: 1.38543785, g_loss: 0.69503546\n",
      "Step: [25577] d_loss: 1.38635874, g_loss: 0.69292474\n",
      "Step: [25578] d_loss: 1.38644719, g_loss: 0.69414103\n",
      "Step: [25579] d_loss: 1.38640738, g_loss: 0.69383591\n",
      "Step: [25580] d_loss: 1.38638771, g_loss: 0.69136512\n",
      "Step: [25581] d_loss: 1.38654315, g_loss: 0.69174385\n",
      "Step: [25582] d_loss: 1.38666940, g_loss: 0.69483316\n",
      "Step: [25583] d_loss: 1.38658881, g_loss: 0.69543409\n",
      "Step: [25584] d_loss: 1.38648105, g_loss: 0.69542420\n",
      "Step: [25585] d_loss: 1.38632941, g_loss: 0.69438756\n",
      "Step: [25586] d_loss: 1.38626146, g_loss: 0.69198912\n",
      "Step: [25587] d_loss: 1.38630509, g_loss: 0.69159943\n",
      "Step: [25588] d_loss: 1.38627470, g_loss: 0.69238991\n",
      "Step: [25589] d_loss: 1.38624024, g_loss: 0.69350410\n",
      "Step: [25590] d_loss: 1.38627982, g_loss: 0.69375151\n",
      "Step: [25591] d_loss: 1.38626170, g_loss: 0.69353449\n",
      "Step: [25592] d_loss: 1.38602757, g_loss: 0.69632167\n",
      "Step: [25593] d_loss: 1.38644993, g_loss: 0.69263780\n",
      "Step: [25594] d_loss: 1.38644218, g_loss: 0.69317043\n",
      "Step: [25595] d_loss: 1.38701952, g_loss: 0.69498497\n",
      "Step: [25596] d_loss: 1.38627267, g_loss: 0.69475687\n",
      "Step: [25597] d_loss: 1.38626146, g_loss: 0.69331717\n",
      "Step: [25598] d_loss: 1.38618588, g_loss: 0.69239300\n",
      "Step: [25599] d_loss: 1.38626552, g_loss: 0.69278437\n",
      "Step: [25600] d_loss: 1.38625515, g_loss: 0.69273555\n",
      "Step: [25601] d_loss: 1.38625824, g_loss: 0.69370806\n",
      "Step: [25602] d_loss: 1.38619232, g_loss: 0.69353092\n",
      "Step: [25603] d_loss: 1.38634801, g_loss: 0.69351494\n",
      "Step: [25604] d_loss: 1.38632631, g_loss: 0.69419605\n",
      "Step: [25605] d_loss: 1.38631368, g_loss: 0.69352341\n",
      "Step: [25606] d_loss: 1.38631988, g_loss: 0.69314033\n",
      "Step: [25607] d_loss: 1.38619995, g_loss: 0.69329739\n",
      "Step: [25608] d_loss: 1.38621688, g_loss: 0.69297552\n",
      "Step: [25609] d_loss: 1.38624513, g_loss: 0.69354975\n",
      "Step: [25610] d_loss: 1.38623857, g_loss: 0.69295633\n",
      "Step: [25611] d_loss: 1.38629961, g_loss: 0.69302928\n",
      "Step: [25612] d_loss: 1.38636065, g_loss: 0.69297653\n",
      "Step: [25613] d_loss: 1.38631022, g_loss: 0.69323909\n",
      "Step: [25614] d_loss: 1.38630581, g_loss: 0.69338208\n",
      "Step: [25615] d_loss: 1.38626266, g_loss: 0.69322729\n",
      "Step: [25616] d_loss: 1.38643277, g_loss: 0.69311905\n",
      "Step: [25617] d_loss: 1.38630509, g_loss: 0.69314480\n",
      "Step: [25618] d_loss: 1.38630068, g_loss: 0.69397616\n",
      "Step: [25619] d_loss: 1.38621736, g_loss: 0.69332844\n",
      "Step: [25620] d_loss: 1.38626575, g_loss: 0.69316649\n",
      "Step: [25621] d_loss: 1.38621783, g_loss: 0.69335938\n",
      "Step: [25622] d_loss: 1.38621235, g_loss: 0.69287348\n",
      "Step: [25623] d_loss: 1.38624167, g_loss: 0.69308478\n",
      "Step: [25624] d_loss: 1.38629639, g_loss: 0.69223624\n",
      "Step: [25625] d_loss: 1.38620186, g_loss: 0.69228232\n",
      "Step: [25626] d_loss: 1.38634109, g_loss: 0.69251436\n",
      "Step: [25627] d_loss: 1.38629961, g_loss: 0.69358373\n",
      "Step: [25628] d_loss: 1.38628793, g_loss: 0.69313180\n",
      "Step: [25629] d_loss: 1.38628340, g_loss: 0.69276166\n",
      "Step: [25630] d_loss: 1.38659525, g_loss: 0.69236374\n",
      "Step: [25631] d_loss: 1.38668156, g_loss: 0.69224536\n",
      "Step: [25632] d_loss: 1.38671231, g_loss: 0.69237155\n",
      "Step: [25633] d_loss: 1.38661540, g_loss: 0.69448495\n",
      "Step: [25634] d_loss: 1.38650405, g_loss: 0.69412869\n",
      "Step: [25635] d_loss: 1.38637424, g_loss: 0.69248694\n",
      "Step: [25636] d_loss: 1.38631487, g_loss: 0.69219816\n",
      "Step: [25637] d_loss: 1.38626790, g_loss: 0.69277668\n",
      "Step: [25638] d_loss: 1.38629282, g_loss: 0.69347906\n",
      "Step: [25639] d_loss: 1.38626266, g_loss: 0.69344085\n",
      "Step: [25640] d_loss: 1.38627315, g_loss: 0.69318390\n",
      "Step: [25641] d_loss: 1.38630533, g_loss: 0.69338340\n",
      "Step: [25642] d_loss: 1.38629103, g_loss: 0.69281137\n",
      "Step: [25643] d_loss: 1.38629198, g_loss: 0.69317544\n",
      "Step: [25644] d_loss: 1.38638079, g_loss: 0.69353151\n",
      "Step: [25645] d_loss: 1.38625574, g_loss: 0.69383395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25646] d_loss: 1.38635409, g_loss: 0.69345069\n",
      "Step: [25647] d_loss: 1.38636327, g_loss: 0.69375598\n",
      "Step: [25648] d_loss: 1.38664389, g_loss: 0.69590247\n",
      "Step: [25649] d_loss: 1.38710117, g_loss: 0.69455647\n",
      "Step: [25650] d_loss: 1.38737929, g_loss: 0.69337511\n",
      "Step: [25651] d_loss: 1.38720167, g_loss: 0.69736171\n",
      "Step: [25652] d_loss: 1.38676119, g_loss: 0.69693619\n",
      "Step: [25653] d_loss: 1.38678730, g_loss: 0.69466007\n",
      "Step: [25654] d_loss: 1.38674593, g_loss: 0.69293356\n",
      "Step: [25655] d_loss: 1.38653827, g_loss: 0.69474214\n",
      "Step: [25656] d_loss: 1.38637376, g_loss: 0.69426596\n",
      "Step: [25657] d_loss: 1.38625669, g_loss: 0.69403982\n",
      "Step: [25658] d_loss: 1.38630962, g_loss: 0.69273913\n",
      "Step: [25659] d_loss: 1.38625801, g_loss: 0.69187117\n",
      "Step: [25660] d_loss: 1.38641369, g_loss: 0.69221222\n",
      "Step: [25661] d_loss: 1.38626087, g_loss: 0.69382435\n",
      "Step: [25662] d_loss: 1.38627458, g_loss: 0.69335347\n",
      "Step: [25663] d_loss: 1.38627100, g_loss: 0.69306767\n",
      "Step: [25664] d_loss: 1.38627672, g_loss: 0.69329107\n",
      "Step: [25665] d_loss: 1.38622570, g_loss: 0.69337219\n",
      "Step: [25666] d_loss: 1.38625193, g_loss: 0.69306344\n",
      "Step: [25667] d_loss: 1.38633513, g_loss: 0.69292420\n",
      "Step: [25668] d_loss: 1.38632178, g_loss: 0.69301432\n",
      "Step: [25669] d_loss: 1.38670969, g_loss: 0.69396806\n",
      "Step: [25670] d_loss: 1.38632977, g_loss: 0.69447452\n",
      "Step: [25671] d_loss: 1.38631225, g_loss: 0.69214422\n",
      "Step: [25672] d_loss: 1.38627958, g_loss: 0.69102967\n",
      "Step: [25673] d_loss: 1.38749897, g_loss: 0.69191796\n",
      "Step: [25674] d_loss: 1.38643861, g_loss: 0.69176513\n",
      "Step: [25675] d_loss: 1.38649940, g_loss: 0.69359064\n",
      "Step: [25676] d_loss: 1.38642299, g_loss: 0.69358599\n",
      "Step: [25677] d_loss: 1.38637269, g_loss: 0.69320631\n",
      "Step: [25678] d_loss: 1.38628697, g_loss: 0.69164544\n",
      "Step: [25679] d_loss: 1.38629162, g_loss: 0.69250345\n",
      "Step: [25680] d_loss: 1.38628602, g_loss: 0.69399631\n",
      "Step: [25681] d_loss: 1.38629818, g_loss: 0.69289374\n",
      "Step: [25682] d_loss: 1.38632941, g_loss: 0.69335687\n",
      "Step: [25683] d_loss: 1.38632667, g_loss: 0.69311601\n",
      "Step: [25684] d_loss: 1.38628054, g_loss: 0.69334656\n",
      "Step: [25685] d_loss: 1.38623834, g_loss: 0.69351995\n",
      "Step: [25686] d_loss: 1.38627720, g_loss: 0.69331437\n",
      "Step: [25687] d_loss: 1.38628745, g_loss: 0.69435465\n",
      "Step: [25688] d_loss: 1.38626695, g_loss: 0.69188130\n",
      "Step: [25689] d_loss: 1.38647461, g_loss: 0.69285238\n",
      "Step: [25690] d_loss: 1.38659883, g_loss: 0.69297069\n",
      "Step: [25691] d_loss: 1.38652968, g_loss: 0.69341075\n",
      "Step: [25692] d_loss: 1.38642764, g_loss: 0.69330776\n",
      "Step: [25693] d_loss: 1.38633192, g_loss: 0.69339287\n",
      "Step: [25694] d_loss: 1.38629699, g_loss: 0.69342375\n",
      "Step: [25695] d_loss: 1.38627887, g_loss: 0.69312525\n",
      "Step: [25696] d_loss: 1.38626838, g_loss: 0.69301277\n",
      "Step: [25697] d_loss: 1.38621032, g_loss: 0.69261128\n",
      "Step: [25698] d_loss: 1.38628173, g_loss: 0.69300359\n",
      "Step: [25699] d_loss: 1.38625765, g_loss: 0.69343609\n",
      "Step: [25700] d_loss: 1.38631082, g_loss: 0.69113398\n",
      "Step: [25701] d_loss: 1.38626742, g_loss: 0.69000787\n",
      "Step: [25702] d_loss: 1.38630915, g_loss: 0.69208086\n",
      "Step: [25703] d_loss: 1.38633919, g_loss: 0.69345289\n",
      "Step: [25704] d_loss: 1.38634062, g_loss: 0.69416463\n",
      "Step: [25705] d_loss: 1.38626528, g_loss: 0.69366795\n",
      "Step: [25706] d_loss: 1.38625479, g_loss: 0.69329536\n",
      "Step: [25707] d_loss: 1.38626862, g_loss: 0.69300377\n",
      "Step: [25708] d_loss: 1.38627124, g_loss: 0.69299209\n",
      "Step: [25709] d_loss: 1.38625574, g_loss: 0.69304687\n",
      "Step: [25710] d_loss: 1.38629794, g_loss: 0.69310236\n",
      "Step: [25711] d_loss: 1.38623095, g_loss: 0.69330895\n",
      "Step: [25712] d_loss: 1.38625669, g_loss: 0.69328797\n",
      "Step: [25713] d_loss: 1.38628352, g_loss: 0.69304085\n",
      "Step: [25714] d_loss: 1.38630009, g_loss: 0.69285953\n",
      "Step: [25715] d_loss: 1.38623810, g_loss: 0.69315982\n",
      "Step: [25716] d_loss: 1.38626456, g_loss: 0.69334871\n",
      "Step: [25717] d_loss: 1.38627827, g_loss: 0.69324237\n",
      "Step: [25718] d_loss: 1.38612080, g_loss: 0.69316602\n",
      "Step: [25719] d_loss: 1.38627481, g_loss: 0.69302279\n",
      "Step: [25720] d_loss: 1.38622713, g_loss: 0.69213849\n",
      "Step: [25721] d_loss: 1.38625932, g_loss: 0.69294298\n",
      "Step: [25722] d_loss: 1.38630271, g_loss: 0.69340813\n",
      "Step: [25723] d_loss: 1.38633680, g_loss: 0.69340509\n",
      "Step: [25724] d_loss: 1.38630676, g_loss: 0.69315881\n",
      "Step: [25725] d_loss: 1.38629019, g_loss: 0.69358170\n",
      "Step: [25726] d_loss: 1.38624048, g_loss: 0.69338900\n",
      "Step: [25727] d_loss: 1.38626385, g_loss: 0.69303572\n",
      "Step: [25728] d_loss: 1.38626850, g_loss: 0.69317341\n",
      "Step: [25729] d_loss: 1.38622975, g_loss: 0.69308627\n",
      "Step: [25730] d_loss: 1.38626409, g_loss: 0.69319016\n",
      "Step: [25731] d_loss: 1.38606095, g_loss: 0.69316918\n",
      "Step: [25732] d_loss: 1.38625145, g_loss: 0.69282138\n",
      "Step: [25733] d_loss: 1.38616443, g_loss: 0.69316208\n",
      "Step: [25734] d_loss: 1.38619256, g_loss: 0.69279027\n",
      "Step: [25735] d_loss: 1.38633227, g_loss: 0.69456524\n",
      "Step: [25736] d_loss: 1.38640738, g_loss: 0.69525933\n",
      "Step: [25737] d_loss: 1.38663626, g_loss: 0.69301826\n",
      "Step: [25738] d_loss: 1.38643515, g_loss: 0.69144797\n",
      "Step: [25739] d_loss: 1.38621819, g_loss: 0.69365966\n",
      "Step: [25740] d_loss: 1.38637638, g_loss: 0.69400746\n",
      "Step: [25741] d_loss: 1.38681734, g_loss: 0.69528878\n",
      "Step: [25742] d_loss: 1.38699317, g_loss: 0.69341111\n",
      "Step: [25743] d_loss: 1.38674247, g_loss: 0.69521391\n",
      "Step: [25744] d_loss: 1.38666260, g_loss: 0.69307584\n",
      "Step: [25745] d_loss: 1.38652992, g_loss: 0.69209725\n",
      "Step: [25746] d_loss: 1.38640785, g_loss: 0.69268692\n",
      "Step: [25747] d_loss: 1.38626766, g_loss: 0.69327688\n",
      "Step: [25748] d_loss: 1.38633299, g_loss: 0.69315845\n",
      "Step: [25749] d_loss: 1.38621855, g_loss: 0.69298625\n",
      "Step: [25750] d_loss: 1.38628280, g_loss: 0.69316959\n",
      "Step: [25751] d_loss: 1.38629436, g_loss: 0.69342285\n",
      "Step: [25752] d_loss: 1.38627076, g_loss: 0.69330180\n",
      "Step: [25753] d_loss: 1.38630712, g_loss: 0.69315779\n",
      "Step: [25754] d_loss: 1.38628256, g_loss: 0.69288605\n",
      "Step: [25755] d_loss: 1.38620281, g_loss: 0.69288260\n",
      "Step: [25756] d_loss: 1.38621545, g_loss: 0.69307941\n",
      "Step: [25757] d_loss: 1.38624454, g_loss: 0.69342589\n",
      "Step: [25758] d_loss: 1.38624942, g_loss: 0.69374961\n",
      "Step: [25759] d_loss: 1.38626838, g_loss: 0.69287169\n",
      "Step: [25760] d_loss: 1.38623965, g_loss: 0.69289291\n",
      "Step: [25761] d_loss: 1.38621891, g_loss: 0.69248146\n",
      "Step: [25762] d_loss: 1.38630271, g_loss: 0.69353056\n",
      "Step: [25763] d_loss: 1.38632202, g_loss: 0.69313163\n",
      "Step: [25764] d_loss: 1.38620830, g_loss: 0.69317555\n",
      "Step: [25765] d_loss: 1.38613605, g_loss: 0.69280195\n",
      "Step: [25766] d_loss: 1.38622975, g_loss: 0.69286823\n",
      "Step: [25767] d_loss: 1.38626194, g_loss: 0.69353259\n",
      "Step: [25768] d_loss: 1.38625419, g_loss: 0.69241893\n",
      "Step: [25769] d_loss: 1.38637805, g_loss: 0.69357604\n",
      "Step: [25770] d_loss: 1.38662124, g_loss: 0.69338918\n",
      "Step: [25771] d_loss: 1.38703501, g_loss: 0.69318360\n",
      "Step: [25772] d_loss: 1.38700151, g_loss: 0.69712454\n",
      "Step: [25773] d_loss: 1.38685894, g_loss: 0.69655532\n",
      "Step: [25774] d_loss: 1.38652515, g_loss: 0.69504458\n",
      "Step: [25775] d_loss: 1.38612771, g_loss: 0.69481963\n",
      "Step: [25776] d_loss: 1.38636887, g_loss: 0.69298816\n",
      "Step: [25777] d_loss: 1.38634324, g_loss: 0.69265831\n",
      "Step: [25778] d_loss: 1.38636041, g_loss: 0.69306970\n",
      "Step: [25779] d_loss: 1.38636613, g_loss: 0.69214451\n",
      "Step: [25780] d_loss: 1.38632059, g_loss: 0.69348800\n",
      "Step: [25781] d_loss: 1.38619614, g_loss: 0.69333076\n",
      "Step: [25782] d_loss: 1.38623500, g_loss: 0.69317877\n",
      "Step: [25783] d_loss: 1.38629317, g_loss: 0.69356316\n",
      "Step: [25784] d_loss: 1.38623083, g_loss: 0.69267678\n",
      "Step: [25785] d_loss: 1.38630557, g_loss: 0.69298738\n",
      "Step: [25786] d_loss: 1.38628232, g_loss: 0.69358552\n",
      "Step: [25787] d_loss: 1.38635206, g_loss: 0.69313908\n",
      "Step: [25788] d_loss: 1.38645792, g_loss: 0.69391775\n",
      "Step: [25789] d_loss: 1.38648438, g_loss: 0.69436687\n",
      "Step: [25790] d_loss: 1.38636398, g_loss: 0.69417912\n",
      "Step: [25791] d_loss: 1.38645935, g_loss: 0.69332707\n",
      "Step: [25792] d_loss: 1.38602781, g_loss: 0.69360554\n",
      "Step: [25793] d_loss: 1.38625658, g_loss: 0.69333518\n",
      "Step: [25794] d_loss: 1.38625121, g_loss: 0.69295233\n",
      "Step: [25795] d_loss: 1.38628316, g_loss: 0.69291377\n",
      "Step: [25796] d_loss: 1.38625419, g_loss: 0.69342101\n",
      "Step: [25797] d_loss: 1.38626885, g_loss: 0.69313562\n",
      "Step: [25798] d_loss: 1.38628066, g_loss: 0.69296372\n",
      "Step: [25799] d_loss: 1.38627577, g_loss: 0.69207454\n",
      "Step: [25800] d_loss: 1.38629878, g_loss: 0.69343656\n",
      "Step: [25801] d_loss: 1.38624966, g_loss: 0.69341123\n",
      "Step: [25802] d_loss: 1.38620579, g_loss: 0.69340539\n",
      "Step: [25803] d_loss: 1.38639224, g_loss: 0.69273984\n",
      "Step: [25804] d_loss: 1.38625944, g_loss: 0.69299436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25805] d_loss: 1.38642681, g_loss: 0.69370580\n",
      "Step: [25806] d_loss: 1.38751674, g_loss: 0.69705582\n",
      "Step: [25807] d_loss: 1.38969970, g_loss: 0.69973010\n",
      "Step: [25808] d_loss: 1.39153051, g_loss: 0.69125640\n",
      "Step: [25809] d_loss: 1.39089334, g_loss: 0.68713135\n",
      "Step: [25810] d_loss: 1.38761139, g_loss: 0.69307196\n",
      "Step: [25811] d_loss: 1.38667870, g_loss: 0.69745630\n",
      "Step: [25812] d_loss: 1.38631105, g_loss: 0.69411588\n",
      "Step: [25813] d_loss: 1.38625324, g_loss: 0.69170702\n",
      "Step: [25814] d_loss: 1.38623071, g_loss: 0.69146872\n",
      "Step: [25815] d_loss: 1.38629436, g_loss: 0.69261163\n",
      "Step: [25816] d_loss: 1.38620710, g_loss: 0.69203073\n",
      "Step: [25817] d_loss: 1.38623893, g_loss: 0.69255996\n",
      "Step: [25818] d_loss: 1.38632715, g_loss: 0.69295555\n",
      "Step: [25819] d_loss: 1.38634849, g_loss: 0.69368100\n",
      "Step: [25820] d_loss: 1.38670278, g_loss: 0.69353539\n",
      "Step: [25821] d_loss: 1.38686109, g_loss: 0.69173443\n",
      "Step: [25822] d_loss: 1.38671601, g_loss: 0.69317502\n",
      "Step: [25823] d_loss: 1.38589025, g_loss: 0.69466007\n",
      "Step: [25824] d_loss: 1.38627911, g_loss: 0.69267124\n",
      "Step: [25825] d_loss: 1.38626623, g_loss: 0.69304448\n",
      "Step: [25826] d_loss: 1.38622952, g_loss: 0.69345766\n",
      "Step: [25827] d_loss: 1.38628590, g_loss: 0.69311500\n",
      "Step: [25828] d_loss: 1.38672709, g_loss: 0.69255662\n",
      "Step: [25829] d_loss: 1.38629460, g_loss: 0.69250458\n",
      "Step: [25830] d_loss: 1.38643253, g_loss: 0.69231915\n",
      "Step: [25831] d_loss: 1.38785613, g_loss: 0.69296139\n",
      "Step: [25832] d_loss: 1.38638830, g_loss: 0.69060814\n",
      "Step: [25833] d_loss: 1.38630033, g_loss: 0.69250238\n",
      "Step: [25834] d_loss: 1.38629496, g_loss: 0.69355166\n",
      "Step: [25835] d_loss: 1.38628054, g_loss: 0.69387090\n",
      "Step: [25836] d_loss: 1.38627100, g_loss: 0.69383061\n",
      "Step: [25837] d_loss: 1.38623142, g_loss: 0.69196510\n",
      "Step: [25838] d_loss: 1.38626826, g_loss: 0.69233632\n",
      "Step: [25839] d_loss: 1.38636708, g_loss: 0.69330513\n",
      "Step: [25840] d_loss: 1.38673854, g_loss: 0.69488084\n",
      "Step: [25841] d_loss: 1.38696980, g_loss: 0.69651759\n",
      "Step: [25842] d_loss: 1.38644111, g_loss: 0.69247979\n",
      "Step: [25843] d_loss: 1.38627052, g_loss: 0.69114506\n",
      "Step: [25844] d_loss: 1.38636506, g_loss: 0.69298863\n",
      "Step: [25845] d_loss: 1.38636100, g_loss: 0.69379431\n",
      "Step: [25846] d_loss: 1.38625038, g_loss: 0.69431520\n",
      "Step: [25847] d_loss: 1.38754272, g_loss: 0.69355500\n",
      "Step: [25848] d_loss: 1.38637376, g_loss: 0.69440591\n",
      "Step: [25849] d_loss: 1.38774788, g_loss: 0.69088137\n",
      "Step: [25850] d_loss: 1.38693333, g_loss: 0.69081032\n",
      "Step: [25851] d_loss: 1.38659072, g_loss: 0.69276732\n",
      "Step: [25852] d_loss: 1.38634598, g_loss: 0.69452727\n",
      "Step: [25853] d_loss: 1.38626623, g_loss: 0.69456446\n",
      "Step: [25854] d_loss: 1.38625407, g_loss: 0.69348097\n",
      "Step: [25855] d_loss: 1.38629472, g_loss: 0.69143862\n",
      "Step: [25856] d_loss: 1.38629389, g_loss: 0.69134355\n",
      "Step: [25857] d_loss: 1.38692141, g_loss: 0.69214177\n",
      "Step: [25858] d_loss: 1.38732409, g_loss: 0.69265205\n",
      "Step: [25859] d_loss: 1.38742375, g_loss: 0.69254655\n",
      "Step: [25860] d_loss: 1.38699675, g_loss: 0.69373912\n",
      "Step: [25861] d_loss: 1.38643956, g_loss: 0.69320846\n",
      "Step: [25862] d_loss: 1.38626289, g_loss: 0.69397986\n",
      "Step: [25863] d_loss: 1.38698018, g_loss: 0.69526547\n",
      "Step: [25864] d_loss: 1.38732648, g_loss: 0.69301337\n",
      "Step: [25865] d_loss: 1.38678026, g_loss: 0.69385934\n",
      "Step: [25866] d_loss: 1.38631642, g_loss: 0.69489455\n",
      "Step: [25867] d_loss: 1.38624859, g_loss: 0.69405031\n",
      "Step: [25868] d_loss: 1.38631868, g_loss: 0.69242311\n",
      "Step: [25869] d_loss: 1.38629198, g_loss: 0.69247323\n",
      "Step: [25870] d_loss: 1.38626814, g_loss: 0.69303441\n",
      "Step: [25871] d_loss: 1.38624477, g_loss: 0.69343472\n",
      "Step: [25872] d_loss: 1.38627970, g_loss: 0.69340080\n",
      "Step: [25873] d_loss: 1.38621891, g_loss: 0.69331104\n",
      "Step: [25874] d_loss: 1.38628948, g_loss: 0.69310713\n",
      "Step: [25875] d_loss: 1.38624418, g_loss: 0.69293767\n",
      "Step: [25876] d_loss: 1.38628042, g_loss: 0.69306397\n",
      "Step: [25877] d_loss: 1.38627744, g_loss: 0.69310009\n",
      "Step: [25878] d_loss: 1.38629937, g_loss: 0.69310427\n",
      "Step: [25879] d_loss: 1.38626909, g_loss: 0.69339585\n",
      "Step: [25880] d_loss: 1.38628244, g_loss: 0.69330013\n",
      "Step: [25881] d_loss: 1.38627052, g_loss: 0.69315398\n",
      "Step: [25882] d_loss: 1.38627696, g_loss: 0.69305855\n",
      "Step: [25883] d_loss: 1.38627505, g_loss: 0.69310403\n",
      "Step: [25884] d_loss: 1.38630199, g_loss: 0.69317454\n",
      "Step: [25885] d_loss: 1.38625455, g_loss: 0.69331545\n",
      "Step: [25886] d_loss: 1.38625252, g_loss: 0.69306850\n",
      "Step: [25887] d_loss: 1.38629985, g_loss: 0.69313943\n",
      "Step: [25888] d_loss: 1.38627124, g_loss: 0.69288385\n",
      "Step: [25889] d_loss: 1.38640320, g_loss: 0.69163412\n",
      "Step: [25890] d_loss: 1.38636732, g_loss: 0.69227087\n",
      "Step: [25891] d_loss: 1.38632417, g_loss: 0.69307375\n",
      "Step: [25892] d_loss: 1.38628113, g_loss: 0.69359744\n",
      "Step: [25893] d_loss: 1.38630009, g_loss: 0.69368339\n",
      "Step: [25894] d_loss: 1.38627052, g_loss: 0.69297469\n",
      "Step: [25895] d_loss: 1.38627696, g_loss: 0.69299096\n",
      "Step: [25896] d_loss: 1.38631749, g_loss: 0.69364411\n",
      "Step: [25897] d_loss: 1.38626552, g_loss: 0.69325429\n",
      "Step: [25898] d_loss: 1.38631034, g_loss: 0.69316745\n",
      "Step: [25899] d_loss: 1.38623524, g_loss: 0.69380558\n",
      "Step: [25900] d_loss: 1.38637304, g_loss: 0.69282711\n",
      "Step: [25901] d_loss: 1.38638520, g_loss: 0.69322073\n",
      "Step: [25902] d_loss: 1.38637471, g_loss: 0.69161820\n",
      "Step: [25903] d_loss: 1.38642824, g_loss: 0.69104588\n",
      "Step: [25904] d_loss: 1.38640106, g_loss: 0.69361067\n",
      "Step: [25905] d_loss: 1.38626099, g_loss: 0.69374454\n",
      "Step: [25906] d_loss: 1.38629079, g_loss: 0.69338083\n",
      "Step: [25907] d_loss: 1.38662243, g_loss: 0.69379812\n",
      "Step: [25908] d_loss: 1.38638568, g_loss: 0.69271660\n",
      "Step: [25909] d_loss: 1.38633311, g_loss: 0.69163144\n",
      "Step: [25910] d_loss: 1.38643050, g_loss: 0.69281650\n",
      "Step: [25911] d_loss: 1.38632298, g_loss: 0.69401449\n",
      "Step: [25912] d_loss: 1.38628912, g_loss: 0.69326103\n",
      "Step: [25913] d_loss: 1.38630795, g_loss: 0.69339347\n",
      "Step: [25914] d_loss: 1.38629031, g_loss: 0.69308400\n",
      "Step: [25915] d_loss: 1.38630247, g_loss: 0.69297916\n",
      "Step: [25916] d_loss: 1.38627577, g_loss: 0.69323277\n",
      "Step: [25917] d_loss: 1.38637543, g_loss: 0.69345748\n",
      "Step: [25918] d_loss: 1.38629985, g_loss: 0.69289333\n",
      "Step: [25919] d_loss: 1.38629341, g_loss: 0.69335413\n",
      "Step: [25920] d_loss: 1.38629603, g_loss: 0.69394130\n",
      "Step: [25921] d_loss: 1.38634026, g_loss: 0.69453382\n",
      "Step: [25922] d_loss: 1.38625944, g_loss: 0.69616932\n",
      "Step: [25923] d_loss: 1.38638031, g_loss: 0.69442594\n",
      "Step: [25924] d_loss: 1.38632464, g_loss: 0.69337147\n",
      "Step: [25925] d_loss: 1.38642335, g_loss: 0.69264066\n",
      "Step: [25926] d_loss: 1.38629675, g_loss: 0.69267255\n",
      "Step: [25927] d_loss: 1.38631546, g_loss: 0.69327819\n",
      "Step: [25928] d_loss: 1.38631749, g_loss: 0.69351202\n",
      "Step: [25929] d_loss: 1.38629246, g_loss: 0.69317949\n",
      "Step: [25930] d_loss: 1.38632905, g_loss: 0.69352978\n",
      "Step: [25931] d_loss: 1.38630760, g_loss: 0.69262838\n",
      "Step: [25932] d_loss: 1.38631439, g_loss: 0.69309735\n",
      "Step: [25933] d_loss: 1.38628995, g_loss: 0.69360340\n",
      "Step: [25934] d_loss: 1.38628995, g_loss: 0.69325930\n",
      "Step: [25935] d_loss: 1.38631463, g_loss: 0.69310129\n",
      "Step: [25936] d_loss: 1.38630998, g_loss: 0.69305223\n",
      "Step: [25937] d_loss: 1.38614416, g_loss: 0.69382364\n",
      "Step: [25938] d_loss: 1.38631487, g_loss: 0.69248688\n",
      "Step: [25939] d_loss: 1.38634729, g_loss: 0.69330657\n",
      "Step: [25940] d_loss: 1.38628268, g_loss: 0.69367319\n",
      "Step: [25941] d_loss: 1.38628018, g_loss: 0.69344735\n",
      "Step: [25942] d_loss: 1.38629580, g_loss: 0.69308358\n",
      "Step: [25943] d_loss: 1.38633573, g_loss: 0.69311440\n",
      "Step: [25944] d_loss: 1.38631010, g_loss: 0.69304609\n",
      "Step: [25945] d_loss: 1.38629448, g_loss: 0.69300592\n",
      "Step: [25946] d_loss: 1.38630223, g_loss: 0.69322145\n",
      "Step: [25947] d_loss: 1.38624585, g_loss: 0.69374728\n",
      "Step: [25948] d_loss: 1.38631880, g_loss: 0.69342113\n",
      "Step: [25949] d_loss: 1.38630414, g_loss: 0.69341266\n",
      "Step: [25950] d_loss: 1.38629103, g_loss: 0.69304329\n",
      "Step: [25951] d_loss: 1.38611710, g_loss: 0.69420516\n",
      "Step: [25952] d_loss: 1.38629806, g_loss: 0.69294661\n",
      "Step: [25953] d_loss: 1.38628578, g_loss: 0.69301724\n",
      "Step: [25954] d_loss: 1.38629627, g_loss: 0.69313705\n",
      "Step: [25955] d_loss: 1.38627601, g_loss: 0.69345641\n",
      "Step: [25956] d_loss: 1.38627887, g_loss: 0.69310373\n",
      "Step: [25957] d_loss: 1.38624334, g_loss: 0.69329745\n",
      "Step: [25958] d_loss: 1.38629746, g_loss: 0.69316769\n",
      "Step: [25959] d_loss: 1.38627207, g_loss: 0.69321567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [25960] d_loss: 1.38628197, g_loss: 0.69289070\n",
      "Step: [25961] d_loss: 1.38626742, g_loss: 0.69352829\n",
      "Step: [25962] d_loss: 1.38629436, g_loss: 0.69316638\n",
      "Step: [25963] d_loss: 1.38630104, g_loss: 0.69317663\n",
      "Step: [25964] d_loss: 1.38629746, g_loss: 0.69302118\n",
      "Step: [25965] d_loss: 1.38629222, g_loss: 0.69298911\n",
      "Step: [25966] d_loss: 1.38614416, g_loss: 0.69517237\n",
      "Step: [25967] d_loss: 1.38653183, g_loss: 0.69347739\n",
      "Step: [25968] d_loss: 1.38719463, g_loss: 0.69169879\n",
      "Step: [25969] d_loss: 1.38720441, g_loss: 0.69209540\n",
      "Step: [25970] d_loss: 1.38673282, g_loss: 0.69584638\n",
      "Step: [25971] d_loss: 1.38653767, g_loss: 0.69536877\n",
      "Step: [25972] d_loss: 1.38634300, g_loss: 0.69409889\n",
      "Step: [25973] d_loss: 1.38629055, g_loss: 0.69295108\n",
      "Step: [25974] d_loss: 1.38629127, g_loss: 0.69226408\n",
      "Step: [25975] d_loss: 1.38630819, g_loss: 0.69271600\n",
      "Step: [25976] d_loss: 1.38627529, g_loss: 0.69328266\n",
      "Step: [25977] d_loss: 1.38628471, g_loss: 0.69380558\n",
      "Step: [25978] d_loss: 1.38626695, g_loss: 0.69280994\n",
      "Step: [25979] d_loss: 1.38630271, g_loss: 0.69297093\n",
      "Step: [25980] d_loss: 1.38638020, g_loss: 0.69255525\n",
      "Step: [25981] d_loss: 1.38634849, g_loss: 0.69266522\n",
      "Step: [25982] d_loss: 1.38631344, g_loss: 0.69373763\n",
      "Step: [25983] d_loss: 1.38632512, g_loss: 0.69393986\n",
      "Step: [25984] d_loss: 1.38652718, g_loss: 0.69308954\n",
      "Step: [25985] d_loss: 1.38657153, g_loss: 0.69183993\n",
      "Step: [25986] d_loss: 1.38646364, g_loss: 0.69210434\n",
      "Step: [25987] d_loss: 1.38635349, g_loss: 0.69437754\n",
      "Step: [25988] d_loss: 1.38630545, g_loss: 0.69471389\n",
      "Step: [25989] d_loss: 1.38628936, g_loss: 0.69358575\n",
      "Step: [25990] d_loss: 1.38628006, g_loss: 0.69258475\n",
      "Step: [25991] d_loss: 1.38634610, g_loss: 0.69272077\n",
      "Step: [25992] d_loss: 1.38628364, g_loss: 0.69330108\n",
      "Step: [25993] d_loss: 1.38633311, g_loss: 0.69359386\n",
      "Step: [25994] d_loss: 1.38630021, g_loss: 0.69320905\n",
      "Step: [25995] d_loss: 1.38628936, g_loss: 0.69299591\n",
      "Step: [25996] d_loss: 1.38627613, g_loss: 0.69301796\n",
      "Step: [25997] d_loss: 1.38627172, g_loss: 0.69388652\n",
      "Step: [25998] d_loss: 1.38635492, g_loss: 0.69368500\n",
      "Step: [25999] d_loss: 1.38671410, g_loss: 0.69417179\n",
      "Step: [26000] d_loss: 1.38692880, g_loss: 0.69196546\n",
      "Step: [26001] d_loss: 1.38731086, g_loss: 0.69313252\n",
      "Step: [26002] d_loss: 1.38648069, g_loss: 0.68884653\n",
      "Step: [26003] d_loss: 1.38626206, g_loss: 0.69272053\n",
      "Step: [26004] d_loss: 1.38632572, g_loss: 0.69574606\n",
      "Step: [26005] d_loss: 1.38631940, g_loss: 0.69698244\n",
      "Step: [26006] d_loss: 1.38632858, g_loss: 0.69370729\n",
      "Step: [26007] d_loss: 1.38630486, g_loss: 0.69195604\n",
      "Step: [26008] d_loss: 1.38628483, g_loss: 0.69265866\n",
      "Step: [26009] d_loss: 1.38627219, g_loss: 0.69351304\n",
      "Step: [26010] d_loss: 1.38630617, g_loss: 0.69290209\n",
      "Step: [26011] d_loss: 1.38628304, g_loss: 0.69340146\n",
      "Step: [26012] d_loss: 1.38648319, g_loss: 0.68994856\n",
      "Step: [26013] d_loss: 1.38649940, g_loss: 0.69056022\n",
      "Step: [26014] d_loss: 1.38668215, g_loss: 0.69322824\n",
      "Step: [26015] d_loss: 1.38716185, g_loss: 0.70545685\n",
      "Step: [26016] d_loss: 1.38726807, g_loss: 0.70225728\n",
      "Step: [26017] d_loss: 1.38683772, g_loss: 0.69586229\n",
      "Step: [26018] d_loss: 1.38644218, g_loss: 0.69174415\n",
      "Step: [26019] d_loss: 1.38630903, g_loss: 0.69098741\n",
      "Step: [26020] d_loss: 1.38630962, g_loss: 0.69043750\n",
      "Step: [26021] d_loss: 1.38634121, g_loss: 0.69272119\n",
      "Step: [26022] d_loss: 1.38633633, g_loss: 0.69401664\n",
      "Step: [26023] d_loss: 1.38633525, g_loss: 0.69408160\n",
      "Step: [26024] d_loss: 1.38628042, g_loss: 0.69406807\n",
      "Step: [26025] d_loss: 1.38628721, g_loss: 0.69289517\n",
      "Step: [26026] d_loss: 1.38616276, g_loss: 0.69188893\n",
      "Step: [26027] d_loss: 1.38629866, g_loss: 0.69293535\n",
      "Step: [26028] d_loss: 1.38629770, g_loss: 0.69329882\n",
      "Step: [26029] d_loss: 1.38627434, g_loss: 0.69334531\n",
      "Step: [26030] d_loss: 1.38629603, g_loss: 0.69218606\n",
      "Step: [26031] d_loss: 1.38634753, g_loss: 0.69288635\n",
      "Step: [26032] d_loss: 1.38629413, g_loss: 0.69316119\n",
      "Step: [26033] d_loss: 1.38640249, g_loss: 0.69269168\n",
      "Step: [26034] d_loss: 1.38655090, g_loss: 0.69371855\n",
      "Step: [26035] d_loss: 1.38654864, g_loss: 0.69191623\n",
      "Step: [26036] d_loss: 1.38677597, g_loss: 0.69394505\n",
      "Step: [26037] d_loss: 1.38718629, g_loss: 0.69388759\n",
      "Step: [26038] d_loss: 1.38725579, g_loss: 0.69124877\n",
      "Step: [26039] d_loss: 1.38679063, g_loss: 0.69325709\n",
      "Step: [26040] d_loss: 1.38639235, g_loss: 0.69453061\n",
      "Step: [26041] d_loss: 1.38629699, g_loss: 0.69442654\n",
      "Step: [26042] d_loss: 1.38630342, g_loss: 0.69348663\n",
      "Step: [26043] d_loss: 1.38630319, g_loss: 0.69189847\n",
      "Step: [26044] d_loss: 1.38626885, g_loss: 0.69237757\n",
      "Step: [26045] d_loss: 1.38627625, g_loss: 0.69312167\n",
      "Step: [26046] d_loss: 1.38633752, g_loss: 0.69339383\n",
      "Step: [26047] d_loss: 1.38652825, g_loss: 0.69306117\n",
      "Step: [26048] d_loss: 1.38650727, g_loss: 0.69389868\n",
      "Step: [26049] d_loss: 1.38638318, g_loss: 0.69361657\n",
      "Step: [26050] d_loss: 1.38634121, g_loss: 0.69318938\n",
      "Step: [26051] d_loss: 1.38626695, g_loss: 0.69283414\n",
      "Step: [26052] d_loss: 1.38628757, g_loss: 0.69278097\n",
      "Step: [26053] d_loss: 1.38629198, g_loss: 0.69318569\n",
      "Step: [26054] d_loss: 1.38626766, g_loss: 0.69276017\n",
      "Step: [26055] d_loss: 1.38630676, g_loss: 0.69310188\n",
      "Step: [26056] d_loss: 1.38629425, g_loss: 0.69320208\n",
      "Step: [26057] d_loss: 1.38629007, g_loss: 0.69321394\n",
      "Step: [26058] d_loss: 1.38628078, g_loss: 0.69339347\n",
      "Step: [26059] d_loss: 1.38626719, g_loss: 0.69308341\n",
      "Step: [26060] d_loss: 1.38627434, g_loss: 0.69275707\n",
      "Step: [26061] d_loss: 1.38629270, g_loss: 0.69303298\n",
      "Step: [26062] d_loss: 1.38626552, g_loss: 0.69347471\n",
      "Step: [26063] d_loss: 1.38628054, g_loss: 0.69316077\n",
      "Step: [26064] d_loss: 1.38628256, g_loss: 0.69304234\n",
      "Step: [26065] d_loss: 1.38622522, g_loss: 0.69246542\n",
      "Step: [26066] d_loss: 1.38629913, g_loss: 0.69311261\n",
      "Step: [26067] d_loss: 1.38628757, g_loss: 0.69298637\n",
      "Step: [26068] d_loss: 1.38626385, g_loss: 0.69335687\n",
      "Step: [26069] d_loss: 1.38628685, g_loss: 0.69330966\n",
      "Step: [26070] d_loss: 1.38630652, g_loss: 0.69290185\n",
      "Step: [26071] d_loss: 1.38628721, g_loss: 0.69310218\n",
      "Step: [26072] d_loss: 1.38632011, g_loss: 0.69254053\n",
      "Step: [26073] d_loss: 1.38643789, g_loss: 0.69403052\n",
      "Step: [26074] d_loss: 1.38667560, g_loss: 0.69471723\n",
      "Step: [26075] d_loss: 1.38609123, g_loss: 0.69624066\n",
      "Step: [26076] d_loss: 1.38670671, g_loss: 0.69498789\n",
      "Step: [26077] d_loss: 1.38724804, g_loss: 0.69461375\n",
      "Step: [26078] d_loss: 1.38779175, g_loss: 0.68978596\n",
      "Step: [26079] d_loss: 1.38707662, g_loss: 0.69065404\n",
      "Step: [26080] d_loss: 1.38658094, g_loss: 0.69241118\n",
      "Step: [26081] d_loss: 1.38631320, g_loss: 0.69378716\n",
      "Step: [26082] d_loss: 1.38628125, g_loss: 0.69394994\n",
      "Step: [26083] d_loss: 1.38670826, g_loss: 0.69465643\n",
      "Step: [26084] d_loss: 1.38645291, g_loss: 0.69313097\n",
      "Step: [26085] d_loss: 1.38661671, g_loss: 0.69155419\n",
      "Step: [26086] d_loss: 1.38654125, g_loss: 0.69149482\n",
      "Step: [26087] d_loss: 1.38640475, g_loss: 0.69323987\n",
      "Step: [26088] d_loss: 1.38625491, g_loss: 0.69458640\n",
      "Step: [26089] d_loss: 1.38626480, g_loss: 0.69371819\n",
      "Step: [26090] d_loss: 1.38633752, g_loss: 0.69363123\n",
      "Step: [26091] d_loss: 1.38631332, g_loss: 0.69330823\n",
      "Step: [26092] d_loss: 1.38633895, g_loss: 0.69287711\n",
      "Step: [26093] d_loss: 1.38625968, g_loss: 0.69230831\n",
      "Step: [26094] d_loss: 1.38631785, g_loss: 0.69256890\n",
      "Step: [26095] d_loss: 1.38630295, g_loss: 0.69336987\n",
      "Step: [26096] d_loss: 1.38627839, g_loss: 0.69378364\n",
      "Step: [26097] d_loss: 1.38629031, g_loss: 0.69344026\n",
      "Step: [26098] d_loss: 1.38627100, g_loss: 0.69306225\n",
      "Step: [26099] d_loss: 1.38628078, g_loss: 0.69298857\n",
      "Step: [26100] d_loss: 1.38629425, g_loss: 0.69305664\n",
      "Step: [26101] d_loss: 1.38627219, g_loss: 0.69315112\n",
      "Step: [26102] d_loss: 1.38629568, g_loss: 0.69325233\n",
      "Step: [26103] d_loss: 1.38623905, g_loss: 0.69315726\n",
      "Step: [26104] d_loss: 1.38625383, g_loss: 0.69323564\n",
      "Step: [26105] d_loss: 1.38654900, g_loss: 0.69376564\n",
      "Step: [26106] d_loss: 1.38699138, g_loss: 0.69306678\n",
      "Step: [26107] d_loss: 1.38689518, g_loss: 0.69510758\n",
      "Step: [26108] d_loss: 1.38652992, g_loss: 0.69417357\n",
      "Step: [26109] d_loss: 1.38677752, g_loss: 0.69068372\n",
      "Step: [26110] d_loss: 1.38716030, g_loss: 0.68944514\n",
      "Step: [26111] d_loss: 1.38685453, g_loss: 0.69172060\n",
      "Step: [26112] d_loss: 1.38644433, g_loss: 0.69378436\n",
      "Step: [26113] d_loss: 1.38628697, g_loss: 0.69387901\n",
      "Step: [26114] d_loss: 1.38636041, g_loss: 0.69422466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26115] d_loss: 1.38626671, g_loss: 0.69344187\n",
      "Step: [26116] d_loss: 1.38632512, g_loss: 0.69330251\n",
      "Step: [26117] d_loss: 1.38633084, g_loss: 0.69335866\n",
      "Step: [26118] d_loss: 1.38627946, g_loss: 0.69325477\n",
      "Step: [26119] d_loss: 1.38624096, g_loss: 0.69313157\n",
      "Step: [26120] d_loss: 1.38623428, g_loss: 0.69306386\n",
      "Step: [26121] d_loss: 1.38628256, g_loss: 0.69305301\n",
      "Step: [26122] d_loss: 1.38628983, g_loss: 0.69306695\n",
      "Step: [26123] d_loss: 1.38622618, g_loss: 0.69328833\n",
      "Step: [26124] d_loss: 1.38626552, g_loss: 0.69326925\n",
      "Step: [26125] d_loss: 1.38626492, g_loss: 0.69315147\n",
      "Step: [26126] d_loss: 1.38628256, g_loss: 0.69310784\n",
      "Step: [26127] d_loss: 1.38611460, g_loss: 0.69320035\n",
      "Step: [26128] d_loss: 1.38627315, g_loss: 0.69308060\n",
      "Step: [26129] d_loss: 1.38629079, g_loss: 0.69314957\n",
      "Step: [26130] d_loss: 1.38627696, g_loss: 0.69313598\n",
      "Step: [26131] d_loss: 1.38629675, g_loss: 0.69303602\n",
      "Step: [26132] d_loss: 1.38629436, g_loss: 0.69315195\n",
      "Step: [26133] d_loss: 1.38628185, g_loss: 0.69315219\n",
      "Step: [26134] d_loss: 1.38628459, g_loss: 0.69313586\n",
      "Step: [26135] d_loss: 1.38629556, g_loss: 0.69317210\n",
      "Step: [26136] d_loss: 1.38629341, g_loss: 0.69313675\n",
      "Step: [26137] d_loss: 1.38624763, g_loss: 0.69307315\n",
      "Step: [26138] d_loss: 1.38632631, g_loss: 0.69294488\n",
      "Step: [26139] d_loss: 1.38627338, g_loss: 0.69308919\n",
      "Step: [26140] d_loss: 1.38625503, g_loss: 0.69318748\n",
      "Step: [26141] d_loss: 1.38626719, g_loss: 0.69303721\n",
      "Step: [26142] d_loss: 1.38634872, g_loss: 0.69310820\n",
      "Step: [26143] d_loss: 1.38674188, g_loss: 0.69681776\n",
      "Step: [26144] d_loss: 1.38701844, g_loss: 0.69277585\n",
      "Step: [26145] d_loss: 1.38675129, g_loss: 0.69000542\n",
      "Step: [26146] d_loss: 1.38643622, g_loss: 0.69041789\n",
      "Step: [26147] d_loss: 1.38629723, g_loss: 0.69348747\n",
      "Step: [26148] d_loss: 1.38626552, g_loss: 0.69531608\n",
      "Step: [26149] d_loss: 1.38623500, g_loss: 0.69340479\n",
      "Step: [26150] d_loss: 1.38627946, g_loss: 0.69283938\n",
      "Step: [26151] d_loss: 1.38642454, g_loss: 0.69304436\n",
      "Step: [26152] d_loss: 1.38637114, g_loss: 0.69082582\n",
      "Step: [26153] d_loss: 1.38655090, g_loss: 0.69103146\n",
      "Step: [26154] d_loss: 1.38649499, g_loss: 0.69241297\n",
      "Step: [26155] d_loss: 1.38639605, g_loss: 0.69312686\n",
      "Step: [26156] d_loss: 1.38648760, g_loss: 0.69313264\n",
      "Step: [26157] d_loss: 1.38629436, g_loss: 0.69328725\n",
      "Step: [26158] d_loss: 1.38626409, g_loss: 0.69330239\n",
      "Step: [26159] d_loss: 1.38626051, g_loss: 0.69314623\n",
      "Step: [26160] d_loss: 1.38625908, g_loss: 0.69326651\n",
      "Step: [26161] d_loss: 1.38626349, g_loss: 0.69304633\n",
      "Step: [26162] d_loss: 1.38629866, g_loss: 0.69339728\n",
      "Step: [26163] d_loss: 1.38640761, g_loss: 0.69281685\n",
      "Step: [26164] d_loss: 1.38640714, g_loss: 0.69258398\n",
      "Step: [26165] d_loss: 1.38632238, g_loss: 0.69276500\n",
      "Step: [26166] d_loss: 1.38628983, g_loss: 0.69293147\n",
      "Step: [26167] d_loss: 1.38626075, g_loss: 0.69359362\n",
      "Step: [26168] d_loss: 1.38645327, g_loss: 0.69328129\n",
      "Step: [26169] d_loss: 1.38676620, g_loss: 0.69456017\n",
      "Step: [26170] d_loss: 1.38667536, g_loss: 0.69462049\n",
      "Step: [26171] d_loss: 1.38645375, g_loss: 0.69382316\n",
      "Step: [26172] d_loss: 1.38630271, g_loss: 0.69286531\n",
      "Step: [26173] d_loss: 1.38628602, g_loss: 0.69257838\n",
      "Step: [26174] d_loss: 1.38625324, g_loss: 0.69290727\n",
      "Step: [26175] d_loss: 1.38625526, g_loss: 0.69383061\n",
      "Step: [26176] d_loss: 1.38629127, g_loss: 0.69390827\n",
      "Step: [26177] d_loss: 1.38637924, g_loss: 0.69327235\n",
      "Step: [26178] d_loss: 1.38642263, g_loss: 0.69228572\n",
      "Step: [26179] d_loss: 1.38630331, g_loss: 0.69182968\n",
      "Step: [26180] d_loss: 1.38638341, g_loss: 0.69435167\n",
      "Step: [26181] d_loss: 1.38648486, g_loss: 0.69527447\n",
      "Step: [26182] d_loss: 1.38650584, g_loss: 0.69478071\n",
      "Step: [26183] d_loss: 1.38638401, g_loss: 0.69400096\n",
      "Step: [26184] d_loss: 1.38628685, g_loss: 0.69279259\n",
      "Step: [26185] d_loss: 1.38635039, g_loss: 0.69176495\n",
      "Step: [26186] d_loss: 1.38647830, g_loss: 0.69235283\n",
      "Step: [26187] d_loss: 1.38637757, g_loss: 0.69363022\n",
      "Step: [26188] d_loss: 1.38628626, g_loss: 0.69407839\n",
      "Step: [26189] d_loss: 1.38628697, g_loss: 0.69381171\n",
      "Step: [26190] d_loss: 1.38630295, g_loss: 0.69340289\n",
      "Step: [26191] d_loss: 1.38630819, g_loss: 0.69300497\n",
      "Step: [26192] d_loss: 1.38625896, g_loss: 0.69298959\n",
      "Step: [26193] d_loss: 1.38627863, g_loss: 0.69289196\n",
      "Step: [26194] d_loss: 1.38630140, g_loss: 0.69300592\n",
      "Step: [26195] d_loss: 1.38625395, g_loss: 0.69326234\n",
      "Step: [26196] d_loss: 1.38630688, g_loss: 0.69330978\n",
      "Step: [26197] d_loss: 1.38630581, g_loss: 0.69325435\n",
      "Step: [26198] d_loss: 1.38630807, g_loss: 0.69317269\n",
      "Step: [26199] d_loss: 1.38629651, g_loss: 0.69310951\n",
      "Step: [26200] d_loss: 1.38628697, g_loss: 0.69314241\n",
      "Step: [26201] d_loss: 1.38632798, g_loss: 0.69354033\n",
      "Step: [26202] d_loss: 1.38626003, g_loss: 0.69325900\n",
      "Step: [26203] d_loss: 1.38628244, g_loss: 0.69325697\n",
      "Step: [26204] d_loss: 1.38627219, g_loss: 0.69318336\n",
      "Step: [26205] d_loss: 1.38611794, g_loss: 0.69378173\n",
      "Step: [26206] d_loss: 1.38625956, g_loss: 0.69301629\n",
      "Step: [26207] d_loss: 1.38628268, g_loss: 0.69309586\n",
      "Step: [26208] d_loss: 1.38628602, g_loss: 0.69321585\n",
      "Step: [26209] d_loss: 1.38627100, g_loss: 0.69321734\n",
      "Step: [26210] d_loss: 1.38640380, g_loss: 0.69254994\n",
      "Step: [26211] d_loss: 1.38629270, g_loss: 0.69302386\n",
      "Step: [26212] d_loss: 1.38625932, g_loss: 0.69315231\n",
      "Step: [26213] d_loss: 1.38631380, g_loss: 0.69313574\n",
      "Step: [26214] d_loss: 1.38629127, g_loss: 0.69313622\n",
      "Step: [26215] d_loss: 1.38627720, g_loss: 0.69311607\n",
      "Step: [26216] d_loss: 1.38626754, g_loss: 0.69334948\n",
      "Step: [26217] d_loss: 1.38636446, g_loss: 0.69349903\n",
      "Step: [26218] d_loss: 1.38697481, g_loss: 0.69475555\n",
      "Step: [26219] d_loss: 1.38732243, g_loss: 0.69726729\n",
      "Step: [26220] d_loss: 1.38764644, g_loss: 0.69074523\n",
      "Step: [26221] d_loss: 1.38856244, g_loss: 0.68786454\n",
      "Step: [26222] d_loss: 1.38865662, g_loss: 0.68683994\n",
      "Step: [26223] d_loss: 1.38779116, g_loss: 0.69523180\n",
      "Step: [26224] d_loss: 1.38671005, g_loss: 0.69824654\n",
      "Step: [26225] d_loss: 1.38631189, g_loss: 0.69641137\n",
      "Step: [26226] d_loss: 1.38629699, g_loss: 0.69354862\n",
      "Step: [26227] d_loss: 1.38641262, g_loss: 0.69289327\n",
      "Step: [26228] d_loss: 1.38631463, g_loss: 0.69300044\n",
      "Step: [26229] d_loss: 1.38727951, g_loss: 0.69480288\n",
      "Step: [26230] d_loss: 1.38630390, g_loss: 0.69313180\n",
      "Step: [26231] d_loss: 1.38630438, g_loss: 0.69324392\n",
      "Step: [26232] d_loss: 1.38628447, g_loss: 0.69326073\n",
      "Step: [26233] d_loss: 1.38628864, g_loss: 0.69325650\n",
      "Step: [26234] d_loss: 1.38632143, g_loss: 0.69317985\n",
      "Step: [26235] d_loss: 1.38628972, g_loss: 0.69314277\n",
      "Step: [26236] d_loss: 1.38628054, g_loss: 0.69318897\n",
      "Step: [26237] d_loss: 1.38627970, g_loss: 0.69314909\n",
      "Step: [26238] d_loss: 1.38626969, g_loss: 0.69319314\n",
      "Step: [26239] d_loss: 1.38627315, g_loss: 0.69319588\n",
      "Step: [26240] d_loss: 1.38627434, g_loss: 0.69313991\n",
      "Step: [26241] d_loss: 1.38628685, g_loss: 0.69322777\n",
      "Step: [26242] d_loss: 1.38628244, g_loss: 0.69321024\n",
      "Step: [26243] d_loss: 1.38627899, g_loss: 0.69314897\n",
      "Step: [26244] d_loss: 1.38627970, g_loss: 0.69315016\n",
      "Step: [26245] d_loss: 1.38629746, g_loss: 0.69313025\n",
      "Step: [26246] d_loss: 1.38626802, g_loss: 0.69299698\n",
      "Step: [26247] d_loss: 1.38632715, g_loss: 0.69317091\n",
      "Step: [26248] d_loss: 1.38624763, g_loss: 0.69323242\n",
      "Step: [26249] d_loss: 1.38627267, g_loss: 0.69316971\n",
      "Step: [26250] d_loss: 1.38627172, g_loss: 0.69311112\n",
      "Step: [26251] d_loss: 1.38628471, g_loss: 0.69307864\n",
      "Step: [26252] d_loss: 1.38648820, g_loss: 0.69046527\n",
      "Step: [26253] d_loss: 1.38686121, g_loss: 0.69294608\n",
      "Step: [26254] d_loss: 1.38672566, g_loss: 0.68911648\n",
      "Step: [26255] d_loss: 1.38642943, g_loss: 0.69079441\n",
      "Step: [26256] d_loss: 1.38629341, g_loss: 0.69271618\n",
      "Step: [26257] d_loss: 1.38626313, g_loss: 0.69403803\n",
      "Step: [26258] d_loss: 1.38626361, g_loss: 0.69445944\n",
      "Step: [26259] d_loss: 1.38627934, g_loss: 0.69410658\n",
      "Step: [26260] d_loss: 1.38625991, g_loss: 0.69310451\n",
      "Step: [26261] d_loss: 1.38624001, g_loss: 0.69279170\n",
      "Step: [26262] d_loss: 1.38625312, g_loss: 0.69288814\n",
      "Step: [26263] d_loss: 1.38626361, g_loss: 0.69344521\n",
      "Step: [26264] d_loss: 1.38624430, g_loss: 0.69343287\n",
      "Step: [26265] d_loss: 1.38626647, g_loss: 0.69339788\n",
      "Step: [26266] d_loss: 1.38623941, g_loss: 0.69308352\n",
      "Step: [26267] d_loss: 1.38624716, g_loss: 0.69312662\n",
      "Step: [26268] d_loss: 1.38620639, g_loss: 0.69303310\n",
      "Step: [26269] d_loss: 1.38625622, g_loss: 0.69309318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26270] d_loss: 1.38622916, g_loss: 0.69333023\n",
      "Step: [26271] d_loss: 1.38635135, g_loss: 0.69367617\n",
      "Step: [26272] d_loss: 1.38627303, g_loss: 0.69431925\n",
      "Step: [26273] d_loss: 1.38643730, g_loss: 0.69370437\n",
      "Step: [26274] d_loss: 1.38639569, g_loss: 0.69273019\n",
      "Step: [26275] d_loss: 1.38626993, g_loss: 0.69268489\n",
      "Step: [26276] d_loss: 1.38607705, g_loss: 0.69142497\n",
      "Step: [26277] d_loss: 1.38626540, g_loss: 0.69312727\n",
      "Step: [26278] d_loss: 1.38619018, g_loss: 0.69348085\n",
      "Step: [26279] d_loss: 1.38629222, g_loss: 0.69596475\n",
      "Step: [26280] d_loss: 1.38674986, g_loss: 0.69353068\n",
      "Step: [26281] d_loss: 1.38687277, g_loss: 0.69112039\n",
      "Step: [26282] d_loss: 1.38669062, g_loss: 0.68977499\n",
      "Step: [26283] d_loss: 1.38635778, g_loss: 0.69469982\n",
      "Step: [26284] d_loss: 1.38620162, g_loss: 0.69632173\n",
      "Step: [26285] d_loss: 1.38720858, g_loss: 0.69488436\n",
      "Step: [26286] d_loss: 1.38625228, g_loss: 0.69356430\n",
      "Step: [26287] d_loss: 1.38623857, g_loss: 0.69267786\n",
      "Step: [26288] d_loss: 1.38624871, g_loss: 0.69261354\n",
      "Step: [26289] d_loss: 1.38623130, g_loss: 0.69365335\n",
      "Step: [26290] d_loss: 1.38629770, g_loss: 0.69357109\n",
      "Step: [26291] d_loss: 1.38626623, g_loss: 0.69331145\n",
      "Step: [26292] d_loss: 1.38622403, g_loss: 0.69319361\n",
      "Step: [26293] d_loss: 1.38621080, g_loss: 0.69318926\n",
      "Step: [26294] d_loss: 1.38627160, g_loss: 0.69362414\n",
      "Step: [26295] d_loss: 1.38638699, g_loss: 0.69411767\n",
      "Step: [26296] d_loss: 1.38627875, g_loss: 0.69367826\n",
      "Step: [26297] d_loss: 1.38658834, g_loss: 0.69264501\n",
      "Step: [26298] d_loss: 1.38743734, g_loss: 0.69340670\n",
      "Step: [26299] d_loss: 1.38839078, g_loss: 0.69647461\n",
      "Step: [26300] d_loss: 1.38899922, g_loss: 0.69353485\n",
      "Step: [26301] d_loss: 1.38698530, g_loss: 0.69279540\n",
      "Step: [26302] d_loss: 1.38627934, g_loss: 0.69325209\n",
      "Step: [26303] d_loss: 1.38617659, g_loss: 0.69365168\n",
      "Step: [26304] d_loss: 1.38624763, g_loss: 0.69403821\n",
      "Step: [26305] d_loss: 1.38632488, g_loss: 0.69335669\n",
      "Step: [26306] d_loss: 1.38613653, g_loss: 0.69270968\n",
      "Step: [26307] d_loss: 1.38619351, g_loss: 0.69308317\n",
      "Step: [26308] d_loss: 1.38607693, g_loss: 0.69312060\n",
      "Step: [26309] d_loss: 1.38603306, g_loss: 0.69296515\n",
      "Step: [26310] d_loss: 1.38621163, g_loss: 0.69399798\n",
      "Step: [26311] d_loss: 1.38618708, g_loss: 0.69348252\n",
      "Step: [26312] d_loss: 1.38621926, g_loss: 0.69327629\n",
      "Step: [26313] d_loss: 1.38611782, g_loss: 0.69320631\n",
      "Step: [26314] d_loss: 1.38647473, g_loss: 0.69241112\n",
      "Step: [26315] d_loss: 1.38681674, g_loss: 0.69019401\n",
      "Step: [26316] d_loss: 1.38671362, g_loss: 0.69135714\n",
      "Step: [26317] d_loss: 1.38641381, g_loss: 0.69213533\n",
      "Step: [26318] d_loss: 1.38650632, g_loss: 0.69497329\n",
      "Step: [26319] d_loss: 1.38633847, g_loss: 0.69527435\n",
      "Step: [26320] d_loss: 1.38621712, g_loss: 0.69507754\n",
      "Step: [26321] d_loss: 1.38643897, g_loss: 0.69297838\n",
      "Step: [26322] d_loss: 1.38644493, g_loss: 0.69083357\n",
      "Step: [26323] d_loss: 1.38635337, g_loss: 0.69213498\n",
      "Step: [26324] d_loss: 1.38628995, g_loss: 0.69282901\n",
      "Step: [26325] d_loss: 1.38630378, g_loss: 0.69429255\n",
      "Step: [26326] d_loss: 1.38628006, g_loss: 0.69363648\n",
      "Step: [26327] d_loss: 1.38625240, g_loss: 0.69378567\n",
      "Step: [26328] d_loss: 1.38627923, g_loss: 0.69450140\n",
      "Step: [26329] d_loss: 1.38633454, g_loss: 0.69518942\n",
      "Step: [26330] d_loss: 1.38634133, g_loss: 0.69306707\n",
      "Step: [26331] d_loss: 1.38634610, g_loss: 0.69341159\n",
      "Step: [26332] d_loss: 1.38676333, g_loss: 0.69424427\n",
      "Step: [26333] d_loss: 1.38702536, g_loss: 0.69268113\n",
      "Step: [26334] d_loss: 1.38681817, g_loss: 0.69446468\n",
      "Step: [26335] d_loss: 1.38649797, g_loss: 0.69512254\n",
      "Step: [26336] d_loss: 1.38756037, g_loss: 0.69602942\n",
      "Step: [26337] d_loss: 1.38770604, g_loss: 0.69356537\n",
      "Step: [26338] d_loss: 1.38687396, g_loss: 0.69843793\n",
      "Step: [26339] d_loss: 1.38640475, g_loss: 0.69536579\n",
      "Step: [26340] d_loss: 1.38632154, g_loss: 0.69313717\n",
      "Step: [26341] d_loss: 1.38632035, g_loss: 0.69186699\n",
      "Step: [26342] d_loss: 1.38632405, g_loss: 0.69202602\n",
      "Step: [26343] d_loss: 1.38631344, g_loss: 0.69359517\n",
      "Step: [26344] d_loss: 1.38626242, g_loss: 0.69391900\n",
      "Step: [26345] d_loss: 1.38630652, g_loss: 0.69358826\n",
      "Step: [26346] d_loss: 1.38628030, g_loss: 0.69308752\n",
      "Step: [26347] d_loss: 1.38620651, g_loss: 0.69314551\n",
      "Step: [26348] d_loss: 1.38628209, g_loss: 0.69320834\n",
      "Step: [26349] d_loss: 1.38623893, g_loss: 0.69344068\n",
      "Step: [26350] d_loss: 1.38628960, g_loss: 0.69325161\n",
      "Step: [26351] d_loss: 1.38625479, g_loss: 0.69310570\n",
      "Step: [26352] d_loss: 1.38627052, g_loss: 0.69311470\n",
      "Step: [26353] d_loss: 1.38612914, g_loss: 0.69320178\n",
      "Step: [26354] d_loss: 1.38627076, g_loss: 0.69320524\n",
      "Step: [26355] d_loss: 1.38623571, g_loss: 0.69316775\n",
      "Step: [26356] d_loss: 1.38622952, g_loss: 0.69242024\n",
      "Step: [26357] d_loss: 1.38636541, g_loss: 0.69472027\n",
      "Step: [26358] d_loss: 1.38636100, g_loss: 0.69421017\n",
      "Step: [26359] d_loss: 1.38631666, g_loss: 0.69329727\n",
      "Step: [26360] d_loss: 1.38630760, g_loss: 0.69359481\n",
      "Step: [26361] d_loss: 1.38615513, g_loss: 0.69386613\n",
      "Step: [26362] d_loss: 1.38631046, g_loss: 0.69400489\n",
      "Step: [26363] d_loss: 1.38630283, g_loss: 0.69278961\n",
      "Step: [26364] d_loss: 1.38629866, g_loss: 0.69370961\n",
      "Step: [26365] d_loss: 1.38631654, g_loss: 0.69329441\n",
      "Step: [26366] d_loss: 1.38631868, g_loss: 0.69303340\n",
      "Step: [26367] d_loss: 1.38629103, g_loss: 0.69295859\n",
      "Step: [26368] d_loss: 1.38624907, g_loss: 0.69306815\n",
      "Step: [26369] d_loss: 1.38626897, g_loss: 0.69324017\n",
      "Step: [26370] d_loss: 1.38630927, g_loss: 0.69335884\n",
      "Step: [26371] d_loss: 1.38627386, g_loss: 0.69327432\n",
      "Step: [26372] d_loss: 1.38632751, g_loss: 0.69314992\n",
      "Step: [26373] d_loss: 1.38628459, g_loss: 0.69311541\n",
      "Step: [26374] d_loss: 1.38627791, g_loss: 0.69315910\n",
      "Step: [26375] d_loss: 1.38628006, g_loss: 0.69326174\n",
      "Step: [26376] d_loss: 1.38627648, g_loss: 0.69315451\n",
      "Step: [26377] d_loss: 1.38632417, g_loss: 0.69173384\n",
      "Step: [26378] d_loss: 1.38647389, g_loss: 0.69530606\n",
      "Step: [26379] d_loss: 1.38684916, g_loss: 0.69588637\n",
      "Step: [26380] d_loss: 1.38710475, g_loss: 0.69294262\n",
      "Step: [26381] d_loss: 1.38715518, g_loss: 0.69070911\n",
      "Step: [26382] d_loss: 1.38709223, g_loss: 0.69078028\n",
      "Step: [26383] d_loss: 1.38669968, g_loss: 0.68726289\n",
      "Step: [26384] d_loss: 1.38634908, g_loss: 0.69192195\n",
      "Step: [26385] d_loss: 1.38652968, g_loss: 0.69260180\n",
      "Step: [26386] d_loss: 1.38639200, g_loss: 0.69699788\n",
      "Step: [26387] d_loss: 1.38700557, g_loss: 0.69591820\n",
      "Step: [26388] d_loss: 1.38719189, g_loss: 0.69232345\n",
      "Step: [26389] d_loss: 1.38675964, g_loss: 0.68803346\n",
      "Step: [26390] d_loss: 1.38637328, g_loss: 0.69096994\n",
      "Step: [26391] d_loss: 1.38631201, g_loss: 0.69289792\n",
      "Step: [26392] d_loss: 1.38623023, g_loss: 0.69419968\n",
      "Step: [26393] d_loss: 1.38633132, g_loss: 0.69426453\n",
      "Step: [26394] d_loss: 1.38625431, g_loss: 0.69403440\n",
      "Step: [26395] d_loss: 1.38628268, g_loss: 0.69309467\n",
      "Step: [26396] d_loss: 1.38618970, g_loss: 0.69315052\n",
      "Step: [26397] d_loss: 1.38630891, g_loss: 0.69300902\n",
      "Step: [26398] d_loss: 1.38628697, g_loss: 0.69344079\n",
      "Step: [26399] d_loss: 1.38624740, g_loss: 0.69307870\n",
      "Step: [26400] d_loss: 1.38625801, g_loss: 0.69312042\n",
      "Step: [26401] d_loss: 1.38637364, g_loss: 0.69293571\n",
      "Step: [26402] d_loss: 1.38625312, g_loss: 0.69294882\n",
      "Step: [26403] d_loss: 1.38633680, g_loss: 0.69318211\n",
      "Step: [26404] d_loss: 1.38632059, g_loss: 0.69223219\n",
      "Step: [26405] d_loss: 1.38631332, g_loss: 0.69176650\n",
      "Step: [26406] d_loss: 1.38631439, g_loss: 0.69320005\n",
      "Step: [26407] d_loss: 1.38626671, g_loss: 0.69298840\n",
      "Step: [26408] d_loss: 1.38630843, g_loss: 0.69341230\n",
      "Step: [26409] d_loss: 1.38631129, g_loss: 0.69186151\n",
      "Step: [26410] d_loss: 1.38660610, g_loss: 0.69156557\n",
      "Step: [26411] d_loss: 1.38649142, g_loss: 0.69356322\n",
      "Step: [26412] d_loss: 1.38642883, g_loss: 0.69291508\n",
      "Step: [26413] d_loss: 1.38626909, g_loss: 0.69224101\n",
      "Step: [26414] d_loss: 1.38633156, g_loss: 0.69269860\n",
      "Step: [26415] d_loss: 1.38633394, g_loss: 0.69315296\n",
      "Step: [26416] d_loss: 1.38630295, g_loss: 0.69320053\n",
      "Step: [26417] d_loss: 1.38628173, g_loss: 0.69187969\n",
      "Step: [26418] d_loss: 1.38632226, g_loss: 0.69303489\n",
      "Step: [26419] d_loss: 1.38641095, g_loss: 0.69316304\n",
      "Step: [26420] d_loss: 1.38644338, g_loss: 0.69282472\n",
      "Step: [26421] d_loss: 1.38636088, g_loss: 0.69224441\n",
      "Step: [26422] d_loss: 1.38632607, g_loss: 0.69228280\n",
      "Step: [26423] d_loss: 1.38625562, g_loss: 0.69330561\n",
      "Step: [26424] d_loss: 1.38618088, g_loss: 0.69376779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26425] d_loss: 1.38662386, g_loss: 0.69086903\n",
      "Step: [26426] d_loss: 1.38681650, g_loss: 0.69269562\n",
      "Step: [26427] d_loss: 1.38663995, g_loss: 0.69432366\n",
      "Step: [26428] d_loss: 1.38643193, g_loss: 0.69506407\n",
      "Step: [26429] d_loss: 1.38632464, g_loss: 0.69421959\n",
      "Step: [26430] d_loss: 1.38626111, g_loss: 0.69291592\n",
      "Step: [26431] d_loss: 1.38628995, g_loss: 0.69329196\n",
      "Step: [26432] d_loss: 1.38625181, g_loss: 0.69247627\n",
      "Step: [26433] d_loss: 1.38630557, g_loss: 0.69288683\n",
      "Step: [26434] d_loss: 1.38624287, g_loss: 0.69350648\n",
      "Step: [26435] d_loss: 1.38627076, g_loss: 0.69331205\n",
      "Step: [26436] d_loss: 1.38627601, g_loss: 0.69297421\n",
      "Step: [26437] d_loss: 1.38630390, g_loss: 0.69307065\n",
      "Step: [26438] d_loss: 1.38647008, g_loss: 0.69312817\n",
      "Step: [26439] d_loss: 1.38632774, g_loss: 0.69316161\n",
      "Step: [26440] d_loss: 1.38630605, g_loss: 0.69370043\n",
      "Step: [26441] d_loss: 1.38630271, g_loss: 0.69345105\n",
      "Step: [26442] d_loss: 1.38631225, g_loss: 0.69285321\n",
      "Step: [26443] d_loss: 1.38629174, g_loss: 0.69324905\n",
      "Step: [26444] d_loss: 1.38629949, g_loss: 0.69411880\n",
      "Step: [26445] d_loss: 1.38628411, g_loss: 0.69380921\n",
      "Step: [26446] d_loss: 1.38630712, g_loss: 0.69258749\n",
      "Step: [26447] d_loss: 1.38628411, g_loss: 0.69261217\n",
      "Step: [26448] d_loss: 1.38634408, g_loss: 0.69369197\n",
      "Step: [26449] d_loss: 1.38633525, g_loss: 0.69478035\n",
      "Step: [26450] d_loss: 1.38637424, g_loss: 0.69371444\n",
      "Step: [26451] d_loss: 1.38629079, g_loss: 0.69240719\n",
      "Step: [26452] d_loss: 1.38619637, g_loss: 0.69417703\n",
      "Step: [26453] d_loss: 1.38636565, g_loss: 0.69272465\n",
      "Step: [26454] d_loss: 1.38631213, g_loss: 0.69371009\n",
      "Step: [26455] d_loss: 1.38635540, g_loss: 0.69341063\n",
      "Step: [26456] d_loss: 1.38626921, g_loss: 0.69269103\n",
      "Step: [26457] d_loss: 1.38627875, g_loss: 0.69298732\n",
      "Step: [26458] d_loss: 1.38649869, g_loss: 0.69334793\n",
      "Step: [26459] d_loss: 1.38651323, g_loss: 0.69404471\n",
      "Step: [26460] d_loss: 1.38565445, g_loss: 0.69585764\n",
      "Step: [26461] d_loss: 1.38638926, g_loss: 0.69447267\n",
      "Step: [26462] d_loss: 1.38660288, g_loss: 0.69147527\n",
      "Step: [26463] d_loss: 1.38694251, g_loss: 0.69135368\n",
      "Step: [26464] d_loss: 1.38699031, g_loss: 0.69201231\n",
      "Step: [26465] d_loss: 1.38631713, g_loss: 0.69354671\n",
      "Step: [26466] d_loss: 1.38637292, g_loss: 0.69387084\n",
      "Step: [26467] d_loss: 1.38643026, g_loss: 0.69259977\n",
      "Step: [26468] d_loss: 1.38637710, g_loss: 0.69267297\n",
      "Step: [26469] d_loss: 1.38634646, g_loss: 0.69297576\n",
      "Step: [26470] d_loss: 1.38654912, g_loss: 0.69434363\n",
      "Step: [26471] d_loss: 1.38682985, g_loss: 0.69484699\n",
      "Step: [26472] d_loss: 1.38684940, g_loss: 0.69500721\n",
      "Step: [26473] d_loss: 1.38650894, g_loss: 0.69355512\n",
      "Step: [26474] d_loss: 1.38632786, g_loss: 0.69259548\n",
      "Step: [26475] d_loss: 1.38626337, g_loss: 0.69205153\n",
      "Step: [26476] d_loss: 1.38726115, g_loss: 0.69625854\n",
      "Step: [26477] d_loss: 1.38632178, g_loss: 0.69257927\n",
      "Step: [26478] d_loss: 1.38630915, g_loss: 0.69290650\n",
      "Step: [26479] d_loss: 1.38638759, g_loss: 0.69377077\n",
      "Step: [26480] d_loss: 1.38629282, g_loss: 0.69399691\n",
      "Step: [26481] d_loss: 1.38629127, g_loss: 0.69343948\n",
      "Step: [26482] d_loss: 1.38625097, g_loss: 0.69239211\n",
      "Step: [26483] d_loss: 1.38643301, g_loss: 0.69334257\n",
      "Step: [26484] d_loss: 1.38622928, g_loss: 0.69319916\n",
      "Step: [26485] d_loss: 1.38627172, g_loss: 0.69332492\n",
      "Step: [26486] d_loss: 1.38624227, g_loss: 0.69309938\n",
      "Step: [26487] d_loss: 1.38634968, g_loss: 0.69474149\n",
      "Step: [26488] d_loss: 1.38657570, g_loss: 0.69462514\n",
      "Step: [26489] d_loss: 1.38635516, g_loss: 0.69309205\n",
      "Step: [26490] d_loss: 1.38631856, g_loss: 0.69134748\n",
      "Step: [26491] d_loss: 1.38620389, g_loss: 0.69305408\n",
      "Step: [26492] d_loss: 1.38631940, g_loss: 0.69512671\n",
      "Step: [26493] d_loss: 1.38648367, g_loss: 0.69433129\n",
      "Step: [26494] d_loss: 1.38708174, g_loss: 0.69556159\n",
      "Step: [26495] d_loss: 1.38707173, g_loss: 0.70066029\n",
      "Step: [26496] d_loss: 1.38666642, g_loss: 0.69677281\n",
      "Step: [26497] d_loss: 1.38631141, g_loss: 0.69272506\n",
      "Step: [26498] d_loss: 1.38664877, g_loss: 0.69063866\n",
      "Step: [26499] d_loss: 1.38673294, g_loss: 0.69238639\n",
      "Step: [26500] d_loss: 1.38643169, g_loss: 0.69356799\n",
      "Step: [26501] d_loss: 1.38721871, g_loss: 0.69447851\n",
      "Step: [26502] d_loss: 1.38625526, g_loss: 0.69439703\n",
      "Step: [26503] d_loss: 1.38623965, g_loss: 0.69191355\n",
      "Step: [26504] d_loss: 1.38635087, g_loss: 0.69292688\n",
      "Step: [26505] d_loss: 1.38628316, g_loss: 0.69291484\n",
      "Step: [26506] d_loss: 1.38644660, g_loss: 0.69336486\n",
      "Step: [26507] d_loss: 1.38638783, g_loss: 0.69237417\n",
      "Step: [26508] d_loss: 1.38625515, g_loss: 0.69246495\n",
      "Step: [26509] d_loss: 1.38626862, g_loss: 0.69294488\n",
      "Step: [26510] d_loss: 1.38626611, g_loss: 0.69338036\n",
      "Step: [26511] d_loss: 1.38628101, g_loss: 0.69341397\n",
      "Step: [26512] d_loss: 1.38628805, g_loss: 0.69306827\n",
      "Step: [26513] d_loss: 1.38628221, g_loss: 0.69298959\n",
      "Step: [26514] d_loss: 1.38624215, g_loss: 0.69307029\n",
      "Step: [26515] d_loss: 1.38629031, g_loss: 0.69236112\n",
      "Step: [26516] d_loss: 1.38631725, g_loss: 0.69360709\n",
      "Step: [26517] d_loss: 1.38624930, g_loss: 0.69357967\n",
      "Step: [26518] d_loss: 1.38628292, g_loss: 0.69355345\n",
      "Step: [26519] d_loss: 1.38636148, g_loss: 0.69327700\n",
      "Step: [26520] d_loss: 1.38625777, g_loss: 0.69409114\n",
      "Step: [26521] d_loss: 1.38619852, g_loss: 0.69266224\n",
      "Step: [26522] d_loss: 1.38633394, g_loss: 0.69236529\n",
      "Step: [26523] d_loss: 1.38633728, g_loss: 0.69182611\n",
      "Step: [26524] d_loss: 1.38641870, g_loss: 0.69470239\n",
      "Step: [26525] d_loss: 1.38637900, g_loss: 0.69480377\n",
      "Step: [26526] d_loss: 1.38633943, g_loss: 0.69152206\n",
      "Step: [26527] d_loss: 1.38660049, g_loss: 0.69397807\n",
      "Step: [26528] d_loss: 1.38686728, g_loss: 0.69395030\n",
      "Step: [26529] d_loss: 1.38737726, g_loss: 0.69808829\n",
      "Step: [26530] d_loss: 1.38712859, g_loss: 0.69401389\n",
      "Step: [26531] d_loss: 1.38649213, g_loss: 0.69176757\n",
      "Step: [26532] d_loss: 1.38627410, g_loss: 0.69181561\n",
      "Step: [26533] d_loss: 1.38635349, g_loss: 0.69297224\n",
      "Step: [26534] d_loss: 1.38631499, g_loss: 0.69359994\n",
      "Step: [26535] d_loss: 1.38632977, g_loss: 0.69336826\n",
      "Step: [26536] d_loss: 1.38633239, g_loss: 0.69303679\n",
      "Step: [26537] d_loss: 1.38629651, g_loss: 0.69287419\n",
      "Step: [26538] d_loss: 1.38619745, g_loss: 0.69313854\n",
      "Step: [26539] d_loss: 1.38652945, g_loss: 0.69254744\n",
      "Step: [26540] d_loss: 1.38667393, g_loss: 0.69365585\n",
      "Step: [26541] d_loss: 1.38654840, g_loss: 0.69029576\n",
      "Step: [26542] d_loss: 1.38640702, g_loss: 0.69072199\n",
      "Step: [26543] d_loss: 1.38630605, g_loss: 0.69383532\n",
      "Step: [26544] d_loss: 1.38627696, g_loss: 0.69399208\n",
      "Step: [26545] d_loss: 1.38629091, g_loss: 0.69510567\n",
      "Step: [26546] d_loss: 1.38621449, g_loss: 0.69360995\n",
      "Step: [26547] d_loss: 1.38629127, g_loss: 0.69291335\n",
      "Step: [26548] d_loss: 1.38622200, g_loss: 0.69281507\n",
      "Step: [26549] d_loss: 1.38628149, g_loss: 0.69303733\n",
      "Step: [26550] d_loss: 1.38624716, g_loss: 0.69378448\n",
      "Step: [26551] d_loss: 1.38625515, g_loss: 0.69183660\n",
      "Step: [26552] d_loss: 1.38630819, g_loss: 0.69230187\n",
      "Step: [26553] d_loss: 1.38655889, g_loss: 0.69528186\n",
      "Step: [26554] d_loss: 1.38651824, g_loss: 0.69366127\n",
      "Step: [26555] d_loss: 1.38656056, g_loss: 0.69280624\n",
      "Step: [26556] d_loss: 1.38644934, g_loss: 0.69318354\n",
      "Step: [26557] d_loss: 1.38627791, g_loss: 0.69415438\n",
      "Step: [26558] d_loss: 1.38631916, g_loss: 0.69606233\n",
      "Step: [26559] d_loss: 1.38625979, g_loss: 0.69320279\n",
      "Step: [26560] d_loss: 1.38634229, g_loss: 0.69344777\n",
      "Step: [26561] d_loss: 1.38649821, g_loss: 0.69419873\n",
      "Step: [26562] d_loss: 1.38640738, g_loss: 0.69467330\n",
      "Step: [26563] d_loss: 1.38629460, g_loss: 0.69366413\n",
      "Step: [26564] d_loss: 1.38631737, g_loss: 0.69307399\n",
      "Step: [26565] d_loss: 1.38625622, g_loss: 0.69313127\n",
      "Step: [26566] d_loss: 1.38699222, g_loss: 0.69322503\n",
      "Step: [26567] d_loss: 1.38630581, g_loss: 0.69328427\n",
      "Step: [26568] d_loss: 1.38625836, g_loss: 0.69314581\n",
      "Step: [26569] d_loss: 1.38620377, g_loss: 0.69272649\n",
      "Step: [26570] d_loss: 1.38624370, g_loss: 0.69257379\n",
      "Step: [26571] d_loss: 1.38623035, g_loss: 0.69271660\n",
      "Step: [26572] d_loss: 1.38625550, g_loss: 0.69321048\n",
      "Step: [26573] d_loss: 1.38654399, g_loss: 0.69283062\n",
      "Step: [26574] d_loss: 1.38644290, g_loss: 0.69543290\n",
      "Step: [26575] d_loss: 1.38647091, g_loss: 0.69511837\n",
      "Step: [26576] d_loss: 1.38610959, g_loss: 0.69206703\n",
      "Step: [26577] d_loss: 1.38657057, g_loss: 0.69086647\n",
      "Step: [26578] d_loss: 1.38678408, g_loss: 0.69299895\n",
      "Step: [26579] d_loss: 1.38661599, g_loss: 0.69433039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26580] d_loss: 1.38634992, g_loss: 0.69485891\n",
      "Step: [26581] d_loss: 1.38624883, g_loss: 0.69372261\n",
      "Step: [26582] d_loss: 1.38629317, g_loss: 0.69167966\n",
      "Step: [26583] d_loss: 1.38620448, g_loss: 0.69237739\n",
      "Step: [26584] d_loss: 1.38609862, g_loss: 0.69292098\n",
      "Step: [26585] d_loss: 1.38626146, g_loss: 0.69363421\n",
      "Step: [26586] d_loss: 1.38620925, g_loss: 0.69353700\n",
      "Step: [26587] d_loss: 1.38620877, g_loss: 0.69321883\n",
      "Step: [26588] d_loss: 1.38688421, g_loss: 0.69098938\n",
      "Step: [26589] d_loss: 1.38638759, g_loss: 0.69222337\n",
      "Step: [26590] d_loss: 1.38622665, g_loss: 0.69460773\n",
      "Step: [26591] d_loss: 1.38661551, g_loss: 0.69439399\n",
      "Step: [26592] d_loss: 1.38655305, g_loss: 0.69500017\n",
      "Step: [26593] d_loss: 1.38635516, g_loss: 0.69314682\n",
      "Step: [26594] d_loss: 1.38636923, g_loss: 0.69143307\n",
      "Step: [26595] d_loss: 1.38715291, g_loss: 0.69080424\n",
      "Step: [26596] d_loss: 1.38702583, g_loss: 0.68941963\n",
      "Step: [26597] d_loss: 1.38629270, g_loss: 0.69165862\n",
      "Step: [26598] d_loss: 1.38632572, g_loss: 0.69401169\n",
      "Step: [26599] d_loss: 1.38629913, g_loss: 0.69392884\n",
      "Step: [26600] d_loss: 1.38630724, g_loss: 0.69307935\n",
      "Step: [26601] d_loss: 1.38631582, g_loss: 0.69137394\n",
      "Step: [26602] d_loss: 1.38623309, g_loss: 0.69178146\n",
      "Step: [26603] d_loss: 1.38618135, g_loss: 0.69294357\n",
      "Step: [26604] d_loss: 1.38626194, g_loss: 0.69396579\n",
      "Step: [26605] d_loss: 1.38615084, g_loss: 0.69345343\n",
      "Step: [26606] d_loss: 1.38625407, g_loss: 0.69266593\n",
      "Step: [26607] d_loss: 1.38620591, g_loss: 0.69277799\n",
      "Step: [26608] d_loss: 1.38632512, g_loss: 0.69324458\n",
      "Step: [26609] d_loss: 1.38632131, g_loss: 0.69331068\n",
      "Step: [26610] d_loss: 1.38616729, g_loss: 0.69309914\n",
      "Step: [26611] d_loss: 1.38618302, g_loss: 0.69277811\n",
      "Step: [26612] d_loss: 1.38636327, g_loss: 0.69354850\n",
      "Step: [26613] d_loss: 1.38628173, g_loss: 0.69414091\n",
      "Step: [26614] d_loss: 1.38652921, g_loss: 0.69366002\n",
      "Step: [26615] d_loss: 1.38646376, g_loss: 0.69245577\n",
      "Step: [26616] d_loss: 1.38628507, g_loss: 0.69207036\n",
      "Step: [26617] d_loss: 1.38606763, g_loss: 0.69237852\n",
      "Step: [26618] d_loss: 1.38655043, g_loss: 0.69992799\n",
      "Step: [26619] d_loss: 1.38636947, g_loss: 0.69134557\n",
      "Step: [26620] d_loss: 1.38641739, g_loss: 0.69424897\n",
      "Step: [26621] d_loss: 1.38693023, g_loss: 0.69673109\n",
      "Step: [26622] d_loss: 1.38691962, g_loss: 0.69424373\n",
      "Step: [26623] d_loss: 1.38735890, g_loss: 0.69283080\n",
      "Step: [26624] d_loss: 1.38764668, g_loss: 0.69612437\n",
      "Step: [26625] d_loss: 1.38671660, g_loss: 0.69500816\n",
      "Step: [26626] d_loss: 1.38618040, g_loss: 0.69459248\n",
      "Step: [26627] d_loss: 1.38626826, g_loss: 0.69284558\n",
      "Step: [26628] d_loss: 1.38629639, g_loss: 0.69255090\n",
      "Step: [26629] d_loss: 1.38619113, g_loss: 0.69294357\n",
      "Step: [26630] d_loss: 1.38609743, g_loss: 0.69352257\n",
      "Step: [26631] d_loss: 1.38616633, g_loss: 0.69432682\n",
      "Step: [26632] d_loss: 1.38627195, g_loss: 0.69343686\n",
      "Step: [26633] d_loss: 1.38601375, g_loss: 0.69457108\n",
      "Step: [26634] d_loss: 1.38637757, g_loss: 0.69343436\n",
      "Step: [26635] d_loss: 1.38619471, g_loss: 0.69313389\n",
      "Step: [26636] d_loss: 1.38623500, g_loss: 0.69279742\n",
      "Step: [26637] d_loss: 1.38650775, g_loss: 0.69270927\n",
      "Step: [26638] d_loss: 1.38637567, g_loss: 0.69300550\n",
      "Step: [26639] d_loss: 1.38648891, g_loss: 0.69337493\n",
      "Step: [26640] d_loss: 1.38620663, g_loss: 0.69239485\n",
      "Step: [26641] d_loss: 1.38615131, g_loss: 0.69308698\n",
      "Step: [26642] d_loss: 1.38608861, g_loss: 0.69296765\n",
      "Step: [26643] d_loss: 1.38608193, g_loss: 0.69455498\n",
      "Step: [26644] d_loss: 1.38657594, g_loss: 0.69275922\n",
      "Step: [26645] d_loss: 1.38715005, g_loss: 0.69875276\n",
      "Step: [26646] d_loss: 1.38773322, g_loss: 0.68987060\n",
      "Step: [26647] d_loss: 1.38738871, g_loss: 0.69434220\n",
      "Step: [26648] d_loss: 1.38658404, g_loss: 0.69518852\n",
      "Step: [26649] d_loss: 1.38633251, g_loss: 0.69601476\n",
      "Step: [26650] d_loss: 1.38704658, g_loss: 0.69121963\n",
      "Step: [26651] d_loss: 1.38682473, g_loss: 0.69315708\n",
      "Step: [26652] d_loss: 1.38646352, g_loss: 0.69393635\n",
      "Step: [26653] d_loss: 1.38627231, g_loss: 0.69425207\n",
      "Step: [26654] d_loss: 1.38639176, g_loss: 0.69055665\n",
      "Step: [26655] d_loss: 1.38635325, g_loss: 0.69140375\n",
      "Step: [26656] d_loss: 1.38640559, g_loss: 0.69315660\n",
      "Step: [26657] d_loss: 1.38676286, g_loss: 0.69271541\n",
      "Step: [26658] d_loss: 1.38682437, g_loss: 0.69191718\n",
      "Step: [26659] d_loss: 1.38652003, g_loss: 0.69191384\n",
      "Step: [26660] d_loss: 1.38632417, g_loss: 0.69245291\n",
      "Step: [26661] d_loss: 1.38626957, g_loss: 0.69339937\n",
      "Step: [26662] d_loss: 1.38632369, g_loss: 0.69392955\n",
      "Step: [26663] d_loss: 1.38619566, g_loss: 0.69370353\n",
      "Step: [26664] d_loss: 1.38622022, g_loss: 0.69329524\n",
      "Step: [26665] d_loss: 1.38624167, g_loss: 0.69296390\n",
      "Step: [26666] d_loss: 1.38621545, g_loss: 0.69245243\n",
      "Step: [26667] d_loss: 1.38653266, g_loss: 0.69408262\n",
      "Step: [26668] d_loss: 1.38602483, g_loss: 0.69561183\n",
      "Step: [26669] d_loss: 1.38672709, g_loss: 0.69422054\n",
      "Step: [26670] d_loss: 1.38653851, g_loss: 0.69332254\n",
      "Step: [26671] d_loss: 1.38619757, g_loss: 0.69304585\n",
      "Step: [26672] d_loss: 1.38627779, g_loss: 0.69301140\n",
      "Step: [26673] d_loss: 1.38634396, g_loss: 0.69363379\n",
      "Step: [26674] d_loss: 1.38647676, g_loss: 0.69046092\n",
      "Step: [26675] d_loss: 1.38706231, g_loss: 0.69317770\n",
      "Step: [26676] d_loss: 1.38876474, g_loss: 0.69398403\n",
      "Step: [26677] d_loss: 1.38987327, g_loss: 0.70364374\n",
      "Step: [26678] d_loss: 1.38900113, g_loss: 0.70172167\n",
      "Step: [26679] d_loss: 1.38698840, g_loss: 0.69656217\n",
      "Step: [26680] d_loss: 1.38630319, g_loss: 0.69276869\n",
      "Step: [26681] d_loss: 1.38687301, g_loss: 0.69194555\n",
      "Step: [26682] d_loss: 1.38689423, g_loss: 0.69314194\n",
      "Step: [26683] d_loss: 1.38633847, g_loss: 0.69284326\n",
      "Step: [26684] d_loss: 1.38624001, g_loss: 0.69517714\n",
      "Step: [26685] d_loss: 1.38688779, g_loss: 0.69385612\n",
      "Step: [26686] d_loss: 1.38635516, g_loss: 0.69561064\n",
      "Step: [26687] d_loss: 1.38640475, g_loss: 0.69409263\n",
      "Step: [26688] d_loss: 1.38652992, g_loss: 0.69100368\n",
      "Step: [26689] d_loss: 1.38627744, g_loss: 0.69085312\n",
      "Step: [26690] d_loss: 1.38645709, g_loss: 0.69269478\n",
      "Step: [26691] d_loss: 1.38652921, g_loss: 0.69486219\n",
      "Step: [26692] d_loss: 1.38642716, g_loss: 0.69424415\n",
      "Step: [26693] d_loss: 1.38629043, g_loss: 0.69356203\n",
      "Step: [26694] d_loss: 1.38623929, g_loss: 0.69293177\n",
      "Step: [26695] d_loss: 1.38621020, g_loss: 0.69304216\n",
      "Step: [26696] d_loss: 1.38626385, g_loss: 0.69276053\n",
      "Step: [26697] d_loss: 1.38636315, g_loss: 0.69300908\n",
      "Step: [26698] d_loss: 1.38643026, g_loss: 0.69375587\n",
      "Step: [26699] d_loss: 1.38636208, g_loss: 0.69392431\n",
      "Step: [26700] d_loss: 1.38614261, g_loss: 0.69314015\n",
      "Step: [26701] d_loss: 1.38624692, g_loss: 0.69388652\n",
      "Step: [26702] d_loss: 1.38624787, g_loss: 0.69376016\n",
      "Step: [26703] d_loss: 1.38629758, g_loss: 0.69297278\n",
      "Step: [26704] d_loss: 1.38618159, g_loss: 0.69254631\n",
      "Step: [26705] d_loss: 1.38652682, g_loss: 0.69223231\n",
      "Step: [26706] d_loss: 1.38660574, g_loss: 0.69152361\n",
      "Step: [26707] d_loss: 1.38629627, g_loss: 0.69282752\n",
      "Step: [26708] d_loss: 1.38649154, g_loss: 0.69356555\n",
      "Step: [26709] d_loss: 1.38616574, g_loss: 0.69450295\n",
      "Step: [26710] d_loss: 1.38629913, g_loss: 0.69187325\n",
      "Step: [26711] d_loss: 1.38628268, g_loss: 0.69172239\n",
      "Step: [26712] d_loss: 1.38625765, g_loss: 0.69281232\n",
      "Step: [26713] d_loss: 1.38621891, g_loss: 0.69349092\n",
      "Step: [26714] d_loss: 1.38625145, g_loss: 0.69325811\n",
      "Step: [26715] d_loss: 1.38622200, g_loss: 0.69384813\n",
      "Step: [26716] d_loss: 1.38621628, g_loss: 0.69386393\n",
      "Step: [26717] d_loss: 1.38622439, g_loss: 0.69293761\n",
      "Step: [26718] d_loss: 1.38626719, g_loss: 0.69264388\n",
      "Step: [26719] d_loss: 1.38626218, g_loss: 0.69234860\n",
      "Step: [26720] d_loss: 1.38621378, g_loss: 0.69335848\n",
      "Step: [26721] d_loss: 1.38631010, g_loss: 0.69351804\n",
      "Step: [26722] d_loss: 1.38625288, g_loss: 0.69311005\n",
      "Step: [26723] d_loss: 1.38631415, g_loss: 0.69288528\n",
      "Step: [26724] d_loss: 1.38616860, g_loss: 0.69396305\n",
      "Step: [26725] d_loss: 1.38612080, g_loss: 0.69293916\n",
      "Step: [26726] d_loss: 1.38628173, g_loss: 0.69286263\n",
      "Step: [26727] d_loss: 1.38620305, g_loss: 0.69301957\n",
      "Step: [26728] d_loss: 1.38610280, g_loss: 0.69256932\n",
      "Step: [26729] d_loss: 1.38564897, g_loss: 0.69230711\n",
      "Step: [26730] d_loss: 1.38629603, g_loss: 0.69282210\n",
      "Step: [26731] d_loss: 1.38648009, g_loss: 0.69239551\n",
      "Step: [26732] d_loss: 1.38640141, g_loss: 0.69290245\n",
      "Step: [26733] d_loss: 1.38615942, g_loss: 0.69332194\n",
      "Step: [26734] d_loss: 1.38604236, g_loss: 0.69280440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26735] d_loss: 1.38601947, g_loss: 0.69454145\n",
      "Step: [26736] d_loss: 1.38709128, g_loss: 0.69522530\n",
      "Step: [26737] d_loss: 1.38858008, g_loss: 0.69647980\n",
      "Step: [26738] d_loss: 1.38951600, g_loss: 0.69759572\n",
      "Step: [26739] d_loss: 1.38809276, g_loss: 0.70254481\n",
      "Step: [26740] d_loss: 1.38670874, g_loss: 0.69619030\n",
      "Step: [26741] d_loss: 1.38644099, g_loss: 0.69470620\n",
      "Step: [26742] d_loss: 1.38649654, g_loss: 0.69193357\n",
      "Step: [26743] d_loss: 1.38653946, g_loss: 0.68949986\n",
      "Step: [26744] d_loss: 1.38636398, g_loss: 0.69120187\n",
      "Step: [26745] d_loss: 1.38652480, g_loss: 0.69147933\n",
      "Step: [26746] d_loss: 1.38654411, g_loss: 0.69503832\n",
      "Step: [26747] d_loss: 1.38640833, g_loss: 0.69501829\n",
      "Step: [26748] d_loss: 1.38623953, g_loss: 0.69470990\n",
      "Step: [26749] d_loss: 1.38623428, g_loss: 0.69364548\n",
      "Step: [26750] d_loss: 1.38620615, g_loss: 0.69184846\n",
      "Step: [26751] d_loss: 1.38625133, g_loss: 0.69222414\n",
      "Step: [26752] d_loss: 1.38618708, g_loss: 0.69356489\n",
      "Step: [26753] d_loss: 1.38617885, g_loss: 0.69356221\n",
      "Step: [26754] d_loss: 1.38619816, g_loss: 0.69217563\n",
      "Step: [26755] d_loss: 1.38629508, g_loss: 0.69164068\n",
      "Step: [26756] d_loss: 1.38624430, g_loss: 0.69288868\n",
      "Step: [26757] d_loss: 1.38622189, g_loss: 0.69416142\n",
      "Step: [26758] d_loss: 1.38621378, g_loss: 0.69366354\n",
      "Step: [26759] d_loss: 1.38625216, g_loss: 0.69342852\n",
      "Step: [26760] d_loss: 1.38621485, g_loss: 0.69276625\n",
      "Step: [26761] d_loss: 1.38614416, g_loss: 0.69300282\n",
      "Step: [26762] d_loss: 1.38624156, g_loss: 0.69356465\n",
      "Step: [26763] d_loss: 1.38618279, g_loss: 0.69330126\n",
      "Step: [26764] d_loss: 1.38622594, g_loss: 0.69269228\n",
      "Step: [26765] d_loss: 1.38669133, g_loss: 0.69292736\n",
      "Step: [26766] d_loss: 1.38625681, g_loss: 0.69330096\n",
      "Step: [26767] d_loss: 1.38652372, g_loss: 0.69186985\n",
      "Step: [26768] d_loss: 1.38627744, g_loss: 0.69247103\n",
      "Step: [26769] d_loss: 1.38623691, g_loss: 0.69259965\n",
      "Step: [26770] d_loss: 1.38632202, g_loss: 0.69292909\n",
      "Step: [26771] d_loss: 1.38636220, g_loss: 0.69341063\n",
      "Step: [26772] d_loss: 1.38625240, g_loss: 0.69247860\n",
      "Step: [26773] d_loss: 1.38638341, g_loss: 0.69235122\n",
      "Step: [26774] d_loss: 1.38658643, g_loss: 0.69205636\n",
      "Step: [26775] d_loss: 1.38658690, g_loss: 0.69119638\n",
      "Step: [26776] d_loss: 1.38612032, g_loss: 0.69262230\n",
      "Step: [26777] d_loss: 1.38653302, g_loss: 0.69103944\n",
      "Step: [26778] d_loss: 1.38634253, g_loss: 0.69436419\n",
      "Step: [26779] d_loss: 1.38619256, g_loss: 0.69517410\n",
      "Step: [26780] d_loss: 1.38623548, g_loss: 0.69354469\n",
      "Step: [26781] d_loss: 1.38625693, g_loss: 0.69265652\n",
      "Step: [26782] d_loss: 1.38647878, g_loss: 0.69186187\n",
      "Step: [26783] d_loss: 1.38629842, g_loss: 0.69230342\n",
      "Step: [26784] d_loss: 1.38633990, g_loss: 0.69378555\n",
      "Step: [26785] d_loss: 1.38633800, g_loss: 0.69236696\n",
      "Step: [26786] d_loss: 1.38655901, g_loss: 0.69305670\n",
      "Step: [26787] d_loss: 1.38658237, g_loss: 0.69378901\n",
      "Step: [26788] d_loss: 1.38636971, g_loss: 0.69456196\n",
      "Step: [26789] d_loss: 1.38631618, g_loss: 0.69356787\n",
      "Step: [26790] d_loss: 1.38648534, g_loss: 0.69346774\n",
      "Step: [26791] d_loss: 1.38666999, g_loss: 0.69312894\n",
      "Step: [26792] d_loss: 1.38670075, g_loss: 0.69295764\n",
      "Step: [26793] d_loss: 1.38662815, g_loss: 0.69242334\n",
      "Step: [26794] d_loss: 1.38649893, g_loss: 0.69245374\n",
      "Step: [26795] d_loss: 1.38629627, g_loss: 0.69266963\n",
      "Step: [26796] d_loss: 1.38635695, g_loss: 0.69446957\n",
      "Step: [26797] d_loss: 1.38631797, g_loss: 0.69342840\n",
      "Step: [26798] d_loss: 1.38626409, g_loss: 0.69354558\n",
      "Step: [26799] d_loss: 1.38629508, g_loss: 0.69289780\n",
      "Step: [26800] d_loss: 1.38641036, g_loss: 0.69208825\n",
      "Step: [26801] d_loss: 1.38632691, g_loss: 0.69259417\n",
      "Step: [26802] d_loss: 1.38635111, g_loss: 0.69281983\n",
      "Step: [26803] d_loss: 1.38680351, g_loss: 0.69403410\n",
      "Step: [26804] d_loss: 1.38648665, g_loss: 0.69302589\n",
      "Step: [26805] d_loss: 1.38647723, g_loss: 0.69370848\n",
      "Step: [26806] d_loss: 1.38627267, g_loss: 0.69355500\n",
      "Step: [26807] d_loss: 1.38637114, g_loss: 0.69319522\n",
      "Step: [26808] d_loss: 1.38635087, g_loss: 0.69333172\n",
      "Step: [26809] d_loss: 1.38645577, g_loss: 0.69318390\n",
      "Step: [26810] d_loss: 1.38630176, g_loss: 0.69323516\n",
      "Step: [26811] d_loss: 1.38651967, g_loss: 0.69324327\n",
      "Step: [26812] d_loss: 1.38629007, g_loss: 0.69275296\n",
      "Step: [26813] d_loss: 1.38636816, g_loss: 0.69309598\n",
      "Step: [26814] d_loss: 1.38621473, g_loss: 0.69280678\n",
      "Step: [26815] d_loss: 1.38630867, g_loss: 0.69311225\n",
      "Step: [26816] d_loss: 1.38618934, g_loss: 0.69340289\n",
      "Step: [26817] d_loss: 1.38628209, g_loss: 0.69301689\n",
      "Step: [26818] d_loss: 1.38640094, g_loss: 0.69275433\n",
      "Step: [26819] d_loss: 1.38630223, g_loss: 0.69291812\n",
      "Step: [26820] d_loss: 1.38627410, g_loss: 0.69336927\n",
      "Step: [26821] d_loss: 1.38626552, g_loss: 0.69318539\n",
      "Step: [26822] d_loss: 1.38628793, g_loss: 0.69248116\n",
      "Step: [26823] d_loss: 1.38627577, g_loss: 0.69433069\n",
      "Step: [26824] d_loss: 1.38648820, g_loss: 0.69365799\n",
      "Step: [26825] d_loss: 1.38827217, g_loss: 0.69363713\n",
      "Step: [26826] d_loss: 1.38904631, g_loss: 0.69531357\n",
      "Step: [26827] d_loss: 1.38713980, g_loss: 0.69550252\n",
      "Step: [26828] d_loss: 1.38622046, g_loss: 0.69342703\n",
      "Step: [26829] d_loss: 1.38633156, g_loss: 0.69361824\n",
      "Step: [26830] d_loss: 1.38632166, g_loss: 0.69384676\n",
      "Step: [26831] d_loss: 1.38628101, g_loss: 0.69244230\n",
      "Step: [26832] d_loss: 1.38628697, g_loss: 0.69248754\n",
      "Step: [26833] d_loss: 1.38629174, g_loss: 0.69283867\n",
      "Step: [26834] d_loss: 1.38631368, g_loss: 0.69325531\n",
      "Step: [26835] d_loss: 1.38627517, g_loss: 0.69320208\n",
      "Step: [26836] d_loss: 1.38628244, g_loss: 0.69284040\n",
      "Step: [26837] d_loss: 1.38625908, g_loss: 0.69277453\n",
      "Step: [26838] d_loss: 1.38630438, g_loss: 0.69387341\n",
      "Step: [26839] d_loss: 1.38622546, g_loss: 0.69316673\n",
      "Step: [26840] d_loss: 1.38624537, g_loss: 0.69302899\n",
      "Step: [26841] d_loss: 1.38648343, g_loss: 0.69293523\n",
      "Step: [26842] d_loss: 1.38654256, g_loss: 0.69226259\n",
      "Step: [26843] d_loss: 1.38633871, g_loss: 0.69227135\n",
      "Step: [26844] d_loss: 1.38616264, g_loss: 0.69247431\n",
      "Step: [26845] d_loss: 1.38622975, g_loss: 0.69327921\n",
      "Step: [26846] d_loss: 1.38621545, g_loss: 0.69319117\n",
      "Step: [26847] d_loss: 1.38639057, g_loss: 0.69541669\n",
      "Step: [26848] d_loss: 1.38661134, g_loss: 0.69345379\n",
      "Step: [26849] d_loss: 1.38646460, g_loss: 0.69216430\n",
      "Step: [26850] d_loss: 1.38632298, g_loss: 0.69206411\n",
      "Step: [26851] d_loss: 1.38624167, g_loss: 0.69286126\n",
      "Step: [26852] d_loss: 1.38625646, g_loss: 0.69409025\n",
      "Step: [26853] d_loss: 1.38615966, g_loss: 0.69366109\n",
      "Step: [26854] d_loss: 1.38614488, g_loss: 0.69341743\n",
      "Step: [26855] d_loss: 1.38618135, g_loss: 0.69385439\n",
      "Step: [26856] d_loss: 1.38638759, g_loss: 0.69391090\n",
      "Step: [26857] d_loss: 1.38641524, g_loss: 0.69252175\n",
      "Step: [26858] d_loss: 1.38634396, g_loss: 0.69259804\n",
      "Step: [26859] d_loss: 1.38624907, g_loss: 0.69298404\n",
      "Step: [26860] d_loss: 1.38632655, g_loss: 0.69355059\n",
      "Step: [26861] d_loss: 1.38618100, g_loss: 0.69382185\n",
      "Step: [26862] d_loss: 1.38616633, g_loss: 0.69296539\n",
      "Step: [26863] d_loss: 1.38644218, g_loss: 0.69516528\n",
      "Step: [26864] d_loss: 1.38666153, g_loss: 0.69347346\n",
      "Step: [26865] d_loss: 1.38641906, g_loss: 0.69254583\n",
      "Step: [26866] d_loss: 1.38620400, g_loss: 0.69409490\n",
      "Step: [26867] d_loss: 1.38637161, g_loss: 0.69222063\n",
      "Step: [26868] d_loss: 1.38617992, g_loss: 0.69222695\n",
      "Step: [26869] d_loss: 1.38625360, g_loss: 0.69410872\n",
      "Step: [26870] d_loss: 1.38644850, g_loss: 0.69336462\n",
      "Step: [26871] d_loss: 1.38649023, g_loss: 0.69437855\n",
      "Step: [26872] d_loss: 1.38638544, g_loss: 0.69439256\n",
      "Step: [26873] d_loss: 1.38628268, g_loss: 0.69363832\n",
      "Step: [26874] d_loss: 1.38625717, g_loss: 0.69291115\n",
      "Step: [26875] d_loss: 1.38623345, g_loss: 0.69322824\n",
      "Step: [26876] d_loss: 1.38630581, g_loss: 0.69298565\n",
      "Step: [26877] d_loss: 1.38628578, g_loss: 0.69328487\n",
      "Step: [26878] d_loss: 1.38620949, g_loss: 0.69330460\n",
      "Step: [26879] d_loss: 1.38627923, g_loss: 0.69288665\n",
      "Step: [26880] d_loss: 1.38631284, g_loss: 0.69306141\n",
      "Step: [26881] d_loss: 1.38621533, g_loss: 0.69415069\n",
      "Step: [26882] d_loss: 1.38619804, g_loss: 0.69396681\n",
      "Step: [26883] d_loss: 1.38655221, g_loss: 0.69323337\n",
      "Step: [26884] d_loss: 1.38629913, g_loss: 0.69282770\n",
      "Step: [26885] d_loss: 1.38629770, g_loss: 0.69309855\n",
      "Step: [26886] d_loss: 1.38643646, g_loss: 0.69407600\n",
      "Step: [26887] d_loss: 1.38671541, g_loss: 0.69319439\n",
      "Step: [26888] d_loss: 1.38688660, g_loss: 0.69305921\n",
      "Step: [26889] d_loss: 1.38660324, g_loss: 0.69196403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [26890] d_loss: 1.38634646, g_loss: 0.69286847\n",
      "Step: [26891] d_loss: 1.38627768, g_loss: 0.69380468\n",
      "Step: [26892] d_loss: 1.38649046, g_loss: 0.69531965\n",
      "Step: [26893] d_loss: 1.38624465, g_loss: 0.69393349\n",
      "Step: [26894] d_loss: 1.38618517, g_loss: 0.69237900\n",
      "Step: [26895] d_loss: 1.38626599, g_loss: 0.69213212\n",
      "Step: [26896] d_loss: 1.38626122, g_loss: 0.69288313\n",
      "Step: [26897] d_loss: 1.38628531, g_loss: 0.69364810\n",
      "Step: [26898] d_loss: 1.38632989, g_loss: 0.69311213\n",
      "Step: [26899] d_loss: 1.38626122, g_loss: 0.69283772\n",
      "Step: [26900] d_loss: 1.38631773, g_loss: 0.69284296\n",
      "Step: [26901] d_loss: 1.38624561, g_loss: 0.69298226\n",
      "Step: [26902] d_loss: 1.38619471, g_loss: 0.69334745\n",
      "Step: [26903] d_loss: 1.38628435, g_loss: 0.69360530\n",
      "Step: [26904] d_loss: 1.38628829, g_loss: 0.69240654\n",
      "Step: [26905] d_loss: 1.38615346, g_loss: 0.69242960\n",
      "Step: [26906] d_loss: 1.38629210, g_loss: 0.69325638\n",
      "Step: [26907] d_loss: 1.38634300, g_loss: 0.69332397\n",
      "Step: [26908] d_loss: 1.38662851, g_loss: 0.69336867\n",
      "Step: [26909] d_loss: 1.38628662, g_loss: 0.69370210\n",
      "Step: [26910] d_loss: 1.38637722, g_loss: 0.69317758\n",
      "Step: [26911] d_loss: 1.38625813, g_loss: 0.69284272\n",
      "Step: [26912] d_loss: 1.38633621, g_loss: 0.69327831\n",
      "Step: [26913] d_loss: 1.38627911, g_loss: 0.69294512\n",
      "Step: [26914] d_loss: 1.38635862, g_loss: 0.69309849\n",
      "Step: [26915] d_loss: 1.38626289, g_loss: 0.69324380\n",
      "Step: [26916] d_loss: 1.38637495, g_loss: 0.69352973\n",
      "Step: [26917] d_loss: 1.38631022, g_loss: 0.69332713\n",
      "Step: [26918] d_loss: 1.38622856, g_loss: 0.69326138\n",
      "Step: [26919] d_loss: 1.38626897, g_loss: 0.69317794\n",
      "Step: [26920] d_loss: 1.38629532, g_loss: 0.69313413\n",
      "Step: [26921] d_loss: 1.38622284, g_loss: 0.69322205\n",
      "Step: [26922] d_loss: 1.38623142, g_loss: 0.69328117\n",
      "Step: [26923] d_loss: 1.38619852, g_loss: 0.69382352\n",
      "Step: [26924] d_loss: 1.38626432, g_loss: 0.69305515\n",
      "Step: [26925] d_loss: 1.38625646, g_loss: 0.69343048\n",
      "Step: [26926] d_loss: 1.38621736, g_loss: 0.69323158\n",
      "Step: [26927] d_loss: 1.38628674, g_loss: 0.69312942\n",
      "Step: [26928] d_loss: 1.38627744, g_loss: 0.69293082\n",
      "Step: [26929] d_loss: 1.38652515, g_loss: 0.69338107\n",
      "Step: [26930] d_loss: 1.38630688, g_loss: 0.69335747\n",
      "Step: [26931] d_loss: 1.38626504, g_loss: 0.69299924\n",
      "Step: [26932] d_loss: 1.38620806, g_loss: 0.69337493\n",
      "Step: [26933] d_loss: 1.38621879, g_loss: 0.69336724\n",
      "Step: [26934] d_loss: 1.38628674, g_loss: 0.69278193\n",
      "Step: [26935] d_loss: 1.38631558, g_loss: 0.69297898\n",
      "Step: [26936] d_loss: 1.38942862, g_loss: 0.69518936\n",
      "Step: [26937] d_loss: 1.38667858, g_loss: 0.69291997\n",
      "Step: [26938] d_loss: 1.38660777, g_loss: 0.69266051\n",
      "Step: [26939] d_loss: 1.38638639, g_loss: 0.69276178\n",
      "Step: [26940] d_loss: 1.38652647, g_loss: 0.69258004\n",
      "Step: [26941] d_loss: 1.38654709, g_loss: 0.69198126\n",
      "Step: [26942] d_loss: 1.38646090, g_loss: 0.69297278\n",
      "Step: [26943] d_loss: 1.38630819, g_loss: 0.69332695\n",
      "Step: [26944] d_loss: 1.38627744, g_loss: 0.69357848\n",
      "Step: [26945] d_loss: 1.38626003, g_loss: 0.69412744\n",
      "Step: [26946] d_loss: 1.38629591, g_loss: 0.69330239\n",
      "Step: [26947] d_loss: 1.38619804, g_loss: 0.69288993\n",
      "Step: [26948] d_loss: 1.38613987, g_loss: 0.69284612\n",
      "Step: [26949] d_loss: 1.38630116, g_loss: 0.69299281\n",
      "Step: [26950] d_loss: 1.38642836, g_loss: 0.69520950\n",
      "Step: [26951] d_loss: 1.38642979, g_loss: 0.69240582\n",
      "Step: [26952] d_loss: 1.38659334, g_loss: 0.69593406\n",
      "Step: [26953] d_loss: 1.38669038, g_loss: 0.69595397\n",
      "Step: [26954] d_loss: 1.38665569, g_loss: 0.69501173\n",
      "Step: [26955] d_loss: 1.38625491, g_loss: 0.69279057\n",
      "Step: [26956] d_loss: 1.38614058, g_loss: 0.69226229\n",
      "Step: [26957] d_loss: 1.38627350, g_loss: 0.69215000\n",
      "Step: [26958] d_loss: 1.38619769, g_loss: 0.69295824\n",
      "Step: [26959] d_loss: 1.38611364, g_loss: 0.69349456\n",
      "Step: [26960] d_loss: 1.38616133, g_loss: 0.69385326\n",
      "Step: [26961] d_loss: 1.38616431, g_loss: 0.69341558\n",
      "Step: [26962] d_loss: 1.38693595, g_loss: 0.69258749\n",
      "Step: [26963] d_loss: 1.38622308, g_loss: 0.69446725\n",
      "Step: [26964] d_loss: 1.38650179, g_loss: 0.69359773\n",
      "Step: [26965] d_loss: 1.38653636, g_loss: 0.69277143\n",
      "Step: [26966] d_loss: 1.38651276, g_loss: 0.69258839\n",
      "Step: [26967] d_loss: 1.38674676, g_loss: 0.70078790\n",
      "Step: [26968] d_loss: 1.38695168, g_loss: 0.69458050\n",
      "Step: [26969] d_loss: 1.38677645, g_loss: 0.69210070\n",
      "Step: [26970] d_loss: 1.38635564, g_loss: 0.68875742\n",
      "Step: [26971] d_loss: 1.38600576, g_loss: 0.69093108\n",
      "Step: [26972] d_loss: 1.38628244, g_loss: 0.69402832\n",
      "Step: [26973] d_loss: 1.38627458, g_loss: 0.69556296\n",
      "Step: [26974] d_loss: 1.38631582, g_loss: 0.69519585\n",
      "Step: [26975] d_loss: 1.38623524, g_loss: 0.69362992\n",
      "Step: [26976] d_loss: 1.38641596, g_loss: 0.69393480\n",
      "Step: [26977] d_loss: 1.38633800, g_loss: 0.69297481\n",
      "Step: [26978] d_loss: 1.38638616, g_loss: 0.69266438\n",
      "Step: [26979] d_loss: 1.38618541, g_loss: 0.69299841\n",
      "Step: [26980] d_loss: 1.38623714, g_loss: 0.69352871\n",
      "Step: [26981] d_loss: 1.38623989, g_loss: 0.69217503\n",
      "Step: [26982] d_loss: 1.38633108, g_loss: 0.69271469\n",
      "Step: [26983] d_loss: 1.38647366, g_loss: 0.69321656\n",
      "Step: [26984] d_loss: 1.38610852, g_loss: 0.69364405\n",
      "Step: [26985] d_loss: 1.38622260, g_loss: 0.69363320\n",
      "Step: [26986] d_loss: 1.38619721, g_loss: 0.69311905\n",
      "Step: [26987] d_loss: 1.38623905, g_loss: 0.69297516\n",
      "Step: [26988] d_loss: 1.38627863, g_loss: 0.69301468\n",
      "Step: [26989] d_loss: 1.38628161, g_loss: 0.69281739\n",
      "Step: [26990] d_loss: 1.38631225, g_loss: 0.69239664\n",
      "Step: [26991] d_loss: 1.38630939, g_loss: 0.69333512\n",
      "Step: [26992] d_loss: 1.38645291, g_loss: 0.69393826\n",
      "Step: [26993] d_loss: 1.38647783, g_loss: 0.69364977\n",
      "Step: [26994] d_loss: 1.38628292, g_loss: 0.69427264\n",
      "Step: [26995] d_loss: 1.38629198, g_loss: 0.69319582\n",
      "Step: [26996] d_loss: 1.38596594, g_loss: 0.69394886\n",
      "Step: [26997] d_loss: 1.38629293, g_loss: 0.69343078\n",
      "Step: [26998] d_loss: 1.38615310, g_loss: 0.69236493\n",
      "Step: [26999] d_loss: 1.38637948, g_loss: 0.69264758\n",
      "Step: [27000] d_loss: 1.38636923, g_loss: 0.69375592\n",
      "Step: [27001] d_loss: 1.38622165, g_loss: 0.69333595\n",
      "Step: [27002] d_loss: 1.38628554, g_loss: 0.69293904\n",
      "Step: [27003] d_loss: 1.38643157, g_loss: 0.69051754\n",
      "Step: [27004] d_loss: 1.38631868, g_loss: 0.69407272\n",
      "Step: [27005] d_loss: 1.38684618, g_loss: 0.69418854\n",
      "Step: [27006] d_loss: 1.38706136, g_loss: 0.69469881\n",
      "Step: [27007] d_loss: 1.38675189, g_loss: 0.69419873\n",
      "Step: [27008] d_loss: 1.38625646, g_loss: 0.69368827\n",
      "Step: [27009] d_loss: 1.38630807, g_loss: 0.69313085\n",
      "Step: [27010] d_loss: 1.38629794, g_loss: 0.69242072\n",
      "Step: [27011] d_loss: 1.38633120, g_loss: 0.69295800\n",
      "Step: [27012] d_loss: 1.38471150, g_loss: 0.69609994\n",
      "Step: [27013] d_loss: 1.38632846, g_loss: 0.69362128\n",
      "Step: [27014] d_loss: 1.38634956, g_loss: 0.69334143\n",
      "Step: [27015] d_loss: 1.38638210, g_loss: 0.69274533\n",
      "Step: [27016] d_loss: 1.38628280, g_loss: 0.69311351\n",
      "Step: [27017] d_loss: 1.38624573, g_loss: 0.69304657\n",
      "Step: [27018] d_loss: 1.38598430, g_loss: 0.69330227\n",
      "Step: [27019] d_loss: 1.38633788, g_loss: 0.69337893\n",
      "Step: [27020] d_loss: 1.38654590, g_loss: 0.69168448\n",
      "Step: [27021] d_loss: 1.38618851, g_loss: 0.69243884\n",
      "Step: [27022] d_loss: 1.38621306, g_loss: 0.69498593\n",
      "Step: [27023] d_loss: 1.38630545, g_loss: 0.69374859\n",
      "Step: [27024] d_loss: 1.38611054, g_loss: 0.69327044\n",
      "Step: [27025] d_loss: 1.38605380, g_loss: 0.69373953\n",
      "Step: [27026] d_loss: 1.38618517, g_loss: 0.69331431\n",
      "Step: [27027] d_loss: 1.38609529, g_loss: 0.69304264\n",
      "Step: [27028] d_loss: 1.38621938, g_loss: 0.69240320\n",
      "Step: [27029] d_loss: 1.38614082, g_loss: 0.69351721\n",
      "Step: [27030] d_loss: 1.38596451, g_loss: 0.69411474\n",
      "Step: [27031] d_loss: 1.38635349, g_loss: 0.69231766\n",
      "Step: [27032] d_loss: 1.38612127, g_loss: 0.69301927\n",
      "Step: [27033] d_loss: 1.38594949, g_loss: 0.69266880\n",
      "Step: [27034] d_loss: 1.38622248, g_loss: 0.69343662\n",
      "Step: [27035] d_loss: 1.38641298, g_loss: 0.69333845\n",
      "Step: [27036] d_loss: 1.38631403, g_loss: 0.69302011\n",
      "Step: [27037] d_loss: 1.38629305, g_loss: 0.69345534\n",
      "Step: [27038] d_loss: 1.38631690, g_loss: 0.69229692\n",
      "Step: [27039] d_loss: 1.38675427, g_loss: 0.69475853\n",
      "Step: [27040] d_loss: 1.38686895, g_loss: 0.69481218\n",
      "Step: [27041] d_loss: 1.38615012, g_loss: 0.69231886\n",
      "Step: [27042] d_loss: 1.38636243, g_loss: 0.69072270\n",
      "Step: [27043] d_loss: 1.38623810, g_loss: 0.69008958\n",
      "Step: [27044] d_loss: 1.38626838, g_loss: 0.69254023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [27045] d_loss: 1.38564348, g_loss: 0.69748926\n",
      "Step: [27046] d_loss: 1.38631248, g_loss: 0.69334149\n",
      "Step: [27047] d_loss: 1.38627172, g_loss: 0.69252026\n",
      "Step: [27048] d_loss: 1.38619578, g_loss: 0.69184518\n",
      "Step: [27049] d_loss: 1.38617074, g_loss: 0.69282138\n",
      "Step: [27050] d_loss: 1.38626289, g_loss: 0.69304037\n",
      "Step: [27051] d_loss: 1.38627386, g_loss: 0.69359332\n",
      "Step: [27052] d_loss: 1.38625383, g_loss: 0.69333148\n",
      "Step: [27053] d_loss: 1.38629961, g_loss: 0.69314420\n",
      "Step: [27054] d_loss: 1.38612962, g_loss: 0.69287395\n",
      "Step: [27055] d_loss: 1.38628495, g_loss: 0.69319856\n",
      "Step: [27056] d_loss: 1.38631058, g_loss: 0.69331789\n",
      "Step: [27057] d_loss: 1.38601017, g_loss: 0.69315422\n",
      "Step: [27058] d_loss: 1.38627839, g_loss: 0.69301426\n",
      "Step: [27059] d_loss: 1.38656509, g_loss: 0.69412196\n",
      "Step: [27060] d_loss: 1.38635349, g_loss: 0.69284570\n",
      "Step: [27061] d_loss: 1.38641667, g_loss: 0.69390643\n",
      "Step: [27062] d_loss: 1.38665390, g_loss: 0.69385791\n",
      "Step: [27063] d_loss: 1.38661563, g_loss: 0.69003576\n",
      "Step: [27064] d_loss: 1.38618648, g_loss: 0.69115961\n",
      "Step: [27065] d_loss: 1.38621759, g_loss: 0.69160438\n",
      "Step: [27066] d_loss: 1.38634562, g_loss: 0.69484746\n",
      "Step: [27067] d_loss: 1.38623667, g_loss: 0.69455004\n",
      "Step: [27068] d_loss: 1.38625634, g_loss: 0.69349301\n",
      "Step: [27069] d_loss: 1.38623846, g_loss: 0.69321918\n",
      "Step: [27070] d_loss: 1.38612342, g_loss: 0.69221127\n",
      "Step: [27071] d_loss: 1.38634789, g_loss: 0.69314122\n",
      "Step: [27072] d_loss: 1.38621557, g_loss: 0.69351256\n",
      "Step: [27073] d_loss: 1.38625109, g_loss: 0.69413257\n",
      "Step: [27074] d_loss: 1.38615561, g_loss: 0.69287676\n",
      "Step: [27075] d_loss: 1.38594007, g_loss: 0.69120753\n",
      "Step: [27076] d_loss: 1.38623548, g_loss: 0.69236648\n",
      "Step: [27077] d_loss: 1.38627458, g_loss: 0.69560021\n",
      "Step: [27078] d_loss: 1.38640249, g_loss: 0.69363785\n",
      "Step: [27079] d_loss: 1.38634050, g_loss: 0.69272494\n",
      "Step: [27080] d_loss: 1.38616705, g_loss: 0.69270444\n",
      "Step: [27081] d_loss: 1.38621604, g_loss: 0.69367921\n",
      "Step: [27082] d_loss: 1.38632703, g_loss: 0.69309825\n",
      "Step: [27083] d_loss: 1.38640332, g_loss: 0.69442463\n",
      "Step: [27084] d_loss: 1.38723564, g_loss: 0.69512689\n",
      "Step: [27085] d_loss: 1.38786650, g_loss: 0.69527930\n",
      "Step: [27086] d_loss: 1.38761020, g_loss: 0.68760550\n",
      "Step: [27087] d_loss: 1.38677430, g_loss: 0.69155610\n",
      "Step: [27088] d_loss: 1.38630915, g_loss: 0.69390619\n",
      "Step: [27089] d_loss: 1.38622403, g_loss: 0.69455445\n",
      "Step: [27090] d_loss: 1.38632584, g_loss: 0.69379592\n",
      "Step: [27091] d_loss: 1.38626969, g_loss: 0.69287395\n",
      "Step: [27092] d_loss: 1.38631630, g_loss: 0.69295651\n",
      "Step: [27093] d_loss: 1.38626933, g_loss: 0.69346029\n",
      "Step: [27094] d_loss: 1.38619483, g_loss: 0.69321752\n",
      "Step: [27095] d_loss: 1.38631880, g_loss: 0.69336092\n",
      "Step: [27096] d_loss: 1.38622713, g_loss: 0.69335973\n",
      "Step: [27097] d_loss: 1.38634050, g_loss: 0.69223458\n",
      "Step: [27098] d_loss: 1.38638544, g_loss: 0.69278252\n",
      "Step: [27099] d_loss: 1.38645196, g_loss: 0.69202602\n",
      "Step: [27100] d_loss: 1.38626003, g_loss: 0.69346905\n",
      "Step: [27101] d_loss: 1.38687611, g_loss: 0.69360399\n",
      "Step: [27102] d_loss: 1.38621294, g_loss: 0.69340038\n",
      "Step: [27103] d_loss: 1.38636506, g_loss: 0.69318932\n",
      "Step: [27104] d_loss: 1.38615108, g_loss: 0.69322437\n",
      "Step: [27105] d_loss: 1.38613534, g_loss: 0.69322729\n",
      "Step: [27106] d_loss: 1.38608050, g_loss: 0.69335395\n",
      "Step: [27107] d_loss: 1.38629913, g_loss: 0.69301903\n",
      "Step: [27108] d_loss: 1.38623130, g_loss: 0.69369006\n",
      "Step: [27109] d_loss: 1.38618076, g_loss: 0.69342488\n",
      "Step: [27110] d_loss: 1.38630486, g_loss: 0.69297808\n",
      "Step: [27111] d_loss: 1.38613296, g_loss: 0.69357085\n",
      "Step: [27112] d_loss: 1.38639522, g_loss: 0.69334590\n",
      "Step: [27113] d_loss: 1.38632858, g_loss: 0.69418490\n",
      "Step: [27114] d_loss: 1.38649249, g_loss: 0.69359183\n",
      "Step: [27115] d_loss: 1.38633561, g_loss: 0.69282317\n",
      "Step: [27116] d_loss: 1.38640785, g_loss: 0.69155645\n",
      "Step: [27117] d_loss: 1.38627815, g_loss: 0.69326007\n",
      "Step: [27118] d_loss: 1.38621199, g_loss: 0.69432753\n",
      "Step: [27119] d_loss: 1.38657272, g_loss: 0.69491959\n",
      "Step: [27120] d_loss: 1.38629591, g_loss: 0.69259787\n",
      "Step: [27121] d_loss: 1.38629842, g_loss: 0.69200218\n",
      "Step: [27122] d_loss: 1.38632250, g_loss: 0.69166541\n",
      "Step: [27123] d_loss: 1.38635552, g_loss: 0.69371599\n",
      "Step: [27124] d_loss: 1.38632715, g_loss: 0.69489688\n",
      "Step: [27125] d_loss: 1.38647747, g_loss: 0.69439232\n",
      "Step: [27126] d_loss: 1.38638115, g_loss: 0.69382262\n",
      "Step: [27127] d_loss: 1.38617158, g_loss: 0.69253647\n",
      "Step: [27128] d_loss: 1.38625383, g_loss: 0.69195396\n",
      "Step: [27129] d_loss: 1.38657057, g_loss: 0.69364035\n",
      "Step: [27130] d_loss: 1.38677013, g_loss: 0.69382185\n",
      "Step: [27131] d_loss: 1.38667858, g_loss: 0.69682002\n",
      "Step: [27132] d_loss: 1.38640594, g_loss: 0.69517100\n",
      "Step: [27133] d_loss: 1.38633549, g_loss: 0.69350541\n",
      "Step: [27134] d_loss: 1.38621163, g_loss: 0.69302768\n",
      "Step: [27135] d_loss: 1.38635576, g_loss: 0.69306314\n",
      "Step: [27136] d_loss: 1.38648844, g_loss: 0.69117188\n",
      "Step: [27137] d_loss: 1.38633859, g_loss: 0.69276667\n",
      "Step: [27138] d_loss: 1.38802624, g_loss: 0.69404566\n",
      "Step: [27139] d_loss: 1.38623393, g_loss: 0.69627345\n",
      "Step: [27140] d_loss: 1.38617373, g_loss: 0.69214845\n",
      "Step: [27141] d_loss: 1.38619053, g_loss: 0.69231367\n",
      "Step: [27142] d_loss: 1.38667333, g_loss: 0.69284940\n",
      "Step: [27143] d_loss: 1.38742089, g_loss: 0.69343007\n",
      "Step: [27144] d_loss: 1.38748908, g_loss: 0.69452250\n",
      "Step: [27145] d_loss: 1.38691950, g_loss: 0.69464827\n",
      "Step: [27146] d_loss: 1.38642871, g_loss: 0.69357049\n",
      "Step: [27147] d_loss: 1.38619089, g_loss: 0.69268930\n",
      "Step: [27148] d_loss: 1.38628793, g_loss: 0.69319344\n",
      "Step: [27149] d_loss: 1.38703823, g_loss: 0.69227099\n",
      "Step: [27150] d_loss: 1.38630939, g_loss: 0.69470179\n",
      "Step: [27151] d_loss: 1.38625288, g_loss: 0.69381225\n",
      "Step: [27152] d_loss: 1.38637376, g_loss: 0.69287443\n",
      "Step: [27153] d_loss: 1.38626885, g_loss: 0.69279230\n",
      "Step: [27154] d_loss: 1.38629425, g_loss: 0.69192207\n",
      "Step: [27155] d_loss: 1.38646221, g_loss: 0.69322312\n",
      "Step: [27156] d_loss: 1.38660359, g_loss: 0.69074547\n",
      "Step: [27157] d_loss: 1.38648164, g_loss: 0.69319594\n",
      "Step: [27158] d_loss: 1.38641620, g_loss: 0.69418079\n",
      "Step: [27159] d_loss: 1.38621211, g_loss: 0.69264197\n",
      "Step: [27160] d_loss: 1.38667917, g_loss: 0.70006329\n",
      "Step: [27161] d_loss: 1.38707769, g_loss: 0.69850719\n",
      "Step: [27162] d_loss: 1.38693953, g_loss: 0.69509560\n",
      "Step: [27163] d_loss: 1.38642979, g_loss: 0.68900418\n",
      "Step: [27164] d_loss: 1.38629663, g_loss: 0.68979669\n",
      "Step: [27165] d_loss: 1.38637686, g_loss: 0.69210136\n",
      "Step: [27166] d_loss: 1.38669038, g_loss: 0.69584596\n",
      "Step: [27167] d_loss: 1.38671565, g_loss: 0.69540060\n",
      "Step: [27168] d_loss: 1.38618457, g_loss: 0.69607949\n",
      "Step: [27169] d_loss: 1.38636029, g_loss: 0.69307613\n",
      "Step: [27170] d_loss: 1.38633204, g_loss: 0.69120926\n",
      "Step: [27171] d_loss: 1.38629484, g_loss: 0.69277155\n",
      "Step: [27172] d_loss: 1.38635981, g_loss: 0.69316947\n",
      "Step: [27173] d_loss: 1.38630939, g_loss: 0.69314957\n",
      "Step: [27174] d_loss: 1.38627541, g_loss: 0.69258922\n",
      "Step: [27175] d_loss: 1.38623381, g_loss: 0.69255888\n",
      "Step: [27176] d_loss: 1.38643420, g_loss: 0.69415635\n",
      "Step: [27177] d_loss: 1.38658786, g_loss: 0.69347477\n",
      "Step: [27178] d_loss: 1.38653231, g_loss: 0.69443148\n",
      "Step: [27179] d_loss: 1.38632369, g_loss: 0.69398910\n",
      "Step: [27180] d_loss: 1.38629794, g_loss: 0.69393468\n",
      "Step: [27181] d_loss: 1.38631344, g_loss: 0.69360507\n",
      "Step: [27182] d_loss: 1.38630748, g_loss: 0.69239295\n",
      "Step: [27183] d_loss: 1.38628292, g_loss: 0.69227678\n",
      "Step: [27184] d_loss: 1.38636923, g_loss: 0.69276059\n",
      "Step: [27185] d_loss: 1.38632441, g_loss: 0.69333547\n",
      "Step: [27186] d_loss: 1.38630664, g_loss: 0.69353843\n",
      "Step: [27187] d_loss: 1.38628817, g_loss: 0.69263494\n",
      "Step: [27188] d_loss: 1.38630652, g_loss: 0.69302613\n",
      "Step: [27189] d_loss: 1.38652885, g_loss: 0.69299060\n",
      "Step: [27190] d_loss: 1.38631165, g_loss: 0.69319367\n",
      "Step: [27191] d_loss: 1.38632989, g_loss: 0.69305348\n",
      "Step: [27192] d_loss: 1.38629007, g_loss: 0.69313556\n",
      "Step: [27193] d_loss: 1.38630080, g_loss: 0.69301152\n",
      "Step: [27194] d_loss: 1.38627386, g_loss: 0.69311774\n",
      "Step: [27195] d_loss: 1.38629723, g_loss: 0.69316870\n",
      "Step: [27196] d_loss: 1.38629961, g_loss: 0.69296592\n",
      "Step: [27197] d_loss: 1.38629603, g_loss: 0.69275439\n",
      "Step: [27198] d_loss: 1.38634634, g_loss: 0.69233239\n",
      "Step: [27199] d_loss: 1.38631475, g_loss: 0.69337720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [27200] d_loss: 1.38411283, g_loss: 0.69021827\n",
      "Step: [27201] d_loss: 1.38632858, g_loss: 0.69374907\n",
      "Step: [27202] d_loss: 1.38641286, g_loss: 0.69262081\n",
      "Step: [27203] d_loss: 1.38632226, g_loss: 0.69153607\n",
      "Step: [27204] d_loss: 1.38631725, g_loss: 0.69182301\n",
      "Step: [27205] d_loss: 1.38631010, g_loss: 0.69252640\n",
      "Step: [27206] d_loss: 1.38625669, g_loss: 0.69332248\n",
      "Step: [27207] d_loss: 1.38631690, g_loss: 0.69364583\n",
      "Step: [27208] d_loss: 1.38625395, g_loss: 0.69331884\n",
      "Step: [27209] d_loss: 1.38627577, g_loss: 0.69308233\n",
      "Step: [27210] d_loss: 1.38762307, g_loss: 0.69207770\n",
      "Step: [27211] d_loss: 1.38624668, g_loss: 0.69395047\n",
      "Step: [27212] d_loss: 1.38631010, g_loss: 0.69304001\n",
      "Step: [27213] d_loss: 1.38629127, g_loss: 0.69288641\n",
      "Step: [27214] d_loss: 1.38625753, g_loss: 0.69296455\n",
      "Step: [27215] d_loss: 1.38626814, g_loss: 0.69307542\n",
      "Step: [27216] d_loss: 1.38620901, g_loss: 0.69295812\n",
      "Step: [27217] d_loss: 1.38744402, g_loss: 0.69165003\n",
      "Step: [27218] d_loss: 1.38623357, g_loss: 0.69465566\n",
      "Step: [27219] d_loss: 1.38628650, g_loss: 0.69333470\n",
      "Step: [27220] d_loss: 1.38632965, g_loss: 0.69274133\n",
      "Step: [27221] d_loss: 1.38627148, g_loss: 0.69280493\n",
      "Step: [27222] d_loss: 1.38637662, g_loss: 0.69268537\n",
      "Step: [27223] d_loss: 1.38634074, g_loss: 0.69263375\n",
      "Step: [27224] d_loss: 1.38632083, g_loss: 0.69307506\n",
      "Step: [27225] d_loss: 1.38626993, g_loss: 0.69322181\n",
      "Step: [27226] d_loss: 1.38625586, g_loss: 0.69333333\n",
      "Step: [27227] d_loss: 1.38625836, g_loss: 0.69329774\n",
      "Step: [27228] d_loss: 1.38629079, g_loss: 0.69316721\n",
      "Step: [27229] d_loss: 1.38672316, g_loss: 0.68876338\n",
      "Step: [27230] d_loss: 1.38627696, g_loss: 0.69603837\n",
      "Step: [27231] d_loss: 1.38629687, g_loss: 0.69420952\n",
      "Step: [27232] d_loss: 1.38627470, g_loss: 0.69306338\n",
      "Step: [27233] d_loss: 1.38622642, g_loss: 0.69261879\n",
      "Step: [27234] d_loss: 1.38624632, g_loss: 0.69299960\n",
      "Step: [27235] d_loss: 1.38629341, g_loss: 0.69307679\n",
      "Step: [27236] d_loss: 1.38628185, g_loss: 0.69375461\n",
      "Step: [27237] d_loss: 1.38635957, g_loss: 0.69407368\n",
      "Step: [27238] d_loss: 1.38649940, g_loss: 0.69406652\n",
      "Step: [27239] d_loss: 1.38665843, g_loss: 0.69299620\n",
      "Step: [27240] d_loss: 1.38633776, g_loss: 0.69274330\n",
      "Step: [27241] d_loss: 1.38629508, g_loss: 0.69381249\n",
      "Step: [27242] d_loss: 1.38629651, g_loss: 0.69406110\n",
      "Step: [27243] d_loss: 1.38629007, g_loss: 0.69299507\n",
      "Step: [27244] d_loss: 1.38628757, g_loss: 0.69284695\n",
      "Step: [27245] d_loss: 1.38627887, g_loss: 0.69270420\n",
      "Step: [27246] d_loss: 1.38630986, g_loss: 0.69306231\n",
      "Step: [27247] d_loss: 1.38629723, g_loss: 0.69343966\n",
      "Step: [27248] d_loss: 1.38623691, g_loss: 0.69352543\n",
      "Step: [27249] d_loss: 1.38626981, g_loss: 0.69349557\n",
      "Step: [27250] d_loss: 1.38627744, g_loss: 0.69338071\n",
      "Step: [27251] d_loss: 1.38627148, g_loss: 0.69311261\n",
      "Step: [27252] d_loss: 1.38625717, g_loss: 0.69292176\n",
      "Step: [27253] d_loss: 1.38623524, g_loss: 0.69290352\n",
      "Step: [27254] d_loss: 1.38632643, g_loss: 0.69322741\n",
      "Step: [27255] d_loss: 1.38629198, g_loss: 0.69306850\n",
      "Step: [27256] d_loss: 1.38626492, g_loss: 0.69303727\n",
      "Step: [27257] d_loss: 1.38623869, g_loss: 0.69308168\n",
      "Step: [27258] d_loss: 1.38616610, g_loss: 0.69323999\n",
      "Step: [27259] d_loss: 1.38644564, g_loss: 0.69176972\n",
      "Step: [27260] d_loss: 1.38635278, g_loss: 0.69370008\n",
      "Step: [27261] d_loss: 1.38627613, g_loss: 0.69354659\n",
      "Step: [27262] d_loss: 1.38630652, g_loss: 0.69331270\n",
      "Step: [27263] d_loss: 1.38625813, g_loss: 0.69309700\n",
      "Step: [27264] d_loss: 1.38632226, g_loss: 0.69277954\n",
      "Step: [27265] d_loss: 1.38621569, g_loss: 0.69283390\n",
      "Step: [27266] d_loss: 1.38622618, g_loss: 0.69385684\n",
      "Step: [27267] d_loss: 1.38629627, g_loss: 0.69261765\n",
      "Step: [27268] d_loss: 1.38626695, g_loss: 0.69273353\n",
      "Step: [27269] d_loss: 1.38640964, g_loss: 0.69384772\n",
      "Step: [27270] d_loss: 1.38639736, g_loss: 0.69208884\n",
      "Step: [27271] d_loss: 1.38633716, g_loss: 0.69511104\n",
      "Step: [27272] d_loss: 1.38632202, g_loss: 0.69387192\n",
      "Step: [27273] d_loss: 1.38624215, g_loss: 0.69377625\n",
      "Step: [27274] d_loss: 1.38628781, g_loss: 0.69265783\n",
      "Step: [27275] d_loss: 1.38634157, g_loss: 0.69197547\n",
      "Step: [27276] d_loss: 1.38640976, g_loss: 0.69228417\n",
      "Step: [27277] d_loss: 1.38627481, g_loss: 0.69317055\n",
      "Step: [27278] d_loss: 1.38631785, g_loss: 0.69308531\n",
      "Step: [27279] d_loss: 1.38626051, g_loss: 0.69319087\n",
      "Step: [27280] d_loss: 1.38627338, g_loss: 0.69316864\n",
      "Step: [27281] d_loss: 1.38621211, g_loss: 0.69319952\n",
      "Step: [27282] d_loss: 1.38624740, g_loss: 0.69312394\n",
      "Step: [27283] d_loss: 1.38626349, g_loss: 0.69306064\n",
      "Step: [27284] d_loss: 1.38616586, g_loss: 0.69292456\n",
      "Step: [27285] d_loss: 1.38634515, g_loss: 0.69309592\n",
      "Step: [27286] d_loss: 1.38626111, g_loss: 0.69360667\n",
      "Step: [27287] d_loss: 1.38628459, g_loss: 0.69336855\n",
      "Step: [27288] d_loss: 1.38623488, g_loss: 0.69305575\n",
      "Step: [27289] d_loss: 1.38748455, g_loss: 0.69263905\n",
      "Step: [27290] d_loss: 1.38621759, g_loss: 0.69297540\n",
      "Step: [27291] d_loss: 1.38627601, g_loss: 0.69345522\n",
      "Step: [27292] d_loss: 1.38628840, g_loss: 0.69330859\n",
      "Step: [27293] d_loss: 1.38625622, g_loss: 0.69319665\n",
      "Step: [27294] d_loss: 1.38628185, g_loss: 0.69303644\n",
      "Step: [27295] d_loss: 1.38620496, g_loss: 0.69316924\n",
      "Step: [27296] d_loss: 1.38632119, g_loss: 0.69325852\n",
      "Step: [27297] d_loss: 1.38632321, g_loss: 0.69308174\n",
      "Step: [27298] d_loss: 1.38626897, g_loss: 0.69348121\n",
      "Step: [27299] d_loss: 1.38635218, g_loss: 0.69268209\n",
      "Step: [27300] d_loss: 1.38631749, g_loss: 0.69405329\n",
      "Step: [27301] d_loss: 1.38636386, g_loss: 0.69356751\n",
      "Step: [27302] d_loss: 1.38628697, g_loss: 0.69304889\n",
      "Step: [27303] d_loss: 1.38626349, g_loss: 0.69307834\n",
      "Step: [27304] d_loss: 1.38629997, g_loss: 0.69292676\n",
      "Step: [27305] d_loss: 1.38629317, g_loss: 0.69341850\n",
      "Step: [27306] d_loss: 1.38628697, g_loss: 0.69328284\n",
      "Step: [27307] d_loss: 1.38627422, g_loss: 0.69316798\n",
      "Step: [27308] d_loss: 1.38623714, g_loss: 0.69338715\n",
      "Step: [27309] d_loss: 1.38625896, g_loss: 0.69301140\n",
      "Step: [27310] d_loss: 1.38631940, g_loss: 0.69306946\n",
      "Step: [27311] d_loss: 1.38618588, g_loss: 0.69338650\n",
      "Step: [27312] d_loss: 1.38628078, g_loss: 0.69369555\n",
      "Step: [27313] d_loss: 1.38620400, g_loss: 0.69420618\n",
      "Step: [27314] d_loss: 1.38639951, g_loss: 0.69241965\n",
      "Step: [27315] d_loss: 1.38625407, g_loss: 0.69362718\n",
      "Step: [27316] d_loss: 1.38639677, g_loss: 0.69426680\n",
      "Step: [27317] d_loss: 1.38671219, g_loss: 0.69318557\n",
      "Step: [27318] d_loss: 1.38720024, g_loss: 0.69401586\n",
      "Step: [27319] d_loss: 1.38721585, g_loss: 0.68918109\n",
      "Step: [27320] d_loss: 1.38691163, g_loss: 0.69943297\n",
      "Step: [27321] d_loss: 1.38653994, g_loss: 0.69820297\n",
      "Step: [27322] d_loss: 1.38635755, g_loss: 0.69427925\n",
      "Step: [27323] d_loss: 1.38625407, g_loss: 0.69133449\n",
      "Step: [27324] d_loss: 1.38631856, g_loss: 0.69154716\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 25*1093\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    \n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "    # update D network\n",
    "    _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, z: batch_z, y:batch_labels})\n",
    "    # update G network\n",
    "    _, G_loss = sess.run([g_optim, g_loss], feed_dict={z: batch_z, y:batch_labels})\n",
    "\n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "\n",
    "        samples = sess.run(fake_images, feed_dict={z:sample_z, y:test_labels_onehot})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :, :, :].squeeze()\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center/>one of the result image</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
