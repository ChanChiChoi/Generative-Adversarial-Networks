{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most codes from https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(net,scope,is_training):\n",
    "    return tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope=scope)\n",
    "\n",
    "def conv(net,wscope,bscope,output_depth = 64, receptive_field=[5,5],stride=[2,2]):\n",
    "    shape = net.get_shape()\n",
    "    weights = tf.get_variable(wscope, receptive_field+[shape[-1], output_depth],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.nn.conv2d(net, weights, strides=[1]+stride+[1], padding='SAME')\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def linear(net,wscope,bscope,output_depth):\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "\n",
    "def deconv(net,wscope,bscope,output_shape,receptive_field=[5,5],stride=[2,2]):\n",
    "    weights = tf.get_variable(wscope, receptive_field+[output_shape[-1], net.get_shape()[-1]],\n",
    "                                       initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "    net = tf.nn.conv2d_transpose(net, weights, output_shape=output_shape, strides=[1]+stride+[1])\n",
    "    biases = tf.get_variable(bscope, [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def get_shape(net):\n",
    "    return net.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"discriminator\" function, the \"discriminator_y\" only has more one \"y\" parameter than \"discriminator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inputs, batch_size, is_training=True, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: Conv -> lrelu'''\n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',64)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''2nd: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv2','d_bconv2',64*2)\n",
    "        net = bn(net, scope = 'd_bn2',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''3th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv3','d_bconv3',64*4)\n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''4th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv4','d_bconv4',64*8)\n",
    "        net = bn(net, scope = 'd_bn4',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        net = tf.reshape(net, [batch_size, -1])   \n",
    "\n",
    "        '''5th: linear '''\n",
    "        out_logit = linear(net,\"d_wlinear5\",\"d_blinear5\",1)\n",
    "        '''6th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "\n",
    "        return out, out_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_y(inputs, y, y_dim, batch_size, is_training=True, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator_y\", reuse=reuse):\n",
    "\n",
    "        yb = tf.reshape(y,shape=[batch_size,1,1,y_dim])\n",
    "        '''1st:Concat -> conv -> lrelu'''\n",
    "        inputs = tf.concat([inputs, yb*tf.ones(get_shape(inputs)[:-1]+[get_shape(yb)[-1]])],axis=3)       \n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',inputs.get_shape()[-1])\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''2nd:Concat -> conv -> bn -> lrelu'''\n",
    "        net = tf.concat([net, yb*tf.ones(get_shape(net)[:-1]+[get_shape(yb)[-1]] )],axis=3)\n",
    "        net = conv(inputs,'d_wconv2','d_bconv2',64+y_dim)        \n",
    "        net = bn(net, scope = 'd_bn2',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        net = tf.reshape(net,[batch_size,-1])\n",
    "        \n",
    "        '''3th:Concat -> linear -> bn -> lrelu'''\n",
    "        net = tf.concat([net,y],axis=1)\n",
    "        net = linear(net,\"d_wlinear3\",\"d_blinear3\",1024)        \n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''4th: Concat -> linear'''\n",
    "        net = tf.concat([net,y],axis=1)\n",
    "        out_logit = linear(net,\"d_wlinear4\",\"d_blinear4\",1)  \n",
    "        '''5th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "\n",
    "        return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"generator\" function, the \"generator_y\" only has more one \"y\" parameter than \"generator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator( z, batch_size, output_height=28, output_width=28,output_depth=1, is_training=True, reuse=False):\n",
    "    \n",
    "    height0,width0 = output_height,output_width\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2)\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2)\n",
    "    height8,width8 = math.ceil(float(height4)/2),math.ceil(float(width4)/2)\n",
    "    height16,width16 = math.ceil(float(height8)/2),math.ceil(float(width8)/2)\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: linear -> reshape -> bn -> relu '''\n",
    "        net = linear(z,\"g_wlinear1\",\"g_blinear1\",64*8*height16*width16)\n",
    "        net = tf.reshape(net,[batch_size,height16,width16,64*8])\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        '''2nd: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height8, width8, 64*4]\n",
    "        net = deconv(net,'g_wdeconv2','g_bdeconv2',output_shape)\n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''3th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height4, width4, 64*2]\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape)\n",
    "        net = bn(net,scope='g_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''4th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height2, width2, 64*1]\n",
    "        net = deconv(net,'g_wdeconv4','g_bdeconv4',output_shape)\n",
    "        net = bn(net,scope='g_bn4',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''5th: deconv -> tanh '''\n",
    "        output_shape = [batch_size, height0, width0, output_depth]\n",
    "        net = deconv(net,'g_wdeconv5','g_bdeconv5',output_shape)\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_y( z, y, y_dim, batch_size, output_height, output_width,output_depth, is_training=True, reuse=False):\n",
    "    \n",
    "    height0,width0 = output_height,output_width\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2)\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2)\n",
    "    \n",
    "    with tf.variable_scope(\"generator_y\", reuse=reuse):\n",
    "        \n",
    "        yb = tf.reshape(y,[batch_size,1,1,y_dim])\n",
    "        '''1st:Concat -> linear -> relu '''\n",
    "        z = tf.concat([z,y],axis=1)\n",
    "        net = linear(z,\"g_wlinear1\",\"g_blinear1\",1024)\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        '''2nd: Concat -> linear -> bn -> relu '''\n",
    "        net = tf.concat([net,y],axis=1)       \n",
    "        net = linear(net,\"g_wlinear2\",\"g_blinear2\",1024*2*height4*width4)\n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''3th: reshape -> concat -> deconv -> bn -> relu '''\n",
    "        net = tf.reshape(net,[batch_size,height4,width4,1024*2])\n",
    "        net = tf.concat([net, yb*tf.ones(get_shape(net)[:-1]+[get_shape(yb)[-1]])],axis=3)\n",
    "        output_shape = [batch_size, height2, width2, 1024*2]\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape)\n",
    "        net = bn(net,scope='g_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''4th: concat -> deconv -> tanh '''\n",
    "        net = tf.concat([net, yb*tf.ones(get_shape(net)[:-1]+[get_shape(yb)[-1]])],axis=3)\n",
    "        output_shape = [batch_size, height0, width0, output_depth]\n",
    "        net = deconv(net,'g_wdeconv4','g_bdeconv4',output_shape)\n",
    "        out = tf.nn.tanh(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "y_dim = 10\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "y_flag = False\n",
    "output_height,output_width,output_depth = [28,28,1]\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "#labels\n",
    "y = tf.placeholder(tf.float32, [batch_size,y_dim], name='y')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "if not y_flag:\n",
    "    # output of D for real images\n",
    "    D_real, D_real_logits = discriminator(inputs, batch_size, is_training=True, reuse=False)\n",
    "    # output of D for fake images\n",
    "    G = generator(z, batch_size, is_training=True, reuse=False)\n",
    "    D_fake, D_fake_logits = discriminator(G, batch_size, is_training=True, reuse=True)\n",
    "else:  \n",
    "    D_real, D_real_logits = discriminator_y(inputs,y,y_dim, batch_size, is_training=True, reuse=False)\n",
    "    G = generator_y(z, y, y_dim, batch_size, output_height, output_width,output_depth, is_training=True, reuse=False)\n",
    "    D_fake, D_fake_logits = discriminator_y(G,y,y_dim, batch_size, is_training=True, reuse=True)\n",
    "    \n",
    "# get loss for discriminator\n",
    "d_loss_real = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# get loss for generator\n",
    "g_loss = tf.reduce_mean(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate*5, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "if not y_flag:\n",
    "    fake_images = generator(z,batch_size, is_training=False, reuse=True)\n",
    "else:\n",
    "    fake_images = generator_y(z,y,y_dim,batch_size, output_height, output_width,output_depth, is_training=False, reuse=True)\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim))\n",
    "test_labels_onehot = np.zeros([batch_size, y_dim],dtype = np.float32)\n",
    "test_labels_onehot[np.arange(batch_size), np.arange(batch_size)%int(np.sqrt(batch_size))] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 1.32991338, g_loss: 1.33764780\n",
      "Step: [1] d_loss: 1.59148777, g_loss: 3.77227211\n",
      "Step: [2] d_loss: 0.34677559, g_loss: 2.70106816\n",
      "Step: [3] d_loss: 0.38980940, g_loss: 3.71385574\n",
      "Step: [4] d_loss: 0.25435412, g_loss: 2.51633716\n",
      "Step: [5] d_loss: 1.20502901, g_loss: 5.37426376\n",
      "Step: [6] d_loss: 0.90248138, g_loss: 0.17683277\n",
      "Step: [7] d_loss: 2.89483094, g_loss: 3.77885008\n",
      "Step: [8] d_loss: 0.78907514, g_loss: 0.57541418\n",
      "Step: [9] d_loss: 2.11405802, g_loss: 4.19201374\n",
      "Step: [10] d_loss: 1.69127345, g_loss: 0.11773265\n",
      "Step: [11] d_loss: 2.93908429, g_loss: 2.80983162\n",
      "Step: [12] d_loss: 0.95322043, g_loss: 0.52748996\n",
      "Step: [13] d_loss: 2.24267411, g_loss: 1.73296511\n",
      "Step: [14] d_loss: 2.10455608, g_loss: 1.40009880\n",
      "Step: [15] d_loss: 3.30094099, g_loss: 3.36932898\n",
      "Step: [16] d_loss: 1.42032027, g_loss: 0.26487523\n",
      "Step: [17] d_loss: 2.68262506, g_loss: 0.42688176\n",
      "Step: [18] d_loss: 2.73643970, g_loss: 1.08897829\n",
      "Step: [19] d_loss: 2.16417956, g_loss: 1.90861154\n",
      "Step: [20] d_loss: 1.92157960, g_loss: 0.69358432\n",
      "Step: [21] d_loss: 1.74033618, g_loss: 2.53774905\n",
      "Step: [22] d_loss: 0.88881373, g_loss: 0.49486148\n",
      "Step: [23] d_loss: 1.77584255, g_loss: 1.55430603\n",
      "Step: [24] d_loss: 1.38153136, g_loss: 1.16838050\n",
      "Step: [25] d_loss: 1.55783439, g_loss: 1.39460397\n",
      "Step: [26] d_loss: 1.67511582, g_loss: 0.65490282\n",
      "Step: [27] d_loss: 2.01729321, g_loss: 2.90728474\n",
      "Step: [28] d_loss: 1.09772038, g_loss: 1.17515099\n",
      "Step: [29] d_loss: 1.25190282, g_loss: 0.62210459\n",
      "Step: [30] d_loss: 1.86076951, g_loss: 0.99494135\n",
      "Step: [31] d_loss: 1.54517841, g_loss: 1.39298129\n",
      "Step: [32] d_loss: 1.53747618, g_loss: 1.01192284\n",
      "Step: [33] d_loss: 1.75418973, g_loss: 2.77870703\n",
      "Step: [34] d_loss: 0.87280786, g_loss: 0.93349528\n",
      "Step: [35] d_loss: 1.44832253, g_loss: 1.17052257\n",
      "Step: [36] d_loss: 1.52549744, g_loss: 0.96661174\n",
      "Step: [37] d_loss: 1.67197323, g_loss: 1.74858379\n",
      "Step: [38] d_loss: 1.33828330, g_loss: 0.93499821\n",
      "Step: [39] d_loss: 1.43057203, g_loss: 1.13471353\n",
      "Step: [40] d_loss: 1.31310534, g_loss: 2.07873344\n",
      "Step: [41] d_loss: 1.03752863, g_loss: 1.65714014\n",
      "Step: [42] d_loss: 1.03950369, g_loss: 1.02734804\n",
      "Step: [43] d_loss: 1.39610100, g_loss: 1.17361379\n",
      "Step: [44] d_loss: 1.33068979, g_loss: 0.97747982\n",
      "Step: [45] d_loss: 1.82155132, g_loss: 0.98211724\n",
      "Step: [46] d_loss: 1.59575605, g_loss: 0.74795943\n",
      "Step: [47] d_loss: 1.64666617, g_loss: 0.77551830\n",
      "Step: [48] d_loss: 1.36295700, g_loss: 0.94922549\n",
      "Step: [49] d_loss: 1.16992068, g_loss: 0.78742605\n",
      "Step: [50] d_loss: 1.30771041, g_loss: 0.85102117\n",
      "Step: [51] d_loss: 1.52930832, g_loss: 0.74425387\n",
      "Step: [52] d_loss: 1.83613992, g_loss: 0.66074926\n",
      "Step: [53] d_loss: 1.57966948, g_loss: 1.21289730\n",
      "Step: [54] d_loss: 1.47669125, g_loss: 0.97211617\n",
      "Step: [55] d_loss: 1.20294714, g_loss: 1.05462790\n",
      "Step: [56] d_loss: 1.24382031, g_loss: 0.70558751\n",
      "Step: [57] d_loss: 1.57441211, g_loss: 0.84399605\n",
      "Step: [58] d_loss: 2.03085327, g_loss: 1.41680169\n",
      "Step: [59] d_loss: 1.72131121, g_loss: 1.32033873\n",
      "Step: [60] d_loss: 1.35030425, g_loss: 1.14281285\n",
      "Step: [61] d_loss: 1.07327390, g_loss: 1.27606905\n",
      "Step: [62] d_loss: 1.06187224, g_loss: 0.93198729\n",
      "Step: [63] d_loss: 1.27917504, g_loss: 1.52573228\n",
      "Step: [64] d_loss: 1.05363441, g_loss: 0.90546846\n",
      "Step: [65] d_loss: 1.36962843, g_loss: 1.23254919\n",
      "Step: [66] d_loss: 1.39140868, g_loss: 0.98592150\n",
      "Step: [67] d_loss: 1.58044672, g_loss: 0.86380303\n",
      "Step: [68] d_loss: 1.49068046, g_loss: 1.39393282\n",
      "Step: [69] d_loss: 1.29426527, g_loss: 0.98164535\n",
      "Step: [70] d_loss: 1.13040352, g_loss: 1.26989484\n",
      "Step: [71] d_loss: 0.98858500, g_loss: 1.20093226\n",
      "Step: [72] d_loss: 0.90346289, g_loss: 1.04169965\n",
      "Step: [73] d_loss: 1.35241675, g_loss: 0.98441410\n",
      "Step: [74] d_loss: 1.63333368, g_loss: 0.52910620\n",
      "Step: [75] d_loss: 2.58551383, g_loss: 1.33033812\n",
      "Step: [76] d_loss: 1.94700968, g_loss: 1.48666346\n",
      "Step: [77] d_loss: 1.47695875, g_loss: 0.79454923\n",
      "Step: [78] d_loss: 1.26220715, g_loss: 1.14619970\n",
      "Step: [79] d_loss: 0.92331791, g_loss: 1.17646790\n",
      "Step: [80] d_loss: 0.87623745, g_loss: 0.94657111\n",
      "Step: [81] d_loss: 1.00684416, g_loss: 0.86255062\n",
      "Step: [82] d_loss: 1.15038931, g_loss: 1.03327477\n",
      "Step: [83] d_loss: 1.26393771, g_loss: 0.64896441\n",
      "Step: [84] d_loss: 1.82202768, g_loss: 0.84542668\n",
      "Step: [85] d_loss: 1.73529077, g_loss: 0.89185858\n",
      "Step: [86] d_loss: 1.84982061, g_loss: 1.07211804\n",
      "Step: [87] d_loss: 1.66280890, g_loss: 0.96070981\n",
      "Step: [88] d_loss: 1.51492012, g_loss: 1.14338183\n",
      "Step: [89] d_loss: 1.16552544, g_loss: 1.05195272\n",
      "Step: [90] d_loss: 1.15801084, g_loss: 0.91742808\n",
      "Step: [91] d_loss: 1.47583091, g_loss: 1.71558428\n",
      "Step: [92] d_loss: 1.21142769, g_loss: 1.07057381\n",
      "Step: [93] d_loss: 1.48608613, g_loss: 0.78158325\n",
      "Step: [94] d_loss: 1.56571317, g_loss: 1.10369849\n",
      "Step: [95] d_loss: 1.55406642, g_loss: 1.13774300\n",
      "Step: [96] d_loss: 1.32828009, g_loss: 1.05689132\n",
      "Step: [97] d_loss: 1.27459037, g_loss: 0.97894406\n",
      "Step: [98] d_loss: 1.13591194, g_loss: 1.08373928\n",
      "Step: [99] d_loss: 1.11997390, g_loss: 0.77543736\n",
      "Step: [100] d_loss: 1.64608550, g_loss: 0.67380166\n",
      "Step: [101] d_loss: 1.95674372, g_loss: 0.87883025\n",
      "Step: [102] d_loss: 1.87942028, g_loss: 1.14254451\n",
      "Step: [103] d_loss: 1.43850851, g_loss: 0.89252257\n",
      "Step: [104] d_loss: 1.31210995, g_loss: 0.72520149\n",
      "Step: [105] d_loss: 1.38087642, g_loss: 0.74959248\n",
      "Step: [106] d_loss: 1.44770253, g_loss: 0.87524593\n",
      "Step: [107] d_loss: 1.28144252, g_loss: 1.02836907\n",
      "Step: [108] d_loss: 1.25160396, g_loss: 0.96075255\n",
      "Step: [109] d_loss: 1.26264489, g_loss: 0.90644127\n",
      "Step: [110] d_loss: 1.25317001, g_loss: 0.83047992\n",
      "Step: [111] d_loss: 1.30997372, g_loss: 0.84216976\n",
      "Step: [112] d_loss: 1.44736159, g_loss: 0.91763705\n",
      "Step: [113] d_loss: 1.32969475, g_loss: 0.99849093\n",
      "Step: [114] d_loss: 1.16692913, g_loss: 0.83100581\n",
      "Step: [115] d_loss: 1.17898440, g_loss: 0.76660085\n",
      "Step: [116] d_loss: 1.37043047, g_loss: 0.92791605\n",
      "Step: [117] d_loss: 1.40574801, g_loss: 0.82696068\n",
      "Step: [118] d_loss: 1.52850199, g_loss: 0.78996658\n",
      "Step: [119] d_loss: 1.53664303, g_loss: 0.84722376\n",
      "Step: [120] d_loss: 1.64262414, g_loss: 1.12375247\n",
      "Step: [121] d_loss: 1.25239038, g_loss: 1.30207467\n",
      "Step: [122] d_loss: 0.92162943, g_loss: 1.04234576\n",
      "Step: [123] d_loss: 0.99792039, g_loss: 1.09591103\n",
      "Step: [124] d_loss: 1.05458868, g_loss: 0.92996061\n",
      "Step: [125] d_loss: 1.50660634, g_loss: 1.25572264\n",
      "Step: [126] d_loss: 1.34779596, g_loss: 1.07273316\n",
      "Step: [127] d_loss: 1.38082302, g_loss: 0.52474314\n",
      "Step: [128] d_loss: 1.73098433, g_loss: 0.83215654\n",
      "Step: [129] d_loss: 1.73001182, g_loss: 1.05350566\n",
      "Step: [130] d_loss: 1.58553934, g_loss: 1.09952378\n",
      "Step: [131] d_loss: 1.36459970, g_loss: 0.80858785\n",
      "Step: [132] d_loss: 1.22033167, g_loss: 0.87906122\n",
      "Step: [133] d_loss: 1.06652617, g_loss: 1.17163777\n",
      "Step: [134] d_loss: 1.10710263, g_loss: 1.07018232\n",
      "Step: [135] d_loss: 1.12886548, g_loss: 0.88425136\n",
      "Step: [136] d_loss: 1.17475164, g_loss: 1.53920424\n",
      "Step: [137] d_loss: 0.92879689, g_loss: 0.95828080\n",
      "Step: [138] d_loss: 1.20673466, g_loss: 0.88928688\n",
      "Step: [139] d_loss: 1.27889311, g_loss: 1.29537225\n",
      "Step: [140] d_loss: 1.22597909, g_loss: 1.54492712\n",
      "Step: [141] d_loss: 1.05381262, g_loss: 0.96718419\n",
      "Step: [142] d_loss: 1.25762153, g_loss: 1.04573345\n",
      "Step: [143] d_loss: 1.07381415, g_loss: 1.34152603\n",
      "Step: [144] d_loss: 1.06672573, g_loss: 0.87261993\n",
      "Step: [145] d_loss: 1.14470351, g_loss: 0.83890629\n",
      "Step: [146] d_loss: 1.34993720, g_loss: 1.08492303\n",
      "Step: [147] d_loss: 1.37872577, g_loss: 1.23533511\n",
      "Step: [148] d_loss: 1.12816584, g_loss: 0.98626155\n",
      "Step: [149] d_loss: 1.33955944, g_loss: 1.04899931\n",
      "Step: [150] d_loss: 1.32269645, g_loss: 1.01643801\n",
      "Step: [151] d_loss: 1.46829915, g_loss: 0.85877877\n",
      "Step: [152] d_loss: 1.40905452, g_loss: 0.92035770\n",
      "Step: [153] d_loss: 1.30473471, g_loss: 0.97890747\n",
      "Step: [154] d_loss: 1.30296338, g_loss: 1.33940148\n",
      "Step: [155] d_loss: 1.17437267, g_loss: 0.99330270\n",
      "Step: [156] d_loss: 1.46089411, g_loss: 1.16552520\n",
      "Step: [157] d_loss: 1.25507438, g_loss: 0.81702846\n",
      "Step: [158] d_loss: 1.55365324, g_loss: 1.15665460\n",
      "Step: [159] d_loss: 1.25996518, g_loss: 0.97209239\n",
      "Step: [160] d_loss: 1.26501071, g_loss: 0.87507224\n",
      "Step: [161] d_loss: 1.23746407, g_loss: 1.21447742\n",
      "Step: [162] d_loss: 1.07237840, g_loss: 1.15389466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [163] d_loss: 0.99557734, g_loss: 1.03802252\n",
      "Step: [164] d_loss: 0.97304714, g_loss: 0.98849827\n",
      "Step: [165] d_loss: 1.10065794, g_loss: 0.95989025\n",
      "Step: [166] d_loss: 1.18099451, g_loss: 1.24818587\n",
      "Step: [167] d_loss: 1.08818817, g_loss: 0.90425825\n",
      "Step: [168] d_loss: 1.23924732, g_loss: 1.74791121\n",
      "Step: [169] d_loss: 0.96849906, g_loss: 0.88987440\n",
      "Step: [170] d_loss: 1.15545988, g_loss: 1.19864988\n",
      "Step: [171] d_loss: 1.18008268, g_loss: 1.11385059\n",
      "Step: [172] d_loss: 1.39244163, g_loss: 0.97731543\n",
      "Step: [173] d_loss: 1.51166916, g_loss: 0.98329759\n",
      "Step: [174] d_loss: 1.51537025, g_loss: 1.41432703\n",
      "Step: [175] d_loss: 1.09888065, g_loss: 0.92028904\n",
      "Step: [176] d_loss: 1.39937341, g_loss: 1.21536875\n",
      "Step: [177] d_loss: 1.23101699, g_loss: 1.09090912\n",
      "Step: [178] d_loss: 1.09435165, g_loss: 1.03279853\n",
      "Step: [179] d_loss: 0.98687041, g_loss: 1.13234925\n",
      "Step: [180] d_loss: 0.93645352, g_loss: 1.13175166\n",
      "Step: [181] d_loss: 0.83711672, g_loss: 1.07061982\n",
      "Step: [182] d_loss: 1.27325416, g_loss: 0.98621047\n",
      "Step: [183] d_loss: 1.35970795, g_loss: 1.05545306\n",
      "Step: [184] d_loss: 1.38798976, g_loss: 1.19958246\n",
      "Step: [185] d_loss: 1.07316041, g_loss: 0.80924845\n",
      "Step: [186] d_loss: 1.15233803, g_loss: 1.24700403\n",
      "Step: [187] d_loss: 1.15982366, g_loss: 0.91989875\n",
      "Step: [188] d_loss: 1.42609704, g_loss: 0.62177253\n",
      "Step: [189] d_loss: 2.26765490, g_loss: 1.33637631\n",
      "Step: [190] d_loss: 1.98097539, g_loss: 1.14004827\n",
      "Step: [191] d_loss: 1.42185593, g_loss: 0.85004520\n",
      "Step: [192] d_loss: 1.18016803, g_loss: 0.97753733\n",
      "Step: [193] d_loss: 1.05343759, g_loss: 1.25597310\n",
      "Step: [194] d_loss: 1.06522083, g_loss: 0.76369816\n",
      "Step: [195] d_loss: 1.52350521, g_loss: 1.17517006\n",
      "Step: [196] d_loss: 1.56497586, g_loss: 0.96396661\n",
      "Step: [197] d_loss: 1.63015258, g_loss: 0.90761626\n",
      "Step: [198] d_loss: 1.33835006, g_loss: 1.20500469\n",
      "Step: [199] d_loss: 1.10502565, g_loss: 1.25511110\n",
      "Step: [200] d_loss: 0.89442545, g_loss: 0.96959937\n",
      "Step: [201] d_loss: 1.16675520, g_loss: 0.73365295\n",
      "Step: [202] d_loss: 1.40117860, g_loss: 1.26205373\n",
      "Step: [203] d_loss: 1.11944032, g_loss: 1.26311111\n",
      "Step: [204] d_loss: 1.12340999, g_loss: 1.20017242\n",
      "Step: [205] d_loss: 0.89962095, g_loss: 0.93947661\n",
      "Step: [206] d_loss: 0.99807644, g_loss: 0.83864325\n",
      "Step: [207] d_loss: 1.30416083, g_loss: 0.68591845\n",
      "Step: [208] d_loss: 1.59884584, g_loss: 1.44048619\n",
      "Step: [209] d_loss: 1.56892347, g_loss: 1.33932304\n",
      "Step: [210] d_loss: 1.06744123, g_loss: 0.93045914\n",
      "Step: [211] d_loss: 0.99489224, g_loss: 0.95469010\n",
      "Step: [212] d_loss: 1.08315825, g_loss: 1.03244042\n",
      "Step: [213] d_loss: 1.07280970, g_loss: 1.29645729\n",
      "Step: [214] d_loss: 1.10004652, g_loss: 0.88366824\n",
      "Step: [215] d_loss: 1.43526292, g_loss: 0.85949028\n",
      "Step: [216] d_loss: 1.26823866, g_loss: 1.13855159\n",
      "Step: [217] d_loss: 1.23223829, g_loss: 0.87125027\n",
      "Step: [218] d_loss: 1.17180228, g_loss: 0.83319026\n",
      "Step: [219] d_loss: 1.29900992, g_loss: 1.06260920\n",
      "Step: [220] d_loss: 1.24245203, g_loss: 0.94537628\n",
      "Step: [221] d_loss: 1.16559482, g_loss: 0.81762671\n",
      "Step: [222] d_loss: 1.21453631, g_loss: 0.74557626\n",
      "Step: [223] d_loss: 1.38304234, g_loss: 0.62943882\n",
      "Step: [224] d_loss: 1.45355749, g_loss: 0.68767083\n",
      "Step: [225] d_loss: 1.51467633, g_loss: 0.86617756\n",
      "Step: [226] d_loss: 1.38721371, g_loss: 1.00536489\n",
      "Step: [227] d_loss: 1.26672459, g_loss: 0.71521914\n",
      "Step: [228] d_loss: 1.32085729, g_loss: 0.57939517\n",
      "Step: [229] d_loss: 1.67096877, g_loss: 0.96215999\n",
      "Step: [230] d_loss: 1.36544514, g_loss: 1.08652067\n",
      "Step: [231] d_loss: 1.24207723, g_loss: 0.93309760\n",
      "Step: [232] d_loss: 1.19032323, g_loss: 0.80637252\n",
      "Step: [233] d_loss: 1.20536852, g_loss: 0.72023076\n",
      "Step: [234] d_loss: 1.41650605, g_loss: 0.76924872\n",
      "Step: [235] d_loss: 1.73406267, g_loss: 1.31826258\n",
      "Step: [236] d_loss: 1.62420785, g_loss: 0.99651504\n",
      "Step: [237] d_loss: 1.45781016, g_loss: 0.97438264\n",
      "Step: [238] d_loss: 1.29734421, g_loss: 0.96294284\n",
      "Step: [239] d_loss: 1.35057449, g_loss: 0.71917617\n",
      "Step: [240] d_loss: 1.41303253, g_loss: 0.81953943\n",
      "Step: [241] d_loss: 1.49764132, g_loss: 0.87298381\n",
      "Step: [242] d_loss: 1.74520218, g_loss: 0.87991011\n",
      "Step: [243] d_loss: 1.60227454, g_loss: 0.90383977\n",
      "Step: [244] d_loss: 1.28719449, g_loss: 0.95189685\n",
      "Step: [245] d_loss: 1.06200838, g_loss: 0.91770935\n",
      "Step: [246] d_loss: 1.11994338, g_loss: 0.75332254\n",
      "Step: [247] d_loss: 1.25996327, g_loss: 0.70962387\n",
      "Step: [248] d_loss: 1.57067108, g_loss: 0.83150566\n",
      "Step: [249] d_loss: 1.38742685, g_loss: 0.98210919\n",
      "Step: [250] d_loss: 1.41126513, g_loss: 0.91151059\n",
      "Step: [251] d_loss: 1.16400898, g_loss: 0.83795434\n",
      "Step: [252] d_loss: 1.15141582, g_loss: 0.89303887\n",
      "Step: [253] d_loss: 1.15966427, g_loss: 0.70476615\n",
      "Step: [254] d_loss: 1.45968151, g_loss: 0.66109574\n",
      "Step: [255] d_loss: 1.49690807, g_loss: 0.70112348\n",
      "Step: [256] d_loss: 1.62868333, g_loss: 0.95531541\n",
      "Step: [257] d_loss: 1.60923839, g_loss: 0.74563956\n",
      "Step: [258] d_loss: 1.52202940, g_loss: 0.68133962\n",
      "Step: [259] d_loss: 1.48202503, g_loss: 0.89237761\n",
      "Step: [260] d_loss: 1.37021685, g_loss: 0.76775628\n",
      "Step: [261] d_loss: 1.47380304, g_loss: 0.83735543\n",
      "Step: [262] d_loss: 1.29751360, g_loss: 0.77058089\n",
      "Step: [263] d_loss: 1.26455832, g_loss: 0.79366326\n",
      "Step: [264] d_loss: 1.24933112, g_loss: 0.77050257\n",
      "Step: [265] d_loss: 1.31226039, g_loss: 0.85503137\n",
      "Step: [266] d_loss: 1.49306440, g_loss: 0.75237662\n",
      "Step: [267] d_loss: 1.47435355, g_loss: 0.76275522\n",
      "Step: [268] d_loss: 1.32218933, g_loss: 0.87697941\n",
      "Step: [269] d_loss: 1.38219070, g_loss: 0.88293266\n",
      "Step: [270] d_loss: 1.15902925, g_loss: 0.84527713\n",
      "Step: [271] d_loss: 1.38090611, g_loss: 0.61935490\n",
      "Step: [272] d_loss: 1.44951415, g_loss: 0.70686597\n",
      "Step: [273] d_loss: 1.61025834, g_loss: 0.81013215\n",
      "Step: [274] d_loss: 1.58878124, g_loss: 0.73310137\n",
      "Step: [275] d_loss: 1.45557547, g_loss: 0.71035677\n",
      "Step: [276] d_loss: 1.53669643, g_loss: 0.74748647\n",
      "Step: [277] d_loss: 1.48264718, g_loss: 0.81136405\n",
      "Step: [278] d_loss: 1.42637062, g_loss: 0.89470661\n",
      "Step: [279] d_loss: 1.32389927, g_loss: 0.85229015\n",
      "Step: [280] d_loss: 1.22954011, g_loss: 0.75736368\n",
      "Step: [281] d_loss: 1.37614179, g_loss: 0.74721646\n",
      "Step: [282] d_loss: 1.29051590, g_loss: 0.88075060\n",
      "Step: [283] d_loss: 1.20582497, g_loss: 0.81743145\n",
      "Step: [284] d_loss: 1.24742198, g_loss: 0.73666382\n",
      "Step: [285] d_loss: 1.41168284, g_loss: 0.84198076\n",
      "Step: [286] d_loss: 1.35999632, g_loss: 0.78890908\n",
      "Step: [287] d_loss: 1.47976148, g_loss: 0.79116160\n",
      "Step: [288] d_loss: 1.41359353, g_loss: 0.83351815\n",
      "Step: [289] d_loss: 1.44419038, g_loss: 0.71925998\n",
      "Step: [290] d_loss: 1.39741349, g_loss: 0.75322044\n",
      "Step: [291] d_loss: 1.45372748, g_loss: 0.68506944\n",
      "Step: [292] d_loss: 1.45927119, g_loss: 0.76452208\n",
      "Step: [293] d_loss: 1.40622115, g_loss: 0.73061538\n",
      "Step: [294] d_loss: 1.40630341, g_loss: 0.75575888\n",
      "Step: [295] d_loss: 1.34023166, g_loss: 0.78835630\n",
      "Step: [296] d_loss: 1.29043067, g_loss: 0.81385505\n",
      "Step: [297] d_loss: 1.29951358, g_loss: 0.78206575\n",
      "Step: [298] d_loss: 1.22769403, g_loss: 0.82342887\n",
      "Step: [299] d_loss: 1.32162106, g_loss: 0.77577239\n",
      "Step: [300] d_loss: 1.32786238, g_loss: 0.80875671\n",
      "Step: [301] d_loss: 1.35159039, g_loss: 0.80531192\n",
      "Step: [302] d_loss: 1.34789205, g_loss: 0.84664762\n",
      "Step: [303] d_loss: 1.27246404, g_loss: 0.76211411\n",
      "Step: [304] d_loss: 1.36350393, g_loss: 0.85181367\n",
      "Step: [305] d_loss: 1.23912108, g_loss: 0.72143942\n",
      "Step: [306] d_loss: 1.44272923, g_loss: 0.72461796\n",
      "Step: [307] d_loss: 1.44599736, g_loss: 0.71094590\n",
      "Step: [308] d_loss: 1.72499001, g_loss: 0.73286438\n",
      "Step: [309] d_loss: 1.57704401, g_loss: 0.77776074\n",
      "Step: [310] d_loss: 1.46916699, g_loss: 0.80254328\n",
      "Step: [311] d_loss: 1.35744488, g_loss: 0.76055622\n",
      "Step: [312] d_loss: 1.34881055, g_loss: 0.67671567\n",
      "Step: [313] d_loss: 1.53775263, g_loss: 0.72105587\n",
      "Step: [314] d_loss: 1.43911457, g_loss: 0.84196407\n",
      "Step: [315] d_loss: 1.42288113, g_loss: 0.81042242\n",
      "Step: [316] d_loss: 1.26248384, g_loss: 0.74943310\n",
      "Step: [317] d_loss: 1.23545420, g_loss: 0.72250682\n",
      "Step: [318] d_loss: 1.29945898, g_loss: 0.73896855\n",
      "Step: [319] d_loss: 1.32301307, g_loss: 0.76599330\n",
      "Step: [320] d_loss: 1.15938008, g_loss: 0.85178542\n",
      "Step: [321] d_loss: 1.22231197, g_loss: 0.81532860\n",
      "Step: [322] d_loss: 1.22756159, g_loss: 0.83147073\n",
      "Step: [323] d_loss: 1.29294312, g_loss: 0.86419117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [324] d_loss: 1.18382406, g_loss: 0.86003542\n",
      "Step: [325] d_loss: 1.25065899, g_loss: 0.75421119\n",
      "Step: [326] d_loss: 1.56037056, g_loss: 0.66353083\n",
      "Step: [327] d_loss: 1.30410194, g_loss: 0.75888824\n",
      "Step: [328] d_loss: 1.32621336, g_loss: 0.64462996\n",
      "Step: [329] d_loss: 1.60927272, g_loss: 0.60262644\n",
      "Step: [330] d_loss: 1.65050650, g_loss: 0.76254058\n",
      "Step: [331] d_loss: 1.38738084, g_loss: 0.87140197\n",
      "Step: [332] d_loss: 1.35635638, g_loss: 0.80082595\n",
      "Step: [333] d_loss: 1.30762959, g_loss: 0.74549979\n",
      "Step: [334] d_loss: 1.26398706, g_loss: 0.78132021\n",
      "Step: [335] d_loss: 1.41272974, g_loss: 0.76076770\n",
      "Step: [336] d_loss: 1.29337597, g_loss: 0.76921171\n",
      "Step: [337] d_loss: 1.36765146, g_loss: 0.73080438\n",
      "Step: [338] d_loss: 1.35786033, g_loss: 0.73576140\n",
      "Step: [339] d_loss: 1.33533216, g_loss: 0.72925848\n",
      "Step: [340] d_loss: 1.32906353, g_loss: 0.68336517\n",
      "Step: [341] d_loss: 1.33934855, g_loss: 0.73104161\n",
      "Step: [342] d_loss: 1.33172846, g_loss: 0.73150134\n",
      "Step: [343] d_loss: 1.45468831, g_loss: 0.64612627\n",
      "Step: [344] d_loss: 1.48587739, g_loss: 0.66581821\n",
      "Step: [345] d_loss: 1.51474297, g_loss: 0.71944737\n",
      "Step: [346] d_loss: 1.39121592, g_loss: 0.69993907\n",
      "Step: [347] d_loss: 1.42677665, g_loss: 0.62821746\n",
      "Step: [348] d_loss: 1.51774955, g_loss: 0.67161012\n",
      "Step: [349] d_loss: 1.42600083, g_loss: 0.71151859\n",
      "Step: [350] d_loss: 1.41844308, g_loss: 0.66442263\n",
      "Step: [351] d_loss: 1.42474198, g_loss: 0.72738618\n",
      "Step: [352] d_loss: 1.41313410, g_loss: 0.67792451\n",
      "Step: [353] d_loss: 1.41068101, g_loss: 0.66757405\n",
      "Step: [354] d_loss: 1.49353814, g_loss: 0.70305967\n",
      "Step: [355] d_loss: 1.46947408, g_loss: 0.65788984\n",
      "Step: [356] d_loss: 1.53780150, g_loss: 0.63541126\n",
      "Step: [357] d_loss: 1.46813655, g_loss: 0.74178433\n",
      "Step: [358] d_loss: 1.43011212, g_loss: 0.67806172\n",
      "Step: [359] d_loss: 1.42480206, g_loss: 0.65716171\n",
      "Step: [360] d_loss: 1.41435623, g_loss: 0.60446298\n",
      "Step: [361] d_loss: 1.40849400, g_loss: 0.59919930\n",
      "Step: [362] d_loss: 1.42110646, g_loss: 0.64820689\n",
      "Step: [363] d_loss: 1.46719575, g_loss: 0.66235256\n",
      "Step: [364] d_loss: 1.45453358, g_loss: 0.67033875\n",
      "Step: [365] d_loss: 1.44395447, g_loss: 0.71274400\n",
      "Step: [366] d_loss: 1.40026832, g_loss: 0.71915925\n",
      "Step: [367] d_loss: 1.35861218, g_loss: 0.68747699\n",
      "Step: [368] d_loss: 1.36999118, g_loss: 0.68183064\n",
      "Step: [369] d_loss: 1.38347864, g_loss: 0.71355808\n",
      "Step: [370] d_loss: 1.38679552, g_loss: 0.71640098\n",
      "Step: [371] d_loss: 1.45071614, g_loss: 0.64911520\n",
      "Step: [372] d_loss: 1.52685690, g_loss: 0.64600724\n",
      "Step: [373] d_loss: 1.55346942, g_loss: 0.68941545\n",
      "Step: [374] d_loss: 1.57532072, g_loss: 0.65927958\n",
      "Step: [375] d_loss: 1.43840802, g_loss: 0.69178772\n",
      "Step: [376] d_loss: 1.42480421, g_loss: 0.72052670\n",
      "Step: [377] d_loss: 1.40100121, g_loss: 0.67818332\n",
      "Step: [378] d_loss: 1.35250437, g_loss: 0.70462352\n",
      "Step: [379] d_loss: 1.44629884, g_loss: 0.69449550\n",
      "Step: [380] d_loss: 1.42123842, g_loss: 0.69871759\n",
      "Step: [381] d_loss: 1.33948779, g_loss: 0.71791518\n",
      "Step: [382] d_loss: 1.34253764, g_loss: 0.73611307\n",
      "Step: [383] d_loss: 1.35227203, g_loss: 0.73377430\n",
      "Step: [384] d_loss: 1.29320526, g_loss: 0.72304487\n",
      "Step: [385] d_loss: 1.27512515, g_loss: 0.70754886\n",
      "Step: [386] d_loss: 1.35745716, g_loss: 0.71150541\n",
      "Step: [387] d_loss: 1.36680460, g_loss: 0.71420014\n",
      "Step: [388] d_loss: 1.33142018, g_loss: 0.70731807\n",
      "Step: [389] d_loss: 1.31820059, g_loss: 0.71055400\n",
      "Step: [390] d_loss: 1.45314741, g_loss: 0.63736022\n",
      "Step: [391] d_loss: 1.41873693, g_loss: 0.66124094\n",
      "Step: [392] d_loss: 1.56982303, g_loss: 0.64248741\n",
      "Step: [393] d_loss: 1.43205667, g_loss: 0.67130303\n",
      "Step: [394] d_loss: 1.58594751, g_loss: 0.64531028\n",
      "Step: [395] d_loss: 1.51917970, g_loss: 0.68096191\n",
      "Step: [396] d_loss: 1.38710105, g_loss: 0.72381747\n",
      "Step: [397] d_loss: 1.39685464, g_loss: 0.71465456\n",
      "Step: [398] d_loss: 1.44857502, g_loss: 0.68851757\n",
      "Step: [399] d_loss: 1.37141573, g_loss: 0.72971714\n",
      "Step: [400] d_loss: 1.34609151, g_loss: 0.69804025\n",
      "Step: [401] d_loss: 1.40926862, g_loss: 0.62358284\n",
      "Step: [402] d_loss: 1.47515786, g_loss: 0.64356065\n",
      "Step: [403] d_loss: 1.51442194, g_loss: 0.71716911\n",
      "Step: [404] d_loss: 1.45941114, g_loss: 0.74198413\n",
      "Step: [405] d_loss: 1.49206746, g_loss: 0.69043767\n",
      "Step: [406] d_loss: 1.36336696, g_loss: 0.67680323\n",
      "Step: [407] d_loss: 1.28963399, g_loss: 0.72653461\n",
      "Step: [408] d_loss: 1.33094025, g_loss: 0.73680836\n",
      "Step: [409] d_loss: 1.33184004, g_loss: 0.75177854\n",
      "Step: [410] d_loss: 1.32201064, g_loss: 0.75476801\n",
      "Step: [411] d_loss: 1.31816852, g_loss: 0.72810137\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "    if not y_flag:\n",
    "        # update D network\n",
    "        _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, z: batch_z})\n",
    "        # update G network\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "    else:\n",
    "        _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, y:batch_labels, z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z,y:batch_labels})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z,y:batch_labels})\n",
    "        \n",
    "        \n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "        if not y_flag:\n",
    "            samples = sess.run(fake_images, feed_dict={z: sample_z})\n",
    "        else:\n",
    "            samples = sess.run(fake_images, feed_dict={z: sample_z,y:test_labels_onehot})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :, :, :].squeeze()\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
