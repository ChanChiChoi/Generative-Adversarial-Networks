{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most codes from https://github.com/hwalsuklee/tensorflow-generative-model-collections/blob/master/GAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy readingï¼ŒI did not put the \"get_variable\",\"conv2d\" into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, batch_size, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: Conv -> lrelu'''\n",
    "        #conv\n",
    "        w1 = tf.get_variable('d_wconv1', [4, 4, 1, 64],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('d_bconv1', [64], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.nn.conv2d(inputs, w1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b1), net.get_shape())\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        '''2nd: Conv -> bn -> lrelu'''\n",
    "        #conv\n",
    "        w2 = tf.get_variable('d_wconv2', [4, 4, 64, 128],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable('d_bconv2', [128], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.nn.conv2d(net, w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope='d_bn2')\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b2), net.get_shape())\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        net = tf.reshape(net, [batch_size, -1])\n",
    "        \n",
    "        '''3th: linear -> bn -> lrelu'''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w3 = tf.get_variable(\"d_wlinear3\", [shape[1], 1024], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b3 = tf.get_variable(\"d_blinear3\", [1024], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(net, w3) + b3\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='d_bn3')\n",
    "        #lrelu\n",
    "        net = tf.maximum(net, 0.2*net)\n",
    "        \n",
    "        '''4th: linear '''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w4 = tf.get_variable(\"d_wlinear4\", [shape[1], 1], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b4 = tf.get_variable(\"d_blinear4\", [1], initializer=tf.constant_initializer(0.0))\n",
    "        out_logit = tf.matmul(net, w4) + b4\n",
    "        \n",
    "        '''5th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "        \n",
    "        return out, out_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator( z,batch_size, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: linear -> bn -> relu '''\n",
    "        #linear\n",
    "        shape = z.get_shape().as_list()        \n",
    "        w1 = tf.get_variable(\"g_wlinear1\", [shape[1], 1024], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable(\"g_blinear1\", [1024], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(z, w1) + b1\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn1')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net)  \n",
    "        \n",
    "        '''2nd: linear -> bn -> relu '''\n",
    "        #linear\n",
    "        shape = net.get_shape().as_list()        \n",
    "        w2 = tf.get_variable(\"g_wlinear2\", [shape[1], 128*7*7], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable(\"g_blinear2\", [128*7*7], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.matmul(net, w2) + b2\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn2')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net) \n",
    "                     \n",
    "        net = tf.reshape(net, [batch_size, 7, 7, 128])   \n",
    "        \n",
    "        '''3th: deconv -> bn -> relu '''\n",
    "        #deconv\n",
    "        output_shape = [batch_size, 14, 14, 64]\n",
    "        w3 = tf.get_variable('g_wdeconv3', [4, 4, output_shape[-1], net.get_shape()[-1]],\n",
    "                                           initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        net = tf.nn.conv2d_transpose(net, w3, output_shape=output_shape, strides=[1, 2, 2, 1])\n",
    "        b3 = tf.get_variable('g_bdeconv3', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b3), net.get_shape())\n",
    "        #bn\n",
    "        net = tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                        is_training=is_training, scope='g_bn3')\n",
    "        #relu\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''4th: deconv -> bn -> sigmoid '''\n",
    "        #deconv\n",
    "        output_shape = [batch_size, 28, 28, 1]\n",
    "        w4 = tf.get_variable('g_wdeconv4', [4, 4, output_shape[-1], net.get_shape()[-1]],\n",
    "                                           initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        net = tf.nn.conv2d_transpose(net, w4, output_shape=output_shape, strides=[1, 2, 2, 1])\n",
    "        b4 = tf.get_variable('g_bdeconv4', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        net = tf.reshape(tf.nn.bias_add(net, b4), net.get_shape())\n",
    "        #sigmoid\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 62\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "# output of D for real images\n",
    "D_real, D_real_logits = discriminator(inputs, batch_size, is_training=True, reuse=False)\n",
    "\n",
    "# output of D for fake images\n",
    "G = generator(z, batch_size, is_training=True, reuse=False)\n",
    "D_fake, D_fake_logits = discriminator(G, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "# get loss for discriminator\n",
    "d_loss_real = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# get loss for generator\n",
    "g_loss = tf.reduce_mean(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate*5, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "fake_images = generator(z,batch_size, is_training=False, reuse=True)\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 1.46458840, g_loss: 0.64777845\n",
      "Step: [1] d_loss: 1.46579719, g_loss: 0.65457553\n",
      "Step: [2] d_loss: 1.42761779, g_loss: 0.67831904\n",
      "Step: [3] d_loss: 1.42042303, g_loss: 0.69852638\n",
      "Step: [4] d_loss: 1.40911579, g_loss: 0.71736163\n",
      "Step: [5] d_loss: 1.39809585, g_loss: 0.73251450\n",
      "Step: [6] d_loss: 1.39723253, g_loss: 0.74750620\n",
      "Step: [7] d_loss: 1.38970423, g_loss: 0.75682068\n",
      "Step: [8] d_loss: 1.37036037, g_loss: 0.76273489\n",
      "Step: [9] d_loss: 1.38706350, g_loss: 0.75738072\n",
      "Step: [10] d_loss: 1.36794412, g_loss: 0.76199967\n",
      "Step: [11] d_loss: 1.37723041, g_loss: 0.76501459\n",
      "Step: [12] d_loss: 1.36540079, g_loss: 0.76265597\n",
      "Step: [13] d_loss: 1.34757054, g_loss: 0.77868283\n",
      "Step: [14] d_loss: 1.34565091, g_loss: 0.77489394\n",
      "Step: [15] d_loss: 1.36014152, g_loss: 0.77082920\n",
      "Step: [16] d_loss: 1.33849001, g_loss: 0.78033423\n",
      "Step: [17] d_loss: 1.34064722, g_loss: 0.78339583\n",
      "Step: [18] d_loss: 1.35505939, g_loss: 0.77385914\n",
      "Step: [19] d_loss: 1.34183013, g_loss: 0.78090453\n",
      "Step: [20] d_loss: 1.36079967, g_loss: 0.77756023\n",
      "Step: [21] d_loss: 1.32738400, g_loss: 0.78630900\n",
      "Step: [22] d_loss: 1.32388353, g_loss: 0.78774273\n",
      "Step: [23] d_loss: 1.33605552, g_loss: 0.78613853\n",
      "Step: [24] d_loss: 1.30403197, g_loss: 0.79311705\n",
      "Step: [25] d_loss: 1.30491686, g_loss: 0.80291462\n",
      "Step: [26] d_loss: 1.32655811, g_loss: 0.80185950\n",
      "Step: [27] d_loss: 1.31505990, g_loss: 0.79834116\n",
      "Step: [28] d_loss: 1.31840694, g_loss: 0.79240656\n",
      "Step: [29] d_loss: 1.30458558, g_loss: 0.79761648\n",
      "Step: [30] d_loss: 1.30511308, g_loss: 0.79951751\n",
      "Step: [31] d_loss: 1.29043531, g_loss: 0.80940640\n",
      "Step: [32] d_loss: 1.28810620, g_loss: 0.81368381\n",
      "Step: [33] d_loss: 1.28281367, g_loss: 0.81592262\n",
      "Step: [34] d_loss: 1.27998936, g_loss: 0.82067728\n",
      "Step: [35] d_loss: 1.27429557, g_loss: 0.82224965\n",
      "Step: [36] d_loss: 1.23835659, g_loss: 0.83060247\n",
      "Step: [37] d_loss: 1.24831557, g_loss: 0.83668840\n",
      "Step: [38] d_loss: 1.27059710, g_loss: 0.83893269\n",
      "Step: [39] d_loss: 1.27242148, g_loss: 0.83584762\n",
      "Step: [40] d_loss: 1.29314470, g_loss: 0.83295184\n",
      "Step: [41] d_loss: 1.25730848, g_loss: 0.83138573\n",
      "Step: [42] d_loss: 1.24921918, g_loss: 0.83431590\n",
      "Step: [43] d_loss: 1.24768186, g_loss: 0.84678119\n",
      "Step: [44] d_loss: 1.23072195, g_loss: 0.85113740\n",
      "Step: [45] d_loss: 1.22993660, g_loss: 0.85339570\n",
      "Step: [46] d_loss: 1.29587293, g_loss: 0.84924221\n",
      "Step: [47] d_loss: 1.24817848, g_loss: 0.83817947\n",
      "Step: [48] d_loss: 1.25056040, g_loss: 0.83468354\n",
      "Step: [49] d_loss: 1.19561267, g_loss: 0.85179126\n",
      "Step: [50] d_loss: 1.22370303, g_loss: 0.86176431\n",
      "Step: [51] d_loss: 1.22637868, g_loss: 0.86254787\n",
      "Step: [52] d_loss: 1.26271391, g_loss: 0.85960674\n",
      "Step: [53] d_loss: 1.19870543, g_loss: 0.87005359\n",
      "Step: [54] d_loss: 1.22388256, g_loss: 0.86446935\n",
      "Step: [55] d_loss: 1.20298886, g_loss: 0.87732363\n",
      "Step: [56] d_loss: 1.20231938, g_loss: 0.88498807\n",
      "Step: [57] d_loss: 1.21872103, g_loss: 0.88020003\n",
      "Step: [58] d_loss: 1.17232490, g_loss: 0.88668424\n",
      "Step: [59] d_loss: 1.22309494, g_loss: 0.88356841\n",
      "Step: [60] d_loss: 1.17544699, g_loss: 0.89051700\n",
      "Step: [61] d_loss: 1.21019375, g_loss: 0.88826489\n",
      "Step: [62] d_loss: 1.15748155, g_loss: 0.90193093\n",
      "Step: [63] d_loss: 1.15523493, g_loss: 0.90731823\n",
      "Step: [64] d_loss: 1.16981506, g_loss: 0.89614487\n",
      "Step: [65] d_loss: 1.23181057, g_loss: 0.90131450\n",
      "Step: [66] d_loss: 1.19252098, g_loss: 0.89127833\n",
      "Step: [67] d_loss: 1.24385333, g_loss: 0.86766320\n",
      "Step: [68] d_loss: 1.19875574, g_loss: 0.89457548\n",
      "Step: [69] d_loss: 1.16854930, g_loss: 0.88919199\n",
      "Step: [70] d_loss: 1.19399977, g_loss: 0.89292634\n",
      "Step: [71] d_loss: 1.16548538, g_loss: 0.87778318\n",
      "Step: [72] d_loss: 1.14783168, g_loss: 0.90463650\n",
      "Step: [73] d_loss: 1.15826869, g_loss: 0.91408157\n",
      "Step: [74] d_loss: 1.18215299, g_loss: 0.93296385\n",
      "Step: [75] d_loss: 1.19740629, g_loss: 0.96810842\n",
      "Step: [76] d_loss: 1.14138913, g_loss: 0.92569661\n",
      "Step: [77] d_loss: 1.16742134, g_loss: 0.91266847\n",
      "Step: [78] d_loss: 1.17561245, g_loss: 0.89820731\n",
      "Step: [79] d_loss: 1.18582499, g_loss: 0.90234149\n",
      "Step: [80] d_loss: 1.14618170, g_loss: 0.88793308\n",
      "Step: [81] d_loss: 1.19546533, g_loss: 0.89777690\n",
      "Step: [82] d_loss: 1.17299843, g_loss: 0.90574563\n",
      "Step: [83] d_loss: 1.13347292, g_loss: 0.91492164\n",
      "Step: [84] d_loss: 1.16064048, g_loss: 0.90433300\n",
      "Step: [85] d_loss: 1.12679493, g_loss: 0.92622364\n",
      "Step: [86] d_loss: 1.17227650, g_loss: 0.89582884\n",
      "Step: [87] d_loss: 1.14582157, g_loss: 0.92301631\n",
      "Step: [88] d_loss: 1.10904217, g_loss: 0.90720534\n",
      "Step: [89] d_loss: 1.16887951, g_loss: 0.92624784\n",
      "Step: [90] d_loss: 1.12063265, g_loss: 0.97048938\n",
      "Step: [91] d_loss: 1.11758995, g_loss: 0.94788057\n",
      "Step: [92] d_loss: 1.11468184, g_loss: 0.92521191\n",
      "Step: [93] d_loss: 1.19378924, g_loss: 0.90946794\n",
      "Step: [94] d_loss: 1.12650788, g_loss: 0.91841131\n",
      "Step: [95] d_loss: 1.08283150, g_loss: 0.92326391\n",
      "Step: [96] d_loss: 1.19839311, g_loss: 0.96546674\n",
      "Step: [97] d_loss: 1.16469610, g_loss: 0.92697531\n",
      "Step: [98] d_loss: 1.14885902, g_loss: 0.92023265\n",
      "Step: [99] d_loss: 1.11379993, g_loss: 0.95403123\n",
      "Step: [100] d_loss: 1.10944915, g_loss: 0.94271588\n",
      "Step: [101] d_loss: 1.16886783, g_loss: 0.95326990\n",
      "Step: [102] d_loss: 1.10696006, g_loss: 0.94110113\n",
      "Step: [103] d_loss: 1.12665701, g_loss: 0.92855436\n",
      "Step: [104] d_loss: 1.10964417, g_loss: 0.94063073\n",
      "Step: [105] d_loss: 1.09164774, g_loss: 0.92544132\n",
      "Step: [106] d_loss: 1.16239190, g_loss: 0.95414853\n",
      "Step: [107] d_loss: 1.10942531, g_loss: 0.92581773\n",
      "Step: [108] d_loss: 1.13944674, g_loss: 0.94289017\n",
      "Step: [109] d_loss: 1.16086984, g_loss: 0.95419955\n",
      "Step: [110] d_loss: 1.15398622, g_loss: 1.05521226\n",
      "Step: [111] d_loss: 1.21503901, g_loss: 0.98116791\n",
      "Step: [112] d_loss: 1.20111012, g_loss: 0.94885206\n",
      "Step: [113] d_loss: 1.19221008, g_loss: 0.99434477\n",
      "Step: [114] d_loss: 1.13622034, g_loss: 0.91506833\n",
      "Step: [115] d_loss: 1.12107098, g_loss: 0.91564870\n",
      "Step: [116] d_loss: 1.10233474, g_loss: 0.90733796\n",
      "Step: [117] d_loss: 1.08369732, g_loss: 0.93055475\n",
      "Step: [118] d_loss: 1.07488823, g_loss: 0.91198701\n",
      "Step: [119] d_loss: 1.13424873, g_loss: 0.94998854\n",
      "Step: [120] d_loss: 1.21742058, g_loss: 0.92205632\n",
      "Step: [121] d_loss: 1.17714310, g_loss: 0.97191405\n",
      "Step: [122] d_loss: 1.15264940, g_loss: 0.90397501\n",
      "Step: [123] d_loss: 1.17042208, g_loss: 0.90051776\n",
      "Step: [124] d_loss: 1.13728786, g_loss: 0.92859638\n",
      "Step: [125] d_loss: 1.13457370, g_loss: 0.97429806\n",
      "Step: [126] d_loss: 1.15354371, g_loss: 0.95111883\n",
      "Step: [127] d_loss: 1.19118595, g_loss: 0.93940377\n",
      "Step: [128] d_loss: 1.16323352, g_loss: 0.94176221\n",
      "Step: [129] d_loss: 1.21517026, g_loss: 0.93819416\n",
      "Step: [130] d_loss: 1.19342017, g_loss: 0.90521502\n",
      "Step: [131] d_loss: 1.12645853, g_loss: 0.87863535\n",
      "Step: [132] d_loss: 1.18581784, g_loss: 0.91828090\n",
      "Step: [133] d_loss: 1.17648208, g_loss: 0.94661349\n",
      "Step: [134] d_loss: 1.18464494, g_loss: 0.91267598\n",
      "Step: [135] d_loss: 1.17240000, g_loss: 0.91834223\n",
      "Step: [136] d_loss: 1.15160704, g_loss: 0.90686083\n",
      "Step: [137] d_loss: 1.12568855, g_loss: 0.90185308\n",
      "Step: [138] d_loss: 1.08317590, g_loss: 0.93802470\n",
      "Step: [139] d_loss: 1.11926675, g_loss: 0.93712217\n",
      "Step: [140] d_loss: 1.11116540, g_loss: 0.95786434\n",
      "Step: [141] d_loss: 1.14039898, g_loss: 0.91237140\n",
      "Step: [142] d_loss: 1.18107939, g_loss: 0.90169883\n",
      "Step: [143] d_loss: 1.12729704, g_loss: 0.89275861\n",
      "Step: [144] d_loss: 1.15619516, g_loss: 0.90997642\n",
      "Step: [145] d_loss: 1.17009163, g_loss: 0.91544223\n",
      "Step: [146] d_loss: 1.16300893, g_loss: 0.90998274\n",
      "Step: [147] d_loss: 1.19951689, g_loss: 0.94822741\n",
      "Step: [148] d_loss: 1.25188851, g_loss: 0.92271316\n",
      "Step: [149] d_loss: 1.19459772, g_loss: 0.89674610\n",
      "Step: [150] d_loss: 1.18645680, g_loss: 0.88609487\n",
      "Step: [151] d_loss: 1.11934960, g_loss: 0.92508990\n",
      "Step: [152] d_loss: 1.13753462, g_loss: 0.91840124\n",
      "Step: [153] d_loss: 1.12994218, g_loss: 0.91282988\n",
      "Step: [154] d_loss: 1.19934726, g_loss: 0.88700545\n",
      "Step: [155] d_loss: 1.14396024, g_loss: 0.88791728\n",
      "Step: [156] d_loss: 1.12024677, g_loss: 0.92596173\n",
      "Step: [157] d_loss: 1.18098986, g_loss: 0.89464927\n",
      "Step: [158] d_loss: 1.19415247, g_loss: 0.93405008\n",
      "Step: [159] d_loss: 1.17180324, g_loss: 0.92556840\n",
      "Step: [160] d_loss: 1.18370080, g_loss: 0.89431024\n",
      "Step: [161] d_loss: 1.15260243, g_loss: 0.89381218\n",
      "Step: [162] d_loss: 1.15128875, g_loss: 0.90585458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [163] d_loss: 1.16021383, g_loss: 0.90096569\n",
      "Step: [164] d_loss: 1.13746166, g_loss: 0.93918234\n",
      "Step: [165] d_loss: 1.29154968, g_loss: 0.92979228\n",
      "Step: [166] d_loss: 1.19562268, g_loss: 0.92627609\n",
      "Step: [167] d_loss: 1.25973105, g_loss: 0.90020436\n",
      "Step: [168] d_loss: 1.13062263, g_loss: 0.90589327\n",
      "Step: [169] d_loss: 1.11436629, g_loss: 0.92416775\n",
      "Step: [170] d_loss: 1.17126727, g_loss: 0.86015511\n",
      "Step: [171] d_loss: 1.18663919, g_loss: 0.90839958\n",
      "Step: [172] d_loss: 1.19090140, g_loss: 0.92560428\n",
      "Step: [173] d_loss: 1.18500829, g_loss: 0.93888623\n",
      "Step: [174] d_loss: 1.29322529, g_loss: 0.91154134\n",
      "Step: [175] d_loss: 1.21504068, g_loss: 0.99424374\n",
      "Step: [176] d_loss: 1.32586646, g_loss: 0.95394939\n",
      "Step: [177] d_loss: 1.28439057, g_loss: 0.90463310\n",
      "Step: [178] d_loss: 1.26655209, g_loss: 0.86106753\n",
      "Step: [179] d_loss: 1.24316525, g_loss: 0.86450154\n",
      "Step: [180] d_loss: 1.23305821, g_loss: 0.91394925\n",
      "Step: [181] d_loss: 1.19141841, g_loss: 0.89416999\n",
      "Step: [182] d_loss: 1.25196147, g_loss: 0.90646768\n",
      "Step: [183] d_loss: 1.20383549, g_loss: 0.90264171\n",
      "Step: [184] d_loss: 1.29907990, g_loss: 0.85972440\n",
      "Step: [185] d_loss: 1.25390446, g_loss: 0.84797198\n",
      "Step: [186] d_loss: 1.17578781, g_loss: 0.88794434\n",
      "Step: [187] d_loss: 1.21621251, g_loss: 0.86063248\n",
      "Step: [188] d_loss: 1.22351313, g_loss: 0.84932572\n",
      "Step: [189] d_loss: 1.18750048, g_loss: 0.86363196\n",
      "Step: [190] d_loss: 1.20845103, g_loss: 0.88038832\n",
      "Step: [191] d_loss: 1.25163805, g_loss: 0.86986721\n",
      "Step: [192] d_loss: 1.29272711, g_loss: 0.88367444\n",
      "Step: [193] d_loss: 1.25817132, g_loss: 0.87032652\n",
      "Step: [194] d_loss: 1.23281431, g_loss: 0.88744205\n",
      "Step: [195] d_loss: 1.22478437, g_loss: 0.90864539\n",
      "Step: [196] d_loss: 1.29053628, g_loss: 0.87844032\n",
      "Step: [197] d_loss: 1.22259521, g_loss: 0.90626597\n",
      "Step: [198] d_loss: 1.20017648, g_loss: 0.83680511\n",
      "Step: [199] d_loss: 1.24714875, g_loss: 0.86401391\n",
      "Step: [200] d_loss: 1.31163740, g_loss: 0.86329675\n",
      "Step: [201] d_loss: 1.25030470, g_loss: 0.88046908\n",
      "Step: [202] d_loss: 1.23821282, g_loss: 0.83311957\n",
      "Step: [203] d_loss: 1.27529430, g_loss: 0.83535981\n",
      "Step: [204] d_loss: 1.27010846, g_loss: 0.86298048\n",
      "Step: [205] d_loss: 1.27600849, g_loss: 0.85077345\n",
      "Step: [206] d_loss: 1.27774763, g_loss: 0.85581934\n",
      "Step: [207] d_loss: 1.26450872, g_loss: 0.85740542\n",
      "Step: [208] d_loss: 1.24486589, g_loss: 0.83787555\n",
      "Step: [209] d_loss: 1.24843049, g_loss: 0.81513560\n",
      "Step: [210] d_loss: 1.24310505, g_loss: 0.84193081\n",
      "Step: [211] d_loss: 1.25828195, g_loss: 0.83146578\n",
      "Step: [212] d_loss: 1.24579024, g_loss: 0.86579478\n",
      "Step: [213] d_loss: 1.28122497, g_loss: 0.86247253\n",
      "Step: [214] d_loss: 1.29048812, g_loss: 0.84252548\n",
      "Step: [215] d_loss: 1.24372339, g_loss: 0.83287728\n",
      "Step: [216] d_loss: 1.26543021, g_loss: 0.82348132\n",
      "Step: [217] d_loss: 1.26926470, g_loss: 0.81070185\n",
      "Step: [218] d_loss: 1.25726509, g_loss: 0.84955752\n",
      "Step: [219] d_loss: 1.27239108, g_loss: 0.87762529\n",
      "Step: [220] d_loss: 1.30143809, g_loss: 0.87039495\n",
      "Step: [221] d_loss: 1.29214358, g_loss: 0.85859334\n",
      "Step: [222] d_loss: 1.27771461, g_loss: 0.82457852\n",
      "Step: [223] d_loss: 1.22396088, g_loss: 0.81958807\n",
      "Step: [224] d_loss: 1.26752186, g_loss: 0.84291285\n",
      "Step: [225] d_loss: 1.30690980, g_loss: 0.84243673\n",
      "Step: [226] d_loss: 1.26760542, g_loss: 0.85250962\n",
      "Step: [227] d_loss: 1.28209567, g_loss: 0.81965363\n",
      "Step: [228] d_loss: 1.27293789, g_loss: 0.83191764\n",
      "Step: [229] d_loss: 1.28571987, g_loss: 0.82933241\n",
      "Step: [230] d_loss: 1.32035899, g_loss: 0.82466507\n",
      "Step: [231] d_loss: 1.26114559, g_loss: 0.86526811\n",
      "Step: [232] d_loss: 1.25498307, g_loss: 0.86627364\n",
      "Step: [233] d_loss: 1.25492656, g_loss: 0.86759448\n",
      "Step: [234] d_loss: 1.26631021, g_loss: 0.81573528\n",
      "Step: [235] d_loss: 1.31213212, g_loss: 0.81934506\n",
      "Step: [236] d_loss: 1.24740148, g_loss: 0.81455916\n",
      "Step: [237] d_loss: 1.26493597, g_loss: 0.85838765\n",
      "Step: [238] d_loss: 1.22596431, g_loss: 0.83707011\n",
      "Step: [239] d_loss: 1.29007936, g_loss: 0.84515768\n",
      "Step: [240] d_loss: 1.30200160, g_loss: 0.85692948\n",
      "Step: [241] d_loss: 1.25451541, g_loss: 0.86534911\n",
      "Step: [242] d_loss: 1.25971174, g_loss: 0.84276080\n",
      "Step: [243] d_loss: 1.25654626, g_loss: 0.82018495\n",
      "Step: [244] d_loss: 1.29494154, g_loss: 0.81826395\n",
      "Step: [245] d_loss: 1.33681583, g_loss: 0.79906827\n",
      "Step: [246] d_loss: 1.25785589, g_loss: 0.84688175\n",
      "Step: [247] d_loss: 1.24419320, g_loss: 0.84508604\n",
      "Step: [248] d_loss: 1.26797056, g_loss: 0.82543778\n",
      "Step: [249] d_loss: 1.25633502, g_loss: 0.83854258\n",
      "Step: [250] d_loss: 1.31215656, g_loss: 0.83772457\n",
      "Step: [251] d_loss: 1.31071925, g_loss: 0.85722411\n",
      "Step: [252] d_loss: 1.31299615, g_loss: 0.85544527\n",
      "Step: [253] d_loss: 1.28368139, g_loss: 0.86585474\n",
      "Step: [254] d_loss: 1.25325155, g_loss: 0.84692699\n",
      "Step: [255] d_loss: 1.27212763, g_loss: 0.80414838\n",
      "Step: [256] d_loss: 1.28235054, g_loss: 0.81106782\n",
      "Step: [257] d_loss: 1.27559972, g_loss: 0.82043231\n",
      "Step: [258] d_loss: 1.30310941, g_loss: 0.82607436\n",
      "Step: [259] d_loss: 1.27969551, g_loss: 0.82524371\n",
      "Step: [260] d_loss: 1.26149929, g_loss: 0.82127142\n",
      "Step: [261] d_loss: 1.28228641, g_loss: 0.88344276\n",
      "Step: [262] d_loss: 1.27070391, g_loss: 0.87862194\n",
      "Step: [263] d_loss: 1.26347506, g_loss: 0.82672960\n",
      "Step: [264] d_loss: 1.28806376, g_loss: 0.82753003\n",
      "Step: [265] d_loss: 1.25123429, g_loss: 0.84495145\n",
      "Step: [266] d_loss: 1.27950907, g_loss: 0.87786841\n",
      "Step: [267] d_loss: 1.28542602, g_loss: 0.87659353\n",
      "Step: [268] d_loss: 1.32125199, g_loss: 0.85665268\n",
      "Step: [269] d_loss: 1.24766397, g_loss: 0.85064876\n",
      "Step: [270] d_loss: 1.29824567, g_loss: 0.82050788\n",
      "Step: [271] d_loss: 1.26115918, g_loss: 0.81049085\n",
      "Step: [272] d_loss: 1.28694606, g_loss: 0.80341119\n",
      "Step: [273] d_loss: 1.29182434, g_loss: 0.83717579\n",
      "Step: [274] d_loss: 1.26954663, g_loss: 0.88074970\n",
      "Step: [275] d_loss: 1.31532347, g_loss: 0.83936340\n",
      "Step: [276] d_loss: 1.25767303, g_loss: 0.85263544\n",
      "Step: [277] d_loss: 1.26323128, g_loss: 0.84014583\n",
      "Step: [278] d_loss: 1.26979697, g_loss: 0.83329499\n",
      "Step: [279] d_loss: 1.27781343, g_loss: 0.81106830\n",
      "Step: [280] d_loss: 1.29090834, g_loss: 0.83102965\n",
      "Step: [281] d_loss: 1.26397109, g_loss: 0.84223336\n",
      "Step: [282] d_loss: 1.26423383, g_loss: 0.83289182\n",
      "Step: [283] d_loss: 1.28218257, g_loss: 0.82145548\n",
      "Step: [284] d_loss: 1.30691385, g_loss: 0.80297256\n",
      "Step: [285] d_loss: 1.26972985, g_loss: 0.83318758\n",
      "Step: [286] d_loss: 1.25768960, g_loss: 0.84780234\n",
      "Step: [287] d_loss: 1.26909411, g_loss: 0.83372301\n",
      "Step: [288] d_loss: 1.26905107, g_loss: 0.84521830\n",
      "Step: [289] d_loss: 1.27112520, g_loss: 0.86499202\n",
      "Step: [290] d_loss: 1.33295155, g_loss: 0.81899476\n",
      "Step: [291] d_loss: 1.29473591, g_loss: 0.83754885\n",
      "Step: [292] d_loss: 1.31145906, g_loss: 0.84368646\n",
      "Step: [293] d_loss: 1.28081703, g_loss: 0.85155785\n",
      "Step: [294] d_loss: 1.27209544, g_loss: 0.85877323\n",
      "Step: [295] d_loss: 1.26853144, g_loss: 0.82746613\n",
      "Step: [296] d_loss: 1.26043272, g_loss: 0.80201304\n",
      "Step: [297] d_loss: 1.25199461, g_loss: 0.80827016\n",
      "Step: [298] d_loss: 1.26353049, g_loss: 0.81579554\n",
      "Step: [299] d_loss: 1.29825997, g_loss: 0.82123601\n",
      "Step: [300] d_loss: 1.28584790, g_loss: 0.86271048\n",
      "Step: [301] d_loss: 1.28808761, g_loss: 0.85523909\n",
      "Step: [302] d_loss: 1.25722432, g_loss: 0.85456824\n",
      "Step: [303] d_loss: 1.27786231, g_loss: 0.83745897\n",
      "Step: [304] d_loss: 1.32031465, g_loss: 0.82080454\n",
      "Step: [305] d_loss: 1.27518749, g_loss: 0.83240461\n",
      "Step: [306] d_loss: 1.33305097, g_loss: 0.82866120\n",
      "Step: [307] d_loss: 1.30994320, g_loss: 0.79900694\n",
      "Step: [308] d_loss: 1.28019011, g_loss: 0.82626820\n",
      "Step: [309] d_loss: 1.31880200, g_loss: 0.84381092\n",
      "Step: [310] d_loss: 1.24548423, g_loss: 0.85241365\n",
      "Step: [311] d_loss: 1.24940789, g_loss: 0.84645402\n",
      "Step: [312] d_loss: 1.27102816, g_loss: 0.80427814\n",
      "Step: [313] d_loss: 1.26763451, g_loss: 0.83361232\n",
      "Step: [314] d_loss: 1.26459742, g_loss: 0.82014620\n",
      "Step: [315] d_loss: 1.33137226, g_loss: 0.83282167\n",
      "Step: [316] d_loss: 1.32176042, g_loss: 0.79366136\n",
      "Step: [317] d_loss: 1.25174630, g_loss: 0.85623503\n",
      "Step: [318] d_loss: 1.28155899, g_loss: 0.81808519\n",
      "Step: [319] d_loss: 1.26538801, g_loss: 0.85226864\n",
      "Step: [320] d_loss: 1.32550108, g_loss: 0.77684760\n",
      "Step: [321] d_loss: 1.28641450, g_loss: 0.83672756\n",
      "Step: [322] d_loss: 1.31141925, g_loss: 0.86454338\n",
      "Step: [323] d_loss: 1.29273963, g_loss: 0.87047541\n",
      "Step: [324] d_loss: 1.30864263, g_loss: 0.85153449\n",
      "Step: [325] d_loss: 1.32820964, g_loss: 0.82825601\n",
      "Step: [326] d_loss: 1.26035464, g_loss: 0.81693125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [327] d_loss: 1.24658334, g_loss: 0.84208733\n",
      "Step: [328] d_loss: 1.32064581, g_loss: 0.80755973\n",
      "Step: [329] d_loss: 1.26866007, g_loss: 0.83582515\n",
      "Step: [330] d_loss: 1.27691209, g_loss: 0.82558954\n",
      "Step: [331] d_loss: 1.34289765, g_loss: 0.84369421\n",
      "Step: [332] d_loss: 1.27578294, g_loss: 0.84940726\n",
      "Step: [333] d_loss: 1.28198922, g_loss: 0.84183192\n",
      "Step: [334] d_loss: 1.29296589, g_loss: 0.83730906\n",
      "Step: [335] d_loss: 1.27596307, g_loss: 0.84055465\n",
      "Step: [336] d_loss: 1.30536985, g_loss: 0.82763636\n",
      "Step: [337] d_loss: 1.31339622, g_loss: 0.81619519\n",
      "Step: [338] d_loss: 1.29856896, g_loss: 0.86170655\n",
      "Step: [339] d_loss: 1.30065000, g_loss: 0.80911672\n",
      "Step: [340] d_loss: 1.28131664, g_loss: 0.78773981\n",
      "Step: [341] d_loss: 1.24631858, g_loss: 0.80717397\n",
      "Step: [342] d_loss: 1.29670191, g_loss: 0.80046040\n",
      "Step: [343] d_loss: 1.28210974, g_loss: 0.85831815\n",
      "Step: [344] d_loss: 1.26836264, g_loss: 0.85105300\n",
      "Step: [345] d_loss: 1.29909682, g_loss: 0.85288274\n",
      "Step: [346] d_loss: 1.33700538, g_loss: 0.88724566\n",
      "Step: [347] d_loss: 1.29473603, g_loss: 0.86838889\n",
      "Step: [348] d_loss: 1.28117752, g_loss: 0.81432331\n",
      "Step: [349] d_loss: 1.31393385, g_loss: 0.81309164\n",
      "Step: [350] d_loss: 1.32263732, g_loss: 0.79351175\n",
      "Step: [351] d_loss: 1.28466702, g_loss: 0.80166531\n",
      "Step: [352] d_loss: 1.28710151, g_loss: 0.82862419\n",
      "Step: [353] d_loss: 1.29214358, g_loss: 0.82077914\n",
      "Step: [354] d_loss: 1.31231213, g_loss: 0.84462225\n",
      "Step: [355] d_loss: 1.31698930, g_loss: 0.83156562\n",
      "Step: [356] d_loss: 1.29340899, g_loss: 0.85158402\n",
      "Step: [357] d_loss: 1.25531340, g_loss: 0.81953216\n",
      "Step: [358] d_loss: 1.32729232, g_loss: 0.77409482\n",
      "Step: [359] d_loss: 1.32524991, g_loss: 0.82754409\n",
      "Step: [360] d_loss: 1.27774620, g_loss: 0.84491229\n",
      "Step: [361] d_loss: 1.31477606, g_loss: 0.82779866\n",
      "Step: [362] d_loss: 1.31575346, g_loss: 0.80594784\n",
      "Step: [363] d_loss: 1.34981799, g_loss: 0.81094038\n",
      "Step: [364] d_loss: 1.27373588, g_loss: 0.84809959\n",
      "Step: [365] d_loss: 1.26714158, g_loss: 0.82292712\n",
      "Step: [366] d_loss: 1.30857754, g_loss: 0.84075081\n",
      "Step: [367] d_loss: 1.31186259, g_loss: 0.83139348\n",
      "Step: [368] d_loss: 1.28166819, g_loss: 0.82805020\n",
      "Step: [369] d_loss: 1.31959534, g_loss: 0.82413930\n",
      "Step: [370] d_loss: 1.32757878, g_loss: 0.81880122\n",
      "Step: [371] d_loss: 1.32626724, g_loss: 0.85407579\n",
      "Step: [372] d_loss: 1.31663251, g_loss: 0.84861565\n",
      "Step: [373] d_loss: 1.28750610, g_loss: 0.85260653\n",
      "Step: [374] d_loss: 1.31498325, g_loss: 0.84297907\n",
      "Step: [375] d_loss: 1.27980220, g_loss: 0.83634984\n",
      "Step: [376] d_loss: 1.29177737, g_loss: 0.81123865\n",
      "Step: [377] d_loss: 1.33698440, g_loss: 0.77749395\n",
      "Step: [378] d_loss: 1.31573510, g_loss: 0.80134457\n",
      "Step: [379] d_loss: 1.33700132, g_loss: 0.84286976\n",
      "Step: [380] d_loss: 1.31650424, g_loss: 0.88496482\n",
      "Step: [381] d_loss: 1.34691763, g_loss: 0.84432232\n",
      "Step: [382] d_loss: 1.31414247, g_loss: 0.85222393\n",
      "Step: [383] d_loss: 1.29483926, g_loss: 0.83011597\n",
      "Step: [384] d_loss: 1.28937030, g_loss: 0.82897860\n",
      "Step: [385] d_loss: 1.30456734, g_loss: 0.83594263\n",
      "Step: [386] d_loss: 1.30640054, g_loss: 0.81585485\n",
      "Step: [387] d_loss: 1.30026734, g_loss: 0.82561427\n",
      "Step: [388] d_loss: 1.31165147, g_loss: 0.82033038\n",
      "Step: [389] d_loss: 1.27172947, g_loss: 0.83387601\n",
      "Step: [390] d_loss: 1.32301843, g_loss: 0.80760962\n",
      "Step: [391] d_loss: 1.25666165, g_loss: 0.83909202\n",
      "Step: [392] d_loss: 1.28863621, g_loss: 0.82527256\n",
      "Step: [393] d_loss: 1.30659890, g_loss: 0.80340642\n",
      "Step: [394] d_loss: 1.27451277, g_loss: 0.81209421\n",
      "Step: [395] d_loss: 1.25987148, g_loss: 0.83814216\n",
      "Step: [396] d_loss: 1.28831649, g_loss: 0.79573601\n",
      "Step: [397] d_loss: 1.29377389, g_loss: 0.79513562\n",
      "Step: [398] d_loss: 1.28933239, g_loss: 0.84017503\n",
      "Step: [399] d_loss: 1.30208039, g_loss: 0.84170818\n",
      "Step: [400] d_loss: 1.30072808, g_loss: 0.81651413\n",
      "Step: [401] d_loss: 1.26212478, g_loss: 0.81937647\n",
      "Step: [402] d_loss: 1.26229429, g_loss: 0.81860280\n",
      "Step: [403] d_loss: 1.33179617, g_loss: 0.78162205\n",
      "Step: [404] d_loss: 1.25141501, g_loss: 0.82978690\n",
      "Step: [405] d_loss: 1.28348613, g_loss: 0.81024909\n",
      "Step: [406] d_loss: 1.30758810, g_loss: 0.82011127\n",
      "Step: [407] d_loss: 1.32730281, g_loss: 0.82305974\n",
      "Step: [408] d_loss: 1.33840668, g_loss: 0.83103848\n",
      "Step: [409] d_loss: 1.32978761, g_loss: 0.82016242\n",
      "Step: [410] d_loss: 1.30999446, g_loss: 0.84288073\n",
      "Step: [411] d_loss: 1.31678987, g_loss: 0.81825578\n",
      "Step: [412] d_loss: 1.28752255, g_loss: 0.84088671\n",
      "Step: [413] d_loss: 1.30894971, g_loss: 0.81140715\n",
      "Step: [414] d_loss: 1.29159641, g_loss: 0.85854661\n",
      "Step: [415] d_loss: 1.32635200, g_loss: 0.81522340\n",
      "Step: [416] d_loss: 1.32684255, g_loss: 0.80874109\n",
      "Step: [417] d_loss: 1.28448296, g_loss: 0.85062623\n",
      "Step: [418] d_loss: 1.29939854, g_loss: 0.83362412\n",
      "Step: [419] d_loss: 1.29698718, g_loss: 0.81996250\n",
      "Step: [420] d_loss: 1.36587358, g_loss: 0.77718520\n",
      "Step: [421] d_loss: 1.27077889, g_loss: 0.84372711\n",
      "Step: [422] d_loss: 1.30102253, g_loss: 0.84636086\n",
      "Step: [423] d_loss: 1.33624077, g_loss: 0.82323378\n",
      "Step: [424] d_loss: 1.34307361, g_loss: 0.83294785\n",
      "Step: [425] d_loss: 1.29518735, g_loss: 0.81648993\n",
      "Step: [426] d_loss: 1.31041324, g_loss: 0.83812129\n",
      "Step: [427] d_loss: 1.29261243, g_loss: 0.85297132\n",
      "Step: [428] d_loss: 1.30331063, g_loss: 0.85960686\n",
      "Step: [429] d_loss: 1.32663202, g_loss: 0.81264949\n",
      "Step: [430] d_loss: 1.31998682, g_loss: 0.80136299\n",
      "Step: [431] d_loss: 1.29122460, g_loss: 0.80530858\n",
      "Step: [432] d_loss: 1.30289817, g_loss: 0.80464578\n",
      "Step: [433] d_loss: 1.28215170, g_loss: 0.81223881\n",
      "Step: [434] d_loss: 1.30757141, g_loss: 0.80546963\n",
      "Step: [435] d_loss: 1.27046251, g_loss: 0.81941706\n",
      "Step: [436] d_loss: 1.30763733, g_loss: 0.80178505\n",
      "Step: [437] d_loss: 1.31496096, g_loss: 0.78472924\n",
      "Step: [438] d_loss: 1.31363964, g_loss: 0.79675758\n",
      "Step: [439] d_loss: 1.29179299, g_loss: 0.80721879\n",
      "Step: [440] d_loss: 1.27034676, g_loss: 0.82668781\n",
      "Step: [441] d_loss: 1.31504238, g_loss: 0.82771266\n",
      "Step: [442] d_loss: 1.29067278, g_loss: 0.83870095\n",
      "Step: [443] d_loss: 1.28326869, g_loss: 0.81773365\n",
      "Step: [444] d_loss: 1.29341173, g_loss: 0.84960222\n",
      "Step: [445] d_loss: 1.33039224, g_loss: 0.81730932\n",
      "Step: [446] d_loss: 1.31642902, g_loss: 0.83754832\n",
      "Step: [447] d_loss: 1.29806340, g_loss: 0.85982311\n",
      "Step: [448] d_loss: 1.31631637, g_loss: 0.84911698\n",
      "Step: [449] d_loss: 1.34138882, g_loss: 0.78915817\n",
      "Step: [450] d_loss: 1.28117597, g_loss: 0.82415509\n",
      "Step: [451] d_loss: 1.28225017, g_loss: 0.82205164\n",
      "Step: [452] d_loss: 1.29677761, g_loss: 0.82243145\n",
      "Step: [453] d_loss: 1.32490742, g_loss: 0.80779636\n",
      "Step: [454] d_loss: 1.31731701, g_loss: 0.80802965\n",
      "Step: [455] d_loss: 1.32110715, g_loss: 0.81242156\n",
      "Step: [456] d_loss: 1.26327062, g_loss: 0.83807296\n",
      "Step: [457] d_loss: 1.28523159, g_loss: 0.81558633\n",
      "Step: [458] d_loss: 1.28357279, g_loss: 0.81256604\n",
      "Step: [459] d_loss: 1.25950611, g_loss: 0.81151402\n",
      "Step: [460] d_loss: 1.27116966, g_loss: 0.78872788\n",
      "Step: [461] d_loss: 1.29728317, g_loss: 0.80231714\n",
      "Step: [462] d_loss: 1.26505518, g_loss: 0.81337965\n",
      "Step: [463] d_loss: 1.27604735, g_loss: 0.82936859\n",
      "Step: [464] d_loss: 1.34918308, g_loss: 0.81390989\n",
      "Step: [465] d_loss: 1.35533428, g_loss: 0.83242327\n",
      "Step: [466] d_loss: 1.34701025, g_loss: 0.87323689\n",
      "Step: [467] d_loss: 1.36122251, g_loss: 0.81760740\n",
      "Step: [468] d_loss: 1.27115083, g_loss: 0.82153279\n",
      "Step: [469] d_loss: 1.31092596, g_loss: 0.78222382\n",
      "Step: [470] d_loss: 1.30858040, g_loss: 0.78831756\n",
      "Step: [471] d_loss: 1.29200768, g_loss: 0.79979378\n",
      "Step: [472] d_loss: 1.29225719, g_loss: 0.82823801\n",
      "Step: [473] d_loss: 1.30174446, g_loss: 0.82049787\n",
      "Step: [474] d_loss: 1.32644701, g_loss: 0.83001399\n",
      "Step: [475] d_loss: 1.32231998, g_loss: 0.80158925\n",
      "Step: [476] d_loss: 1.29630184, g_loss: 0.83149147\n",
      "Step: [477] d_loss: 1.30917168, g_loss: 0.83308524\n",
      "Step: [478] d_loss: 1.31713545, g_loss: 0.80848718\n",
      "Step: [479] d_loss: 1.30957532, g_loss: 0.79495245\n",
      "Step: [480] d_loss: 1.30953193, g_loss: 0.80329919\n",
      "Step: [481] d_loss: 1.31324184, g_loss: 0.80759490\n",
      "Step: [482] d_loss: 1.34877014, g_loss: 0.80367696\n",
      "Step: [483] d_loss: 1.33000374, g_loss: 0.80275142\n",
      "Step: [484] d_loss: 1.30517459, g_loss: 0.81661230\n",
      "Step: [485] d_loss: 1.33027196, g_loss: 0.82321203\n",
      "Step: [486] d_loss: 1.33774090, g_loss: 0.80841380\n",
      "Step: [487] d_loss: 1.31852770, g_loss: 0.82278126\n",
      "Step: [488] d_loss: 1.29561329, g_loss: 0.82959509\n",
      "Step: [489] d_loss: 1.28965163, g_loss: 0.83155286\n",
      "Step: [490] d_loss: 1.27830076, g_loss: 0.83217883\n",
      "Step: [491] d_loss: 1.30644011, g_loss: 0.82587409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [492] d_loss: 1.31919992, g_loss: 0.82600820\n",
      "Step: [493] d_loss: 1.29894137, g_loss: 0.82632840\n",
      "Step: [494] d_loss: 1.32972932, g_loss: 0.80943882\n",
      "Step: [495] d_loss: 1.29131925, g_loss: 0.80868787\n",
      "Step: [496] d_loss: 1.32562935, g_loss: 0.80618685\n",
      "Step: [497] d_loss: 1.29935098, g_loss: 0.80482489\n",
      "Step: [498] d_loss: 1.28808022, g_loss: 0.83069849\n",
      "Step: [499] d_loss: 1.27031851, g_loss: 0.82768548\n",
      "Step: [500] d_loss: 1.32106447, g_loss: 0.81960690\n",
      "Step: [501] d_loss: 1.31202292, g_loss: 0.79664785\n",
      "Step: [502] d_loss: 1.32104421, g_loss: 0.82707864\n",
      "Step: [503] d_loss: 1.28995538, g_loss: 0.83127499\n",
      "Step: [504] d_loss: 1.28819084, g_loss: 0.80740374\n",
      "Step: [505] d_loss: 1.28957784, g_loss: 0.81623751\n",
      "Step: [506] d_loss: 1.31816471, g_loss: 0.79930735\n",
      "Step: [507] d_loss: 1.27488804, g_loss: 0.82572919\n",
      "Step: [508] d_loss: 1.31626678, g_loss: 0.81521809\n",
      "Step: [509] d_loss: 1.31395650, g_loss: 0.79788440\n",
      "Step: [510] d_loss: 1.29861200, g_loss: 0.81360614\n",
      "Step: [511] d_loss: 1.30751872, g_loss: 0.80879331\n",
      "Step: [512] d_loss: 1.32133257, g_loss: 0.83997130\n",
      "Step: [513] d_loss: 1.32563424, g_loss: 0.80936265\n",
      "Step: [514] d_loss: 1.34159636, g_loss: 0.80531585\n",
      "Step: [515] d_loss: 1.30733490, g_loss: 0.82057881\n",
      "Step: [516] d_loss: 1.31611609, g_loss: 0.83491552\n",
      "Step: [517] d_loss: 1.33387911, g_loss: 0.83415782\n",
      "Step: [518] d_loss: 1.33034027, g_loss: 0.81460989\n",
      "Step: [519] d_loss: 1.31384420, g_loss: 0.81684089\n",
      "Step: [520] d_loss: 1.30539453, g_loss: 0.80580759\n",
      "Step: [521] d_loss: 1.29234815, g_loss: 0.81214678\n",
      "Step: [522] d_loss: 1.31687832, g_loss: 0.80291212\n",
      "Step: [523] d_loss: 1.33909273, g_loss: 0.80415988\n",
      "Step: [524] d_loss: 1.32167482, g_loss: 0.81375200\n",
      "Step: [525] d_loss: 1.29130805, g_loss: 0.85461807\n",
      "Step: [526] d_loss: 1.32160091, g_loss: 0.83467633\n",
      "Step: [527] d_loss: 1.32538939, g_loss: 0.82182020\n",
      "Step: [528] d_loss: 1.33514726, g_loss: 0.82616621\n",
      "Step: [529] d_loss: 1.31028938, g_loss: 0.79966760\n",
      "Step: [530] d_loss: 1.33980298, g_loss: 0.77694649\n",
      "Step: [531] d_loss: 1.26763141, g_loss: 0.82802331\n",
      "Step: [532] d_loss: 1.28891754, g_loss: 0.80146003\n",
      "Step: [533] d_loss: 1.28316379, g_loss: 0.81982678\n",
      "Step: [534] d_loss: 1.29588223, g_loss: 0.84024680\n",
      "Step: [535] d_loss: 1.28934240, g_loss: 0.83364201\n",
      "Step: [536] d_loss: 1.35669684, g_loss: 0.81533122\n",
      "Step: [537] d_loss: 1.31902623, g_loss: 0.82384229\n",
      "Step: [538] d_loss: 1.32446194, g_loss: 0.81509453\n",
      "Step: [539] d_loss: 1.31904030, g_loss: 0.82198048\n",
      "Step: [540] d_loss: 1.34382164, g_loss: 0.79563069\n",
      "Step: [541] d_loss: 1.29119062, g_loss: 0.81842887\n",
      "Step: [542] d_loss: 1.33471358, g_loss: 0.81750876\n",
      "Step: [543] d_loss: 1.32116580, g_loss: 0.82627511\n",
      "Step: [544] d_loss: 1.27634811, g_loss: 0.81970757\n",
      "Step: [545] d_loss: 1.30184197, g_loss: 0.83419085\n",
      "Step: [546] d_loss: 1.28602076, g_loss: 0.83343339\n",
      "Step: [547] d_loss: 1.26756179, g_loss: 0.84660208\n",
      "Step: [548] d_loss: 1.31101561, g_loss: 0.82074285\n",
      "Step: [549] d_loss: 1.26259160, g_loss: 0.83019322\n",
      "Step: [550] d_loss: 1.31561947, g_loss: 0.80279833\n",
      "Step: [551] d_loss: 1.29709816, g_loss: 0.80626595\n",
      "Step: [552] d_loss: 1.34100950, g_loss: 0.79224485\n",
      "Step: [553] d_loss: 1.34067822, g_loss: 0.77233171\n",
      "Step: [554] d_loss: 1.36086440, g_loss: 0.77293706\n",
      "Step: [555] d_loss: 1.31589687, g_loss: 0.79067218\n",
      "Step: [556] d_loss: 1.30252039, g_loss: 0.80809879\n",
      "Step: [557] d_loss: 1.29136372, g_loss: 0.82196790\n",
      "Step: [558] d_loss: 1.37144005, g_loss: 0.79061341\n",
      "Step: [559] d_loss: 1.30468428, g_loss: 0.83063322\n",
      "Step: [560] d_loss: 1.29557872, g_loss: 0.85086167\n",
      "Step: [561] d_loss: 1.30048585, g_loss: 0.82211041\n",
      "Step: [562] d_loss: 1.30896449, g_loss: 0.83191967\n",
      "Step: [563] d_loss: 1.34353209, g_loss: 0.82401830\n",
      "Step: [564] d_loss: 1.34421825, g_loss: 0.82286245\n",
      "Step: [565] d_loss: 1.29926729, g_loss: 0.83134234\n",
      "Step: [566] d_loss: 1.34813309, g_loss: 0.83607155\n",
      "Step: [567] d_loss: 1.38049340, g_loss: 0.80290043\n",
      "Step: [568] d_loss: 1.34312332, g_loss: 0.77504718\n",
      "Step: [569] d_loss: 1.30983007, g_loss: 0.79557121\n",
      "Step: [570] d_loss: 1.33381796, g_loss: 0.79207295\n",
      "Step: [571] d_loss: 1.30850625, g_loss: 0.81124640\n",
      "Step: [572] d_loss: 1.29001260, g_loss: 0.82447338\n",
      "Step: [573] d_loss: 1.33576012, g_loss: 0.79195791\n",
      "Step: [574] d_loss: 1.37008893, g_loss: 0.76978111\n",
      "Step: [575] d_loss: 1.31822038, g_loss: 0.82371402\n",
      "Step: [576] d_loss: 1.33647668, g_loss: 0.79277694\n",
      "Step: [577] d_loss: 1.31200242, g_loss: 0.81517065\n",
      "Step: [578] d_loss: 1.32960594, g_loss: 0.80088496\n",
      "Step: [579] d_loss: 1.32633710, g_loss: 0.80669248\n",
      "Step: [580] d_loss: 1.31753993, g_loss: 0.80988103\n",
      "Step: [581] d_loss: 1.32809365, g_loss: 0.79841614\n",
      "Step: [582] d_loss: 1.31550419, g_loss: 0.79500961\n",
      "Step: [583] d_loss: 1.32409978, g_loss: 0.79717410\n",
      "Step: [584] d_loss: 1.33686495, g_loss: 0.78754646\n",
      "Step: [585] d_loss: 1.32581306, g_loss: 0.81758553\n",
      "Step: [586] d_loss: 1.30992723, g_loss: 0.85121846\n",
      "Step: [587] d_loss: 1.32692778, g_loss: 0.81272745\n",
      "Step: [588] d_loss: 1.30263352, g_loss: 0.81799793\n",
      "Step: [589] d_loss: 1.33205128, g_loss: 0.77983963\n",
      "Step: [590] d_loss: 1.32585454, g_loss: 0.78662467\n",
      "Step: [591] d_loss: 1.36305261, g_loss: 0.79369843\n",
      "Step: [592] d_loss: 1.30517077, g_loss: 0.81405503\n",
      "Step: [593] d_loss: 1.30343783, g_loss: 0.81552887\n",
      "Step: [594] d_loss: 1.31429720, g_loss: 0.82356417\n",
      "Step: [595] d_loss: 1.33469081, g_loss: 0.83441401\n",
      "Step: [596] d_loss: 1.33361077, g_loss: 0.79540396\n",
      "Step: [597] d_loss: 1.32692361, g_loss: 0.79296052\n",
      "Step: [598] d_loss: 1.30297756, g_loss: 0.79788083\n",
      "Step: [599] d_loss: 1.28140330, g_loss: 0.79074097\n",
      "Step: [600] d_loss: 1.32773638, g_loss: 0.81432462\n",
      "Step: [601] d_loss: 1.31395602, g_loss: 0.82750183\n",
      "Step: [602] d_loss: 1.33915234, g_loss: 0.81693399\n",
      "Step: [603] d_loss: 1.32955766, g_loss: 0.79366738\n",
      "Step: [604] d_loss: 1.31162930, g_loss: 0.80950105\n",
      "Step: [605] d_loss: 1.30894995, g_loss: 0.79693043\n",
      "Step: [606] d_loss: 1.32969034, g_loss: 0.83277518\n",
      "Step: [607] d_loss: 1.30245769, g_loss: 0.84941900\n",
      "Step: [608] d_loss: 1.35146070, g_loss: 0.77379841\n",
      "Step: [609] d_loss: 1.30472362, g_loss: 0.82165432\n",
      "Step: [610] d_loss: 1.31967974, g_loss: 0.81088787\n",
      "Step: [611] d_loss: 1.34421706, g_loss: 0.80332470\n",
      "Step: [612] d_loss: 1.31174111, g_loss: 0.79793417\n",
      "Step: [613] d_loss: 1.31258392, g_loss: 0.79202384\n",
      "Step: [614] d_loss: 1.29035115, g_loss: 0.81762969\n",
      "Step: [615] d_loss: 1.28988647, g_loss: 0.82393897\n",
      "Step: [616] d_loss: 1.28224468, g_loss: 0.81366611\n",
      "Step: [617] d_loss: 1.29604590, g_loss: 0.81994355\n",
      "Step: [618] d_loss: 1.26863170, g_loss: 0.82206917\n",
      "Step: [619] d_loss: 1.34539509, g_loss: 0.79607415\n",
      "Step: [620] d_loss: 1.29433703, g_loss: 0.81279373\n",
      "Step: [621] d_loss: 1.29338193, g_loss: 0.80796134\n",
      "Step: [622] d_loss: 1.35691643, g_loss: 0.77148420\n",
      "Step: [623] d_loss: 1.32985890, g_loss: 0.82021087\n",
      "Step: [624] d_loss: 1.33782053, g_loss: 0.80544639\n",
      "Step: [625] d_loss: 1.33958423, g_loss: 0.81974983\n",
      "Step: [626] d_loss: 1.33800316, g_loss: 0.79944909\n",
      "Step: [627] d_loss: 1.31253862, g_loss: 0.81692499\n",
      "Step: [628] d_loss: 1.34466171, g_loss: 0.81001663\n",
      "Step: [629] d_loss: 1.31601250, g_loss: 0.82505375\n",
      "Step: [630] d_loss: 1.29972589, g_loss: 0.79131138\n",
      "Step: [631] d_loss: 1.32931864, g_loss: 0.79508770\n",
      "Step: [632] d_loss: 1.33372009, g_loss: 0.78768039\n",
      "Step: [633] d_loss: 1.28171480, g_loss: 0.82670152\n",
      "Step: [634] d_loss: 1.30578065, g_loss: 0.82540333\n",
      "Step: [635] d_loss: 1.29378510, g_loss: 0.81661439\n",
      "Step: [636] d_loss: 1.30699086, g_loss: 0.81962848\n",
      "Step: [637] d_loss: 1.31434727, g_loss: 0.83077908\n",
      "Step: [638] d_loss: 1.33618903, g_loss: 0.83411193\n",
      "Step: [639] d_loss: 1.32666230, g_loss: 0.82405496\n",
      "Step: [640] d_loss: 1.33478022, g_loss: 0.79951209\n",
      "Step: [641] d_loss: 1.34741068, g_loss: 0.78443712\n",
      "Step: [642] d_loss: 1.31141210, g_loss: 0.79288661\n",
      "Step: [643] d_loss: 1.33741641, g_loss: 0.80435383\n",
      "Step: [644] d_loss: 1.34626114, g_loss: 0.79353166\n",
      "Step: [645] d_loss: 1.29472327, g_loss: 0.81274742\n",
      "Step: [646] d_loss: 1.32346761, g_loss: 0.82975650\n",
      "Step: [647] d_loss: 1.30729759, g_loss: 0.83195937\n",
      "Step: [648] d_loss: 1.30932117, g_loss: 0.81765437\n",
      "Step: [649] d_loss: 1.29820836, g_loss: 0.81918263\n",
      "Step: [650] d_loss: 1.28053021, g_loss: 0.80521494\n",
      "Step: [651] d_loss: 1.30999804, g_loss: 0.80934262\n",
      "Step: [652] d_loss: 1.32891369, g_loss: 0.80451953\n",
      "Step: [653] d_loss: 1.31068254, g_loss: 0.83128208\n",
      "Step: [654] d_loss: 1.31626749, g_loss: 0.81269991\n",
      "Step: [655] d_loss: 1.33596909, g_loss: 0.77763307\n",
      "Step: [656] d_loss: 1.31375682, g_loss: 0.80675328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [657] d_loss: 1.36312556, g_loss: 0.77613235\n",
      "Step: [658] d_loss: 1.29463995, g_loss: 0.82174325\n",
      "Step: [659] d_loss: 1.32012284, g_loss: 0.81341636\n",
      "Step: [660] d_loss: 1.33217204, g_loss: 0.80624276\n",
      "Step: [661] d_loss: 1.32786918, g_loss: 0.80297601\n",
      "Step: [662] d_loss: 1.29863572, g_loss: 0.82023185\n",
      "Step: [663] d_loss: 1.27490652, g_loss: 0.82199132\n",
      "Step: [664] d_loss: 1.33112705, g_loss: 0.82189906\n",
      "Step: [665] d_loss: 1.32108426, g_loss: 0.79312301\n",
      "Step: [666] d_loss: 1.31127477, g_loss: 0.80638045\n",
      "Step: [667] d_loss: 1.31000996, g_loss: 0.80903995\n",
      "Step: [668] d_loss: 1.30697501, g_loss: 0.81195432\n",
      "Step: [669] d_loss: 1.36269605, g_loss: 0.79791838\n",
      "Step: [670] d_loss: 1.30798066, g_loss: 0.80204159\n",
      "Step: [671] d_loss: 1.31512094, g_loss: 0.80292547\n",
      "Step: [672] d_loss: 1.35746884, g_loss: 0.79257369\n",
      "Step: [673] d_loss: 1.34614563, g_loss: 0.80340099\n",
      "Step: [674] d_loss: 1.31588888, g_loss: 0.80986667\n",
      "Step: [675] d_loss: 1.32730138, g_loss: 0.80052900\n",
      "Step: [676] d_loss: 1.28981042, g_loss: 0.83331358\n",
      "Step: [677] d_loss: 1.29549336, g_loss: 0.82908767\n",
      "Step: [678] d_loss: 1.30800962, g_loss: 0.81188577\n",
      "Step: [679] d_loss: 1.33249819, g_loss: 0.79465115\n",
      "Step: [680] d_loss: 1.31191969, g_loss: 0.81701267\n",
      "Step: [681] d_loss: 1.29784298, g_loss: 0.80433083\n",
      "Step: [682] d_loss: 1.30929470, g_loss: 0.78528202\n",
      "Step: [683] d_loss: 1.36920822, g_loss: 0.77146322\n",
      "Step: [684] d_loss: 1.30984426, g_loss: 0.80828130\n",
      "Step: [685] d_loss: 1.31801951, g_loss: 0.83442128\n",
      "Step: [686] d_loss: 1.29688382, g_loss: 0.82648659\n",
      "Step: [687] d_loss: 1.30483198, g_loss: 0.81896055\n",
      "Step: [688] d_loss: 1.31206191, g_loss: 0.79794180\n",
      "Step: [689] d_loss: 1.32241178, g_loss: 0.79222608\n",
      "Step: [690] d_loss: 1.29822516, g_loss: 0.82434702\n",
      "Step: [691] d_loss: 1.30786610, g_loss: 0.80518925\n",
      "Step: [692] d_loss: 1.29905748, g_loss: 0.81298012\n",
      "Step: [693] d_loss: 1.31458461, g_loss: 0.79895759\n",
      "Step: [694] d_loss: 1.29601359, g_loss: 0.82319659\n",
      "Step: [695] d_loss: 1.33043599, g_loss: 0.80169582\n",
      "Step: [696] d_loss: 1.30846524, g_loss: 0.81521118\n",
      "Step: [697] d_loss: 1.28302789, g_loss: 0.84224319\n",
      "Step: [698] d_loss: 1.28760469, g_loss: 0.82407159\n",
      "Step: [699] d_loss: 1.30450189, g_loss: 0.80793929\n",
      "Step: [700] d_loss: 1.32217383, g_loss: 0.78290677\n",
      "Step: [701] d_loss: 1.27674878, g_loss: 0.82442975\n",
      "Step: [702] d_loss: 1.32044256, g_loss: 0.79602242\n",
      "Step: [703] d_loss: 1.31475759, g_loss: 0.80680549\n",
      "Step: [704] d_loss: 1.32465827, g_loss: 0.79508889\n",
      "Step: [705] d_loss: 1.33375478, g_loss: 0.82009923\n",
      "Step: [706] d_loss: 1.33827412, g_loss: 0.80498219\n",
      "Step: [707] d_loss: 1.27161610, g_loss: 0.83109874\n",
      "Step: [708] d_loss: 1.33029556, g_loss: 0.77901441\n",
      "Step: [709] d_loss: 1.29175067, g_loss: 0.80111051\n",
      "Step: [710] d_loss: 1.30489194, g_loss: 0.79073888\n",
      "Step: [711] d_loss: 1.34353900, g_loss: 0.77561891\n",
      "Step: [712] d_loss: 1.31532145, g_loss: 0.81877244\n",
      "Step: [713] d_loss: 1.31962216, g_loss: 0.81652194\n",
      "Step: [714] d_loss: 1.29972482, g_loss: 0.82156217\n",
      "Step: [715] d_loss: 1.32174683, g_loss: 0.79992282\n",
      "Step: [716] d_loss: 1.29896617, g_loss: 0.80901378\n",
      "Step: [717] d_loss: 1.30630207, g_loss: 0.79587370\n",
      "Step: [718] d_loss: 1.32367802, g_loss: 0.79464316\n",
      "Step: [719] d_loss: 1.31399739, g_loss: 0.79933280\n",
      "Step: [720] d_loss: 1.29856062, g_loss: 0.82058740\n",
      "Step: [721] d_loss: 1.33520627, g_loss: 0.81202960\n",
      "Step: [722] d_loss: 1.34623837, g_loss: 0.80890894\n",
      "Step: [723] d_loss: 1.33666706, g_loss: 0.80018449\n",
      "Step: [724] d_loss: 1.30113399, g_loss: 0.82555556\n",
      "Step: [725] d_loss: 1.27946150, g_loss: 0.82013547\n",
      "Step: [726] d_loss: 1.31603217, g_loss: 0.78684402\n",
      "Step: [727] d_loss: 1.34514546, g_loss: 0.79193670\n",
      "Step: [728] d_loss: 1.34903836, g_loss: 0.80443323\n",
      "Step: [729] d_loss: 1.29192448, g_loss: 0.81757277\n",
      "Step: [730] d_loss: 1.30014396, g_loss: 0.79893947\n",
      "Step: [731] d_loss: 1.32817030, g_loss: 0.79220736\n",
      "Step: [732] d_loss: 1.31374240, g_loss: 0.80769074\n",
      "Step: [733] d_loss: 1.28555417, g_loss: 0.82303870\n",
      "Step: [734] d_loss: 1.30051625, g_loss: 0.82750779\n",
      "Step: [735] d_loss: 1.30099940, g_loss: 0.85751438\n",
      "Step: [736] d_loss: 1.31992722, g_loss: 0.83537388\n",
      "Step: [737] d_loss: 1.30992579, g_loss: 0.80509967\n",
      "Step: [738] d_loss: 1.25059366, g_loss: 0.83161008\n",
      "Step: [739] d_loss: 1.32107925, g_loss: 0.79916811\n",
      "Step: [740] d_loss: 1.29677939, g_loss: 0.83457255\n",
      "Step: [741] d_loss: 1.31460428, g_loss: 0.80353040\n",
      "Step: [742] d_loss: 1.31510973, g_loss: 0.80530512\n",
      "Step: [743] d_loss: 1.31998348, g_loss: 0.82453674\n",
      "Step: [744] d_loss: 1.32342386, g_loss: 0.83273089\n",
      "Step: [745] d_loss: 1.32979417, g_loss: 0.81648785\n",
      "Step: [746] d_loss: 1.31497741, g_loss: 0.81932902\n",
      "Step: [747] d_loss: 1.33643138, g_loss: 0.78782713\n",
      "Step: [748] d_loss: 1.31334460, g_loss: 0.81598538\n",
      "Step: [749] d_loss: 1.33125961, g_loss: 0.80781090\n",
      "Step: [750] d_loss: 1.32305002, g_loss: 0.82070768\n",
      "Step: [751] d_loss: 1.31520009, g_loss: 0.79734123\n",
      "Step: [752] d_loss: 1.35061026, g_loss: 0.78176272\n",
      "Step: [753] d_loss: 1.30484438, g_loss: 0.80875456\n",
      "Step: [754] d_loss: 1.28910780, g_loss: 0.82600760\n",
      "Step: [755] d_loss: 1.28091967, g_loss: 0.83387005\n",
      "Step: [756] d_loss: 1.29364753, g_loss: 0.83790702\n",
      "Step: [757] d_loss: 1.33168030, g_loss: 0.80496758\n",
      "Step: [758] d_loss: 1.34014153, g_loss: 0.81188905\n",
      "Step: [759] d_loss: 1.32963002, g_loss: 0.81530851\n",
      "Step: [760] d_loss: 1.32949233, g_loss: 0.80850112\n",
      "Step: [761] d_loss: 1.32888150, g_loss: 0.83979166\n",
      "Step: [762] d_loss: 1.34933805, g_loss: 0.78621137\n",
      "Step: [763] d_loss: 1.35751939, g_loss: 0.81052738\n",
      "Step: [764] d_loss: 1.31438839, g_loss: 0.82333332\n",
      "Step: [765] d_loss: 1.30021620, g_loss: 0.80602819\n",
      "Step: [766] d_loss: 1.27477884, g_loss: 0.84327453\n",
      "Step: [767] d_loss: 1.29639423, g_loss: 0.83523178\n",
      "Step: [768] d_loss: 1.32370472, g_loss: 0.81854749\n",
      "Step: [769] d_loss: 1.30693710, g_loss: 0.83265018\n",
      "Step: [770] d_loss: 1.35390174, g_loss: 0.81354833\n",
      "Step: [771] d_loss: 1.32320309, g_loss: 0.81196511\n",
      "Step: [772] d_loss: 1.32952714, g_loss: 0.80327260\n",
      "Step: [773] d_loss: 1.31528807, g_loss: 0.80113363\n",
      "Step: [774] d_loss: 1.32433438, g_loss: 0.81204772\n",
      "Step: [775] d_loss: 1.30545616, g_loss: 0.83625495\n",
      "Step: [776] d_loss: 1.31204915, g_loss: 0.82939816\n",
      "Step: [777] d_loss: 1.27598333, g_loss: 0.83947808\n",
      "Step: [778] d_loss: 1.35203803, g_loss: 0.80021483\n",
      "Step: [779] d_loss: 1.32183766, g_loss: 0.81053710\n",
      "Step: [780] d_loss: 1.34530520, g_loss: 0.77678931\n",
      "Step: [781] d_loss: 1.36056614, g_loss: 0.76618028\n",
      "Step: [782] d_loss: 1.31190491, g_loss: 0.79353106\n",
      "Step: [783] d_loss: 1.31652987, g_loss: 0.82157797\n",
      "Step: [784] d_loss: 1.30527878, g_loss: 0.81872833\n",
      "Step: [785] d_loss: 1.30272555, g_loss: 0.80597341\n",
      "Step: [786] d_loss: 1.29156947, g_loss: 0.81688833\n",
      "Step: [787] d_loss: 1.27292025, g_loss: 0.79718113\n",
      "Step: [788] d_loss: 1.32529473, g_loss: 0.79282367\n",
      "Step: [789] d_loss: 1.31986701, g_loss: 0.78568691\n",
      "Step: [790] d_loss: 1.32655931, g_loss: 0.78620780\n",
      "Step: [791] d_loss: 1.28051591, g_loss: 0.80868000\n",
      "Step: [792] d_loss: 1.31828928, g_loss: 0.80225313\n",
      "Step: [793] d_loss: 1.33279240, g_loss: 0.81136549\n",
      "Step: [794] d_loss: 1.30655003, g_loss: 0.83324093\n",
      "Step: [795] d_loss: 1.34467793, g_loss: 0.79996002\n",
      "Step: [796] d_loss: 1.29409289, g_loss: 0.83142525\n",
      "Step: [797] d_loss: 1.29389310, g_loss: 0.81913579\n",
      "Step: [798] d_loss: 1.30180597, g_loss: 0.80305016\n",
      "Step: [799] d_loss: 1.33560395, g_loss: 0.77583575\n",
      "Step: [800] d_loss: 1.28923595, g_loss: 0.79255497\n",
      "Step: [801] d_loss: 1.30718470, g_loss: 0.81205928\n",
      "Step: [802] d_loss: 1.32497609, g_loss: 0.81795704\n",
      "Step: [803] d_loss: 1.36262393, g_loss: 0.80776954\n",
      "Step: [804] d_loss: 1.32733166, g_loss: 0.81672895\n",
      "Step: [805] d_loss: 1.32761598, g_loss: 0.79955804\n",
      "Step: [806] d_loss: 1.31040633, g_loss: 0.79789042\n",
      "Step: [807] d_loss: 1.33954251, g_loss: 0.78107870\n",
      "Step: [808] d_loss: 1.31169796, g_loss: 0.80985528\n",
      "Step: [809] d_loss: 1.31157255, g_loss: 0.81628710\n",
      "Step: [810] d_loss: 1.29939008, g_loss: 0.84580600\n",
      "Step: [811] d_loss: 1.27279353, g_loss: 0.85396534\n",
      "Step: [812] d_loss: 1.30463862, g_loss: 0.83883655\n",
      "Step: [813] d_loss: 1.32028449, g_loss: 0.82135367\n",
      "Step: [814] d_loss: 1.35792375, g_loss: 0.80336642\n",
      "Step: [815] d_loss: 1.34426272, g_loss: 0.80289423\n",
      "Step: [816] d_loss: 1.30020702, g_loss: 0.83179712\n",
      "Step: [817] d_loss: 1.33968818, g_loss: 0.78204358\n",
      "Step: [818] d_loss: 1.33687770, g_loss: 0.78215587\n",
      "Step: [819] d_loss: 1.33513308, g_loss: 0.79908931\n",
      "Step: [820] d_loss: 1.33628297, g_loss: 0.78779209\n",
      "Step: [821] d_loss: 1.33359182, g_loss: 0.78901613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [822] d_loss: 1.33271825, g_loss: 0.78093171\n",
      "Step: [823] d_loss: 1.30086923, g_loss: 0.82239234\n",
      "Step: [824] d_loss: 1.34602296, g_loss: 0.80993307\n",
      "Step: [825] d_loss: 1.26838779, g_loss: 0.80732465\n",
      "Step: [826] d_loss: 1.27906096, g_loss: 0.80745131\n",
      "Step: [827] d_loss: 1.33008850, g_loss: 0.80747497\n",
      "Step: [828] d_loss: 1.30239081, g_loss: 0.81249660\n",
      "Step: [829] d_loss: 1.31433368, g_loss: 0.81301957\n",
      "Step: [830] d_loss: 1.34615266, g_loss: 0.80187845\n",
      "Step: [831] d_loss: 1.35070288, g_loss: 0.78647161\n",
      "Step: [832] d_loss: 1.30778897, g_loss: 0.79355395\n",
      "Step: [833] d_loss: 1.34301090, g_loss: 0.80066371\n",
      "Step: [834] d_loss: 1.30225599, g_loss: 0.82452953\n",
      "Step: [835] d_loss: 1.32577586, g_loss: 0.81280363\n",
      "Step: [836] d_loss: 1.34576333, g_loss: 0.81598687\n",
      "Step: [837] d_loss: 1.29941869, g_loss: 0.79865259\n",
      "Step: [838] d_loss: 1.28317881, g_loss: 0.80402428\n",
      "Step: [839] d_loss: 1.35094762, g_loss: 0.78340650\n",
      "Step: [840] d_loss: 1.35650766, g_loss: 0.79896533\n",
      "Step: [841] d_loss: 1.30264091, g_loss: 0.82518291\n",
      "Step: [842] d_loss: 1.33892882, g_loss: 0.79302657\n",
      "Step: [843] d_loss: 1.33318973, g_loss: 0.79932022\n",
      "Step: [844] d_loss: 1.32029343, g_loss: 0.79196781\n",
      "Step: [845] d_loss: 1.32940531, g_loss: 0.79507327\n",
      "Step: [846] d_loss: 1.34417427, g_loss: 0.81233132\n",
      "Step: [847] d_loss: 1.34434986, g_loss: 0.80756986\n",
      "Step: [848] d_loss: 1.29505742, g_loss: 0.80244446\n",
      "Step: [849] d_loss: 1.27179706, g_loss: 0.83786654\n",
      "Step: [850] d_loss: 1.33683777, g_loss: 0.80116504\n",
      "Step: [851] d_loss: 1.31471109, g_loss: 0.81471086\n",
      "Step: [852] d_loss: 1.29159081, g_loss: 0.82565761\n",
      "Step: [853] d_loss: 1.33905888, g_loss: 0.80592227\n",
      "Step: [854] d_loss: 1.33507776, g_loss: 0.79220986\n",
      "Step: [855] d_loss: 1.34943056, g_loss: 0.80388403\n",
      "Step: [856] d_loss: 1.34262979, g_loss: 0.77530539\n",
      "Step: [857] d_loss: 1.33286190, g_loss: 0.79023516\n",
      "Step: [858] d_loss: 1.34320426, g_loss: 0.77426755\n",
      "Step: [859] d_loss: 1.31174397, g_loss: 0.79929101\n",
      "Step: [860] d_loss: 1.33290231, g_loss: 0.80301869\n",
      "Step: [861] d_loss: 1.28074491, g_loss: 0.84467393\n",
      "Step: [862] d_loss: 1.28275967, g_loss: 0.84966373\n",
      "Step: [863] d_loss: 1.29657030, g_loss: 0.84886742\n",
      "Step: [864] d_loss: 1.33924174, g_loss: 0.79179478\n",
      "Step: [865] d_loss: 1.32580054, g_loss: 0.82611942\n",
      "Step: [866] d_loss: 1.32320118, g_loss: 0.80259049\n",
      "Step: [867] d_loss: 1.34142184, g_loss: 0.81715721\n",
      "Step: [868] d_loss: 1.32921350, g_loss: 0.81681085\n",
      "Step: [869] d_loss: 1.30022359, g_loss: 0.81879956\n",
      "Step: [870] d_loss: 1.30332148, g_loss: 0.80454254\n",
      "Step: [871] d_loss: 1.27448225, g_loss: 0.82667202\n",
      "Step: [872] d_loss: 1.28771281, g_loss: 0.81237346\n",
      "Step: [873] d_loss: 1.28329468, g_loss: 0.80321294\n",
      "Step: [874] d_loss: 1.27215743, g_loss: 0.82652795\n",
      "Step: [875] d_loss: 1.30242980, g_loss: 0.81958365\n",
      "Step: [876] d_loss: 1.28522134, g_loss: 0.83866626\n",
      "Step: [877] d_loss: 1.30983305, g_loss: 0.82254881\n",
      "Step: [878] d_loss: 1.30647278, g_loss: 0.82516837\n",
      "Step: [879] d_loss: 1.31462932, g_loss: 0.84248239\n",
      "Step: [880] d_loss: 1.32376158, g_loss: 0.82332838\n",
      "Step: [881] d_loss: 1.28367710, g_loss: 0.83062303\n",
      "Step: [882] d_loss: 1.32115591, g_loss: 0.81644905\n",
      "Step: [883] d_loss: 1.31341398, g_loss: 0.79451096\n",
      "Step: [884] d_loss: 1.30631995, g_loss: 0.78327137\n",
      "Step: [885] d_loss: 1.32332981, g_loss: 0.79663527\n",
      "Step: [886] d_loss: 1.30532312, g_loss: 0.82264936\n",
      "Step: [887] d_loss: 1.32057738, g_loss: 0.83476877\n",
      "Step: [888] d_loss: 1.32381546, g_loss: 0.83697343\n",
      "Step: [889] d_loss: 1.31869709, g_loss: 0.82059002\n",
      "Step: [890] d_loss: 1.29281592, g_loss: 0.82056856\n",
      "Step: [891] d_loss: 1.31382728, g_loss: 0.81103647\n",
      "Step: [892] d_loss: 1.31479239, g_loss: 0.80532813\n",
      "Step: [893] d_loss: 1.27488065, g_loss: 0.82207286\n",
      "Step: [894] d_loss: 1.31751025, g_loss: 0.80062807\n",
      "Step: [895] d_loss: 1.28100741, g_loss: 0.81898725\n",
      "Step: [896] d_loss: 1.28357649, g_loss: 0.82075560\n",
      "Step: [897] d_loss: 1.28219199, g_loss: 0.82472026\n",
      "Step: [898] d_loss: 1.28577554, g_loss: 0.81233746\n",
      "Step: [899] d_loss: 1.33267283, g_loss: 0.80269027\n",
      "Step: [900] d_loss: 1.26004398, g_loss: 0.83145303\n",
      "Step: [901] d_loss: 1.29374099, g_loss: 0.80870557\n",
      "Step: [902] d_loss: 1.28792810, g_loss: 0.82302833\n",
      "Step: [903] d_loss: 1.27582741, g_loss: 0.82335281\n",
      "Step: [904] d_loss: 1.29404092, g_loss: 0.80898815\n",
      "Step: [905] d_loss: 1.31344664, g_loss: 0.80666292\n",
      "Step: [906] d_loss: 1.32502651, g_loss: 0.80353361\n",
      "Step: [907] d_loss: 1.32852435, g_loss: 0.81626046\n",
      "Step: [908] d_loss: 1.34107733, g_loss: 0.80360502\n",
      "Step: [909] d_loss: 1.31075299, g_loss: 0.82805067\n",
      "Step: [910] d_loss: 1.32653284, g_loss: 0.80099785\n",
      "Step: [911] d_loss: 1.30673718, g_loss: 0.81408203\n",
      "Step: [912] d_loss: 1.30658674, g_loss: 0.82742494\n",
      "Step: [913] d_loss: 1.30033004, g_loss: 0.82010627\n",
      "Step: [914] d_loss: 1.29952109, g_loss: 0.82089937\n",
      "Step: [915] d_loss: 1.35318720, g_loss: 0.81347656\n",
      "Step: [916] d_loss: 1.29118741, g_loss: 0.83332896\n",
      "Step: [917] d_loss: 1.34009004, g_loss: 0.79684412\n",
      "Step: [918] d_loss: 1.35488486, g_loss: 0.80272245\n",
      "Step: [919] d_loss: 1.32837224, g_loss: 0.81937003\n",
      "Step: [920] d_loss: 1.31266546, g_loss: 0.81520128\n",
      "Step: [921] d_loss: 1.30799139, g_loss: 0.80479932\n",
      "Step: [922] d_loss: 1.31322026, g_loss: 0.81617999\n",
      "Step: [923] d_loss: 1.32597077, g_loss: 0.78572363\n",
      "Step: [924] d_loss: 1.34516346, g_loss: 0.79150784\n",
      "Step: [925] d_loss: 1.33857477, g_loss: 0.80523360\n",
      "Step: [926] d_loss: 1.30277622, g_loss: 0.81400812\n",
      "Step: [927] d_loss: 1.30890012, g_loss: 0.81422853\n",
      "Step: [928] d_loss: 1.36473346, g_loss: 0.79174167\n",
      "Step: [929] d_loss: 1.29220724, g_loss: 0.83318651\n",
      "Step: [930] d_loss: 1.30441427, g_loss: 0.82331377\n",
      "Step: [931] d_loss: 1.31024981, g_loss: 0.84039867\n",
      "Step: [932] d_loss: 1.31065917, g_loss: 0.80636895\n",
      "Step: [933] d_loss: 1.30656421, g_loss: 0.81399435\n",
      "Step: [934] d_loss: 1.30913889, g_loss: 0.80345929\n",
      "Step: [935] d_loss: 1.28568769, g_loss: 0.83561683\n",
      "Step: [936] d_loss: 1.32046771, g_loss: 0.81803638\n",
      "Step: [937] d_loss: 1.31905746, g_loss: 0.81206250\n",
      "Step: [938] d_loss: 1.28446567, g_loss: 0.82464576\n",
      "Step: [939] d_loss: 1.27803218, g_loss: 0.83491802\n",
      "Step: [940] d_loss: 1.30120146, g_loss: 0.82259166\n",
      "Step: [941] d_loss: 1.33294356, g_loss: 0.78012216\n",
      "Step: [942] d_loss: 1.26233602, g_loss: 0.82564539\n",
      "Step: [943] d_loss: 1.28331733, g_loss: 0.83280778\n",
      "Step: [944] d_loss: 1.32225823, g_loss: 0.82222289\n",
      "Step: [945] d_loss: 1.33488929, g_loss: 0.82121515\n",
      "Step: [946] d_loss: 1.29105806, g_loss: 0.83573699\n",
      "Step: [947] d_loss: 1.30355096, g_loss: 0.82225502\n",
      "Step: [948] d_loss: 1.31815100, g_loss: 0.80613017\n",
      "Step: [949] d_loss: 1.27437675, g_loss: 0.81285983\n",
      "Step: [950] d_loss: 1.31789517, g_loss: 0.80728948\n",
      "Step: [951] d_loss: 1.34377265, g_loss: 0.82949674\n",
      "Step: [952] d_loss: 1.29604220, g_loss: 0.84878755\n",
      "Step: [953] d_loss: 1.29504514, g_loss: 0.83033532\n",
      "Step: [954] d_loss: 1.31393409, g_loss: 0.82347953\n",
      "Step: [955] d_loss: 1.31360054, g_loss: 0.79439706\n",
      "Step: [956] d_loss: 1.32695127, g_loss: 0.78694916\n",
      "Step: [957] d_loss: 1.31264150, g_loss: 0.79122537\n",
      "Step: [958] d_loss: 1.31846213, g_loss: 0.81481242\n",
      "Step: [959] d_loss: 1.29985464, g_loss: 0.83143264\n",
      "Step: [960] d_loss: 1.32989061, g_loss: 0.85713577\n",
      "Step: [961] d_loss: 1.33273351, g_loss: 0.80872172\n",
      "Step: [962] d_loss: 1.33425951, g_loss: 0.81405175\n",
      "Step: [963] d_loss: 1.29957747, g_loss: 0.82513195\n",
      "Step: [964] d_loss: 1.32788658, g_loss: 0.77828979\n",
      "Step: [965] d_loss: 1.32974923, g_loss: 0.80146325\n",
      "Step: [966] d_loss: 1.32855725, g_loss: 0.81943434\n",
      "Step: [967] d_loss: 1.33783913, g_loss: 0.80320001\n",
      "Step: [968] d_loss: 1.30339527, g_loss: 0.83144128\n",
      "Step: [969] d_loss: 1.28008425, g_loss: 0.83156395\n",
      "Step: [970] d_loss: 1.29849052, g_loss: 0.82504946\n",
      "Step: [971] d_loss: 1.27957857, g_loss: 0.84398907\n",
      "Step: [972] d_loss: 1.32197952, g_loss: 0.81410635\n",
      "Step: [973] d_loss: 1.32039952, g_loss: 0.81141198\n",
      "Step: [974] d_loss: 1.27620852, g_loss: 0.80433583\n",
      "Step: [975] d_loss: 1.32992077, g_loss: 0.80595475\n",
      "Step: [976] d_loss: 1.28688455, g_loss: 0.83179319\n",
      "Step: [977] d_loss: 1.32107115, g_loss: 0.82001030\n",
      "Step: [978] d_loss: 1.29482055, g_loss: 0.83593154\n",
      "Step: [979] d_loss: 1.32691503, g_loss: 0.80650949\n",
      "Step: [980] d_loss: 1.29265451, g_loss: 0.83704138\n",
      "Step: [981] d_loss: 1.31379378, g_loss: 0.81922650\n",
      "Step: [982] d_loss: 1.28329432, g_loss: 0.82110167\n",
      "Step: [983] d_loss: 1.29490876, g_loss: 0.80545139\n",
      "Step: [984] d_loss: 1.31024027, g_loss: 0.81390822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [985] d_loss: 1.29851627, g_loss: 0.83815289\n",
      "Step: [986] d_loss: 1.34135604, g_loss: 0.80725688\n",
      "Step: [987] d_loss: 1.30714369, g_loss: 0.82349306\n",
      "Step: [988] d_loss: 1.31900072, g_loss: 0.81602651\n",
      "Step: [989] d_loss: 1.30903816, g_loss: 0.81499010\n",
      "Step: [990] d_loss: 1.32335544, g_loss: 0.79887807\n",
      "Step: [991] d_loss: 1.28480244, g_loss: 0.84898472\n",
      "Step: [992] d_loss: 1.32285953, g_loss: 0.82787466\n",
      "Step: [993] d_loss: 1.28275514, g_loss: 0.81424403\n",
      "Step: [994] d_loss: 1.31768107, g_loss: 0.82676792\n",
      "Step: [995] d_loss: 1.31438589, g_loss: 0.81864429\n",
      "Step: [996] d_loss: 1.30981839, g_loss: 0.81004667\n",
      "Step: [997] d_loss: 1.33934987, g_loss: 0.79847783\n",
      "Step: [998] d_loss: 1.34644413, g_loss: 0.80133510\n",
      "Step: [999] d_loss: 1.27507520, g_loss: 0.83282137\n",
      "Step: [1000] d_loss: 1.30347049, g_loss: 0.81862581\n",
      "Step: [1001] d_loss: 1.32662499, g_loss: 0.82092631\n",
      "Step: [1002] d_loss: 1.30039120, g_loss: 0.83063281\n",
      "Step: [1003] d_loss: 1.30285764, g_loss: 0.86348963\n",
      "Step: [1004] d_loss: 1.31899977, g_loss: 0.83116823\n",
      "Step: [1005] d_loss: 1.32489979, g_loss: 0.81355715\n",
      "Step: [1006] d_loss: 1.30150628, g_loss: 0.82300258\n",
      "Step: [1007] d_loss: 1.32004464, g_loss: 0.84834045\n",
      "Step: [1008] d_loss: 1.32699847, g_loss: 0.80399472\n",
      "Step: [1009] d_loss: 1.30334270, g_loss: 0.82510865\n",
      "Step: [1010] d_loss: 1.31336522, g_loss: 0.83138704\n",
      "Step: [1011] d_loss: 1.33169460, g_loss: 0.81193900\n",
      "Step: [1012] d_loss: 1.31099713, g_loss: 0.81376684\n",
      "Step: [1013] d_loss: 1.31081402, g_loss: 0.81836522\n",
      "Step: [1014] d_loss: 1.28773391, g_loss: 0.83482420\n",
      "Step: [1015] d_loss: 1.28576684, g_loss: 0.85553360\n",
      "Step: [1016] d_loss: 1.34304476, g_loss: 0.79338968\n",
      "Step: [1017] d_loss: 1.30354524, g_loss: 0.81411898\n",
      "Step: [1018] d_loss: 1.32597661, g_loss: 0.78732079\n",
      "Step: [1019] d_loss: 1.32953906, g_loss: 0.80910516\n",
      "Step: [1020] d_loss: 1.27574062, g_loss: 0.81913251\n",
      "Step: [1021] d_loss: 1.32277107, g_loss: 0.79560131\n",
      "Step: [1022] d_loss: 1.34531784, g_loss: 0.80946708\n",
      "Step: [1023] d_loss: 1.30525851, g_loss: 0.82921016\n",
      "Step: [1024] d_loss: 1.32821274, g_loss: 0.80278093\n",
      "Step: [1025] d_loss: 1.30966520, g_loss: 0.78237528\n",
      "Step: [1026] d_loss: 1.34589529, g_loss: 0.78418016\n",
      "Step: [1027] d_loss: 1.30528522, g_loss: 0.81324804\n",
      "Step: [1028] d_loss: 1.28184247, g_loss: 0.81137037\n",
      "Step: [1029] d_loss: 1.31753361, g_loss: 0.82430226\n",
      "Step: [1030] d_loss: 1.31051707, g_loss: 0.81429458\n",
      "Step: [1031] d_loss: 1.34967101, g_loss: 0.78223073\n",
      "Step: [1032] d_loss: 1.34525275, g_loss: 0.79649335\n",
      "Step: [1033] d_loss: 1.35910630, g_loss: 0.78876424\n",
      "Step: [1034] d_loss: 1.31459451, g_loss: 0.79412544\n",
      "Step: [1035] d_loss: 1.31310201, g_loss: 0.82190228\n",
      "Step: [1036] d_loss: 1.36426985, g_loss: 0.76781708\n",
      "Step: [1037] d_loss: 1.28391266, g_loss: 0.82425249\n",
      "Step: [1038] d_loss: 1.33129096, g_loss: 0.79748523\n",
      "Step: [1039] d_loss: 1.29780054, g_loss: 0.82865798\n",
      "Step: [1040] d_loss: 1.30345440, g_loss: 0.81685334\n",
      "Step: [1041] d_loss: 1.33878517, g_loss: 0.79059899\n",
      "Step: [1042] d_loss: 1.33458710, g_loss: 0.81392503\n",
      "Step: [1043] d_loss: 1.33945787, g_loss: 0.80597484\n",
      "Step: [1044] d_loss: 1.32694173, g_loss: 0.85243762\n",
      "Step: [1045] d_loss: 1.33790183, g_loss: 0.82834506\n",
      "Step: [1046] d_loss: 1.38434458, g_loss: 0.79554880\n",
      "Step: [1047] d_loss: 1.32872105, g_loss: 0.82708573\n",
      "Step: [1048] d_loss: 1.31813312, g_loss: 0.81994462\n",
      "Step: [1049] d_loss: 1.33900261, g_loss: 0.80314732\n",
      "Step: [1050] d_loss: 1.32482016, g_loss: 0.81653726\n",
      "Step: [1051] d_loss: 1.32174432, g_loss: 0.80613828\n",
      "Step: [1052] d_loss: 1.31581712, g_loss: 0.80520618\n",
      "Step: [1053] d_loss: 1.33802891, g_loss: 0.82361126\n",
      "Step: [1054] d_loss: 1.32717848, g_loss: 0.82782185\n",
      "Step: [1055] d_loss: 1.31630743, g_loss: 0.84192777\n",
      "Step: [1056] d_loss: 1.30294180, g_loss: 0.82402205\n",
      "Step: [1057] d_loss: 1.27290678, g_loss: 0.81570816\n",
      "Step: [1058] d_loss: 1.29931140, g_loss: 0.81340289\n",
      "Step: [1059] d_loss: 1.31743503, g_loss: 0.77634186\n",
      "Step: [1060] d_loss: 1.31279778, g_loss: 0.82253146\n",
      "Step: [1061] d_loss: 1.29415977, g_loss: 0.83114749\n",
      "Step: [1062] d_loss: 1.33560705, g_loss: 0.79436290\n",
      "Step: [1063] d_loss: 1.32960391, g_loss: 0.81856418\n",
      "Step: [1064] d_loss: 1.35242867, g_loss: 0.79591596\n",
      "Step: [1065] d_loss: 1.25520492, g_loss: 0.84022856\n",
      "Step: [1066] d_loss: 1.33525753, g_loss: 0.79778785\n",
      "Step: [1067] d_loss: 1.35529125, g_loss: 0.79866040\n",
      "Step: [1068] d_loss: 1.31163454, g_loss: 0.81636959\n",
      "Step: [1069] d_loss: 1.34662747, g_loss: 0.80804753\n",
      "Step: [1070] d_loss: 1.31370378, g_loss: 0.80711639\n",
      "Step: [1071] d_loss: 1.29784667, g_loss: 0.81713104\n",
      "Step: [1072] d_loss: 1.34988856, g_loss: 0.78389502\n",
      "Step: [1073] d_loss: 1.28138423, g_loss: 0.82675135\n",
      "Step: [1074] d_loss: 1.34670758, g_loss: 0.81578970\n",
      "Step: [1075] d_loss: 1.32483089, g_loss: 0.82414979\n",
      "Step: [1076] d_loss: 1.36491942, g_loss: 0.82588661\n",
      "Step: [1077] d_loss: 1.30974030, g_loss: 0.84800446\n",
      "Step: [1078] d_loss: 1.31611466, g_loss: 0.81542999\n",
      "Step: [1079] d_loss: 1.32312596, g_loss: 0.79032797\n",
      "Step: [1080] d_loss: 1.28844893, g_loss: 0.81703508\n",
      "Step: [1081] d_loss: 1.30342722, g_loss: 0.82606870\n",
      "Step: [1082] d_loss: 1.33649302, g_loss: 0.79717970\n",
      "Step: [1083] d_loss: 1.31547880, g_loss: 0.81279045\n",
      "Step: [1084] d_loss: 1.32118285, g_loss: 0.82224619\n",
      "Step: [1085] d_loss: 1.34726453, g_loss: 0.79590541\n",
      "Step: [1086] d_loss: 1.33066046, g_loss: 0.81472379\n",
      "Step: [1087] d_loss: 1.32228088, g_loss: 0.82967091\n",
      "Step: [1088] d_loss: 1.30275559, g_loss: 0.82085073\n",
      "Step: [1089] d_loss: 1.32844138, g_loss: 0.77957928\n",
      "Step: [1090] d_loss: 1.31974709, g_loss: 0.81412542\n",
      "Step: [1091] d_loss: 1.32738686, g_loss: 0.82063079\n",
      "Step: [1092] d_loss: 1.31814504, g_loss: 0.79913694\n",
      "Step: [1093] d_loss: 1.30349827, g_loss: 0.82352775\n",
      "Step: [1094] d_loss: 1.31476140, g_loss: 0.82567739\n",
      "Step: [1095] d_loss: 1.33478749, g_loss: 0.80243409\n",
      "Step: [1096] d_loss: 1.32161856, g_loss: 0.82929820\n",
      "Step: [1097] d_loss: 1.34435546, g_loss: 0.79124016\n",
      "Step: [1098] d_loss: 1.34050691, g_loss: 0.78700215\n",
      "Step: [1099] d_loss: 1.30256677, g_loss: 0.81770056\n",
      "Step: [1100] d_loss: 1.29087889, g_loss: 0.84202856\n",
      "Step: [1101] d_loss: 1.32143569, g_loss: 0.81830394\n",
      "Step: [1102] d_loss: 1.31732559, g_loss: 0.80263937\n",
      "Step: [1103] d_loss: 1.31468606, g_loss: 0.80688566\n",
      "Step: [1104] d_loss: 1.33467197, g_loss: 0.79977375\n",
      "Step: [1105] d_loss: 1.33428991, g_loss: 0.81155598\n",
      "Step: [1106] d_loss: 1.29573965, g_loss: 0.84219509\n",
      "Step: [1107] d_loss: 1.26760769, g_loss: 0.83572686\n",
      "Step: [1108] d_loss: 1.30253851, g_loss: 0.85073400\n",
      "Step: [1109] d_loss: 1.34899259, g_loss: 0.82113171\n",
      "Step: [1110] d_loss: 1.30654263, g_loss: 0.81899160\n",
      "Step: [1111] d_loss: 1.29630494, g_loss: 0.81803703\n",
      "Step: [1112] d_loss: 1.30725312, g_loss: 0.81539547\n",
      "Step: [1113] d_loss: 1.33532095, g_loss: 0.78630656\n",
      "Step: [1114] d_loss: 1.31687975, g_loss: 0.81373602\n",
      "Step: [1115] d_loss: 1.32236052, g_loss: 0.83801925\n",
      "Step: [1116] d_loss: 1.32359874, g_loss: 0.78508997\n",
      "Step: [1117] d_loss: 1.32163072, g_loss: 0.81986707\n",
      "Step: [1118] d_loss: 1.32157230, g_loss: 0.79342723\n",
      "Step: [1119] d_loss: 1.27808583, g_loss: 0.83573663\n",
      "Step: [1120] d_loss: 1.29714966, g_loss: 0.80165112\n",
      "Step: [1121] d_loss: 1.34372389, g_loss: 0.77896327\n",
      "Step: [1122] d_loss: 1.32587767, g_loss: 0.81395829\n",
      "Step: [1123] d_loss: 1.31300712, g_loss: 0.82319832\n",
      "Step: [1124] d_loss: 1.31260538, g_loss: 0.81163824\n",
      "Step: [1125] d_loss: 1.32006836, g_loss: 0.83794367\n",
      "Step: [1126] d_loss: 1.29594719, g_loss: 0.81735682\n",
      "Step: [1127] d_loss: 1.33347750, g_loss: 0.79846811\n",
      "Step: [1128] d_loss: 1.32408547, g_loss: 0.79300129\n",
      "Step: [1129] d_loss: 1.29259253, g_loss: 0.79603064\n",
      "Step: [1130] d_loss: 1.30931985, g_loss: 0.80855954\n",
      "Step: [1131] d_loss: 1.30057073, g_loss: 0.83527315\n",
      "Step: [1132] d_loss: 1.33232737, g_loss: 0.81791317\n",
      "Step: [1133] d_loss: 1.33039832, g_loss: 0.79790515\n",
      "Step: [1134] d_loss: 1.25941813, g_loss: 0.87134141\n",
      "Step: [1135] d_loss: 1.32492840, g_loss: 0.82612306\n",
      "Step: [1136] d_loss: 1.30170131, g_loss: 0.81917089\n",
      "Step: [1137] d_loss: 1.32727110, g_loss: 0.79603195\n",
      "Step: [1138] d_loss: 1.28258061, g_loss: 0.83888102\n",
      "Step: [1139] d_loss: 1.31408882, g_loss: 0.80970019\n",
      "Step: [1140] d_loss: 1.33348560, g_loss: 0.83759910\n",
      "Step: [1141] d_loss: 1.35003090, g_loss: 0.79544604\n",
      "Step: [1142] d_loss: 1.33398902, g_loss: 0.81751078\n",
      "Step: [1143] d_loss: 1.29888010, g_loss: 0.82834566\n",
      "Step: [1144] d_loss: 1.30937624, g_loss: 0.82473648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1145] d_loss: 1.31746531, g_loss: 0.80307907\n",
      "Step: [1146] d_loss: 1.30404258, g_loss: 0.78568530\n",
      "Step: [1147] d_loss: 1.34362280, g_loss: 0.78182352\n",
      "Step: [1148] d_loss: 1.32548857, g_loss: 0.80168128\n",
      "Step: [1149] d_loss: 1.30567050, g_loss: 0.82982117\n",
      "Step: [1150] d_loss: 1.35459399, g_loss: 0.81724119\n",
      "Step: [1151] d_loss: 1.33389866, g_loss: 0.80214000\n",
      "Step: [1152] d_loss: 1.29951143, g_loss: 0.86383969\n",
      "Step: [1153] d_loss: 1.33082438, g_loss: 0.81661928\n",
      "Step: [1154] d_loss: 1.30460060, g_loss: 0.83918321\n",
      "Step: [1155] d_loss: 1.34145033, g_loss: 0.83300042\n",
      "Step: [1156] d_loss: 1.37533426, g_loss: 0.78103393\n",
      "Step: [1157] d_loss: 1.33488226, g_loss: 0.80922961\n",
      "Step: [1158] d_loss: 1.32662535, g_loss: 0.82021838\n",
      "Step: [1159] d_loss: 1.30652785, g_loss: 0.81927824\n",
      "Step: [1160] d_loss: 1.32178855, g_loss: 0.82448643\n",
      "Step: [1161] d_loss: 1.28564787, g_loss: 0.84295541\n",
      "Step: [1162] d_loss: 1.31333184, g_loss: 0.82009685\n",
      "Step: [1163] d_loss: 1.30760527, g_loss: 0.83193648\n",
      "Step: [1164] d_loss: 1.28936028, g_loss: 0.81361020\n",
      "Step: [1165] d_loss: 1.32888150, g_loss: 0.79540610\n",
      "Step: [1166] d_loss: 1.33071578, g_loss: 0.82364124\n",
      "Step: [1167] d_loss: 1.31238437, g_loss: 0.82707709\n",
      "Step: [1168] d_loss: 1.32746220, g_loss: 0.81807649\n",
      "Step: [1169] d_loss: 1.33354652, g_loss: 0.81358898\n",
      "Step: [1170] d_loss: 1.29240775, g_loss: 0.83134782\n",
      "Step: [1171] d_loss: 1.29148817, g_loss: 0.84041047\n",
      "Step: [1172] d_loss: 1.30261111, g_loss: 0.79418409\n",
      "Step: [1173] d_loss: 1.30398560, g_loss: 0.82789820\n",
      "Step: [1174] d_loss: 1.31028831, g_loss: 0.80771554\n",
      "Step: [1175] d_loss: 1.29749513, g_loss: 0.83979845\n",
      "Step: [1176] d_loss: 1.33722866, g_loss: 0.81004483\n",
      "Step: [1177] d_loss: 1.34475946, g_loss: 0.81404173\n",
      "Step: [1178] d_loss: 1.32216871, g_loss: 0.80892861\n",
      "Step: [1179] d_loss: 1.31445146, g_loss: 0.82348293\n",
      "Step: [1180] d_loss: 1.31150615, g_loss: 0.80608833\n",
      "Step: [1181] d_loss: 1.31501305, g_loss: 0.81120157\n",
      "Step: [1182] d_loss: 1.29659128, g_loss: 0.82159090\n",
      "Step: [1183] d_loss: 1.33899581, g_loss: 0.80682784\n",
      "Step: [1184] d_loss: 1.31221104, g_loss: 0.83249795\n",
      "Step: [1185] d_loss: 1.33976376, g_loss: 0.82747877\n",
      "Step: [1186] d_loss: 1.33181322, g_loss: 0.81680733\n",
      "Step: [1187] d_loss: 1.32960927, g_loss: 0.80125564\n",
      "Step: [1188] d_loss: 1.36602044, g_loss: 0.79549485\n",
      "Step: [1189] d_loss: 1.32245779, g_loss: 0.82458913\n",
      "Step: [1190] d_loss: 1.30497694, g_loss: 0.84116197\n",
      "Step: [1191] d_loss: 1.28681886, g_loss: 0.82829744\n",
      "Step: [1192] d_loss: 1.30819368, g_loss: 0.81487012\n",
      "Step: [1193] d_loss: 1.32813370, g_loss: 0.80199510\n",
      "Step: [1194] d_loss: 1.29923594, g_loss: 0.82565975\n",
      "Step: [1195] d_loss: 1.32358801, g_loss: 0.81604749\n",
      "Step: [1196] d_loss: 1.31422043, g_loss: 0.81345010\n",
      "Step: [1197] d_loss: 1.32546329, g_loss: 0.80431437\n",
      "Step: [1198] d_loss: 1.33146691, g_loss: 0.81012028\n",
      "Step: [1199] d_loss: 1.35189545, g_loss: 0.80793709\n",
      "Step: [1200] d_loss: 1.38035274, g_loss: 0.79879737\n",
      "Step: [1201] d_loss: 1.29851770, g_loss: 0.83473557\n",
      "Step: [1202] d_loss: 1.32697821, g_loss: 0.79856086\n",
      "Step: [1203] d_loss: 1.32246804, g_loss: 0.82370609\n",
      "Step: [1204] d_loss: 1.31388402, g_loss: 0.81599015\n",
      "Step: [1205] d_loss: 1.34143102, g_loss: 0.79475504\n",
      "Step: [1206] d_loss: 1.27383316, g_loss: 0.84491289\n",
      "Step: [1207] d_loss: 1.32566476, g_loss: 0.81657594\n",
      "Step: [1208] d_loss: 1.32204998, g_loss: 0.81476730\n",
      "Step: [1209] d_loss: 1.31980586, g_loss: 0.80883563\n",
      "Step: [1210] d_loss: 1.31869197, g_loss: 0.80604017\n",
      "Step: [1211] d_loss: 1.31332016, g_loss: 0.82185185\n",
      "Step: [1212] d_loss: 1.31659663, g_loss: 0.81330836\n",
      "Step: [1213] d_loss: 1.31134021, g_loss: 0.82638240\n",
      "Step: [1214] d_loss: 1.29218388, g_loss: 0.87215865\n",
      "Step: [1215] d_loss: 1.30914593, g_loss: 0.81795359\n",
      "Step: [1216] d_loss: 1.33536816, g_loss: 0.80013090\n",
      "Step: [1217] d_loss: 1.31772900, g_loss: 0.79823792\n",
      "Step: [1218] d_loss: 1.36062086, g_loss: 0.78233415\n",
      "Step: [1219] d_loss: 1.28596115, g_loss: 0.81416267\n",
      "Step: [1220] d_loss: 1.31699228, g_loss: 0.82458347\n",
      "Step: [1221] d_loss: 1.31501675, g_loss: 0.83138883\n",
      "Step: [1222] d_loss: 1.29551578, g_loss: 0.84406525\n",
      "Step: [1223] d_loss: 1.30902910, g_loss: 0.84440583\n",
      "Step: [1224] d_loss: 1.33566248, g_loss: 0.82505620\n",
      "Step: [1225] d_loss: 1.33294523, g_loss: 0.79165959\n",
      "Step: [1226] d_loss: 1.32945609, g_loss: 0.80933189\n",
      "Step: [1227] d_loss: 1.33520985, g_loss: 0.80111164\n",
      "Step: [1228] d_loss: 1.32732105, g_loss: 0.79528713\n",
      "Step: [1229] d_loss: 1.31078315, g_loss: 0.82082683\n",
      "Step: [1230] d_loss: 1.32122016, g_loss: 0.81762648\n",
      "Step: [1231] d_loss: 1.30849016, g_loss: 0.82025087\n",
      "Step: [1232] d_loss: 1.28836584, g_loss: 0.81559205\n",
      "Step: [1233] d_loss: 1.29991817, g_loss: 0.84280396\n",
      "Step: [1234] d_loss: 1.34409094, g_loss: 0.78360903\n",
      "Step: [1235] d_loss: 1.32703352, g_loss: 0.82629782\n",
      "Step: [1236] d_loss: 1.34008026, g_loss: 0.81212556\n",
      "Step: [1237] d_loss: 1.33838606, g_loss: 0.81687939\n",
      "Step: [1238] d_loss: 1.34773624, g_loss: 0.79132891\n",
      "Step: [1239] d_loss: 1.30888116, g_loss: 0.82969248\n",
      "Step: [1240] d_loss: 1.30269504, g_loss: 0.81988800\n",
      "Step: [1241] d_loss: 1.33968556, g_loss: 0.79485321\n",
      "Step: [1242] d_loss: 1.30384409, g_loss: 0.83321995\n",
      "Step: [1243] d_loss: 1.30308771, g_loss: 0.81568849\n",
      "Step: [1244] d_loss: 1.27635550, g_loss: 0.82426691\n",
      "Step: [1245] d_loss: 1.32860184, g_loss: 0.80277359\n",
      "Step: [1246] d_loss: 1.33464396, g_loss: 0.81073749\n",
      "Step: [1247] d_loss: 1.29956913, g_loss: 0.82219493\n",
      "Step: [1248] d_loss: 1.33086407, g_loss: 0.81474042\n",
      "Step: [1249] d_loss: 1.30120349, g_loss: 0.81043428\n",
      "Step: [1250] d_loss: 1.33350563, g_loss: 0.80378050\n",
      "Step: [1251] d_loss: 1.29478586, g_loss: 0.82741803\n",
      "Step: [1252] d_loss: 1.27254748, g_loss: 0.84039181\n",
      "Step: [1253] d_loss: 1.33280015, g_loss: 0.81738663\n",
      "Step: [1254] d_loss: 1.33299375, g_loss: 0.84244406\n",
      "Step: [1255] d_loss: 1.32290006, g_loss: 0.80325866\n",
      "Step: [1256] d_loss: 1.31451714, g_loss: 0.82216746\n",
      "Step: [1257] d_loss: 1.32864892, g_loss: 0.82545799\n",
      "Step: [1258] d_loss: 1.35301065, g_loss: 0.80020189\n",
      "Step: [1259] d_loss: 1.34637833, g_loss: 0.83722633\n",
      "Step: [1260] d_loss: 1.31677270, g_loss: 0.84226793\n",
      "Step: [1261] d_loss: 1.32422686, g_loss: 0.82606936\n",
      "Step: [1262] d_loss: 1.35199475, g_loss: 0.81120515\n",
      "Step: [1263] d_loss: 1.34905124, g_loss: 0.80626005\n",
      "Step: [1264] d_loss: 1.30690455, g_loss: 0.82806015\n",
      "Step: [1265] d_loss: 1.29733801, g_loss: 0.82984126\n",
      "Step: [1266] d_loss: 1.27788997, g_loss: 0.83020961\n",
      "Step: [1267] d_loss: 1.35120082, g_loss: 0.78557140\n",
      "Step: [1268] d_loss: 1.31950033, g_loss: 0.81855470\n",
      "Step: [1269] d_loss: 1.30682182, g_loss: 0.83146995\n",
      "Step: [1270] d_loss: 1.27697361, g_loss: 0.84393656\n",
      "Step: [1271] d_loss: 1.35175276, g_loss: 0.82094556\n",
      "Step: [1272] d_loss: 1.33625841, g_loss: 0.83236331\n",
      "Step: [1273] d_loss: 1.33621180, g_loss: 0.79616147\n",
      "Step: [1274] d_loss: 1.29643238, g_loss: 0.82679009\n",
      "Step: [1275] d_loss: 1.32108259, g_loss: 0.79603159\n",
      "Step: [1276] d_loss: 1.31307936, g_loss: 0.81858915\n",
      "Step: [1277] d_loss: 1.35915840, g_loss: 0.77969277\n",
      "Step: [1278] d_loss: 1.30410123, g_loss: 0.83113503\n",
      "Step: [1279] d_loss: 1.32053864, g_loss: 0.82992077\n",
      "Step: [1280] d_loss: 1.32048345, g_loss: 0.82829350\n",
      "Step: [1281] d_loss: 1.35336554, g_loss: 0.82398331\n",
      "Step: [1282] d_loss: 1.36540258, g_loss: 0.80678773\n",
      "Step: [1283] d_loss: 1.31049979, g_loss: 0.81878418\n",
      "Step: [1284] d_loss: 1.32870615, g_loss: 0.78700411\n",
      "Step: [1285] d_loss: 1.31095648, g_loss: 0.81575108\n",
      "Step: [1286] d_loss: 1.29959953, g_loss: 0.82370120\n",
      "Step: [1287] d_loss: 1.28544128, g_loss: 0.81204146\n",
      "Step: [1288] d_loss: 1.33919120, g_loss: 0.78838217\n",
      "Step: [1289] d_loss: 1.36469412, g_loss: 0.79301983\n",
      "Step: [1290] d_loss: 1.31401062, g_loss: 0.81407809\n",
      "Step: [1291] d_loss: 1.35262632, g_loss: 0.80520606\n",
      "Step: [1292] d_loss: 1.35860157, g_loss: 0.80361950\n",
      "Step: [1293] d_loss: 1.31318688, g_loss: 0.81949168\n",
      "Step: [1294] d_loss: 1.30239820, g_loss: 0.83199179\n",
      "Step: [1295] d_loss: 1.31595397, g_loss: 0.82822889\n",
      "Step: [1296] d_loss: 1.32913017, g_loss: 0.82405639\n",
      "Step: [1297] d_loss: 1.31511354, g_loss: 0.81542206\n",
      "Step: [1298] d_loss: 1.32208943, g_loss: 0.80837673\n",
      "Step: [1299] d_loss: 1.31373668, g_loss: 0.81529379\n",
      "Step: [1300] d_loss: 1.33342993, g_loss: 0.80903530\n",
      "Step: [1301] d_loss: 1.33288419, g_loss: 0.80718350\n",
      "Step: [1302] d_loss: 1.31299353, g_loss: 0.79743230\n",
      "Step: [1303] d_loss: 1.34719992, g_loss: 0.81057572\n",
      "Step: [1304] d_loss: 1.31346846, g_loss: 0.82533091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1305] d_loss: 1.33077896, g_loss: 0.80569065\n",
      "Step: [1306] d_loss: 1.31988513, g_loss: 0.80607271\n",
      "Step: [1307] d_loss: 1.31357074, g_loss: 0.80975848\n",
      "Step: [1308] d_loss: 1.34657049, g_loss: 0.78716993\n",
      "Step: [1309] d_loss: 1.31489468, g_loss: 0.81526792\n",
      "Step: [1310] d_loss: 1.33237886, g_loss: 0.81808537\n",
      "Step: [1311] d_loss: 1.29578471, g_loss: 0.83490288\n",
      "Step: [1312] d_loss: 1.30077183, g_loss: 0.82418448\n",
      "Step: [1313] d_loss: 1.29216182, g_loss: 0.82546246\n",
      "Step: [1314] d_loss: 1.32005501, g_loss: 0.79864764\n",
      "Step: [1315] d_loss: 1.31661582, g_loss: 0.81455660\n",
      "Step: [1316] d_loss: 1.33523035, g_loss: 0.81197917\n",
      "Step: [1317] d_loss: 1.33349156, g_loss: 0.79860198\n",
      "Step: [1318] d_loss: 1.29982078, g_loss: 0.82271492\n",
      "Step: [1319] d_loss: 1.32520139, g_loss: 0.79895329\n",
      "Step: [1320] d_loss: 1.32284093, g_loss: 0.81765485\n",
      "Step: [1321] d_loss: 1.34945035, g_loss: 0.78436536\n",
      "Step: [1322] d_loss: 1.31100011, g_loss: 0.81690067\n",
      "Step: [1323] d_loss: 1.28782976, g_loss: 0.82992506\n",
      "Step: [1324] d_loss: 1.28617883, g_loss: 0.83350080\n",
      "Step: [1325] d_loss: 1.32868505, g_loss: 0.81717700\n",
      "Step: [1326] d_loss: 1.34975803, g_loss: 0.77812839\n",
      "Step: [1327] d_loss: 1.29554069, g_loss: 0.83510554\n",
      "Step: [1328] d_loss: 1.33620834, g_loss: 0.81080818\n",
      "Step: [1329] d_loss: 1.29114366, g_loss: 0.83795732\n",
      "Step: [1330] d_loss: 1.30140567, g_loss: 0.83343542\n",
      "Step: [1331] d_loss: 1.32574964, g_loss: 0.81835723\n",
      "Step: [1332] d_loss: 1.31444216, g_loss: 0.82019961\n",
      "Step: [1333] d_loss: 1.26943350, g_loss: 0.84788144\n",
      "Step: [1334] d_loss: 1.27678657, g_loss: 0.81659067\n",
      "Step: [1335] d_loss: 1.30554187, g_loss: 0.83569831\n",
      "Step: [1336] d_loss: 1.31250775, g_loss: 0.79558969\n",
      "Step: [1337] d_loss: 1.31512213, g_loss: 0.81776488\n",
      "Step: [1338] d_loss: 1.29898989, g_loss: 0.84083068\n",
      "Step: [1339] d_loss: 1.33776653, g_loss: 0.80754685\n",
      "Step: [1340] d_loss: 1.28311181, g_loss: 0.83245158\n",
      "Step: [1341] d_loss: 1.32973957, g_loss: 0.80314076\n",
      "Step: [1342] d_loss: 1.31830478, g_loss: 0.80747831\n",
      "Step: [1343] d_loss: 1.30566609, g_loss: 0.84281933\n",
      "Step: [1344] d_loss: 1.30673552, g_loss: 0.83139002\n",
      "Step: [1345] d_loss: 1.31545401, g_loss: 0.82415462\n",
      "Step: [1346] d_loss: 1.32472348, g_loss: 0.82157695\n",
      "Step: [1347] d_loss: 1.31154418, g_loss: 0.79914868\n",
      "Step: [1348] d_loss: 1.31127501, g_loss: 0.81740546\n",
      "Step: [1349] d_loss: 1.36243939, g_loss: 0.78961146\n",
      "Step: [1350] d_loss: 1.31502593, g_loss: 0.81455117\n",
      "Step: [1351] d_loss: 1.28557575, g_loss: 0.83808666\n",
      "Step: [1352] d_loss: 1.26522994, g_loss: 0.83480775\n",
      "Step: [1353] d_loss: 1.31316447, g_loss: 0.84069765\n",
      "Step: [1354] d_loss: 1.31592798, g_loss: 0.86215264\n",
      "Step: [1355] d_loss: 1.28270161, g_loss: 0.83869970\n",
      "Step: [1356] d_loss: 1.34062171, g_loss: 0.79171771\n",
      "Step: [1357] d_loss: 1.31423020, g_loss: 0.80798507\n",
      "Step: [1358] d_loss: 1.29681861, g_loss: 0.82086051\n",
      "Step: [1359] d_loss: 1.32781100, g_loss: 0.81333578\n",
      "Step: [1360] d_loss: 1.32882178, g_loss: 0.82182783\n",
      "Step: [1361] d_loss: 1.33761752, g_loss: 0.81094927\n",
      "Step: [1362] d_loss: 1.31845522, g_loss: 0.81747323\n",
      "Step: [1363] d_loss: 1.31021488, g_loss: 0.81946230\n",
      "Step: [1364] d_loss: 1.31767201, g_loss: 0.80539745\n",
      "Step: [1365] d_loss: 1.33027029, g_loss: 0.81040478\n",
      "Step: [1366] d_loss: 1.30854118, g_loss: 0.82309580\n",
      "Step: [1367] d_loss: 1.34956813, g_loss: 0.80532438\n",
      "Step: [1368] d_loss: 1.32537532, g_loss: 0.79198825\n",
      "Step: [1369] d_loss: 1.32379472, g_loss: 0.79236984\n",
      "Step: [1370] d_loss: 1.34875607, g_loss: 0.81916398\n",
      "Step: [1371] d_loss: 1.34934735, g_loss: 0.78985089\n",
      "Step: [1372] d_loss: 1.27523744, g_loss: 0.83691049\n",
      "Step: [1373] d_loss: 1.30309713, g_loss: 0.83056509\n",
      "Step: [1374] d_loss: 1.28411603, g_loss: 0.82645702\n",
      "Step: [1375] d_loss: 1.26723576, g_loss: 0.82613695\n",
      "Step: [1376] d_loss: 1.30195665, g_loss: 0.83015400\n",
      "Step: [1377] d_loss: 1.33908403, g_loss: 0.79514289\n",
      "Step: [1378] d_loss: 1.31634259, g_loss: 0.81375730\n",
      "Step: [1379] d_loss: 1.30452120, g_loss: 0.83592129\n",
      "Step: [1380] d_loss: 1.30269790, g_loss: 0.82094872\n",
      "Step: [1381] d_loss: 1.31684339, g_loss: 0.83103430\n",
      "Step: [1382] d_loss: 1.28538609, g_loss: 0.84308016\n",
      "Step: [1383] d_loss: 1.25859332, g_loss: 0.84996170\n",
      "Step: [1384] d_loss: 1.32184184, g_loss: 0.81339586\n",
      "Step: [1385] d_loss: 1.32300282, g_loss: 0.83391315\n",
      "Step: [1386] d_loss: 1.34003007, g_loss: 0.80379260\n",
      "Step: [1387] d_loss: 1.31883121, g_loss: 0.79697049\n",
      "Step: [1388] d_loss: 1.33585966, g_loss: 0.80029309\n",
      "Step: [1389] d_loss: 1.31483865, g_loss: 0.82582021\n",
      "Step: [1390] d_loss: 1.31037080, g_loss: 0.81282133\n",
      "Step: [1391] d_loss: 1.28389454, g_loss: 0.84441888\n",
      "Step: [1392] d_loss: 1.29955196, g_loss: 0.82446218\n",
      "Step: [1393] d_loss: 1.30550468, g_loss: 0.82593185\n",
      "Step: [1394] d_loss: 1.29753530, g_loss: 0.82377017\n",
      "Step: [1395] d_loss: 1.30169427, g_loss: 0.83112061\n",
      "Step: [1396] d_loss: 1.34324956, g_loss: 0.79159725\n",
      "Step: [1397] d_loss: 1.30876255, g_loss: 0.81414151\n",
      "Step: [1398] d_loss: 1.33643413, g_loss: 0.83596909\n",
      "Step: [1399] d_loss: 1.36892724, g_loss: 0.81312066\n",
      "Step: [1400] d_loss: 1.31565809, g_loss: 0.82900345\n",
      "Step: [1401] d_loss: 1.31052887, g_loss: 0.82205647\n",
      "Step: [1402] d_loss: 1.32788396, g_loss: 0.82434225\n",
      "Step: [1403] d_loss: 1.35991955, g_loss: 0.80307460\n",
      "Step: [1404] d_loss: 1.35131311, g_loss: 0.79381019\n",
      "Step: [1405] d_loss: 1.28847706, g_loss: 0.82303238\n",
      "Step: [1406] d_loss: 1.30643296, g_loss: 0.85475969\n",
      "Step: [1407] d_loss: 1.32679582, g_loss: 0.80311286\n",
      "Step: [1408] d_loss: 1.36578584, g_loss: 0.80091107\n",
      "Step: [1409] d_loss: 1.33486521, g_loss: 0.81273699\n",
      "Step: [1410] d_loss: 1.32594132, g_loss: 0.79641914\n",
      "Step: [1411] d_loss: 1.34707010, g_loss: 0.83062398\n",
      "Step: [1412] d_loss: 1.27793765, g_loss: 0.84459424\n",
      "Step: [1413] d_loss: 1.30536461, g_loss: 0.81493378\n",
      "Step: [1414] d_loss: 1.32439971, g_loss: 0.80441552\n",
      "Step: [1415] d_loss: 1.30388212, g_loss: 0.83060575\n",
      "Step: [1416] d_loss: 1.34010160, g_loss: 0.79618025\n",
      "Step: [1417] d_loss: 1.33213699, g_loss: 0.80502450\n",
      "Step: [1418] d_loss: 1.31626725, g_loss: 0.80439180\n",
      "Step: [1419] d_loss: 1.32979035, g_loss: 0.79273629\n",
      "Step: [1420] d_loss: 1.32074118, g_loss: 0.80290174\n",
      "Step: [1421] d_loss: 1.29263473, g_loss: 0.85654700\n",
      "Step: [1422] d_loss: 1.34936571, g_loss: 0.80822337\n",
      "Step: [1423] d_loss: 1.32762849, g_loss: 0.80230844\n",
      "Step: [1424] d_loss: 1.30040550, g_loss: 0.81757396\n",
      "Step: [1425] d_loss: 1.32664013, g_loss: 0.83898121\n",
      "Step: [1426] d_loss: 1.28237367, g_loss: 0.81999189\n",
      "Step: [1427] d_loss: 1.32286763, g_loss: 0.80644166\n",
      "Step: [1428] d_loss: 1.33704710, g_loss: 0.79576838\n",
      "Step: [1429] d_loss: 1.32023466, g_loss: 0.79760087\n",
      "Step: [1430] d_loss: 1.30947304, g_loss: 0.81189418\n",
      "Step: [1431] d_loss: 1.33421910, g_loss: 0.80551004\n",
      "Step: [1432] d_loss: 1.32583165, g_loss: 0.83337593\n",
      "Step: [1433] d_loss: 1.34531760, g_loss: 0.80002916\n",
      "Step: [1434] d_loss: 1.33895826, g_loss: 0.80695069\n",
      "Step: [1435] d_loss: 1.32445025, g_loss: 0.81184453\n",
      "Step: [1436] d_loss: 1.31197155, g_loss: 0.80701190\n",
      "Step: [1437] d_loss: 1.34259701, g_loss: 0.79314512\n",
      "Step: [1438] d_loss: 1.35034871, g_loss: 0.80754924\n",
      "Step: [1439] d_loss: 1.30854595, g_loss: 0.83486867\n",
      "Step: [1440] d_loss: 1.33498240, g_loss: 0.81358582\n",
      "Step: [1441] d_loss: 1.32570362, g_loss: 0.80132151\n",
      "Step: [1442] d_loss: 1.31012249, g_loss: 0.81026506\n",
      "Step: [1443] d_loss: 1.30919254, g_loss: 0.80330002\n",
      "Step: [1444] d_loss: 1.33186007, g_loss: 0.81097239\n",
      "Step: [1445] d_loss: 1.29187489, g_loss: 0.83138853\n",
      "Step: [1446] d_loss: 1.29488444, g_loss: 0.83259594\n",
      "Step: [1447] d_loss: 1.33879495, g_loss: 0.80773598\n",
      "Step: [1448] d_loss: 1.33151126, g_loss: 0.81666446\n",
      "Step: [1449] d_loss: 1.30088532, g_loss: 0.81539893\n",
      "Step: [1450] d_loss: 1.34318686, g_loss: 0.79937863\n",
      "Step: [1451] d_loss: 1.33057010, g_loss: 0.80151391\n",
      "Step: [1452] d_loss: 1.28004622, g_loss: 0.83655620\n",
      "Step: [1453] d_loss: 1.33530283, g_loss: 0.81070316\n",
      "Step: [1454] d_loss: 1.32608676, g_loss: 0.81192386\n",
      "Step: [1455] d_loss: 1.32450068, g_loss: 0.81994265\n",
      "Step: [1456] d_loss: 1.32232976, g_loss: 0.79843986\n",
      "Step: [1457] d_loss: 1.31720757, g_loss: 0.80022573\n",
      "Step: [1458] d_loss: 1.32703209, g_loss: 0.80267119\n",
      "Step: [1459] d_loss: 1.29188704, g_loss: 0.84302175\n",
      "Step: [1460] d_loss: 1.31592894, g_loss: 0.81428397\n",
      "Step: [1461] d_loss: 1.32332170, g_loss: 0.84084314\n",
      "Step: [1462] d_loss: 1.33137870, g_loss: 0.80715334\n",
      "Step: [1463] d_loss: 1.32308531, g_loss: 0.82722747\n",
      "Step: [1464] d_loss: 1.32645118, g_loss: 0.81317735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1465] d_loss: 1.33238912, g_loss: 0.81379175\n",
      "Step: [1466] d_loss: 1.28761864, g_loss: 0.82564366\n",
      "Step: [1467] d_loss: 1.35000956, g_loss: 0.78651702\n",
      "Step: [1468] d_loss: 1.29375613, g_loss: 0.79866302\n",
      "Step: [1469] d_loss: 1.29084885, g_loss: 0.81959766\n",
      "Step: [1470] d_loss: 1.31030178, g_loss: 0.80134284\n",
      "Step: [1471] d_loss: 1.35569954, g_loss: 0.79349530\n",
      "Step: [1472] d_loss: 1.32390404, g_loss: 0.82861227\n",
      "Step: [1473] d_loss: 1.30950642, g_loss: 0.82686269\n",
      "Step: [1474] d_loss: 1.30716813, g_loss: 0.80627596\n",
      "Step: [1475] d_loss: 1.29788589, g_loss: 0.81913877\n",
      "Step: [1476] d_loss: 1.30767024, g_loss: 0.80783033\n",
      "Step: [1477] d_loss: 1.29179227, g_loss: 0.82607859\n",
      "Step: [1478] d_loss: 1.24094129, g_loss: 0.86618769\n",
      "Step: [1479] d_loss: 1.29343724, g_loss: 0.83159041\n",
      "Step: [1480] d_loss: 1.26462412, g_loss: 0.82211721\n",
      "Step: [1481] d_loss: 1.31635690, g_loss: 0.81472576\n",
      "Step: [1482] d_loss: 1.32202268, g_loss: 0.80046844\n",
      "Step: [1483] d_loss: 1.28242922, g_loss: 0.84393305\n",
      "Step: [1484] d_loss: 1.29882181, g_loss: 0.83750892\n",
      "Step: [1485] d_loss: 1.28994536, g_loss: 0.82476318\n",
      "Step: [1486] d_loss: 1.34357023, g_loss: 0.80226326\n",
      "Step: [1487] d_loss: 1.33868599, g_loss: 0.80213076\n",
      "Step: [1488] d_loss: 1.29447699, g_loss: 0.83863646\n",
      "Step: [1489] d_loss: 1.31519222, g_loss: 0.82146871\n",
      "Step: [1490] d_loss: 1.28376698, g_loss: 0.83953214\n",
      "Step: [1491] d_loss: 1.31605673, g_loss: 0.82644159\n",
      "Step: [1492] d_loss: 1.29089522, g_loss: 0.81342322\n",
      "Step: [1493] d_loss: 1.28731108, g_loss: 0.81073666\n",
      "Step: [1494] d_loss: 1.29875267, g_loss: 0.83003974\n",
      "Step: [1495] d_loss: 1.31030822, g_loss: 0.82063627\n",
      "Step: [1496] d_loss: 1.33775520, g_loss: 0.81706047\n",
      "Step: [1497] d_loss: 1.30745673, g_loss: 0.81241798\n",
      "Step: [1498] d_loss: 1.33265495, g_loss: 0.78821528\n",
      "Step: [1499] d_loss: 1.34112358, g_loss: 0.79938078\n",
      "Step: [1500] d_loss: 1.31117821, g_loss: 0.81683159\n",
      "Step: [1501] d_loss: 1.35073793, g_loss: 0.79756409\n",
      "Step: [1502] d_loss: 1.29941940, g_loss: 0.83069521\n",
      "Step: [1503] d_loss: 1.35805345, g_loss: 0.80960852\n",
      "Step: [1504] d_loss: 1.31508398, g_loss: 0.81258631\n",
      "Step: [1505] d_loss: 1.32608724, g_loss: 0.83876461\n",
      "Step: [1506] d_loss: 1.35386157, g_loss: 0.79378265\n",
      "Step: [1507] d_loss: 1.29377997, g_loss: 0.82485902\n",
      "Step: [1508] d_loss: 1.31086659, g_loss: 0.80904108\n",
      "Step: [1509] d_loss: 1.29037571, g_loss: 0.81567961\n",
      "Step: [1510] d_loss: 1.33226943, g_loss: 0.79541534\n",
      "Step: [1511] d_loss: 1.30562031, g_loss: 0.81638962\n",
      "Step: [1512] d_loss: 1.30117822, g_loss: 0.82661867\n",
      "Step: [1513] d_loss: 1.29375279, g_loss: 0.83206916\n",
      "Step: [1514] d_loss: 1.32540417, g_loss: 0.82913589\n",
      "Step: [1515] d_loss: 1.32163632, g_loss: 0.82558495\n",
      "Step: [1516] d_loss: 1.35538673, g_loss: 0.80097926\n",
      "Step: [1517] d_loss: 1.35836852, g_loss: 0.82131159\n",
      "Step: [1518] d_loss: 1.38023973, g_loss: 0.80278993\n",
      "Step: [1519] d_loss: 1.34894013, g_loss: 0.78221357\n",
      "Step: [1520] d_loss: 1.32124496, g_loss: 0.81630933\n",
      "Step: [1521] d_loss: 1.28877044, g_loss: 0.82116199\n",
      "Step: [1522] d_loss: 1.34505367, g_loss: 0.80082405\n",
      "Step: [1523] d_loss: 1.30709124, g_loss: 0.82202542\n",
      "Step: [1524] d_loss: 1.28988898, g_loss: 0.82129848\n",
      "Step: [1525] d_loss: 1.30634594, g_loss: 0.81491292\n",
      "Step: [1526] d_loss: 1.34007740, g_loss: 0.80478740\n",
      "Step: [1527] d_loss: 1.30868959, g_loss: 0.81561530\n",
      "Step: [1528] d_loss: 1.32759476, g_loss: 0.82110101\n",
      "Step: [1529] d_loss: 1.32845640, g_loss: 0.83409315\n",
      "Step: [1530] d_loss: 1.32773459, g_loss: 0.83268112\n",
      "Step: [1531] d_loss: 1.34204626, g_loss: 0.80429667\n",
      "Step: [1532] d_loss: 1.35401750, g_loss: 0.80294096\n",
      "Step: [1533] d_loss: 1.29580283, g_loss: 0.83391869\n",
      "Step: [1534] d_loss: 1.31783891, g_loss: 0.83682370\n",
      "Step: [1535] d_loss: 1.32823110, g_loss: 0.80787557\n",
      "Step: [1536] d_loss: 1.31181896, g_loss: 0.85669166\n",
      "Step: [1537] d_loss: 1.32622790, g_loss: 0.82446510\n",
      "Step: [1538] d_loss: 1.28832054, g_loss: 0.84157711\n",
      "Step: [1539] d_loss: 1.31678748, g_loss: 0.83118045\n",
      "Step: [1540] d_loss: 1.33378506, g_loss: 0.81007665\n",
      "Step: [1541] d_loss: 1.35578716, g_loss: 0.78870368\n",
      "Step: [1542] d_loss: 1.30663395, g_loss: 0.82180870\n",
      "Step: [1543] d_loss: 1.30896115, g_loss: 0.85758662\n",
      "Step: [1544] d_loss: 1.32313299, g_loss: 0.81440884\n",
      "Step: [1545] d_loss: 1.33848608, g_loss: 0.80865926\n",
      "Step: [1546] d_loss: 1.28441024, g_loss: 0.82933813\n",
      "Step: [1547] d_loss: 1.30027497, g_loss: 0.84304786\n",
      "Step: [1548] d_loss: 1.34285140, g_loss: 0.80777776\n",
      "Step: [1549] d_loss: 1.32765079, g_loss: 0.82022953\n",
      "Step: [1550] d_loss: 1.35505867, g_loss: 0.81295872\n",
      "Step: [1551] d_loss: 1.29085076, g_loss: 0.81013811\n",
      "Step: [1552] d_loss: 1.30716753, g_loss: 0.81802702\n",
      "Step: [1553] d_loss: 1.29548335, g_loss: 0.82123768\n",
      "Step: [1554] d_loss: 1.31379151, g_loss: 0.82393420\n",
      "Step: [1555] d_loss: 1.31440806, g_loss: 0.81290293\n",
      "Step: [1556] d_loss: 1.31799793, g_loss: 0.80997205\n",
      "Step: [1557] d_loss: 1.29707325, g_loss: 0.82629520\n",
      "Step: [1558] d_loss: 1.30923724, g_loss: 0.80544281\n",
      "Step: [1559] d_loss: 1.29915190, g_loss: 0.82686341\n",
      "Step: [1560] d_loss: 1.31008005, g_loss: 0.80387014\n",
      "Step: [1561] d_loss: 1.29765177, g_loss: 0.81935924\n",
      "Step: [1562] d_loss: 1.31688499, g_loss: 0.81390870\n",
      "Step: [1563] d_loss: 1.29559040, g_loss: 0.83128333\n",
      "Step: [1564] d_loss: 1.33891630, g_loss: 0.78787887\n",
      "Step: [1565] d_loss: 1.29006541, g_loss: 0.82359320\n",
      "Step: [1566] d_loss: 1.32189095, g_loss: 0.82156646\n",
      "Step: [1567] d_loss: 1.32012761, g_loss: 0.84040940\n",
      "Step: [1568] d_loss: 1.30414891, g_loss: 0.83136117\n",
      "Step: [1569] d_loss: 1.28069711, g_loss: 0.83168006\n",
      "Step: [1570] d_loss: 1.28355300, g_loss: 0.83435547\n",
      "Step: [1571] d_loss: 1.29073918, g_loss: 0.83358610\n",
      "Step: [1572] d_loss: 1.28274226, g_loss: 0.81616700\n",
      "Step: [1573] d_loss: 1.32125986, g_loss: 0.80679536\n",
      "Step: [1574] d_loss: 1.30430830, g_loss: 0.81832266\n",
      "Step: [1575] d_loss: 1.30241227, g_loss: 0.81794858\n",
      "Step: [1576] d_loss: 1.31487429, g_loss: 0.81096816\n",
      "Step: [1577] d_loss: 1.35936213, g_loss: 0.81349272\n",
      "Step: [1578] d_loss: 1.30593395, g_loss: 0.83117408\n",
      "Step: [1579] d_loss: 1.33694339, g_loss: 0.81661475\n",
      "Step: [1580] d_loss: 1.33100176, g_loss: 0.80557466\n",
      "Step: [1581] d_loss: 1.31091082, g_loss: 0.81320566\n",
      "Step: [1582] d_loss: 1.30984211, g_loss: 0.82133669\n",
      "Step: [1583] d_loss: 1.31234157, g_loss: 0.84287727\n",
      "Step: [1584] d_loss: 1.32465851, g_loss: 0.81674421\n",
      "Step: [1585] d_loss: 1.30301833, g_loss: 0.81491679\n",
      "Step: [1586] d_loss: 1.35467696, g_loss: 0.80479360\n",
      "Step: [1587] d_loss: 1.30259442, g_loss: 0.81450713\n",
      "Step: [1588] d_loss: 1.33774543, g_loss: 0.82675636\n",
      "Step: [1589] d_loss: 1.33765841, g_loss: 0.78836191\n",
      "Step: [1590] d_loss: 1.30775762, g_loss: 0.81302243\n",
      "Step: [1591] d_loss: 1.28751349, g_loss: 0.84232008\n",
      "Step: [1592] d_loss: 1.31158781, g_loss: 0.82944471\n",
      "Step: [1593] d_loss: 1.33542216, g_loss: 0.81146002\n",
      "Step: [1594] d_loss: 1.34056783, g_loss: 0.80253959\n",
      "Step: [1595] d_loss: 1.33494151, g_loss: 0.81802845\n",
      "Step: [1596] d_loss: 1.36464226, g_loss: 0.79164034\n",
      "Step: [1597] d_loss: 1.29924726, g_loss: 0.82187641\n",
      "Step: [1598] d_loss: 1.33619642, g_loss: 0.80089891\n",
      "Step: [1599] d_loss: 1.34633100, g_loss: 0.81240427\n",
      "Step: [1600] d_loss: 1.27529788, g_loss: 0.85276330\n",
      "Step: [1601] d_loss: 1.29963934, g_loss: 0.82750785\n",
      "Step: [1602] d_loss: 1.26627660, g_loss: 0.84826899\n",
      "Step: [1603] d_loss: 1.28554249, g_loss: 0.82676184\n",
      "Step: [1604] d_loss: 1.34803724, g_loss: 0.82133847\n",
      "Step: [1605] d_loss: 1.34704089, g_loss: 0.82981288\n",
      "Step: [1606] d_loss: 1.31942916, g_loss: 0.83356702\n",
      "Step: [1607] d_loss: 1.34686196, g_loss: 0.79471546\n",
      "Step: [1608] d_loss: 1.32500362, g_loss: 0.81173062\n",
      "Step: [1609] d_loss: 1.29777932, g_loss: 0.84188199\n",
      "Step: [1610] d_loss: 1.32222486, g_loss: 0.83193541\n",
      "Step: [1611] d_loss: 1.34290051, g_loss: 0.84057891\n",
      "Step: [1612] d_loss: 1.38324857, g_loss: 0.79305255\n",
      "Step: [1613] d_loss: 1.34473729, g_loss: 0.80158228\n",
      "Step: [1614] d_loss: 1.27583766, g_loss: 0.83537543\n",
      "Step: [1615] d_loss: 1.32805490, g_loss: 0.81721091\n",
      "Step: [1616] d_loss: 1.27544153, g_loss: 0.83980376\n",
      "Step: [1617] d_loss: 1.30996966, g_loss: 0.82975578\n",
      "Step: [1618] d_loss: 1.33140123, g_loss: 0.81696743\n",
      "Step: [1619] d_loss: 1.31534898, g_loss: 0.81775981\n",
      "Step: [1620] d_loss: 1.33857012, g_loss: 0.80875039\n",
      "Step: [1621] d_loss: 1.34553313, g_loss: 0.81540507\n",
      "Step: [1622] d_loss: 1.29841113, g_loss: 0.84648526\n",
      "Step: [1623] d_loss: 1.36362219, g_loss: 0.80827010\n",
      "Step: [1624] d_loss: 1.30263543, g_loss: 0.81331110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1625] d_loss: 1.32968187, g_loss: 0.80212450\n",
      "Step: [1626] d_loss: 1.34037483, g_loss: 0.81395769\n",
      "Step: [1627] d_loss: 1.31679916, g_loss: 0.82886845\n",
      "Step: [1628] d_loss: 1.33553910, g_loss: 0.81808126\n",
      "Step: [1629] d_loss: 1.32799983, g_loss: 0.81335026\n",
      "Step: [1630] d_loss: 1.28340149, g_loss: 0.82688177\n",
      "Step: [1631] d_loss: 1.34919608, g_loss: 0.79592091\n",
      "Step: [1632] d_loss: 1.29432714, g_loss: 0.82019430\n",
      "Step: [1633] d_loss: 1.31636286, g_loss: 0.81183034\n",
      "Step: [1634] d_loss: 1.33456159, g_loss: 0.79331917\n",
      "Step: [1635] d_loss: 1.31200564, g_loss: 0.83301008\n",
      "Step: [1636] d_loss: 1.27861679, g_loss: 0.86654806\n",
      "Step: [1637] d_loss: 1.31224346, g_loss: 0.82788783\n",
      "Step: [1638] d_loss: 1.33448172, g_loss: 0.83346826\n",
      "Step: [1639] d_loss: 1.30683923, g_loss: 0.82644188\n",
      "Step: [1640] d_loss: 1.32360613, g_loss: 0.79976547\n",
      "Step: [1641] d_loss: 1.32127285, g_loss: 0.81365514\n",
      "Step: [1642] d_loss: 1.34528816, g_loss: 0.79075897\n",
      "Step: [1643] d_loss: 1.30392015, g_loss: 0.80764973\n",
      "Step: [1644] d_loss: 1.34785450, g_loss: 0.80045682\n",
      "Step: [1645] d_loss: 1.34155095, g_loss: 0.81517208\n",
      "Step: [1646] d_loss: 1.32051945, g_loss: 0.82996964\n",
      "Step: [1647] d_loss: 1.31111908, g_loss: 0.85149002\n",
      "Step: [1648] d_loss: 1.31419659, g_loss: 0.83866507\n",
      "Step: [1649] d_loss: 1.31842566, g_loss: 0.83061594\n",
      "Step: [1650] d_loss: 1.29950190, g_loss: 0.83251429\n",
      "Step: [1651] d_loss: 1.30751705, g_loss: 0.81915033\n",
      "Step: [1652] d_loss: 1.30189753, g_loss: 0.81636888\n",
      "Step: [1653] d_loss: 1.35068083, g_loss: 0.81470329\n",
      "Step: [1654] d_loss: 1.28153491, g_loss: 0.83978105\n",
      "Step: [1655] d_loss: 1.29154134, g_loss: 0.82891941\n",
      "Step: [1656] d_loss: 1.30129218, g_loss: 0.83680129\n",
      "Step: [1657] d_loss: 1.29571974, g_loss: 0.83985627\n",
      "Step: [1658] d_loss: 1.33448839, g_loss: 0.79341996\n",
      "Step: [1659] d_loss: 1.34571218, g_loss: 0.82300860\n",
      "Step: [1660] d_loss: 1.31661332, g_loss: 0.81110609\n",
      "Step: [1661] d_loss: 1.30392313, g_loss: 0.83392334\n",
      "Step: [1662] d_loss: 1.36604500, g_loss: 0.78898370\n",
      "Step: [1663] d_loss: 1.30244362, g_loss: 0.83677208\n",
      "Step: [1664] d_loss: 1.31046641, g_loss: 0.82964838\n",
      "Step: [1665] d_loss: 1.29303706, g_loss: 0.81329560\n",
      "Step: [1666] d_loss: 1.27559948, g_loss: 0.82238209\n",
      "Step: [1667] d_loss: 1.29569006, g_loss: 0.82774973\n",
      "Step: [1668] d_loss: 1.34642184, g_loss: 0.78966248\n",
      "Step: [1669] d_loss: 1.32153988, g_loss: 0.82115912\n",
      "Step: [1670] d_loss: 1.30378056, g_loss: 0.84562075\n",
      "Step: [1671] d_loss: 1.33742142, g_loss: 0.79811990\n",
      "Step: [1672] d_loss: 1.32137299, g_loss: 0.81709272\n",
      "Step: [1673] d_loss: 1.32940590, g_loss: 0.79450530\n",
      "Step: [1674] d_loss: 1.29207540, g_loss: 0.82887429\n",
      "Step: [1675] d_loss: 1.31450319, g_loss: 0.80514157\n",
      "Step: [1676] d_loss: 1.35553098, g_loss: 0.82513577\n",
      "Step: [1677] d_loss: 1.32500768, g_loss: 0.83687568\n",
      "Step: [1678] d_loss: 1.34886789, g_loss: 0.78860497\n",
      "Step: [1679] d_loss: 1.29147220, g_loss: 0.83159101\n",
      "Step: [1680] d_loss: 1.27707279, g_loss: 0.84997272\n",
      "Step: [1681] d_loss: 1.30264878, g_loss: 0.83054614\n",
      "Step: [1682] d_loss: 1.32771051, g_loss: 0.81647372\n",
      "Step: [1683] d_loss: 1.28914082, g_loss: 0.84996939\n",
      "Step: [1684] d_loss: 1.31147766, g_loss: 0.80940962\n",
      "Step: [1685] d_loss: 1.33153439, g_loss: 0.81514692\n",
      "Step: [1686] d_loss: 1.33411193, g_loss: 0.80845767\n",
      "Step: [1687] d_loss: 1.29396081, g_loss: 0.84228933\n",
      "Step: [1688] d_loss: 1.33956289, g_loss: 0.81657958\n",
      "Step: [1689] d_loss: 1.35650182, g_loss: 0.82925212\n",
      "Step: [1690] d_loss: 1.36978185, g_loss: 0.79846442\n",
      "Step: [1691] d_loss: 1.32611108, g_loss: 0.80259556\n",
      "Step: [1692] d_loss: 1.30947459, g_loss: 0.83901632\n",
      "Step: [1693] d_loss: 1.29092801, g_loss: 0.81841105\n",
      "Step: [1694] d_loss: 1.33663881, g_loss: 0.82404494\n",
      "Step: [1695] d_loss: 1.28355527, g_loss: 0.83905971\n",
      "Step: [1696] d_loss: 1.31799412, g_loss: 0.83201039\n",
      "Step: [1697] d_loss: 1.32354152, g_loss: 0.83740604\n",
      "Step: [1698] d_loss: 1.32134545, g_loss: 0.83997190\n",
      "Step: [1699] d_loss: 1.34130216, g_loss: 0.79587317\n",
      "Step: [1700] d_loss: 1.34799230, g_loss: 0.81621855\n",
      "Step: [1701] d_loss: 1.31789577, g_loss: 0.82127094\n",
      "Step: [1702] d_loss: 1.33408773, g_loss: 0.84287661\n",
      "Step: [1703] d_loss: 1.27374554, g_loss: 0.82293046\n",
      "Step: [1704] d_loss: 1.31682658, g_loss: 0.81184918\n",
      "Step: [1705] d_loss: 1.29936528, g_loss: 0.82817072\n",
      "Step: [1706] d_loss: 1.35917997, g_loss: 0.80004376\n",
      "Step: [1707] d_loss: 1.35389686, g_loss: 0.82580221\n",
      "Step: [1708] d_loss: 1.39483583, g_loss: 0.80239791\n",
      "Step: [1709] d_loss: 1.31490350, g_loss: 0.81177986\n",
      "Step: [1710] d_loss: 1.34094369, g_loss: 0.82574022\n",
      "Step: [1711] d_loss: 1.35913181, g_loss: 0.80424136\n",
      "Step: [1712] d_loss: 1.29644012, g_loss: 0.82324761\n",
      "Step: [1713] d_loss: 1.32383394, g_loss: 0.80275607\n",
      "Step: [1714] d_loss: 1.31542397, g_loss: 0.81252837\n",
      "Step: [1715] d_loss: 1.31119728, g_loss: 0.81254780\n",
      "Step: [1716] d_loss: 1.29728746, g_loss: 0.82554239\n",
      "Step: [1717] d_loss: 1.33186102, g_loss: 0.81511986\n",
      "Step: [1718] d_loss: 1.26584840, g_loss: 0.83809626\n",
      "Step: [1719] d_loss: 1.26191223, g_loss: 0.85285270\n",
      "Step: [1720] d_loss: 1.26960194, g_loss: 0.86399162\n",
      "Step: [1721] d_loss: 1.31778014, g_loss: 0.86381799\n",
      "Step: [1722] d_loss: 1.32107162, g_loss: 0.82605207\n",
      "Step: [1723] d_loss: 1.29142165, g_loss: 0.81729329\n",
      "Step: [1724] d_loss: 1.31160259, g_loss: 0.81424844\n",
      "Step: [1725] d_loss: 1.31748307, g_loss: 0.79231584\n",
      "Step: [1726] d_loss: 1.25768054, g_loss: 0.84974670\n",
      "Step: [1727] d_loss: 1.30152428, g_loss: 0.83508998\n",
      "Step: [1728] d_loss: 1.33529902, g_loss: 0.85561204\n",
      "Step: [1729] d_loss: 1.30575037, g_loss: 0.83611453\n",
      "Step: [1730] d_loss: 1.32675612, g_loss: 0.83537483\n",
      "Step: [1731] d_loss: 1.29675174, g_loss: 0.83098817\n",
      "Step: [1732] d_loss: 1.27547836, g_loss: 0.84591359\n",
      "Step: [1733] d_loss: 1.32601774, g_loss: 0.79191762\n",
      "Step: [1734] d_loss: 1.31230402, g_loss: 0.81563586\n",
      "Step: [1735] d_loss: 1.34582520, g_loss: 0.82701254\n",
      "Step: [1736] d_loss: 1.35067225, g_loss: 0.81006730\n",
      "Step: [1737] d_loss: 1.30155969, g_loss: 0.83655840\n",
      "Step: [1738] d_loss: 1.30785084, g_loss: 0.80936009\n",
      "Step: [1739] d_loss: 1.28890932, g_loss: 0.84367424\n",
      "Step: [1740] d_loss: 1.35263872, g_loss: 0.80748433\n",
      "Step: [1741] d_loss: 1.30543661, g_loss: 0.84481192\n",
      "Step: [1742] d_loss: 1.30245423, g_loss: 0.83849633\n",
      "Step: [1743] d_loss: 1.29077125, g_loss: 0.81148678\n",
      "Step: [1744] d_loss: 1.35904002, g_loss: 0.78249216\n",
      "Step: [1745] d_loss: 1.31980014, g_loss: 0.81785929\n",
      "Step: [1746] d_loss: 1.31410003, g_loss: 0.82105863\n",
      "Step: [1747] d_loss: 1.32385075, g_loss: 0.83709502\n",
      "Step: [1748] d_loss: 1.30639696, g_loss: 0.85318816\n",
      "Step: [1749] d_loss: 1.29694438, g_loss: 0.83392394\n",
      "Step: [1750] d_loss: 1.30316377, g_loss: 0.85511357\n",
      "Step: [1751] d_loss: 1.28813124, g_loss: 0.80586994\n",
      "Step: [1752] d_loss: 1.35026717, g_loss: 0.79065621\n",
      "Step: [1753] d_loss: 1.25975180, g_loss: 0.84157473\n",
      "Step: [1754] d_loss: 1.30755937, g_loss: 0.84440637\n",
      "Step: [1755] d_loss: 1.29102111, g_loss: 0.84886646\n",
      "Step: [1756] d_loss: 1.32254529, g_loss: 0.82107478\n",
      "Step: [1757] d_loss: 1.32998455, g_loss: 0.81148237\n",
      "Step: [1758] d_loss: 1.31122446, g_loss: 0.83270442\n",
      "Step: [1759] d_loss: 1.28318274, g_loss: 0.81621927\n",
      "Step: [1760] d_loss: 1.33036470, g_loss: 0.81257766\n",
      "Step: [1761] d_loss: 1.34950554, g_loss: 0.80900335\n",
      "Step: [1762] d_loss: 1.31409705, g_loss: 0.84437227\n",
      "Step: [1763] d_loss: 1.29813886, g_loss: 0.84708261\n",
      "Step: [1764] d_loss: 1.28810930, g_loss: 0.87097335\n",
      "Step: [1765] d_loss: 1.30794656, g_loss: 0.80916584\n",
      "Step: [1766] d_loss: 1.29708672, g_loss: 0.83329320\n",
      "Step: [1767] d_loss: 1.29371846, g_loss: 0.84945917\n",
      "Step: [1768] d_loss: 1.33370566, g_loss: 0.80088490\n",
      "Step: [1769] d_loss: 1.28773892, g_loss: 0.81222951\n",
      "Step: [1770] d_loss: 1.33994627, g_loss: 0.78473538\n",
      "Step: [1771] d_loss: 1.29181790, g_loss: 0.84855032\n",
      "Step: [1772] d_loss: 1.37230277, g_loss: 0.81456202\n",
      "Step: [1773] d_loss: 1.31543314, g_loss: 0.82922578\n",
      "Step: [1774] d_loss: 1.36233997, g_loss: 0.78327781\n",
      "Step: [1775] d_loss: 1.30493760, g_loss: 0.84390175\n",
      "Step: [1776] d_loss: 1.30551016, g_loss: 0.84069848\n",
      "Step: [1777] d_loss: 1.30404329, g_loss: 0.81799769\n",
      "Step: [1778] d_loss: 1.28943539, g_loss: 0.85730982\n",
      "Step: [1779] d_loss: 1.29356861, g_loss: 0.83302355\n",
      "Step: [1780] d_loss: 1.24871039, g_loss: 0.86692423\n",
      "Step: [1781] d_loss: 1.33275509, g_loss: 0.83461308\n",
      "Step: [1782] d_loss: 1.29550827, g_loss: 0.85568619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1783] d_loss: 1.34319043, g_loss: 0.83862758\n",
      "Step: [1784] d_loss: 1.33600450, g_loss: 0.82981479\n",
      "Step: [1785] d_loss: 1.33162391, g_loss: 0.84922653\n",
      "Step: [1786] d_loss: 1.33914566, g_loss: 0.81626254\n",
      "Step: [1787] d_loss: 1.31608057, g_loss: 0.82401311\n",
      "Step: [1788] d_loss: 1.30679119, g_loss: 0.81616229\n",
      "Step: [1789] d_loss: 1.29342496, g_loss: 0.84346581\n",
      "Step: [1790] d_loss: 1.26634848, g_loss: 0.84432268\n",
      "Step: [1791] d_loss: 1.31213307, g_loss: 0.81764078\n",
      "Step: [1792] d_loss: 1.33023906, g_loss: 0.81101131\n",
      "Step: [1793] d_loss: 1.32127523, g_loss: 0.82139409\n",
      "Step: [1794] d_loss: 1.24571490, g_loss: 0.86390847\n",
      "Step: [1795] d_loss: 1.33968818, g_loss: 0.81733716\n",
      "Step: [1796] d_loss: 1.32086945, g_loss: 0.82803905\n",
      "Step: [1797] d_loss: 1.30511379, g_loss: 0.83668137\n",
      "Step: [1798] d_loss: 1.33063221, g_loss: 0.81233943\n",
      "Step: [1799] d_loss: 1.30380189, g_loss: 0.80785728\n",
      "Step: [1800] d_loss: 1.28975999, g_loss: 0.84296632\n",
      "Step: [1801] d_loss: 1.32441604, g_loss: 0.82579362\n",
      "Step: [1802] d_loss: 1.29267526, g_loss: 0.82490587\n",
      "Step: [1803] d_loss: 1.33464980, g_loss: 0.82875878\n",
      "Step: [1804] d_loss: 1.35357928, g_loss: 0.77168471\n",
      "Step: [1805] d_loss: 1.29851401, g_loss: 0.81726241\n",
      "Step: [1806] d_loss: 1.30531645, g_loss: 0.84278327\n",
      "Step: [1807] d_loss: 1.31178057, g_loss: 0.85283923\n",
      "Step: [1808] d_loss: 1.28716886, g_loss: 0.84255725\n",
      "Step: [1809] d_loss: 1.30943036, g_loss: 0.84540153\n",
      "Step: [1810] d_loss: 1.28194690, g_loss: 0.80360329\n",
      "Step: [1811] d_loss: 1.32274485, g_loss: 0.80605149\n",
      "Step: [1812] d_loss: 1.29804850, g_loss: 0.80884176\n",
      "Step: [1813] d_loss: 1.26711822, g_loss: 0.83084929\n",
      "Step: [1814] d_loss: 1.30910373, g_loss: 0.83954781\n",
      "Step: [1815] d_loss: 1.34092879, g_loss: 0.80876553\n",
      "Step: [1816] d_loss: 1.31547070, g_loss: 0.82138848\n",
      "Step: [1817] d_loss: 1.30280042, g_loss: 0.82251626\n",
      "Step: [1818] d_loss: 1.30885577, g_loss: 0.83100313\n",
      "Step: [1819] d_loss: 1.28424132, g_loss: 0.86585522\n",
      "Step: [1820] d_loss: 1.33683968, g_loss: 0.80777538\n",
      "Step: [1821] d_loss: 1.31397831, g_loss: 0.82812387\n",
      "Step: [1822] d_loss: 1.32101035, g_loss: 0.83250868\n",
      "Step: [1823] d_loss: 1.27712870, g_loss: 0.82133377\n",
      "Step: [1824] d_loss: 1.31455445, g_loss: 0.82244354\n",
      "Step: [1825] d_loss: 1.34511161, g_loss: 0.81346667\n",
      "Step: [1826] d_loss: 1.37856126, g_loss: 0.80579197\n",
      "Step: [1827] d_loss: 1.31197762, g_loss: 0.83470964\n",
      "Step: [1828] d_loss: 1.32150912, g_loss: 0.83489895\n",
      "Step: [1829] d_loss: 1.29342139, g_loss: 0.84246594\n",
      "Step: [1830] d_loss: 1.30687368, g_loss: 0.83602804\n",
      "Step: [1831] d_loss: 1.27723491, g_loss: 0.83332407\n",
      "Step: [1832] d_loss: 1.30623555, g_loss: 0.82419848\n",
      "Step: [1833] d_loss: 1.32698333, g_loss: 0.79786044\n",
      "Step: [1834] d_loss: 1.29658914, g_loss: 0.82630074\n",
      "Step: [1835] d_loss: 1.33178663, g_loss: 0.83709431\n",
      "Step: [1836] d_loss: 1.29030681, g_loss: 0.84341240\n",
      "Step: [1837] d_loss: 1.30812323, g_loss: 0.82372820\n",
      "Step: [1838] d_loss: 1.30962753, g_loss: 0.81975430\n",
      "Step: [1839] d_loss: 1.31058705, g_loss: 0.82844293\n",
      "Step: [1840] d_loss: 1.29361439, g_loss: 0.79263270\n",
      "Step: [1841] d_loss: 1.32796431, g_loss: 0.81469667\n",
      "Step: [1842] d_loss: 1.31371808, g_loss: 0.83520699\n",
      "Step: [1843] d_loss: 1.33392048, g_loss: 0.83083087\n",
      "Step: [1844] d_loss: 1.32000840, g_loss: 0.81140947\n",
      "Step: [1845] d_loss: 1.29108763, g_loss: 0.82129776\n",
      "Step: [1846] d_loss: 1.31542993, g_loss: 0.84279609\n",
      "Step: [1847] d_loss: 1.32337165, g_loss: 0.82166350\n",
      "Step: [1848] d_loss: 1.32636857, g_loss: 0.81625104\n",
      "Step: [1849] d_loss: 1.27528763, g_loss: 0.83233649\n",
      "Step: [1850] d_loss: 1.30346107, g_loss: 0.83988351\n",
      "Step: [1851] d_loss: 1.32806444, g_loss: 0.83695501\n",
      "Step: [1852] d_loss: 1.35246992, g_loss: 0.80673689\n",
      "Step: [1853] d_loss: 1.29995275, g_loss: 0.84515816\n",
      "Step: [1854] d_loss: 1.31897962, g_loss: 0.81057453\n",
      "Step: [1855] d_loss: 1.31415749, g_loss: 0.81095749\n",
      "Step: [1856] d_loss: 1.29498887, g_loss: 0.81529897\n",
      "Step: [1857] d_loss: 1.29655027, g_loss: 0.82782584\n",
      "Step: [1858] d_loss: 1.30978334, g_loss: 0.83999789\n",
      "Step: [1859] d_loss: 1.31105947, g_loss: 0.81727004\n",
      "Step: [1860] d_loss: 1.30267704, g_loss: 0.82673252\n",
      "Step: [1861] d_loss: 1.32216561, g_loss: 0.80640638\n",
      "Step: [1862] d_loss: 1.28399682, g_loss: 0.83786559\n",
      "Step: [1863] d_loss: 1.32785594, g_loss: 0.81775397\n",
      "Step: [1864] d_loss: 1.28492582, g_loss: 0.81348193\n",
      "Step: [1865] d_loss: 1.28865254, g_loss: 0.83316213\n",
      "Step: [1866] d_loss: 1.31219804, g_loss: 0.82696480\n",
      "Step: [1867] d_loss: 1.32698572, g_loss: 0.81692314\n",
      "Step: [1868] d_loss: 1.31398404, g_loss: 0.82742882\n",
      "Step: [1869] d_loss: 1.31775594, g_loss: 0.82006490\n",
      "Step: [1870] d_loss: 1.30417228, g_loss: 0.84147161\n",
      "Step: [1871] d_loss: 1.33376360, g_loss: 0.82587707\n",
      "Step: [1872] d_loss: 1.30356681, g_loss: 0.83675838\n",
      "Step: [1873] d_loss: 1.31760216, g_loss: 0.81520641\n",
      "Step: [1874] d_loss: 1.33702672, g_loss: 0.80912906\n",
      "Step: [1875] d_loss: 1.33534193, g_loss: 0.80424911\n",
      "Step: [1876] d_loss: 1.26162624, g_loss: 0.85590768\n",
      "Step: [1877] d_loss: 1.27554893, g_loss: 0.82989806\n",
      "Step: [1878] d_loss: 1.28783655, g_loss: 0.85909116\n",
      "Step: [1879] d_loss: 1.29312801, g_loss: 0.82907319\n",
      "Step: [1880] d_loss: 1.29438806, g_loss: 0.82729197\n",
      "Step: [1881] d_loss: 1.33836126, g_loss: 0.83703959\n",
      "Step: [1882] d_loss: 1.28809714, g_loss: 0.81637478\n",
      "Step: [1883] d_loss: 1.32189000, g_loss: 0.80331481\n",
      "Step: [1884] d_loss: 1.28829956, g_loss: 0.82078242\n",
      "Step: [1885] d_loss: 1.30656612, g_loss: 0.82460999\n",
      "Step: [1886] d_loss: 1.32599127, g_loss: 0.82198226\n",
      "Step: [1887] d_loss: 1.33694458, g_loss: 0.83887100\n",
      "Step: [1888] d_loss: 1.28468561, g_loss: 0.84117007\n",
      "Step: [1889] d_loss: 1.33466029, g_loss: 0.80051386\n",
      "Step: [1890] d_loss: 1.30065608, g_loss: 0.82800317\n",
      "Step: [1891] d_loss: 1.32656646, g_loss: 0.79998052\n",
      "Step: [1892] d_loss: 1.33055282, g_loss: 0.80387580\n",
      "Step: [1893] d_loss: 1.26040804, g_loss: 0.82680333\n",
      "Step: [1894] d_loss: 1.32915282, g_loss: 0.80666560\n",
      "Step: [1895] d_loss: 1.31717134, g_loss: 0.81219542\n",
      "Step: [1896] d_loss: 1.27307129, g_loss: 0.84005451\n",
      "Step: [1897] d_loss: 1.31797481, g_loss: 0.82077837\n",
      "Step: [1898] d_loss: 1.28517449, g_loss: 0.83452332\n",
      "Step: [1899] d_loss: 1.27109301, g_loss: 0.83155811\n",
      "Step: [1900] d_loss: 1.30616760, g_loss: 0.81069541\n",
      "Step: [1901] d_loss: 1.32804537, g_loss: 0.83495367\n",
      "Step: [1902] d_loss: 1.29259920, g_loss: 0.85008079\n",
      "Step: [1903] d_loss: 1.31401277, g_loss: 0.83678281\n",
      "Step: [1904] d_loss: 1.26939344, g_loss: 0.82923591\n",
      "Step: [1905] d_loss: 1.33335137, g_loss: 0.80249739\n",
      "Step: [1906] d_loss: 1.27310562, g_loss: 0.83928251\n",
      "Step: [1907] d_loss: 1.33434868, g_loss: 0.80405962\n",
      "Step: [1908] d_loss: 1.32028508, g_loss: 0.80811954\n",
      "Step: [1909] d_loss: 1.28261387, g_loss: 0.83480310\n",
      "Step: [1910] d_loss: 1.28324699, g_loss: 0.84918344\n",
      "Step: [1911] d_loss: 1.35046721, g_loss: 0.81883973\n",
      "Step: [1912] d_loss: 1.30088401, g_loss: 0.85637319\n",
      "Step: [1913] d_loss: 1.30728805, g_loss: 0.84379810\n",
      "Step: [1914] d_loss: 1.31429386, g_loss: 0.82800323\n",
      "Step: [1915] d_loss: 1.35132682, g_loss: 0.79165173\n",
      "Step: [1916] d_loss: 1.30779922, g_loss: 0.82700861\n",
      "Step: [1917] d_loss: 1.29009724, g_loss: 0.80988765\n",
      "Step: [1918] d_loss: 1.30077267, g_loss: 0.82260936\n",
      "Step: [1919] d_loss: 1.24610519, g_loss: 0.83828074\n",
      "Step: [1920] d_loss: 1.30563855, g_loss: 0.81942660\n",
      "Step: [1921] d_loss: 1.31100345, g_loss: 0.83632767\n",
      "Step: [1922] d_loss: 1.32101440, g_loss: 0.83940017\n",
      "Step: [1923] d_loss: 1.29853392, g_loss: 0.84899151\n",
      "Step: [1924] d_loss: 1.30408382, g_loss: 0.85264385\n",
      "Step: [1925] d_loss: 1.30863309, g_loss: 0.84954011\n",
      "Step: [1926] d_loss: 1.31606650, g_loss: 0.83200449\n",
      "Step: [1927] d_loss: 1.27656555, g_loss: 0.82757103\n",
      "Step: [1928] d_loss: 1.31393695, g_loss: 0.77644920\n",
      "Step: [1929] d_loss: 1.31712508, g_loss: 0.83078158\n",
      "Step: [1930] d_loss: 1.31338453, g_loss: 0.83426225\n",
      "Step: [1931] d_loss: 1.29300845, g_loss: 0.85847050\n",
      "Step: [1932] d_loss: 1.30189824, g_loss: 0.84436738\n",
      "Step: [1933] d_loss: 1.31176829, g_loss: 0.85733187\n",
      "Step: [1934] d_loss: 1.27491927, g_loss: 0.83527374\n",
      "Step: [1935] d_loss: 1.26853335, g_loss: 0.86007583\n",
      "Step: [1936] d_loss: 1.26887441, g_loss: 0.82620525\n",
      "Step: [1937] d_loss: 1.30890727, g_loss: 0.80552709\n",
      "Step: [1938] d_loss: 1.30123556, g_loss: 0.80081427\n",
      "Step: [1939] d_loss: 1.30858707, g_loss: 0.83480477\n",
      "Step: [1940] d_loss: 1.34286237, g_loss: 0.84511715\n",
      "Step: [1941] d_loss: 1.30749321, g_loss: 0.81894350\n",
      "Step: [1942] d_loss: 1.30740130, g_loss: 0.82362109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1943] d_loss: 1.29106891, g_loss: 0.84179997\n",
      "Step: [1944] d_loss: 1.29665875, g_loss: 0.82760364\n",
      "Step: [1945] d_loss: 1.30154598, g_loss: 0.82645369\n",
      "Step: [1946] d_loss: 1.31328464, g_loss: 0.82814276\n",
      "Step: [1947] d_loss: 1.29825163, g_loss: 0.82854211\n",
      "Step: [1948] d_loss: 1.27467728, g_loss: 0.85245788\n",
      "Step: [1949] d_loss: 1.31766498, g_loss: 0.82782555\n",
      "Step: [1950] d_loss: 1.35036576, g_loss: 0.83004004\n",
      "Step: [1951] d_loss: 1.27622986, g_loss: 0.87248838\n",
      "Step: [1952] d_loss: 1.29703617, g_loss: 0.84635460\n",
      "Step: [1953] d_loss: 1.31854343, g_loss: 0.81868243\n",
      "Step: [1954] d_loss: 1.28703499, g_loss: 0.81990385\n",
      "Step: [1955] d_loss: 1.31248879, g_loss: 0.81722957\n",
      "Step: [1956] d_loss: 1.32922506, g_loss: 0.81114966\n",
      "Step: [1957] d_loss: 1.29200101, g_loss: 0.83598596\n",
      "Step: [1958] d_loss: 1.29231095, g_loss: 0.85291719\n",
      "Step: [1959] d_loss: 1.30437422, g_loss: 0.82304752\n",
      "Step: [1960] d_loss: 1.31508303, g_loss: 0.81817484\n",
      "Step: [1961] d_loss: 1.34324026, g_loss: 0.80982023\n",
      "Step: [1962] d_loss: 1.30995750, g_loss: 0.84617996\n",
      "Step: [1963] d_loss: 1.31853056, g_loss: 0.86004221\n",
      "Step: [1964] d_loss: 1.29825211, g_loss: 0.83346182\n",
      "Step: [1965] d_loss: 1.33956599, g_loss: 0.84983730\n",
      "Step: [1966] d_loss: 1.32555330, g_loss: 0.82244945\n",
      "Step: [1967] d_loss: 1.30796170, g_loss: 0.83109736\n",
      "Step: [1968] d_loss: 1.28971076, g_loss: 0.83297706\n",
      "Step: [1969] d_loss: 1.28137958, g_loss: 0.81859493\n",
      "Step: [1970] d_loss: 1.29565609, g_loss: 0.82240659\n",
      "Step: [1971] d_loss: 1.33446836, g_loss: 0.84354448\n",
      "Step: [1972] d_loss: 1.30547595, g_loss: 0.84222674\n",
      "Step: [1973] d_loss: 1.35128009, g_loss: 0.79799664\n",
      "Step: [1974] d_loss: 1.28452694, g_loss: 0.83421779\n",
      "Step: [1975] d_loss: 1.28538895, g_loss: 0.82500976\n",
      "Step: [1976] d_loss: 1.28585029, g_loss: 0.82006902\n",
      "Step: [1977] d_loss: 1.31957054, g_loss: 0.81716317\n",
      "Step: [1978] d_loss: 1.28493285, g_loss: 0.84830046\n",
      "Step: [1979] d_loss: 1.32293034, g_loss: 0.83454198\n",
      "Step: [1980] d_loss: 1.31666708, g_loss: 0.84665430\n",
      "Step: [1981] d_loss: 1.30297911, g_loss: 0.81954253\n",
      "Step: [1982] d_loss: 1.30100989, g_loss: 0.85103381\n",
      "Step: [1983] d_loss: 1.27455592, g_loss: 0.82918245\n",
      "Step: [1984] d_loss: 1.29608047, g_loss: 0.81834579\n",
      "Step: [1985] d_loss: 1.31908643, g_loss: 0.82192987\n",
      "Step: [1986] d_loss: 1.32060647, g_loss: 0.82724792\n",
      "Step: [1987] d_loss: 1.28072047, g_loss: 0.84928238\n",
      "Step: [1988] d_loss: 1.33543909, g_loss: 0.81522602\n",
      "Step: [1989] d_loss: 1.27066052, g_loss: 0.85908961\n",
      "Step: [1990] d_loss: 1.27568471, g_loss: 0.86692166\n",
      "Step: [1991] d_loss: 1.30461586, g_loss: 0.86019015\n",
      "Step: [1992] d_loss: 1.30276489, g_loss: 0.84176332\n",
      "Step: [1993] d_loss: 1.32215369, g_loss: 0.81384146\n",
      "Step: [1994] d_loss: 1.30610275, g_loss: 0.84136212\n",
      "Step: [1995] d_loss: 1.28473067, g_loss: 0.84702480\n",
      "Step: [1996] d_loss: 1.31597602, g_loss: 0.82648206\n",
      "Step: [1997] d_loss: 1.28457284, g_loss: 0.83195847\n",
      "Step: [1998] d_loss: 1.30145669, g_loss: 0.86091590\n",
      "Step: [1999] d_loss: 1.26706469, g_loss: 0.86378229\n",
      "Step: [2000] d_loss: 1.34336615, g_loss: 0.82442880\n",
      "Step: [2001] d_loss: 1.26336741, g_loss: 0.83580673\n",
      "Step: [2002] d_loss: 1.27537441, g_loss: 0.84481323\n",
      "Step: [2003] d_loss: 1.30575573, g_loss: 0.84692901\n",
      "Step: [2004] d_loss: 1.29896283, g_loss: 0.84747112\n",
      "Step: [2005] d_loss: 1.31163681, g_loss: 0.85279709\n",
      "Step: [2006] d_loss: 1.28055692, g_loss: 0.84875834\n",
      "Step: [2007] d_loss: 1.32785177, g_loss: 0.83091950\n",
      "Step: [2008] d_loss: 1.28482604, g_loss: 0.82534623\n",
      "Step: [2009] d_loss: 1.33828640, g_loss: 0.81002587\n",
      "Step: [2010] d_loss: 1.33235550, g_loss: 0.82413340\n",
      "Step: [2011] d_loss: 1.33424115, g_loss: 0.85076392\n",
      "Step: [2012] d_loss: 1.28686905, g_loss: 0.86018050\n",
      "Step: [2013] d_loss: 1.29496980, g_loss: 0.84401733\n",
      "Step: [2014] d_loss: 1.29664421, g_loss: 0.83059800\n",
      "Step: [2015] d_loss: 1.27636194, g_loss: 0.83027983\n",
      "Step: [2016] d_loss: 1.28474951, g_loss: 0.83833128\n",
      "Step: [2017] d_loss: 1.28889358, g_loss: 0.83014929\n",
      "Step: [2018] d_loss: 1.28950155, g_loss: 0.83863425\n",
      "Step: [2019] d_loss: 1.29591811, g_loss: 0.84231830\n",
      "Step: [2020] d_loss: 1.30259645, g_loss: 0.83225960\n",
      "Step: [2021] d_loss: 1.30304813, g_loss: 0.85818219\n",
      "Step: [2022] d_loss: 1.31108117, g_loss: 0.84391677\n",
      "Step: [2023] d_loss: 1.30443072, g_loss: 0.85350013\n",
      "Step: [2024] d_loss: 1.27493954, g_loss: 0.85964155\n",
      "Step: [2025] d_loss: 1.28586900, g_loss: 0.85465139\n",
      "Step: [2026] d_loss: 1.27242410, g_loss: 0.83098900\n",
      "Step: [2027] d_loss: 1.33475053, g_loss: 0.81426847\n",
      "Step: [2028] d_loss: 1.31531739, g_loss: 0.81426400\n",
      "Step: [2029] d_loss: 1.29269409, g_loss: 0.84197879\n",
      "Step: [2030] d_loss: 1.30031836, g_loss: 0.84103984\n",
      "Step: [2031] d_loss: 1.28225315, g_loss: 0.84462136\n",
      "Step: [2032] d_loss: 1.30705726, g_loss: 0.84518689\n",
      "Step: [2033] d_loss: 1.32030940, g_loss: 0.84371614\n",
      "Step: [2034] d_loss: 1.29066539, g_loss: 0.83069623\n",
      "Step: [2035] d_loss: 1.28484297, g_loss: 0.85283637\n",
      "Step: [2036] d_loss: 1.29815841, g_loss: 0.84350443\n",
      "Step: [2037] d_loss: 1.28418303, g_loss: 0.84189039\n",
      "Step: [2038] d_loss: 1.30098403, g_loss: 0.82416105\n",
      "Step: [2039] d_loss: 1.30697894, g_loss: 0.80574965\n",
      "Step: [2040] d_loss: 1.28569078, g_loss: 0.84433156\n",
      "Step: [2041] d_loss: 1.32514894, g_loss: 0.81480908\n",
      "Step: [2042] d_loss: 1.27255607, g_loss: 0.85516292\n",
      "Step: [2043] d_loss: 1.31350899, g_loss: 0.83441532\n",
      "Step: [2044] d_loss: 1.29523158, g_loss: 0.80261463\n",
      "Step: [2045] d_loss: 1.29554176, g_loss: 0.82916528\n",
      "Step: [2046] d_loss: 1.26739788, g_loss: 0.83795059\n",
      "Step: [2047] d_loss: 1.32621622, g_loss: 0.81613505\n",
      "Step: [2048] d_loss: 1.30662525, g_loss: 0.85077393\n",
      "Step: [2049] d_loss: 1.28104699, g_loss: 0.85478032\n",
      "Step: [2050] d_loss: 1.33002949, g_loss: 0.79676837\n",
      "Step: [2051] d_loss: 1.34044981, g_loss: 0.81119001\n",
      "Step: [2052] d_loss: 1.31921101, g_loss: 0.80866623\n",
      "Step: [2053] d_loss: 1.36215413, g_loss: 0.80483145\n",
      "Step: [2054] d_loss: 1.31599116, g_loss: 0.83244717\n",
      "Step: [2055] d_loss: 1.28823876, g_loss: 0.86733109\n",
      "Step: [2056] d_loss: 1.31646872, g_loss: 0.81165004\n",
      "Step: [2057] d_loss: 1.30805755, g_loss: 0.82979822\n",
      "Step: [2058] d_loss: 1.27793908, g_loss: 0.84916437\n",
      "Step: [2059] d_loss: 1.29566491, g_loss: 0.84391516\n",
      "Step: [2060] d_loss: 1.31847858, g_loss: 0.81588876\n",
      "Step: [2061] d_loss: 1.30374563, g_loss: 0.83722794\n",
      "Step: [2062] d_loss: 1.28497601, g_loss: 0.85054421\n",
      "Step: [2063] d_loss: 1.29965281, g_loss: 0.85553670\n",
      "Step: [2064] d_loss: 1.30112338, g_loss: 0.82729906\n",
      "Step: [2065] d_loss: 1.30742204, g_loss: 0.82271254\n",
      "Step: [2066] d_loss: 1.27809668, g_loss: 0.82831788\n",
      "Step: [2067] d_loss: 1.34876847, g_loss: 0.78529942\n",
      "Step: [2068] d_loss: 1.31149578, g_loss: 0.82566601\n",
      "Step: [2069] d_loss: 1.32475471, g_loss: 0.82149589\n",
      "Step: [2070] d_loss: 1.31174338, g_loss: 0.82470620\n",
      "Step: [2071] d_loss: 1.30257058, g_loss: 0.82723057\n",
      "Step: [2072] d_loss: 1.30843067, g_loss: 0.83550751\n",
      "Step: [2073] d_loss: 1.30722392, g_loss: 0.82093668\n",
      "Step: [2074] d_loss: 1.31689394, g_loss: 0.82918429\n",
      "Step: [2075] d_loss: 1.28138959, g_loss: 0.87604439\n",
      "Step: [2076] d_loss: 1.29467201, g_loss: 0.85631508\n",
      "Step: [2077] d_loss: 1.26811385, g_loss: 0.81305110\n",
      "Step: [2078] d_loss: 1.25882053, g_loss: 0.86058855\n",
      "Step: [2079] d_loss: 1.31024814, g_loss: 0.83188176\n",
      "Step: [2080] d_loss: 1.29617906, g_loss: 0.80729997\n",
      "Step: [2081] d_loss: 1.33917332, g_loss: 0.80210882\n",
      "Step: [2082] d_loss: 1.30058277, g_loss: 0.83108664\n",
      "Step: [2083] d_loss: 1.29693854, g_loss: 0.84668285\n",
      "Step: [2084] d_loss: 1.31932020, g_loss: 0.85119951\n",
      "Step: [2085] d_loss: 1.30537534, g_loss: 0.81721866\n",
      "Step: [2086] d_loss: 1.30175138, g_loss: 0.82093513\n",
      "Step: [2087] d_loss: 1.29203987, g_loss: 0.83000785\n",
      "Step: [2088] d_loss: 1.27829194, g_loss: 0.86443782\n",
      "Step: [2089] d_loss: 1.30829430, g_loss: 0.83530092\n",
      "Step: [2090] d_loss: 1.29394960, g_loss: 0.82913065\n",
      "Step: [2091] d_loss: 1.29503465, g_loss: 0.82746255\n",
      "Step: [2092] d_loss: 1.30270875, g_loss: 0.82529581\n",
      "Step: [2093] d_loss: 1.28258395, g_loss: 0.84801102\n",
      "Step: [2094] d_loss: 1.28770864, g_loss: 0.83498764\n",
      "Step: [2095] d_loss: 1.30818105, g_loss: 0.84368265\n",
      "Step: [2096] d_loss: 1.28863084, g_loss: 0.83710563\n",
      "Step: [2097] d_loss: 1.31785917, g_loss: 0.83134151\n",
      "Step: [2098] d_loss: 1.29360795, g_loss: 0.85744202\n",
      "Step: [2099] d_loss: 1.32665229, g_loss: 0.81268430\n",
      "Step: [2100] d_loss: 1.28839719, g_loss: 0.84215033\n",
      "Step: [2101] d_loss: 1.35242486, g_loss: 0.79567528\n",
      "Step: [2102] d_loss: 1.33041728, g_loss: 0.82755798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2103] d_loss: 1.30685139, g_loss: 0.82410431\n",
      "Step: [2104] d_loss: 1.29865122, g_loss: 0.83512074\n",
      "Step: [2105] d_loss: 1.27356827, g_loss: 0.83983845\n",
      "Step: [2106] d_loss: 1.23808551, g_loss: 0.86139601\n",
      "Step: [2107] d_loss: 1.31446481, g_loss: 0.81083512\n",
      "Step: [2108] d_loss: 1.32974064, g_loss: 0.83210361\n",
      "Step: [2109] d_loss: 1.27272499, g_loss: 0.85867822\n",
      "Step: [2110] d_loss: 1.27062297, g_loss: 0.86149132\n",
      "Step: [2111] d_loss: 1.29445791, g_loss: 0.85595983\n",
      "Step: [2112] d_loss: 1.28823507, g_loss: 0.88306129\n",
      "Step: [2113] d_loss: 1.33698213, g_loss: 0.88122618\n",
      "Step: [2114] d_loss: 1.32694840, g_loss: 0.83358288\n",
      "Step: [2115] d_loss: 1.30037010, g_loss: 0.84520578\n",
      "Step: [2116] d_loss: 1.28179157, g_loss: 0.85834968\n",
      "Step: [2117] d_loss: 1.28132188, g_loss: 0.84013915\n",
      "Step: [2118] d_loss: 1.31609845, g_loss: 0.81779110\n",
      "Step: [2119] d_loss: 1.33468378, g_loss: 0.81209838\n",
      "Step: [2120] d_loss: 1.30893898, g_loss: 0.82664037\n",
      "Step: [2121] d_loss: 1.34301114, g_loss: 0.84044290\n",
      "Step: [2122] d_loss: 1.30198252, g_loss: 0.83243549\n",
      "Step: [2123] d_loss: 1.31663680, g_loss: 0.83214903\n",
      "Step: [2124] d_loss: 1.29299212, g_loss: 0.83465958\n",
      "Step: [2125] d_loss: 1.31298614, g_loss: 0.83844531\n",
      "Step: [2126] d_loss: 1.31750584, g_loss: 0.82753289\n",
      "Step: [2127] d_loss: 1.28467143, g_loss: 0.79629618\n",
      "Step: [2128] d_loss: 1.25277638, g_loss: 0.84256554\n",
      "Step: [2129] d_loss: 1.29047418, g_loss: 0.85866439\n",
      "Step: [2130] d_loss: 1.28743458, g_loss: 0.85256445\n",
      "Step: [2131] d_loss: 1.33391428, g_loss: 0.81785160\n",
      "Step: [2132] d_loss: 1.26932395, g_loss: 0.85758185\n",
      "Step: [2133] d_loss: 1.28937316, g_loss: 0.85832518\n",
      "Step: [2134] d_loss: 1.27157116, g_loss: 0.85139257\n",
      "Step: [2135] d_loss: 1.31613553, g_loss: 0.81299913\n",
      "Step: [2136] d_loss: 1.30918705, g_loss: 0.81598687\n",
      "Step: [2137] d_loss: 1.28383434, g_loss: 0.85400689\n",
      "Step: [2138] d_loss: 1.29368424, g_loss: 0.86171770\n",
      "Step: [2139] d_loss: 1.32996583, g_loss: 0.86533833\n",
      "Step: [2140] d_loss: 1.30465174, g_loss: 0.83282959\n",
      "Step: [2141] d_loss: 1.31703115, g_loss: 0.86676294\n",
      "Step: [2142] d_loss: 1.31843770, g_loss: 0.82344055\n",
      "Step: [2143] d_loss: 1.28500807, g_loss: 0.83408934\n",
      "Step: [2144] d_loss: 1.31750226, g_loss: 0.82817340\n",
      "Step: [2145] d_loss: 1.29525661, g_loss: 0.87109911\n",
      "Step: [2146] d_loss: 1.33268952, g_loss: 0.86633164\n",
      "Step: [2147] d_loss: 1.31549335, g_loss: 0.86919355\n",
      "Step: [2148] d_loss: 1.33058333, g_loss: 0.82575905\n",
      "Step: [2149] d_loss: 1.32815909, g_loss: 0.83265758\n",
      "Step: [2150] d_loss: 1.30902624, g_loss: 0.84500003\n",
      "Step: [2151] d_loss: 1.32707953, g_loss: 0.81891334\n",
      "Step: [2152] d_loss: 1.30672193, g_loss: 0.80993736\n",
      "Step: [2153] d_loss: 1.30529213, g_loss: 0.82800508\n",
      "Step: [2154] d_loss: 1.29823256, g_loss: 0.84657598\n",
      "Step: [2155] d_loss: 1.31483376, g_loss: 0.85193211\n",
      "Step: [2156] d_loss: 1.31131268, g_loss: 0.80738461\n",
      "Step: [2157] d_loss: 1.31424284, g_loss: 0.81699920\n",
      "Step: [2158] d_loss: 1.32742286, g_loss: 0.85231411\n",
      "Step: [2159] d_loss: 1.31922066, g_loss: 0.80304676\n",
      "Step: [2160] d_loss: 1.30419087, g_loss: 0.85508251\n",
      "Step: [2161] d_loss: 1.29675436, g_loss: 0.86680162\n",
      "Step: [2162] d_loss: 1.27919090, g_loss: 0.86912459\n",
      "Step: [2163] d_loss: 1.31514835, g_loss: 0.81576699\n",
      "Step: [2164] d_loss: 1.26565802, g_loss: 0.85202026\n",
      "Step: [2165] d_loss: 1.28477502, g_loss: 0.84437215\n",
      "Step: [2166] d_loss: 1.28054583, g_loss: 0.84734708\n",
      "Step: [2167] d_loss: 1.33121371, g_loss: 0.80690169\n",
      "Step: [2168] d_loss: 1.31162751, g_loss: 0.83381331\n",
      "Step: [2169] d_loss: 1.29065073, g_loss: 0.84912217\n",
      "Step: [2170] d_loss: 1.27961659, g_loss: 0.84278154\n",
      "Step: [2171] d_loss: 1.32668447, g_loss: 0.81091410\n",
      "Step: [2172] d_loss: 1.29836571, g_loss: 0.80733746\n",
      "Step: [2173] d_loss: 1.34336209, g_loss: 0.81943280\n",
      "Step: [2174] d_loss: 1.27366757, g_loss: 0.85818088\n",
      "Step: [2175] d_loss: 1.25582504, g_loss: 0.87186694\n",
      "Step: [2176] d_loss: 1.29551053, g_loss: 0.86799586\n",
      "Step: [2177] d_loss: 1.26871157, g_loss: 0.82185680\n",
      "Step: [2178] d_loss: 1.27027583, g_loss: 0.85252213\n",
      "Step: [2179] d_loss: 1.28461492, g_loss: 0.81972140\n",
      "Step: [2180] d_loss: 1.34042931, g_loss: 0.79313993\n",
      "Step: [2181] d_loss: 1.31622624, g_loss: 0.81490350\n",
      "Step: [2182] d_loss: 1.29340804, g_loss: 0.85493100\n",
      "Step: [2183] d_loss: 1.32414412, g_loss: 0.83079404\n",
      "Step: [2184] d_loss: 1.22480321, g_loss: 0.87854338\n",
      "Step: [2185] d_loss: 1.27353930, g_loss: 0.85413682\n",
      "Step: [2186] d_loss: 1.25397158, g_loss: 0.84950101\n",
      "Step: [2187] d_loss: 1.31503808, g_loss: 0.84031385\n",
      "Step: [2188] d_loss: 1.29693234, g_loss: 0.84208536\n",
      "Step: [2189] d_loss: 1.30447400, g_loss: 0.83275902\n",
      "Step: [2190] d_loss: 1.27726161, g_loss: 0.83169281\n",
      "Step: [2191] d_loss: 1.30770922, g_loss: 0.83480215\n",
      "Step: [2192] d_loss: 1.31806493, g_loss: 0.80842727\n",
      "Step: [2193] d_loss: 1.25343490, g_loss: 0.85257328\n",
      "Step: [2194] d_loss: 1.33589089, g_loss: 0.80390573\n",
      "Step: [2195] d_loss: 1.24991798, g_loss: 0.86485136\n",
      "Step: [2196] d_loss: 1.30591536, g_loss: 0.86330253\n",
      "Step: [2197] d_loss: 1.32297802, g_loss: 0.86390144\n",
      "Step: [2198] d_loss: 1.31958079, g_loss: 0.83632165\n",
      "Step: [2199] d_loss: 1.28986907, g_loss: 0.83786917\n",
      "Step: [2200] d_loss: 1.30572772, g_loss: 0.81484604\n",
      "Step: [2201] d_loss: 1.26800382, g_loss: 0.85040551\n",
      "Step: [2202] d_loss: 1.34898567, g_loss: 0.79210758\n",
      "Step: [2203] d_loss: 1.29185140, g_loss: 0.82056791\n",
      "Step: [2204] d_loss: 1.27316952, g_loss: 0.85624075\n",
      "Step: [2205] d_loss: 1.32446527, g_loss: 0.79754424\n",
      "Step: [2206] d_loss: 1.31116307, g_loss: 0.82909763\n",
      "Step: [2207] d_loss: 1.33439684, g_loss: 0.80294681\n",
      "Step: [2208] d_loss: 1.32370234, g_loss: 0.79921353\n",
      "Step: [2209] d_loss: 1.29335701, g_loss: 0.84096241\n",
      "Step: [2210] d_loss: 1.34636199, g_loss: 0.81687045\n",
      "Step: [2211] d_loss: 1.30312145, g_loss: 0.82700980\n",
      "Step: [2212] d_loss: 1.34830058, g_loss: 0.86176765\n",
      "Step: [2213] d_loss: 1.34866679, g_loss: 0.87448597\n",
      "Step: [2214] d_loss: 1.35648441, g_loss: 0.85769188\n",
      "Step: [2215] d_loss: 1.31622267, g_loss: 0.85999537\n",
      "Step: [2216] d_loss: 1.33717799, g_loss: 0.84635812\n",
      "Step: [2217] d_loss: 1.30171645, g_loss: 0.82530200\n",
      "Step: [2218] d_loss: 1.28537250, g_loss: 0.82803237\n",
      "Step: [2219] d_loss: 1.32204223, g_loss: 0.81772614\n",
      "Step: [2220] d_loss: 1.34812772, g_loss: 0.82127190\n",
      "Step: [2221] d_loss: 1.29012883, g_loss: 0.82414252\n",
      "Step: [2222] d_loss: 1.31468892, g_loss: 0.86700845\n",
      "Step: [2223] d_loss: 1.31904900, g_loss: 0.86496580\n",
      "Step: [2224] d_loss: 1.31623816, g_loss: 0.83861315\n",
      "Step: [2225] d_loss: 1.27917123, g_loss: 0.83476597\n",
      "Step: [2226] d_loss: 1.34206665, g_loss: 0.82208842\n",
      "Step: [2227] d_loss: 1.31316853, g_loss: 0.82410181\n",
      "Step: [2228] d_loss: 1.32147896, g_loss: 0.84916174\n",
      "Step: [2229] d_loss: 1.31963158, g_loss: 0.87037867\n",
      "Step: [2230] d_loss: 1.28596473, g_loss: 0.86687768\n",
      "Step: [2231] d_loss: 1.29214084, g_loss: 0.83644193\n",
      "Step: [2232] d_loss: 1.30994248, g_loss: 0.84565347\n",
      "Step: [2233] d_loss: 1.31437540, g_loss: 0.83706212\n",
      "Step: [2234] d_loss: 1.30087876, g_loss: 0.83039343\n",
      "Step: [2235] d_loss: 1.26927626, g_loss: 0.85770148\n",
      "Step: [2236] d_loss: 1.28189611, g_loss: 0.83491755\n",
      "Step: [2237] d_loss: 1.32973087, g_loss: 0.80354822\n",
      "Step: [2238] d_loss: 1.27890611, g_loss: 0.85819596\n",
      "Step: [2239] d_loss: 1.27123499, g_loss: 0.90244043\n",
      "Step: [2240] d_loss: 1.32220840, g_loss: 0.82444143\n",
      "Step: [2241] d_loss: 1.29082322, g_loss: 0.82848132\n",
      "Step: [2242] d_loss: 1.30985379, g_loss: 0.82679492\n",
      "Step: [2243] d_loss: 1.29309344, g_loss: 0.80467188\n",
      "Step: [2244] d_loss: 1.27513659, g_loss: 0.82566547\n",
      "Step: [2245] d_loss: 1.30381453, g_loss: 0.81927967\n",
      "Step: [2246] d_loss: 1.30985534, g_loss: 0.85433310\n",
      "Step: [2247] d_loss: 1.33383369, g_loss: 0.82154894\n",
      "Step: [2248] d_loss: 1.32846570, g_loss: 0.82872581\n",
      "Step: [2249] d_loss: 1.31345832, g_loss: 0.84405386\n",
      "Step: [2250] d_loss: 1.36175513, g_loss: 0.82410681\n",
      "Step: [2251] d_loss: 1.29786384, g_loss: 0.82797182\n",
      "Step: [2252] d_loss: 1.35112321, g_loss: 0.79514021\n",
      "Step: [2253] d_loss: 1.28666377, g_loss: 0.84967756\n",
      "Step: [2254] d_loss: 1.29208815, g_loss: 0.83934295\n",
      "Step: [2255] d_loss: 1.28564763, g_loss: 0.83829856\n",
      "Step: [2256] d_loss: 1.29848337, g_loss: 0.83947420\n",
      "Step: [2257] d_loss: 1.27778769, g_loss: 0.84661943\n",
      "Step: [2258] d_loss: 1.31697237, g_loss: 0.81808102\n",
      "Step: [2259] d_loss: 1.29139757, g_loss: 0.86111492\n",
      "Step: [2260] d_loss: 1.27309191, g_loss: 0.85791767\n",
      "Step: [2261] d_loss: 1.27758634, g_loss: 0.86411697\n",
      "Step: [2262] d_loss: 1.28210783, g_loss: 0.87177575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2263] d_loss: 1.30796528, g_loss: 0.82890236\n",
      "Step: [2264] d_loss: 1.31077957, g_loss: 0.83751166\n",
      "Step: [2265] d_loss: 1.30676222, g_loss: 0.84549022\n",
      "Step: [2266] d_loss: 1.29145718, g_loss: 0.83690858\n",
      "Step: [2267] d_loss: 1.29790390, g_loss: 0.83645010\n",
      "Step: [2268] d_loss: 1.29485679, g_loss: 0.84560698\n",
      "Step: [2269] d_loss: 1.33460009, g_loss: 0.84149981\n",
      "Step: [2270] d_loss: 1.32982588, g_loss: 0.83727765\n",
      "Step: [2271] d_loss: 1.30272150, g_loss: 0.85106939\n",
      "Step: [2272] d_loss: 1.26225090, g_loss: 0.84279239\n",
      "Step: [2273] d_loss: 1.26017833, g_loss: 0.84903693\n",
      "Step: [2274] d_loss: 1.28577888, g_loss: 0.85199016\n",
      "Step: [2275] d_loss: 1.30112672, g_loss: 0.81627238\n",
      "Step: [2276] d_loss: 1.29327261, g_loss: 0.84928221\n",
      "Step: [2277] d_loss: 1.33524573, g_loss: 0.79723608\n",
      "Step: [2278] d_loss: 1.31551456, g_loss: 0.81699550\n",
      "Step: [2279] d_loss: 1.30063415, g_loss: 0.84161091\n",
      "Step: [2280] d_loss: 1.29460526, g_loss: 0.86043489\n",
      "Step: [2281] d_loss: 1.36440229, g_loss: 0.83170348\n",
      "Step: [2282] d_loss: 1.28592920, g_loss: 0.86116207\n",
      "Step: [2283] d_loss: 1.30759704, g_loss: 0.83779931\n",
      "Step: [2284] d_loss: 1.29722440, g_loss: 0.84596956\n",
      "Step: [2285] d_loss: 1.31792843, g_loss: 0.87365067\n",
      "Step: [2286] d_loss: 1.28385592, g_loss: 0.83182395\n",
      "Step: [2287] d_loss: 1.31932247, g_loss: 0.81253594\n",
      "Step: [2288] d_loss: 1.30189967, g_loss: 0.80413461\n",
      "Step: [2289] d_loss: 1.30005085, g_loss: 0.83617830\n",
      "Step: [2290] d_loss: 1.30091262, g_loss: 0.84214103\n",
      "Step: [2291] d_loss: 1.30932736, g_loss: 0.87913084\n",
      "Step: [2292] d_loss: 1.34004116, g_loss: 0.84022158\n",
      "Step: [2293] d_loss: 1.31055474, g_loss: 0.86005145\n",
      "Step: [2294] d_loss: 1.28088534, g_loss: 0.85063267\n",
      "Step: [2295] d_loss: 1.28808260, g_loss: 0.82107681\n",
      "Step: [2296] d_loss: 1.31032979, g_loss: 0.79572237\n",
      "Step: [2297] d_loss: 1.30374146, g_loss: 0.82149297\n",
      "Step: [2298] d_loss: 1.36136627, g_loss: 0.81463528\n",
      "Step: [2299] d_loss: 1.28451371, g_loss: 0.85346544\n",
      "Step: [2300] d_loss: 1.29517972, g_loss: 0.81905609\n",
      "Step: [2301] d_loss: 1.30662799, g_loss: 0.85300535\n",
      "Step: [2302] d_loss: 1.33620584, g_loss: 0.81999213\n",
      "Step: [2303] d_loss: 1.32811034, g_loss: 0.82024348\n",
      "Step: [2304] d_loss: 1.29645252, g_loss: 0.83565092\n",
      "Step: [2305] d_loss: 1.26382136, g_loss: 0.85424203\n",
      "Step: [2306] d_loss: 1.34819508, g_loss: 0.81635439\n",
      "Step: [2307] d_loss: 1.30541015, g_loss: 0.82139504\n",
      "Step: [2308] d_loss: 1.28586531, g_loss: 0.82130039\n",
      "Step: [2309] d_loss: 1.32464743, g_loss: 0.83909053\n",
      "Step: [2310] d_loss: 1.30146074, g_loss: 0.84193993\n",
      "Step: [2311] d_loss: 1.31536913, g_loss: 0.83359283\n",
      "Step: [2312] d_loss: 1.31816363, g_loss: 0.83260167\n",
      "Step: [2313] d_loss: 1.31174469, g_loss: 0.82290250\n",
      "Step: [2314] d_loss: 1.33516777, g_loss: 0.81374949\n",
      "Step: [2315] d_loss: 1.29262972, g_loss: 0.85534805\n",
      "Step: [2316] d_loss: 1.26340222, g_loss: 0.84301400\n",
      "Step: [2317] d_loss: 1.30158997, g_loss: 0.82430124\n",
      "Step: [2318] d_loss: 1.27626801, g_loss: 0.82646692\n",
      "Step: [2319] d_loss: 1.28929758, g_loss: 0.85833395\n",
      "Step: [2320] d_loss: 1.28426051, g_loss: 0.83369547\n",
      "Step: [2321] d_loss: 1.30438018, g_loss: 0.82860219\n",
      "Step: [2322] d_loss: 1.34501386, g_loss: 0.82617295\n",
      "Step: [2323] d_loss: 1.26766551, g_loss: 0.84776771\n",
      "Step: [2324] d_loss: 1.28076816, g_loss: 0.83807302\n",
      "Step: [2325] d_loss: 1.28216815, g_loss: 0.86046892\n",
      "Step: [2326] d_loss: 1.31069016, g_loss: 0.85206091\n",
      "Step: [2327] d_loss: 1.33782673, g_loss: 0.80323291\n",
      "Step: [2328] d_loss: 1.31018722, g_loss: 0.82446235\n",
      "Step: [2329] d_loss: 1.29786754, g_loss: 0.80597460\n",
      "Step: [2330] d_loss: 1.25263453, g_loss: 0.87199128\n",
      "Step: [2331] d_loss: 1.30410194, g_loss: 0.84906113\n",
      "Step: [2332] d_loss: 1.29238629, g_loss: 0.84037924\n",
      "Step: [2333] d_loss: 1.30610788, g_loss: 0.83409506\n",
      "Step: [2334] d_loss: 1.30672264, g_loss: 0.84697598\n",
      "Step: [2335] d_loss: 1.30849063, g_loss: 0.85401338\n",
      "Step: [2336] d_loss: 1.25218451, g_loss: 0.87289876\n",
      "Step: [2337] d_loss: 1.27318215, g_loss: 0.87843132\n",
      "Step: [2338] d_loss: 1.33764577, g_loss: 0.83512253\n",
      "Step: [2339] d_loss: 1.30941391, g_loss: 0.86966252\n",
      "Step: [2340] d_loss: 1.33905482, g_loss: 0.84077305\n",
      "Step: [2341] d_loss: 1.28772390, g_loss: 0.84983677\n",
      "Step: [2342] d_loss: 1.30196655, g_loss: 0.79869390\n",
      "Step: [2343] d_loss: 1.32304633, g_loss: 0.82353067\n",
      "Step: [2344] d_loss: 1.30036807, g_loss: 0.85841292\n",
      "Step: [2345] d_loss: 1.30981648, g_loss: 0.83597422\n",
      "Step: [2346] d_loss: 1.30611897, g_loss: 0.84031588\n",
      "Step: [2347] d_loss: 1.28827214, g_loss: 0.83974826\n",
      "Step: [2348] d_loss: 1.27869844, g_loss: 0.85019791\n",
      "Step: [2349] d_loss: 1.34182811, g_loss: 0.82223231\n",
      "Step: [2350] d_loss: 1.31672835, g_loss: 0.80762923\n",
      "Step: [2351] d_loss: 1.29061580, g_loss: 0.83855033\n",
      "Step: [2352] d_loss: 1.25509715, g_loss: 0.85158241\n",
      "Step: [2353] d_loss: 1.28805900, g_loss: 0.83521235\n",
      "Step: [2354] d_loss: 1.25902653, g_loss: 0.87376380\n",
      "Step: [2355] d_loss: 1.29676783, g_loss: 0.85417211\n",
      "Step: [2356] d_loss: 1.31073856, g_loss: 0.84041792\n",
      "Step: [2357] d_loss: 1.30734038, g_loss: 0.82735127\n",
      "Step: [2358] d_loss: 1.28698742, g_loss: 0.83988583\n",
      "Step: [2359] d_loss: 1.27516747, g_loss: 0.85344017\n",
      "Step: [2360] d_loss: 1.33096433, g_loss: 0.82208562\n",
      "Step: [2361] d_loss: 1.30895162, g_loss: 0.82177275\n",
      "Step: [2362] d_loss: 1.33582497, g_loss: 0.83725983\n",
      "Step: [2363] d_loss: 1.29863286, g_loss: 0.83117354\n",
      "Step: [2364] d_loss: 1.34765434, g_loss: 0.82267797\n",
      "Step: [2365] d_loss: 1.33034801, g_loss: 0.84624350\n",
      "Step: [2366] d_loss: 1.30270278, g_loss: 0.83700013\n",
      "Step: [2367] d_loss: 1.31204200, g_loss: 0.81095862\n",
      "Step: [2368] d_loss: 1.31732881, g_loss: 0.86657143\n",
      "Step: [2369] d_loss: 1.28595424, g_loss: 0.83344018\n",
      "Step: [2370] d_loss: 1.30218625, g_loss: 0.83744043\n",
      "Step: [2371] d_loss: 1.24171424, g_loss: 0.88036764\n",
      "Step: [2372] d_loss: 1.35047626, g_loss: 0.82258970\n",
      "Step: [2373] d_loss: 1.30757236, g_loss: 0.83455002\n",
      "Step: [2374] d_loss: 1.30925417, g_loss: 0.79400897\n",
      "Step: [2375] d_loss: 1.29032421, g_loss: 0.82834154\n",
      "Step: [2376] d_loss: 1.29792154, g_loss: 0.82832062\n",
      "Step: [2377] d_loss: 1.34150362, g_loss: 0.80999708\n",
      "Step: [2378] d_loss: 1.32514381, g_loss: 0.83013052\n",
      "Step: [2379] d_loss: 1.32061756, g_loss: 0.83244348\n",
      "Step: [2380] d_loss: 1.32990420, g_loss: 0.84290349\n",
      "Step: [2381] d_loss: 1.26366377, g_loss: 0.84366250\n",
      "Step: [2382] d_loss: 1.27288866, g_loss: 0.84044218\n",
      "Step: [2383] d_loss: 1.32656157, g_loss: 0.80741751\n",
      "Step: [2384] d_loss: 1.25027966, g_loss: 0.84194613\n",
      "Step: [2385] d_loss: 1.31143260, g_loss: 0.82068712\n",
      "Step: [2386] d_loss: 1.30646515, g_loss: 0.83269346\n",
      "Step: [2387] d_loss: 1.29485703, g_loss: 0.86580318\n",
      "Step: [2388] d_loss: 1.30161238, g_loss: 0.84144932\n",
      "Step: [2389] d_loss: 1.27506900, g_loss: 0.86526680\n",
      "Step: [2390] d_loss: 1.29637551, g_loss: 0.83444810\n",
      "Step: [2391] d_loss: 1.27697325, g_loss: 0.81103826\n",
      "Step: [2392] d_loss: 1.27956009, g_loss: 0.84018439\n",
      "Step: [2393] d_loss: 1.30187845, g_loss: 0.82232875\n",
      "Step: [2394] d_loss: 1.32546592, g_loss: 0.81875181\n",
      "Step: [2395] d_loss: 1.28447938, g_loss: 0.84116805\n",
      "Step: [2396] d_loss: 1.32313597, g_loss: 0.82938236\n",
      "Step: [2397] d_loss: 1.29809058, g_loss: 0.84788704\n",
      "Step: [2398] d_loss: 1.31280971, g_loss: 0.85135031\n",
      "Step: [2399] d_loss: 1.28802466, g_loss: 0.84939730\n",
      "Step: [2400] d_loss: 1.28277028, g_loss: 0.86943412\n",
      "Step: [2401] d_loss: 1.33970952, g_loss: 0.82338810\n",
      "Step: [2402] d_loss: 1.35012949, g_loss: 0.79997391\n",
      "Step: [2403] d_loss: 1.27922487, g_loss: 0.84460962\n",
      "Step: [2404] d_loss: 1.33607435, g_loss: 0.80784440\n",
      "Step: [2405] d_loss: 1.27898109, g_loss: 0.85487318\n",
      "Step: [2406] d_loss: 1.25439847, g_loss: 0.88972521\n",
      "Step: [2407] d_loss: 1.25646162, g_loss: 0.83912212\n",
      "Step: [2408] d_loss: 1.34380865, g_loss: 0.83072472\n",
      "Step: [2409] d_loss: 1.30871367, g_loss: 0.80735892\n",
      "Step: [2410] d_loss: 1.24781120, g_loss: 0.86761367\n",
      "Step: [2411] d_loss: 1.30360723, g_loss: 0.84482950\n",
      "Step: [2412] d_loss: 1.34108090, g_loss: 0.82435036\n",
      "Step: [2413] d_loss: 1.31547821, g_loss: 0.81828320\n",
      "Step: [2414] d_loss: 1.26939750, g_loss: 0.86289603\n",
      "Step: [2415] d_loss: 1.30361390, g_loss: 0.81399000\n",
      "Step: [2416] d_loss: 1.29960370, g_loss: 0.84401500\n",
      "Step: [2417] d_loss: 1.29806519, g_loss: 0.82946926\n",
      "Step: [2418] d_loss: 1.29598451, g_loss: 0.83406037\n",
      "Step: [2419] d_loss: 1.29080021, g_loss: 0.85349596\n",
      "Step: [2420] d_loss: 1.33097577, g_loss: 0.83124691\n",
      "Step: [2421] d_loss: 1.27631342, g_loss: 0.82583207\n",
      "Step: [2422] d_loss: 1.32604694, g_loss: 0.84038317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2423] d_loss: 1.30777860, g_loss: 0.84274733\n",
      "Step: [2424] d_loss: 1.34110022, g_loss: 0.80361855\n",
      "Step: [2425] d_loss: 1.28730547, g_loss: 0.84306836\n",
      "Step: [2426] d_loss: 1.28903317, g_loss: 0.82775432\n",
      "Step: [2427] d_loss: 1.31643343, g_loss: 0.81890047\n",
      "Step: [2428] d_loss: 1.29618156, g_loss: 0.82200426\n",
      "Step: [2429] d_loss: 1.28507650, g_loss: 0.84486234\n",
      "Step: [2430] d_loss: 1.31797409, g_loss: 0.87513345\n",
      "Step: [2431] d_loss: 1.27199161, g_loss: 0.85785306\n",
      "Step: [2432] d_loss: 1.27883959, g_loss: 0.84806371\n",
      "Step: [2433] d_loss: 1.30218697, g_loss: 0.83031982\n",
      "Step: [2434] d_loss: 1.31373835, g_loss: 0.82450950\n",
      "Step: [2435] d_loss: 1.31097031, g_loss: 0.85326099\n",
      "Step: [2436] d_loss: 1.31193757, g_loss: 0.82164001\n",
      "Step: [2437] d_loss: 1.27189255, g_loss: 0.82372236\n",
      "Step: [2438] d_loss: 1.27302563, g_loss: 0.82211053\n",
      "Step: [2439] d_loss: 1.25647831, g_loss: 0.84505266\n",
      "Step: [2440] d_loss: 1.24888134, g_loss: 0.85833573\n",
      "Step: [2441] d_loss: 1.32124150, g_loss: 0.81076431\n",
      "Step: [2442] d_loss: 1.27522564, g_loss: 0.85206157\n",
      "Step: [2443] d_loss: 1.27782738, g_loss: 0.85836840\n",
      "Step: [2444] d_loss: 1.25647593, g_loss: 0.85221159\n",
      "Step: [2445] d_loss: 1.26780641, g_loss: 0.83107460\n",
      "Step: [2446] d_loss: 1.22747493, g_loss: 0.87009859\n",
      "Step: [2447] d_loss: 1.28336442, g_loss: 0.86140966\n",
      "Step: [2448] d_loss: 1.30419326, g_loss: 0.84383011\n",
      "Step: [2449] d_loss: 1.29029822, g_loss: 0.83180285\n",
      "Step: [2450] d_loss: 1.26970136, g_loss: 0.84804642\n",
      "Step: [2451] d_loss: 1.33636844, g_loss: 0.82641768\n",
      "Step: [2452] d_loss: 1.29518986, g_loss: 0.83510149\n",
      "Step: [2453] d_loss: 1.32342029, g_loss: 0.82996666\n",
      "Step: [2454] d_loss: 1.34016311, g_loss: 0.82912040\n",
      "Step: [2455] d_loss: 1.30197620, g_loss: 0.84779847\n",
      "Step: [2456] d_loss: 1.29939175, g_loss: 0.86991048\n",
      "Step: [2457] d_loss: 1.34072208, g_loss: 0.85067654\n",
      "Step: [2458] d_loss: 1.32836044, g_loss: 0.83599627\n",
      "Step: [2459] d_loss: 1.33858347, g_loss: 0.81998354\n",
      "Step: [2460] d_loss: 1.33110237, g_loss: 0.81426919\n",
      "Step: [2461] d_loss: 1.32183683, g_loss: 0.81041384\n",
      "Step: [2462] d_loss: 1.29815519, g_loss: 0.82696199\n",
      "Step: [2463] d_loss: 1.23695755, g_loss: 0.85214311\n",
      "Step: [2464] d_loss: 1.27929735, g_loss: 0.85092390\n",
      "Step: [2465] d_loss: 1.31332231, g_loss: 0.84770817\n",
      "Step: [2466] d_loss: 1.31327510, g_loss: 0.87367576\n",
      "Step: [2467] d_loss: 1.29584432, g_loss: 0.85179871\n",
      "Step: [2468] d_loss: 1.29401207, g_loss: 0.82524139\n",
      "Step: [2469] d_loss: 1.30560613, g_loss: 0.80856174\n",
      "Step: [2470] d_loss: 1.29950142, g_loss: 0.86454082\n",
      "Step: [2471] d_loss: 1.33114195, g_loss: 0.83610487\n",
      "Step: [2472] d_loss: 1.33126962, g_loss: 0.84764409\n",
      "Step: [2473] d_loss: 1.32024515, g_loss: 0.84219086\n",
      "Step: [2474] d_loss: 1.29757655, g_loss: 0.87595499\n",
      "Step: [2475] d_loss: 1.30180025, g_loss: 0.86100495\n",
      "Step: [2476] d_loss: 1.26986122, g_loss: 0.82663095\n",
      "Step: [2477] d_loss: 1.25423574, g_loss: 0.82915181\n",
      "Step: [2478] d_loss: 1.36802077, g_loss: 0.78106755\n",
      "Step: [2479] d_loss: 1.31556797, g_loss: 0.82172501\n",
      "Step: [2480] d_loss: 1.29795313, g_loss: 0.85841668\n",
      "Step: [2481] d_loss: 1.30623853, g_loss: 0.82952368\n",
      "Step: [2482] d_loss: 1.23049188, g_loss: 0.88015592\n",
      "Step: [2483] d_loss: 1.29470515, g_loss: 0.84279001\n",
      "Step: [2484] d_loss: 1.29860044, g_loss: 0.82785237\n",
      "Step: [2485] d_loss: 1.32751822, g_loss: 0.85023153\n",
      "Step: [2486] d_loss: 1.28573990, g_loss: 0.83898205\n",
      "Step: [2487] d_loss: 1.26488149, g_loss: 0.86089784\n",
      "Step: [2488] d_loss: 1.28943050, g_loss: 0.88284266\n",
      "Step: [2489] d_loss: 1.33610892, g_loss: 0.83016145\n",
      "Step: [2490] d_loss: 1.32275653, g_loss: 0.82324076\n",
      "Step: [2491] d_loss: 1.28360891, g_loss: 0.88323545\n",
      "Step: [2492] d_loss: 1.36330187, g_loss: 0.84261972\n",
      "Step: [2493] d_loss: 1.30446792, g_loss: 0.84482706\n",
      "Step: [2494] d_loss: 1.26198363, g_loss: 0.83690578\n",
      "Step: [2495] d_loss: 1.26076984, g_loss: 0.83748400\n",
      "Step: [2496] d_loss: 1.31492364, g_loss: 0.83361244\n",
      "Step: [2497] d_loss: 1.28105330, g_loss: 0.85470223\n",
      "Step: [2498] d_loss: 1.30440485, g_loss: 0.84352601\n",
      "Step: [2499] d_loss: 1.31302190, g_loss: 0.84143484\n",
      "Step: [2500] d_loss: 1.31943142, g_loss: 0.84100115\n",
      "Step: [2501] d_loss: 1.31089997, g_loss: 0.81292522\n",
      "Step: [2502] d_loss: 1.33870029, g_loss: 0.80981743\n",
      "Step: [2503] d_loss: 1.29452705, g_loss: 0.85562235\n",
      "Step: [2504] d_loss: 1.28779113, g_loss: 0.88986665\n",
      "Step: [2505] d_loss: 1.29892468, g_loss: 0.83315581\n",
      "Step: [2506] d_loss: 1.29244268, g_loss: 0.83867514\n",
      "Step: [2507] d_loss: 1.27442753, g_loss: 0.83385599\n",
      "Step: [2508] d_loss: 1.29617238, g_loss: 0.85089684\n",
      "Step: [2509] d_loss: 1.30722618, g_loss: 0.81472993\n",
      "Step: [2510] d_loss: 1.33740306, g_loss: 0.79798317\n",
      "Step: [2511] d_loss: 1.32245827, g_loss: 0.83549041\n",
      "Step: [2512] d_loss: 1.28653169, g_loss: 0.85062665\n",
      "Step: [2513] d_loss: 1.28912342, g_loss: 0.84037596\n",
      "Step: [2514] d_loss: 1.30079818, g_loss: 0.85802031\n",
      "Step: [2515] d_loss: 1.29792309, g_loss: 0.85825419\n",
      "Step: [2516] d_loss: 1.28785932, g_loss: 0.85091841\n",
      "Step: [2517] d_loss: 1.30207229, g_loss: 0.85579407\n",
      "Step: [2518] d_loss: 1.26840782, g_loss: 0.85782361\n",
      "Step: [2519] d_loss: 1.30531275, g_loss: 0.83371758\n",
      "Step: [2520] d_loss: 1.29488373, g_loss: 0.85233068\n",
      "Step: [2521] d_loss: 1.28841567, g_loss: 0.86226559\n",
      "Step: [2522] d_loss: 1.32350242, g_loss: 0.79275918\n",
      "Step: [2523] d_loss: 1.29214454, g_loss: 0.82822901\n",
      "Step: [2524] d_loss: 1.28234577, g_loss: 0.85229683\n",
      "Step: [2525] d_loss: 1.24872530, g_loss: 0.88201439\n",
      "Step: [2526] d_loss: 1.28824377, g_loss: 0.86515754\n",
      "Step: [2527] d_loss: 1.27530813, g_loss: 0.84485275\n",
      "Step: [2528] d_loss: 1.26806760, g_loss: 0.83186412\n",
      "Step: [2529] d_loss: 1.29389966, g_loss: 0.82693309\n",
      "Step: [2530] d_loss: 1.31561089, g_loss: 0.82061106\n",
      "Step: [2531] d_loss: 1.29172695, g_loss: 0.85357773\n",
      "Step: [2532] d_loss: 1.26432347, g_loss: 0.85300291\n",
      "Step: [2533] d_loss: 1.29017711, g_loss: 0.89426392\n",
      "Step: [2534] d_loss: 1.28773010, g_loss: 0.89586318\n",
      "Step: [2535] d_loss: 1.32724929, g_loss: 0.80846477\n",
      "Step: [2536] d_loss: 1.30869341, g_loss: 0.79847229\n",
      "Step: [2537] d_loss: 1.31930566, g_loss: 0.81545722\n",
      "Step: [2538] d_loss: 1.29484737, g_loss: 0.81727767\n",
      "Step: [2539] d_loss: 1.29852486, g_loss: 0.85983157\n",
      "Step: [2540] d_loss: 1.27501917, g_loss: 0.85461104\n",
      "Step: [2541] d_loss: 1.30194032, g_loss: 0.84167135\n",
      "Step: [2542] d_loss: 1.30868435, g_loss: 0.83515620\n",
      "Step: [2543] d_loss: 1.29000568, g_loss: 0.84076899\n",
      "Step: [2544] d_loss: 1.28793156, g_loss: 0.84950423\n",
      "Step: [2545] d_loss: 1.30755401, g_loss: 0.80867004\n",
      "Step: [2546] d_loss: 1.25788081, g_loss: 0.86259556\n",
      "Step: [2547] d_loss: 1.29247546, g_loss: 0.85910273\n",
      "Step: [2548] d_loss: 1.31102598, g_loss: 0.85087425\n",
      "Step: [2549] d_loss: 1.29297352, g_loss: 0.85336590\n",
      "Step: [2550] d_loss: 1.33083701, g_loss: 0.80079639\n",
      "Step: [2551] d_loss: 1.25319171, g_loss: 0.84424138\n",
      "Step: [2552] d_loss: 1.32716680, g_loss: 0.80048805\n",
      "Step: [2553] d_loss: 1.24648178, g_loss: 0.84658766\n",
      "Step: [2554] d_loss: 1.33369565, g_loss: 0.83421016\n",
      "Step: [2555] d_loss: 1.29198265, g_loss: 0.85857904\n",
      "Step: [2556] d_loss: 1.29300237, g_loss: 0.84957528\n",
      "Step: [2557] d_loss: 1.27360916, g_loss: 0.83917177\n",
      "Step: [2558] d_loss: 1.28980088, g_loss: 0.83134937\n",
      "Step: [2559] d_loss: 1.33547068, g_loss: 0.81699920\n",
      "Step: [2560] d_loss: 1.33508599, g_loss: 0.84130883\n",
      "Step: [2561] d_loss: 1.33179998, g_loss: 0.82935929\n",
      "Step: [2562] d_loss: 1.29933202, g_loss: 0.85319048\n",
      "Step: [2563] d_loss: 1.29468429, g_loss: 0.85322112\n",
      "Step: [2564] d_loss: 1.32150388, g_loss: 0.83659291\n",
      "Step: [2565] d_loss: 1.29109907, g_loss: 0.86657768\n",
      "Step: [2566] d_loss: 1.32410038, g_loss: 0.81120002\n",
      "Step: [2567] d_loss: 1.31185603, g_loss: 0.83486104\n",
      "Step: [2568] d_loss: 1.32624578, g_loss: 0.87083429\n",
      "Step: [2569] d_loss: 1.32335043, g_loss: 0.80789596\n",
      "Step: [2570] d_loss: 1.34825420, g_loss: 0.81121325\n",
      "Step: [2571] d_loss: 1.27606976, g_loss: 0.85313773\n",
      "Step: [2572] d_loss: 1.25441396, g_loss: 0.83001173\n",
      "Step: [2573] d_loss: 1.26286948, g_loss: 0.83554840\n",
      "Step: [2574] d_loss: 1.25667882, g_loss: 0.84665400\n",
      "Step: [2575] d_loss: 1.33489382, g_loss: 0.85648835\n",
      "Step: [2576] d_loss: 1.28217363, g_loss: 0.86294168\n",
      "Step: [2577] d_loss: 1.25349021, g_loss: 0.90851831\n",
      "Step: [2578] d_loss: 1.27523255, g_loss: 0.87145370\n",
      "Step: [2579] d_loss: 1.26316524, g_loss: 0.87608016\n",
      "Step: [2580] d_loss: 1.22952616, g_loss: 0.85978818\n",
      "Step: [2581] d_loss: 1.24379587, g_loss: 0.88752329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2582] d_loss: 1.27764463, g_loss: 0.84578031\n",
      "Step: [2583] d_loss: 1.24065387, g_loss: 0.83704919\n",
      "Step: [2584] d_loss: 1.28176475, g_loss: 0.84575289\n",
      "Step: [2585] d_loss: 1.26648116, g_loss: 0.87585998\n",
      "Step: [2586] d_loss: 1.29277539, g_loss: 0.86445701\n",
      "Step: [2587] d_loss: 1.31399155, g_loss: 0.83730745\n",
      "Step: [2588] d_loss: 1.29247165, g_loss: 0.86935061\n",
      "Step: [2589] d_loss: 1.27935314, g_loss: 0.85674077\n",
      "Step: [2590] d_loss: 1.27499771, g_loss: 0.91359621\n",
      "Step: [2591] d_loss: 1.29193747, g_loss: 0.84901226\n",
      "Step: [2592] d_loss: 1.29612994, g_loss: 0.82737410\n",
      "Step: [2593] d_loss: 1.29541838, g_loss: 0.79622936\n",
      "Step: [2594] d_loss: 1.24678922, g_loss: 0.85546440\n",
      "Step: [2595] d_loss: 1.27906847, g_loss: 0.87022328\n",
      "Step: [2596] d_loss: 1.32053232, g_loss: 0.87220049\n",
      "Step: [2597] d_loss: 1.23738945, g_loss: 0.86848038\n",
      "Step: [2598] d_loss: 1.28834152, g_loss: 0.85549688\n",
      "Step: [2599] d_loss: 1.29514027, g_loss: 0.85834086\n",
      "Step: [2600] d_loss: 1.32883286, g_loss: 0.80864978\n",
      "Step: [2601] d_loss: 1.28725266, g_loss: 0.85813868\n",
      "Step: [2602] d_loss: 1.26656902, g_loss: 0.89520872\n",
      "Step: [2603] d_loss: 1.31629503, g_loss: 0.84610939\n",
      "Step: [2604] d_loss: 1.32147229, g_loss: 0.86117494\n",
      "Step: [2605] d_loss: 1.32036781, g_loss: 0.81577611\n",
      "Step: [2606] d_loss: 1.27143085, g_loss: 0.86434299\n",
      "Step: [2607] d_loss: 1.33893788, g_loss: 0.82503098\n",
      "Step: [2608] d_loss: 1.25315034, g_loss: 0.88659155\n",
      "Step: [2609] d_loss: 1.30544889, g_loss: 0.83031631\n",
      "Step: [2610] d_loss: 1.29402590, g_loss: 0.83831394\n",
      "Step: [2611] d_loss: 1.33158493, g_loss: 0.82578641\n",
      "Step: [2612] d_loss: 1.26771665, g_loss: 0.87528861\n",
      "Step: [2613] d_loss: 1.29433537, g_loss: 0.87669128\n",
      "Step: [2614] d_loss: 1.30784702, g_loss: 0.85792309\n",
      "Step: [2615] d_loss: 1.26118731, g_loss: 0.88202012\n",
      "Step: [2616] d_loss: 1.28369582, g_loss: 0.85463160\n",
      "Step: [2617] d_loss: 1.24704289, g_loss: 0.87551010\n",
      "Step: [2618] d_loss: 1.23521566, g_loss: 0.89547396\n",
      "Step: [2619] d_loss: 1.30350626, g_loss: 0.84774506\n",
      "Step: [2620] d_loss: 1.26787889, g_loss: 0.87670732\n",
      "Step: [2621] d_loss: 1.27621317, g_loss: 0.88570380\n",
      "Step: [2622] d_loss: 1.33803701, g_loss: 0.82605970\n",
      "Step: [2623] d_loss: 1.25827289, g_loss: 0.88439262\n",
      "Step: [2624] d_loss: 1.31700087, g_loss: 0.84944582\n",
      "Step: [2625] d_loss: 1.27284300, g_loss: 0.87388539\n",
      "Step: [2626] d_loss: 1.25361228, g_loss: 0.85891610\n",
      "Step: [2627] d_loss: 1.23080993, g_loss: 0.84202027\n",
      "Step: [2628] d_loss: 1.29439139, g_loss: 0.83702928\n",
      "Step: [2629] d_loss: 1.26553559, g_loss: 0.84785992\n",
      "Step: [2630] d_loss: 1.26181257, g_loss: 0.85940576\n",
      "Step: [2631] d_loss: 1.24457502, g_loss: 0.87866390\n",
      "Step: [2632] d_loss: 1.22033906, g_loss: 0.86650133\n",
      "Step: [2633] d_loss: 1.27306926, g_loss: 0.84593540\n",
      "Step: [2634] d_loss: 1.30533421, g_loss: 0.86841196\n",
      "Step: [2635] d_loss: 1.30689013, g_loss: 0.88999838\n",
      "Step: [2636] d_loss: 1.31043899, g_loss: 0.85462409\n",
      "Step: [2637] d_loss: 1.27663827, g_loss: 0.86042178\n",
      "Step: [2638] d_loss: 1.23095381, g_loss: 0.85897177\n",
      "Step: [2639] d_loss: 1.28253627, g_loss: 0.85484934\n",
      "Step: [2640] d_loss: 1.24158049, g_loss: 0.84107208\n",
      "Step: [2641] d_loss: 1.22887731, g_loss: 0.86584234\n",
      "Step: [2642] d_loss: 1.29779696, g_loss: 0.83247077\n",
      "Step: [2643] d_loss: 1.30321646, g_loss: 0.86773813\n",
      "Step: [2644] d_loss: 1.27327883, g_loss: 0.88175476\n",
      "Step: [2645] d_loss: 1.30027974, g_loss: 0.85856354\n",
      "Step: [2646] d_loss: 1.25931239, g_loss: 0.87817591\n",
      "Step: [2647] d_loss: 1.25617695, g_loss: 0.87453574\n",
      "Step: [2648] d_loss: 1.27863574, g_loss: 0.83300990\n",
      "Step: [2649] d_loss: 1.26708353, g_loss: 0.85347170\n",
      "Step: [2650] d_loss: 1.24564981, g_loss: 0.86353362\n",
      "Step: [2651] d_loss: 1.28958654, g_loss: 0.88537264\n",
      "Step: [2652] d_loss: 1.26378226, g_loss: 0.88896394\n",
      "Step: [2653] d_loss: 1.29749322, g_loss: 0.86862636\n",
      "Step: [2654] d_loss: 1.27634645, g_loss: 0.84199959\n",
      "Step: [2655] d_loss: 1.27738523, g_loss: 0.86173189\n",
      "Step: [2656] d_loss: 1.26486182, g_loss: 0.90244067\n",
      "Step: [2657] d_loss: 1.30916810, g_loss: 0.84564877\n",
      "Step: [2658] d_loss: 1.28346789, g_loss: 0.83401608\n",
      "Step: [2659] d_loss: 1.22018456, g_loss: 0.89208031\n",
      "Step: [2660] d_loss: 1.30107391, g_loss: 0.83042639\n",
      "Step: [2661] d_loss: 1.27455115, g_loss: 0.84676099\n",
      "Step: [2662] d_loss: 1.30293608, g_loss: 0.84528804\n",
      "Step: [2663] d_loss: 1.31512940, g_loss: 0.85015655\n",
      "Step: [2664] d_loss: 1.28277159, g_loss: 0.87179476\n",
      "Step: [2665] d_loss: 1.27254343, g_loss: 0.87871510\n",
      "Step: [2666] d_loss: 1.27172565, g_loss: 0.84454191\n",
      "Step: [2667] d_loss: 1.28500891, g_loss: 0.84312129\n",
      "Step: [2668] d_loss: 1.28865969, g_loss: 0.85210085\n",
      "Step: [2669] d_loss: 1.25107050, g_loss: 0.85163373\n",
      "Step: [2670] d_loss: 1.30453146, g_loss: 0.84429586\n",
      "Step: [2671] d_loss: 1.31615710, g_loss: 0.83600247\n",
      "Step: [2672] d_loss: 1.30296183, g_loss: 0.85258698\n",
      "Step: [2673] d_loss: 1.27642441, g_loss: 0.87378114\n",
      "Step: [2674] d_loss: 1.29600215, g_loss: 0.86730075\n",
      "Step: [2675] d_loss: 1.29813850, g_loss: 0.85288167\n",
      "Step: [2676] d_loss: 1.26701808, g_loss: 0.85751516\n",
      "Step: [2677] d_loss: 1.27447033, g_loss: 0.85774058\n",
      "Step: [2678] d_loss: 1.26839185, g_loss: 0.84608126\n",
      "Step: [2679] d_loss: 1.32744491, g_loss: 0.82352662\n",
      "Step: [2680] d_loss: 1.33920276, g_loss: 0.82837212\n",
      "Step: [2681] d_loss: 1.32780743, g_loss: 0.87783915\n",
      "Step: [2682] d_loss: 1.29831016, g_loss: 0.82045794\n",
      "Step: [2683] d_loss: 1.25940704, g_loss: 0.86722445\n",
      "Step: [2684] d_loss: 1.26362574, g_loss: 0.83070028\n",
      "Step: [2685] d_loss: 1.31503212, g_loss: 0.81317914\n",
      "Step: [2686] d_loss: 1.26388121, g_loss: 0.84249580\n",
      "Step: [2687] d_loss: 1.26681995, g_loss: 0.85612345\n",
      "Step: [2688] d_loss: 1.31791186, g_loss: 0.85847449\n",
      "Step: [2689] d_loss: 1.30143571, g_loss: 0.85926294\n",
      "Step: [2690] d_loss: 1.34742117, g_loss: 0.86838698\n",
      "Step: [2691] d_loss: 1.31988120, g_loss: 0.82049018\n",
      "Step: [2692] d_loss: 1.30819511, g_loss: 0.82372653\n",
      "Step: [2693] d_loss: 1.29907334, g_loss: 0.84346569\n",
      "Step: [2694] d_loss: 1.31571662, g_loss: 0.85209435\n",
      "Step: [2695] d_loss: 1.22236419, g_loss: 0.89119732\n",
      "Step: [2696] d_loss: 1.27353048, g_loss: 0.87177664\n",
      "Step: [2697] d_loss: 1.30908990, g_loss: 0.83774304\n",
      "Step: [2698] d_loss: 1.25558305, g_loss: 0.84556711\n",
      "Step: [2699] d_loss: 1.24583387, g_loss: 0.85490823\n",
      "Step: [2700] d_loss: 1.30158484, g_loss: 0.82642478\n",
      "Step: [2701] d_loss: 1.28024697, g_loss: 0.85014588\n",
      "Step: [2702] d_loss: 1.27704287, g_loss: 0.90191233\n",
      "Step: [2703] d_loss: 1.25351858, g_loss: 0.86548984\n",
      "Step: [2704] d_loss: 1.30627918, g_loss: 0.86271942\n",
      "Step: [2705] d_loss: 1.30997944, g_loss: 0.85179973\n",
      "Step: [2706] d_loss: 1.26308107, g_loss: 0.86920261\n",
      "Step: [2707] d_loss: 1.28034782, g_loss: 0.84295481\n",
      "Step: [2708] d_loss: 1.30835533, g_loss: 0.82242101\n",
      "Step: [2709] d_loss: 1.29155791, g_loss: 0.86395413\n",
      "Step: [2710] d_loss: 1.30377340, g_loss: 0.85278529\n",
      "Step: [2711] d_loss: 1.26191533, g_loss: 0.86274326\n",
      "Step: [2712] d_loss: 1.27726042, g_loss: 0.85580254\n",
      "Step: [2713] d_loss: 1.26989222, g_loss: 0.88592666\n",
      "Step: [2714] d_loss: 1.28838718, g_loss: 0.83719575\n",
      "Step: [2715] d_loss: 1.30625224, g_loss: 0.82923251\n",
      "Step: [2716] d_loss: 1.25718892, g_loss: 0.86869812\n",
      "Step: [2717] d_loss: 1.28152061, g_loss: 0.87083340\n",
      "Step: [2718] d_loss: 1.25220108, g_loss: 0.85195994\n",
      "Step: [2719] d_loss: 1.33104360, g_loss: 0.83188486\n",
      "Step: [2720] d_loss: 1.25276875, g_loss: 0.90004396\n",
      "Step: [2721] d_loss: 1.29860413, g_loss: 0.87623990\n",
      "Step: [2722] d_loss: 1.30553854, g_loss: 0.84628534\n",
      "Step: [2723] d_loss: 1.28737044, g_loss: 0.85683036\n",
      "Step: [2724] d_loss: 1.26806045, g_loss: 0.88207638\n",
      "Step: [2725] d_loss: 1.33685946, g_loss: 0.84504032\n",
      "Step: [2726] d_loss: 1.25349617, g_loss: 0.84252095\n",
      "Step: [2727] d_loss: 1.28317332, g_loss: 0.83855069\n",
      "Step: [2728] d_loss: 1.28722930, g_loss: 0.85038972\n",
      "Step: [2729] d_loss: 1.31450796, g_loss: 0.86486083\n",
      "Step: [2730] d_loss: 1.26755309, g_loss: 0.88033819\n",
      "Step: [2731] d_loss: 1.31027114, g_loss: 0.84646422\n",
      "Step: [2732] d_loss: 1.25771523, g_loss: 0.88230431\n",
      "Step: [2733] d_loss: 1.28570938, g_loss: 0.85890687\n",
      "Step: [2734] d_loss: 1.24591994, g_loss: 0.85422373\n",
      "Step: [2735] d_loss: 1.27219212, g_loss: 0.84713554\n",
      "Step: [2736] d_loss: 1.32338703, g_loss: 0.82889807\n",
      "Step: [2737] d_loss: 1.26199174, g_loss: 0.85203099\n",
      "Step: [2738] d_loss: 1.27102625, g_loss: 0.84796858\n",
      "Step: [2739] d_loss: 1.29962599, g_loss: 0.84041595\n",
      "Step: [2740] d_loss: 1.25287056, g_loss: 0.87440258\n",
      "Step: [2741] d_loss: 1.27374768, g_loss: 0.82642436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2742] d_loss: 1.28557062, g_loss: 0.86984342\n",
      "Step: [2743] d_loss: 1.25294948, g_loss: 0.87167490\n",
      "Step: [2744] d_loss: 1.26990581, g_loss: 0.84761947\n",
      "Step: [2745] d_loss: 1.29047716, g_loss: 0.84860361\n",
      "Step: [2746] d_loss: 1.27354074, g_loss: 0.88175231\n",
      "Step: [2747] d_loss: 1.30870938, g_loss: 0.86164844\n",
      "Step: [2748] d_loss: 1.25423312, g_loss: 0.87643522\n",
      "Step: [2749] d_loss: 1.26961017, g_loss: 0.88176227\n",
      "Step: [2750] d_loss: 1.26688409, g_loss: 0.84623230\n",
      "Step: [2751] d_loss: 1.29779828, g_loss: 0.86188745\n",
      "Step: [2752] d_loss: 1.27633536, g_loss: 0.86704481\n",
      "Step: [2753] d_loss: 1.24840844, g_loss: 0.86639762\n",
      "Step: [2754] d_loss: 1.28151011, g_loss: 0.87857938\n",
      "Step: [2755] d_loss: 1.26324081, g_loss: 0.85248816\n",
      "Step: [2756] d_loss: 1.29219985, g_loss: 0.82473934\n",
      "Step: [2757] d_loss: 1.26779294, g_loss: 0.82608843\n",
      "Step: [2758] d_loss: 1.28068793, g_loss: 0.85469341\n",
      "Step: [2759] d_loss: 1.29465854, g_loss: 0.86511314\n",
      "Step: [2760] d_loss: 1.33011198, g_loss: 0.84070039\n",
      "Step: [2761] d_loss: 1.28623235, g_loss: 0.85334224\n",
      "Step: [2762] d_loss: 1.29026616, g_loss: 0.86899698\n",
      "Step: [2763] d_loss: 1.27006876, g_loss: 0.85501885\n",
      "Step: [2764] d_loss: 1.29167819, g_loss: 0.85774863\n",
      "Step: [2765] d_loss: 1.28833652, g_loss: 0.86014557\n",
      "Step: [2766] d_loss: 1.29725766, g_loss: 0.82796019\n",
      "Step: [2767] d_loss: 1.26236570, g_loss: 0.84766126\n",
      "Step: [2768] d_loss: 1.28837442, g_loss: 0.83032060\n",
      "Step: [2769] d_loss: 1.28144288, g_loss: 0.85382664\n",
      "Step: [2770] d_loss: 1.28355765, g_loss: 0.84513998\n",
      "Step: [2771] d_loss: 1.27609611, g_loss: 0.87453693\n",
      "Step: [2772] d_loss: 1.29760480, g_loss: 0.84051251\n",
      "Step: [2773] d_loss: 1.30319476, g_loss: 0.85177386\n",
      "Step: [2774] d_loss: 1.32780051, g_loss: 0.81450063\n",
      "Step: [2775] d_loss: 1.29515958, g_loss: 0.85686296\n",
      "Step: [2776] d_loss: 1.31798100, g_loss: 0.84214312\n",
      "Step: [2777] d_loss: 1.30520308, g_loss: 0.83443075\n",
      "Step: [2778] d_loss: 1.24961507, g_loss: 0.89264977\n",
      "Step: [2779] d_loss: 1.27734816, g_loss: 0.87376666\n",
      "Step: [2780] d_loss: 1.31195402, g_loss: 0.83656508\n",
      "Step: [2781] d_loss: 1.30606532, g_loss: 0.84163767\n",
      "Step: [2782] d_loss: 1.28022647, g_loss: 0.85838127\n",
      "Step: [2783] d_loss: 1.24381471, g_loss: 0.89560401\n",
      "Step: [2784] d_loss: 1.27349365, g_loss: 0.88754392\n",
      "Step: [2785] d_loss: 1.31893599, g_loss: 0.83289051\n",
      "Step: [2786] d_loss: 1.29535902, g_loss: 0.86648619\n",
      "Step: [2787] d_loss: 1.31968451, g_loss: 0.82848078\n",
      "Step: [2788] d_loss: 1.27963328, g_loss: 0.88449645\n",
      "Step: [2789] d_loss: 1.30886388, g_loss: 0.85648870\n",
      "Step: [2790] d_loss: 1.28789163, g_loss: 0.85471666\n",
      "Step: [2791] d_loss: 1.25548577, g_loss: 0.85922062\n",
      "Step: [2792] d_loss: 1.28915977, g_loss: 0.85394478\n",
      "Step: [2793] d_loss: 1.25808847, g_loss: 0.87411016\n",
      "Step: [2794] d_loss: 1.31014562, g_loss: 0.87582606\n",
      "Step: [2795] d_loss: 1.26802742, g_loss: 0.86153054\n",
      "Step: [2796] d_loss: 1.28173769, g_loss: 0.83591926\n",
      "Step: [2797] d_loss: 1.25280154, g_loss: 0.85892355\n",
      "Step: [2798] d_loss: 1.26765180, g_loss: 0.86539221\n",
      "Step: [2799] d_loss: 1.28133225, g_loss: 0.87165761\n",
      "Step: [2800] d_loss: 1.30100155, g_loss: 0.83431762\n",
      "Step: [2801] d_loss: 1.24478531, g_loss: 0.91372395\n",
      "Step: [2802] d_loss: 1.28650403, g_loss: 0.89592206\n",
      "Step: [2803] d_loss: 1.26844394, g_loss: 0.89105970\n",
      "Step: [2804] d_loss: 1.30735850, g_loss: 0.85174185\n",
      "Step: [2805] d_loss: 1.30041063, g_loss: 0.82779729\n",
      "Step: [2806] d_loss: 1.23783076, g_loss: 0.88374364\n",
      "Step: [2807] d_loss: 1.34076715, g_loss: 0.86645794\n",
      "Step: [2808] d_loss: 1.25774240, g_loss: 0.87776744\n",
      "Step: [2809] d_loss: 1.32728171, g_loss: 0.80715275\n",
      "Step: [2810] d_loss: 1.29473734, g_loss: 0.83035302\n",
      "Step: [2811] d_loss: 1.25842810, g_loss: 0.83222461\n",
      "Step: [2812] d_loss: 1.24621165, g_loss: 0.92304665\n",
      "Step: [2813] d_loss: 1.28168762, g_loss: 0.85700929\n",
      "Step: [2814] d_loss: 1.26015329, g_loss: 0.87745988\n",
      "Step: [2815] d_loss: 1.30655515, g_loss: 0.87286592\n",
      "Step: [2816] d_loss: 1.27347922, g_loss: 0.85938793\n",
      "Step: [2817] d_loss: 1.27866292, g_loss: 0.84976226\n",
      "Step: [2818] d_loss: 1.27764010, g_loss: 0.88006169\n",
      "Step: [2819] d_loss: 1.32299304, g_loss: 0.81539065\n",
      "Step: [2820] d_loss: 1.25503945, g_loss: 0.85882914\n",
      "Step: [2821] d_loss: 1.30937505, g_loss: 0.84237897\n",
      "Step: [2822] d_loss: 1.28076816, g_loss: 0.85383809\n",
      "Step: [2823] d_loss: 1.26599002, g_loss: 0.89641291\n",
      "Step: [2824] d_loss: 1.28365231, g_loss: 0.85080844\n",
      "Step: [2825] d_loss: 1.29220212, g_loss: 0.82530808\n",
      "Step: [2826] d_loss: 1.28153753, g_loss: 0.84545290\n",
      "Step: [2827] d_loss: 1.25281096, g_loss: 0.84073025\n",
      "Step: [2828] d_loss: 1.28944016, g_loss: 0.86445808\n",
      "Step: [2829] d_loss: 1.26769328, g_loss: 0.83864486\n",
      "Step: [2830] d_loss: 1.28442764, g_loss: 0.85085243\n",
      "Step: [2831] d_loss: 1.26291060, g_loss: 0.85750169\n",
      "Step: [2832] d_loss: 1.22980666, g_loss: 0.89125007\n",
      "Step: [2833] d_loss: 1.26403856, g_loss: 0.88391215\n",
      "Step: [2834] d_loss: 1.27456903, g_loss: 0.86589324\n",
      "Step: [2835] d_loss: 1.26328135, g_loss: 0.87657595\n",
      "Step: [2836] d_loss: 1.30713010, g_loss: 0.86610830\n",
      "Step: [2837] d_loss: 1.30978131, g_loss: 0.85549343\n",
      "Step: [2838] d_loss: 1.36332464, g_loss: 0.83010042\n",
      "Step: [2839] d_loss: 1.29216385, g_loss: 0.86259484\n",
      "Step: [2840] d_loss: 1.25947607, g_loss: 0.85245734\n",
      "Step: [2841] d_loss: 1.26981449, g_loss: 0.86770332\n",
      "Step: [2842] d_loss: 1.31436646, g_loss: 0.86474913\n",
      "Step: [2843] d_loss: 1.28207374, g_loss: 0.85769331\n",
      "Step: [2844] d_loss: 1.26958382, g_loss: 0.86720568\n",
      "Step: [2845] d_loss: 1.25125790, g_loss: 0.88627422\n",
      "Step: [2846] d_loss: 1.28573573, g_loss: 0.90517890\n",
      "Step: [2847] d_loss: 1.31534553, g_loss: 0.83105499\n",
      "Step: [2848] d_loss: 1.33969700, g_loss: 0.81564546\n",
      "Step: [2849] d_loss: 1.30781710, g_loss: 0.85528153\n",
      "Step: [2850] d_loss: 1.25730968, g_loss: 0.88167685\n",
      "Step: [2851] d_loss: 1.28117049, g_loss: 0.83550608\n",
      "Step: [2852] d_loss: 1.20742393, g_loss: 0.89902997\n",
      "Step: [2853] d_loss: 1.25433719, g_loss: 0.86919677\n",
      "Step: [2854] d_loss: 1.27902007, g_loss: 0.88701892\n",
      "Step: [2855] d_loss: 1.26486444, g_loss: 0.86960864\n",
      "Step: [2856] d_loss: 1.27156556, g_loss: 0.85471141\n",
      "Step: [2857] d_loss: 1.29027796, g_loss: 0.82392704\n",
      "Step: [2858] d_loss: 1.28112721, g_loss: 0.85310549\n",
      "Step: [2859] d_loss: 1.30649352, g_loss: 0.89794111\n",
      "Step: [2860] d_loss: 1.33640647, g_loss: 0.83226448\n",
      "Step: [2861] d_loss: 1.27605700, g_loss: 0.86724192\n",
      "Step: [2862] d_loss: 1.28687596, g_loss: 0.86126804\n",
      "Step: [2863] d_loss: 1.25528646, g_loss: 0.88078785\n",
      "Step: [2864] d_loss: 1.29334843, g_loss: 0.84174478\n",
      "Step: [2865] d_loss: 1.29324389, g_loss: 0.82054186\n",
      "Step: [2866] d_loss: 1.29083955, g_loss: 0.82957339\n",
      "Step: [2867] d_loss: 1.27811825, g_loss: 0.82026309\n",
      "Step: [2868] d_loss: 1.31158280, g_loss: 0.82272750\n",
      "Step: [2869] d_loss: 1.27401257, g_loss: 0.83808553\n",
      "Step: [2870] d_loss: 1.26235986, g_loss: 0.87434161\n",
      "Step: [2871] d_loss: 1.28336382, g_loss: 0.85502756\n",
      "Step: [2872] d_loss: 1.26668525, g_loss: 0.90285635\n",
      "Step: [2873] d_loss: 1.30849004, g_loss: 0.85697043\n",
      "Step: [2874] d_loss: 1.30641925, g_loss: 0.85238892\n",
      "Step: [2875] d_loss: 1.33134186, g_loss: 0.85214818\n",
      "Step: [2876] d_loss: 1.27630401, g_loss: 0.91034245\n",
      "Step: [2877] d_loss: 1.27731860, g_loss: 0.88695574\n",
      "Step: [2878] d_loss: 1.27589965, g_loss: 0.86935687\n",
      "Step: [2879] d_loss: 1.30559158, g_loss: 0.79702157\n",
      "Step: [2880] d_loss: 1.28162420, g_loss: 0.82205480\n",
      "Step: [2881] d_loss: 1.24799657, g_loss: 0.84583634\n",
      "Step: [2882] d_loss: 1.21497917, g_loss: 0.89464676\n",
      "Step: [2883] d_loss: 1.34453940, g_loss: 0.83685446\n",
      "Step: [2884] d_loss: 1.30133605, g_loss: 0.81356990\n",
      "Step: [2885] d_loss: 1.29017830, g_loss: 0.81188989\n",
      "Step: [2886] d_loss: 1.30548704, g_loss: 0.86699522\n",
      "Step: [2887] d_loss: 1.25081050, g_loss: 0.91733670\n",
      "Step: [2888] d_loss: 1.25572634, g_loss: 0.88446379\n",
      "Step: [2889] d_loss: 1.26001358, g_loss: 0.86933339\n",
      "Step: [2890] d_loss: 1.33807659, g_loss: 0.79677486\n",
      "Step: [2891] d_loss: 1.29538834, g_loss: 0.84429002\n",
      "Step: [2892] d_loss: 1.26159191, g_loss: 0.85817707\n",
      "Step: [2893] d_loss: 1.30855811, g_loss: 0.83135509\n",
      "Step: [2894] d_loss: 1.26919818, g_loss: 0.85254836\n",
      "Step: [2895] d_loss: 1.27211201, g_loss: 0.83039516\n",
      "Step: [2896] d_loss: 1.26903224, g_loss: 0.86413562\n",
      "Step: [2897] d_loss: 1.27955580, g_loss: 0.89396811\n",
      "Step: [2898] d_loss: 1.28742123, g_loss: 0.87784922\n",
      "Step: [2899] d_loss: 1.27210653, g_loss: 0.87824363\n",
      "Step: [2900] d_loss: 1.29868329, g_loss: 0.87009412\n",
      "Step: [2901] d_loss: 1.26392722, g_loss: 0.86197299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2902] d_loss: 1.29018664, g_loss: 0.86499780\n",
      "Step: [2903] d_loss: 1.28476810, g_loss: 0.87443584\n",
      "Step: [2904] d_loss: 1.28649902, g_loss: 0.86274755\n",
      "Step: [2905] d_loss: 1.28688073, g_loss: 0.85515106\n",
      "Step: [2906] d_loss: 1.30704451, g_loss: 0.83460450\n",
      "Step: [2907] d_loss: 1.28894556, g_loss: 0.87603796\n",
      "Step: [2908] d_loss: 1.25448132, g_loss: 0.83774340\n",
      "Step: [2909] d_loss: 1.26771319, g_loss: 0.88016176\n",
      "Step: [2910] d_loss: 1.27036536, g_loss: 0.87051582\n",
      "Step: [2911] d_loss: 1.27553356, g_loss: 0.86665654\n",
      "Step: [2912] d_loss: 1.33495498, g_loss: 0.86129451\n",
      "Step: [2913] d_loss: 1.29939580, g_loss: 0.83931369\n",
      "Step: [2914] d_loss: 1.25529885, g_loss: 0.85280192\n",
      "Step: [2915] d_loss: 1.25464582, g_loss: 0.87656587\n",
      "Step: [2916] d_loss: 1.35521460, g_loss: 0.82926196\n",
      "Step: [2917] d_loss: 1.32457709, g_loss: 0.83233148\n",
      "Step: [2918] d_loss: 1.29760861, g_loss: 0.85858184\n",
      "Step: [2919] d_loss: 1.26936221, g_loss: 0.89592892\n",
      "Step: [2920] d_loss: 1.25959754, g_loss: 0.86957860\n",
      "Step: [2921] d_loss: 1.21854472, g_loss: 0.91307133\n",
      "Step: [2922] d_loss: 1.30969644, g_loss: 0.84786296\n",
      "Step: [2923] d_loss: 1.23903728, g_loss: 0.85806650\n",
      "Step: [2924] d_loss: 1.28216851, g_loss: 0.86615038\n",
      "Step: [2925] d_loss: 1.28015161, g_loss: 0.84743142\n",
      "Step: [2926] d_loss: 1.28810549, g_loss: 0.83251595\n",
      "Step: [2927] d_loss: 1.23333704, g_loss: 0.88893092\n",
      "Step: [2928] d_loss: 1.24361563, g_loss: 0.85163140\n",
      "Step: [2929] d_loss: 1.24881113, g_loss: 0.88596761\n",
      "Step: [2930] d_loss: 1.28223562, g_loss: 0.87931454\n",
      "Step: [2931] d_loss: 1.30436611, g_loss: 0.85550094\n",
      "Step: [2932] d_loss: 1.25569820, g_loss: 0.86706173\n",
      "Step: [2933] d_loss: 1.28021336, g_loss: 0.88559133\n",
      "Step: [2934] d_loss: 1.27970719, g_loss: 0.86764181\n",
      "Step: [2935] d_loss: 1.26097274, g_loss: 0.86393505\n",
      "Step: [2936] d_loss: 1.24548078, g_loss: 0.88218987\n",
      "Step: [2937] d_loss: 1.28479826, g_loss: 0.88312727\n",
      "Step: [2938] d_loss: 1.24162996, g_loss: 0.88620824\n",
      "Step: [2939] d_loss: 1.32147098, g_loss: 0.84009677\n",
      "Step: [2940] d_loss: 1.31434357, g_loss: 0.84409338\n",
      "Step: [2941] d_loss: 1.30009866, g_loss: 0.85522568\n",
      "Step: [2942] d_loss: 1.29261160, g_loss: 0.84088862\n",
      "Step: [2943] d_loss: 1.26629400, g_loss: 0.86054218\n",
      "Step: [2944] d_loss: 1.25461066, g_loss: 0.87912863\n",
      "Step: [2945] d_loss: 1.25876558, g_loss: 0.86401886\n",
      "Step: [2946] d_loss: 1.29404223, g_loss: 0.83962727\n",
      "Step: [2947] d_loss: 1.31116462, g_loss: 0.80642301\n",
      "Step: [2948] d_loss: 1.26654053, g_loss: 0.86447752\n",
      "Step: [2949] d_loss: 1.29119682, g_loss: 0.83821088\n",
      "Step: [2950] d_loss: 1.26777744, g_loss: 0.87225342\n",
      "Step: [2951] d_loss: 1.26188302, g_loss: 0.87854600\n",
      "Step: [2952] d_loss: 1.26858139, g_loss: 0.85179871\n",
      "Step: [2953] d_loss: 1.25713503, g_loss: 0.89414513\n",
      "Step: [2954] d_loss: 1.31963837, g_loss: 0.88307554\n",
      "Step: [2955] d_loss: 1.22002411, g_loss: 0.86472201\n",
      "Step: [2956] d_loss: 1.27952182, g_loss: 0.79890120\n",
      "Step: [2957] d_loss: 1.28099990, g_loss: 0.87845755\n",
      "Step: [2958] d_loss: 1.31391573, g_loss: 0.84319776\n",
      "Step: [2959] d_loss: 1.28698623, g_loss: 0.90940273\n",
      "Step: [2960] d_loss: 1.28440440, g_loss: 0.88275635\n",
      "Step: [2961] d_loss: 1.24814141, g_loss: 0.85744107\n",
      "Step: [2962] d_loss: 1.30102241, g_loss: 0.83819425\n",
      "Step: [2963] d_loss: 1.28353715, g_loss: 0.81896096\n",
      "Step: [2964] d_loss: 1.32339668, g_loss: 0.84577233\n",
      "Step: [2965] d_loss: 1.26685750, g_loss: 0.88760591\n",
      "Step: [2966] d_loss: 1.28889823, g_loss: 0.88763118\n",
      "Step: [2967] d_loss: 1.33940077, g_loss: 0.86104846\n",
      "Step: [2968] d_loss: 1.30569315, g_loss: 0.85741127\n",
      "Step: [2969] d_loss: 1.29379869, g_loss: 0.86855519\n",
      "Step: [2970] d_loss: 1.25641835, g_loss: 0.86675042\n",
      "Step: [2971] d_loss: 1.22857642, g_loss: 0.90381289\n",
      "Step: [2972] d_loss: 1.25076675, g_loss: 0.86437911\n",
      "Step: [2973] d_loss: 1.33631730, g_loss: 0.83702999\n",
      "Step: [2974] d_loss: 1.28806651, g_loss: 0.86228216\n",
      "Step: [2975] d_loss: 1.28237820, g_loss: 0.82698643\n",
      "Step: [2976] d_loss: 1.26254773, g_loss: 0.84972382\n",
      "Step: [2977] d_loss: 1.25557303, g_loss: 0.85757124\n",
      "Step: [2978] d_loss: 1.25500703, g_loss: 0.88571203\n",
      "Step: [2979] d_loss: 1.31152284, g_loss: 0.85673857\n",
      "Step: [2980] d_loss: 1.27915502, g_loss: 0.90237105\n",
      "Step: [2981] d_loss: 1.30094934, g_loss: 0.84655166\n",
      "Step: [2982] d_loss: 1.29308510, g_loss: 0.82637465\n",
      "Step: [2983] d_loss: 1.31352603, g_loss: 0.82656670\n",
      "Step: [2984] d_loss: 1.25920343, g_loss: 0.87711471\n",
      "Step: [2985] d_loss: 1.30984282, g_loss: 0.85068578\n",
      "Step: [2986] d_loss: 1.28145432, g_loss: 0.86151385\n",
      "Step: [2987] d_loss: 1.31680524, g_loss: 0.84709334\n",
      "Step: [2988] d_loss: 1.28207040, g_loss: 0.89611351\n",
      "Step: [2989] d_loss: 1.23232031, g_loss: 0.89406615\n",
      "Step: [2990] d_loss: 1.23617625, g_loss: 0.84681076\n",
      "Step: [2991] d_loss: 1.27737367, g_loss: 0.87133819\n",
      "Step: [2992] d_loss: 1.29259467, g_loss: 0.86044896\n",
      "Step: [2993] d_loss: 1.26296496, g_loss: 0.86582816\n",
      "Step: [2994] d_loss: 1.29560852, g_loss: 0.85646307\n",
      "Step: [2995] d_loss: 1.26189888, g_loss: 0.84579372\n",
      "Step: [2996] d_loss: 1.26391542, g_loss: 0.86660564\n",
      "Step: [2997] d_loss: 1.25030732, g_loss: 0.88349634\n",
      "Step: [2998] d_loss: 1.29026532, g_loss: 0.85294986\n",
      "Step: [2999] d_loss: 1.24362528, g_loss: 0.87911046\n",
      "Step: [3000] d_loss: 1.27291751, g_loss: 0.89173007\n",
      "Step: [3001] d_loss: 1.22764635, g_loss: 0.90998304\n",
      "Step: [3002] d_loss: 1.30609190, g_loss: 0.86072773\n",
      "Step: [3003] d_loss: 1.30171943, g_loss: 0.85376370\n",
      "Step: [3004] d_loss: 1.30549145, g_loss: 0.85229212\n",
      "Step: [3005] d_loss: 1.29679489, g_loss: 0.86344343\n",
      "Step: [3006] d_loss: 1.28489828, g_loss: 0.87558866\n",
      "Step: [3007] d_loss: 1.24030483, g_loss: 0.87525892\n",
      "Step: [3008] d_loss: 1.29212785, g_loss: 0.85487664\n",
      "Step: [3009] d_loss: 1.27716553, g_loss: 0.89170969\n",
      "Step: [3010] d_loss: 1.29257369, g_loss: 0.88358587\n",
      "Step: [3011] d_loss: 1.30424619, g_loss: 0.85682625\n",
      "Step: [3012] d_loss: 1.29206657, g_loss: 0.82969296\n",
      "Step: [3013] d_loss: 1.29396927, g_loss: 0.80780268\n",
      "Step: [3014] d_loss: 1.28663421, g_loss: 0.80058527\n",
      "Step: [3015] d_loss: 1.27692199, g_loss: 0.83075190\n",
      "Step: [3016] d_loss: 1.23885179, g_loss: 0.85758990\n",
      "Step: [3017] d_loss: 1.33725572, g_loss: 0.86444819\n",
      "Step: [3018] d_loss: 1.29621947, g_loss: 0.87689829\n",
      "Step: [3019] d_loss: 1.26161075, g_loss: 0.88393563\n",
      "Step: [3020] d_loss: 1.28854585, g_loss: 0.83015609\n",
      "Step: [3021] d_loss: 1.28495538, g_loss: 0.81633949\n",
      "Step: [3022] d_loss: 1.23619163, g_loss: 0.91353750\n",
      "Step: [3023] d_loss: 1.21257615, g_loss: 0.89727235\n",
      "Step: [3024] d_loss: 1.28606176, g_loss: 0.85524499\n",
      "Step: [3025] d_loss: 1.30538511, g_loss: 0.86396551\n",
      "Step: [3026] d_loss: 1.26231861, g_loss: 0.88490653\n",
      "Step: [3027] d_loss: 1.24787784, g_loss: 0.92175901\n",
      "Step: [3028] d_loss: 1.26407075, g_loss: 0.91537642\n",
      "Step: [3029] d_loss: 1.25492668, g_loss: 0.88959175\n",
      "Step: [3030] d_loss: 1.27271843, g_loss: 0.85239422\n",
      "Step: [3031] d_loss: 1.25114012, g_loss: 0.85567784\n",
      "Step: [3032] d_loss: 1.26370943, g_loss: 0.85993540\n",
      "Step: [3033] d_loss: 1.32084513, g_loss: 0.86130989\n",
      "Step: [3034] d_loss: 1.28697407, g_loss: 0.87997139\n",
      "Step: [3035] d_loss: 1.32659698, g_loss: 0.82853663\n",
      "Step: [3036] d_loss: 1.25651622, g_loss: 0.83877981\n",
      "Step: [3037] d_loss: 1.29096174, g_loss: 0.84681010\n",
      "Step: [3038] d_loss: 1.27266192, g_loss: 0.83582997\n",
      "Step: [3039] d_loss: 1.25068927, g_loss: 0.86094242\n",
      "Step: [3040] d_loss: 1.29465568, g_loss: 0.85189229\n",
      "Step: [3041] d_loss: 1.31106472, g_loss: 0.82908309\n",
      "Step: [3042] d_loss: 1.26870418, g_loss: 0.89653611\n",
      "Step: [3043] d_loss: 1.31271231, g_loss: 0.88745320\n",
      "Step: [3044] d_loss: 1.31113052, g_loss: 0.87255931\n",
      "Step: [3045] d_loss: 1.26515579, g_loss: 0.86949730\n",
      "Step: [3046] d_loss: 1.29944515, g_loss: 0.81691396\n",
      "Step: [3047] d_loss: 1.29748654, g_loss: 0.82667392\n",
      "Step: [3048] d_loss: 1.26507258, g_loss: 0.88104808\n",
      "Step: [3049] d_loss: 1.30363536, g_loss: 0.88685197\n",
      "Step: [3050] d_loss: 1.31105888, g_loss: 0.87695742\n",
      "Step: [3051] d_loss: 1.30604887, g_loss: 0.86005443\n",
      "Step: [3052] d_loss: 1.24320006, g_loss: 0.90509164\n",
      "Step: [3053] d_loss: 1.23433161, g_loss: 0.90004128\n",
      "Step: [3054] d_loss: 1.26598597, g_loss: 0.88372630\n",
      "Step: [3055] d_loss: 1.35812843, g_loss: 0.81767738\n",
      "Step: [3056] d_loss: 1.28880310, g_loss: 0.85841095\n",
      "Step: [3057] d_loss: 1.26733470, g_loss: 0.85814309\n",
      "Step: [3058] d_loss: 1.27117610, g_loss: 0.89036638\n",
      "Step: [3059] d_loss: 1.32654488, g_loss: 0.86487937\n",
      "Step: [3060] d_loss: 1.30969059, g_loss: 0.80531740\n",
      "Step: [3061] d_loss: 1.26536977, g_loss: 0.82526278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3062] d_loss: 1.29957914, g_loss: 0.84229624\n",
      "Step: [3063] d_loss: 1.30592608, g_loss: 0.84846067\n",
      "Step: [3064] d_loss: 1.30920970, g_loss: 0.90818465\n",
      "Step: [3065] d_loss: 1.30763626, g_loss: 0.85381472\n",
      "Step: [3066] d_loss: 1.32627892, g_loss: 0.86748570\n",
      "Step: [3067] d_loss: 1.27348161, g_loss: 0.87119138\n",
      "Step: [3068] d_loss: 1.28569090, g_loss: 0.85765779\n",
      "Step: [3069] d_loss: 1.29495597, g_loss: 0.85902846\n",
      "Step: [3070] d_loss: 1.25057173, g_loss: 0.86141729\n",
      "Step: [3071] d_loss: 1.29803038, g_loss: 0.83419067\n",
      "Step: [3072] d_loss: 1.24335957, g_loss: 0.87135625\n",
      "Step: [3073] d_loss: 1.26991081, g_loss: 0.85273230\n",
      "Step: [3074] d_loss: 1.26514685, g_loss: 0.86853087\n",
      "Step: [3075] d_loss: 1.29536986, g_loss: 0.85885322\n",
      "Step: [3076] d_loss: 1.30284989, g_loss: 0.86306632\n",
      "Step: [3077] d_loss: 1.28280425, g_loss: 0.87169135\n",
      "Step: [3078] d_loss: 1.27624428, g_loss: 0.83554369\n",
      "Step: [3079] d_loss: 1.31083679, g_loss: 0.83037835\n",
      "Step: [3080] d_loss: 1.23816919, g_loss: 0.87953651\n",
      "Step: [3081] d_loss: 1.28489375, g_loss: 0.86142957\n",
      "Step: [3082] d_loss: 1.26244521, g_loss: 0.90747237\n",
      "Step: [3083] d_loss: 1.30484653, g_loss: 0.82495785\n",
      "Step: [3084] d_loss: 1.29257011, g_loss: 0.87445265\n",
      "Step: [3085] d_loss: 1.25921166, g_loss: 0.89371681\n",
      "Step: [3086] d_loss: 1.26864064, g_loss: 0.86670232\n",
      "Step: [3087] d_loss: 1.24483943, g_loss: 0.87146366\n",
      "Step: [3088] d_loss: 1.26710451, g_loss: 0.82381958\n",
      "Step: [3089] d_loss: 1.28027320, g_loss: 0.81362683\n",
      "Step: [3090] d_loss: 1.27822316, g_loss: 0.84193808\n",
      "Step: [3091] d_loss: 1.26947522, g_loss: 0.84459603\n",
      "Step: [3092] d_loss: 1.28347993, g_loss: 0.85523880\n",
      "Step: [3093] d_loss: 1.27514172, g_loss: 0.86241859\n",
      "Step: [3094] d_loss: 1.28281128, g_loss: 0.86799538\n",
      "Step: [3095] d_loss: 1.25464797, g_loss: 0.89199221\n",
      "Step: [3096] d_loss: 1.29619336, g_loss: 0.83588243\n",
      "Step: [3097] d_loss: 1.23865008, g_loss: 0.86939442\n",
      "Step: [3098] d_loss: 1.32364345, g_loss: 0.86178529\n",
      "Step: [3099] d_loss: 1.26956093, g_loss: 0.92481613\n",
      "Step: [3100] d_loss: 1.31738949, g_loss: 0.86040843\n",
      "Step: [3101] d_loss: 1.26955414, g_loss: 0.83831537\n",
      "Step: [3102] d_loss: 1.26323259, g_loss: 0.88157862\n",
      "Step: [3103] d_loss: 1.27289224, g_loss: 0.88427764\n",
      "Step: [3104] d_loss: 1.26797318, g_loss: 0.87110412\n",
      "Step: [3105] d_loss: 1.29230595, g_loss: 0.87954760\n",
      "Step: [3106] d_loss: 1.23956215, g_loss: 0.89526737\n",
      "Step: [3107] d_loss: 1.28609562, g_loss: 0.83988482\n",
      "Step: [3108] d_loss: 1.25659227, g_loss: 0.82950139\n",
      "Step: [3109] d_loss: 1.28409576, g_loss: 0.83213633\n",
      "Step: [3110] d_loss: 1.25028360, g_loss: 0.85002577\n",
      "Step: [3111] d_loss: 1.30315685, g_loss: 0.83288699\n",
      "Step: [3112] d_loss: 1.29471576, g_loss: 0.86395395\n",
      "Step: [3113] d_loss: 1.26486039, g_loss: 0.87991279\n",
      "Step: [3114] d_loss: 1.30556500, g_loss: 0.88329160\n",
      "Step: [3115] d_loss: 1.25332856, g_loss: 0.94864929\n",
      "Step: [3116] d_loss: 1.31691420, g_loss: 0.88623571\n",
      "Step: [3117] d_loss: 1.25510812, g_loss: 0.85807890\n",
      "Step: [3118] d_loss: 1.29213238, g_loss: 0.85819268\n",
      "Step: [3119] d_loss: 1.27892017, g_loss: 0.87810135\n",
      "Step: [3120] d_loss: 1.27789164, g_loss: 0.89397144\n",
      "Step: [3121] d_loss: 1.28902650, g_loss: 0.83058512\n",
      "Step: [3122] d_loss: 1.25854325, g_loss: 0.84860498\n",
      "Step: [3123] d_loss: 1.26499116, g_loss: 0.88570833\n",
      "Step: [3124] d_loss: 1.28745782, g_loss: 0.83421111\n",
      "Step: [3125] d_loss: 1.26920176, g_loss: 0.84661388\n",
      "Step: [3126] d_loss: 1.27707100, g_loss: 0.87669307\n",
      "Step: [3127] d_loss: 1.26887393, g_loss: 0.88572168\n",
      "Step: [3128] d_loss: 1.25134039, g_loss: 0.89856231\n",
      "Step: [3129] d_loss: 1.31241202, g_loss: 0.86655867\n",
      "Step: [3130] d_loss: 1.28920603, g_loss: 0.85346818\n",
      "Step: [3131] d_loss: 1.23962355, g_loss: 0.90344918\n",
      "Step: [3132] d_loss: 1.30527294, g_loss: 0.89253473\n",
      "Step: [3133] d_loss: 1.31567013, g_loss: 0.86493313\n",
      "Step: [3134] d_loss: 1.29481959, g_loss: 0.83427036\n",
      "Step: [3135] d_loss: 1.26556849, g_loss: 0.89774239\n",
      "Step: [3136] d_loss: 1.24957991, g_loss: 0.89128810\n",
      "Step: [3137] d_loss: 1.28766942, g_loss: 0.86819369\n",
      "Step: [3138] d_loss: 1.26336002, g_loss: 0.90134186\n",
      "Step: [3139] d_loss: 1.29918146, g_loss: 0.87259388\n",
      "Step: [3140] d_loss: 1.27892327, g_loss: 0.85515696\n",
      "Step: [3141] d_loss: 1.23619568, g_loss: 0.86561173\n",
      "Step: [3142] d_loss: 1.28990507, g_loss: 0.88525724\n",
      "Step: [3143] d_loss: 1.24388194, g_loss: 0.87005955\n",
      "Step: [3144] d_loss: 1.24756455, g_loss: 0.87798238\n",
      "Step: [3145] d_loss: 1.28572631, g_loss: 0.82975912\n",
      "Step: [3146] d_loss: 1.24743128, g_loss: 0.88826525\n",
      "Step: [3147] d_loss: 1.32695377, g_loss: 0.79482782\n",
      "Step: [3148] d_loss: 1.24103415, g_loss: 0.87491822\n",
      "Step: [3149] d_loss: 1.24067712, g_loss: 0.88458371\n",
      "Step: [3150] d_loss: 1.32030940, g_loss: 0.84312999\n",
      "Step: [3151] d_loss: 1.28219223, g_loss: 0.85358119\n",
      "Step: [3152] d_loss: 1.29846573, g_loss: 0.81330419\n",
      "Step: [3153] d_loss: 1.27443027, g_loss: 0.83960819\n",
      "Step: [3154] d_loss: 1.28964388, g_loss: 0.84761983\n",
      "Step: [3155] d_loss: 1.28313303, g_loss: 0.87759298\n",
      "Step: [3156] d_loss: 1.31299281, g_loss: 0.86915421\n",
      "Step: [3157] d_loss: 1.27170324, g_loss: 0.89872295\n",
      "Step: [3158] d_loss: 1.29606152, g_loss: 0.88352293\n",
      "Step: [3159] d_loss: 1.28522158, g_loss: 0.86422360\n",
      "Step: [3160] d_loss: 1.30428112, g_loss: 0.84214920\n",
      "Step: [3161] d_loss: 1.26531839, g_loss: 0.87128317\n",
      "Step: [3162] d_loss: 1.31337214, g_loss: 0.86917067\n",
      "Step: [3163] d_loss: 1.28623438, g_loss: 0.86385149\n",
      "Step: [3164] d_loss: 1.29469001, g_loss: 0.87335110\n",
      "Step: [3165] d_loss: 1.22199273, g_loss: 0.89907640\n",
      "Step: [3166] d_loss: 1.33651328, g_loss: 0.83959317\n",
      "Step: [3167] d_loss: 1.28520834, g_loss: 0.86849797\n",
      "Step: [3168] d_loss: 1.24887156, g_loss: 0.87331271\n",
      "Step: [3169] d_loss: 1.27106702, g_loss: 0.85023671\n",
      "Step: [3170] d_loss: 1.27409410, g_loss: 0.84904218\n",
      "Step: [3171] d_loss: 1.25591731, g_loss: 0.88217777\n",
      "Step: [3172] d_loss: 1.23872352, g_loss: 0.87277198\n",
      "Step: [3173] d_loss: 1.30659914, g_loss: 0.85589719\n",
      "Step: [3174] d_loss: 1.25083458, g_loss: 0.86654472\n",
      "Step: [3175] d_loss: 1.25562334, g_loss: 0.89335155\n",
      "Step: [3176] d_loss: 1.26698375, g_loss: 0.87628996\n",
      "Step: [3177] d_loss: 1.23840737, g_loss: 0.85347819\n",
      "Step: [3178] d_loss: 1.27209473, g_loss: 0.84829277\n",
      "Step: [3179] d_loss: 1.26400542, g_loss: 0.86289424\n",
      "Step: [3180] d_loss: 1.24392855, g_loss: 0.88589561\n",
      "Step: [3181] d_loss: 1.25205672, g_loss: 0.89020663\n",
      "Step: [3182] d_loss: 1.23089671, g_loss: 0.92871106\n",
      "Step: [3183] d_loss: 1.26791620, g_loss: 0.86774659\n",
      "Step: [3184] d_loss: 1.22841358, g_loss: 0.87724280\n",
      "Step: [3185] d_loss: 1.26050937, g_loss: 0.82416791\n",
      "Step: [3186] d_loss: 1.36483502, g_loss: 0.77738416\n",
      "Step: [3187] d_loss: 1.27270222, g_loss: 0.86392993\n",
      "Step: [3188] d_loss: 1.32251000, g_loss: 0.87669158\n",
      "Step: [3189] d_loss: 1.28532410, g_loss: 0.89989507\n",
      "Step: [3190] d_loss: 1.29159498, g_loss: 0.88157296\n",
      "Step: [3191] d_loss: 1.27427125, g_loss: 0.87464666\n",
      "Step: [3192] d_loss: 1.31096578, g_loss: 0.79900420\n",
      "Step: [3193] d_loss: 1.24657297, g_loss: 0.86450160\n",
      "Step: [3194] d_loss: 1.28347850, g_loss: 0.86547405\n",
      "Step: [3195] d_loss: 1.28017211, g_loss: 0.85457671\n",
      "Step: [3196] d_loss: 1.28733420, g_loss: 0.87245613\n",
      "Step: [3197] d_loss: 1.26178682, g_loss: 0.87763608\n",
      "Step: [3198] d_loss: 1.25674510, g_loss: 0.88043547\n",
      "Step: [3199] d_loss: 1.30662441, g_loss: 0.86899400\n",
      "Step: [3200] d_loss: 1.24513841, g_loss: 0.84525621\n",
      "Step: [3201] d_loss: 1.27737713, g_loss: 0.83763981\n",
      "Step: [3202] d_loss: 1.26341510, g_loss: 0.86366260\n",
      "Step: [3203] d_loss: 1.27782083, g_loss: 0.86426151\n",
      "Step: [3204] d_loss: 1.24157834, g_loss: 0.89195871\n",
      "Step: [3205] d_loss: 1.25313830, g_loss: 0.91015124\n",
      "Step: [3206] d_loss: 1.23043859, g_loss: 0.87072515\n",
      "Step: [3207] d_loss: 1.26469278, g_loss: 0.86896914\n",
      "Step: [3208] d_loss: 1.28317642, g_loss: 0.82192981\n",
      "Step: [3209] d_loss: 1.23291719, g_loss: 0.85865295\n",
      "Step: [3210] d_loss: 1.23446453, g_loss: 0.87986529\n",
      "Step: [3211] d_loss: 1.23690081, g_loss: 0.87483978\n",
      "Step: [3212] d_loss: 1.24888349, g_loss: 0.87138617\n",
      "Step: [3213] d_loss: 1.22475278, g_loss: 0.88212395\n",
      "Step: [3214] d_loss: 1.27232254, g_loss: 0.84002978\n",
      "Step: [3215] d_loss: 1.31990159, g_loss: 0.85661119\n",
      "Step: [3216] d_loss: 1.30820370, g_loss: 0.87264490\n",
      "Step: [3217] d_loss: 1.28719473, g_loss: 0.87087530\n",
      "Step: [3218] d_loss: 1.24189496, g_loss: 0.89910024\n",
      "Step: [3219] d_loss: 1.30628705, g_loss: 0.87544727\n",
      "Step: [3220] d_loss: 1.21435368, g_loss: 0.87378913\n",
      "Step: [3221] d_loss: 1.24616992, g_loss: 0.87885010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3222] d_loss: 1.25809407, g_loss: 0.88059151\n",
      "Step: [3223] d_loss: 1.26488662, g_loss: 0.89735699\n",
      "Step: [3224] d_loss: 1.27736855, g_loss: 0.87499654\n",
      "Step: [3225] d_loss: 1.31456065, g_loss: 0.87054443\n",
      "Step: [3226] d_loss: 1.25037694, g_loss: 0.87725341\n",
      "Step: [3227] d_loss: 1.28260124, g_loss: 0.85831034\n",
      "Step: [3228] d_loss: 1.27809024, g_loss: 0.86056262\n",
      "Step: [3229] d_loss: 1.26027799, g_loss: 0.88681269\n",
      "Step: [3230] d_loss: 1.31218755, g_loss: 0.90058547\n",
      "Step: [3231] d_loss: 1.39055133, g_loss: 0.85053122\n",
      "Step: [3232] d_loss: 1.33778167, g_loss: 0.80553216\n",
      "Step: [3233] d_loss: 1.27211785, g_loss: 0.82776582\n",
      "Step: [3234] d_loss: 1.31666648, g_loss: 0.83567870\n",
      "Step: [3235] d_loss: 1.31970465, g_loss: 0.85253465\n",
      "Step: [3236] d_loss: 1.32733870, g_loss: 0.85349530\n",
      "Step: [3237] d_loss: 1.27098608, g_loss: 0.87636149\n",
      "Step: [3238] d_loss: 1.34733367, g_loss: 0.82990050\n",
      "Step: [3239] d_loss: 1.21864605, g_loss: 0.91020495\n",
      "Step: [3240] d_loss: 1.31444812, g_loss: 0.84479213\n",
      "Step: [3241] d_loss: 1.26925087, g_loss: 0.87734103\n",
      "Step: [3242] d_loss: 1.31555903, g_loss: 0.82419533\n",
      "Step: [3243] d_loss: 1.25277042, g_loss: 0.84700811\n",
      "Step: [3244] d_loss: 1.33456373, g_loss: 0.83584076\n",
      "Step: [3245] d_loss: 1.22541475, g_loss: 0.89187276\n",
      "Step: [3246] d_loss: 1.27257299, g_loss: 0.87618899\n",
      "Step: [3247] d_loss: 1.32044411, g_loss: 0.83534229\n",
      "Step: [3248] d_loss: 1.24541128, g_loss: 0.89964718\n",
      "Step: [3249] d_loss: 1.26104689, g_loss: 0.85710597\n",
      "Step: [3250] d_loss: 1.26685333, g_loss: 0.89322591\n",
      "Step: [3251] d_loss: 1.28100526, g_loss: 0.87990427\n",
      "Step: [3252] d_loss: 1.22362161, g_loss: 0.88904834\n",
      "Step: [3253] d_loss: 1.27523232, g_loss: 0.86717290\n",
      "Step: [3254] d_loss: 1.27165031, g_loss: 0.85901546\n",
      "Step: [3255] d_loss: 1.24653196, g_loss: 0.88599575\n",
      "Step: [3256] d_loss: 1.29417634, g_loss: 0.87654376\n",
      "Step: [3257] d_loss: 1.29354620, g_loss: 0.86498141\n",
      "Step: [3258] d_loss: 1.26491332, g_loss: 0.89083135\n",
      "Step: [3259] d_loss: 1.24466705, g_loss: 0.86163688\n",
      "Step: [3260] d_loss: 1.30943072, g_loss: 0.86168468\n",
      "Step: [3261] d_loss: 1.28549778, g_loss: 0.83938628\n",
      "Step: [3262] d_loss: 1.28314042, g_loss: 0.81930828\n",
      "Step: [3263] d_loss: 1.31294870, g_loss: 0.84621042\n",
      "Step: [3264] d_loss: 1.23494291, g_loss: 0.90164578\n",
      "Step: [3265] d_loss: 1.26182282, g_loss: 0.84086931\n",
      "Step: [3266] d_loss: 1.27174497, g_loss: 0.89667010\n",
      "Step: [3267] d_loss: 1.27696586, g_loss: 0.90566707\n",
      "Step: [3268] d_loss: 1.30117071, g_loss: 0.87182224\n",
      "Step: [3269] d_loss: 1.28662705, g_loss: 0.88487762\n",
      "Step: [3270] d_loss: 1.26469183, g_loss: 0.87577784\n",
      "Step: [3271] d_loss: 1.28720093, g_loss: 0.88029122\n",
      "Step: [3272] d_loss: 1.24076390, g_loss: 0.84352243\n",
      "Step: [3273] d_loss: 1.21175206, g_loss: 0.87414491\n",
      "Step: [3274] d_loss: 1.30320716, g_loss: 0.83864135\n",
      "Step: [3275] d_loss: 1.25898790, g_loss: 0.87594146\n",
      "Step: [3276] d_loss: 1.31464076, g_loss: 0.87080115\n",
      "Step: [3277] d_loss: 1.22333431, g_loss: 0.92035770\n",
      "Step: [3278] d_loss: 1.28392088, g_loss: 0.87214386\n",
      "Step: [3279] d_loss: 1.31374323, g_loss: 0.89190650\n",
      "Step: [3280] d_loss: 1.30860949, g_loss: 0.85783285\n",
      "Step: [3281] d_loss: 1.29954743, g_loss: 0.82575881\n",
      "Step: [3282] d_loss: 1.30663347, g_loss: 0.83786190\n",
      "Step: [3283] d_loss: 1.32505906, g_loss: 0.81393230\n",
      "Step: [3284] d_loss: 1.26857758, g_loss: 0.86001593\n",
      "Step: [3285] d_loss: 1.25019622, g_loss: 0.87632883\n",
      "Step: [3286] d_loss: 1.23999858, g_loss: 0.86773235\n",
      "Step: [3287] d_loss: 1.29288268, g_loss: 0.87235391\n",
      "Step: [3288] d_loss: 1.34612346, g_loss: 0.82172132\n",
      "Step: [3289] d_loss: 1.24020123, g_loss: 0.85620266\n",
      "Step: [3290] d_loss: 1.31256175, g_loss: 0.82545078\n",
      "Step: [3291] d_loss: 1.27521515, g_loss: 0.86810684\n",
      "Step: [3292] d_loss: 1.28315878, g_loss: 0.82610881\n",
      "Step: [3293] d_loss: 1.31131613, g_loss: 0.93399441\n",
      "Step: [3294] d_loss: 1.31258941, g_loss: 0.89678425\n",
      "Step: [3295] d_loss: 1.27410030, g_loss: 0.87009597\n",
      "Step: [3296] d_loss: 1.29402232, g_loss: 0.89992541\n",
      "Step: [3297] d_loss: 1.26470125, g_loss: 0.85430324\n",
      "Step: [3298] d_loss: 1.25331306, g_loss: 0.84372634\n",
      "Step: [3299] d_loss: 1.23551762, g_loss: 0.84936070\n",
      "Step: [3300] d_loss: 1.31775522, g_loss: 0.83177561\n",
      "Step: [3301] d_loss: 1.29080760, g_loss: 0.85271132\n",
      "Step: [3302] d_loss: 1.22447002, g_loss: 0.90201026\n",
      "Step: [3303] d_loss: 1.28047347, g_loss: 0.87722552\n",
      "Step: [3304] d_loss: 1.26314986, g_loss: 0.87411278\n",
      "Step: [3305] d_loss: 1.30885744, g_loss: 0.86688423\n",
      "Step: [3306] d_loss: 1.32022870, g_loss: 0.85881495\n",
      "Step: [3307] d_loss: 1.27856898, g_loss: 0.86318874\n",
      "Step: [3308] d_loss: 1.30483484, g_loss: 0.83878690\n",
      "Step: [3309] d_loss: 1.29904580, g_loss: 0.84506154\n",
      "Step: [3310] d_loss: 1.22962737, g_loss: 0.88114035\n",
      "Step: [3311] d_loss: 1.27097797, g_loss: 0.89722872\n",
      "Step: [3312] d_loss: 1.34106922, g_loss: 0.82394540\n",
      "Step: [3313] d_loss: 1.29575789, g_loss: 0.86583781\n",
      "Step: [3314] d_loss: 1.31238580, g_loss: 0.89238942\n",
      "Step: [3315] d_loss: 1.27655375, g_loss: 0.85001045\n",
      "Step: [3316] d_loss: 1.27761364, g_loss: 0.89358014\n",
      "Step: [3317] d_loss: 1.30944836, g_loss: 0.85810113\n",
      "Step: [3318] d_loss: 1.28950357, g_loss: 0.81925255\n",
      "Step: [3319] d_loss: 1.25494528, g_loss: 0.85529721\n",
      "Step: [3320] d_loss: 1.27890813, g_loss: 0.88188004\n",
      "Step: [3321] d_loss: 1.37443280, g_loss: 0.84802949\n",
      "Step: [3322] d_loss: 1.27440679, g_loss: 0.87831551\n",
      "Step: [3323] d_loss: 1.27633440, g_loss: 0.88656282\n",
      "Step: [3324] d_loss: 1.28211725, g_loss: 0.84172142\n",
      "Step: [3325] d_loss: 1.29107285, g_loss: 0.81949848\n",
      "Step: [3326] d_loss: 1.25541449, g_loss: 0.87248719\n",
      "Step: [3327] d_loss: 1.28777909, g_loss: 0.90278393\n",
      "Step: [3328] d_loss: 1.28645611, g_loss: 0.89694667\n",
      "Step: [3329] d_loss: 1.31567621, g_loss: 0.81625378\n",
      "Step: [3330] d_loss: 1.24724567, g_loss: 0.85386729\n",
      "Step: [3331] d_loss: 1.33348680, g_loss: 0.82350349\n",
      "Step: [3332] d_loss: 1.21758807, g_loss: 0.91975570\n",
      "Step: [3333] d_loss: 1.24193501, g_loss: 0.88471532\n",
      "Step: [3334] d_loss: 1.21133327, g_loss: 0.89856744\n",
      "Step: [3335] d_loss: 1.24460208, g_loss: 0.88088405\n",
      "Step: [3336] d_loss: 1.26759219, g_loss: 0.84945315\n",
      "Step: [3337] d_loss: 1.18677425, g_loss: 0.88680065\n",
      "Step: [3338] d_loss: 1.30114079, g_loss: 0.89697611\n",
      "Step: [3339] d_loss: 1.24639583, g_loss: 0.90046561\n",
      "Step: [3340] d_loss: 1.26865327, g_loss: 0.88978392\n",
      "Step: [3341] d_loss: 1.28547144, g_loss: 0.88537240\n",
      "Step: [3342] d_loss: 1.25677514, g_loss: 0.84916222\n",
      "Step: [3343] d_loss: 1.26140594, g_loss: 0.86169356\n",
      "Step: [3344] d_loss: 1.24830925, g_loss: 0.87413812\n",
      "Step: [3345] d_loss: 1.27056968, g_loss: 0.91553402\n",
      "Step: [3346] d_loss: 1.26235020, g_loss: 0.92447424\n",
      "Step: [3347] d_loss: 1.27114451, g_loss: 0.91579038\n",
      "Step: [3348] d_loss: 1.26171446, g_loss: 0.89037710\n",
      "Step: [3349] d_loss: 1.31414008, g_loss: 0.85715508\n",
      "Step: [3350] d_loss: 1.25789595, g_loss: 0.81822491\n",
      "Step: [3351] d_loss: 1.28721166, g_loss: 0.81953710\n",
      "Step: [3352] d_loss: 1.27799916, g_loss: 0.85565472\n",
      "Step: [3353] d_loss: 1.25092769, g_loss: 0.91226888\n",
      "Step: [3354] d_loss: 1.27941346, g_loss: 0.89449680\n",
      "Step: [3355] d_loss: 1.24654305, g_loss: 0.90044314\n",
      "Step: [3356] d_loss: 1.28267694, g_loss: 0.87596494\n",
      "Step: [3357] d_loss: 1.31224990, g_loss: 0.85931796\n",
      "Step: [3358] d_loss: 1.24311829, g_loss: 0.89890504\n",
      "Step: [3359] d_loss: 1.28912020, g_loss: 0.88056755\n",
      "Step: [3360] d_loss: 1.28289175, g_loss: 0.88234818\n",
      "Step: [3361] d_loss: 1.29318738, g_loss: 0.84339106\n",
      "Step: [3362] d_loss: 1.22791052, g_loss: 0.87997365\n",
      "Step: [3363] d_loss: 1.31649756, g_loss: 0.88032794\n",
      "Step: [3364] d_loss: 1.28606379, g_loss: 0.88699162\n",
      "Step: [3365] d_loss: 1.31521857, g_loss: 0.85122383\n",
      "Step: [3366] d_loss: 1.29556763, g_loss: 0.87309921\n",
      "Step: [3367] d_loss: 1.23440611, g_loss: 0.89647418\n",
      "Step: [3368] d_loss: 1.25944805, g_loss: 0.90523005\n",
      "Step: [3369] d_loss: 1.30580568, g_loss: 0.85779911\n",
      "Step: [3370] d_loss: 1.26093483, g_loss: 0.88301748\n",
      "Step: [3371] d_loss: 1.31730485, g_loss: 0.84667766\n",
      "Step: [3372] d_loss: 1.30388939, g_loss: 0.86860764\n",
      "Step: [3373] d_loss: 1.30230331, g_loss: 0.89128196\n",
      "Step: [3374] d_loss: 1.24073052, g_loss: 0.92267394\n",
      "Step: [3375] d_loss: 1.31630695, g_loss: 0.84106708\n",
      "Step: [3376] d_loss: 1.26315427, g_loss: 0.85677135\n",
      "Step: [3377] d_loss: 1.28511822, g_loss: 0.86287767\n",
      "Step: [3378] d_loss: 1.26295996, g_loss: 0.89283800\n",
      "Step: [3379] d_loss: 1.26407957, g_loss: 0.91773319\n",
      "Step: [3380] d_loss: 1.33201504, g_loss: 0.87858194\n",
      "Step: [3381] d_loss: 1.29823017, g_loss: 0.85139239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3382] d_loss: 1.28218174, g_loss: 0.88974482\n",
      "Step: [3383] d_loss: 1.27877939, g_loss: 0.84062493\n",
      "Step: [3384] d_loss: 1.27514434, g_loss: 0.85384494\n",
      "Step: [3385] d_loss: 1.30023360, g_loss: 0.85297376\n",
      "Step: [3386] d_loss: 1.22108102, g_loss: 0.90575063\n",
      "Step: [3387] d_loss: 1.30016649, g_loss: 0.86979723\n",
      "Step: [3388] d_loss: 1.25325346, g_loss: 0.89607680\n",
      "Step: [3389] d_loss: 1.30867600, g_loss: 0.85087639\n",
      "Step: [3390] d_loss: 1.24445879, g_loss: 0.86207044\n",
      "Step: [3391] d_loss: 1.30845475, g_loss: 0.89245522\n",
      "Step: [3392] d_loss: 1.29488301, g_loss: 0.87490427\n",
      "Step: [3393] d_loss: 1.25496483, g_loss: 0.88500059\n",
      "Step: [3394] d_loss: 1.24542952, g_loss: 0.92096001\n",
      "Step: [3395] d_loss: 1.28516698, g_loss: 0.85670125\n",
      "Step: [3396] d_loss: 1.25821364, g_loss: 0.85652441\n",
      "Step: [3397] d_loss: 1.24202073, g_loss: 0.86224920\n",
      "Step: [3398] d_loss: 1.22504568, g_loss: 0.88445830\n",
      "Step: [3399] d_loss: 1.26336420, g_loss: 0.85370231\n",
      "Step: [3400] d_loss: 1.28869796, g_loss: 0.86520422\n",
      "Step: [3401] d_loss: 1.23729134, g_loss: 0.88278073\n",
      "Step: [3402] d_loss: 1.24419546, g_loss: 0.94348806\n",
      "Step: [3403] d_loss: 1.29505253, g_loss: 0.88611805\n",
      "Step: [3404] d_loss: 1.26661706, g_loss: 0.86807120\n",
      "Step: [3405] d_loss: 1.24339485, g_loss: 0.85160732\n",
      "Step: [3406] d_loss: 1.29840112, g_loss: 0.83448541\n",
      "Step: [3407] d_loss: 1.31261420, g_loss: 0.84315509\n",
      "Step: [3408] d_loss: 1.30393481, g_loss: 0.88775933\n",
      "Step: [3409] d_loss: 1.23102427, g_loss: 0.89764988\n",
      "Step: [3410] d_loss: 1.28211117, g_loss: 0.89761102\n",
      "Step: [3411] d_loss: 1.30943274, g_loss: 0.83258510\n",
      "Step: [3412] d_loss: 1.28540051, g_loss: 0.86659944\n",
      "Step: [3413] d_loss: 1.26642489, g_loss: 0.87333626\n",
      "Step: [3414] d_loss: 1.27932644, g_loss: 0.86022705\n",
      "Step: [3415] d_loss: 1.24392200, g_loss: 0.88854146\n",
      "Step: [3416] d_loss: 1.35474229, g_loss: 0.81668431\n",
      "Step: [3417] d_loss: 1.32296479, g_loss: 0.83672476\n",
      "Step: [3418] d_loss: 1.25205982, g_loss: 0.86524236\n",
      "Step: [3419] d_loss: 1.23149574, g_loss: 0.88593501\n",
      "Step: [3420] d_loss: 1.32837200, g_loss: 0.84638405\n",
      "Step: [3421] d_loss: 1.23698735, g_loss: 0.89371669\n",
      "Step: [3422] d_loss: 1.23644686, g_loss: 0.86937594\n",
      "Step: [3423] d_loss: 1.28456545, g_loss: 0.88400835\n",
      "Step: [3424] d_loss: 1.22276151, g_loss: 0.91177791\n",
      "Step: [3425] d_loss: 1.29491866, g_loss: 0.88588804\n",
      "Step: [3426] d_loss: 1.29019618, g_loss: 0.89722383\n",
      "Step: [3427] d_loss: 1.25130522, g_loss: 0.89012158\n",
      "Step: [3428] d_loss: 1.27361941, g_loss: 0.95070159\n",
      "Step: [3429] d_loss: 1.26855803, g_loss: 0.92632782\n",
      "Step: [3430] d_loss: 1.21430802, g_loss: 0.89941585\n",
      "Step: [3431] d_loss: 1.24134886, g_loss: 0.83948034\n",
      "Step: [3432] d_loss: 1.27081835, g_loss: 0.84354907\n",
      "Step: [3433] d_loss: 1.32977414, g_loss: 0.80988526\n",
      "Step: [3434] d_loss: 1.29735601, g_loss: 0.87779748\n",
      "Step: [3435] d_loss: 1.27822506, g_loss: 0.90346670\n",
      "Step: [3436] d_loss: 1.27263808, g_loss: 0.90852475\n",
      "Step: [3437] d_loss: 1.26373696, g_loss: 0.85794061\n",
      "Step: [3438] d_loss: 1.26474714, g_loss: 0.85972011\n",
      "Step: [3439] d_loss: 1.28529894, g_loss: 0.86507297\n",
      "Step: [3440] d_loss: 1.24657130, g_loss: 0.92123199\n",
      "Step: [3441] d_loss: 1.27521205, g_loss: 0.84064019\n",
      "Step: [3442] d_loss: 1.27321136, g_loss: 0.87384677\n",
      "Step: [3443] d_loss: 1.21432495, g_loss: 0.90111291\n",
      "Step: [3444] d_loss: 1.23613727, g_loss: 0.87092769\n",
      "Step: [3445] d_loss: 1.23196244, g_loss: 0.87652367\n",
      "Step: [3446] d_loss: 1.24293244, g_loss: 0.91600209\n",
      "Step: [3447] d_loss: 1.30110550, g_loss: 0.87836367\n",
      "Step: [3448] d_loss: 1.32449293, g_loss: 0.86656123\n",
      "Step: [3449] d_loss: 1.21596575, g_loss: 0.92118472\n",
      "Step: [3450] d_loss: 1.22077417, g_loss: 0.87446010\n",
      "Step: [3451] d_loss: 1.22406471, g_loss: 0.89882147\n",
      "Step: [3452] d_loss: 1.25982392, g_loss: 0.89415932\n",
      "Step: [3453] d_loss: 1.25956845, g_loss: 0.86751807\n",
      "Step: [3454] d_loss: 1.22379076, g_loss: 0.89908081\n",
      "Step: [3455] d_loss: 1.18523479, g_loss: 0.91587114\n",
      "Step: [3456] d_loss: 1.26241887, g_loss: 0.90839392\n",
      "Step: [3457] d_loss: 1.25169706, g_loss: 0.92741060\n",
      "Step: [3458] d_loss: 1.26947904, g_loss: 0.85638070\n",
      "Step: [3459] d_loss: 1.22448015, g_loss: 0.84865952\n",
      "Step: [3460] d_loss: 1.29794264, g_loss: 0.85289854\n",
      "Step: [3461] d_loss: 1.31271553, g_loss: 0.89120889\n",
      "Step: [3462] d_loss: 1.28795266, g_loss: 0.95407486\n",
      "Step: [3463] d_loss: 1.34037364, g_loss: 0.88522494\n",
      "Step: [3464] d_loss: 1.37269223, g_loss: 0.85874933\n",
      "Step: [3465] d_loss: 1.31700706, g_loss: 0.84782469\n",
      "Step: [3466] d_loss: 1.24073696, g_loss: 0.87750661\n",
      "Step: [3467] d_loss: 1.20948124, g_loss: 0.92064375\n",
      "Step: [3468] d_loss: 1.23326731, g_loss: 0.89224142\n",
      "Step: [3469] d_loss: 1.23031974, g_loss: 0.89391130\n",
      "Step: [3470] d_loss: 1.27614129, g_loss: 0.84337401\n",
      "Step: [3471] d_loss: 1.25272763, g_loss: 0.88666975\n",
      "Step: [3472] d_loss: 1.28886366, g_loss: 0.87612808\n",
      "Step: [3473] d_loss: 1.25142837, g_loss: 0.85939717\n",
      "Step: [3474] d_loss: 1.28705525, g_loss: 0.85612166\n",
      "Step: [3475] d_loss: 1.27335119, g_loss: 0.87889469\n",
      "Step: [3476] d_loss: 1.28272164, g_loss: 0.89072394\n",
      "Step: [3477] d_loss: 1.21109354, g_loss: 0.86613935\n",
      "Step: [3478] d_loss: 1.27244639, g_loss: 0.84242511\n",
      "Step: [3479] d_loss: 1.25847936, g_loss: 0.87556112\n",
      "Step: [3480] d_loss: 1.26146817, g_loss: 0.85481513\n",
      "Step: [3481] d_loss: 1.21561146, g_loss: 0.94740587\n",
      "Step: [3482] d_loss: 1.24779439, g_loss: 0.91116822\n",
      "Step: [3483] d_loss: 1.21301293, g_loss: 0.92134523\n",
      "Step: [3484] d_loss: 1.27903616, g_loss: 0.87160158\n",
      "Step: [3485] d_loss: 1.29839849, g_loss: 0.84486318\n",
      "Step: [3486] d_loss: 1.24175382, g_loss: 0.91596144\n",
      "Step: [3487] d_loss: 1.25912189, g_loss: 0.89368606\n",
      "Step: [3488] d_loss: 1.25271320, g_loss: 0.87962198\n",
      "Step: [3489] d_loss: 1.31504536, g_loss: 0.87103534\n",
      "Step: [3490] d_loss: 1.21221793, g_loss: 0.94624323\n",
      "Step: [3491] d_loss: 1.27318263, g_loss: 0.83378726\n",
      "Step: [3492] d_loss: 1.24330914, g_loss: 0.86587280\n",
      "Step: [3493] d_loss: 1.21574569, g_loss: 0.90041029\n",
      "Step: [3494] d_loss: 1.23178935, g_loss: 0.89410400\n",
      "Step: [3495] d_loss: 1.20693684, g_loss: 0.93232524\n",
      "Step: [3496] d_loss: 1.24768448, g_loss: 0.87679970\n",
      "Step: [3497] d_loss: 1.21814179, g_loss: 0.88815951\n",
      "Step: [3498] d_loss: 1.27243042, g_loss: 0.83172405\n",
      "Step: [3499] d_loss: 1.28526068, g_loss: 0.87822366\n",
      "Step: [3500] d_loss: 1.23913383, g_loss: 0.90330720\n",
      "Step: [3501] d_loss: 1.30320060, g_loss: 0.87458730\n",
      "Step: [3502] d_loss: 1.25784075, g_loss: 0.88852781\n",
      "Step: [3503] d_loss: 1.26935005, g_loss: 0.86179852\n",
      "Step: [3504] d_loss: 1.22180772, g_loss: 0.90505451\n",
      "Step: [3505] d_loss: 1.25635266, g_loss: 0.89523011\n",
      "Step: [3506] d_loss: 1.25364304, g_loss: 0.87234902\n",
      "Step: [3507] d_loss: 1.23151803, g_loss: 0.92122000\n",
      "Step: [3508] d_loss: 1.26404953, g_loss: 0.90035498\n",
      "Step: [3509] d_loss: 1.23477077, g_loss: 0.91604602\n",
      "Step: [3510] d_loss: 1.24418604, g_loss: 0.92227507\n",
      "Step: [3511] d_loss: 1.26584196, g_loss: 0.87187386\n",
      "Step: [3512] d_loss: 1.28762734, g_loss: 0.90995300\n",
      "Step: [3513] d_loss: 1.22220707, g_loss: 0.91082418\n",
      "Step: [3514] d_loss: 1.28379226, g_loss: 0.84931970\n",
      "Step: [3515] d_loss: 1.27276778, g_loss: 0.87938124\n",
      "Step: [3516] d_loss: 1.33275700, g_loss: 0.89549494\n",
      "Step: [3517] d_loss: 1.27298677, g_loss: 0.90945506\n",
      "Step: [3518] d_loss: 1.34402061, g_loss: 0.93535328\n",
      "Step: [3519] d_loss: 1.26631427, g_loss: 0.88674927\n",
      "Step: [3520] d_loss: 1.29662991, g_loss: 0.85051966\n",
      "Step: [3521] d_loss: 1.27937281, g_loss: 0.86025870\n",
      "Step: [3522] d_loss: 1.29009879, g_loss: 0.86075318\n",
      "Step: [3523] d_loss: 1.24840927, g_loss: 0.89626503\n",
      "Step: [3524] d_loss: 1.25155854, g_loss: 0.90470088\n",
      "Step: [3525] d_loss: 1.26818752, g_loss: 0.89360213\n",
      "Step: [3526] d_loss: 1.25517344, g_loss: 0.89233881\n",
      "Step: [3527] d_loss: 1.22860456, g_loss: 0.91340578\n",
      "Step: [3528] d_loss: 1.25625730, g_loss: 0.88372856\n",
      "Step: [3529] d_loss: 1.25205040, g_loss: 0.91847038\n",
      "Step: [3530] d_loss: 1.25594282, g_loss: 0.91373408\n",
      "Step: [3531] d_loss: 1.26856351, g_loss: 0.89432418\n",
      "Step: [3532] d_loss: 1.23745954, g_loss: 0.90868306\n",
      "Step: [3533] d_loss: 1.28837144, g_loss: 0.85265923\n",
      "Step: [3534] d_loss: 1.22168124, g_loss: 0.89646542\n",
      "Step: [3535] d_loss: 1.25910544, g_loss: 0.86979210\n",
      "Step: [3536] d_loss: 1.21990800, g_loss: 0.94839680\n",
      "Step: [3537] d_loss: 1.26629424, g_loss: 0.93723512\n",
      "Step: [3538] d_loss: 1.28113031, g_loss: 0.92539310\n",
      "Step: [3539] d_loss: 1.28891015, g_loss: 0.82750124\n",
      "Step: [3540] d_loss: 1.30941844, g_loss: 0.82036477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3541] d_loss: 1.24536407, g_loss: 0.85851973\n",
      "Step: [3542] d_loss: 1.27318251, g_loss: 0.85040766\n",
      "Step: [3543] d_loss: 1.25120211, g_loss: 0.90428698\n",
      "Step: [3544] d_loss: 1.26428485, g_loss: 0.89775777\n",
      "Step: [3545] d_loss: 1.24066794, g_loss: 0.90226603\n",
      "Step: [3546] d_loss: 1.26421762, g_loss: 0.87585318\n",
      "Step: [3547] d_loss: 1.26481640, g_loss: 0.85540485\n",
      "Step: [3548] d_loss: 1.27002966, g_loss: 0.88370484\n",
      "Step: [3549] d_loss: 1.22547162, g_loss: 0.88648129\n",
      "Step: [3550] d_loss: 1.24505329, g_loss: 0.87473989\n",
      "Step: [3551] d_loss: 1.25556374, g_loss: 0.85817492\n",
      "Step: [3552] d_loss: 1.24588633, g_loss: 0.86195642\n",
      "Step: [3553] d_loss: 1.22803891, g_loss: 0.87444311\n",
      "Step: [3554] d_loss: 1.23996282, g_loss: 0.91698158\n",
      "Step: [3555] d_loss: 1.26958334, g_loss: 0.91021454\n",
      "Step: [3556] d_loss: 1.27239347, g_loss: 0.89522207\n",
      "Step: [3557] d_loss: 1.28129363, g_loss: 0.83805406\n",
      "Step: [3558] d_loss: 1.24280047, g_loss: 0.89178145\n",
      "Step: [3559] d_loss: 1.20851755, g_loss: 0.89239061\n",
      "Step: [3560] d_loss: 1.30382013, g_loss: 0.82850552\n",
      "Step: [3561] d_loss: 1.27754974, g_loss: 0.91071910\n",
      "Step: [3562] d_loss: 1.32313001, g_loss: 0.90436077\n",
      "Step: [3563] d_loss: 1.23053360, g_loss: 0.85202694\n",
      "Step: [3564] d_loss: 1.26646352, g_loss: 0.82313555\n",
      "Step: [3565] d_loss: 1.22626019, g_loss: 0.89616382\n",
      "Step: [3566] d_loss: 1.27763355, g_loss: 0.94114244\n",
      "Step: [3567] d_loss: 1.31368399, g_loss: 0.95781636\n",
      "Step: [3568] d_loss: 1.29545546, g_loss: 0.85203004\n",
      "Step: [3569] d_loss: 1.24806762, g_loss: 0.80710453\n",
      "Step: [3570] d_loss: 1.27406406, g_loss: 0.83824909\n",
      "Step: [3571] d_loss: 1.28454220, g_loss: 0.88085997\n",
      "Step: [3572] d_loss: 1.27416515, g_loss: 0.87113523\n",
      "Step: [3573] d_loss: 1.23632109, g_loss: 0.89858210\n",
      "Step: [3574] d_loss: 1.26649368, g_loss: 0.92858982\n",
      "Step: [3575] d_loss: 1.22261071, g_loss: 0.89036864\n",
      "Step: [3576] d_loss: 1.22773433, g_loss: 0.87373877\n",
      "Step: [3577] d_loss: 1.27045345, g_loss: 0.85700285\n",
      "Step: [3578] d_loss: 1.24935687, g_loss: 0.90485567\n",
      "Step: [3579] d_loss: 1.25786757, g_loss: 0.91892338\n",
      "Step: [3580] d_loss: 1.28719401, g_loss: 0.90290159\n",
      "Step: [3581] d_loss: 1.25174451, g_loss: 0.85508114\n",
      "Step: [3582] d_loss: 1.29395938, g_loss: 0.85783303\n",
      "Step: [3583] d_loss: 1.20915246, g_loss: 0.89752740\n",
      "Step: [3584] d_loss: 1.22622955, g_loss: 0.90910280\n",
      "Step: [3585] d_loss: 1.23827541, g_loss: 0.91631258\n",
      "Step: [3586] d_loss: 1.26046920, g_loss: 0.91527522\n",
      "Step: [3587] d_loss: 1.28070974, g_loss: 0.87545037\n",
      "Step: [3588] d_loss: 1.20231891, g_loss: 0.90803182\n",
      "Step: [3589] d_loss: 1.23936558, g_loss: 0.87071609\n",
      "Step: [3590] d_loss: 1.22457623, g_loss: 0.91037035\n",
      "Step: [3591] d_loss: 1.21784163, g_loss: 0.93633974\n",
      "Step: [3592] d_loss: 1.23538494, g_loss: 0.84864074\n",
      "Step: [3593] d_loss: 1.21289754, g_loss: 0.94590598\n",
      "Step: [3594] d_loss: 1.26841831, g_loss: 0.87828618\n",
      "Step: [3595] d_loss: 1.26805556, g_loss: 0.86119550\n",
      "Step: [3596] d_loss: 1.28978109, g_loss: 0.87087548\n",
      "Step: [3597] d_loss: 1.30219531, g_loss: 0.87181711\n",
      "Step: [3598] d_loss: 1.23766458, g_loss: 0.85956359\n",
      "Step: [3599] d_loss: 1.28610396, g_loss: 0.88031280\n",
      "Step: [3600] d_loss: 1.24592066, g_loss: 0.87684715\n",
      "Step: [3601] d_loss: 1.30879998, g_loss: 0.84262788\n",
      "Step: [3602] d_loss: 1.27788925, g_loss: 0.82395363\n",
      "Step: [3603] d_loss: 1.32388341, g_loss: 0.84406823\n",
      "Step: [3604] d_loss: 1.29022765, g_loss: 0.84442282\n",
      "Step: [3605] d_loss: 1.30584598, g_loss: 0.86106658\n",
      "Step: [3606] d_loss: 1.25491691, g_loss: 0.91216445\n",
      "Step: [3607] d_loss: 1.23856461, g_loss: 0.93426692\n",
      "Step: [3608] d_loss: 1.24304402, g_loss: 0.98548412\n",
      "Step: [3609] d_loss: 1.25769401, g_loss: 0.90468395\n",
      "Step: [3610] d_loss: 1.23448074, g_loss: 0.90688443\n",
      "Step: [3611] d_loss: 1.23267031, g_loss: 0.92124879\n",
      "Step: [3612] d_loss: 1.23107433, g_loss: 0.89569682\n",
      "Step: [3613] d_loss: 1.27872610, g_loss: 0.87360156\n",
      "Step: [3614] d_loss: 1.26309299, g_loss: 0.88594794\n",
      "Step: [3615] d_loss: 1.31129932, g_loss: 0.88905156\n",
      "Step: [3616] d_loss: 1.26591110, g_loss: 0.85281020\n",
      "Step: [3617] d_loss: 1.26910472, g_loss: 0.83395135\n",
      "Step: [3618] d_loss: 1.26541710, g_loss: 0.86628044\n",
      "Step: [3619] d_loss: 1.25112057, g_loss: 0.86631322\n",
      "Step: [3620] d_loss: 1.30241442, g_loss: 0.83810318\n",
      "Step: [3621] d_loss: 1.23268974, g_loss: 0.96787941\n",
      "Step: [3622] d_loss: 1.25057185, g_loss: 0.91363055\n",
      "Step: [3623] d_loss: 1.24662662, g_loss: 0.85272205\n",
      "Step: [3624] d_loss: 1.22781777, g_loss: 0.84118283\n",
      "Step: [3625] d_loss: 1.21626616, g_loss: 0.89406425\n",
      "Step: [3626] d_loss: 1.23168325, g_loss: 0.91522014\n",
      "Step: [3627] d_loss: 1.22578621, g_loss: 0.90377253\n",
      "Step: [3628] d_loss: 1.28381932, g_loss: 0.92135692\n",
      "Step: [3629] d_loss: 1.27107692, g_loss: 0.88723028\n",
      "Step: [3630] d_loss: 1.30983520, g_loss: 0.87866807\n",
      "Step: [3631] d_loss: 1.25351191, g_loss: 0.90329075\n",
      "Step: [3632] d_loss: 1.24125934, g_loss: 0.90037400\n",
      "Step: [3633] d_loss: 1.22554564, g_loss: 0.90010679\n",
      "Step: [3634] d_loss: 1.24450040, g_loss: 0.88476044\n",
      "Step: [3635] d_loss: 1.25334418, g_loss: 0.87533164\n",
      "Step: [3636] d_loss: 1.26590157, g_loss: 0.83653480\n",
      "Step: [3637] d_loss: 1.26566911, g_loss: 0.88402379\n",
      "Step: [3638] d_loss: 1.20083261, g_loss: 0.91315734\n",
      "Step: [3639] d_loss: 1.29012203, g_loss: 0.86167830\n",
      "Step: [3640] d_loss: 1.28001690, g_loss: 0.86700439\n",
      "Step: [3641] d_loss: 1.24502802, g_loss: 0.89664334\n",
      "Step: [3642] d_loss: 1.26256442, g_loss: 0.91122115\n",
      "Step: [3643] d_loss: 1.22145224, g_loss: 0.90880787\n",
      "Step: [3644] d_loss: 1.22217476, g_loss: 0.89456451\n",
      "Step: [3645] d_loss: 1.22676134, g_loss: 0.88557661\n",
      "Step: [3646] d_loss: 1.23663664, g_loss: 0.87953597\n",
      "Step: [3647] d_loss: 1.23746347, g_loss: 0.88688648\n",
      "Step: [3648] d_loss: 1.29677999, g_loss: 0.82377076\n",
      "Step: [3649] d_loss: 1.20871758, g_loss: 0.90472299\n",
      "Step: [3650] d_loss: 1.26884234, g_loss: 0.91149360\n",
      "Step: [3651] d_loss: 1.21607351, g_loss: 0.93286490\n",
      "Step: [3652] d_loss: 1.25380433, g_loss: 0.91375971\n",
      "Step: [3653] d_loss: 1.23738122, g_loss: 0.85368222\n",
      "Step: [3654] d_loss: 1.19391394, g_loss: 0.88090467\n",
      "Step: [3655] d_loss: 1.22349477, g_loss: 0.90897584\n",
      "Step: [3656] d_loss: 1.22734237, g_loss: 0.92471367\n",
      "Step: [3657] d_loss: 1.27082765, g_loss: 0.86397076\n",
      "Step: [3658] d_loss: 1.23342419, g_loss: 0.85962784\n",
      "Step: [3659] d_loss: 1.25152624, g_loss: 0.85819811\n",
      "Step: [3660] d_loss: 1.30083573, g_loss: 0.84568965\n",
      "Step: [3661] d_loss: 1.16841078, g_loss: 0.96546602\n",
      "Step: [3662] d_loss: 1.28630364, g_loss: 0.90155524\n",
      "Step: [3663] d_loss: 1.22716045, g_loss: 0.99016988\n",
      "Step: [3664] d_loss: 1.28379679, g_loss: 0.89110851\n",
      "Step: [3665] d_loss: 1.20047474, g_loss: 0.88576913\n",
      "Step: [3666] d_loss: 1.25588036, g_loss: 0.90311110\n",
      "Step: [3667] d_loss: 1.35240626, g_loss: 0.82405591\n",
      "Step: [3668] d_loss: 1.25578010, g_loss: 0.90823966\n",
      "Step: [3669] d_loss: 1.22887897, g_loss: 0.90082401\n",
      "Step: [3670] d_loss: 1.26485515, g_loss: 0.90717572\n",
      "Step: [3671] d_loss: 1.23447275, g_loss: 0.88084841\n",
      "Step: [3672] d_loss: 1.26096606, g_loss: 0.87941742\n",
      "Step: [3673] d_loss: 1.26347446, g_loss: 0.87284344\n",
      "Step: [3674] d_loss: 1.24514842, g_loss: 0.90690166\n",
      "Step: [3675] d_loss: 1.28146827, g_loss: 0.90332347\n",
      "Step: [3676] d_loss: 1.29707491, g_loss: 0.88961053\n",
      "Step: [3677] d_loss: 1.25082839, g_loss: 0.85166633\n",
      "Step: [3678] d_loss: 1.26501012, g_loss: 0.88686717\n",
      "Step: [3679] d_loss: 1.27646542, g_loss: 0.89817500\n",
      "Step: [3680] d_loss: 1.24405217, g_loss: 0.87557340\n",
      "Step: [3681] d_loss: 1.24072433, g_loss: 0.86092722\n",
      "Step: [3682] d_loss: 1.25381637, g_loss: 0.86863840\n",
      "Step: [3683] d_loss: 1.24423194, g_loss: 0.86511576\n",
      "Step: [3684] d_loss: 1.29784775, g_loss: 0.87607348\n",
      "Step: [3685] d_loss: 1.25501025, g_loss: 0.90182215\n",
      "Step: [3686] d_loss: 1.19861400, g_loss: 0.88101774\n",
      "Step: [3687] d_loss: 1.23050046, g_loss: 0.85950625\n",
      "Step: [3688] d_loss: 1.23611200, g_loss: 0.87801898\n",
      "Step: [3689] d_loss: 1.24129128, g_loss: 0.86237860\n",
      "Step: [3690] d_loss: 1.27302694, g_loss: 0.88826418\n",
      "Step: [3691] d_loss: 1.21676016, g_loss: 0.89403820\n",
      "Step: [3692] d_loss: 1.29860377, g_loss: 0.90211117\n",
      "Step: [3693] d_loss: 1.24739814, g_loss: 0.92857230\n",
      "Step: [3694] d_loss: 1.21588051, g_loss: 0.91801298\n",
      "Step: [3695] d_loss: 1.26254725, g_loss: 0.86988503\n",
      "Step: [3696] d_loss: 1.29933357, g_loss: 0.86425602\n",
      "Step: [3697] d_loss: 1.22645664, g_loss: 0.91475362\n",
      "Step: [3698] d_loss: 1.24796557, g_loss: 0.91336322\n",
      "Step: [3699] d_loss: 1.26536047, g_loss: 0.89962119\n",
      "Step: [3700] d_loss: 1.27180493, g_loss: 0.93151355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3701] d_loss: 1.22062373, g_loss: 0.91746056\n",
      "Step: [3702] d_loss: 1.20167017, g_loss: 0.87389481\n",
      "Step: [3703] d_loss: 1.26975179, g_loss: 0.86403632\n",
      "Step: [3704] d_loss: 1.26171756, g_loss: 0.87961423\n",
      "Step: [3705] d_loss: 1.25960183, g_loss: 0.91804236\n",
      "Step: [3706] d_loss: 1.30984139, g_loss: 0.84245509\n",
      "Step: [3707] d_loss: 1.28483701, g_loss: 0.84993529\n",
      "Step: [3708] d_loss: 1.25754714, g_loss: 0.87152189\n",
      "Step: [3709] d_loss: 1.28665590, g_loss: 0.86150306\n",
      "Step: [3710] d_loss: 1.28079402, g_loss: 0.87708133\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images = np.reshape(real_image_batch[0],[batch_size,28,28,1]).astype(np.float32)\n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "    # update D network\n",
    "    _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, z: batch_z})\n",
    "    # update G network\n",
    "    _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "\n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(counter, 300) == 0:\n",
    "\n",
    "        samples = sess.run(fake_images, feed_dict={z: sample_z})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :, :, :].squeeze()\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
