{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the without '_y' network structure borrow from https://github.com/xudonmao/LSGAN/blob/master/vgg.py   \n",
    "the with 'y' network structure borrow from https://github.com/xudonmao/LSGAN/blob/master/mcgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn(net,scope,is_training):\n",
    "    return tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope=scope)\n",
    "\n",
    "def conv(net,wscope,bscope,output_depth = 64, receptive_field=[5,5],stride=[2,2]):\n",
    "    shape = net.get_shape()\n",
    "    weights = tf.get_variable(wscope, receptive_field+[shape[-1], output_depth],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.nn.conv2d(net, weights, strides=[1]+stride+[1], padding='SAME')\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def linear(net,wscope,bscope,output_depth):\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "\n",
    "def deconv(net,wscope,bscope,output_shape,receptive_field=[5,5],stride=[2,2]):\n",
    "    weights = tf.get_variable(wscope, receptive_field+[output_shape[-1], net.get_shape()[-1]],\n",
    "                                       initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "    net = tf.nn.conv2d_transpose(net, weights, output_shape=output_shape, strides=[1]+stride+[1])\n",
    "    biases = tf.get_variable(bscope, [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def get_shape(net):\n",
    "    return net.get_shape().as_list()\n",
    "\n",
    "def mse(pred,label):\n",
    "    return tf.reduce_mean(tf.nn.l2_loss(pred - label)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"discriminator\" function, the \"discriminator_y\" only has more one \"y\" parameter than \"discriminator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inputs, batch_size, is_training=True, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: Conv -> lrelu'''\n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',64)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''2nd: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv2','d_bconv2',64*2)\n",
    "        net = bn(net, scope = 'd_bn2',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''3th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv3','d_bconv3',64*4)\n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        '''4th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv4','d_bconv4',64*8)\n",
    "        net = bn(net, scope = 'd_bn4',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        net = tf.reshape(net, [batch_size, -1])   \n",
    "\n",
    "        '''5th: linear '''\n",
    "        out_logit = linear(net,\"d_wlinear5\",\"d_blinear5\",1)\n",
    "        '''6th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "\n",
    "        return out, out_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_y(inputs, y, y_dim, batch_size, is_training=True, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator_y\", reuse=reuse):\n",
    "\n",
    "        '''0:label onehot -> linear'''\n",
    "        y_linear = linear(y,'d_wlinear0','d_linear0',256)\n",
    "        \n",
    "        yb = tf.reshape(y_linear,shape=[batch_size,1,1,256])\n",
    "        \n",
    "        '''1st:conv -> lrelu -> Concat'''\n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',inputs.get_shape()[-1]+256)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        net = tf.concat([net, yb*tf.ones(get_shape(net)[:-1]+[get_shape(yb)[-1]])],axis=3) \n",
    "        \n",
    "        '''2nd:conv -> bn -> lrelu -> reshape -> Concat'''\n",
    "        net = conv(net,'d_wconv2','d_bconv2',64+256)        \n",
    "        net = bn(net, scope = 'd_bn2',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        net = tf.reshape(net,[batch_size,-1])\n",
    "        net = tf.concat([net, y_linear],axis=1)\n",
    "\n",
    "        '''3th:linear -> bn -> lrelu -> Concat'''\n",
    "        net = linear(net,\"d_wlinear3\",\"d_blinear3\",1024)        \n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        net = tf.concat([net, y_linear],axis=1)\n",
    "        \n",
    "        '''4th: linear'''\n",
    "        out_logit = linear(net,\"d_wlinear4\",\"d_blinear4\",1)  \n",
    "        '''5th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)        \n",
    "\n",
    "        return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"generator\" function, the \"generator_y\" only has more one \"y\" parameter than \"generator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator( z, batch_size, output_height=28, output_width=28,output_depth=1, is_training=True, reuse=False):\n",
    "    \n",
    "    height0,width0 = output_height,output_width\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2)\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2)\n",
    "    height8,width8 = math.ceil(float(height4)/2),math.ceil(float(width4)/2)\n",
    "    height16,width16 = math.ceil(float(height8)/2),math.ceil(float(width8)/2)\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: linear -> reshape -> bn -> relu '''\n",
    "        net = linear(z,\"g_wlinear1\",\"g_blinear1\",64*4*height16*width16)\n",
    "        net = tf.reshape(net,[batch_size,height16,width16,64*4])\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        '''2nd: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height8, width8, 64*4]\n",
    "        net = deconv(net,'g_wdeconv2','g_bdeconv2',output_shape,[3,3],[2,2])\n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''2th-extend: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height8, width8, 64*4]\n",
    "        net = deconv(net,'g_wdeconv21','g_bdeconv21',output_shape,[3,3],[1,1])\n",
    "        net = bn(net,scope='g_bn21',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''3th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height4, width4, 64*4]\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape,[3,3],[2,2])\n",
    "        net = bn(net,scope='g_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''3th-extend: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height4, width4, 64*4]\n",
    "        net = deconv(net,'g_wdeconv31','g_bdeconv31',output_shape,[3,3],[1,1])\n",
    "        net = bn(net,scope='g_bn31',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''4th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height2, width2, 64*2]\n",
    "        net = deconv(net,'g_wdeconv4','g_bdeconv4',output_shape,[3,3])\n",
    "        net = bn(net,scope='g_bn4',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''5th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height0, width0, 64*1]\n",
    "        net = deconv(net,'g_wdeconv5','g_bdeconv5',output_shape,[3,3])\n",
    "        net = bn(net,scope='g_bn5',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''6th: deconv -> tanh '''\n",
    "        output_shape = [batch_size, height0, width0, output_depth]\n",
    "        net = deconv(net,'g_wdeconv6','g_bdeconv6',output_shape,[3,3],[1,1])\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_y( z, y, y_dim, batch_size, output_height, output_width,output_depth, is_training=True, reuse=False):\n",
    "    \n",
    "    height0,width0 = output_height,output_width\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2)\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2)\n",
    "    \n",
    "    with tf.variable_scope(\"generator_y\", reuse=reuse):\n",
    "        \n",
    "        '''0:label onehot -> linear'''\n",
    "        y_linear = linear(y,'g_wlinear0','g_linear0',256)\n",
    "        \n",
    "        yb = tf.reshape(y_linear,[batch_size,1,1,256])\n",
    "        \n",
    "        '''1st:Concat -> linear -> bn -> relu '''\n",
    "        z = tf.concat([z,y_linear],axis=1)\n",
    "        net = linear(z,\"g_wlinear1\",\"g_blinear1\",128*height4*width4)\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        net = tf.reshape(net,[batch_size,height4,width4,128])\n",
    "        \n",
    "        '''2nd: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height2, width2, 128]\n",
    "        net = deconv(net,'g_wdeconv2','g_bdeconv2',output_shape)     \n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''3th: deconv -> sigmoid '''\n",
    "        output_shape = [batch_size, height0, width0, output_depth]\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape)\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the value of \"y_flag\" to choose the different network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 1024\n",
    "y_dim = 10\n",
    "learning_rate = 0.0002 #0.001\n",
    "beta1 = 0.5\n",
    "y_flag = True\n",
    "output_height,output_width,output_depth = [28,28,1]\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "#labels\n",
    "y = tf.placeholder(tf.float32, [batch_size,y_dim], name='y')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the \"mse\" loss function borrowed from \"loss_l2\" in https://github.com/xudonmao/LSGAN/blob/master/vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "if not y_flag:\n",
    "    # output of D for real images\n",
    "    D_real, D_real_logits = discriminator(inputs, batch_size, is_training=True, reuse=False)\n",
    "    # output of D for fake images\n",
    "    G = generator(z, batch_size, is_training=True, reuse=False)\n",
    "    D_fake, D_fake_logits = discriminator(G, batch_size, is_training=True, reuse=True)\n",
    "else:  \n",
    "    D_real, D_real_logits = discriminator_y(inputs, y, y_dim, batch_size, is_training=True, reuse=False)\n",
    "    G = generator_y(z, y, y_dim, batch_size, output_height, output_width,output_depth, is_training=True, reuse=False)\n",
    "    D_fake, D_fake_logits = discriminator_y(G, y, y_dim, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "\n",
    "# get loss for discriminator\n",
    "d_loss_real = mse(D_real_logits,tf.ones_like(D_real_logits))\n",
    "d_loss_fake = mse(D_fake_logits,tf.zeros_like(D_fake_logits))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# get loss for generator\n",
    "g_loss = mse(D_fake_logits,tf.ones_like(D_fake_logits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate*5, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "if not y_flag:\n",
    "    fake_images = generator(z,batch_size, is_training=False, reuse=True)\n",
    "else:\n",
    "    fake_images = generator_y(z, y, y_dim, batch_size, output_height, output_width,output_depth, is_training=False, reuse=True)\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim))\n",
    "\n",
    "test_labels_onehot = np.zeros([batch_size, y_dim],dtype = np.float32)\n",
    "test_labels_onehot[np.arange(batch_size), np.arange(batch_size)%int(y_dim)] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 59.80548477, g_loss: 198.83297729\n",
      "Step: [1] d_loss: 704.10308838, g_loss: 31.14851761\n",
      "Step: [2] d_loss: 96.46445465, g_loss: 30.23890495\n",
      "Step: [3] d_loss: 80.44414520, g_loss: 25.73360443\n",
      "Step: [4] d_loss: 58.87501526, g_loss: 22.19931221\n",
      "Step: [5] d_loss: 33.26681519, g_loss: 19.41390991\n",
      "Step: [6] d_loss: 33.03694153, g_loss: 17.47875977\n",
      "Step: [7] d_loss: 29.14501381, g_loss: 16.15901756\n",
      "Step: [8] d_loss: 24.76525497, g_loss: 15.32536221\n",
      "Step: [9] d_loss: 23.73504448, g_loss: 13.89806366\n",
      "Step: [10] d_loss: 19.98196602, g_loss: 12.84476376\n",
      "Step: [11] d_loss: 23.16379547, g_loss: 12.90688992\n",
      "Step: [12] d_loss: 27.85366821, g_loss: 14.37609482\n",
      "Step: [13] d_loss: 37.55335617, g_loss: 13.52400398\n",
      "Step: [14] d_loss: 33.28873825, g_loss: 12.26737309\n",
      "Step: [15] d_loss: 16.50245094, g_loss: 12.29934502\n",
      "Step: [16] d_loss: 15.07438087, g_loss: 11.79363632\n",
      "Step: [17] d_loss: 15.42598629, g_loss: 11.70466423\n",
      "Step: [18] d_loss: 17.17868996, g_loss: 11.50112152\n",
      "Step: [19] d_loss: 15.08098793, g_loss: 11.19160938\n",
      "Step: [20] d_loss: 10.57997704, g_loss: 10.93328857\n",
      "Step: [21] d_loss: 11.15130997, g_loss: 10.26736641\n",
      "Step: [22] d_loss: 12.46465397, g_loss: 10.62137222\n",
      "Step: [23] d_loss: 12.61430550, g_loss: 9.48461437\n",
      "Step: [24] d_loss: 13.82164955, g_loss: 8.92901421\n",
      "Step: [25] d_loss: 13.45305920, g_loss: 9.06617260\n",
      "Step: [26] d_loss: 12.86699486, g_loss: 8.77327633\n",
      "Step: [27] d_loss: 15.50156212, g_loss: 8.98674774\n",
      "Step: [28] d_loss: 17.28386497, g_loss: 8.71140480\n",
      "Step: [29] d_loss: 18.89978600, g_loss: 9.41761780\n",
      "Step: [30] d_loss: 25.09692383, g_loss: 8.57778931\n",
      "Step: [31] d_loss: 25.61021996, g_loss: 9.57786751\n",
      "Step: [32] d_loss: 23.02672577, g_loss: 8.69332504\n",
      "Step: [33] d_loss: 18.41459274, g_loss: 7.96753407\n",
      "Step: [34] d_loss: 17.02973175, g_loss: 9.00461483\n",
      "Step: [35] d_loss: 15.93503380, g_loss: 8.30086136\n",
      "Step: [36] d_loss: 15.80493355, g_loss: 8.47895050\n",
      "Step: [37] d_loss: 18.14968491, g_loss: 9.29979134\n",
      "Step: [38] d_loss: 21.64308548, g_loss: 8.90957642\n",
      "Step: [39] d_loss: 19.65830421, g_loss: 8.46012306\n",
      "Step: [40] d_loss: 17.46029282, g_loss: 8.14459229\n",
      "Step: [41] d_loss: 14.81957817, g_loss: 8.22333527\n",
      "Step: [42] d_loss: 14.89809513, g_loss: 8.47373962\n",
      "Step: [43] d_loss: 15.06404686, g_loss: 9.01563644\n",
      "Step: [44] d_loss: 16.80693817, g_loss: 8.66047478\n",
      "Step: [45] d_loss: 17.55259323, g_loss: 8.56035042\n",
      "Step: [46] d_loss: 16.69849396, g_loss: 9.19936657\n",
      "Step: [47] d_loss: 18.99494553, g_loss: 9.85309219\n",
      "Step: [48] d_loss: 30.39303589, g_loss: 10.99558067\n",
      "Step: [49] d_loss: 33.48938751, g_loss: 9.53547287\n",
      "Step: [50] d_loss: 24.54086113, g_loss: 9.80009556\n",
      "Step: [51] d_loss: 19.28333473, g_loss: 8.83922672\n",
      "Step: [52] d_loss: 22.91745758, g_loss: 8.86882210\n",
      "Step: [53] d_loss: 20.59933853, g_loss: 9.39658165\n",
      "Step: [54] d_loss: 18.55493736, g_loss: 9.96831131\n",
      "Step: [55] d_loss: 23.47414017, g_loss: 8.50119686\n",
      "Step: [56] d_loss: 16.76772308, g_loss: 8.72990417\n",
      "Step: [57] d_loss: 20.19976997, g_loss: 8.61391449\n",
      "Step: [58] d_loss: 17.94460869, g_loss: 8.84267426\n",
      "Step: [59] d_loss: 20.00327682, g_loss: 8.91859150\n",
      "Step: [60] d_loss: 17.72804642, g_loss: 8.49081039\n",
      "Step: [61] d_loss: 17.44965935, g_loss: 8.65375710\n",
      "Step: [62] d_loss: 17.15754318, g_loss: 8.66269398\n",
      "Step: [63] d_loss: 18.38876534, g_loss: 8.42957211\n",
      "Step: [64] d_loss: 16.84303284, g_loss: 8.48545647\n",
      "Step: [65] d_loss: 16.23982239, g_loss: 8.74239540\n",
      "Step: [66] d_loss: 14.99751282, g_loss: 8.38249016\n",
      "Step: [67] d_loss: 15.48977089, g_loss: 8.33225250\n",
      "Step: [68] d_loss: 18.32796288, g_loss: 8.80924225\n",
      "Step: [69] d_loss: 16.73967552, g_loss: 8.53767014\n",
      "Step: [70] d_loss: 14.95251942, g_loss: 8.28267193\n",
      "Step: [71] d_loss: 15.10219479, g_loss: 8.04138947\n",
      "Step: [72] d_loss: 14.82225037, g_loss: 8.30362320\n",
      "Step: [73] d_loss: 15.23012543, g_loss: 7.91906786\n",
      "Step: [74] d_loss: 15.59691238, g_loss: 8.24921131\n",
      "Step: [75] d_loss: 15.34337425, g_loss: 8.60321712\n",
      "Step: [76] d_loss: 15.82134056, g_loss: 8.27773476\n",
      "Step: [77] d_loss: 15.29898643, g_loss: 8.82796955\n",
      "Step: [78] d_loss: 15.98047066, g_loss: 8.16400528\n",
      "Step: [79] d_loss: 14.38532639, g_loss: 8.20484638\n",
      "Step: [80] d_loss: 15.70386696, g_loss: 8.19651985\n",
      "Step: [81] d_loss: 16.52529526, g_loss: 9.19622040\n",
      "Step: [82] d_loss: 21.58845520, g_loss: 9.76892281\n",
      "Step: [83] d_loss: 20.88180542, g_loss: 7.89383125\n",
      "Step: [84] d_loss: 15.64104939, g_loss: 8.48316383\n",
      "Step: [85] d_loss: 18.80598831, g_loss: 9.61378098\n",
      "Step: [86] d_loss: 19.23277473, g_loss: 8.06275749\n",
      "Step: [87] d_loss: 14.90029716, g_loss: 8.24689293\n",
      "Step: [88] d_loss: 15.72234249, g_loss: 8.59027863\n",
      "Step: [89] d_loss: 18.61446953, g_loss: 9.12121582\n",
      "Step: [90] d_loss: 19.51211929, g_loss: 9.64459324\n",
      "Step: [91] d_loss: 17.13202286, g_loss: 8.58446217\n",
      "Step: [92] d_loss: 16.26313591, g_loss: 8.39787674\n",
      "Step: [93] d_loss: 15.82013321, g_loss: 7.79600239\n",
      "Step: [94] d_loss: 15.07131195, g_loss: 8.77494240\n",
      "Step: [95] d_loss: 18.03437042, g_loss: 7.98954153\n",
      "Step: [96] d_loss: 17.65783501, g_loss: 8.03148651\n",
      "Step: [97] d_loss: 14.88729382, g_loss: 7.73439789\n",
      "Step: [98] d_loss: 14.63977623, g_loss: 7.98978996\n",
      "Step: [99] d_loss: 15.58868217, g_loss: 7.79988432\n",
      "Step: [100] d_loss: 15.21749878, g_loss: 8.03899765\n",
      "Step: [101] d_loss: 15.27808762, g_loss: 8.74360085\n",
      "Step: [102] d_loss: 17.34509087, g_loss: 8.33973408\n",
      "Step: [103] d_loss: 16.30701637, g_loss: 8.57936859\n",
      "Step: [104] d_loss: 15.38571739, g_loss: 8.18665028\n",
      "Step: [105] d_loss: 15.15663815, g_loss: 8.34472466\n",
      "Step: [106] d_loss: 15.21433258, g_loss: 8.05019760\n",
      "Step: [107] d_loss: 16.37147522, g_loss: 8.71337414\n",
      "Step: [108] d_loss: 16.30158234, g_loss: 7.92060184\n",
      "Step: [109] d_loss: 14.60381317, g_loss: 7.81190777\n",
      "Step: [110] d_loss: 15.18278885, g_loss: 8.53645706\n",
      "Step: [111] d_loss: 15.69459534, g_loss: 8.63006592\n",
      "Step: [112] d_loss: 15.44334221, g_loss: 8.34127045\n",
      "Step: [113] d_loss: 16.82580566, g_loss: 8.71274853\n",
      "Step: [114] d_loss: 15.57010651, g_loss: 8.81543159\n",
      "Step: [115] d_loss: 15.63609791, g_loss: 9.28821564\n",
      "Step: [116] d_loss: 15.51394081, g_loss: 8.49294472\n",
      "Step: [117] d_loss: 14.18013954, g_loss: 8.14870262\n",
      "Step: [118] d_loss: 14.91810036, g_loss: 8.16906834\n",
      "Step: [119] d_loss: 14.76907158, g_loss: 7.95373297\n",
      "Step: [120] d_loss: 17.43737411, g_loss: 9.26780224\n",
      "Step: [121] d_loss: 17.15120316, g_loss: 8.41382980\n",
      "Step: [122] d_loss: 16.24162674, g_loss: 7.65289974\n",
      "Step: [123] d_loss: 15.02713013, g_loss: 8.22839832\n",
      "Step: [124] d_loss: 14.69344711, g_loss: 8.04456711\n",
      "Step: [125] d_loss: 15.06087685, g_loss: 8.74025536\n",
      "Step: [126] d_loss: 16.37773514, g_loss: 8.39010525\n",
      "Step: [127] d_loss: 16.12052155, g_loss: 7.93340397\n",
      "Step: [128] d_loss: 16.39884949, g_loss: 8.07379150\n",
      "Step: [129] d_loss: 17.16840744, g_loss: 8.10026169\n",
      "Step: [130] d_loss: 16.89408875, g_loss: 7.72031975\n",
      "Step: [131] d_loss: 17.60450745, g_loss: 8.29710388\n",
      "Step: [132] d_loss: 18.30501366, g_loss: 7.66126537\n",
      "Step: [133] d_loss: 15.39253902, g_loss: 8.21021938\n",
      "Step: [134] d_loss: 19.19220352, g_loss: 9.24223137\n",
      "Step: [135] d_loss: 18.99022675, g_loss: 8.07669640\n",
      "Step: [136] d_loss: 17.41363144, g_loss: 9.61909866\n",
      "Step: [137] d_loss: 19.69958496, g_loss: 13.26277447\n",
      "Step: [138] d_loss: 29.06227112, g_loss: 14.97676849\n",
      "Step: [139] d_loss: 30.25937653, g_loss: 10.13297081\n",
      "Step: [140] d_loss: 17.50722885, g_loss: 8.89821243\n",
      "Step: [141] d_loss: 15.09865189, g_loss: 8.63102818\n",
      "Step: [142] d_loss: 15.28919983, g_loss: 8.26996231\n",
      "Step: [143] d_loss: 15.04068184, g_loss: 7.67416286\n",
      "Step: [144] d_loss: 14.94855690, g_loss: 7.95205975\n",
      "Step: [145] d_loss: 15.60972214, g_loss: 8.14066315\n",
      "Step: [146] d_loss: 14.95596504, g_loss: 8.10364532\n",
      "Step: [147] d_loss: 13.49844265, g_loss: 7.87831879\n",
      "Step: [148] d_loss: 14.27262497, g_loss: 7.54670668\n",
      "Step: [149] d_loss: 14.34834480, g_loss: 8.25736618\n",
      "Step: [150] d_loss: 15.11633492, g_loss: 8.25735855\n",
      "Step: [151] d_loss: 14.37913513, g_loss: 7.83683634\n",
      "Step: [152] d_loss: 16.08501053, g_loss: 8.48231316\n",
      "Step: [153] d_loss: 18.12137985, g_loss: 7.61326313\n",
      "Step: [154] d_loss: 14.98102283, g_loss: 7.68711472\n",
      "Step: [155] d_loss: 15.77771568, g_loss: 7.53719187\n",
      "Step: [156] d_loss: 14.41743946, g_loss: 7.81748343\n",
      "Step: [157] d_loss: 15.17121506, g_loss: 7.24101448\n",
      "Step: [158] d_loss: 13.78454971, g_loss: 7.63693047\n",
      "Step: [159] d_loss: 14.07203102, g_loss: 8.53204250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [160] d_loss: 16.22226334, g_loss: 7.80475855\n",
      "Step: [161] d_loss: 16.44029999, g_loss: 8.43337727\n",
      "Step: [162] d_loss: 14.57096100, g_loss: 7.84488487\n",
      "Step: [163] d_loss: 15.20720387, g_loss: 8.55621147\n",
      "Step: [164] d_loss: 16.44831276, g_loss: 8.02939606\n",
      "Step: [165] d_loss: 15.50082397, g_loss: 7.82508564\n",
      "Step: [166] d_loss: 14.06492805, g_loss: 8.36922264\n",
      "Step: [167] d_loss: 15.02459812, g_loss: 8.32875919\n",
      "Step: [168] d_loss: 14.34440804, g_loss: 7.71554852\n",
      "Step: [169] d_loss: 14.45543861, g_loss: 8.29036045\n",
      "Step: [170] d_loss: 17.37260437, g_loss: 9.51391315\n",
      "Step: [171] d_loss: 18.63050270, g_loss: 8.52787399\n",
      "Step: [172] d_loss: 15.46462822, g_loss: 8.27594471\n",
      "Step: [173] d_loss: 15.45125771, g_loss: 7.83220863\n",
      "Step: [174] d_loss: 14.31082344, g_loss: 8.05735016\n",
      "Step: [175] d_loss: 14.90434837, g_loss: 7.57246876\n",
      "Step: [176] d_loss: 14.96989822, g_loss: 8.15033436\n",
      "Step: [177] d_loss: 16.94496155, g_loss: 8.56702995\n",
      "Step: [178] d_loss: 17.08020782, g_loss: 8.06845379\n",
      "Step: [179] d_loss: 16.55248260, g_loss: 8.47521687\n",
      "Step: [180] d_loss: 15.95118141, g_loss: 7.97698784\n",
      "Step: [181] d_loss: 15.74154854, g_loss: 7.48177624\n",
      "Step: [182] d_loss: 14.59061432, g_loss: 8.02733994\n",
      "Step: [183] d_loss: 15.69454193, g_loss: 8.51859951\n",
      "Step: [184] d_loss: 15.46405792, g_loss: 7.74376917\n",
      "Step: [185] d_loss: 14.00112724, g_loss: 8.65636730\n",
      "Step: [186] d_loss: 20.96080017, g_loss: 8.17434311\n",
      "Step: [187] d_loss: 17.35185432, g_loss: 7.73342943\n",
      "Step: [188] d_loss: 14.31853962, g_loss: 7.22597933\n",
      "Step: [189] d_loss: 15.08670807, g_loss: 8.60287952\n",
      "Step: [190] d_loss: 16.85297394, g_loss: 8.15490341\n",
      "Step: [191] d_loss: 16.22457123, g_loss: 8.08046913\n",
      "Step: [192] d_loss: 14.96140957, g_loss: 8.17445755\n",
      "Step: [193] d_loss: 14.27540112, g_loss: 7.85560799\n",
      "Step: [194] d_loss: 14.36799431, g_loss: 8.00115967\n",
      "Step: [195] d_loss: 15.18516254, g_loss: 7.67387867\n",
      "Step: [196] d_loss: 15.40067863, g_loss: 8.52367306\n",
      "Step: [197] d_loss: 14.90476990, g_loss: 8.04608440\n",
      "Step: [198] d_loss: 14.12661839, g_loss: 8.69497681\n",
      "Step: [199] d_loss: 14.77840233, g_loss: 7.97926140\n",
      "Step: [200] d_loss: 14.78832436, g_loss: 8.23735428\n",
      "Step: [201] d_loss: 16.04841614, g_loss: 8.27265644\n",
      "Step: [202] d_loss: 16.70376968, g_loss: 8.09714890\n",
      "Step: [203] d_loss: 15.28574753, g_loss: 8.18683243\n",
      "Step: [204] d_loss: 14.20227909, g_loss: 8.01770020\n",
      "Step: [205] d_loss: 15.44484329, g_loss: 7.82445812\n",
      "Step: [206] d_loss: 13.59303665, g_loss: 7.63692379\n",
      "Step: [207] d_loss: 14.08243752, g_loss: 8.02310562\n",
      "Step: [208] d_loss: 16.38315582, g_loss: 8.13699055\n",
      "Step: [209] d_loss: 15.32808876, g_loss: 7.92157745\n",
      "Step: [210] d_loss: 16.30513763, g_loss: 7.79991055\n",
      "Step: [211] d_loss: 15.33765793, g_loss: 7.94300604\n",
      "Step: [212] d_loss: 15.36483097, g_loss: 8.01794052\n",
      "Step: [213] d_loss: 14.60034752, g_loss: 7.87252426\n",
      "Step: [214] d_loss: 14.81935120, g_loss: 8.24056721\n",
      "Step: [215] d_loss: 13.76826096, g_loss: 7.56298780\n",
      "Step: [216] d_loss: 15.08620071, g_loss: 7.98609209\n",
      "Step: [217] d_loss: 15.88744164, g_loss: 8.11014366\n",
      "Step: [218] d_loss: 16.21015930, g_loss: 7.22301674\n",
      "Step: [219] d_loss: 13.79191017, g_loss: 7.58654928\n",
      "Step: [220] d_loss: 14.39500046, g_loss: 7.75969410\n",
      "Step: [221] d_loss: 15.50726604, g_loss: 7.36094093\n",
      "Step: [222] d_loss: 14.89880371, g_loss: 7.02510834\n",
      "Step: [223] d_loss: 14.94876003, g_loss: 7.31799030\n",
      "Step: [224] d_loss: 15.00808811, g_loss: 7.91404247\n",
      "Step: [225] d_loss: 15.59091187, g_loss: 7.44690847\n",
      "Step: [226] d_loss: 15.72028351, g_loss: 7.45817757\n",
      "Step: [227] d_loss: 14.74461555, g_loss: 7.86912823\n",
      "Step: [228] d_loss: 14.23907661, g_loss: 8.64035034\n",
      "Step: [229] d_loss: 15.23882484, g_loss: 8.23084927\n",
      "Step: [230] d_loss: 15.74482822, g_loss: 9.10357380\n",
      "Step: [231] d_loss: 16.77419662, g_loss: 7.42876625\n",
      "Step: [232] d_loss: 15.16383934, g_loss: 7.50959206\n",
      "Step: [233] d_loss: 14.48853874, g_loss: 8.03443813\n",
      "Step: [234] d_loss: 14.91771889, g_loss: 8.06794930\n",
      "Step: [235] d_loss: 15.70366859, g_loss: 8.22884560\n",
      "Step: [236] d_loss: 13.31003189, g_loss: 7.82775736\n",
      "Step: [237] d_loss: 13.60288525, g_loss: 7.87609100\n",
      "Step: [238] d_loss: 13.60190487, g_loss: 7.86638069\n",
      "Step: [239] d_loss: 12.95953083, g_loss: 8.01246166\n",
      "Step: [240] d_loss: 14.30673409, g_loss: 6.48512936\n",
      "Step: [241] d_loss: 13.67858791, g_loss: 6.85443783\n",
      "Step: [242] d_loss: 13.22377110, g_loss: 7.26619387\n",
      "Step: [243] d_loss: 15.08201027, g_loss: 6.96931124\n",
      "Step: [244] d_loss: 15.40866566, g_loss: 6.84939671\n",
      "Step: [245] d_loss: 14.66885185, g_loss: 7.35810471\n",
      "Step: [246] d_loss: 14.99246216, g_loss: 7.25794697\n",
      "Step: [247] d_loss: 13.99238968, g_loss: 7.43170643\n",
      "Step: [248] d_loss: 15.57035828, g_loss: 6.98965073\n",
      "Step: [249] d_loss: 14.95281410, g_loss: 7.12929869\n",
      "Step: [250] d_loss: 15.20507431, g_loss: 7.29403353\n",
      "Step: [251] d_loss: 15.12242985, g_loss: 7.60642099\n",
      "Step: [252] d_loss: 16.18776131, g_loss: 7.68706799\n",
      "Step: [253] d_loss: 14.73145390, g_loss: 7.57393122\n",
      "Step: [254] d_loss: 14.60929871, g_loss: 7.96197128\n",
      "Step: [255] d_loss: 15.85507965, g_loss: 8.20156288\n",
      "Step: [256] d_loss: 14.53710365, g_loss: 7.55918503\n",
      "Step: [257] d_loss: 14.03807259, g_loss: 8.43617916\n",
      "Step: [258] d_loss: 14.75170422, g_loss: 8.31615257\n",
      "Step: [259] d_loss: 13.53019714, g_loss: 8.43267441\n",
      "Step: [260] d_loss: 14.26551056, g_loss: 8.64210033\n",
      "Step: [261] d_loss: 14.11757469, g_loss: 8.15307045\n",
      "Step: [262] d_loss: 12.87234974, g_loss: 8.38756847\n",
      "Step: [263] d_loss: 13.89661217, g_loss: 7.97472572\n",
      "Step: [264] d_loss: 14.81459713, g_loss: 9.29338455\n",
      "Step: [265] d_loss: 15.91322708, g_loss: 8.32000065\n",
      "Step: [266] d_loss: 14.37247372, g_loss: 8.19142818\n",
      "Step: [267] d_loss: 14.42919922, g_loss: 6.89019585\n",
      "Step: [268] d_loss: 13.47662449, g_loss: 7.29656315\n",
      "Step: [269] d_loss: 14.90104866, g_loss: 7.29516459\n",
      "Step: [270] d_loss: 15.69359016, g_loss: 6.90211058\n",
      "Step: [271] d_loss: 13.58313751, g_loss: 7.50901413\n",
      "Step: [272] d_loss: 14.84044170, g_loss: 7.45056105\n",
      "Step: [273] d_loss: 16.78749657, g_loss: 6.90145874\n",
      "Step: [274] d_loss: 14.91533470, g_loss: 7.06826782\n",
      "Step: [275] d_loss: 14.62061501, g_loss: 8.66637135\n",
      "Step: [276] d_loss: 14.87428284, g_loss: 7.59893703\n",
      "Step: [277] d_loss: 14.67873955, g_loss: 8.40046787\n",
      "Step: [278] d_loss: 15.51536846, g_loss: 7.77966595\n",
      "Step: [279] d_loss: 14.86948872, g_loss: 8.75954056\n",
      "Step: [280] d_loss: 16.84513092, g_loss: 8.04506111\n",
      "Step: [281] d_loss: 16.10251617, g_loss: 8.24385357\n",
      "Step: [282] d_loss: 15.47300720, g_loss: 8.79209232\n",
      "Step: [283] d_loss: 15.25852489, g_loss: 8.72247696\n",
      "Step: [284] d_loss: 14.38215637, g_loss: 8.25677872\n",
      "Step: [285] d_loss: 14.79838943, g_loss: 8.23577309\n",
      "Step: [286] d_loss: 13.76502705, g_loss: 8.20111084\n",
      "Step: [287] d_loss: 12.87223721, g_loss: 8.34413528\n",
      "Step: [288] d_loss: 14.74316788, g_loss: 7.92468929\n",
      "Step: [289] d_loss: 13.34731293, g_loss: 7.83128166\n",
      "Step: [290] d_loss: 13.50480461, g_loss: 8.32215118\n",
      "Step: [291] d_loss: 14.84738159, g_loss: 7.15094614\n",
      "Step: [292] d_loss: 13.88092613, g_loss: 7.67961502\n",
      "Step: [293] d_loss: 14.53151321, g_loss: 6.60491467\n",
      "Step: [294] d_loss: 13.13294411, g_loss: 7.62218809\n",
      "Step: [295] d_loss: 14.32300758, g_loss: 8.14870453\n",
      "Step: [296] d_loss: 14.94373322, g_loss: 7.58880901\n",
      "Step: [297] d_loss: 14.86438370, g_loss: 8.69544983\n",
      "Step: [298] d_loss: 17.56974030, g_loss: 7.26952744\n",
      "Step: [299] d_loss: 14.68611240, g_loss: 7.39377499\n",
      "Step: [300] d_loss: 17.15336227, g_loss: 6.62176991\n",
      "Step: [301] d_loss: 15.99247932, g_loss: 6.92290783\n",
      "Step: [302] d_loss: 16.87728310, g_loss: 7.40780640\n",
      "Step: [303] d_loss: 19.01643372, g_loss: 11.19746494\n",
      "Step: [304] d_loss: 25.78512573, g_loss: 11.07117653\n",
      "Step: [305] d_loss: 23.54026413, g_loss: 9.12303543\n",
      "Step: [306] d_loss: 17.47832108, g_loss: 8.60227966\n",
      "Step: [307] d_loss: 16.66627502, g_loss: 8.71104431\n",
      "Step: [308] d_loss: 15.69315338, g_loss: 8.22369957\n",
      "Step: [309] d_loss: 15.60132599, g_loss: 8.39488125\n",
      "Step: [310] d_loss: 14.29009628, g_loss: 8.51247406\n",
      "Step: [311] d_loss: 13.12004089, g_loss: 8.66982651\n",
      "Step: [312] d_loss: 14.67429161, g_loss: 7.94888973\n",
      "Step: [313] d_loss: 14.63969231, g_loss: 8.43194485\n",
      "Step: [314] d_loss: 14.04333115, g_loss: 7.84970045\n",
      "Step: [315] d_loss: 14.18807411, g_loss: 8.18104458\n",
      "Step: [316] d_loss: 14.02289009, g_loss: 7.56614971\n",
      "Step: [317] d_loss: 14.24687576, g_loss: 8.00673676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [318] d_loss: 14.25713348, g_loss: 7.45875263\n",
      "Step: [319] d_loss: 14.13189793, g_loss: 7.88650370\n",
      "Step: [320] d_loss: 14.76621819, g_loss: 7.28332901\n",
      "Step: [321] d_loss: 14.39624786, g_loss: 8.62301826\n",
      "Step: [322] d_loss: 15.06126499, g_loss: 8.25271034\n",
      "Step: [323] d_loss: 14.40639591, g_loss: 8.00706577\n",
      "Step: [324] d_loss: 14.48849678, g_loss: 7.99848795\n",
      "Step: [325] d_loss: 16.44473267, g_loss: 8.70788670\n",
      "Step: [326] d_loss: 18.27835464, g_loss: 10.03105736\n",
      "Step: [327] d_loss: 20.91108894, g_loss: 10.37660027\n",
      "Step: [328] d_loss: 20.30762863, g_loss: 9.84776402\n",
      "Step: [329] d_loss: 17.08173561, g_loss: 7.88208103\n",
      "Step: [330] d_loss: 16.32494736, g_loss: 7.26143026\n",
      "Step: [331] d_loss: 16.42480278, g_loss: 7.62334442\n",
      "Step: [332] d_loss: 15.71599293, g_loss: 6.98663568\n",
      "Step: [333] d_loss: 14.19481564, g_loss: 8.19939995\n",
      "Step: [334] d_loss: 14.78369713, g_loss: 7.17254591\n",
      "Step: [335] d_loss: 15.58484840, g_loss: 6.91064739\n",
      "Step: [336] d_loss: 14.89207268, g_loss: 7.00489187\n",
      "Step: [337] d_loss: 14.43219185, g_loss: 7.37792873\n",
      "Step: [338] d_loss: 15.66027069, g_loss: 6.96739197\n",
      "Step: [339] d_loss: 15.84423542, g_loss: 7.73010540\n",
      "Step: [340] d_loss: 16.55703545, g_loss: 6.93449259\n",
      "Step: [341] d_loss: 15.31534195, g_loss: 7.39984512\n",
      "Step: [342] d_loss: 15.32349491, g_loss: 7.28392124\n",
      "Step: [343] d_loss: 15.11019802, g_loss: 7.23847961\n",
      "Step: [344] d_loss: 14.27791023, g_loss: 7.78509665\n",
      "Step: [345] d_loss: 15.06019974, g_loss: 7.35941696\n",
      "Step: [346] d_loss: 13.85091972, g_loss: 7.73230076\n",
      "Step: [347] d_loss: 14.83722305, g_loss: 7.51634073\n",
      "Step: [348] d_loss: 15.84019279, g_loss: 7.62055111\n",
      "Step: [349] d_loss: 13.12968445, g_loss: 7.47822618\n",
      "Step: [350] d_loss: 13.75038528, g_loss: 7.55900145\n",
      "Step: [351] d_loss: 14.66433334, g_loss: 7.81656075\n",
      "Step: [352] d_loss: 14.28712559, g_loss: 7.41886044\n",
      "Step: [353] d_loss: 13.54442406, g_loss: 7.60203886\n",
      "Step: [354] d_loss: 14.09618950, g_loss: 7.40919733\n",
      "Step: [355] d_loss: 14.06140518, g_loss: 7.52612972\n",
      "Step: [356] d_loss: 14.93172169, g_loss: 8.22633839\n",
      "Step: [357] d_loss: 16.37578201, g_loss: 7.76179314\n",
      "Step: [358] d_loss: 15.69257355, g_loss: 8.29862976\n",
      "Step: [359] d_loss: 15.36903286, g_loss: 7.63779545\n",
      "Step: [360] d_loss: 14.82741547, g_loss: 7.57113504\n",
      "Step: [361] d_loss: 15.33196449, g_loss: 7.34089565\n",
      "Step: [362] d_loss: 14.88674927, g_loss: 7.81522083\n",
      "Step: [363] d_loss: 14.94463253, g_loss: 7.06367683\n",
      "Step: [364] d_loss: 14.13459969, g_loss: 7.69368458\n",
      "Step: [365] d_loss: 13.87803841, g_loss: 7.26766968\n",
      "Step: [366] d_loss: 15.00963879, g_loss: 6.56234407\n",
      "Step: [367] d_loss: 14.32355499, g_loss: 6.63612986\n",
      "Step: [368] d_loss: 14.88892937, g_loss: 7.19120789\n",
      "Step: [369] d_loss: 14.95063782, g_loss: 6.86893845\n",
      "Step: [370] d_loss: 15.07608604, g_loss: 7.41223335\n",
      "Step: [371] d_loss: 14.93208313, g_loss: 7.89956141\n",
      "Step: [372] d_loss: 14.78002739, g_loss: 7.89987326\n",
      "Step: [373] d_loss: 14.74455261, g_loss: 7.63214588\n",
      "Step: [374] d_loss: 14.11169338, g_loss: 8.22727966\n",
      "Step: [375] d_loss: 14.95874214, g_loss: 7.37381887\n",
      "Step: [376] d_loss: 14.99563694, g_loss: 7.73510647\n",
      "Step: [377] d_loss: 15.10727119, g_loss: 7.13139915\n",
      "Step: [378] d_loss: 14.03709793, g_loss: 7.23194027\n",
      "Step: [379] d_loss: 15.16778660, g_loss: 6.91423225\n",
      "Step: [380] d_loss: 15.25845909, g_loss: 7.34728765\n",
      "Step: [381] d_loss: 16.80646515, g_loss: 8.16885471\n",
      "Step: [382] d_loss: 16.24492264, g_loss: 7.04649734\n",
      "Step: [383] d_loss: 14.34232712, g_loss: 6.86920357\n",
      "Step: [384] d_loss: 15.09633064, g_loss: 6.86288500\n",
      "Step: [385] d_loss: 15.31049728, g_loss: 7.02903891\n",
      "Step: [386] d_loss: 14.96058655, g_loss: 6.98206520\n",
      "Step: [387] d_loss: 15.46212387, g_loss: 6.34625149\n",
      "Step: [388] d_loss: 15.17357254, g_loss: 7.41439772\n",
      "Step: [389] d_loss: 13.73282433, g_loss: 7.69012165\n",
      "Step: [390] d_loss: 14.39608574, g_loss: 7.73479366\n",
      "Step: [391] d_loss: 14.74958706, g_loss: 8.03489304\n",
      "Step: [392] d_loss: 14.86436081, g_loss: 7.86408520\n",
      "Step: [393] d_loss: 14.54464531, g_loss: 8.09606934\n",
      "Step: [394] d_loss: 15.01527786, g_loss: 7.55372429\n",
      "Step: [395] d_loss: 14.76279926, g_loss: 7.56122684\n",
      "Step: [396] d_loss: 14.34524155, g_loss: 7.59603786\n",
      "Step: [397] d_loss: 14.78812981, g_loss: 7.30138111\n",
      "Step: [398] d_loss: 14.54132748, g_loss: 7.75331879\n",
      "Step: [399] d_loss: 15.16842270, g_loss: 7.31962919\n",
      "Step: [400] d_loss: 15.39001274, g_loss: 7.50243473\n",
      "Step: [401] d_loss: 15.65982151, g_loss: 7.71869183\n",
      "Step: [402] d_loss: 14.70463943, g_loss: 8.33719254\n",
      "Step: [403] d_loss: 17.16382599, g_loss: 7.73211765\n",
      "Step: [404] d_loss: 16.06383133, g_loss: 7.07517052\n",
      "Step: [405] d_loss: 14.14701843, g_loss: 7.00981522\n",
      "Step: [406] d_loss: 14.49343872, g_loss: 6.65617180\n",
      "Step: [407] d_loss: 15.70371246, g_loss: 6.77612972\n",
      "Step: [408] d_loss: 14.95437622, g_loss: 6.75184488\n",
      "Step: [409] d_loss: 16.12292099, g_loss: 6.69010115\n",
      "Step: [410] d_loss: 16.90178299, g_loss: 7.46344519\n",
      "Step: [411] d_loss: 16.23346901, g_loss: 7.87879610\n",
      "Step: [412] d_loss: 14.96607399, g_loss: 8.12519646\n",
      "Step: [413] d_loss: 15.24954796, g_loss: 7.41657639\n",
      "Step: [414] d_loss: 15.19881248, g_loss: 7.11844254\n",
      "Step: [415] d_loss: 15.35526371, g_loss: 6.72154474\n",
      "Step: [416] d_loss: 16.21316338, g_loss: 6.88419437\n",
      "Step: [417] d_loss: 15.76030731, g_loss: 7.16596127\n",
      "Step: [418] d_loss: 14.98607635, g_loss: 7.11080647\n",
      "Step: [419] d_loss: 15.22012138, g_loss: 8.42246628\n",
      "Step: [420] d_loss: 16.16327667, g_loss: 8.12536144\n",
      "Step: [421] d_loss: 15.75085258, g_loss: 8.69840050\n",
      "Step: [422] d_loss: 16.81772804, g_loss: 7.75560379\n",
      "Step: [423] d_loss: 16.39779091, g_loss: 8.01389122\n",
      "Step: [424] d_loss: 14.90477753, g_loss: 7.90717125\n",
      "Step: [425] d_loss: 15.56451607, g_loss: 7.69652700\n",
      "Step: [426] d_loss: 14.64273834, g_loss: 7.41457081\n",
      "Step: [427] d_loss: 14.40535450, g_loss: 6.93170214\n",
      "Step: [428] d_loss: 15.13218784, g_loss: 7.11034966\n",
      "Step: [429] d_loss: 14.81778717, g_loss: 7.37877083\n",
      "Step: [430] d_loss: 13.75530720, g_loss: 7.40962172\n",
      "Step: [431] d_loss: 14.36047173, g_loss: 7.16476059\n",
      "Step: [432] d_loss: 16.28384018, g_loss: 7.12581587\n",
      "Step: [433] d_loss: 13.77235222, g_loss: 7.84462881\n",
      "Step: [434] d_loss: 13.82966995, g_loss: 7.27144670\n",
      "Step: [435] d_loss: 15.62091732, g_loss: 6.80285740\n",
      "Step: [436] d_loss: 14.17241478, g_loss: 7.06157589\n",
      "Step: [437] d_loss: 14.97664642, g_loss: 6.64655828\n",
      "Step: [438] d_loss: 13.79417610, g_loss: 7.23826218\n",
      "Step: [439] d_loss: 14.95199013, g_loss: 6.97262192\n",
      "Step: [440] d_loss: 15.60613251, g_loss: 7.31467438\n",
      "Step: [441] d_loss: 15.93364716, g_loss: 6.76764297\n",
      "Step: [442] d_loss: 15.89632225, g_loss: 6.37385511\n",
      "Step: [443] d_loss: 16.18452835, g_loss: 6.39973927\n",
      "Step: [444] d_loss: 16.59650421, g_loss: 7.07839584\n",
      "Step: [445] d_loss: 15.24637032, g_loss: 7.43876410\n",
      "Step: [446] d_loss: 16.12667465, g_loss: 7.43564367\n",
      "Step: [447] d_loss: 15.02328014, g_loss: 7.83536291\n",
      "Step: [448] d_loss: 17.10887146, g_loss: 6.75589228\n",
      "Step: [449] d_loss: 16.36636353, g_loss: 6.87201691\n",
      "Step: [450] d_loss: 14.30855179, g_loss: 7.32684994\n",
      "Step: [451] d_loss: 15.01014900, g_loss: 7.31495953\n",
      "Step: [452] d_loss: 15.92279053, g_loss: 6.81978416\n",
      "Step: [453] d_loss: 15.77814865, g_loss: 7.28012609\n",
      "Step: [454] d_loss: 18.23460197, g_loss: 7.88209820\n",
      "Step: [455] d_loss: 18.34658051, g_loss: 7.92925692\n",
      "Step: [456] d_loss: 16.43764114, g_loss: 7.42704105\n",
      "Step: [457] d_loss: 15.98593235, g_loss: 7.25938416\n",
      "Step: [458] d_loss: 15.55458355, g_loss: 6.91223145\n",
      "Step: [459] d_loss: 14.98448849, g_loss: 7.93953037\n",
      "Step: [460] d_loss: 15.39856625, g_loss: 7.87290525\n",
      "Step: [461] d_loss: 14.13260937, g_loss: 8.10371017\n",
      "Step: [462] d_loss: 15.11491013, g_loss: 7.43735695\n",
      "Step: [463] d_loss: 14.35086823, g_loss: 7.58074045\n",
      "Step: [464] d_loss: 15.09301662, g_loss: 7.05046701\n",
      "Step: [465] d_loss: 14.74584866, g_loss: 7.15940905\n",
      "Step: [466] d_loss: 14.60120010, g_loss: 7.55942440\n",
      "Step: [467] d_loss: 14.57390499, g_loss: 7.55943775\n",
      "Step: [468] d_loss: 15.22553635, g_loss: 7.15531445\n",
      "Step: [469] d_loss: 15.36023521, g_loss: 6.65832329\n",
      "Step: [470] d_loss: 14.77638531, g_loss: 8.09406853\n",
      "Step: [471] d_loss: 15.96601772, g_loss: 7.06810379\n",
      "Step: [472] d_loss: 15.98756695, g_loss: 7.12731647\n",
      "Step: [473] d_loss: 14.94627666, g_loss: 7.66445160\n",
      "Step: [474] d_loss: 14.80997944, g_loss: 7.88347149\n",
      "Step: [475] d_loss: 15.33151054, g_loss: 7.33808231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [476] d_loss: 15.30159378, g_loss: 6.78446674\n",
      "Step: [477] d_loss: 15.94261360, g_loss: 7.42767525\n",
      "Step: [478] d_loss: 15.56146622, g_loss: 7.56897974\n",
      "Step: [479] d_loss: 16.55422974, g_loss: 5.93714142\n",
      "Step: [480] d_loss: 15.48546219, g_loss: 6.81594610\n",
      "Step: [481] d_loss: 15.80926609, g_loss: 7.54415274\n",
      "Step: [482] d_loss: 15.55617142, g_loss: 6.85685158\n",
      "Step: [483] d_loss: 15.25650120, g_loss: 7.67067003\n",
      "Step: [484] d_loss: 15.72834969, g_loss: 7.42464018\n",
      "Step: [485] d_loss: 15.96080971, g_loss: 8.03066826\n",
      "Step: [486] d_loss: 17.03796196, g_loss: 7.73152065\n",
      "Step: [487] d_loss: 15.82363987, g_loss: 7.82967138\n",
      "Step: [488] d_loss: 16.13913345, g_loss: 7.64351797\n",
      "Step: [489] d_loss: 16.16292381, g_loss: 7.57613945\n",
      "Step: [490] d_loss: 15.55464077, g_loss: 7.76876926\n",
      "Step: [491] d_loss: 15.53186226, g_loss: 7.72811937\n",
      "Step: [492] d_loss: 15.29586792, g_loss: 7.13885880\n",
      "Step: [493] d_loss: 15.25792885, g_loss: 7.32140732\n",
      "Step: [494] d_loss: 14.68978310, g_loss: 7.15515804\n",
      "Step: [495] d_loss: 15.09809971, g_loss: 6.99181509\n",
      "Step: [496] d_loss: 16.29746246, g_loss: 6.52267456\n",
      "Step: [497] d_loss: 15.04154778, g_loss: 6.93292141\n",
      "Step: [498] d_loss: 15.73785019, g_loss: 7.24503899\n",
      "Step: [499] d_loss: 15.44551849, g_loss: 7.16920376\n",
      "Step: [500] d_loss: 15.83248520, g_loss: 6.63719130\n",
      "Step: [501] d_loss: 17.12337875, g_loss: 8.28089142\n",
      "Step: [502] d_loss: 17.04348755, g_loss: 7.92033529\n",
      "Step: [503] d_loss: 18.63294029, g_loss: 8.31912136\n",
      "Step: [504] d_loss: 16.64410400, g_loss: 7.80521536\n",
      "Step: [505] d_loss: 16.55967522, g_loss: 7.46375465\n",
      "Step: [506] d_loss: 16.17430115, g_loss: 6.65432739\n",
      "Step: [507] d_loss: 16.21246529, g_loss: 6.83319044\n",
      "Step: [508] d_loss: 15.87756348, g_loss: 6.56631851\n",
      "Step: [509] d_loss: 15.86029434, g_loss: 6.48984528\n",
      "Step: [510] d_loss: 15.42299843, g_loss: 7.31589365\n",
      "Step: [511] d_loss: 15.17427635, g_loss: 7.14347267\n",
      "Step: [512] d_loss: 14.44058037, g_loss: 6.99508858\n",
      "Step: [513] d_loss: 14.88977242, g_loss: 6.94115829\n",
      "Step: [514] d_loss: 15.38462925, g_loss: 7.29898357\n",
      "Step: [515] d_loss: 15.74056053, g_loss: 8.52470207\n",
      "Step: [516] d_loss: 16.67169189, g_loss: 7.40102148\n",
      "Step: [517] d_loss: 15.01551628, g_loss: 7.68742132\n",
      "Step: [518] d_loss: 16.50269127, g_loss: 7.17117977\n",
      "Step: [519] d_loss: 15.77722359, g_loss: 6.59493780\n",
      "Step: [520] d_loss: 15.26088524, g_loss: 7.19565105\n",
      "Step: [521] d_loss: 15.26826859, g_loss: 6.73691559\n",
      "Step: [522] d_loss: 14.45751762, g_loss: 7.29902077\n",
      "Step: [523] d_loss: 14.95714569, g_loss: 7.32835293\n",
      "Step: [524] d_loss: 15.61708832, g_loss: 7.13500738\n",
      "Step: [525] d_loss: 15.57463932, g_loss: 7.34704018\n",
      "Step: [526] d_loss: 15.61824894, g_loss: 7.29328156\n",
      "Step: [527] d_loss: 15.53092861, g_loss: 7.31001186\n",
      "Step: [528] d_loss: 15.38405704, g_loss: 7.28926086\n",
      "Step: [529] d_loss: 14.48768044, g_loss: 7.50123215\n",
      "Step: [530] d_loss: 14.78662682, g_loss: 7.16941166\n",
      "Step: [531] d_loss: 14.98054504, g_loss: 7.75081301\n",
      "Step: [532] d_loss: 16.02724075, g_loss: 7.66688395\n",
      "Step: [533] d_loss: 16.34183311, g_loss: 6.86693144\n",
      "Step: [534] d_loss: 15.48998928, g_loss: 6.34486771\n",
      "Step: [535] d_loss: 15.68573952, g_loss: 6.79109430\n",
      "Step: [536] d_loss: 16.50553322, g_loss: 7.00432873\n",
      "Step: [537] d_loss: 16.27806473, g_loss: 6.55641651\n",
      "Step: [538] d_loss: 15.42959595, g_loss: 7.18461895\n",
      "Step: [539] d_loss: 16.41870308, g_loss: 7.21978664\n",
      "Step: [540] d_loss: 15.48198605, g_loss: 7.33616447\n",
      "Step: [541] d_loss: 15.70140362, g_loss: 7.91809225\n",
      "Step: [542] d_loss: 16.28468704, g_loss: 6.85237074\n",
      "Step: [543] d_loss: 16.04528427, g_loss: 7.13132954\n",
      "Step: [544] d_loss: 16.65686226, g_loss: 8.16766453\n",
      "Step: [545] d_loss: 17.56869507, g_loss: 8.33226967\n",
      "Step: [546] d_loss: 18.58072662, g_loss: 7.75960302\n",
      "Step: [547] d_loss: 15.84908104, g_loss: 7.45779896\n",
      "Step: [548] d_loss: 16.02454948, g_loss: 7.48093987\n",
      "Step: [549] d_loss: 15.76823425, g_loss: 6.82236624\n",
      "Step: [550] d_loss: 15.57595348, g_loss: 7.20350409\n",
      "Step: [551] d_loss: 16.06734848, g_loss: 6.63036442\n",
      "Step: [552] d_loss: 16.59348297, g_loss: 7.42639065\n",
      "Step: [553] d_loss: 15.75306702, g_loss: 7.27809286\n",
      "Step: [554] d_loss: 15.74842834, g_loss: 7.88827419\n",
      "Step: [555] d_loss: 16.37931061, g_loss: 7.61933041\n",
      "Step: [556] d_loss: 17.65439606, g_loss: 7.52561808\n",
      "Step: [557] d_loss: 16.05720520, g_loss: 7.44752121\n",
      "Step: [558] d_loss: 15.89060211, g_loss: 7.77160597\n",
      "Step: [559] d_loss: 15.72924805, g_loss: 7.44621992\n",
      "Step: [560] d_loss: 15.36894226, g_loss: 7.50963306\n",
      "Step: [561] d_loss: 15.58274078, g_loss: 6.79073143\n",
      "Step: [562] d_loss: 15.41635704, g_loss: 7.15868568\n",
      "Step: [563] d_loss: 15.51585197, g_loss: 7.50913239\n",
      "Step: [564] d_loss: 15.66084099, g_loss: 7.17582130\n",
      "Step: [565] d_loss: 14.35933495, g_loss: 7.44485998\n",
      "Step: [566] d_loss: 15.17557335, g_loss: 7.15203762\n",
      "Step: [567] d_loss: 15.55488586, g_loss: 6.70129395\n",
      "Step: [568] d_loss: 15.97382927, g_loss: 6.85437298\n",
      "Step: [569] d_loss: 15.70166779, g_loss: 7.25236654\n",
      "Step: [570] d_loss: 14.88779640, g_loss: 6.82850075\n",
      "Step: [571] d_loss: 15.19035149, g_loss: 6.87125397\n",
      "Step: [572] d_loss: 15.64513302, g_loss: 6.98321295\n",
      "Step: [573] d_loss: 15.56715775, g_loss: 6.94740391\n",
      "Step: [574] d_loss: 15.32488441, g_loss: 7.29161739\n",
      "Step: [575] d_loss: 15.71952820, g_loss: 7.73512268\n",
      "Step: [576] d_loss: 16.20489883, g_loss: 7.15695477\n",
      "Step: [577] d_loss: 15.95275784, g_loss: 7.24265194\n",
      "Step: [578] d_loss: 16.76495743, g_loss: 7.34998131\n",
      "Step: [579] d_loss: 17.40238571, g_loss: 8.90103245\n",
      "Step: [580] d_loss: 19.53127098, g_loss: 7.40744877\n",
      "Step: [581] d_loss: 18.09697342, g_loss: 7.36398458\n",
      "Step: [582] d_loss: 16.21190834, g_loss: 7.89005470\n",
      "Step: [583] d_loss: 16.48964310, g_loss: 7.77652740\n",
      "Step: [584] d_loss: 16.25216866, g_loss: 7.57150078\n",
      "Step: [585] d_loss: 14.90314102, g_loss: 7.97361469\n",
      "Step: [586] d_loss: 15.60508537, g_loss: 7.00254536\n",
      "Step: [587] d_loss: 14.18832016, g_loss: 7.22099018\n",
      "Step: [588] d_loss: 15.28772831, g_loss: 7.02715063\n",
      "Step: [589] d_loss: 14.99768257, g_loss: 7.56376743\n",
      "Step: [590] d_loss: 15.34142399, g_loss: 7.02186632\n",
      "Step: [591] d_loss: 15.16208839, g_loss: 6.93244934\n",
      "Step: [592] d_loss: 14.94604301, g_loss: 7.62726164\n",
      "Step: [593] d_loss: 15.74326229, g_loss: 7.37833881\n",
      "Step: [594] d_loss: 15.17029095, g_loss: 7.15967321\n",
      "Step: [595] d_loss: 14.16346741, g_loss: 6.92884970\n",
      "Step: [596] d_loss: 13.92265511, g_loss: 7.07146788\n",
      "Step: [597] d_loss: 15.82804108, g_loss: 6.59400558\n",
      "Step: [598] d_loss: 14.26026249, g_loss: 7.08063698\n",
      "Step: [599] d_loss: 15.36990833, g_loss: 7.06124210\n",
      "Step: [600] d_loss: 13.54548264, g_loss: 7.25188255\n",
      "Step: [601] d_loss: 14.51115704, g_loss: 7.22485495\n",
      "Step: [602] d_loss: 14.92010975, g_loss: 7.28062582\n",
      "Step: [603] d_loss: 15.61025238, g_loss: 7.05732107\n",
      "Step: [604] d_loss: 14.74772263, g_loss: 7.16452026\n",
      "Step: [605] d_loss: 15.25868416, g_loss: 6.96376705\n",
      "Step: [606] d_loss: 15.07534981, g_loss: 7.28505182\n",
      "Step: [607] d_loss: 16.34218597, g_loss: 7.39654303\n",
      "Step: [608] d_loss: 16.42878723, g_loss: 7.42837143\n",
      "Step: [609] d_loss: 16.24758530, g_loss: 7.21949291\n",
      "Step: [610] d_loss: 16.10118294, g_loss: 7.61442471\n",
      "Step: [611] d_loss: 16.55228996, g_loss: 7.12777090\n",
      "Step: [612] d_loss: 15.88055325, g_loss: 7.32366657\n",
      "Step: [613] d_loss: 14.90445995, g_loss: 7.19659567\n",
      "Step: [614] d_loss: 15.84443951, g_loss: 7.19223642\n",
      "Step: [615] d_loss: 15.72350693, g_loss: 6.86519909\n",
      "Step: [616] d_loss: 15.43570137, g_loss: 6.99800062\n",
      "Step: [617] d_loss: 15.05630016, g_loss: 7.03868389\n",
      "Step: [618] d_loss: 15.64666176, g_loss: 7.15899229\n",
      "Step: [619] d_loss: 15.79817200, g_loss: 7.43889332\n",
      "Step: [620] d_loss: 15.18314171, g_loss: 7.70805836\n",
      "Step: [621] d_loss: 15.49885464, g_loss: 7.58914375\n",
      "Step: [622] d_loss: 15.75112534, g_loss: 7.21257496\n",
      "Step: [623] d_loss: 15.04406357, g_loss: 6.96635056\n",
      "Step: [624] d_loss: 16.02415085, g_loss: 8.16260910\n",
      "Step: [625] d_loss: 15.34390450, g_loss: 7.63744354\n",
      "Step: [626] d_loss: 16.02845764, g_loss: 7.01738882\n",
      "Step: [627] d_loss: 15.41374779, g_loss: 7.16842318\n",
      "Step: [628] d_loss: 14.44904232, g_loss: 7.53272104\n",
      "Step: [629] d_loss: 14.92047977, g_loss: 7.08253193\n",
      "Step: [630] d_loss: 15.40168953, g_loss: 7.13880968\n",
      "Step: [631] d_loss: 15.23424911, g_loss: 7.19716692\n",
      "Step: [632] d_loss: 15.44741154, g_loss: 6.87748241\n",
      "Step: [633] d_loss: 15.49926758, g_loss: 6.56396008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [634] d_loss: 15.50947094, g_loss: 6.99222088\n",
      "Step: [635] d_loss: 16.56053543, g_loss: 6.90817976\n",
      "Step: [636] d_loss: 15.91994381, g_loss: 7.05746222\n",
      "Step: [637] d_loss: 15.40920544, g_loss: 7.20001793\n",
      "Step: [638] d_loss: 15.69064713, g_loss: 7.39384651\n",
      "Step: [639] d_loss: 15.77560234, g_loss: 7.47710705\n",
      "Step: [640] d_loss: 16.33490753, g_loss: 7.70150137\n",
      "Step: [641] d_loss: 16.02876472, g_loss: 7.05621338\n",
      "Step: [642] d_loss: 15.37630463, g_loss: 6.91829586\n",
      "Step: [643] d_loss: 15.09439087, g_loss: 7.72519732\n",
      "Step: [644] d_loss: 15.92959976, g_loss: 7.14422321\n",
      "Step: [645] d_loss: 14.87419891, g_loss: 6.92172718\n",
      "Step: [646] d_loss: 15.60677719, g_loss: 7.15760565\n",
      "Step: [647] d_loss: 15.15883827, g_loss: 7.10189724\n",
      "Step: [648] d_loss: 14.81790924, g_loss: 7.54995251\n",
      "Step: [649] d_loss: 15.56762886, g_loss: 6.92688465\n",
      "Step: [650] d_loss: 15.08373070, g_loss: 7.28453875\n",
      "Step: [651] d_loss: 14.95661640, g_loss: 7.16095734\n",
      "Step: [652] d_loss: 15.56930256, g_loss: 6.95965004\n",
      "Step: [653] d_loss: 14.97839546, g_loss: 7.56640148\n",
      "Step: [654] d_loss: 16.39419556, g_loss: 7.60850000\n",
      "Step: [655] d_loss: 16.62696075, g_loss: 7.43450785\n",
      "Step: [656] d_loss: 16.15528870, g_loss: 7.22113180\n",
      "Step: [657] d_loss: 16.36968613, g_loss: 7.23871517\n",
      "Step: [658] d_loss: 16.01822662, g_loss: 6.78399754\n",
      "Step: [659] d_loss: 15.69264412, g_loss: 6.74116611\n",
      "Step: [660] d_loss: 15.71241379, g_loss: 7.47625351\n",
      "Step: [661] d_loss: 17.03347015, g_loss: 6.89345169\n",
      "Step: [662] d_loss: 16.35282326, g_loss: 6.83281517\n",
      "Step: [663] d_loss: 16.61063004, g_loss: 7.73601818\n",
      "Step: [664] d_loss: 16.08279037, g_loss: 8.04142570\n",
      "Step: [665] d_loss: 17.29431915, g_loss: 8.49920750\n",
      "Step: [666] d_loss: 18.38320923, g_loss: 8.80302143\n",
      "Step: [667] d_loss: 17.49603271, g_loss: 7.92649460\n",
      "Step: [668] d_loss: 16.85291100, g_loss: 8.19963932\n",
      "Step: [669] d_loss: 16.70911980, g_loss: 7.74761057\n",
      "Step: [670] d_loss: 16.48575974, g_loss: 7.53291845\n",
      "Step: [671] d_loss: 17.18378448, g_loss: 7.48366880\n",
      "Step: [672] d_loss: 15.72486496, g_loss: 7.69016171\n",
      "Step: [673] d_loss: 15.87679768, g_loss: 6.90682030\n",
      "Step: [674] d_loss: 15.80393505, g_loss: 7.06304550\n",
      "Step: [675] d_loss: 15.14026260, g_loss: 7.90329170\n",
      "Step: [676] d_loss: 16.11937332, g_loss: 6.87263012\n",
      "Step: [677] d_loss: 15.33919525, g_loss: 7.18410730\n",
      "Step: [678] d_loss: 15.55591583, g_loss: 7.22906876\n",
      "Step: [679] d_loss: 15.30030632, g_loss: 7.18052912\n",
      "Step: [680] d_loss: 17.03251839, g_loss: 6.98068237\n",
      "Step: [681] d_loss: 16.76327133, g_loss: 6.80285263\n",
      "Step: [682] d_loss: 15.52956963, g_loss: 7.83688259\n",
      "Step: [683] d_loss: 16.46934128, g_loss: 7.09086037\n",
      "Step: [684] d_loss: 15.52279091, g_loss: 7.23545504\n",
      "Step: [685] d_loss: 16.10968781, g_loss: 6.84923553\n",
      "Step: [686] d_loss: 15.21625137, g_loss: 6.99722385\n",
      "Step: [687] d_loss: 16.06345940, g_loss: 7.08725071\n",
      "Step: [688] d_loss: 15.47740078, g_loss: 7.21516228\n",
      "Step: [689] d_loss: 15.37529373, g_loss: 7.36429977\n",
      "Step: [690] d_loss: 15.29947090, g_loss: 6.92119026\n",
      "Step: [691] d_loss: 15.57648659, g_loss: 6.80466318\n",
      "Step: [692] d_loss: 15.37284184, g_loss: 7.06888485\n",
      "Step: [693] d_loss: 14.60377979, g_loss: 7.15103722\n",
      "Step: [694] d_loss: 15.23152733, g_loss: 6.84146881\n",
      "Step: [695] d_loss: 14.92633724, g_loss: 7.33716774\n",
      "Step: [696] d_loss: 15.34725761, g_loss: 7.33961296\n",
      "Step: [697] d_loss: 14.99867058, g_loss: 7.73754692\n",
      "Step: [698] d_loss: 14.98109436, g_loss: 7.61390686\n",
      "Step: [699] d_loss: 14.90319061, g_loss: 7.04693890\n",
      "Step: [700] d_loss: 15.76061916, g_loss: 6.92026949\n",
      "Step: [701] d_loss: 15.39604473, g_loss: 7.43083286\n",
      "Step: [702] d_loss: 15.31445122, g_loss: 7.49445677\n",
      "Step: [703] d_loss: 16.72454071, g_loss: 7.01025677\n",
      "Step: [704] d_loss: 15.60184002, g_loss: 7.37722921\n",
      "Step: [705] d_loss: 16.16934967, g_loss: 7.51331615\n",
      "Step: [706] d_loss: 15.54014206, g_loss: 7.68103123\n",
      "Step: [707] d_loss: 15.74443054, g_loss: 7.76012039\n",
      "Step: [708] d_loss: 15.14558697, g_loss: 7.74818850\n",
      "Step: [709] d_loss: 16.34775352, g_loss: 7.82186556\n",
      "Step: [710] d_loss: 16.18233299, g_loss: 7.09435797\n",
      "Step: [711] d_loss: 15.80415726, g_loss: 7.25753880\n",
      "Step: [712] d_loss: 16.60102844, g_loss: 7.18653393\n",
      "Step: [713] d_loss: 15.75918102, g_loss: 7.01857424\n",
      "Step: [714] d_loss: 15.71576881, g_loss: 7.58470440\n",
      "Step: [715] d_loss: 15.55268478, g_loss: 7.28347588\n",
      "Step: [716] d_loss: 16.47604752, g_loss: 7.08565331\n",
      "Step: [717] d_loss: 16.38095856, g_loss: 7.13459587\n",
      "Step: [718] d_loss: 15.92159557, g_loss: 7.53141880\n",
      "Step: [719] d_loss: 14.73912430, g_loss: 7.63889265\n",
      "Step: [720] d_loss: 15.91699028, g_loss: 8.25602150\n",
      "Step: [721] d_loss: 16.51819229, g_loss: 8.07359314\n",
      "Step: [722] d_loss: 16.43349075, g_loss: 7.42981434\n",
      "Step: [723] d_loss: 15.92825317, g_loss: 7.42647886\n",
      "Step: [724] d_loss: 15.80523586, g_loss: 7.74214315\n",
      "Step: [725] d_loss: 16.09515762, g_loss: 7.22009659\n",
      "Step: [726] d_loss: 14.97326469, g_loss: 7.06683397\n",
      "Step: [727] d_loss: 15.50143337, g_loss: 7.17261934\n",
      "Step: [728] d_loss: 15.96773720, g_loss: 6.70007801\n",
      "Step: [729] d_loss: 14.43723965, g_loss: 7.13134766\n",
      "Step: [730] d_loss: 14.89938164, g_loss: 6.83838463\n",
      "Step: [731] d_loss: 14.96390820, g_loss: 7.45945501\n",
      "Step: [732] d_loss: 14.98057938, g_loss: 6.90426540\n",
      "Step: [733] d_loss: 15.03380394, g_loss: 6.68405437\n",
      "Step: [734] d_loss: 15.65368176, g_loss: 6.56891680\n",
      "Step: [735] d_loss: 16.50041771, g_loss: 7.52041340\n",
      "Step: [736] d_loss: 17.99571991, g_loss: 9.21547508\n",
      "Step: [737] d_loss: 17.85992050, g_loss: 8.75078487\n",
      "Step: [738] d_loss: 18.83173180, g_loss: 9.69441414\n",
      "Step: [739] d_loss: 21.50041199, g_loss: 7.40348530\n",
      "Step: [740] d_loss: 16.04652405, g_loss: 7.48243237\n",
      "Step: [741] d_loss: 17.06057739, g_loss: 6.77166033\n",
      "Step: [742] d_loss: 15.58787155, g_loss: 7.27801323\n",
      "Step: [743] d_loss: 16.19637871, g_loss: 7.39472437\n",
      "Step: [744] d_loss: 15.91358757, g_loss: 7.19001961\n",
      "Step: [745] d_loss: 16.32349777, g_loss: 7.56537819\n",
      "Step: [746] d_loss: 15.55562019, g_loss: 7.74293804\n",
      "Step: [747] d_loss: 15.13942146, g_loss: 7.64970684\n",
      "Step: [748] d_loss: 16.87716103, g_loss: 7.60760355\n",
      "Step: [749] d_loss: 16.28237343, g_loss: 7.16547537\n",
      "Step: [750] d_loss: 16.12885094, g_loss: 7.64747429\n",
      "Step: [751] d_loss: 16.04438019, g_loss: 7.50271368\n",
      "Step: [752] d_loss: 15.74096870, g_loss: 7.73103905\n",
      "Step: [753] d_loss: 14.53027725, g_loss: 7.91260290\n",
      "Step: [754] d_loss: 16.09633636, g_loss: 7.00217104\n",
      "Step: [755] d_loss: 15.25902748, g_loss: 7.14845657\n",
      "Step: [756] d_loss: 15.61491203, g_loss: 7.19239616\n",
      "Step: [757] d_loss: 15.39626312, g_loss: 7.43085146\n",
      "Step: [758] d_loss: 15.73191929, g_loss: 7.17686844\n",
      "Step: [759] d_loss: 15.99712086, g_loss: 7.00237131\n",
      "Step: [760] d_loss: 15.89796352, g_loss: 7.20085526\n",
      "Step: [761] d_loss: 16.62322617, g_loss: 7.96411133\n",
      "Step: [762] d_loss: 15.76918411, g_loss: 7.58498383\n",
      "Step: [763] d_loss: 15.89140224, g_loss: 6.84641314\n",
      "Step: [764] d_loss: 16.27268219, g_loss: 7.10095406\n",
      "Step: [765] d_loss: 15.88302898, g_loss: 7.24335718\n",
      "Step: [766] d_loss: 17.20833588, g_loss: 8.46687126\n",
      "Step: [767] d_loss: 20.07737350, g_loss: 9.24372196\n",
      "Step: [768] d_loss: 19.04333878, g_loss: 7.30645847\n",
      "Step: [769] d_loss: 16.05807114, g_loss: 7.58296442\n",
      "Step: [770] d_loss: 16.29459953, g_loss: 7.38074923\n",
      "Step: [771] d_loss: 16.50198364, g_loss: 7.54566765\n",
      "Step: [772] d_loss: 15.72839355, g_loss: 7.23801136\n",
      "Step: [773] d_loss: 15.18325520, g_loss: 7.51662683\n",
      "Step: [774] d_loss: 15.42907143, g_loss: 7.23265457\n",
      "Step: [775] d_loss: 15.34497738, g_loss: 7.13236284\n",
      "Step: [776] d_loss: 14.78759003, g_loss: 7.57773113\n",
      "Step: [777] d_loss: 14.97172356, g_loss: 7.72916031\n",
      "Step: [778] d_loss: 15.39311504, g_loss: 7.46730518\n",
      "Step: [779] d_loss: 15.47143364, g_loss: 7.37807178\n",
      "Step: [780] d_loss: 15.50733757, g_loss: 7.47900963\n",
      "Step: [781] d_loss: 15.69779968, g_loss: 7.21268940\n",
      "Step: [782] d_loss: 16.06786728, g_loss: 7.11318350\n",
      "Step: [783] d_loss: 14.91721344, g_loss: 7.44758797\n",
      "Step: [784] d_loss: 15.38257217, g_loss: 7.67961836\n",
      "Step: [785] d_loss: 15.53151321, g_loss: 7.13239050\n",
      "Step: [786] d_loss: 16.31889153, g_loss: 7.48320580\n",
      "Step: [787] d_loss: 16.82496262, g_loss: 7.43416643\n",
      "Step: [788] d_loss: 15.92968178, g_loss: 7.60980225\n",
      "Step: [789] d_loss: 16.20529938, g_loss: 7.40813255\n",
      "Step: [790] d_loss: 15.14723587, g_loss: 7.79817963\n",
      "Step: [791] d_loss: 15.82251072, g_loss: 7.31858444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [792] d_loss: 15.78077698, g_loss: 7.23689365\n",
      "Step: [793] d_loss: 15.97088051, g_loss: 7.23639393\n",
      "Step: [794] d_loss: 15.00793839, g_loss: 7.22209787\n",
      "Step: [795] d_loss: 15.78606510, g_loss: 6.89696217\n",
      "Step: [796] d_loss: 15.51665688, g_loss: 6.93323183\n",
      "Step: [797] d_loss: 15.68202209, g_loss: 6.95131922\n",
      "Step: [798] d_loss: 15.37296772, g_loss: 6.83602047\n",
      "Step: [799] d_loss: 15.93227386, g_loss: 6.99152374\n",
      "Step: [800] d_loss: 16.69302750, g_loss: 7.06900120\n",
      "Step: [801] d_loss: 16.09112930, g_loss: 7.29573345\n",
      "Step: [802] d_loss: 15.25150013, g_loss: 7.96646976\n",
      "Step: [803] d_loss: 15.54489517, g_loss: 7.22732925\n",
      "Step: [804] d_loss: 15.36219406, g_loss: 7.60109425\n",
      "Step: [805] d_loss: 15.22812557, g_loss: 7.27670622\n",
      "Step: [806] d_loss: 15.83194351, g_loss: 7.55590248\n",
      "Step: [807] d_loss: 16.77607918, g_loss: 7.55351543\n",
      "Step: [808] d_loss: 15.53881359, g_loss: 7.41452694\n",
      "Step: [809] d_loss: 16.26903534, g_loss: 7.07922220\n",
      "Step: [810] d_loss: 15.23577881, g_loss: 7.60271311\n",
      "Step: [811] d_loss: 15.26668549, g_loss: 7.64861870\n",
      "Step: [812] d_loss: 16.00969505, g_loss: 7.38002205\n",
      "Step: [813] d_loss: 15.50214958, g_loss: 7.53005219\n",
      "Step: [814] d_loss: 15.57688808, g_loss: 7.42683220\n",
      "Step: [815] d_loss: 15.72539711, g_loss: 7.11290169\n",
      "Step: [816] d_loss: 15.81655312, g_loss: 7.31634617\n",
      "Step: [817] d_loss: 15.50209618, g_loss: 7.25398350\n",
      "Step: [818] d_loss: 16.85427475, g_loss: 7.49228525\n",
      "Step: [819] d_loss: 15.09906387, g_loss: 7.54142284\n",
      "Step: [820] d_loss: 15.14596176, g_loss: 7.61536837\n",
      "Step: [821] d_loss: 15.61628628, g_loss: 7.22115612\n",
      "Step: [822] d_loss: 15.31504726, g_loss: 7.24033785\n",
      "Step: [823] d_loss: 15.80757523, g_loss: 7.08456707\n",
      "Step: [824] d_loss: 16.95941162, g_loss: 7.42381382\n",
      "Step: [825] d_loss: 16.96311951, g_loss: 7.40544176\n",
      "Step: [826] d_loss: 15.72826385, g_loss: 7.33678627\n",
      "Step: [827] d_loss: 16.71908188, g_loss: 7.34154844\n",
      "Step: [828] d_loss: 16.11870384, g_loss: 7.99218988\n",
      "Step: [829] d_loss: 15.99472332, g_loss: 7.71498680\n",
      "Step: [830] d_loss: 15.97950935, g_loss: 7.36442471\n",
      "Step: [831] d_loss: 15.50021744, g_loss: 7.69442749\n",
      "Step: [832] d_loss: 15.50675201, g_loss: 7.55918121\n",
      "Step: [833] d_loss: 15.99178219, g_loss: 7.26813316\n",
      "Step: [834] d_loss: 16.60793686, g_loss: 6.89928150\n",
      "Step: [835] d_loss: 15.92730713, g_loss: 7.11271763\n",
      "Step: [836] d_loss: 15.62811756, g_loss: 7.26616192\n",
      "Step: [837] d_loss: 15.03437138, g_loss: 7.15859795\n",
      "Step: [838] d_loss: 15.65395451, g_loss: 7.18764782\n",
      "Step: [839] d_loss: 15.26212692, g_loss: 7.29802704\n",
      "Step: [840] d_loss: 15.48122597, g_loss: 7.14608765\n",
      "Step: [841] d_loss: 16.27023315, g_loss: 7.45320320\n",
      "Step: [842] d_loss: 15.27889729, g_loss: 7.08872557\n",
      "Step: [843] d_loss: 15.51020908, g_loss: 6.94137669\n",
      "Step: [844] d_loss: 15.31946182, g_loss: 7.64052963\n",
      "Step: [845] d_loss: 15.12115383, g_loss: 7.72201204\n",
      "Step: [846] d_loss: 15.11594296, g_loss: 6.98172903\n",
      "Step: [847] d_loss: 15.25698662, g_loss: 6.63505173\n",
      "Step: [848] d_loss: 15.22095490, g_loss: 6.77832079\n",
      "Step: [849] d_loss: 15.44193459, g_loss: 6.75118065\n",
      "Step: [850] d_loss: 15.02037907, g_loss: 7.31219769\n",
      "Step: [851] d_loss: 15.29449654, g_loss: 7.01510811\n",
      "Step: [852] d_loss: 15.56990623, g_loss: 7.00537491\n",
      "Step: [853] d_loss: 15.36222744, g_loss: 7.21233082\n",
      "Step: [854] d_loss: 15.59831047, g_loss: 6.86402702\n",
      "Step: [855] d_loss: 15.00235367, g_loss: 7.36905050\n",
      "Step: [856] d_loss: 16.14009666, g_loss: 6.87983513\n",
      "Step: [857] d_loss: 15.34847641, g_loss: 7.75186348\n",
      "Step: [858] d_loss: 15.39595032, g_loss: 7.56709480\n",
      "Step: [859] d_loss: 15.84755135, g_loss: 6.95396090\n",
      "Step: [860] d_loss: 15.34127426, g_loss: 7.15290308\n",
      "Step: [861] d_loss: 15.59331322, g_loss: 7.93204927\n",
      "Step: [862] d_loss: 14.91309929, g_loss: 7.69384384\n",
      "Step: [863] d_loss: 15.53050613, g_loss: 7.45480299\n",
      "Step: [864] d_loss: 15.13274765, g_loss: 7.49811602\n",
      "Step: [865] d_loss: 16.21158600, g_loss: 7.06546354\n",
      "Step: [866] d_loss: 15.68083763, g_loss: 7.38756752\n",
      "Step: [867] d_loss: 16.29055023, g_loss: 7.34175253\n",
      "Step: [868] d_loss: 15.92608547, g_loss: 7.72219276\n",
      "Step: [869] d_loss: 16.13852692, g_loss: 7.41088009\n",
      "Step: [870] d_loss: 16.25514984, g_loss: 7.61862946\n",
      "Step: [871] d_loss: 15.61965561, g_loss: 7.88139296\n",
      "Step: [872] d_loss: 15.26362324, g_loss: 7.47561121\n",
      "Step: [873] d_loss: 15.74278259, g_loss: 7.47803497\n",
      "Step: [874] d_loss: 16.14639091, g_loss: 7.42605782\n",
      "Step: [875] d_loss: 15.82969093, g_loss: 6.79866982\n",
      "Step: [876] d_loss: 15.41472530, g_loss: 7.64641380\n",
      "Step: [877] d_loss: 16.11350632, g_loss: 7.46438885\n",
      "Step: [878] d_loss: 15.06273842, g_loss: 6.96399069\n",
      "Step: [879] d_loss: 15.83147812, g_loss: 7.29271507\n",
      "Step: [880] d_loss: 15.43717957, g_loss: 7.50058174\n",
      "Step: [881] d_loss: 16.29946899, g_loss: 7.46237803\n",
      "Step: [882] d_loss: 16.47907639, g_loss: 7.36177921\n",
      "Step: [883] d_loss: 15.06941223, g_loss: 7.51309443\n",
      "Step: [884] d_loss: 15.18557644, g_loss: 7.61838722\n",
      "Step: [885] d_loss: 15.48941040, g_loss: 7.57240105\n",
      "Step: [886] d_loss: 15.43110466, g_loss: 7.86259460\n",
      "Step: [887] d_loss: 16.15238380, g_loss: 7.25927591\n",
      "Step: [888] d_loss: 15.76885033, g_loss: 7.08098125\n",
      "Step: [889] d_loss: 15.69330120, g_loss: 7.26085663\n",
      "Step: [890] d_loss: 15.74082947, g_loss: 7.37058353\n",
      "Step: [891] d_loss: 15.56362915, g_loss: 7.10043144\n",
      "Step: [892] d_loss: 16.35777664, g_loss: 7.06284237\n",
      "Step: [893] d_loss: 15.36002922, g_loss: 7.47853518\n",
      "Step: [894] d_loss: 15.21147633, g_loss: 7.72103119\n",
      "Step: [895] d_loss: 15.39933586, g_loss: 7.17597866\n",
      "Step: [896] d_loss: 14.84309006, g_loss: 7.35396671\n",
      "Step: [897] d_loss: 14.81037998, g_loss: 8.06656456\n",
      "Step: [898] d_loss: 16.17839813, g_loss: 7.21448135\n",
      "Step: [899] d_loss: 14.82487297, g_loss: 7.33526993\n",
      "Step: [900] d_loss: 15.92935562, g_loss: 7.16004992\n",
      "Step: [901] d_loss: 15.77042294, g_loss: 7.70002937\n",
      "Step: [902] d_loss: 15.88841820, g_loss: 7.39918804\n",
      "Step: [903] d_loss: 15.57536507, g_loss: 7.33413553\n",
      "Step: [904] d_loss: 15.36719894, g_loss: 7.34583712\n",
      "Step: [905] d_loss: 16.50875854, g_loss: 6.93811131\n",
      "Step: [906] d_loss: 15.29946613, g_loss: 8.02272606\n",
      "Step: [907] d_loss: 15.88090611, g_loss: 7.42734671\n",
      "Step: [908] d_loss: 15.56660175, g_loss: 7.12521410\n",
      "Step: [909] d_loss: 16.33693314, g_loss: 6.55290461\n",
      "Step: [910] d_loss: 15.08579350, g_loss: 7.93993378\n",
      "Step: [911] d_loss: 16.42206764, g_loss: 6.93441391\n",
      "Step: [912] d_loss: 15.46971035, g_loss: 7.45215702\n",
      "Step: [913] d_loss: 15.51182556, g_loss: 7.12930107\n",
      "Step: [914] d_loss: 16.14384270, g_loss: 7.17234373\n",
      "Step: [915] d_loss: 16.03466034, g_loss: 7.36253357\n",
      "Step: [916] d_loss: 15.49052429, g_loss: 7.77967167\n",
      "Step: [917] d_loss: 15.46054077, g_loss: 7.36925650\n",
      "Step: [918] d_loss: 15.86378479, g_loss: 6.89613152\n",
      "Step: [919] d_loss: 16.60964394, g_loss: 7.38915348\n",
      "Step: [920] d_loss: 15.21891975, g_loss: 7.12327671\n",
      "Step: [921] d_loss: 15.54629707, g_loss: 7.70575047\n",
      "Step: [922] d_loss: 16.20977020, g_loss: 8.09359550\n",
      "Step: [923] d_loss: 16.24638557, g_loss: 7.34853506\n",
      "Step: [924] d_loss: 15.12763309, g_loss: 7.50736809\n",
      "Step: [925] d_loss: 16.24045944, g_loss: 7.27637005\n",
      "Step: [926] d_loss: 16.49109459, g_loss: 7.25986385\n",
      "Step: [927] d_loss: 15.73618889, g_loss: 7.26548195\n",
      "Step: [928] d_loss: 15.94791794, g_loss: 7.58469105\n",
      "Step: [929] d_loss: 15.31444454, g_loss: 7.24247456\n",
      "Step: [930] d_loss: 15.55238533, g_loss: 6.59042406\n",
      "Step: [931] d_loss: 15.20059586, g_loss: 6.95879364\n",
      "Step: [932] d_loss: 15.31463146, g_loss: 7.65892220\n",
      "Step: [933] d_loss: 15.18358612, g_loss: 7.46351051\n",
      "Step: [934] d_loss: 15.64391899, g_loss: 7.35593176\n",
      "Step: [935] d_loss: 16.07547760, g_loss: 6.90316296\n",
      "Step: [936] d_loss: 15.02108955, g_loss: 7.44385481\n",
      "Step: [937] d_loss: 15.07166290, g_loss: 7.43930912\n",
      "Step: [938] d_loss: 15.23577881, g_loss: 6.94924831\n",
      "Step: [939] d_loss: 15.03630257, g_loss: 7.39070892\n",
      "Step: [940] d_loss: 15.08694553, g_loss: 8.14790726\n",
      "Step: [941] d_loss: 15.77545738, g_loss: 7.38225460\n",
      "Step: [942] d_loss: 15.18291283, g_loss: 7.06694221\n",
      "Step: [943] d_loss: 15.43290329, g_loss: 7.27303743\n",
      "Step: [944] d_loss: 14.60175514, g_loss: 7.35664654\n",
      "Step: [945] d_loss: 15.24787521, g_loss: 7.41500473\n",
      "Step: [946] d_loss: 15.47187996, g_loss: 7.36174583\n",
      "Step: [947] d_loss: 16.05962753, g_loss: 7.30529499\n",
      "Step: [948] d_loss: 16.06917953, g_loss: 7.31707001\n",
      "Step: [949] d_loss: 15.45331001, g_loss: 7.60691071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [950] d_loss: 16.00372124, g_loss: 7.48225117\n",
      "Step: [951] d_loss: 15.99645233, g_loss: 7.87454844\n",
      "Step: [952] d_loss: 16.42450333, g_loss: 7.44393635\n",
      "Step: [953] d_loss: 15.88057899, g_loss: 7.86797619\n",
      "Step: [954] d_loss: 15.61791992, g_loss: 7.88787746\n",
      "Step: [955] d_loss: 16.25084114, g_loss: 7.31485605\n",
      "Step: [956] d_loss: 16.03793526, g_loss: 7.87444401\n",
      "Step: [957] d_loss: 15.22403431, g_loss: 7.56757259\n",
      "Step: [958] d_loss: 15.77691174, g_loss: 7.79559708\n",
      "Step: [959] d_loss: 15.41481209, g_loss: 7.72757816\n",
      "Step: [960] d_loss: 15.77312851, g_loss: 7.84956837\n",
      "Step: [961] d_loss: 15.20653725, g_loss: 7.77120304\n",
      "Step: [962] d_loss: 15.16768360, g_loss: 7.33998013\n",
      "Step: [963] d_loss: 16.00369835, g_loss: 7.18484020\n",
      "Step: [964] d_loss: 15.27992821, g_loss: 7.60610580\n",
      "Step: [965] d_loss: 15.11109352, g_loss: 7.54027843\n",
      "Step: [966] d_loss: 15.61906815, g_loss: 7.40037537\n",
      "Step: [967] d_loss: 15.36152363, g_loss: 7.90552139\n",
      "Step: [968] d_loss: 15.49529648, g_loss: 7.65546846\n",
      "Step: [969] d_loss: 15.05059910, g_loss: 7.63706636\n",
      "Step: [970] d_loss: 16.16932297, g_loss: 7.54700327\n",
      "Step: [971] d_loss: 16.02433014, g_loss: 7.45005608\n",
      "Step: [972] d_loss: 15.96702194, g_loss: 7.33335018\n",
      "Step: [973] d_loss: 15.90154266, g_loss: 7.30979061\n",
      "Step: [974] d_loss: 16.09185791, g_loss: 7.96283197\n",
      "Step: [975] d_loss: 16.88704109, g_loss: 7.03197670\n",
      "Step: [976] d_loss: 16.23047638, g_loss: 7.31334972\n",
      "Step: [977] d_loss: 15.24094009, g_loss: 7.33887434\n",
      "Step: [978] d_loss: 15.13107300, g_loss: 7.77988148\n",
      "Step: [979] d_loss: 15.51051140, g_loss: 7.02807999\n",
      "Step: [980] d_loss: 15.07044220, g_loss: 7.49240780\n",
      "Step: [981] d_loss: 15.82065296, g_loss: 7.74598980\n",
      "Step: [982] d_loss: 15.82198143, g_loss: 7.57103252\n",
      "Step: [983] d_loss: 16.37621498, g_loss: 7.51301193\n",
      "Step: [984] d_loss: 16.28315735, g_loss: 7.47159100\n",
      "Step: [985] d_loss: 16.38753128, g_loss: 7.30941057\n",
      "Step: [986] d_loss: 15.29424667, g_loss: 7.35664463\n",
      "Step: [987] d_loss: 15.32365036, g_loss: 7.40354824\n",
      "Step: [988] d_loss: 16.71868134, g_loss: 7.37369728\n",
      "Step: [989] d_loss: 16.70482254, g_loss: 7.06880951\n",
      "Step: [990] d_loss: 15.88417721, g_loss: 7.25575161\n",
      "Step: [991] d_loss: 15.81262016, g_loss: 7.37351990\n",
      "Step: [992] d_loss: 15.77256203, g_loss: 7.74685907\n",
      "Step: [993] d_loss: 15.47057533, g_loss: 7.25221872\n",
      "Step: [994] d_loss: 15.83413124, g_loss: 7.36257792\n",
      "Step: [995] d_loss: 15.92086983, g_loss: 7.10296917\n",
      "Step: [996] d_loss: 15.65813828, g_loss: 7.07337570\n",
      "Step: [997] d_loss: 15.38263130, g_loss: 7.26528025\n",
      "Step: [998] d_loss: 15.43762207, g_loss: 7.51757145\n",
      "Step: [999] d_loss: 16.07580566, g_loss: 7.50808525\n",
      "Step: [1000] d_loss: 15.86336231, g_loss: 7.01800632\n",
      "Step: [1001] d_loss: 15.02828789, g_loss: 7.84822655\n",
      "Step: [1002] d_loss: 15.30782509, g_loss: 7.32676888\n",
      "Step: [1003] d_loss: 15.26214600, g_loss: 7.04538631\n",
      "Step: [1004] d_loss: 15.18131924, g_loss: 7.62539530\n",
      "Step: [1005] d_loss: 15.35543633, g_loss: 7.88999987\n",
      "Step: [1006] d_loss: 16.27017403, g_loss: 7.62330723\n",
      "Step: [1007] d_loss: 14.63608837, g_loss: 7.49171114\n",
      "Step: [1008] d_loss: 16.52457428, g_loss: 7.07927799\n",
      "Step: [1009] d_loss: 15.95374966, g_loss: 7.30606651\n",
      "Step: [1010] d_loss: 15.52509785, g_loss: 7.08245182\n",
      "Step: [1011] d_loss: 15.45966530, g_loss: 7.26103973\n",
      "Step: [1012] d_loss: 15.73327827, g_loss: 7.57515144\n",
      "Step: [1013] d_loss: 16.30014610, g_loss: 7.94205427\n",
      "Step: [1014] d_loss: 17.03433990, g_loss: 7.80049610\n",
      "Step: [1015] d_loss: 16.66433525, g_loss: 8.33494473\n",
      "Step: [1016] d_loss: 16.28581238, g_loss: 7.58710098\n",
      "Step: [1017] d_loss: 16.16830444, g_loss: 7.29852676\n",
      "Step: [1018] d_loss: 15.87458611, g_loss: 7.14065266\n",
      "Step: [1019] d_loss: 15.30379677, g_loss: 7.66397047\n",
      "Step: [1020] d_loss: 15.28017044, g_loss: 7.42550993\n",
      "Step: [1021] d_loss: 15.97700500, g_loss: 7.23429728\n",
      "Step: [1022] d_loss: 15.24508286, g_loss: 7.70877361\n",
      "Step: [1023] d_loss: 16.02630997, g_loss: 7.55963898\n",
      "Step: [1024] d_loss: 16.53348351, g_loss: 7.73132038\n",
      "Step: [1025] d_loss: 15.95700455, g_loss: 7.45880604\n",
      "Step: [1026] d_loss: 15.37910652, g_loss: 7.88674402\n",
      "Step: [1027] d_loss: 16.98627472, g_loss: 7.60961437\n",
      "Step: [1028] d_loss: 15.19465446, g_loss: 8.22495747\n",
      "Step: [1029] d_loss: 15.62388039, g_loss: 7.49588299\n",
      "Step: [1030] d_loss: 16.36706543, g_loss: 7.29514503\n",
      "Step: [1031] d_loss: 15.24789810, g_loss: 8.04637909\n",
      "Step: [1032] d_loss: 15.16188717, g_loss: 7.43564320\n",
      "Step: [1033] d_loss: 15.38786125, g_loss: 7.50237513\n",
      "Step: [1034] d_loss: 15.33245659, g_loss: 7.36972094\n",
      "Step: [1035] d_loss: 14.90740395, g_loss: 7.45236111\n",
      "Step: [1036] d_loss: 14.87167835, g_loss: 7.56881571\n",
      "Step: [1037] d_loss: 15.58101845, g_loss: 7.46115065\n",
      "Step: [1038] d_loss: 15.06362820, g_loss: 7.52854013\n",
      "Step: [1039] d_loss: 15.70767879, g_loss: 7.71329117\n",
      "Step: [1040] d_loss: 15.01409149, g_loss: 7.16209602\n",
      "Step: [1041] d_loss: 15.37923717, g_loss: 7.48929644\n",
      "Step: [1042] d_loss: 15.86569786, g_loss: 7.62582111\n",
      "Step: [1043] d_loss: 15.59013557, g_loss: 7.55170345\n",
      "Step: [1044] d_loss: 15.40067768, g_loss: 7.57426643\n",
      "Step: [1045] d_loss: 15.44930077, g_loss: 6.99055481\n",
      "Step: [1046] d_loss: 15.03163719, g_loss: 7.36724091\n",
      "Step: [1047] d_loss: 15.52817154, g_loss: 7.24872875\n",
      "Step: [1048] d_loss: 15.86189651, g_loss: 7.03313780\n",
      "Step: [1049] d_loss: 15.35474205, g_loss: 6.90584946\n",
      "Step: [1050] d_loss: 15.29360771, g_loss: 7.91446638\n",
      "Step: [1051] d_loss: 15.07334709, g_loss: 7.31807232\n",
      "Step: [1052] d_loss: 14.89703655, g_loss: 6.92587090\n",
      "Step: [1053] d_loss: 15.18624592, g_loss: 7.36038923\n",
      "Step: [1054] d_loss: 14.89106083, g_loss: 7.58016443\n",
      "Step: [1055] d_loss: 15.25899124, g_loss: 7.78281260\n",
      "Step: [1056] d_loss: 16.04251671, g_loss: 7.23885536\n",
      "Step: [1057] d_loss: 14.49745369, g_loss: 7.02632713\n",
      "Step: [1058] d_loss: 15.07388973, g_loss: 7.27594662\n",
      "Step: [1059] d_loss: 15.71821785, g_loss: 7.36434460\n",
      "Step: [1060] d_loss: 15.08315372, g_loss: 7.43944979\n",
      "Step: [1061] d_loss: 15.30013561, g_loss: 7.42041492\n",
      "Step: [1062] d_loss: 16.07758331, g_loss: 6.84873009\n",
      "Step: [1063] d_loss: 15.93863010, g_loss: 6.74022722\n",
      "Step: [1064] d_loss: 15.99650574, g_loss: 6.93234873\n",
      "Step: [1065] d_loss: 15.86917305, g_loss: 7.33937359\n",
      "Step: [1066] d_loss: 15.92670441, g_loss: 7.49847221\n",
      "Step: [1067] d_loss: 16.02206039, g_loss: 7.75459957\n",
      "Step: [1068] d_loss: 16.25872421, g_loss: 8.13813019\n",
      "Step: [1069] d_loss: 16.73433495, g_loss: 8.70578384\n",
      "Step: [1070] d_loss: 17.30832100, g_loss: 7.66882420\n",
      "Step: [1071] d_loss: 16.89565468, g_loss: 7.53851318\n",
      "Step: [1072] d_loss: 16.29162216, g_loss: 8.16733932\n",
      "Step: [1073] d_loss: 15.71252632, g_loss: 7.68097353\n",
      "Step: [1074] d_loss: 15.66653538, g_loss: 7.55173779\n",
      "Step: [1075] d_loss: 15.77955723, g_loss: 7.86987829\n",
      "Step: [1076] d_loss: 15.91720486, g_loss: 7.64225674\n",
      "Step: [1077] d_loss: 16.00884056, g_loss: 7.27536201\n",
      "Step: [1078] d_loss: 15.67840862, g_loss: 7.97893381\n",
      "Step: [1079] d_loss: 15.71384239, g_loss: 7.26903629\n",
      "Step: [1080] d_loss: 15.44378471, g_loss: 7.51768160\n",
      "Step: [1081] d_loss: 15.09794807, g_loss: 7.86564255\n",
      "Step: [1082] d_loss: 15.58161640, g_loss: 7.49097347\n",
      "Step: [1083] d_loss: 16.05034637, g_loss: 7.04325247\n",
      "Step: [1084] d_loss: 16.20359993, g_loss: 7.22827864\n",
      "Step: [1085] d_loss: 15.11485672, g_loss: 7.88322639\n",
      "Step: [1086] d_loss: 16.26597214, g_loss: 6.99242973\n",
      "Step: [1087] d_loss: 15.34495544, g_loss: 7.32513142\n",
      "Step: [1088] d_loss: 15.21412277, g_loss: 7.30037594\n",
      "Step: [1089] d_loss: 14.84662819, g_loss: 7.52635670\n",
      "Step: [1090] d_loss: 15.41630459, g_loss: 7.59863567\n",
      "Step: [1091] d_loss: 15.42348480, g_loss: 7.30723476\n",
      "Step: [1092] d_loss: 15.60750198, g_loss: 7.62954473\n",
      "Step: [1093] d_loss: 15.49701214, g_loss: 7.70610428\n",
      "Step: [1094] d_loss: 15.79807568, g_loss: 7.30498028\n",
      "Step: [1095] d_loss: 15.19851685, g_loss: 7.35449982\n",
      "Step: [1096] d_loss: 15.05041313, g_loss: 7.78498602\n",
      "Step: [1097] d_loss: 15.89937973, g_loss: 6.86245108\n",
      "Step: [1098] d_loss: 15.73035336, g_loss: 7.18093109\n",
      "Step: [1099] d_loss: 15.07078362, g_loss: 7.68311501\n",
      "Step: [1100] d_loss: 15.40145493, g_loss: 7.27946472\n",
      "Step: [1101] d_loss: 15.08261108, g_loss: 7.49377060\n",
      "Step: [1102] d_loss: 15.28190041, g_loss: 7.26082039\n",
      "Step: [1103] d_loss: 15.30666924, g_loss: 7.40236044\n",
      "Step: [1104] d_loss: 14.40934372, g_loss: 7.62434530\n",
      "Step: [1105] d_loss: 14.91818047, g_loss: 7.29967785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1106] d_loss: 15.65935230, g_loss: 6.93601036\n",
      "Step: [1107] d_loss: 16.03845978, g_loss: 7.51922178\n",
      "Step: [1108] d_loss: 15.45413780, g_loss: 7.57039261\n",
      "Step: [1109] d_loss: 15.39161777, g_loss: 7.75113964\n",
      "Step: [1110] d_loss: 15.90186024, g_loss: 7.37975931\n",
      "Step: [1111] d_loss: 15.88903046, g_loss: 7.42043400\n",
      "Step: [1112] d_loss: 16.41874695, g_loss: 7.65634394\n",
      "Step: [1113] d_loss: 16.98206711, g_loss: 7.07019949\n",
      "Step: [1114] d_loss: 16.34666443, g_loss: 7.06306839\n",
      "Step: [1115] d_loss: 15.98600674, g_loss: 7.18519735\n",
      "Step: [1116] d_loss: 15.25446510, g_loss: 7.42764473\n",
      "Step: [1117] d_loss: 15.38606262, g_loss: 7.24483347\n",
      "Step: [1118] d_loss: 15.48093510, g_loss: 7.54155922\n",
      "Step: [1119] d_loss: 16.20849419, g_loss: 7.18385077\n",
      "Step: [1120] d_loss: 14.22247982, g_loss: 7.64400673\n",
      "Step: [1121] d_loss: 15.01122093, g_loss: 7.74887085\n",
      "Step: [1122] d_loss: 14.67144108, g_loss: 7.66006899\n",
      "Step: [1123] d_loss: 15.77906990, g_loss: 8.05000973\n",
      "Step: [1124] d_loss: 16.18820381, g_loss: 8.34411240\n",
      "Step: [1125] d_loss: 17.07096481, g_loss: 7.70309544\n",
      "Step: [1126] d_loss: 15.58929729, g_loss: 7.81119156\n",
      "Step: [1127] d_loss: 16.43344498, g_loss: 7.19067001\n",
      "Step: [1128] d_loss: 16.08421516, g_loss: 7.17570925\n",
      "Step: [1129] d_loss: 15.64585876, g_loss: 7.44869852\n",
      "Step: [1130] d_loss: 15.49146461, g_loss: 7.44604254\n",
      "Step: [1131] d_loss: 14.77483463, g_loss: 7.71451092\n",
      "Step: [1132] d_loss: 15.53750992, g_loss: 7.44555664\n",
      "Step: [1133] d_loss: 15.69326878, g_loss: 7.84020424\n",
      "Step: [1134] d_loss: 15.10736656, g_loss: 7.69190264\n",
      "Step: [1135] d_loss: 15.32997417, g_loss: 7.62157059\n",
      "Step: [1136] d_loss: 15.28298664, g_loss: 7.34139442\n",
      "Step: [1137] d_loss: 15.39573860, g_loss: 7.26439762\n",
      "Step: [1138] d_loss: 14.74921036, g_loss: 7.80298471\n",
      "Step: [1139] d_loss: 15.06658936, g_loss: 7.11334419\n",
      "Step: [1140] d_loss: 16.39989471, g_loss: 7.26660156\n",
      "Step: [1141] d_loss: 15.97268772, g_loss: 7.49184895\n",
      "Step: [1142] d_loss: 16.01371956, g_loss: 7.17850018\n",
      "Step: [1143] d_loss: 15.87974262, g_loss: 6.91492748\n",
      "Step: [1144] d_loss: 16.13347244, g_loss: 6.98757172\n",
      "Step: [1145] d_loss: 15.43957138, g_loss: 7.32746172\n",
      "Step: [1146] d_loss: 15.91145515, g_loss: 7.74896336\n",
      "Step: [1147] d_loss: 16.90995598, g_loss: 7.85200691\n",
      "Step: [1148] d_loss: 16.48826027, g_loss: 6.82090664\n",
      "Step: [1149] d_loss: 15.41565704, g_loss: 7.55932999\n",
      "Step: [1150] d_loss: 15.89841270, g_loss: 7.21447372\n",
      "Step: [1151] d_loss: 15.61017418, g_loss: 7.58344650\n",
      "Step: [1152] d_loss: 15.77655220, g_loss: 7.24547052\n",
      "Step: [1153] d_loss: 15.86852360, g_loss: 7.43242264\n",
      "Step: [1154] d_loss: 14.74270535, g_loss: 8.08267593\n",
      "Step: [1155] d_loss: 15.37748718, g_loss: 7.99627542\n",
      "Step: [1156] d_loss: 15.90459061, g_loss: 7.65288067\n",
      "Step: [1157] d_loss: 14.84894371, g_loss: 7.45665646\n",
      "Step: [1158] d_loss: 14.50529289, g_loss: 7.81564140\n",
      "Step: [1159] d_loss: 14.77577782, g_loss: 7.58533478\n",
      "Step: [1160] d_loss: 15.60521126, g_loss: 7.72445869\n",
      "Step: [1161] d_loss: 15.20819664, g_loss: 7.53284073\n",
      "Step: [1162] d_loss: 15.53323746, g_loss: 7.30910969\n",
      "Step: [1163] d_loss: 16.44489479, g_loss: 7.69747686\n",
      "Step: [1164] d_loss: 15.71985626, g_loss: 7.84325647\n",
      "Step: [1165] d_loss: 14.83791924, g_loss: 7.09037399\n",
      "Step: [1166] d_loss: 15.44079590, g_loss: 7.75783443\n",
      "Step: [1167] d_loss: 16.05536652, g_loss: 7.45625544\n",
      "Step: [1168] d_loss: 16.80979729, g_loss: 7.41954231\n",
      "Step: [1169] d_loss: 15.84937286, g_loss: 7.17908239\n",
      "Step: [1170] d_loss: 15.60980225, g_loss: 7.10390711\n",
      "Step: [1171] d_loss: 15.24877834, g_loss: 7.81105614\n",
      "Step: [1172] d_loss: 16.00282288, g_loss: 7.29381275\n",
      "Step: [1173] d_loss: 16.01725006, g_loss: 7.82339907\n",
      "Step: [1174] d_loss: 15.61481476, g_loss: 7.62084389\n",
      "Step: [1175] d_loss: 15.32500458, g_loss: 7.72584915\n",
      "Step: [1176] d_loss: 16.23871231, g_loss: 7.41649055\n",
      "Step: [1177] d_loss: 15.69579887, g_loss: 7.59845591\n",
      "Step: [1178] d_loss: 15.18336010, g_loss: 7.72380543\n",
      "Step: [1179] d_loss: 14.97533512, g_loss: 7.84704590\n",
      "Step: [1180] d_loss: 15.33703423, g_loss: 7.51403809\n",
      "Step: [1181] d_loss: 15.27183914, g_loss: 7.91889668\n",
      "Step: [1182] d_loss: 14.71419525, g_loss: 7.54077625\n",
      "Step: [1183] d_loss: 15.52578735, g_loss: 7.44315577\n",
      "Step: [1184] d_loss: 15.28310585, g_loss: 7.42454529\n",
      "Step: [1185] d_loss: 14.74366570, g_loss: 7.64846325\n",
      "Step: [1186] d_loss: 15.23676109, g_loss: 7.84478092\n",
      "Step: [1187] d_loss: 14.92203140, g_loss: 7.63793755\n",
      "Step: [1188] d_loss: 15.60677910, g_loss: 7.00816345\n",
      "Step: [1189] d_loss: 15.43265057, g_loss: 7.23695850\n",
      "Step: [1190] d_loss: 14.31592560, g_loss: 7.48709774\n",
      "Step: [1191] d_loss: 14.98629570, g_loss: 7.41330051\n",
      "Step: [1192] d_loss: 16.79771423, g_loss: 8.02803898\n",
      "Step: [1193] d_loss: 17.45217514, g_loss: 7.82089710\n",
      "Step: [1194] d_loss: 15.33790779, g_loss: 7.46298504\n",
      "Step: [1195] d_loss: 14.41788483, g_loss: 7.62855911\n",
      "Step: [1196] d_loss: 15.64348507, g_loss: 7.21640587\n",
      "Step: [1197] d_loss: 15.95328903, g_loss: 7.20074940\n",
      "Step: [1198] d_loss: 16.86573410, g_loss: 7.30106258\n",
      "Step: [1199] d_loss: 15.80400848, g_loss: 7.33916950\n",
      "Step: [1200] d_loss: 15.23789883, g_loss: 7.57588673\n",
      "Step: [1201] d_loss: 16.00821304, g_loss: 7.24350500\n",
      "Step: [1202] d_loss: 14.85058212, g_loss: 7.39716625\n",
      "Step: [1203] d_loss: 15.34369946, g_loss: 7.51390171\n",
      "Step: [1204] d_loss: 15.63278770, g_loss: 6.92037582\n",
      "Step: [1205] d_loss: 15.24079132, g_loss: 7.04725122\n",
      "Step: [1206] d_loss: 15.96219063, g_loss: 7.33132887\n",
      "Step: [1207] d_loss: 15.25908566, g_loss: 8.48608685\n",
      "Step: [1208] d_loss: 15.99252605, g_loss: 8.06964874\n",
      "Step: [1209] d_loss: 15.79824924, g_loss: 8.10229015\n",
      "Step: [1210] d_loss: 16.40414047, g_loss: 7.12866306\n",
      "Step: [1211] d_loss: 15.93497086, g_loss: 7.67384386\n",
      "Step: [1212] d_loss: 15.73629284, g_loss: 7.39337206\n",
      "Step: [1213] d_loss: 15.68155193, g_loss: 7.53901100\n",
      "Step: [1214] d_loss: 16.05737686, g_loss: 7.38392067\n",
      "Step: [1215] d_loss: 15.41726208, g_loss: 8.03206539\n",
      "Step: [1216] d_loss: 16.42627716, g_loss: 8.29066467\n",
      "Step: [1217] d_loss: 16.50730133, g_loss: 7.21687603\n",
      "Step: [1218] d_loss: 16.27790260, g_loss: 7.57975912\n",
      "Step: [1219] d_loss: 16.39220810, g_loss: 7.56512451\n",
      "Step: [1220] d_loss: 15.78096008, g_loss: 7.85240030\n",
      "Step: [1221] d_loss: 15.84507179, g_loss: 7.84247684\n",
      "Step: [1222] d_loss: 16.31983948, g_loss: 8.33944607\n",
      "Step: [1223] d_loss: 16.33533096, g_loss: 7.52520943\n",
      "Step: [1224] d_loss: 16.86413193, g_loss: 7.45474386\n",
      "Step: [1225] d_loss: 15.79608917, g_loss: 7.50919008\n",
      "Step: [1226] d_loss: 14.99399757, g_loss: 7.80507326\n",
      "Step: [1227] d_loss: 14.97936821, g_loss: 7.61884260\n",
      "Step: [1228] d_loss: 14.43407059, g_loss: 7.38647747\n",
      "Step: [1229] d_loss: 15.57053471, g_loss: 7.25412512\n",
      "Step: [1230] d_loss: 14.51321602, g_loss: 8.04518223\n",
      "Step: [1231] d_loss: 14.85685444, g_loss: 7.90880013\n",
      "Step: [1232] d_loss: 15.06210327, g_loss: 7.62331009\n",
      "Step: [1233] d_loss: 15.34084225, g_loss: 7.58848715\n",
      "Step: [1234] d_loss: 15.67339706, g_loss: 7.08979607\n",
      "Step: [1235] d_loss: 14.91103077, g_loss: 7.39589548\n",
      "Step: [1236] d_loss: 16.01056480, g_loss: 7.04434299\n",
      "Step: [1237] d_loss: 15.46275520, g_loss: 7.55310106\n",
      "Step: [1238] d_loss: 15.20503330, g_loss: 7.43726349\n",
      "Step: [1239] d_loss: 15.42228413, g_loss: 7.43205929\n",
      "Step: [1240] d_loss: 15.33826637, g_loss: 7.70144558\n",
      "Step: [1241] d_loss: 15.12157059, g_loss: 7.65110588\n",
      "Step: [1242] d_loss: 15.88723469, g_loss: 7.15610695\n",
      "Step: [1243] d_loss: 15.52669716, g_loss: 7.23200083\n",
      "Step: [1244] d_loss: 15.71557426, g_loss: 7.37702560\n",
      "Step: [1245] d_loss: 15.71605873, g_loss: 7.43134499\n",
      "Step: [1246] d_loss: 15.75943947, g_loss: 7.07994223\n",
      "Step: [1247] d_loss: 15.74534702, g_loss: 7.34958172\n",
      "Step: [1248] d_loss: 15.92498398, g_loss: 7.33544254\n",
      "Step: [1249] d_loss: 15.97611618, g_loss: 7.50138569\n",
      "Step: [1250] d_loss: 16.34163666, g_loss: 7.47630835\n",
      "Step: [1251] d_loss: 16.38188934, g_loss: 7.50281906\n",
      "Step: [1252] d_loss: 16.20171356, g_loss: 7.62004280\n",
      "Step: [1253] d_loss: 16.19013023, g_loss: 7.74839973\n",
      "Step: [1254] d_loss: 16.19531059, g_loss: 7.57417488\n",
      "Step: [1255] d_loss: 15.76694489, g_loss: 7.23554802\n",
      "Step: [1256] d_loss: 15.60258102, g_loss: 7.26691008\n",
      "Step: [1257] d_loss: 16.53403091, g_loss: 7.25222397\n",
      "Step: [1258] d_loss: 16.02187347, g_loss: 7.75080872\n",
      "Step: [1259] d_loss: 15.69316483, g_loss: 7.38417768\n",
      "Step: [1260] d_loss: 15.34014034, g_loss: 7.86352205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1261] d_loss: 14.71431923, g_loss: 8.10337257\n",
      "Step: [1262] d_loss: 15.06079483, g_loss: 7.53009033\n",
      "Step: [1263] d_loss: 15.47084999, g_loss: 7.95322227\n",
      "Step: [1264] d_loss: 15.63245964, g_loss: 7.47584629\n",
      "Step: [1265] d_loss: 15.16753483, g_loss: 7.72594070\n",
      "Step: [1266] d_loss: 14.39831448, g_loss: 7.99120998\n",
      "Step: [1267] d_loss: 15.10367012, g_loss: 7.62552214\n",
      "Step: [1268] d_loss: 14.79355431, g_loss: 8.24504757\n",
      "Step: [1269] d_loss: 16.78142738, g_loss: 8.08819199\n",
      "Step: [1270] d_loss: 16.78011322, g_loss: 8.16119671\n",
      "Step: [1271] d_loss: 15.68767548, g_loss: 7.27439022\n",
      "Step: [1272] d_loss: 14.35002041, g_loss: 7.53934765\n",
      "Step: [1273] d_loss: 15.50482941, g_loss: 7.04988098\n",
      "Step: [1274] d_loss: 15.37129021, g_loss: 7.15182495\n",
      "Step: [1275] d_loss: 15.81268120, g_loss: 6.96632862\n",
      "Step: [1276] d_loss: 15.06102657, g_loss: 7.50196171\n",
      "Step: [1277] d_loss: 15.20415115, g_loss: 7.20517397\n",
      "Step: [1278] d_loss: 14.63054371, g_loss: 7.83175993\n",
      "Step: [1279] d_loss: 15.72976208, g_loss: 7.46279049\n",
      "Step: [1280] d_loss: 16.64740181, g_loss: 7.10140324\n",
      "Step: [1281] d_loss: 16.67317772, g_loss: 7.14526653\n",
      "Step: [1282] d_loss: 15.21842098, g_loss: 7.53702831\n",
      "Step: [1283] d_loss: 15.34021568, g_loss: 7.68961477\n",
      "Step: [1284] d_loss: 15.81885338, g_loss: 7.21086121\n",
      "Step: [1285] d_loss: 15.70149040, g_loss: 7.31760120\n",
      "Step: [1286] d_loss: 15.25714684, g_loss: 7.42007494\n",
      "Step: [1287] d_loss: 14.58791161, g_loss: 8.00124454\n",
      "Step: [1288] d_loss: 15.81108189, g_loss: 6.99197292\n",
      "Step: [1289] d_loss: 15.62471008, g_loss: 7.28024864\n",
      "Step: [1290] d_loss: 15.77274895, g_loss: 7.39108372\n",
      "Step: [1291] d_loss: 15.63906097, g_loss: 7.19000053\n",
      "Step: [1292] d_loss: 14.48912811, g_loss: 7.37188816\n",
      "Step: [1293] d_loss: 16.07796097, g_loss: 7.47190762\n",
      "Step: [1294] d_loss: 15.86004257, g_loss: 8.02246857\n",
      "Step: [1295] d_loss: 15.09355068, g_loss: 8.28700256\n",
      "Step: [1296] d_loss: 14.88356972, g_loss: 7.28520107\n",
      "Step: [1297] d_loss: 15.05219650, g_loss: 8.02084160\n",
      "Step: [1298] d_loss: 17.21144485, g_loss: 8.84116268\n",
      "Step: [1299] d_loss: 17.71607971, g_loss: 8.05965614\n",
      "Step: [1300] d_loss: 16.96394348, g_loss: 7.65823555\n",
      "Step: [1301] d_loss: 16.60265923, g_loss: 7.63771915\n",
      "Step: [1302] d_loss: 15.66471386, g_loss: 7.66600561\n",
      "Step: [1303] d_loss: 16.76532555, g_loss: 7.31830120\n",
      "Step: [1304] d_loss: 16.04917526, g_loss: 7.46253395\n",
      "Step: [1305] d_loss: 16.23356438, g_loss: 7.47238779\n",
      "Step: [1306] d_loss: 15.28602409, g_loss: 7.77715111\n",
      "Step: [1307] d_loss: 15.27804756, g_loss: 7.45955276\n",
      "Step: [1308] d_loss: 16.19878006, g_loss: 7.03144264\n",
      "Step: [1309] d_loss: 15.28243065, g_loss: 7.35159206\n",
      "Step: [1310] d_loss: 15.15084076, g_loss: 7.72185898\n",
      "Step: [1311] d_loss: 15.35239983, g_loss: 7.77388477\n",
      "Step: [1312] d_loss: 16.00182152, g_loss: 7.93329000\n",
      "Step: [1313] d_loss: 16.02510643, g_loss: 7.82037687\n",
      "Step: [1314] d_loss: 16.57381821, g_loss: 7.30039597\n",
      "Step: [1315] d_loss: 15.50499535, g_loss: 7.34755802\n",
      "Step: [1316] d_loss: 15.68305302, g_loss: 7.97673035\n",
      "Step: [1317] d_loss: 15.58361340, g_loss: 7.72062969\n",
      "Step: [1318] d_loss: 16.35585403, g_loss: 8.00382233\n",
      "Step: [1319] d_loss: 15.82924461, g_loss: 8.44902420\n",
      "Step: [1320] d_loss: 16.07548523, g_loss: 8.03219986\n",
      "Step: [1321] d_loss: 15.06177711, g_loss: 8.13261127\n",
      "Step: [1322] d_loss: 15.73528767, g_loss: 7.58516026\n",
      "Step: [1323] d_loss: 14.70013618, g_loss: 7.97601461\n",
      "Step: [1324] d_loss: 14.68606377, g_loss: 8.07538795\n",
      "Step: [1325] d_loss: 15.73275852, g_loss: 7.54069376\n",
      "Step: [1326] d_loss: 15.48675537, g_loss: 7.62248325\n",
      "Step: [1327] d_loss: 15.33759022, g_loss: 7.77548885\n",
      "Step: [1328] d_loss: 15.80214310, g_loss: 7.92506886\n",
      "Step: [1329] d_loss: 15.55793571, g_loss: 7.55505085\n",
      "Step: [1330] d_loss: 15.57913208, g_loss: 7.69550657\n",
      "Step: [1331] d_loss: 15.77075577, g_loss: 7.30690384\n",
      "Step: [1332] d_loss: 15.05709267, g_loss: 7.63642550\n",
      "Step: [1333] d_loss: 16.17826462, g_loss: 7.37986183\n",
      "Step: [1334] d_loss: 15.81800079, g_loss: 7.70993233\n",
      "Step: [1335] d_loss: 15.01891518, g_loss: 7.68031454\n",
      "Step: [1336] d_loss: 15.10910606, g_loss: 8.06829453\n",
      "Step: [1337] d_loss: 15.30890179, g_loss: 7.43824100\n",
      "Step: [1338] d_loss: 15.59768391, g_loss: 8.03961372\n",
      "Step: [1339] d_loss: 15.78969479, g_loss: 7.98911572\n",
      "Step: [1340] d_loss: 15.35159874, g_loss: 7.67412663\n",
      "Step: [1341] d_loss: 16.03900146, g_loss: 7.55956459\n",
      "Step: [1342] d_loss: 15.37876129, g_loss: 7.44740677\n",
      "Step: [1343] d_loss: 15.67385769, g_loss: 7.52142620\n",
      "Step: [1344] d_loss: 15.91055870, g_loss: 7.30215549\n",
      "Step: [1345] d_loss: 15.54981518, g_loss: 7.42529011\n",
      "Step: [1346] d_loss: 15.60086632, g_loss: 7.28310394\n",
      "Step: [1347] d_loss: 15.96547890, g_loss: 7.45145082\n",
      "Step: [1348] d_loss: 16.74976158, g_loss: 8.20379066\n",
      "Step: [1349] d_loss: 16.82607651, g_loss: 7.25113487\n",
      "Step: [1350] d_loss: 16.34009171, g_loss: 7.59192085\n",
      "Step: [1351] d_loss: 15.79470634, g_loss: 7.81003904\n",
      "Step: [1352] d_loss: 16.26783180, g_loss: 7.16298962\n",
      "Step: [1353] d_loss: 15.29484940, g_loss: 7.71447086\n",
      "Step: [1354] d_loss: 14.75518227, g_loss: 7.52408791\n",
      "Step: [1355] d_loss: 15.38527393, g_loss: 7.36498260\n",
      "Step: [1356] d_loss: 14.74108696, g_loss: 7.55556297\n",
      "Step: [1357] d_loss: 15.53644180, g_loss: 7.54081345\n",
      "Step: [1358] d_loss: 15.42398834, g_loss: 7.89339352\n",
      "Step: [1359] d_loss: 15.43196678, g_loss: 7.71461058\n",
      "Step: [1360] d_loss: 14.95716095, g_loss: 7.62166595\n",
      "Step: [1361] d_loss: 15.01737213, g_loss: 7.84248638\n",
      "Step: [1362] d_loss: 15.05810928, g_loss: 7.49851608\n",
      "Step: [1363] d_loss: 15.85194969, g_loss: 7.39561272\n",
      "Step: [1364] d_loss: 15.20790100, g_loss: 7.29158163\n",
      "Step: [1365] d_loss: 15.18269157, g_loss: 7.75464106\n",
      "Step: [1366] d_loss: 15.43048000, g_loss: 8.20945072\n",
      "Step: [1367] d_loss: 14.70742702, g_loss: 8.00741863\n",
      "Step: [1368] d_loss: 15.30023384, g_loss: 7.83579350\n",
      "Step: [1369] d_loss: 15.38342094, g_loss: 7.77155399\n",
      "Step: [1370] d_loss: 15.74082851, g_loss: 7.15689182\n",
      "Step: [1371] d_loss: 15.55598068, g_loss: 7.35425091\n",
      "Step: [1372] d_loss: 14.99454308, g_loss: 7.55513239\n",
      "Step: [1373] d_loss: 15.80822659, g_loss: 7.72607327\n",
      "Step: [1374] d_loss: 15.35564995, g_loss: 7.80618286\n",
      "Step: [1375] d_loss: 15.79397011, g_loss: 7.44540310\n",
      "Step: [1376] d_loss: 15.33889771, g_loss: 7.47545624\n",
      "Step: [1377] d_loss: 15.57936096, g_loss: 7.27328110\n",
      "Step: [1378] d_loss: 15.25717545, g_loss: 7.45981121\n",
      "Step: [1379] d_loss: 15.68346691, g_loss: 7.36189699\n",
      "Step: [1380] d_loss: 16.27850342, g_loss: 7.96299505\n",
      "Step: [1381] d_loss: 16.44400215, g_loss: 7.89791584\n",
      "Step: [1382] d_loss: 16.44532394, g_loss: 7.74145031\n",
      "Step: [1383] d_loss: 16.07425117, g_loss: 8.04471970\n",
      "Step: [1384] d_loss: 16.34184647, g_loss: 7.76650667\n",
      "Step: [1385] d_loss: 16.35269928, g_loss: 7.99712086\n",
      "Step: [1386] d_loss: 15.90528679, g_loss: 7.50937176\n",
      "Step: [1387] d_loss: 15.52503586, g_loss: 7.94932938\n",
      "Step: [1388] d_loss: 16.93997574, g_loss: 7.87279415\n",
      "Step: [1389] d_loss: 16.04062271, g_loss: 7.40973234\n",
      "Step: [1390] d_loss: 15.23558140, g_loss: 7.54574108\n",
      "Step: [1391] d_loss: 15.53048515, g_loss: 7.78360081\n",
      "Step: [1392] d_loss: 16.15827942, g_loss: 7.64647388\n",
      "Step: [1393] d_loss: 15.93571472, g_loss: 7.49248695\n",
      "Step: [1394] d_loss: 16.22435951, g_loss: 7.19585371\n",
      "Step: [1395] d_loss: 15.16767883, g_loss: 7.55259657\n",
      "Step: [1396] d_loss: 15.91971016, g_loss: 7.42631006\n",
      "Step: [1397] d_loss: 14.86593628, g_loss: 7.69914055\n",
      "Step: [1398] d_loss: 15.21439362, g_loss: 7.46418762\n",
      "Step: [1399] d_loss: 15.53519535, g_loss: 7.72278070\n",
      "Step: [1400] d_loss: 14.93052197, g_loss: 7.66664028\n",
      "Step: [1401] d_loss: 15.26077938, g_loss: 6.87713242\n",
      "Step: [1402] d_loss: 15.47402191, g_loss: 7.43065882\n",
      "Step: [1403] d_loss: 15.29590416, g_loss: 7.53137875\n",
      "Step: [1404] d_loss: 15.06811047, g_loss: 7.72895718\n",
      "Step: [1405] d_loss: 15.08146954, g_loss: 7.76246738\n",
      "Step: [1406] d_loss: 15.19817734, g_loss: 7.38154221\n",
      "Step: [1407] d_loss: 15.30560398, g_loss: 7.28601980\n",
      "Step: [1408] d_loss: 14.33968163, g_loss: 7.26906967\n",
      "Step: [1409] d_loss: 14.88024712, g_loss: 7.37781620\n",
      "Step: [1410] d_loss: 15.28919220, g_loss: 7.97642136\n",
      "Step: [1411] d_loss: 15.93429852, g_loss: 7.61657715\n",
      "Step: [1412] d_loss: 14.83589458, g_loss: 7.26022339\n",
      "Step: [1413] d_loss: 15.10357857, g_loss: 7.30506754\n",
      "Step: [1414] d_loss: 15.60402870, g_loss: 7.43838978\n",
      "Step: [1415] d_loss: 14.77961540, g_loss: 7.57111979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1416] d_loss: 15.44120026, g_loss: 7.74289894\n",
      "Step: [1417] d_loss: 15.44863892, g_loss: 7.93797398\n",
      "Step: [1418] d_loss: 15.87018490, g_loss: 7.93167543\n",
      "Step: [1419] d_loss: 15.29232502, g_loss: 8.08104038\n",
      "Step: [1420] d_loss: 15.60542488, g_loss: 7.60929489\n",
      "Step: [1421] d_loss: 16.54916382, g_loss: 7.77967739\n",
      "Step: [1422] d_loss: 16.13029099, g_loss: 7.79749775\n",
      "Step: [1423] d_loss: 15.06418419, g_loss: 8.28255081\n",
      "Step: [1424] d_loss: 16.04403877, g_loss: 7.82002449\n",
      "Step: [1425] d_loss: 15.66522789, g_loss: 7.73706913\n",
      "Step: [1426] d_loss: 15.07973099, g_loss: 7.69590425\n",
      "Step: [1427] d_loss: 15.08020782, g_loss: 8.13194466\n",
      "Step: [1428] d_loss: 15.20229149, g_loss: 7.87307262\n",
      "Step: [1429] d_loss: 15.21271229, g_loss: 7.82052040\n",
      "Step: [1430] d_loss: 15.11947250, g_loss: 7.90946198\n",
      "Step: [1431] d_loss: 16.10397911, g_loss: 7.94661045\n",
      "Step: [1432] d_loss: 15.73832130, g_loss: 7.67793322\n",
      "Step: [1433] d_loss: 15.80589485, g_loss: 7.61100864\n",
      "Step: [1434] d_loss: 15.49042892, g_loss: 7.52477503\n",
      "Step: [1435] d_loss: 15.16564369, g_loss: 7.63179874\n",
      "Step: [1436] d_loss: 15.71282005, g_loss: 7.70348549\n",
      "Step: [1437] d_loss: 14.89994907, g_loss: 7.63888597\n",
      "Step: [1438] d_loss: 15.56207085, g_loss: 7.76918888\n",
      "Step: [1439] d_loss: 15.25992775, g_loss: 7.62439585\n",
      "Step: [1440] d_loss: 15.34666443, g_loss: 7.46117306\n",
      "Step: [1441] d_loss: 15.03209496, g_loss: 7.48185253\n",
      "Step: [1442] d_loss: 15.12489414, g_loss: 7.25323963\n",
      "Step: [1443] d_loss: 15.28956985, g_loss: 7.35782385\n",
      "Step: [1444] d_loss: 16.22285271, g_loss: 7.18997002\n",
      "Step: [1445] d_loss: 15.20560265, g_loss: 7.43748474\n",
      "Step: [1446] d_loss: 14.88081074, g_loss: 7.56668949\n",
      "Step: [1447] d_loss: 15.77837181, g_loss: 7.20046806\n",
      "Step: [1448] d_loss: 15.19178200, g_loss: 7.37240934\n",
      "Step: [1449] d_loss: 15.26387405, g_loss: 7.76728487\n",
      "Step: [1450] d_loss: 14.89666843, g_loss: 7.78107548\n",
      "Step: [1451] d_loss: 15.77099037, g_loss: 7.52122307\n",
      "Step: [1452] d_loss: 15.50906372, g_loss: 7.69718075\n",
      "Step: [1453] d_loss: 14.98567867, g_loss: 7.63862896\n",
      "Step: [1454] d_loss: 15.48877335, g_loss: 7.92179966\n",
      "Step: [1455] d_loss: 15.83230495, g_loss: 7.22427273\n",
      "Step: [1456] d_loss: 14.88022041, g_loss: 7.60823059\n",
      "Step: [1457] d_loss: 14.16331482, g_loss: 8.02144814\n",
      "Step: [1458] d_loss: 15.11429310, g_loss: 7.83383369\n",
      "Step: [1459] d_loss: 15.51000118, g_loss: 7.70272875\n",
      "Step: [1460] d_loss: 15.75919533, g_loss: 7.68446684\n",
      "Step: [1461] d_loss: 16.40968704, g_loss: 8.24086952\n",
      "Step: [1462] d_loss: 15.83699512, g_loss: 7.85515642\n",
      "Step: [1463] d_loss: 16.13966370, g_loss: 7.99383163\n",
      "Step: [1464] d_loss: 16.25305557, g_loss: 8.17317009\n",
      "Step: [1465] d_loss: 17.43616104, g_loss: 7.40366554\n",
      "Step: [1466] d_loss: 15.16795540, g_loss: 7.98348522\n",
      "Step: [1467] d_loss: 15.36979675, g_loss: 7.83159637\n",
      "Step: [1468] d_loss: 15.65101242, g_loss: 7.47776985\n",
      "Step: [1469] d_loss: 15.43094063, g_loss: 8.34021378\n",
      "Step: [1470] d_loss: 16.09531975, g_loss: 7.62234926\n",
      "Step: [1471] d_loss: 15.17488861, g_loss: 7.65121746\n",
      "Step: [1472] d_loss: 16.06262970, g_loss: 7.25511742\n",
      "Step: [1473] d_loss: 14.56739616, g_loss: 8.08817101\n",
      "Step: [1474] d_loss: 15.49404716, g_loss: 7.62308788\n",
      "Step: [1475] d_loss: 15.48505688, g_loss: 7.84456444\n",
      "Step: [1476] d_loss: 15.18643761, g_loss: 8.08780479\n",
      "Step: [1477] d_loss: 16.15994263, g_loss: 7.76818037\n",
      "Step: [1478] d_loss: 15.06942654, g_loss: 7.78856659\n",
      "Step: [1479] d_loss: 15.67100143, g_loss: 7.39859295\n",
      "Step: [1480] d_loss: 15.47396851, g_loss: 7.21313810\n",
      "Step: [1481] d_loss: 14.63894653, g_loss: 7.93317223\n",
      "Step: [1482] d_loss: 14.47556114, g_loss: 8.00926590\n",
      "Step: [1483] d_loss: 14.96116161, g_loss: 7.71037722\n",
      "Step: [1484] d_loss: 14.42111206, g_loss: 7.86281633\n",
      "Step: [1485] d_loss: 15.14058495, g_loss: 7.36178255\n",
      "Step: [1486] d_loss: 15.96658993, g_loss: 7.54538155\n",
      "Step: [1487] d_loss: 15.80973816, g_loss: 7.55729580\n",
      "Step: [1488] d_loss: 15.92017937, g_loss: 7.62266302\n",
      "Step: [1489] d_loss: 15.50860977, g_loss: 7.56042624\n",
      "Step: [1490] d_loss: 15.65177822, g_loss: 7.52218914\n",
      "Step: [1491] d_loss: 15.89381981, g_loss: 7.46677780\n",
      "Step: [1492] d_loss: 14.94371223, g_loss: 7.70059204\n",
      "Step: [1493] d_loss: 15.88692665, g_loss: 7.33239841\n",
      "Step: [1494] d_loss: 16.31150627, g_loss: 7.56197262\n",
      "Step: [1495] d_loss: 15.23290634, g_loss: 7.36552811\n",
      "Step: [1496] d_loss: 15.89322090, g_loss: 7.64027166\n",
      "Step: [1497] d_loss: 15.88440514, g_loss: 7.51930523\n",
      "Step: [1498] d_loss: 15.52761269, g_loss: 7.78641987\n",
      "Step: [1499] d_loss: 15.60551453, g_loss: 7.19221354\n",
      "Step: [1500] d_loss: 15.62415218, g_loss: 7.89037991\n",
      "Step: [1501] d_loss: 15.25270081, g_loss: 7.57102966\n",
      "Step: [1502] d_loss: 15.15512085, g_loss: 7.80370808\n",
      "Step: [1503] d_loss: 15.31893158, g_loss: 8.11204624\n",
      "Step: [1504] d_loss: 15.60070992, g_loss: 7.78666401\n",
      "Step: [1505] d_loss: 15.84700012, g_loss: 7.68170309\n",
      "Step: [1506] d_loss: 14.67899132, g_loss: 8.31151772\n",
      "Step: [1507] d_loss: 15.33608818, g_loss: 7.44002628\n",
      "Step: [1508] d_loss: 14.98232269, g_loss: 7.43291378\n",
      "Step: [1509] d_loss: 15.85977745, g_loss: 7.32744265\n",
      "Step: [1510] d_loss: 15.38117981, g_loss: 7.80693436\n",
      "Step: [1511] d_loss: 15.73780441, g_loss: 7.72195816\n",
      "Step: [1512] d_loss: 14.99861336, g_loss: 7.82671309\n",
      "Step: [1513] d_loss: 15.36733437, g_loss: 7.69788122\n",
      "Step: [1514] d_loss: 14.84590530, g_loss: 7.90747833\n",
      "Step: [1515] d_loss: 15.26224327, g_loss: 7.69699717\n",
      "Step: [1516] d_loss: 15.15106106, g_loss: 7.54983044\n",
      "Step: [1517] d_loss: 15.10888767, g_loss: 7.80760717\n",
      "Step: [1518] d_loss: 15.03899479, g_loss: 8.03368568\n",
      "Step: [1519] d_loss: 16.47788048, g_loss: 7.03302813\n",
      "Step: [1520] d_loss: 15.09325409, g_loss: 7.79378414\n",
      "Step: [1521] d_loss: 15.81184483, g_loss: 7.71979141\n",
      "Step: [1522] d_loss: 15.78024673, g_loss: 7.90101004\n",
      "Step: [1523] d_loss: 15.45712852, g_loss: 7.85401821\n",
      "Step: [1524] d_loss: 15.55597973, g_loss: 7.77059412\n",
      "Step: [1525] d_loss: 15.05599594, g_loss: 7.59662819\n",
      "Step: [1526] d_loss: 15.74597931, g_loss: 8.11830044\n",
      "Step: [1527] d_loss: 16.46597672, g_loss: 7.87860870\n",
      "Step: [1528] d_loss: 15.32736778, g_loss: 7.43722725\n",
      "Step: [1529] d_loss: 15.47945213, g_loss: 7.43952465\n",
      "Step: [1530] d_loss: 15.35154343, g_loss: 7.71340370\n",
      "Step: [1531] d_loss: 14.80751801, g_loss: 7.21289921\n",
      "Step: [1532] d_loss: 14.98290825, g_loss: 7.56757116\n",
      "Step: [1533] d_loss: 15.32213211, g_loss: 7.35459137\n",
      "Step: [1534] d_loss: 14.76107502, g_loss: 7.86870432\n",
      "Step: [1535] d_loss: 15.22942543, g_loss: 7.75458574\n",
      "Step: [1536] d_loss: 15.29024982, g_loss: 7.90242434\n",
      "Step: [1537] d_loss: 15.04289055, g_loss: 7.41427803\n",
      "Step: [1538] d_loss: 15.55037022, g_loss: 7.39442730\n",
      "Step: [1539] d_loss: 15.79145050, g_loss: 7.48282766\n",
      "Step: [1540] d_loss: 16.26920319, g_loss: 7.77869749\n",
      "Step: [1541] d_loss: 15.94050980, g_loss: 7.64835453\n",
      "Step: [1542] d_loss: 15.21170521, g_loss: 7.48282242\n",
      "Step: [1543] d_loss: 15.16379929, g_loss: 8.09425640\n",
      "Step: [1544] d_loss: 15.03852654, g_loss: 7.91037512\n",
      "Step: [1545] d_loss: 15.26732254, g_loss: 7.56349087\n",
      "Step: [1546] d_loss: 15.10349560, g_loss: 7.81907511\n",
      "Step: [1547] d_loss: 15.81314564, g_loss: 7.76191998\n",
      "Step: [1548] d_loss: 15.58352852, g_loss: 8.20314884\n",
      "Step: [1549] d_loss: 15.03794479, g_loss: 7.59884071\n",
      "Step: [1550] d_loss: 15.26572037, g_loss: 8.11773968\n",
      "Step: [1551] d_loss: 16.00182343, g_loss: 8.87949562\n",
      "Step: [1552] d_loss: 15.58587265, g_loss: 7.92850780\n",
      "Step: [1553] d_loss: 15.80801010, g_loss: 7.67089272\n",
      "Step: [1554] d_loss: 15.49445534, g_loss: 8.14363575\n",
      "Step: [1555] d_loss: 15.64626598, g_loss: 8.13429356\n",
      "Step: [1556] d_loss: 16.13021469, g_loss: 8.07038593\n",
      "Step: [1557] d_loss: 15.62602043, g_loss: 7.37434959\n",
      "Step: [1558] d_loss: 15.25548172, g_loss: 7.70762587\n",
      "Step: [1559] d_loss: 15.31906509, g_loss: 7.73622036\n",
      "Step: [1560] d_loss: 15.19556046, g_loss: 7.45043087\n",
      "Step: [1561] d_loss: 14.17474937, g_loss: 7.75598717\n",
      "Step: [1562] d_loss: 14.95050430, g_loss: 8.14046860\n",
      "Step: [1563] d_loss: 15.14999771, g_loss: 7.40108776\n",
      "Step: [1564] d_loss: 15.74711990, g_loss: 7.27630997\n",
      "Step: [1565] d_loss: 14.21437073, g_loss: 7.73355675\n",
      "Step: [1566] d_loss: 15.20640945, g_loss: 7.18306923\n",
      "Step: [1567] d_loss: 15.79028797, g_loss: 7.97398424\n",
      "Step: [1568] d_loss: 15.27990150, g_loss: 7.89024878\n",
      "Step: [1569] d_loss: 16.08232880, g_loss: 7.75104904\n",
      "Step: [1570] d_loss: 14.94277763, g_loss: 7.70946693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1571] d_loss: 15.20460129, g_loss: 7.69569254\n",
      "Step: [1572] d_loss: 15.80455971, g_loss: 7.56080246\n",
      "Step: [1573] d_loss: 15.15845299, g_loss: 7.75938416\n",
      "Step: [1574] d_loss: 14.95852852, g_loss: 7.88065434\n",
      "Step: [1575] d_loss: 15.71068764, g_loss: 8.14723682\n",
      "Step: [1576] d_loss: 16.99494743, g_loss: 8.80766773\n",
      "Step: [1577] d_loss: 17.60757637, g_loss: 8.07016945\n",
      "Step: [1578] d_loss: 16.21365738, g_loss: 8.20967674\n",
      "Step: [1579] d_loss: 16.36829758, g_loss: 7.58734989\n",
      "Step: [1580] d_loss: 15.01583958, g_loss: 8.03823280\n",
      "Step: [1581] d_loss: 15.96624184, g_loss: 7.87170839\n",
      "Step: [1582] d_loss: 16.29111290, g_loss: 7.46618938\n",
      "Step: [1583] d_loss: 14.90586281, g_loss: 8.21227074\n",
      "Step: [1584] d_loss: 14.86334133, g_loss: 8.24173546\n",
      "Step: [1585] d_loss: 15.43883324, g_loss: 8.20584202\n",
      "Step: [1586] d_loss: 15.51003075, g_loss: 7.55520391\n",
      "Step: [1587] d_loss: 15.56747246, g_loss: 8.04887962\n",
      "Step: [1588] d_loss: 14.92619991, g_loss: 7.59916973\n",
      "Step: [1589] d_loss: 14.52627563, g_loss: 8.15752983\n",
      "Step: [1590] d_loss: 15.21890354, g_loss: 7.83041000\n",
      "Step: [1591] d_loss: 16.23191833, g_loss: 7.81522655\n",
      "Step: [1592] d_loss: 16.03513527, g_loss: 7.90584517\n",
      "Step: [1593] d_loss: 15.56934929, g_loss: 7.72154760\n",
      "Step: [1594] d_loss: 16.14411163, g_loss: 7.94455624\n",
      "Step: [1595] d_loss: 15.53085136, g_loss: 7.75160408\n",
      "Step: [1596] d_loss: 15.73001194, g_loss: 7.61583185\n",
      "Step: [1597] d_loss: 15.60347557, g_loss: 7.48438406\n",
      "Step: [1598] d_loss: 15.51780605, g_loss: 8.18077469\n",
      "Step: [1599] d_loss: 15.47362709, g_loss: 8.15008831\n",
      "Step: [1600] d_loss: 15.82589245, g_loss: 7.73849869\n",
      "Step: [1601] d_loss: 15.21675396, g_loss: 7.94701958\n",
      "Step: [1602] d_loss: 14.74342346, g_loss: 8.10634232\n",
      "Step: [1603] d_loss: 15.27906036, g_loss: 7.58993673\n",
      "Step: [1604] d_loss: 15.19456959, g_loss: 7.17994022\n",
      "Step: [1605] d_loss: 15.23838425, g_loss: 7.68376827\n",
      "Step: [1606] d_loss: 15.00389862, g_loss: 7.88555813\n",
      "Step: [1607] d_loss: 16.32030106, g_loss: 7.81069374\n",
      "Step: [1608] d_loss: 14.94898987, g_loss: 7.93744373\n",
      "Step: [1609] d_loss: 15.46751404, g_loss: 7.67668343\n",
      "Step: [1610] d_loss: 15.05078506, g_loss: 7.33336735\n",
      "Step: [1611] d_loss: 15.13978100, g_loss: 7.68222761\n",
      "Step: [1612] d_loss: 15.17341995, g_loss: 7.47670794\n",
      "Step: [1613] d_loss: 15.34684467, g_loss: 7.58086061\n",
      "Step: [1614] d_loss: 14.67276001, g_loss: 7.77864361\n",
      "Step: [1615] d_loss: 15.03096962, g_loss: 8.15475845\n",
      "Step: [1616] d_loss: 15.73060131, g_loss: 8.02717590\n",
      "Step: [1617] d_loss: 15.23102093, g_loss: 7.96823835\n",
      "Step: [1618] d_loss: 15.98431015, g_loss: 7.46889210\n",
      "Step: [1619] d_loss: 16.15031433, g_loss: 7.75268841\n",
      "Step: [1620] d_loss: 16.07782555, g_loss: 8.26170540\n",
      "Step: [1621] d_loss: 15.49277973, g_loss: 8.43999290\n",
      "Step: [1622] d_loss: 15.67459297, g_loss: 7.68411732\n",
      "Step: [1623] d_loss: 15.20802975, g_loss: 7.54753685\n",
      "Step: [1624] d_loss: 15.54045296, g_loss: 7.46058559\n",
      "Step: [1625] d_loss: 15.62953758, g_loss: 7.59893656\n",
      "Step: [1626] d_loss: 15.70312119, g_loss: 7.85341501\n",
      "Step: [1627] d_loss: 15.31292915, g_loss: 7.55089378\n",
      "Step: [1628] d_loss: 14.77216434, g_loss: 7.69708204\n",
      "Step: [1629] d_loss: 15.06697559, g_loss: 8.08856010\n",
      "Step: [1630] d_loss: 15.27388000, g_loss: 7.90534925\n",
      "Step: [1631] d_loss: 15.84799290, g_loss: 7.32877779\n",
      "Step: [1632] d_loss: 15.08906746, g_loss: 8.22332954\n",
      "Step: [1633] d_loss: 15.30437279, g_loss: 7.65734577\n",
      "Step: [1634] d_loss: 15.99534607, g_loss: 8.11531067\n",
      "Step: [1635] d_loss: 15.18969631, g_loss: 7.81929350\n",
      "Step: [1636] d_loss: 16.17881966, g_loss: 7.26139688\n",
      "Step: [1637] d_loss: 15.88022232, g_loss: 7.17099953\n",
      "Step: [1638] d_loss: 14.74959850, g_loss: 7.79340029\n",
      "Step: [1639] d_loss: 15.08927155, g_loss: 7.53011608\n",
      "Step: [1640] d_loss: 14.88512230, g_loss: 7.84280014\n",
      "Step: [1641] d_loss: 15.04640198, g_loss: 8.29232025\n",
      "Step: [1642] d_loss: 15.16981220, g_loss: 7.70720005\n",
      "Step: [1643] d_loss: 15.12870502, g_loss: 7.79017258\n",
      "Step: [1644] d_loss: 15.70448780, g_loss: 7.24110031\n",
      "Step: [1645] d_loss: 14.89192200, g_loss: 7.59754562\n",
      "Step: [1646] d_loss: 15.49416065, g_loss: 7.50838995\n",
      "Step: [1647] d_loss: 14.59759331, g_loss: 8.05506516\n",
      "Step: [1648] d_loss: 14.98067474, g_loss: 8.15477085\n",
      "Step: [1649] d_loss: 15.70479965, g_loss: 7.32137442\n",
      "Step: [1650] d_loss: 15.36839485, g_loss: 7.44608879\n",
      "Step: [1651] d_loss: 14.76626205, g_loss: 7.46046543\n",
      "Step: [1652] d_loss: 15.17123222, g_loss: 7.47311687\n",
      "Step: [1653] d_loss: 15.42504883, g_loss: 7.29184628\n",
      "Step: [1654] d_loss: 15.35364342, g_loss: 7.61825752\n",
      "Step: [1655] d_loss: 15.78955173, g_loss: 7.51324940\n",
      "Step: [1656] d_loss: 15.21788979, g_loss: 7.62178469\n",
      "Step: [1657] d_loss: 15.43198967, g_loss: 8.07357979\n",
      "Step: [1658] d_loss: 15.73261833, g_loss: 7.30626440\n",
      "Step: [1659] d_loss: 14.57722473, g_loss: 7.56645107\n",
      "Step: [1660] d_loss: 15.85792828, g_loss: 7.52857065\n",
      "Step: [1661] d_loss: 15.69914246, g_loss: 7.29689312\n",
      "Step: [1662] d_loss: 14.75996208, g_loss: 7.81803608\n",
      "Step: [1663] d_loss: 15.34063816, g_loss: 7.58840370\n",
      "Step: [1664] d_loss: 15.68680763, g_loss: 7.48531914\n",
      "Step: [1665] d_loss: 15.56173134, g_loss: 7.40832281\n",
      "Step: [1666] d_loss: 15.48998260, g_loss: 7.56452751\n",
      "Step: [1667] d_loss: 14.80142689, g_loss: 8.15778351\n",
      "Step: [1668] d_loss: 14.74825478, g_loss: 8.00756645\n",
      "Step: [1669] d_loss: 14.79074669, g_loss: 8.46283722\n",
      "Step: [1670] d_loss: 14.71172714, g_loss: 7.63541079\n",
      "Step: [1671] d_loss: 14.50078011, g_loss: 7.61643219\n",
      "Step: [1672] d_loss: 14.70650482, g_loss: 8.32027626\n",
      "Step: [1673] d_loss: 14.78863144, g_loss: 7.95572376\n",
      "Step: [1674] d_loss: 14.80567360, g_loss: 7.10922337\n",
      "Step: [1675] d_loss: 15.53840160, g_loss: 7.18552876\n",
      "Step: [1676] d_loss: 15.49132252, g_loss: 7.74181175\n",
      "Step: [1677] d_loss: 15.17702293, g_loss: 7.61645937\n",
      "Step: [1678] d_loss: 15.20416832, g_loss: 7.83443260\n",
      "Step: [1679] d_loss: 15.50390816, g_loss: 8.69878864\n",
      "Step: [1680] d_loss: 16.06757164, g_loss: 7.53135681\n",
      "Step: [1681] d_loss: 15.85178185, g_loss: 8.05362511\n",
      "Step: [1682] d_loss: 16.22400665, g_loss: 8.16159534\n",
      "Step: [1683] d_loss: 17.24179459, g_loss: 8.58006477\n",
      "Step: [1684] d_loss: 17.20824814, g_loss: 8.89629269\n",
      "Step: [1685] d_loss: 16.75131989, g_loss: 8.51998234\n",
      "Step: [1686] d_loss: 15.89555168, g_loss: 7.85384464\n",
      "Step: [1687] d_loss: 16.21855164, g_loss: 8.04187679\n",
      "Step: [1688] d_loss: 15.97151375, g_loss: 7.80694962\n",
      "Step: [1689] d_loss: 15.93086910, g_loss: 7.51197052\n",
      "Step: [1690] d_loss: 15.69967461, g_loss: 7.76219654\n",
      "Step: [1691] d_loss: 15.30564594, g_loss: 7.96189404\n",
      "Step: [1692] d_loss: 15.67548752, g_loss: 7.83626938\n",
      "Step: [1693] d_loss: 15.74161816, g_loss: 7.99403811\n",
      "Step: [1694] d_loss: 15.59114838, g_loss: 7.34698486\n",
      "Step: [1695] d_loss: 15.69611645, g_loss: 7.81270123\n",
      "Step: [1696] d_loss: 15.39612961, g_loss: 7.86440849\n",
      "Step: [1697] d_loss: 15.58844090, g_loss: 8.28516197\n",
      "Step: [1698] d_loss: 16.21670151, g_loss: 8.54159355\n",
      "Step: [1699] d_loss: 15.49599457, g_loss: 8.11373520\n",
      "Step: [1700] d_loss: 15.49514198, g_loss: 7.66955662\n",
      "Step: [1701] d_loss: 15.27162457, g_loss: 7.54927683\n",
      "Step: [1702] d_loss: 15.66615486, g_loss: 7.39248276\n",
      "Step: [1703] d_loss: 15.31021500, g_loss: 7.36476517\n",
      "Step: [1704] d_loss: 15.75769615, g_loss: 7.69316578\n",
      "Step: [1705] d_loss: 15.14247131, g_loss: 7.19957733\n",
      "Step: [1706] d_loss: 14.95860291, g_loss: 7.50155020\n",
      "Step: [1707] d_loss: 14.70830345, g_loss: 8.25824738\n",
      "Step: [1708] d_loss: 15.51650429, g_loss: 8.04839516\n",
      "Step: [1709] d_loss: 14.49205971, g_loss: 8.14565849\n",
      "Step: [1710] d_loss: 15.88531303, g_loss: 7.95696449\n",
      "Step: [1711] d_loss: 16.38251495, g_loss: 8.21202660\n",
      "Step: [1712] d_loss: 16.35779381, g_loss: 8.10912132\n",
      "Step: [1713] d_loss: 15.37429619, g_loss: 7.66672707\n",
      "Step: [1714] d_loss: 15.75343704, g_loss: 7.97304058\n",
      "Step: [1715] d_loss: 15.63256073, g_loss: 7.87827730\n",
      "Step: [1716] d_loss: 15.67430878, g_loss: 7.76961613\n",
      "Step: [1717] d_loss: 15.43376923, g_loss: 7.59886789\n",
      "Step: [1718] d_loss: 15.32038784, g_loss: 7.92551899\n",
      "Step: [1719] d_loss: 15.21718407, g_loss: 7.90348339\n",
      "Step: [1720] d_loss: 15.18285370, g_loss: 7.66786098\n",
      "Step: [1721] d_loss: 15.69478893, g_loss: 7.41358471\n",
      "Step: [1722] d_loss: 15.06413269, g_loss: 7.83548832\n",
      "Step: [1723] d_loss: 15.07632542, g_loss: 7.39213181\n",
      "Step: [1724] d_loss: 15.58865833, g_loss: 7.88180351\n",
      "Step: [1725] d_loss: 14.60970306, g_loss: 7.97777843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1726] d_loss: 16.44321060, g_loss: 7.26050615\n",
      "Step: [1727] d_loss: 15.24495888, g_loss: 7.61402464\n",
      "Step: [1728] d_loss: 14.30714798, g_loss: 8.28746033\n",
      "Step: [1729] d_loss: 15.31796169, g_loss: 8.13175011\n",
      "Step: [1730] d_loss: 15.14055634, g_loss: 8.08889771\n",
      "Step: [1731] d_loss: 14.83130932, g_loss: 8.03893661\n",
      "Step: [1732] d_loss: 14.61280155, g_loss: 7.96133995\n",
      "Step: [1733] d_loss: 14.58897018, g_loss: 8.05483723\n",
      "Step: [1734] d_loss: 14.26108932, g_loss: 7.59029579\n",
      "Step: [1735] d_loss: 15.37001991, g_loss: 7.38300228\n",
      "Step: [1736] d_loss: 14.97833157, g_loss: 7.61006308\n",
      "Step: [1737] d_loss: 14.75728226, g_loss: 8.10304070\n",
      "Step: [1738] d_loss: 14.67153263, g_loss: 7.96291113\n",
      "Step: [1739] d_loss: 15.43392277, g_loss: 7.61691856\n",
      "Step: [1740] d_loss: 15.37898064, g_loss: 7.35298443\n",
      "Step: [1741] d_loss: 14.96110153, g_loss: 8.26088715\n",
      "Step: [1742] d_loss: 15.68588066, g_loss: 7.97243929\n",
      "Step: [1743] d_loss: 15.87040710, g_loss: 8.06000328\n",
      "Step: [1744] d_loss: 15.99046326, g_loss: 8.08502960\n",
      "Step: [1745] d_loss: 15.75663948, g_loss: 7.97172070\n",
      "Step: [1746] d_loss: 14.69866371, g_loss: 7.88193655\n",
      "Step: [1747] d_loss: 14.87443733, g_loss: 7.86519289\n",
      "Step: [1748] d_loss: 14.88969994, g_loss: 8.09987164\n",
      "Step: [1749] d_loss: 15.26258087, g_loss: 7.72638321\n",
      "Step: [1750] d_loss: 15.67374802, g_loss: 7.77616882\n",
      "Step: [1751] d_loss: 15.57392788, g_loss: 7.80685902\n",
      "Step: [1752] d_loss: 15.81809330, g_loss: 7.58117104\n",
      "Step: [1753] d_loss: 15.71791267, g_loss: 7.74348545\n",
      "Step: [1754] d_loss: 16.38125610, g_loss: 8.54580116\n",
      "Step: [1755] d_loss: 16.05601883, g_loss: 7.80043030\n",
      "Step: [1756] d_loss: 15.27485752, g_loss: 7.49125957\n",
      "Step: [1757] d_loss: 15.38327026, g_loss: 8.06282043\n",
      "Step: [1758] d_loss: 15.92582130, g_loss: 8.45210266\n",
      "Step: [1759] d_loss: 15.83532619, g_loss: 8.41270065\n",
      "Step: [1760] d_loss: 15.76219559, g_loss: 7.43956041\n",
      "Step: [1761] d_loss: 15.34796524, g_loss: 7.52297688\n",
      "Step: [1762] d_loss: 15.81466866, g_loss: 7.20114708\n",
      "Step: [1763] d_loss: 14.94746304, g_loss: 8.20484924\n",
      "Step: [1764] d_loss: 14.67611027, g_loss: 7.60485792\n",
      "Step: [1765] d_loss: 15.49486351, g_loss: 7.88383102\n",
      "Step: [1766] d_loss: 16.20855331, g_loss: 7.47994280\n",
      "Step: [1767] d_loss: 15.38621044, g_loss: 7.91677094\n",
      "Step: [1768] d_loss: 15.25399494, g_loss: 7.93989849\n",
      "Step: [1769] d_loss: 14.96196747, g_loss: 7.95638084\n",
      "Step: [1770] d_loss: 15.01261330, g_loss: 8.20297337\n",
      "Step: [1771] d_loss: 15.58540344, g_loss: 7.33288860\n",
      "Step: [1772] d_loss: 15.43645859, g_loss: 7.86279821\n",
      "Step: [1773] d_loss: 15.17767620, g_loss: 8.39036274\n",
      "Step: [1774] d_loss: 15.35015869, g_loss: 8.09516335\n",
      "Step: [1775] d_loss: 15.45949078, g_loss: 7.99840117\n",
      "Step: [1776] d_loss: 14.75498390, g_loss: 7.95474768\n",
      "Step: [1777] d_loss: 15.33164406, g_loss: 7.74309731\n",
      "Step: [1778] d_loss: 14.85420418, g_loss: 8.10801125\n",
      "Step: [1779] d_loss: 15.73962307, g_loss: 7.94087315\n",
      "Step: [1780] d_loss: 15.00488663, g_loss: 7.88452625\n",
      "Step: [1781] d_loss: 14.69850063, g_loss: 7.94975567\n",
      "Step: [1782] d_loss: 14.34459114, g_loss: 8.23349762\n",
      "Step: [1783] d_loss: 14.38420105, g_loss: 7.93006229\n",
      "Step: [1784] d_loss: 14.57225704, g_loss: 7.71598673\n",
      "Step: [1785] d_loss: 14.43383980, g_loss: 7.83018303\n",
      "Step: [1786] d_loss: 15.76923561, g_loss: 7.40835476\n",
      "Step: [1787] d_loss: 15.23630142, g_loss: 7.90275097\n",
      "Step: [1788] d_loss: 15.12891006, g_loss: 7.82895803\n",
      "Step: [1789] d_loss: 15.76379204, g_loss: 7.93686008\n",
      "Step: [1790] d_loss: 15.25031662, g_loss: 8.19105721\n",
      "Step: [1791] d_loss: 15.84620667, g_loss: 7.65315056\n",
      "Step: [1792] d_loss: 15.12292480, g_loss: 8.01852322\n",
      "Step: [1793] d_loss: 15.42095375, g_loss: 8.13795853\n",
      "Step: [1794] d_loss: 15.80999279, g_loss: 8.51373959\n",
      "Step: [1795] d_loss: 16.84395599, g_loss: 8.04278946\n",
      "Step: [1796] d_loss: 16.54545784, g_loss: 8.19680405\n",
      "Step: [1797] d_loss: 15.84514427, g_loss: 8.78176498\n",
      "Step: [1798] d_loss: 16.18280983, g_loss: 8.36990738\n",
      "Step: [1799] d_loss: 16.31232643, g_loss: 7.60640907\n",
      "Step: [1800] d_loss: 15.63230228, g_loss: 7.66601133\n",
      "Step: [1801] d_loss: 15.09298897, g_loss: 8.02481270\n",
      "Step: [1802] d_loss: 14.85897636, g_loss: 7.64151859\n",
      "Step: [1803] d_loss: 14.65665245, g_loss: 7.58174944\n",
      "Step: [1804] d_loss: 15.61791039, g_loss: 7.77794886\n",
      "Step: [1805] d_loss: 14.83030319, g_loss: 7.76543522\n",
      "Step: [1806] d_loss: 15.17923069, g_loss: 8.12135506\n",
      "Step: [1807] d_loss: 15.42719460, g_loss: 8.06186581\n",
      "Step: [1808] d_loss: 15.86007309, g_loss: 8.17155457\n",
      "Step: [1809] d_loss: 15.13255882, g_loss: 8.11063862\n",
      "Step: [1810] d_loss: 16.06145287, g_loss: 7.52401114\n",
      "Step: [1811] d_loss: 14.82083321, g_loss: 8.12314320\n",
      "Step: [1812] d_loss: 16.02537155, g_loss: 7.74631214\n",
      "Step: [1813] d_loss: 15.96742058, g_loss: 8.27506065\n",
      "Step: [1814] d_loss: 15.19394112, g_loss: 8.23703384\n",
      "Step: [1815] d_loss: 16.08894730, g_loss: 8.17688370\n",
      "Step: [1816] d_loss: 15.69607162, g_loss: 7.89874268\n",
      "Step: [1817] d_loss: 15.54204750, g_loss: 8.16086578\n",
      "Step: [1818] d_loss: 15.46949768, g_loss: 7.58416605\n",
      "Step: [1819] d_loss: 15.22906208, g_loss: 7.85558319\n",
      "Step: [1820] d_loss: 16.01579094, g_loss: 8.23795319\n",
      "Step: [1821] d_loss: 15.37882233, g_loss: 7.92559385\n",
      "Step: [1822] d_loss: 14.74053764, g_loss: 8.28472233\n",
      "Step: [1823] d_loss: 15.94273472, g_loss: 7.95725489\n",
      "Step: [1824] d_loss: 15.82536888, g_loss: 8.11424446\n",
      "Step: [1825] d_loss: 16.44096565, g_loss: 7.81352615\n",
      "Step: [1826] d_loss: 14.83224583, g_loss: 8.24639320\n",
      "Step: [1827] d_loss: 14.91893387, g_loss: 7.80918550\n",
      "Step: [1828] d_loss: 15.28025246, g_loss: 7.45454121\n",
      "Step: [1829] d_loss: 15.25776100, g_loss: 7.71641731\n",
      "Step: [1830] d_loss: 15.37460995, g_loss: 7.99575233\n",
      "Step: [1831] d_loss: 15.45688915, g_loss: 7.65986824\n",
      "Step: [1832] d_loss: 15.38715935, g_loss: 7.94033432\n",
      "Step: [1833] d_loss: 15.10771751, g_loss: 7.89827871\n",
      "Step: [1834] d_loss: 15.22640038, g_loss: 7.49384212\n",
      "Step: [1835] d_loss: 15.05617714, g_loss: 8.01244164\n",
      "Step: [1836] d_loss: 15.02031326, g_loss: 7.76587200\n",
      "Step: [1837] d_loss: 15.03347397, g_loss: 7.95377064\n",
      "Step: [1838] d_loss: 15.66592312, g_loss: 7.64055729\n",
      "Step: [1839] d_loss: 15.31232643, g_loss: 7.60990572\n",
      "Step: [1840] d_loss: 14.62105083, g_loss: 7.89169121\n",
      "Step: [1841] d_loss: 14.98879719, g_loss: 7.86750126\n",
      "Step: [1842] d_loss: 15.63391972, g_loss: 7.50161839\n",
      "Step: [1843] d_loss: 14.59286880, g_loss: 8.32090759\n",
      "Step: [1844] d_loss: 15.46619797, g_loss: 7.80482817\n",
      "Step: [1845] d_loss: 15.39709568, g_loss: 7.89690113\n",
      "Step: [1846] d_loss: 14.86051559, g_loss: 7.66059780\n",
      "Step: [1847] d_loss: 14.98049355, g_loss: 8.00440598\n",
      "Step: [1848] d_loss: 15.10993862, g_loss: 8.54136276\n",
      "Step: [1849] d_loss: 15.27163124, g_loss: 7.59059906\n",
      "Step: [1850] d_loss: 15.25546074, g_loss: 7.53331089\n",
      "Step: [1851] d_loss: 15.20637703, g_loss: 7.65163136\n",
      "Step: [1852] d_loss: 14.66294670, g_loss: 8.07040882\n",
      "Step: [1853] d_loss: 15.20924950, g_loss: 7.65376949\n",
      "Step: [1854] d_loss: 14.58885670, g_loss: 7.71215725\n",
      "Step: [1855] d_loss: 15.02987671, g_loss: 7.56082678\n",
      "Step: [1856] d_loss: 15.13667011, g_loss: 7.75211859\n",
      "Step: [1857] d_loss: 15.31789207, g_loss: 7.55338812\n",
      "Step: [1858] d_loss: 14.79017544, g_loss: 8.05401611\n",
      "Step: [1859] d_loss: 14.98254013, g_loss: 7.83212566\n",
      "Step: [1860] d_loss: 14.59145737, g_loss: 7.68902969\n",
      "Step: [1861] d_loss: 15.37258339, g_loss: 7.66880369\n",
      "Step: [1862] d_loss: 14.55805111, g_loss: 8.19395351\n",
      "Step: [1863] d_loss: 15.63581085, g_loss: 7.47777987\n",
      "Step: [1864] d_loss: 15.31236553, g_loss: 7.78594494\n",
      "Step: [1865] d_loss: 14.97303963, g_loss: 8.12825012\n",
      "Step: [1866] d_loss: 16.28661537, g_loss: 7.47981358\n",
      "Step: [1867] d_loss: 15.70437241, g_loss: 7.79955864\n",
      "Step: [1868] d_loss: 14.52227783, g_loss: 8.16109467\n",
      "Step: [1869] d_loss: 15.20919418, g_loss: 8.30406666\n",
      "Step: [1870] d_loss: 15.13285065, g_loss: 8.07805252\n",
      "Step: [1871] d_loss: 15.08928967, g_loss: 8.33150673\n",
      "Step: [1872] d_loss: 16.06746674, g_loss: 7.86494923\n",
      "Step: [1873] d_loss: 15.36531258, g_loss: 7.66137981\n",
      "Step: [1874] d_loss: 15.06885529, g_loss: 7.67519999\n",
      "Step: [1875] d_loss: 14.38174725, g_loss: 7.96951866\n",
      "Step: [1876] d_loss: 15.90873432, g_loss: 7.96011353\n",
      "Step: [1877] d_loss: 14.46967983, g_loss: 8.04393387\n",
      "Step: [1878] d_loss: 15.31058311, g_loss: 8.06424904\n",
      "Step: [1879] d_loss: 15.62534237, g_loss: 8.12059498\n",
      "Step: [1880] d_loss: 15.87015915, g_loss: 7.71437836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1881] d_loss: 15.06102943, g_loss: 7.58065319\n",
      "Step: [1882] d_loss: 14.84067249, g_loss: 7.83474636\n",
      "Step: [1883] d_loss: 14.64084530, g_loss: 7.75968838\n",
      "Step: [1884] d_loss: 14.38839817, g_loss: 8.25812721\n",
      "Step: [1885] d_loss: 15.08362007, g_loss: 7.77579880\n",
      "Step: [1886] d_loss: 16.03059578, g_loss: 7.55381012\n",
      "Step: [1887] d_loss: 15.28145218, g_loss: 8.16820240\n",
      "Step: [1888] d_loss: 15.55237103, g_loss: 7.68843651\n",
      "Step: [1889] d_loss: 15.45037365, g_loss: 7.40152931\n",
      "Step: [1890] d_loss: 16.51036263, g_loss: 7.18907261\n",
      "Step: [1891] d_loss: 15.54714394, g_loss: 7.54139328\n",
      "Step: [1892] d_loss: 15.58678150, g_loss: 7.97537184\n",
      "Step: [1893] d_loss: 15.46330643, g_loss: 7.57424927\n",
      "Step: [1894] d_loss: 15.17122269, g_loss: 7.99404526\n",
      "Step: [1895] d_loss: 14.99296379, g_loss: 8.11320114\n",
      "Step: [1896] d_loss: 14.76239681, g_loss: 8.51512718\n",
      "Step: [1897] d_loss: 14.84124184, g_loss: 7.92870522\n",
      "Step: [1898] d_loss: 14.77430344, g_loss: 7.78165150\n",
      "Step: [1899] d_loss: 14.41502285, g_loss: 8.02936172\n",
      "Step: [1900] d_loss: 15.14156246, g_loss: 7.95816517\n",
      "Step: [1901] d_loss: 14.79334068, g_loss: 7.54867935\n",
      "Step: [1902] d_loss: 14.88266563, g_loss: 7.83400249\n",
      "Step: [1903] d_loss: 14.39479160, g_loss: 7.61098051\n",
      "Step: [1904] d_loss: 14.71264267, g_loss: 7.91175556\n",
      "Step: [1905] d_loss: 14.39977169, g_loss: 8.25685120\n",
      "Step: [1906] d_loss: 15.39910126, g_loss: 7.35748100\n",
      "Step: [1907] d_loss: 15.37716293, g_loss: 8.24837208\n",
      "Step: [1908] d_loss: 14.80225945, g_loss: 7.96465492\n",
      "Step: [1909] d_loss: 14.21426392, g_loss: 8.44077206\n",
      "Step: [1910] d_loss: 15.84198570, g_loss: 7.63419247\n",
      "Step: [1911] d_loss: 14.94244385, g_loss: 8.17027473\n",
      "Step: [1912] d_loss: 15.53848457, g_loss: 7.58525753\n",
      "Step: [1913] d_loss: 15.00210094, g_loss: 7.98633766\n",
      "Step: [1914] d_loss: 14.77354240, g_loss: 7.92329073\n",
      "Step: [1915] d_loss: 15.82468605, g_loss: 7.62506628\n",
      "Step: [1916] d_loss: 15.92349052, g_loss: 8.31441307\n",
      "Step: [1917] d_loss: 16.35576630, g_loss: 7.96372795\n",
      "Step: [1918] d_loss: 15.52080345, g_loss: 7.77349281\n",
      "Step: [1919] d_loss: 15.99187851, g_loss: 8.20398045\n",
      "Step: [1920] d_loss: 15.59164238, g_loss: 7.63873482\n",
      "Step: [1921] d_loss: 15.68163395, g_loss: 8.34888554\n",
      "Step: [1922] d_loss: 16.06142235, g_loss: 8.17546272\n",
      "Step: [1923] d_loss: 15.62862206, g_loss: 8.05302811\n",
      "Step: [1924] d_loss: 15.56341553, g_loss: 8.11736488\n",
      "Step: [1925] d_loss: 16.18601418, g_loss: 8.31457424\n",
      "Step: [1926] d_loss: 16.10414505, g_loss: 8.22212887\n",
      "Step: [1927] d_loss: 15.56032372, g_loss: 7.70156240\n",
      "Step: [1928] d_loss: 14.72360039, g_loss: 7.91488647\n",
      "Step: [1929] d_loss: 14.64485645, g_loss: 7.91233969\n",
      "Step: [1930] d_loss: 15.07817554, g_loss: 8.58460712\n",
      "Step: [1931] d_loss: 15.61042309, g_loss: 7.68386364\n",
      "Step: [1932] d_loss: 15.40644646, g_loss: 8.44282722\n",
      "Step: [1933] d_loss: 15.21627998, g_loss: 7.89908886\n",
      "Step: [1934] d_loss: 15.39390564, g_loss: 7.63880920\n",
      "Step: [1935] d_loss: 15.28987122, g_loss: 7.67400360\n",
      "Step: [1936] d_loss: 15.29418755, g_loss: 8.24998093\n",
      "Step: [1937] d_loss: 15.95285416, g_loss: 7.77699947\n",
      "Step: [1938] d_loss: 15.50901031, g_loss: 8.53091240\n",
      "Step: [1939] d_loss: 15.94116688, g_loss: 7.68197441\n",
      "Step: [1940] d_loss: 15.59054184, g_loss: 8.04735374\n",
      "Step: [1941] d_loss: 15.63566971, g_loss: 8.25045967\n",
      "Step: [1942] d_loss: 15.90988922, g_loss: 8.04452133\n",
      "Step: [1943] d_loss: 17.07799911, g_loss: 8.58891106\n",
      "Step: [1944] d_loss: 16.03550339, g_loss: 8.26679611\n",
      "Step: [1945] d_loss: 15.35374451, g_loss: 8.12235260\n",
      "Step: [1946] d_loss: 16.42720413, g_loss: 7.75863552\n",
      "Step: [1947] d_loss: 15.56974030, g_loss: 8.03523731\n",
      "Step: [1948] d_loss: 15.88308525, g_loss: 7.98471260\n",
      "Step: [1949] d_loss: 16.20928192, g_loss: 7.28528309\n",
      "Step: [1950] d_loss: 15.67348099, g_loss: 7.69729233\n",
      "Step: [1951] d_loss: 15.04677391, g_loss: 8.13323307\n",
      "Step: [1952] d_loss: 15.71417809, g_loss: 7.52090454\n",
      "Step: [1953] d_loss: 15.56176567, g_loss: 7.64245844\n",
      "Step: [1954] d_loss: 15.64742088, g_loss: 7.82747746\n",
      "Step: [1955] d_loss: 15.91390991, g_loss: 7.88324451\n",
      "Step: [1956] d_loss: 14.94732380, g_loss: 7.35467625\n",
      "Step: [1957] d_loss: 14.56961441, g_loss: 7.83370686\n",
      "Step: [1958] d_loss: 15.43863392, g_loss: 7.37538719\n",
      "Step: [1959] d_loss: 16.26421356, g_loss: 7.25924683\n",
      "Step: [1960] d_loss: 14.94700432, g_loss: 7.98317242\n",
      "Step: [1961] d_loss: 15.84507942, g_loss: 7.97439051\n",
      "Step: [1962] d_loss: 15.76894760, g_loss: 7.92701912\n",
      "Step: [1963] d_loss: 14.66490746, g_loss: 7.39982510\n",
      "Step: [1964] d_loss: 15.46181679, g_loss: 7.25517130\n",
      "Step: [1965] d_loss: 14.53413391, g_loss: 8.30982304\n",
      "Step: [1966] d_loss: 15.58125114, g_loss: 7.59281254\n",
      "Step: [1967] d_loss: 14.28791237, g_loss: 8.28371811\n",
      "Step: [1968] d_loss: 15.20218372, g_loss: 7.63532019\n",
      "Step: [1969] d_loss: 15.06542492, g_loss: 7.94355726\n",
      "Step: [1970] d_loss: 15.28279018, g_loss: 7.84790039\n",
      "Step: [1971] d_loss: 15.43312359, g_loss: 7.83853722\n",
      "Step: [1972] d_loss: 14.97959518, g_loss: 8.22326088\n",
      "Step: [1973] d_loss: 15.88110542, g_loss: 7.66403294\n",
      "Step: [1974] d_loss: 15.53555775, g_loss: 7.77415895\n",
      "Step: [1975] d_loss: 15.36937237, g_loss: 7.71682882\n",
      "Step: [1976] d_loss: 14.31445122, g_loss: 8.11555195\n",
      "Step: [1977] d_loss: 14.98367119, g_loss: 8.07475471\n",
      "Step: [1978] d_loss: 15.60943413, g_loss: 7.51280689\n",
      "Step: [1979] d_loss: 14.67968559, g_loss: 7.71670341\n",
      "Step: [1980] d_loss: 15.11171913, g_loss: 8.01516914\n",
      "Step: [1981] d_loss: 15.14518547, g_loss: 7.95018005\n",
      "Step: [1982] d_loss: 14.98718929, g_loss: 8.52692509\n",
      "Step: [1983] d_loss: 15.67259312, g_loss: 7.75114441\n",
      "Step: [1984] d_loss: 14.60727501, g_loss: 7.72525024\n",
      "Step: [1985] d_loss: 15.18157768, g_loss: 7.78488636\n",
      "Step: [1986] d_loss: 15.63152218, g_loss: 8.01854897\n",
      "Step: [1987] d_loss: 15.66274929, g_loss: 7.63046074\n",
      "Step: [1988] d_loss: 15.01675034, g_loss: 8.08033943\n",
      "Step: [1989] d_loss: 14.54364586, g_loss: 8.00524426\n",
      "Step: [1990] d_loss: 15.51689529, g_loss: 7.85002089\n",
      "Step: [1991] d_loss: 15.38302422, g_loss: 7.71533680\n",
      "Step: [1992] d_loss: 14.97963524, g_loss: 8.03986740\n",
      "Step: [1993] d_loss: 15.01207256, g_loss: 8.29336071\n",
      "Step: [1994] d_loss: 15.13955116, g_loss: 7.72289085\n",
      "Step: [1995] d_loss: 14.95056725, g_loss: 8.36405563\n",
      "Step: [1996] d_loss: 15.55088234, g_loss: 7.89587736\n",
      "Step: [1997] d_loss: 16.15908813, g_loss: 7.90555668\n",
      "Step: [1998] d_loss: 15.09702110, g_loss: 8.08486462\n",
      "Step: [1999] d_loss: 15.12281036, g_loss: 8.26673317\n",
      "Step: [2000] d_loss: 14.58467674, g_loss: 7.83075619\n",
      "Step: [2001] d_loss: 14.95239639, g_loss: 7.81353092\n",
      "Step: [2002] d_loss: 14.68254089, g_loss: 8.39288139\n",
      "Step: [2003] d_loss: 15.45782661, g_loss: 8.05070591\n",
      "Step: [2004] d_loss: 15.95565319, g_loss: 7.70960617\n",
      "Step: [2005] d_loss: 15.89562035, g_loss: 8.08953857\n",
      "Step: [2006] d_loss: 14.72522831, g_loss: 8.43845558\n",
      "Step: [2007] d_loss: 15.53089333, g_loss: 7.78312826\n",
      "Step: [2008] d_loss: 16.14788818, g_loss: 8.63278389\n",
      "Step: [2009] d_loss: 15.64246559, g_loss: 8.03626633\n",
      "Step: [2010] d_loss: 15.26247597, g_loss: 7.92036343\n",
      "Step: [2011] d_loss: 15.09264183, g_loss: 7.62719059\n",
      "Step: [2012] d_loss: 15.49548721, g_loss: 7.86660194\n",
      "Step: [2013] d_loss: 16.27732468, g_loss: 8.38153267\n",
      "Step: [2014] d_loss: 15.70667267, g_loss: 7.81256771\n",
      "Step: [2015] d_loss: 15.23797321, g_loss: 8.12412167\n",
      "Step: [2016] d_loss: 15.74486828, g_loss: 8.02429008\n",
      "Step: [2017] d_loss: 15.74631119, g_loss: 7.99155903\n",
      "Step: [2018] d_loss: 15.81607914, g_loss: 8.13208199\n",
      "Step: [2019] d_loss: 15.68986320, g_loss: 8.30901241\n",
      "Step: [2020] d_loss: 15.09182739, g_loss: 7.65160656\n",
      "Step: [2021] d_loss: 14.81066132, g_loss: 8.21485519\n",
      "Step: [2022] d_loss: 15.62002754, g_loss: 8.00764465\n",
      "Step: [2023] d_loss: 14.88932800, g_loss: 7.79733849\n",
      "Step: [2024] d_loss: 14.21430302, g_loss: 8.18280602\n",
      "Step: [2025] d_loss: 15.00004196, g_loss: 8.25314903\n",
      "Step: [2026] d_loss: 15.32401180, g_loss: 7.95752144\n",
      "Step: [2027] d_loss: 15.16839790, g_loss: 7.78876543\n",
      "Step: [2028] d_loss: 15.38573170, g_loss: 7.55575800\n",
      "Step: [2029] d_loss: 14.45544243, g_loss: 7.82729626\n",
      "Step: [2030] d_loss: 14.65220451, g_loss: 7.79819107\n",
      "Step: [2031] d_loss: 14.98079681, g_loss: 8.44408131\n",
      "Step: [2032] d_loss: 14.19006157, g_loss: 8.12991714\n",
      "Step: [2033] d_loss: 15.37368393, g_loss: 7.85778952\n",
      "Step: [2034] d_loss: 14.81287575, g_loss: 8.22223949\n",
      "Step: [2035] d_loss: 15.18951988, g_loss: 7.91560316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2036] d_loss: 14.85531616, g_loss: 8.34386158\n",
      "Step: [2037] d_loss: 15.03792381, g_loss: 8.34383774\n",
      "Step: [2038] d_loss: 14.65545273, g_loss: 8.09888649\n",
      "Step: [2039] d_loss: 15.38886261, g_loss: 8.25722885\n",
      "Step: [2040] d_loss: 15.94555187, g_loss: 7.54864216\n",
      "Step: [2041] d_loss: 15.55087280, g_loss: 7.97334146\n",
      "Step: [2042] d_loss: 15.88017559, g_loss: 7.59903288\n",
      "Step: [2043] d_loss: 15.92850685, g_loss: 8.22709751\n",
      "Step: [2044] d_loss: 15.56138039, g_loss: 8.64789200\n",
      "Step: [2045] d_loss: 16.84730911, g_loss: 7.88805628\n",
      "Step: [2046] d_loss: 15.74591827, g_loss: 7.82005739\n",
      "Step: [2047] d_loss: 15.33262062, g_loss: 8.00770092\n",
      "Step: [2048] d_loss: 15.19549656, g_loss: 7.96596909\n",
      "Step: [2049] d_loss: 15.54913807, g_loss: 7.74484491\n",
      "Step: [2050] d_loss: 15.59998798, g_loss: 7.63143158\n",
      "Step: [2051] d_loss: 16.38208389, g_loss: 7.63652086\n",
      "Step: [2052] d_loss: 15.49947834, g_loss: 8.10855675\n",
      "Step: [2053] d_loss: 16.01969147, g_loss: 7.84871483\n",
      "Step: [2054] d_loss: 16.37772560, g_loss: 7.90319252\n",
      "Step: [2055] d_loss: 15.73782349, g_loss: 7.99297047\n",
      "Step: [2056] d_loss: 15.73808098, g_loss: 8.18541241\n",
      "Step: [2057] d_loss: 15.92699432, g_loss: 8.02710056\n",
      "Step: [2058] d_loss: 15.46102810, g_loss: 8.28541756\n",
      "Step: [2059] d_loss: 15.39894962, g_loss: 7.72389603\n",
      "Step: [2060] d_loss: 14.81189728, g_loss: 8.24722195\n",
      "Step: [2061] d_loss: 15.01981926, g_loss: 8.44491005\n",
      "Step: [2062] d_loss: 14.72477722, g_loss: 7.62179661\n",
      "Step: [2063] d_loss: 15.18939972, g_loss: 7.74006271\n",
      "Step: [2064] d_loss: 15.03114128, g_loss: 8.03284073\n",
      "Step: [2065] d_loss: 16.17642021, g_loss: 7.26719189\n",
      "Step: [2066] d_loss: 14.77909374, g_loss: 7.98262787\n",
      "Step: [2067] d_loss: 15.11192322, g_loss: 8.25214291\n",
      "Step: [2068] d_loss: 15.88345623, g_loss: 7.75669527\n",
      "Step: [2069] d_loss: 14.45047951, g_loss: 8.04372120\n",
      "Step: [2070] d_loss: 14.83180809, g_loss: 7.86103868\n",
      "Step: [2071] d_loss: 15.42781448, g_loss: 8.01842690\n",
      "Step: [2072] d_loss: 14.98696518, g_loss: 7.69100380\n",
      "Step: [2073] d_loss: 15.83820057, g_loss: 7.79944086\n",
      "Step: [2074] d_loss: 15.07506275, g_loss: 7.94048595\n",
      "Step: [2075] d_loss: 14.85882187, g_loss: 7.86958313\n",
      "Step: [2076] d_loss: 14.57268906, g_loss: 8.28054047\n",
      "Step: [2077] d_loss: 15.34008789, g_loss: 8.21513557\n",
      "Step: [2078] d_loss: 16.30982780, g_loss: 8.12654686\n",
      "Step: [2079] d_loss: 15.83275414, g_loss: 8.28659725\n",
      "Step: [2080] d_loss: 16.70986938, g_loss: 7.41408062\n",
      "Step: [2081] d_loss: 15.29638863, g_loss: 8.00056648\n",
      "Step: [2082] d_loss: 15.72942638, g_loss: 7.76350212\n",
      "Step: [2083] d_loss: 15.09580898, g_loss: 7.97676468\n",
      "Step: [2084] d_loss: 15.49837589, g_loss: 7.89544106\n",
      "Step: [2085] d_loss: 15.27898598, g_loss: 8.18396568\n",
      "Step: [2086] d_loss: 15.16631794, g_loss: 7.84029055\n",
      "Step: [2087] d_loss: 14.98208809, g_loss: 7.79829359\n",
      "Step: [2088] d_loss: 14.50228024, g_loss: 8.35062027\n",
      "Step: [2089] d_loss: 15.57078362, g_loss: 7.96950150\n",
      "Step: [2090] d_loss: 15.06578255, g_loss: 7.76667929\n",
      "Step: [2091] d_loss: 16.07328606, g_loss: 7.67968178\n",
      "Step: [2092] d_loss: 15.20937443, g_loss: 7.78758955\n",
      "Step: [2093] d_loss: 15.40528011, g_loss: 8.03810883\n",
      "Step: [2094] d_loss: 15.43027687, g_loss: 8.29950523\n",
      "Step: [2095] d_loss: 16.19149017, g_loss: 8.53903961\n",
      "Step: [2096] d_loss: 16.16021538, g_loss: 7.81433010\n",
      "Step: [2097] d_loss: 16.19587517, g_loss: 7.99367428\n",
      "Step: [2098] d_loss: 15.83487320, g_loss: 7.98076391\n",
      "Step: [2099] d_loss: 16.43874359, g_loss: 8.13619232\n",
      "Step: [2100] d_loss: 16.40594101, g_loss: 8.09413815\n",
      "Step: [2101] d_loss: 16.75447845, g_loss: 8.18298244\n",
      "Step: [2102] d_loss: 15.06858253, g_loss: 7.81140423\n",
      "Step: [2103] d_loss: 15.35406494, g_loss: 8.15378284\n",
      "Step: [2104] d_loss: 15.14509487, g_loss: 7.74861145\n",
      "Step: [2105] d_loss: 15.22345543, g_loss: 7.84915161\n",
      "Step: [2106] d_loss: 15.23072052, g_loss: 7.89388084\n",
      "Step: [2107] d_loss: 15.68542671, g_loss: 8.68354893\n",
      "Step: [2108] d_loss: 16.92624664, g_loss: 8.86325550\n",
      "Step: [2109] d_loss: 16.99112701, g_loss: 7.81276608\n",
      "Step: [2110] d_loss: 15.29135227, g_loss: 7.75793171\n",
      "Step: [2111] d_loss: 14.56676388, g_loss: 8.58064270\n",
      "Step: [2112] d_loss: 15.56329727, g_loss: 7.89497471\n",
      "Step: [2113] d_loss: 15.02135849, g_loss: 8.03080750\n",
      "Step: [2114] d_loss: 14.73159504, g_loss: 8.81749058\n",
      "Step: [2115] d_loss: 15.61335182, g_loss: 8.50687408\n",
      "Step: [2116] d_loss: 15.87721252, g_loss: 7.47542048\n",
      "Step: [2117] d_loss: 15.23708344, g_loss: 8.00276375\n",
      "Step: [2118] d_loss: 15.33159065, g_loss: 7.90313244\n",
      "Step: [2119] d_loss: 15.03046227, g_loss: 8.39155579\n",
      "Step: [2120] d_loss: 14.71325111, g_loss: 8.23340988\n",
      "Step: [2121] d_loss: 15.32483387, g_loss: 7.60870457\n",
      "Step: [2122] d_loss: 14.32193375, g_loss: 7.82926798\n",
      "Step: [2123] d_loss: 14.78422356, g_loss: 8.35572720\n",
      "Step: [2124] d_loss: 14.39410400, g_loss: 7.97916269\n",
      "Step: [2125] d_loss: 14.92458534, g_loss: 8.39430428\n",
      "Step: [2126] d_loss: 16.11447906, g_loss: 7.83553934\n",
      "Step: [2127] d_loss: 14.75844669, g_loss: 7.35434818\n",
      "Step: [2128] d_loss: 14.87020111, g_loss: 8.02792072\n",
      "Step: [2129] d_loss: 15.18722916, g_loss: 7.64709616\n",
      "Step: [2130] d_loss: 15.02150249, g_loss: 7.33591366\n",
      "Step: [2131] d_loss: 15.59650993, g_loss: 7.94852829\n",
      "Step: [2132] d_loss: 14.84088707, g_loss: 7.88934612\n",
      "Step: [2133] d_loss: 15.10844994, g_loss: 8.33221817\n",
      "Step: [2134] d_loss: 15.28087234, g_loss: 8.57430458\n",
      "Step: [2135] d_loss: 15.43189049, g_loss: 7.98141384\n",
      "Step: [2136] d_loss: 15.17310905, g_loss: 7.96902084\n",
      "Step: [2137] d_loss: 14.88312531, g_loss: 7.95390368\n",
      "Step: [2138] d_loss: 15.06949043, g_loss: 7.91296005\n",
      "Step: [2139] d_loss: 14.44778442, g_loss: 8.33812904\n",
      "Step: [2140] d_loss: 14.89427662, g_loss: 8.36631012\n",
      "Step: [2141] d_loss: 15.29204750, g_loss: 8.60420418\n",
      "Step: [2142] d_loss: 15.42562294, g_loss: 7.70040703\n",
      "Step: [2143] d_loss: 15.48495674, g_loss: 7.67753458\n",
      "Step: [2144] d_loss: 16.00644684, g_loss: 7.67671299\n",
      "Step: [2145] d_loss: 16.01060867, g_loss: 7.82599401\n",
      "Step: [2146] d_loss: 15.41938400, g_loss: 8.24246788\n",
      "Step: [2147] d_loss: 15.43060207, g_loss: 8.09717560\n",
      "Step: [2148] d_loss: 15.16156960, g_loss: 8.12115765\n",
      "Step: [2149] d_loss: 15.57867527, g_loss: 7.65071869\n",
      "Step: [2150] d_loss: 15.27669811, g_loss: 8.57802582\n",
      "Step: [2151] d_loss: 16.04277611, g_loss: 8.28513908\n",
      "Step: [2152] d_loss: 15.06049442, g_loss: 7.90640926\n",
      "Step: [2153] d_loss: 15.28753567, g_loss: 7.88943338\n",
      "Step: [2154] d_loss: 15.39124107, g_loss: 7.80001545\n",
      "Step: [2155] d_loss: 15.64820099, g_loss: 7.71329498\n",
      "Step: [2156] d_loss: 14.85014534, g_loss: 7.72621155\n",
      "Step: [2157] d_loss: 14.70761395, g_loss: 7.90962172\n",
      "Step: [2158] d_loss: 15.15950203, g_loss: 8.25655556\n",
      "Step: [2159] d_loss: 15.96224022, g_loss: 7.82192802\n",
      "Step: [2160] d_loss: 15.04172516, g_loss: 8.14539909\n",
      "Step: [2161] d_loss: 14.94126511, g_loss: 7.95371342\n",
      "Step: [2162] d_loss: 15.62720203, g_loss: 7.70341301\n",
      "Step: [2163] d_loss: 15.82469845, g_loss: 8.02826309\n",
      "Step: [2164] d_loss: 14.92811394, g_loss: 7.69460773\n",
      "Step: [2165] d_loss: 15.45504570, g_loss: 8.20227337\n",
      "Step: [2166] d_loss: 15.23611736, g_loss: 8.07603645\n",
      "Step: [2167] d_loss: 15.55970573, g_loss: 7.88305712\n",
      "Step: [2168] d_loss: 15.43933010, g_loss: 7.93059969\n",
      "Step: [2169] d_loss: 15.54896164, g_loss: 8.44934082\n",
      "Step: [2170] d_loss: 15.45221519, g_loss: 8.19665051\n",
      "Step: [2171] d_loss: 15.71665287, g_loss: 8.22746754\n",
      "Step: [2172] d_loss: 14.96026134, g_loss: 8.12832451\n",
      "Step: [2173] d_loss: 15.52900791, g_loss: 7.91931057\n",
      "Step: [2174] d_loss: 14.52937794, g_loss: 8.37864304\n",
      "Step: [2175] d_loss: 14.56057358, g_loss: 8.04077530\n",
      "Step: [2176] d_loss: 15.07145500, g_loss: 8.12890625\n",
      "Step: [2177] d_loss: 15.80283356, g_loss: 8.06755924\n",
      "Step: [2178] d_loss: 15.49178696, g_loss: 8.55492973\n",
      "Step: [2179] d_loss: 15.33639908, g_loss: 7.93200397\n",
      "Step: [2180] d_loss: 14.69936371, g_loss: 8.65078545\n",
      "Step: [2181] d_loss: 15.28468132, g_loss: 8.01282501\n",
      "Step: [2182] d_loss: 15.30504417, g_loss: 7.78863049\n",
      "Step: [2183] d_loss: 15.27485466, g_loss: 7.76397657\n",
      "Step: [2184] d_loss: 14.70810890, g_loss: 8.35888290\n",
      "Step: [2185] d_loss: 15.37629795, g_loss: 8.95419216\n",
      "Step: [2186] d_loss: 15.41693974, g_loss: 8.87520599\n",
      "Step: [2187] d_loss: 14.92675114, g_loss: 8.22565174\n",
      "Step: [2188] d_loss: 15.79968452, g_loss: 8.45030403\n",
      "Step: [2189] d_loss: 15.82604885, g_loss: 7.54529190\n",
      "Step: [2190] d_loss: 15.56616497, g_loss: 7.27188349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2191] d_loss: 14.87674522, g_loss: 7.91376781\n",
      "Step: [2192] d_loss: 15.64074230, g_loss: 8.52983379\n",
      "Step: [2193] d_loss: 15.45529556, g_loss: 8.37429047\n",
      "Step: [2194] d_loss: 15.21260262, g_loss: 8.42254543\n",
      "Step: [2195] d_loss: 15.24238777, g_loss: 8.06071186\n",
      "Step: [2196] d_loss: 14.76803589, g_loss: 8.12552357\n",
      "Step: [2197] d_loss: 15.58000565, g_loss: 7.93642139\n",
      "Step: [2198] d_loss: 15.20410919, g_loss: 7.55523014\n",
      "Step: [2199] d_loss: 15.18702984, g_loss: 7.59761667\n",
      "Step: [2200] d_loss: 15.57156181, g_loss: 7.63700867\n",
      "Step: [2201] d_loss: 15.61693668, g_loss: 7.68896484\n",
      "Step: [2202] d_loss: 15.24501228, g_loss: 7.89447498\n",
      "Step: [2203] d_loss: 15.01640320, g_loss: 7.96379328\n",
      "Step: [2204] d_loss: 15.63821697, g_loss: 7.78960371\n",
      "Step: [2205] d_loss: 15.10505295, g_loss: 7.57382584\n",
      "Step: [2206] d_loss: 14.96488762, g_loss: 8.35742474\n",
      "Step: [2207] d_loss: 15.56313419, g_loss: 7.79426575\n",
      "Step: [2208] d_loss: 14.84514236, g_loss: 7.86815882\n",
      "Step: [2209] d_loss: 15.29930305, g_loss: 8.30701828\n",
      "Step: [2210] d_loss: 15.57213783, g_loss: 7.75581741\n",
      "Step: [2211] d_loss: 14.94301414, g_loss: 7.92395782\n",
      "Step: [2212] d_loss: 14.95009327, g_loss: 7.70144272\n",
      "Step: [2213] d_loss: 15.24007225, g_loss: 7.67290211\n",
      "Step: [2214] d_loss: 15.02014542, g_loss: 8.03331661\n",
      "Step: [2215] d_loss: 15.60532761, g_loss: 7.89378309\n",
      "Step: [2216] d_loss: 15.58820629, g_loss: 7.97425413\n",
      "Step: [2217] d_loss: 15.02848911, g_loss: 7.86451817\n",
      "Step: [2218] d_loss: 15.63268471, g_loss: 7.87860489\n",
      "Step: [2219] d_loss: 15.32399750, g_loss: 7.83411503\n",
      "Step: [2220] d_loss: 15.27361679, g_loss: 7.97824669\n",
      "Step: [2221] d_loss: 14.98293877, g_loss: 8.04830933\n",
      "Step: [2222] d_loss: 16.01184654, g_loss: 7.91168833\n",
      "Step: [2223] d_loss: 15.52723980, g_loss: 7.89625883\n",
      "Step: [2224] d_loss: 15.96100044, g_loss: 8.00240707\n",
      "Step: [2225] d_loss: 16.10884857, g_loss: 8.14441013\n",
      "Step: [2226] d_loss: 15.28321266, g_loss: 7.84439182\n",
      "Step: [2227] d_loss: 15.15843201, g_loss: 8.55311394\n",
      "Step: [2228] d_loss: 16.13718796, g_loss: 8.57697487\n",
      "Step: [2229] d_loss: 15.15100765, g_loss: 7.96264076\n",
      "Step: [2230] d_loss: 15.54650497, g_loss: 8.16144753\n",
      "Step: [2231] d_loss: 15.28073788, g_loss: 8.10947037\n",
      "Step: [2232] d_loss: 14.54673958, g_loss: 8.04681778\n",
      "Step: [2233] d_loss: 15.28048325, g_loss: 8.05737972\n",
      "Step: [2234] d_loss: 14.86328602, g_loss: 8.12407112\n",
      "Step: [2235] d_loss: 14.84802055, g_loss: 8.05432129\n",
      "Step: [2236] d_loss: 14.98112583, g_loss: 8.29180431\n",
      "Step: [2237] d_loss: 14.74525070, g_loss: 7.69812393\n",
      "Step: [2238] d_loss: 15.66222382, g_loss: 8.15508652\n",
      "Step: [2239] d_loss: 15.62503242, g_loss: 8.21178722\n",
      "Step: [2240] d_loss: 14.36501312, g_loss: 8.46211052\n",
      "Step: [2241] d_loss: 14.91510677, g_loss: 8.46902370\n",
      "Step: [2242] d_loss: 15.82450008, g_loss: 7.86016417\n",
      "Step: [2243] d_loss: 15.39944744, g_loss: 7.82872105\n",
      "Step: [2244] d_loss: 15.30638218, g_loss: 7.66806602\n",
      "Step: [2245] d_loss: 15.81807804, g_loss: 8.53698730\n",
      "Step: [2246] d_loss: 16.20745850, g_loss: 8.56953812\n",
      "Step: [2247] d_loss: 16.26290321, g_loss: 7.99080992\n",
      "Step: [2248] d_loss: 15.91342831, g_loss: 8.50214863\n",
      "Step: [2249] d_loss: 15.68790531, g_loss: 8.47948265\n",
      "Step: [2250] d_loss: 15.40805817, g_loss: 8.45960045\n",
      "Step: [2251] d_loss: 15.55296707, g_loss: 8.31939220\n",
      "Step: [2252] d_loss: 15.47197628, g_loss: 7.68264961\n",
      "Step: [2253] d_loss: 14.64220810, g_loss: 8.54721642\n",
      "Step: [2254] d_loss: 15.28842163, g_loss: 8.04338837\n",
      "Step: [2255] d_loss: 15.97272205, g_loss: 7.89583015\n",
      "Step: [2256] d_loss: 15.29830170, g_loss: 8.36719704\n",
      "Step: [2257] d_loss: 15.89983654, g_loss: 8.59108353\n",
      "Step: [2258] d_loss: 15.63910103, g_loss: 8.47063065\n",
      "Step: [2259] d_loss: 15.97649193, g_loss: 7.92183113\n",
      "Step: [2260] d_loss: 15.44277382, g_loss: 7.78505039\n",
      "Step: [2261] d_loss: 15.40306473, g_loss: 7.85777569\n",
      "Step: [2262] d_loss: 15.07656670, g_loss: 8.32391739\n",
      "Step: [2263] d_loss: 15.54568863, g_loss: 8.41094780\n",
      "Step: [2264] d_loss: 16.01858711, g_loss: 8.46145821\n",
      "Step: [2265] d_loss: 16.74341583, g_loss: 8.23270035\n",
      "Step: [2266] d_loss: 16.20976448, g_loss: 8.76827908\n",
      "Step: [2267] d_loss: 16.76432800, g_loss: 9.10516834\n",
      "Step: [2268] d_loss: 16.55450058, g_loss: 8.25782967\n",
      "Step: [2269] d_loss: 15.89687443, g_loss: 7.93241215\n",
      "Step: [2270] d_loss: 15.39023876, g_loss: 7.97389698\n",
      "Step: [2271] d_loss: 15.25492954, g_loss: 8.02202415\n",
      "Step: [2272] d_loss: 14.78782272, g_loss: 7.99244642\n",
      "Step: [2273] d_loss: 15.58060551, g_loss: 7.97826719\n",
      "Step: [2274] d_loss: 15.64241028, g_loss: 7.80897188\n",
      "Step: [2275] d_loss: 15.71161175, g_loss: 8.32061386\n",
      "Step: [2276] d_loss: 15.31217575, g_loss: 7.96615219\n",
      "Step: [2277] d_loss: 15.89738274, g_loss: 7.61288404\n",
      "Step: [2278] d_loss: 15.07824516, g_loss: 7.75818872\n",
      "Step: [2279] d_loss: 15.01895428, g_loss: 7.81508255\n",
      "Step: [2280] d_loss: 15.16160774, g_loss: 8.46483803\n",
      "Step: [2281] d_loss: 15.18604279, g_loss: 8.14770126\n",
      "Step: [2282] d_loss: 15.33280563, g_loss: 7.95066786\n",
      "Step: [2283] d_loss: 15.10063934, g_loss: 8.75003815\n",
      "Step: [2284] d_loss: 15.33831692, g_loss: 8.21300507\n",
      "Step: [2285] d_loss: 15.51364517, g_loss: 7.85086250\n",
      "Step: [2286] d_loss: 14.46989155, g_loss: 8.08438396\n",
      "Step: [2287] d_loss: 15.88989258, g_loss: 7.76345921\n",
      "Step: [2288] d_loss: 15.53499126, g_loss: 8.33275890\n",
      "Step: [2289] d_loss: 15.42559052, g_loss: 7.47012329\n",
      "Step: [2290] d_loss: 16.09601212, g_loss: 7.85062981\n",
      "Step: [2291] d_loss: 15.53526688, g_loss: 7.51077986\n",
      "Step: [2292] d_loss: 15.47298145, g_loss: 7.90234566\n",
      "Step: [2293] d_loss: 15.16743851, g_loss: 8.25141811\n",
      "Step: [2294] d_loss: 14.93738651, g_loss: 8.16096592\n",
      "Step: [2295] d_loss: 14.80789375, g_loss: 8.01449394\n",
      "Step: [2296] d_loss: 15.03318405, g_loss: 8.32999611\n",
      "Step: [2297] d_loss: 14.43078709, g_loss: 7.88338995\n",
      "Step: [2298] d_loss: 15.48487854, g_loss: 7.27363682\n",
      "Step: [2299] d_loss: 15.14291477, g_loss: 7.83082199\n",
      "Step: [2300] d_loss: 14.74956512, g_loss: 8.05703831\n",
      "Step: [2301] d_loss: 15.78627777, g_loss: 8.30004597\n",
      "Step: [2302] d_loss: 14.91334820, g_loss: 8.18405914\n",
      "Step: [2303] d_loss: 15.58269310, g_loss: 7.49958420\n",
      "Step: [2304] d_loss: 15.42873192, g_loss: 7.82150793\n",
      "Step: [2305] d_loss: 14.78138924, g_loss: 7.76532316\n",
      "Step: [2306] d_loss: 15.83201694, g_loss: 7.85199261\n",
      "Step: [2307] d_loss: 16.02947807, g_loss: 7.54435921\n",
      "Step: [2308] d_loss: 15.01316166, g_loss: 7.87771702\n",
      "Step: [2309] d_loss: 14.77411652, g_loss: 7.94004202\n",
      "Step: [2310] d_loss: 15.12965202, g_loss: 7.95745754\n",
      "Step: [2311] d_loss: 15.05665684, g_loss: 8.19073200\n",
      "Step: [2312] d_loss: 14.78605461, g_loss: 8.18434048\n",
      "Step: [2313] d_loss: 14.68110466, g_loss: 8.51921940\n",
      "Step: [2314] d_loss: 15.02302742, g_loss: 8.24297523\n",
      "Step: [2315] d_loss: 15.27033997, g_loss: 8.64133930\n",
      "Step: [2316] d_loss: 14.94873810, g_loss: 7.92242432\n",
      "Step: [2317] d_loss: 15.35266876, g_loss: 8.31995201\n",
      "Step: [2318] d_loss: 15.50489902, g_loss: 8.24215221\n",
      "Step: [2319] d_loss: 15.99251938, g_loss: 7.95137119\n",
      "Step: [2320] d_loss: 16.23977661, g_loss: 7.58017254\n",
      "Step: [2321] d_loss: 15.25844574, g_loss: 8.31809425\n",
      "Step: [2322] d_loss: 16.49171448, g_loss: 8.25959682\n",
      "Step: [2323] d_loss: 14.40767002, g_loss: 8.52363205\n",
      "Step: [2324] d_loss: 15.80758667, g_loss: 7.94621897\n",
      "Step: [2325] d_loss: 14.89123821, g_loss: 8.32097626\n",
      "Step: [2326] d_loss: 15.59113884, g_loss: 8.09555054\n",
      "Step: [2327] d_loss: 15.61386395, g_loss: 8.34115124\n",
      "Step: [2328] d_loss: 15.51852989, g_loss: 8.43794823\n",
      "Step: [2329] d_loss: 16.08264160, g_loss: 8.33095169\n",
      "Step: [2330] d_loss: 16.28666496, g_loss: 8.17416954\n",
      "Step: [2331] d_loss: 16.61462021, g_loss: 8.15640640\n",
      "Step: [2332] d_loss: 16.04345703, g_loss: 8.47559166\n",
      "Step: [2333] d_loss: 16.19667816, g_loss: 8.87310219\n",
      "Step: [2334] d_loss: 15.69757938, g_loss: 7.78577662\n",
      "Step: [2335] d_loss: 14.50983238, g_loss: 8.45509911\n",
      "Step: [2336] d_loss: 15.98611259, g_loss: 8.22314835\n",
      "Step: [2337] d_loss: 15.11920357, g_loss: 8.74964046\n",
      "Step: [2338] d_loss: 15.37040710, g_loss: 7.92389631\n",
      "Step: [2339] d_loss: 14.99604034, g_loss: 7.43531036\n",
      "Step: [2340] d_loss: 15.58325291, g_loss: 7.39029217\n",
      "Step: [2341] d_loss: 15.52868652, g_loss: 7.61185503\n",
      "Step: [2342] d_loss: 15.66283321, g_loss: 7.72520828\n",
      "Step: [2343] d_loss: 15.60348129, g_loss: 8.27726555\n",
      "Step: [2344] d_loss: 14.71777916, g_loss: 8.09308052\n",
      "Step: [2345] d_loss: 14.96163368, g_loss: 7.75913143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2346] d_loss: 16.10128593, g_loss: 7.89258337\n",
      "Step: [2347] d_loss: 15.37684727, g_loss: 8.42745876\n",
      "Step: [2348] d_loss: 15.22729015, g_loss: 7.83587694\n",
      "Step: [2349] d_loss: 15.15113926, g_loss: 8.29580021\n",
      "Step: [2350] d_loss: 15.24947453, g_loss: 7.87345886\n",
      "Step: [2351] d_loss: 15.32589149, g_loss: 7.95778513\n",
      "Step: [2352] d_loss: 15.05513763, g_loss: 8.30983829\n",
      "Step: [2353] d_loss: 15.21572876, g_loss: 8.41109467\n",
      "Step: [2354] d_loss: 14.77219772, g_loss: 7.93195724\n",
      "Step: [2355] d_loss: 15.94580269, g_loss: 7.78577566\n",
      "Step: [2356] d_loss: 15.49448490, g_loss: 8.22999191\n",
      "Step: [2357] d_loss: 15.69819736, g_loss: 8.83604527\n",
      "Step: [2358] d_loss: 15.29632568, g_loss: 8.37283802\n",
      "Step: [2359] d_loss: 15.64974213, g_loss: 8.20198059\n",
      "Step: [2360] d_loss: 15.82481766, g_loss: 7.72421455\n",
      "Step: [2361] d_loss: 15.82917976, g_loss: 8.14729786\n",
      "Step: [2362] d_loss: 15.96516037, g_loss: 8.28107071\n",
      "Step: [2363] d_loss: 15.76910496, g_loss: 8.48094177\n",
      "Step: [2364] d_loss: 14.59800053, g_loss: 8.25985813\n",
      "Step: [2365] d_loss: 14.83094692, g_loss: 7.89338064\n",
      "Step: [2366] d_loss: 15.16566658, g_loss: 8.21444321\n",
      "Step: [2367] d_loss: 14.92407227, g_loss: 8.40315247\n",
      "Step: [2368] d_loss: 15.06027985, g_loss: 9.03632355\n",
      "Step: [2369] d_loss: 15.46028805, g_loss: 8.44574261\n",
      "Step: [2370] d_loss: 16.19810677, g_loss: 8.25377655\n",
      "Step: [2371] d_loss: 14.99104500, g_loss: 8.77100754\n",
      "Step: [2372] d_loss: 15.29316330, g_loss: 8.49874592\n",
      "Step: [2373] d_loss: 15.61649132, g_loss: 8.02966499\n",
      "Step: [2374] d_loss: 15.30395031, g_loss: 7.92004871\n",
      "Step: [2375] d_loss: 15.76274395, g_loss: 8.43964958\n",
      "Step: [2376] d_loss: 15.11203575, g_loss: 8.09431076\n",
      "Step: [2377] d_loss: 15.57764244, g_loss: 7.92383194\n",
      "Step: [2378] d_loss: 15.96212769, g_loss: 8.05382824\n",
      "Step: [2379] d_loss: 15.88459587, g_loss: 8.36358833\n",
      "Step: [2380] d_loss: 15.20137501, g_loss: 8.16328430\n",
      "Step: [2381] d_loss: 16.00473022, g_loss: 8.09448528\n",
      "Step: [2382] d_loss: 15.86262131, g_loss: 7.55368614\n",
      "Step: [2383] d_loss: 15.06863594, g_loss: 8.17010307\n",
      "Step: [2384] d_loss: 15.40139198, g_loss: 7.76774836\n",
      "Step: [2385] d_loss: 14.87098503, g_loss: 8.30830288\n",
      "Step: [2386] d_loss: 15.83287621, g_loss: 8.54531860\n",
      "Step: [2387] d_loss: 15.47740364, g_loss: 7.61512184\n",
      "Step: [2388] d_loss: 15.60014439, g_loss: 7.77086020\n",
      "Step: [2389] d_loss: 15.45303726, g_loss: 7.63222980\n",
      "Step: [2390] d_loss: 15.14647770, g_loss: 7.54272747\n",
      "Step: [2391] d_loss: 15.79453659, g_loss: 8.31545830\n",
      "Step: [2392] d_loss: 15.36365604, g_loss: 8.47140503\n",
      "Step: [2393] d_loss: 14.74935341, g_loss: 8.07835960\n",
      "Step: [2394] d_loss: 15.16566849, g_loss: 7.79751539\n",
      "Step: [2395] d_loss: 15.68599987, g_loss: 7.66203547\n",
      "Step: [2396] d_loss: 15.35878181, g_loss: 7.74774647\n",
      "Step: [2397] d_loss: 15.40862846, g_loss: 8.03938484\n",
      "Step: [2398] d_loss: 15.94696903, g_loss: 7.39638329\n",
      "Step: [2399] d_loss: 15.20571423, g_loss: 7.98425961\n",
      "Step: [2400] d_loss: 15.92716122, g_loss: 7.97112656\n",
      "Step: [2401] d_loss: 15.42880344, g_loss: 8.21967316\n",
      "Step: [2402] d_loss: 15.51419640, g_loss: 8.12856579\n",
      "Step: [2403] d_loss: 15.45860863, g_loss: 8.39724255\n",
      "Step: [2404] d_loss: 14.79306030, g_loss: 8.64447117\n",
      "Step: [2405] d_loss: 15.20126915, g_loss: 8.02610207\n",
      "Step: [2406] d_loss: 15.27300453, g_loss: 7.64760160\n",
      "Step: [2407] d_loss: 15.63294506, g_loss: 8.32345581\n",
      "Step: [2408] d_loss: 15.81159782, g_loss: 7.65654421\n",
      "Step: [2409] d_loss: 16.44123459, g_loss: 7.90560818\n",
      "Step: [2410] d_loss: 15.21655750, g_loss: 7.85842609\n",
      "Step: [2411] d_loss: 15.28626060, g_loss: 8.14790726\n",
      "Step: [2412] d_loss: 15.19652367, g_loss: 8.24312496\n",
      "Step: [2413] d_loss: 15.23848248, g_loss: 7.84323597\n",
      "Step: [2414] d_loss: 15.66062546, g_loss: 8.31878281\n",
      "Step: [2415] d_loss: 14.88305473, g_loss: 8.01038933\n",
      "Step: [2416] d_loss: 15.56035233, g_loss: 7.53231955\n",
      "Step: [2417] d_loss: 15.35058022, g_loss: 7.91417789\n",
      "Step: [2418] d_loss: 14.72986794, g_loss: 8.19393635\n",
      "Step: [2419] d_loss: 14.40006351, g_loss: 8.65484524\n",
      "Step: [2420] d_loss: 15.29351044, g_loss: 7.75416756\n",
      "Step: [2421] d_loss: 14.80234528, g_loss: 8.34558868\n",
      "Step: [2422] d_loss: 14.79176331, g_loss: 7.69942474\n",
      "Step: [2423] d_loss: 15.66341114, g_loss: 7.47574711\n",
      "Step: [2424] d_loss: 15.03804779, g_loss: 8.23314095\n",
      "Step: [2425] d_loss: 15.44070721, g_loss: 7.71806002\n",
      "Step: [2426] d_loss: 15.83854961, g_loss: 8.11152172\n",
      "Step: [2427] d_loss: 16.36584663, g_loss: 8.25563240\n",
      "Step: [2428] d_loss: 16.36535835, g_loss: 8.09900951\n",
      "Step: [2429] d_loss: 15.44952774, g_loss: 7.89703178\n",
      "Step: [2430] d_loss: 15.98041534, g_loss: 7.67739868\n",
      "Step: [2431] d_loss: 14.71555710, g_loss: 7.84984875\n",
      "Step: [2432] d_loss: 15.30659485, g_loss: 7.78242207\n",
      "Step: [2433] d_loss: 15.68225861, g_loss: 7.66295338\n",
      "Step: [2434] d_loss: 15.25454903, g_loss: 7.93920660\n",
      "Step: [2435] d_loss: 14.89134312, g_loss: 7.97951126\n",
      "Step: [2436] d_loss: 15.93718243, g_loss: 8.22193909\n",
      "Step: [2437] d_loss: 16.05171394, g_loss: 8.05679417\n",
      "Step: [2438] d_loss: 14.91734123, g_loss: 7.94590664\n",
      "Step: [2439] d_loss: 15.15861797, g_loss: 8.06782055\n",
      "Step: [2440] d_loss: 15.08761978, g_loss: 8.55689240\n",
      "Step: [2441] d_loss: 14.78568649, g_loss: 8.54266834\n",
      "Step: [2442] d_loss: 15.39176178, g_loss: 7.93240833\n",
      "Step: [2443] d_loss: 14.33227158, g_loss: 8.14415932\n",
      "Step: [2444] d_loss: 14.85284233, g_loss: 7.93717098\n",
      "Step: [2445] d_loss: 15.37168503, g_loss: 8.54493713\n",
      "Step: [2446] d_loss: 15.37843037, g_loss: 8.17717361\n",
      "Step: [2447] d_loss: 14.72074890, g_loss: 8.01255226\n",
      "Step: [2448] d_loss: 15.67784882, g_loss: 7.87173176\n",
      "Step: [2449] d_loss: 14.90013313, g_loss: 8.69038200\n",
      "Step: [2450] d_loss: 15.74416733, g_loss: 7.87856579\n",
      "Step: [2451] d_loss: 15.60337639, g_loss: 8.40404606\n",
      "Step: [2452] d_loss: 15.60266113, g_loss: 7.64804745\n",
      "Step: [2453] d_loss: 15.09473991, g_loss: 7.90424061\n",
      "Step: [2454] d_loss: 15.06561184, g_loss: 8.01164532\n",
      "Step: [2455] d_loss: 15.81228256, g_loss: 8.24896145\n",
      "Step: [2456] d_loss: 15.41594124, g_loss: 7.79723930\n",
      "Step: [2457] d_loss: 15.63099480, g_loss: 7.66487408\n",
      "Step: [2458] d_loss: 14.87211418, g_loss: 8.08425140\n",
      "Step: [2459] d_loss: 15.78576946, g_loss: 7.85724592\n",
      "Step: [2460] d_loss: 15.49474525, g_loss: 7.97628307\n",
      "Step: [2461] d_loss: 15.32210159, g_loss: 7.85191727\n",
      "Step: [2462] d_loss: 15.11329365, g_loss: 8.56238079\n",
      "Step: [2463] d_loss: 15.74243164, g_loss: 8.46634483\n",
      "Step: [2464] d_loss: 15.49282360, g_loss: 8.14959240\n",
      "Step: [2465] d_loss: 16.36909103, g_loss: 7.88502598\n",
      "Step: [2466] d_loss: 15.45571804, g_loss: 8.04708958\n",
      "Step: [2467] d_loss: 15.41205788, g_loss: 8.12439442\n",
      "Step: [2468] d_loss: 15.66556072, g_loss: 8.15753937\n",
      "Step: [2469] d_loss: 15.59543419, g_loss: 7.55113173\n",
      "Step: [2470] d_loss: 15.06687546, g_loss: 8.18611336\n",
      "Step: [2471] d_loss: 15.35389805, g_loss: 8.07128048\n",
      "Step: [2472] d_loss: 15.69801235, g_loss: 8.30297852\n",
      "Step: [2473] d_loss: 16.51832199, g_loss: 8.09267712\n",
      "Step: [2474] d_loss: 15.51007843, g_loss: 8.31520462\n",
      "Step: [2475] d_loss: 14.84970188, g_loss: 8.17748642\n",
      "Step: [2476] d_loss: 15.16133499, g_loss: 7.99830151\n",
      "Step: [2477] d_loss: 14.95572567, g_loss: 8.25164223\n",
      "Step: [2478] d_loss: 14.77899933, g_loss: 8.04738426\n",
      "Step: [2479] d_loss: 15.19910622, g_loss: 8.16250801\n",
      "Step: [2480] d_loss: 15.86721516, g_loss: 7.84453726\n",
      "Step: [2481] d_loss: 15.42894173, g_loss: 7.78098869\n",
      "Step: [2482] d_loss: 15.47082710, g_loss: 7.79673147\n",
      "Step: [2483] d_loss: 15.72109413, g_loss: 7.96880817\n",
      "Step: [2484] d_loss: 15.22819519, g_loss: 7.98457861\n",
      "Step: [2485] d_loss: 14.64100266, g_loss: 8.36717892\n",
      "Step: [2486] d_loss: 15.91428947, g_loss: 7.66329575\n",
      "Step: [2487] d_loss: 15.43628120, g_loss: 7.82792807\n",
      "Step: [2488] d_loss: 15.18434620, g_loss: 8.02670479\n",
      "Step: [2489] d_loss: 15.01297951, g_loss: 8.10820103\n",
      "Step: [2490] d_loss: 15.17078400, g_loss: 8.06346512\n",
      "Step: [2491] d_loss: 15.49929047, g_loss: 8.68627167\n",
      "Step: [2492] d_loss: 15.32226372, g_loss: 8.95622063\n",
      "Step: [2493] d_loss: 15.96212578, g_loss: 8.22416878\n",
      "Step: [2494] d_loss: 15.01575947, g_loss: 7.98129272\n",
      "Step: [2495] d_loss: 15.51727295, g_loss: 7.63111591\n",
      "Step: [2496] d_loss: 15.75449944, g_loss: 7.98636913\n",
      "Step: [2497] d_loss: 16.01284790, g_loss: 7.66236591\n",
      "Step: [2498] d_loss: 15.14953709, g_loss: 8.41411781\n",
      "Step: [2499] d_loss: 15.12999153, g_loss: 7.93920803\n",
      "Step: [2500] d_loss: 15.58232307, g_loss: 7.75271273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2501] d_loss: 15.59983063, g_loss: 8.16987038\n",
      "Step: [2502] d_loss: 15.68320179, g_loss: 8.12191391\n",
      "Step: [2503] d_loss: 15.47201538, g_loss: 8.48198318\n",
      "Step: [2504] d_loss: 15.46002960, g_loss: 8.00875282\n",
      "Step: [2505] d_loss: 15.08801651, g_loss: 8.25970364\n",
      "Step: [2506] d_loss: 14.83263016, g_loss: 8.60994720\n",
      "Step: [2507] d_loss: 15.81359291, g_loss: 7.69553852\n",
      "Step: [2508] d_loss: 15.35142708, g_loss: 7.64287424\n",
      "Step: [2509] d_loss: 15.35435200, g_loss: 7.72716427\n",
      "Step: [2510] d_loss: 14.87646770, g_loss: 7.68570805\n",
      "Step: [2511] d_loss: 15.03800011, g_loss: 7.92337608\n",
      "Step: [2512] d_loss: 15.66771317, g_loss: 8.68724442\n",
      "Step: [2513] d_loss: 15.88806725, g_loss: 7.51582909\n",
      "Step: [2514] d_loss: 15.52538681, g_loss: 7.74899197\n",
      "Step: [2515] d_loss: 14.94167805, g_loss: 7.82475185\n",
      "Step: [2516] d_loss: 15.55081081, g_loss: 7.75032902\n",
      "Step: [2517] d_loss: 14.60000801, g_loss: 8.00040436\n",
      "Step: [2518] d_loss: 15.20570755, g_loss: 8.10285664\n",
      "Step: [2519] d_loss: 15.46633911, g_loss: 7.94330597\n",
      "Step: [2520] d_loss: 15.42244339, g_loss: 7.50569534\n",
      "Step: [2521] d_loss: 15.54479980, g_loss: 7.84860516\n",
      "Step: [2522] d_loss: 15.08982277, g_loss: 8.16679859\n",
      "Step: [2523] d_loss: 15.77200127, g_loss: 7.99818611\n",
      "Step: [2524] d_loss: 15.74921799, g_loss: 7.82166862\n",
      "Step: [2525] d_loss: 14.81888580, g_loss: 8.32890034\n",
      "Step: [2526] d_loss: 15.83690453, g_loss: 8.79973793\n",
      "Step: [2527] d_loss: 15.19329262, g_loss: 8.21660614\n",
      "Step: [2528] d_loss: 14.73588562, g_loss: 7.54033375\n",
      "Step: [2529] d_loss: 14.92937660, g_loss: 8.04549408\n",
      "Step: [2530] d_loss: 15.07854557, g_loss: 8.14640617\n",
      "Step: [2531] d_loss: 14.51275635, g_loss: 8.48122883\n",
      "Step: [2532] d_loss: 15.08320999, g_loss: 7.76218271\n",
      "Step: [2533] d_loss: 15.32837772, g_loss: 8.04580879\n",
      "Step: [2534] d_loss: 15.59352875, g_loss: 8.15156555\n",
      "Step: [2535] d_loss: 15.15003395, g_loss: 8.09582138\n",
      "Step: [2536] d_loss: 15.59659100, g_loss: 8.25940895\n",
      "Step: [2537] d_loss: 15.53080940, g_loss: 8.01339722\n",
      "Step: [2538] d_loss: 15.59556770, g_loss: 8.41833496\n",
      "Step: [2539] d_loss: 15.10239220, g_loss: 7.67449427\n",
      "Step: [2540] d_loss: 15.08829880, g_loss: 8.00174809\n",
      "Step: [2541] d_loss: 14.81043911, g_loss: 8.40905380\n",
      "Step: [2542] d_loss: 14.29925156, g_loss: 8.31669617\n",
      "Step: [2543] d_loss: 15.42169762, g_loss: 7.75661564\n",
      "Step: [2544] d_loss: 14.63090515, g_loss: 8.11382008\n",
      "Step: [2545] d_loss: 15.04542637, g_loss: 8.42434406\n",
      "Step: [2546] d_loss: 15.21726513, g_loss: 7.96154785\n",
      "Step: [2547] d_loss: 14.64269638, g_loss: 8.29493904\n",
      "Step: [2548] d_loss: 14.97191143, g_loss: 7.91271591\n",
      "Step: [2549] d_loss: 15.33411980, g_loss: 7.79749632\n",
      "Step: [2550] d_loss: 14.74640465, g_loss: 7.80676079\n",
      "Step: [2551] d_loss: 15.44132805, g_loss: 7.88635826\n",
      "Step: [2552] d_loss: 16.12220383, g_loss: 7.96681356\n",
      "Step: [2553] d_loss: 15.05965614, g_loss: 8.38887215\n",
      "Step: [2554] d_loss: 15.61437702, g_loss: 8.38041210\n",
      "Step: [2555] d_loss: 17.61347961, g_loss: 9.51661873\n",
      "Step: [2556] d_loss: 17.18035507, g_loss: 7.81611872\n",
      "Step: [2557] d_loss: 15.22203827, g_loss: 8.31036472\n",
      "Step: [2558] d_loss: 15.69846725, g_loss: 7.94300604\n",
      "Step: [2559] d_loss: 15.44368362, g_loss: 7.76785088\n",
      "Step: [2560] d_loss: 15.07308102, g_loss: 7.87986612\n",
      "Step: [2561] d_loss: 15.55476952, g_loss: 7.69171047\n",
      "Step: [2562] d_loss: 15.52997017, g_loss: 7.42066669\n",
      "Step: [2563] d_loss: 15.53092098, g_loss: 7.92860174\n",
      "Step: [2564] d_loss: 15.36679173, g_loss: 8.49271965\n",
      "Step: [2565] d_loss: 15.80318832, g_loss: 7.59874439\n",
      "Step: [2566] d_loss: 15.68454456, g_loss: 7.62673378\n",
      "Step: [2567] d_loss: 15.56089783, g_loss: 7.32152843\n",
      "Step: [2568] d_loss: 15.79053879, g_loss: 8.02641773\n",
      "Step: [2569] d_loss: 15.37317753, g_loss: 8.22921753\n",
      "Step: [2570] d_loss: 15.26854515, g_loss: 7.81065941\n",
      "Step: [2571] d_loss: 15.43717289, g_loss: 7.75020313\n",
      "Step: [2572] d_loss: 15.24541664, g_loss: 8.06158829\n",
      "Step: [2573] d_loss: 15.61504555, g_loss: 7.86018848\n",
      "Step: [2574] d_loss: 15.51537323, g_loss: 7.85769558\n",
      "Step: [2575] d_loss: 15.22547817, g_loss: 8.58972359\n",
      "Step: [2576] d_loss: 15.20237255, g_loss: 7.96210575\n",
      "Step: [2577] d_loss: 15.82658768, g_loss: 8.00146675\n",
      "Step: [2578] d_loss: 15.88942337, g_loss: 8.33889008\n",
      "Step: [2579] d_loss: 14.56851387, g_loss: 7.85438728\n",
      "Step: [2580] d_loss: 15.67099571, g_loss: 7.86929369\n",
      "Step: [2581] d_loss: 15.34686565, g_loss: 8.78865242\n",
      "Step: [2582] d_loss: 16.07457733, g_loss: 8.51085472\n",
      "Step: [2583] d_loss: 16.15203094, g_loss: 7.99992943\n",
      "Step: [2584] d_loss: 14.95724297, g_loss: 7.96514654\n",
      "Step: [2585] d_loss: 15.73685265, g_loss: 8.44267654\n",
      "Step: [2586] d_loss: 15.33799171, g_loss: 7.99597740\n",
      "Step: [2587] d_loss: 15.48957253, g_loss: 8.30809402\n",
      "Step: [2588] d_loss: 15.13509464, g_loss: 8.16628456\n",
      "Step: [2589] d_loss: 14.93511581, g_loss: 8.20364380\n",
      "Step: [2590] d_loss: 14.90575027, g_loss: 8.20318317\n",
      "Step: [2591] d_loss: 15.06428146, g_loss: 7.85708761\n",
      "Step: [2592] d_loss: 15.28221321, g_loss: 8.22039318\n",
      "Step: [2593] d_loss: 15.10966778, g_loss: 7.77642250\n",
      "Step: [2594] d_loss: 15.19517136, g_loss: 8.07380104\n",
      "Step: [2595] d_loss: 15.93592262, g_loss: 8.11170959\n",
      "Step: [2596] d_loss: 14.34406471, g_loss: 8.36216164\n",
      "Step: [2597] d_loss: 15.41776848, g_loss: 8.29581070\n",
      "Step: [2598] d_loss: 14.52816582, g_loss: 7.71803665\n",
      "Step: [2599] d_loss: 15.75389481, g_loss: 7.62127018\n",
      "Step: [2600] d_loss: 14.97710419, g_loss: 8.21666718\n",
      "Step: [2601] d_loss: 15.00994492, g_loss: 8.03614044\n",
      "Step: [2602] d_loss: 15.01520538, g_loss: 8.24930191\n",
      "Step: [2603] d_loss: 15.49104118, g_loss: 7.84506702\n",
      "Step: [2604] d_loss: 15.04644203, g_loss: 8.11018944\n",
      "Step: [2605] d_loss: 14.57439232, g_loss: 8.07735157\n",
      "Step: [2606] d_loss: 14.93871784, g_loss: 7.88622332\n",
      "Step: [2607] d_loss: 15.58250618, g_loss: 7.86337852\n",
      "Step: [2608] d_loss: 15.23946571, g_loss: 7.95084143\n",
      "Step: [2609] d_loss: 14.99832726, g_loss: 8.36395264\n",
      "Step: [2610] d_loss: 15.45576191, g_loss: 7.86547327\n",
      "Step: [2611] d_loss: 15.24248505, g_loss: 7.83822727\n",
      "Step: [2612] d_loss: 15.29993629, g_loss: 8.25159454\n",
      "Step: [2613] d_loss: 15.89716721, g_loss: 7.94310093\n",
      "Step: [2614] d_loss: 15.18488407, g_loss: 8.18762207\n",
      "Step: [2615] d_loss: 14.97245979, g_loss: 8.01544571\n",
      "Step: [2616] d_loss: 15.30537987, g_loss: 8.26922226\n",
      "Step: [2617] d_loss: 14.74687672, g_loss: 8.77044582\n",
      "Step: [2618] d_loss: 15.32570934, g_loss: 7.95183563\n",
      "Step: [2619] d_loss: 14.58980274, g_loss: 8.30731010\n",
      "Step: [2620] d_loss: 14.90822506, g_loss: 7.86318970\n",
      "Step: [2621] d_loss: 14.33044815, g_loss: 8.16101265\n",
      "Step: [2622] d_loss: 14.36180687, g_loss: 8.33040237\n",
      "Step: [2623] d_loss: 14.73221684, g_loss: 8.75521851\n",
      "Step: [2624] d_loss: 15.22598553, g_loss: 8.50185776\n",
      "Step: [2625] d_loss: 14.60668945, g_loss: 8.10304260\n",
      "Step: [2626] d_loss: 15.45331097, g_loss: 8.00000381\n",
      "Step: [2627] d_loss: 14.95571136, g_loss: 8.03797245\n",
      "Step: [2628] d_loss: 15.25341320, g_loss: 8.52014160\n",
      "Step: [2629] d_loss: 15.51052094, g_loss: 7.56345654\n",
      "Step: [2630] d_loss: 15.55821896, g_loss: 8.00698662\n",
      "Step: [2631] d_loss: 15.11549473, g_loss: 7.97276402\n",
      "Step: [2632] d_loss: 15.27203369, g_loss: 7.90941429\n",
      "Step: [2633] d_loss: 14.58162022, g_loss: 8.52283955\n",
      "Step: [2634] d_loss: 14.20675755, g_loss: 8.34320831\n",
      "Step: [2635] d_loss: 15.21579647, g_loss: 8.14917755\n",
      "Step: [2636] d_loss: 15.75068951, g_loss: 7.50438833\n",
      "Step: [2637] d_loss: 15.65377998, g_loss: 7.63672686\n",
      "Step: [2638] d_loss: 15.79398632, g_loss: 7.58916378\n",
      "Step: [2639] d_loss: 14.80762577, g_loss: 8.16288280\n",
      "Step: [2640] d_loss: 15.93894005, g_loss: 8.19825745\n",
      "Step: [2641] d_loss: 15.15634727, g_loss: 7.94647884\n",
      "Step: [2642] d_loss: 16.24087906, g_loss: 7.90695667\n",
      "Step: [2643] d_loss: 15.00002289, g_loss: 8.34476566\n",
      "Step: [2644] d_loss: 15.13431454, g_loss: 8.01454544\n",
      "Step: [2645] d_loss: 15.04434395, g_loss: 8.35727978\n",
      "Step: [2646] d_loss: 15.48263931, g_loss: 7.80500126\n",
      "Step: [2647] d_loss: 14.88870239, g_loss: 8.12778664\n",
      "Step: [2648] d_loss: 15.78407764, g_loss: 8.68201828\n",
      "Step: [2649] d_loss: 15.52033615, g_loss: 7.81075287\n",
      "Step: [2650] d_loss: 15.12442780, g_loss: 7.57403803\n",
      "Step: [2651] d_loss: 14.47701645, g_loss: 8.18816566\n",
      "Step: [2652] d_loss: 14.85086060, g_loss: 7.91051531\n",
      "Step: [2653] d_loss: 14.71196079, g_loss: 8.32160187\n",
      "Step: [2654] d_loss: 15.70514202, g_loss: 8.00945473\n",
      "Step: [2655] d_loss: 15.83554649, g_loss: 8.12190151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2656] d_loss: 15.83990669, g_loss: 8.11775970\n",
      "Step: [2657] d_loss: 16.09969711, g_loss: 8.32792187\n",
      "Step: [2658] d_loss: 15.79921436, g_loss: 8.56488705\n",
      "Step: [2659] d_loss: 16.20047379, g_loss: 8.06417084\n",
      "Step: [2660] d_loss: 14.98905182, g_loss: 8.42740250\n",
      "Step: [2661] d_loss: 15.31031990, g_loss: 8.25246620\n",
      "Step: [2662] d_loss: 14.90103722, g_loss: 9.06630707\n",
      "Step: [2663] d_loss: 15.74878407, g_loss: 8.02891541\n",
      "Step: [2664] d_loss: 15.17765999, g_loss: 8.11551285\n",
      "Step: [2665] d_loss: 15.50625992, g_loss: 8.28870201\n",
      "Step: [2666] d_loss: 15.61964035, g_loss: 8.13429260\n",
      "Step: [2667] d_loss: 13.81914139, g_loss: 8.69431305\n",
      "Step: [2668] d_loss: 14.93066883, g_loss: 8.19602394\n",
      "Step: [2669] d_loss: 15.32052422, g_loss: 8.10107231\n",
      "Step: [2670] d_loss: 15.02687836, g_loss: 8.45867825\n",
      "Step: [2671] d_loss: 15.57956314, g_loss: 7.76378727\n",
      "Step: [2672] d_loss: 15.44180393, g_loss: 8.05081749\n",
      "Step: [2673] d_loss: 15.62375546, g_loss: 8.41256618\n",
      "Step: [2674] d_loss: 14.87035942, g_loss: 8.05289078\n",
      "Step: [2675] d_loss: 15.86700821, g_loss: 7.76272583\n",
      "Step: [2676] d_loss: 15.19537544, g_loss: 7.86154175\n",
      "Step: [2677] d_loss: 15.57300377, g_loss: 8.13472462\n",
      "Step: [2678] d_loss: 15.45656586, g_loss: 8.59323215\n",
      "Step: [2679] d_loss: 15.04297638, g_loss: 8.92952156\n",
      "Step: [2680] d_loss: 16.03619003, g_loss: 8.83242607\n",
      "Step: [2681] d_loss: 15.77639580, g_loss: 8.92882442\n",
      "Step: [2682] d_loss: 15.97262287, g_loss: 7.91601753\n",
      "Step: [2683] d_loss: 14.97161770, g_loss: 8.15330505\n",
      "Step: [2684] d_loss: 15.26829147, g_loss: 8.05878162\n",
      "Step: [2685] d_loss: 15.87410641, g_loss: 7.67438507\n",
      "Step: [2686] d_loss: 14.69321060, g_loss: 8.24789619\n",
      "Step: [2687] d_loss: 15.52080631, g_loss: 7.93325567\n",
      "Step: [2688] d_loss: 15.14787102, g_loss: 7.95634937\n",
      "Step: [2689] d_loss: 15.83927727, g_loss: 8.30350494\n",
      "Step: [2690] d_loss: 16.37016678, g_loss: 7.85823679\n",
      "Step: [2691] d_loss: 15.46004868, g_loss: 7.34630775\n",
      "Step: [2692] d_loss: 14.84352112, g_loss: 8.48694801\n",
      "Step: [2693] d_loss: 14.76479912, g_loss: 8.59775543\n",
      "Step: [2694] d_loss: 15.02258396, g_loss: 7.55321169\n",
      "Step: [2695] d_loss: 15.21043873, g_loss: 8.21758461\n",
      "Step: [2696] d_loss: 15.65510559, g_loss: 8.00969887\n",
      "Step: [2697] d_loss: 15.86868858, g_loss: 7.82556725\n",
      "Step: [2698] d_loss: 15.24666405, g_loss: 8.15288448\n",
      "Step: [2699] d_loss: 14.48582745, g_loss: 8.68812084\n",
      "Step: [2700] d_loss: 15.45102501, g_loss: 7.81481409\n",
      "Step: [2701] d_loss: 14.80224609, g_loss: 7.98664761\n",
      "Step: [2702] d_loss: 15.02872658, g_loss: 7.81573200\n",
      "Step: [2703] d_loss: 16.64609337, g_loss: 8.17609024\n",
      "Step: [2704] d_loss: 15.67655659, g_loss: 8.18281746\n",
      "Step: [2705] d_loss: 15.67111969, g_loss: 7.98449898\n",
      "Step: [2706] d_loss: 15.91421318, g_loss: 8.67093754\n",
      "Step: [2707] d_loss: 15.90546608, g_loss: 8.11858845\n",
      "Step: [2708] d_loss: 15.53291607, g_loss: 8.16683578\n",
      "Step: [2709] d_loss: 16.06627464, g_loss: 8.07951927\n",
      "Step: [2710] d_loss: 15.42871952, g_loss: 7.94680691\n",
      "Step: [2711] d_loss: 15.23504543, g_loss: 7.89697933\n",
      "Step: [2712] d_loss: 15.79717922, g_loss: 8.04086304\n",
      "Step: [2713] d_loss: 15.81881523, g_loss: 7.64149523\n",
      "Step: [2714] d_loss: 15.40174484, g_loss: 8.33728790\n",
      "Step: [2715] d_loss: 14.90238190, g_loss: 8.55432510\n",
      "Step: [2716] d_loss: 14.62369537, g_loss: 8.28820133\n",
      "Step: [2717] d_loss: 15.42025852, g_loss: 8.45582962\n",
      "Step: [2718] d_loss: 15.81470680, g_loss: 8.26983070\n",
      "Step: [2719] d_loss: 15.99163628, g_loss: 8.45722198\n",
      "Step: [2720] d_loss: 15.36490822, g_loss: 8.07771397\n",
      "Step: [2721] d_loss: 14.71483803, g_loss: 8.53314400\n",
      "Step: [2722] d_loss: 14.98264503, g_loss: 8.23188972\n",
      "Step: [2723] d_loss: 15.52322006, g_loss: 8.86554337\n",
      "Step: [2724] d_loss: 15.59567738, g_loss: 8.32608318\n",
      "Step: [2725] d_loss: 15.23276615, g_loss: 8.48361301\n",
      "Step: [2726] d_loss: 14.96341705, g_loss: 7.91441393\n",
      "Step: [2727] d_loss: 15.96011829, g_loss: 7.67477512\n",
      "Step: [2728] d_loss: 15.02484512, g_loss: 8.37116241\n",
      "Step: [2729] d_loss: 14.44690228, g_loss: 8.15681648\n",
      "Step: [2730] d_loss: 14.96329308, g_loss: 9.01849556\n",
      "Step: [2731] d_loss: 14.35453987, g_loss: 8.27695370\n",
      "Step: [2732] d_loss: 14.56190205, g_loss: 8.26529694\n",
      "Step: [2733] d_loss: 14.43325996, g_loss: 8.28746033\n",
      "Step: [2734] d_loss: 15.26091003, g_loss: 7.89567757\n",
      "Step: [2735] d_loss: 15.15777969, g_loss: 8.16138554\n",
      "Step: [2736] d_loss: 14.46107197, g_loss: 8.42113876\n",
      "Step: [2737] d_loss: 15.22770882, g_loss: 8.61399555\n",
      "Step: [2738] d_loss: 15.27643585, g_loss: 7.62792683\n",
      "Step: [2739] d_loss: 15.16514206, g_loss: 7.69385624\n",
      "Step: [2740] d_loss: 15.80330276, g_loss: 8.60847759\n",
      "Step: [2741] d_loss: 15.02060127, g_loss: 8.03367424\n",
      "Step: [2742] d_loss: 15.70481300, g_loss: 7.58018780\n",
      "Step: [2743] d_loss: 16.23236465, g_loss: 7.85816193\n",
      "Step: [2744] d_loss: 15.49627972, g_loss: 8.32777500\n",
      "Step: [2745] d_loss: 15.65133476, g_loss: 8.14710426\n",
      "Step: [2746] d_loss: 15.95126915, g_loss: 8.16576004\n",
      "Step: [2747] d_loss: 15.78138256, g_loss: 7.84299564\n",
      "Step: [2748] d_loss: 14.86371994, g_loss: 7.43876886\n",
      "Step: [2749] d_loss: 15.24896145, g_loss: 7.79731703\n",
      "Step: [2750] d_loss: 15.12534523, g_loss: 8.49775124\n",
      "Step: [2751] d_loss: 14.98961639, g_loss: 8.00936890\n",
      "Step: [2752] d_loss: 14.81270599, g_loss: 8.51933861\n",
      "Step: [2753] d_loss: 15.39798737, g_loss: 8.34531212\n",
      "Step: [2754] d_loss: 14.86778545, g_loss: 8.18964863\n",
      "Step: [2755] d_loss: 15.36761284, g_loss: 7.79139042\n",
      "Step: [2756] d_loss: 14.90760994, g_loss: 8.10187531\n",
      "Step: [2757] d_loss: 15.97711563, g_loss: 8.32775688\n",
      "Step: [2758] d_loss: 15.35709953, g_loss: 8.59035301\n",
      "Step: [2759] d_loss: 15.96500969, g_loss: 7.63715458\n",
      "Step: [2760] d_loss: 15.00500298, g_loss: 8.54352760\n",
      "Step: [2761] d_loss: 15.63945103, g_loss: 7.93197441\n",
      "Step: [2762] d_loss: 15.11429977, g_loss: 8.24597549\n",
      "Step: [2763] d_loss: 15.11434650, g_loss: 7.85830784\n",
      "Step: [2764] d_loss: 14.59515190, g_loss: 8.42748451\n",
      "Step: [2765] d_loss: 14.45436954, g_loss: 8.53779602\n",
      "Step: [2766] d_loss: 15.37883568, g_loss: 7.61606169\n",
      "Step: [2767] d_loss: 14.66168118, g_loss: 8.11484146\n",
      "Step: [2768] d_loss: 14.32021332, g_loss: 8.39748383\n",
      "Step: [2769] d_loss: 14.77508354, g_loss: 7.74394178\n",
      "Step: [2770] d_loss: 15.15178871, g_loss: 7.48133183\n",
      "Step: [2771] d_loss: 15.22158813, g_loss: 8.07994080\n",
      "Step: [2772] d_loss: 14.73147774, g_loss: 8.24325085\n",
      "Step: [2773] d_loss: 14.83484459, g_loss: 8.24868965\n",
      "Step: [2774] d_loss: 15.74964905, g_loss: 7.65905237\n",
      "Step: [2775] d_loss: 15.39569092, g_loss: 8.08889008\n",
      "Step: [2776] d_loss: 15.59017086, g_loss: 7.97418261\n",
      "Step: [2777] d_loss: 16.45458221, g_loss: 8.15466022\n",
      "Step: [2778] d_loss: 15.48011208, g_loss: 8.06025124\n",
      "Step: [2779] d_loss: 15.85691261, g_loss: 8.20997524\n",
      "Step: [2780] d_loss: 14.67510033, g_loss: 8.23981762\n",
      "Step: [2781] d_loss: 15.55621052, g_loss: 8.13186836\n",
      "Step: [2782] d_loss: 15.41652298, g_loss: 8.14715195\n",
      "Step: [2783] d_loss: 15.81793404, g_loss: 8.10976696\n",
      "Step: [2784] d_loss: 15.40051460, g_loss: 7.95819521\n",
      "Step: [2785] d_loss: 15.27165318, g_loss: 8.27449608\n",
      "Step: [2786] d_loss: 15.25568771, g_loss: 7.91070843\n",
      "Step: [2787] d_loss: 15.08584785, g_loss: 8.10407257\n",
      "Step: [2788] d_loss: 15.64094543, g_loss: 8.27290916\n",
      "Step: [2789] d_loss: 15.19081879, g_loss: 8.03533840\n",
      "Step: [2790] d_loss: 15.89333248, g_loss: 7.90566778\n",
      "Step: [2791] d_loss: 15.97510719, g_loss: 8.39674568\n",
      "Step: [2792] d_loss: 15.88755608, g_loss: 8.30028820\n",
      "Step: [2793] d_loss: 15.34724998, g_loss: 8.40569115\n",
      "Step: [2794] d_loss: 14.43469429, g_loss: 8.29469395\n",
      "Step: [2795] d_loss: 14.77393913, g_loss: 8.51388836\n",
      "Step: [2796] d_loss: 14.93065262, g_loss: 7.75674820\n",
      "Step: [2797] d_loss: 14.43783760, g_loss: 8.48787022\n",
      "Step: [2798] d_loss: 14.79899693, g_loss: 8.11428928\n",
      "Step: [2799] d_loss: 15.38187027, g_loss: 8.27117538\n",
      "Step: [2800] d_loss: 15.49301624, g_loss: 7.86904716\n",
      "Step: [2801] d_loss: 14.77903175, g_loss: 8.19094849\n",
      "Step: [2802] d_loss: 15.36769867, g_loss: 7.84510612\n",
      "Step: [2803] d_loss: 14.68373299, g_loss: 8.21306038\n",
      "Step: [2804] d_loss: 15.70264816, g_loss: 8.26079750\n",
      "Step: [2805] d_loss: 14.56782722, g_loss: 8.30574799\n",
      "Step: [2806] d_loss: 15.47599411, g_loss: 7.66494799\n",
      "Step: [2807] d_loss: 15.59640503, g_loss: 7.77924061\n",
      "Step: [2808] d_loss: 14.63486481, g_loss: 7.99477673\n",
      "Step: [2809] d_loss: 14.45204926, g_loss: 8.00873184\n",
      "Step: [2810] d_loss: 15.78418064, g_loss: 7.64266586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2811] d_loss: 14.94493103, g_loss: 8.03144360\n",
      "Step: [2812] d_loss: 14.37222004, g_loss: 8.46528912\n",
      "Step: [2813] d_loss: 15.40335083, g_loss: 8.28165340\n",
      "Step: [2814] d_loss: 15.42823887, g_loss: 7.73121071\n",
      "Step: [2815] d_loss: 15.36247158, g_loss: 7.46201324\n",
      "Step: [2816] d_loss: 15.35813808, g_loss: 8.06425190\n",
      "Step: [2817] d_loss: 14.97722340, g_loss: 8.04838371\n",
      "Step: [2818] d_loss: 14.70781898, g_loss: 8.18609047\n",
      "Step: [2819] d_loss: 15.89796448, g_loss: 8.08761215\n",
      "Step: [2820] d_loss: 15.70787048, g_loss: 8.00870705\n",
      "Step: [2821] d_loss: 16.15189743, g_loss: 7.83022118\n",
      "Step: [2822] d_loss: 15.12285519, g_loss: 8.27475643\n",
      "Step: [2823] d_loss: 15.16548347, g_loss: 7.80832338\n",
      "Step: [2824] d_loss: 15.23566628, g_loss: 8.10664558\n",
      "Step: [2825] d_loss: 15.46198177, g_loss: 7.65920687\n",
      "Step: [2826] d_loss: 15.46007538, g_loss: 7.78274441\n",
      "Step: [2827] d_loss: 14.93865108, g_loss: 8.71816635\n",
      "Step: [2828] d_loss: 14.99135590, g_loss: 7.64491892\n",
      "Step: [2829] d_loss: 14.55924416, g_loss: 8.19758701\n",
      "Step: [2830] d_loss: 15.43115234, g_loss: 8.79066277\n",
      "Step: [2831] d_loss: 16.28011322, g_loss: 8.45287132\n",
      "Step: [2832] d_loss: 15.34110641, g_loss: 8.25246334\n",
      "Step: [2833] d_loss: 15.26164532, g_loss: 8.37999535\n",
      "Step: [2834] d_loss: 16.14164734, g_loss: 7.88721275\n",
      "Step: [2835] d_loss: 15.12124252, g_loss: 7.88230038\n",
      "Step: [2836] d_loss: 14.79355049, g_loss: 8.42451000\n",
      "Step: [2837] d_loss: 15.44491768, g_loss: 8.15575981\n",
      "Step: [2838] d_loss: 16.34870529, g_loss: 7.78454018\n",
      "Step: [2839] d_loss: 14.69201851, g_loss: 8.59436989\n",
      "Step: [2840] d_loss: 15.01391220, g_loss: 8.17889881\n",
      "Step: [2841] d_loss: 15.52248192, g_loss: 7.81478834\n",
      "Step: [2842] d_loss: 15.37828827, g_loss: 7.63772583\n",
      "Step: [2843] d_loss: 15.77504158, g_loss: 7.62851238\n",
      "Step: [2844] d_loss: 15.52373886, g_loss: 7.90949821\n",
      "Step: [2845] d_loss: 15.24200821, g_loss: 7.62531424\n",
      "Step: [2846] d_loss: 15.22837639, g_loss: 7.97978687\n",
      "Step: [2847] d_loss: 14.51838112, g_loss: 8.23662949\n",
      "Step: [2848] d_loss: 14.29912758, g_loss: 8.21834564\n",
      "Step: [2849] d_loss: 14.47518444, g_loss: 8.51603222\n",
      "Step: [2850] d_loss: 15.02888966, g_loss: 8.14145947\n",
      "Step: [2851] d_loss: 15.09163570, g_loss: 8.37607479\n",
      "Step: [2852] d_loss: 15.11702824, g_loss: 8.73985100\n",
      "Step: [2853] d_loss: 15.46480179, g_loss: 8.49288082\n",
      "Step: [2854] d_loss: 15.55201721, g_loss: 8.45599937\n",
      "Step: [2855] d_loss: 16.07059097, g_loss: 8.36930466\n",
      "Step: [2856] d_loss: 14.99879456, g_loss: 8.50114918\n",
      "Step: [2857] d_loss: 15.37700081, g_loss: 7.92279816\n",
      "Step: [2858] d_loss: 15.41739655, g_loss: 7.94389582\n",
      "Step: [2859] d_loss: 14.66167259, g_loss: 8.33000946\n",
      "Step: [2860] d_loss: 15.53940105, g_loss: 8.23486137\n",
      "Step: [2861] d_loss: 14.45022583, g_loss: 7.99484968\n",
      "Step: [2862] d_loss: 14.55699062, g_loss: 8.03697968\n",
      "Step: [2863] d_loss: 14.59014511, g_loss: 8.46956062\n",
      "Step: [2864] d_loss: 15.41449928, g_loss: 7.38493156\n",
      "Step: [2865] d_loss: 15.65995598, g_loss: 7.78823090\n",
      "Step: [2866] d_loss: 15.47164345, g_loss: 8.15756226\n",
      "Step: [2867] d_loss: 16.59871292, g_loss: 7.88488007\n",
      "Step: [2868] d_loss: 15.35537338, g_loss: 7.90437794\n",
      "Step: [2869] d_loss: 15.81218147, g_loss: 8.04017639\n",
      "Step: [2870] d_loss: 15.57892609, g_loss: 7.49630499\n",
      "Step: [2871] d_loss: 15.65363312, g_loss: 7.65576172\n",
      "Step: [2872] d_loss: 15.14634609, g_loss: 7.76836205\n",
      "Step: [2873] d_loss: 15.12898827, g_loss: 8.19243717\n",
      "Step: [2874] d_loss: 14.84507847, g_loss: 8.33446407\n",
      "Step: [2875] d_loss: 15.26475143, g_loss: 8.25501156\n",
      "Step: [2876] d_loss: 15.80848217, g_loss: 8.26110268\n",
      "Step: [2877] d_loss: 15.98424339, g_loss: 8.34938812\n",
      "Step: [2878] d_loss: 14.28023624, g_loss: 8.00424576\n",
      "Step: [2879] d_loss: 14.70873260, g_loss: 7.90212584\n",
      "Step: [2880] d_loss: 15.14628601, g_loss: 8.08810806\n",
      "Step: [2881] d_loss: 15.81083584, g_loss: 7.46385479\n",
      "Step: [2882] d_loss: 15.55472660, g_loss: 8.03873253\n",
      "Step: [2883] d_loss: 15.52192593, g_loss: 7.97696686\n",
      "Step: [2884] d_loss: 15.44589138, g_loss: 7.81402969\n",
      "Step: [2885] d_loss: 14.91448498, g_loss: 7.93150997\n",
      "Step: [2886] d_loss: 14.96761513, g_loss: 8.01487350\n",
      "Step: [2887] d_loss: 15.93323803, g_loss: 8.23935127\n",
      "Step: [2888] d_loss: 14.16729927, g_loss: 8.20027637\n",
      "Step: [2889] d_loss: 15.18609619, g_loss: 8.25280666\n",
      "Step: [2890] d_loss: 14.42697334, g_loss: 8.50706100\n",
      "Step: [2891] d_loss: 15.13153458, g_loss: 8.08847523\n",
      "Step: [2892] d_loss: 15.12554455, g_loss: 8.20537281\n",
      "Step: [2893] d_loss: 14.63975239, g_loss: 8.21110535\n",
      "Step: [2894] d_loss: 15.09770775, g_loss: 8.37565994\n",
      "Step: [2895] d_loss: 15.06376457, g_loss: 8.33824635\n",
      "Step: [2896] d_loss: 16.25518608, g_loss: 8.28007317\n",
      "Step: [2897] d_loss: 15.57423782, g_loss: 8.30061340\n",
      "Step: [2898] d_loss: 14.50152206, g_loss: 8.26445007\n",
      "Step: [2899] d_loss: 15.23017883, g_loss: 8.52589226\n",
      "Step: [2900] d_loss: 15.64397049, g_loss: 8.73684120\n",
      "Step: [2901] d_loss: 16.15850067, g_loss: 7.87307739\n",
      "Step: [2902] d_loss: 16.04132843, g_loss: 8.01861095\n",
      "Step: [2903] d_loss: 16.09442902, g_loss: 7.48096943\n",
      "Step: [2904] d_loss: 15.52803898, g_loss: 7.99198914\n",
      "Step: [2905] d_loss: 15.59553909, g_loss: 8.21377850\n",
      "Step: [2906] d_loss: 16.31569290, g_loss: 7.83581829\n",
      "Step: [2907] d_loss: 14.69371986, g_loss: 8.48244095\n",
      "Step: [2908] d_loss: 15.51821518, g_loss: 8.31622887\n",
      "Step: [2909] d_loss: 14.96438980, g_loss: 7.63221264\n",
      "Step: [2910] d_loss: 15.26729393, g_loss: 8.15605831\n",
      "Step: [2911] d_loss: 15.22239494, g_loss: 8.29209805\n",
      "Step: [2912] d_loss: 14.84157658, g_loss: 8.59190369\n",
      "Step: [2913] d_loss: 14.72542286, g_loss: 8.22747135\n",
      "Step: [2914] d_loss: 15.54581451, g_loss: 7.80772781\n",
      "Step: [2915] d_loss: 15.20341873, g_loss: 8.30515099\n",
      "Step: [2916] d_loss: 14.96158504, g_loss: 8.37506104\n",
      "Step: [2917] d_loss: 14.85896111, g_loss: 8.62513733\n",
      "Step: [2918] d_loss: 15.52354240, g_loss: 8.25192165\n",
      "Step: [2919] d_loss: 14.63290024, g_loss: 8.09569168\n",
      "Step: [2920] d_loss: 15.48819828, g_loss: 8.13668633\n",
      "Step: [2921] d_loss: 14.44267368, g_loss: 8.21728611\n",
      "Step: [2922] d_loss: 15.53672028, g_loss: 7.72271919\n",
      "Step: [2923] d_loss: 15.42086220, g_loss: 7.93215609\n",
      "Step: [2924] d_loss: 14.76062489, g_loss: 8.10301304\n",
      "Step: [2925] d_loss: 15.83038330, g_loss: 8.22364616\n",
      "Step: [2926] d_loss: 15.98384571, g_loss: 7.34615040\n",
      "Step: [2927] d_loss: 15.38355541, g_loss: 8.02933788\n",
      "Step: [2928] d_loss: 15.35918617, g_loss: 8.13835144\n",
      "Step: [2929] d_loss: 15.09072495, g_loss: 8.07451248\n",
      "Step: [2930] d_loss: 16.17542648, g_loss: 8.05798435\n",
      "Step: [2931] d_loss: 15.41248131, g_loss: 8.20753384\n",
      "Step: [2932] d_loss: 14.91367626, g_loss: 8.23918247\n",
      "Step: [2933] d_loss: 14.58347607, g_loss: 8.19707298\n",
      "Step: [2934] d_loss: 15.17191696, g_loss: 8.60389328\n",
      "Step: [2935] d_loss: 15.84125137, g_loss: 8.38636971\n",
      "Step: [2936] d_loss: 15.15981293, g_loss: 8.41699696\n",
      "Step: [2937] d_loss: 15.65088558, g_loss: 8.09684181\n",
      "Step: [2938] d_loss: 15.43790531, g_loss: 7.90234566\n",
      "Step: [2939] d_loss: 14.67380524, g_loss: 8.53181267\n",
      "Step: [2940] d_loss: 15.50500870, g_loss: 7.98479033\n",
      "Step: [2941] d_loss: 14.39818001, g_loss: 8.68202782\n",
      "Step: [2942] d_loss: 16.32333946, g_loss: 7.49380970\n",
      "Step: [2943] d_loss: 15.51033401, g_loss: 7.94231319\n",
      "Step: [2944] d_loss: 15.09540749, g_loss: 8.33017540\n",
      "Step: [2945] d_loss: 14.77770519, g_loss: 8.10653496\n",
      "Step: [2946] d_loss: 14.84238625, g_loss: 8.54106426\n",
      "Step: [2947] d_loss: 15.24428463, g_loss: 8.12866783\n",
      "Step: [2948] d_loss: 15.88345909, g_loss: 7.64094687\n",
      "Step: [2949] d_loss: 15.88710785, g_loss: 7.72083378\n",
      "Step: [2950] d_loss: 16.00740814, g_loss: 7.66862965\n",
      "Step: [2951] d_loss: 15.73997974, g_loss: 7.87234879\n",
      "Step: [2952] d_loss: 14.88335228, g_loss: 8.35621071\n",
      "Step: [2953] d_loss: 15.02256584, g_loss: 8.12414551\n",
      "Step: [2954] d_loss: 15.23760891, g_loss: 8.08256531\n",
      "Step: [2955] d_loss: 15.34536552, g_loss: 8.08761215\n",
      "Step: [2956] d_loss: 15.23612976, g_loss: 7.55407143\n",
      "Step: [2957] d_loss: 14.45499039, g_loss: 8.34191704\n",
      "Step: [2958] d_loss: 14.67394352, g_loss: 8.58825302\n",
      "Step: [2959] d_loss: 14.73773193, g_loss: 8.17389679\n",
      "Step: [2960] d_loss: 15.55570221, g_loss: 7.82251406\n",
      "Step: [2961] d_loss: 15.02258587, g_loss: 8.01250076\n",
      "Step: [2962] d_loss: 15.69932747, g_loss: 8.26091576\n",
      "Step: [2963] d_loss: 15.27783394, g_loss: 7.87311316\n",
      "Step: [2964] d_loss: 14.75123882, g_loss: 8.14338684\n",
      "Step: [2965] d_loss: 15.36320686, g_loss: 7.91921949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2966] d_loss: 14.79739571, g_loss: 7.85773373\n",
      "Step: [2967] d_loss: 14.71571350, g_loss: 7.93587780\n",
      "Step: [2968] d_loss: 15.21725273, g_loss: 7.73789120\n",
      "Step: [2969] d_loss: 15.48205090, g_loss: 8.01600933\n",
      "Step: [2970] d_loss: 15.00143623, g_loss: 8.51794720\n",
      "Step: [2971] d_loss: 15.16417122, g_loss: 8.73325348\n",
      "Step: [2972] d_loss: 15.42468071, g_loss: 7.96952724\n",
      "Step: [2973] d_loss: 15.95949268, g_loss: 8.17978954\n",
      "Step: [2974] d_loss: 15.38220882, g_loss: 7.96021843\n",
      "Step: [2975] d_loss: 15.22832298, g_loss: 7.89223433\n",
      "Step: [2976] d_loss: 15.01269531, g_loss: 7.76074743\n",
      "Step: [2977] d_loss: 14.85054111, g_loss: 7.75640583\n",
      "Step: [2978] d_loss: 16.03644180, g_loss: 7.96964645\n",
      "Step: [2979] d_loss: 15.15337276, g_loss: 7.98992825\n",
      "Step: [2980] d_loss: 14.93971062, g_loss: 8.95292759\n",
      "Step: [2981] d_loss: 16.11670685, g_loss: 8.59814930\n",
      "Step: [2982] d_loss: 15.94100666, g_loss: 8.53676224\n",
      "Step: [2983] d_loss: 15.17739868, g_loss: 8.10388088\n",
      "Step: [2984] d_loss: 14.82746792, g_loss: 8.58491039\n",
      "Step: [2985] d_loss: 15.71440792, g_loss: 8.41476059\n",
      "Step: [2986] d_loss: 15.19252777, g_loss: 8.43884087\n",
      "Step: [2987] d_loss: 15.33998299, g_loss: 8.37943935\n",
      "Step: [2988] d_loss: 15.05721760, g_loss: 8.11416054\n",
      "Step: [2989] d_loss: 15.09441948, g_loss: 7.94073486\n",
      "Step: [2990] d_loss: 14.70365238, g_loss: 8.14970779\n",
      "Step: [2991] d_loss: 15.36145210, g_loss: 8.25793934\n",
      "Step: [2992] d_loss: 15.54461479, g_loss: 8.26128006\n",
      "Step: [2993] d_loss: 14.20655632, g_loss: 8.32382393\n",
      "Step: [2994] d_loss: 14.56109333, g_loss: 7.91499329\n",
      "Step: [2995] d_loss: 14.99523735, g_loss: 8.11645508\n",
      "Step: [2996] d_loss: 15.01798439, g_loss: 8.02931213\n",
      "Step: [2997] d_loss: 14.58620358, g_loss: 8.24247932\n",
      "Step: [2998] d_loss: 14.86708069, g_loss: 8.47776604\n",
      "Step: [2999] d_loss: 15.66869354, g_loss: 7.78925991\n",
      "Step: [3000] d_loss: 15.23008347, g_loss: 7.81586456\n",
      "Step: [3001] d_loss: 14.66902351, g_loss: 8.16447067\n",
      "Step: [3002] d_loss: 15.36147499, g_loss: 7.86279964\n",
      "Step: [3003] d_loss: 15.26011276, g_loss: 8.08021355\n",
      "Step: [3004] d_loss: 15.57696915, g_loss: 7.49872208\n",
      "Step: [3005] d_loss: 15.33454323, g_loss: 7.98437977\n",
      "Step: [3006] d_loss: 15.57325459, g_loss: 7.93971634\n",
      "Step: [3007] d_loss: 14.90579700, g_loss: 8.09294605\n",
      "Step: [3008] d_loss: 14.61939240, g_loss: 8.07617188\n",
      "Step: [3009] d_loss: 14.83332825, g_loss: 8.00807476\n",
      "Step: [3010] d_loss: 15.10985374, g_loss: 8.27407837\n",
      "Step: [3011] d_loss: 15.18035889, g_loss: 7.81156492\n",
      "Step: [3012] d_loss: 14.62368870, g_loss: 8.70072174\n",
      "Step: [3013] d_loss: 14.86087799, g_loss: 8.41349792\n",
      "Step: [3014] d_loss: 14.87229729, g_loss: 8.34868431\n",
      "Step: [3015] d_loss: 15.17611885, g_loss: 8.16484928\n",
      "Step: [3016] d_loss: 15.24759960, g_loss: 8.55684471\n",
      "Step: [3017] d_loss: 14.69792175, g_loss: 8.16138554\n",
      "Step: [3018] d_loss: 14.53003311, g_loss: 8.08431625\n",
      "Step: [3019] d_loss: 14.36787701, g_loss: 8.10573101\n",
      "Step: [3020] d_loss: 14.20286751, g_loss: 8.11814499\n",
      "Step: [3021] d_loss: 14.51362228, g_loss: 8.51018333\n",
      "Step: [3022] d_loss: 15.00804996, g_loss: 8.58449745\n",
      "Step: [3023] d_loss: 14.77588654, g_loss: 8.67852592\n",
      "Step: [3024] d_loss: 14.80716801, g_loss: 8.43994617\n",
      "Step: [3025] d_loss: 14.83279610, g_loss: 8.00096321\n",
      "Step: [3026] d_loss: 14.34916687, g_loss: 8.27491474\n",
      "Step: [3027] d_loss: 15.01996326, g_loss: 8.12697983\n",
      "Step: [3028] d_loss: 14.93946266, g_loss: 8.32415009\n",
      "Step: [3029] d_loss: 14.41285515, g_loss: 8.78552151\n",
      "Step: [3030] d_loss: 15.44946671, g_loss: 8.13549232\n",
      "Step: [3031] d_loss: 14.92397308, g_loss: 8.06457138\n",
      "Step: [3032] d_loss: 15.20831680, g_loss: 7.74726629\n",
      "Step: [3033] d_loss: 15.07743168, g_loss: 8.28941917\n",
      "Step: [3034] d_loss: 14.79972076, g_loss: 8.45556927\n",
      "Step: [3035] d_loss: 15.58930016, g_loss: 7.95401525\n",
      "Step: [3036] d_loss: 15.44224453, g_loss: 8.04897022\n",
      "Step: [3037] d_loss: 15.73186684, g_loss: 7.98710966\n",
      "Step: [3038] d_loss: 15.84325981, g_loss: 7.88267136\n",
      "Step: [3039] d_loss: 15.48810291, g_loss: 8.64541149\n",
      "Step: [3040] d_loss: 15.76062489, g_loss: 8.72377777\n",
      "Step: [3041] d_loss: 16.95694733, g_loss: 8.95665932\n",
      "Step: [3042] d_loss: 17.23217583, g_loss: 9.54812622\n",
      "Step: [3043] d_loss: 17.44406128, g_loss: 8.51356125\n",
      "Step: [3044] d_loss: 16.71375275, g_loss: 8.01980495\n",
      "Step: [3045] d_loss: 15.93486595, g_loss: 8.08203220\n",
      "Step: [3046] d_loss: 16.05078316, g_loss: 8.38287067\n",
      "Step: [3047] d_loss: 15.86796379, g_loss: 8.04862595\n",
      "Step: [3048] d_loss: 15.38400364, g_loss: 8.43456173\n",
      "Step: [3049] d_loss: 15.60267067, g_loss: 8.88052940\n",
      "Step: [3050] d_loss: 15.57447052, g_loss: 9.18009377\n",
      "Step: [3051] d_loss: 16.02527618, g_loss: 8.36850929\n",
      "Step: [3052] d_loss: 15.44132042, g_loss: 7.66216278\n",
      "Step: [3053] d_loss: 15.21547890, g_loss: 7.57650852\n",
      "Step: [3054] d_loss: 15.46844673, g_loss: 7.63462877\n",
      "Step: [3055] d_loss: 15.56645489, g_loss: 8.01072788\n",
      "Step: [3056] d_loss: 15.95545101, g_loss: 8.07312393\n",
      "Step: [3057] d_loss: 15.24446487, g_loss: 7.81315517\n",
      "Step: [3058] d_loss: 15.10919762, g_loss: 8.05030727\n",
      "Step: [3059] d_loss: 14.79321671, g_loss: 7.87537670\n",
      "Step: [3060] d_loss: 14.53425884, g_loss: 8.40410423\n",
      "Step: [3061] d_loss: 15.85456753, g_loss: 7.96409369\n",
      "Step: [3062] d_loss: 16.30586815, g_loss: 7.68230677\n",
      "Step: [3063] d_loss: 14.78557587, g_loss: 7.93948364\n",
      "Step: [3064] d_loss: 15.06987953, g_loss: 8.14397049\n",
      "Step: [3065] d_loss: 15.17569542, g_loss: 7.88232803\n",
      "Step: [3066] d_loss: 15.46497822, g_loss: 8.06025124\n",
      "Step: [3067] d_loss: 15.34107685, g_loss: 8.26563835\n",
      "Step: [3068] d_loss: 15.43723965, g_loss: 8.00437546\n",
      "Step: [3069] d_loss: 15.07128525, g_loss: 8.36423683\n",
      "Step: [3070] d_loss: 15.41594315, g_loss: 7.94760799\n",
      "Step: [3071] d_loss: 14.83693314, g_loss: 7.99265289\n",
      "Step: [3072] d_loss: 15.30417252, g_loss: 8.05408859\n",
      "Step: [3073] d_loss: 14.82624435, g_loss: 8.51883984\n",
      "Step: [3074] d_loss: 14.42173386, g_loss: 8.06490326\n",
      "Step: [3075] d_loss: 15.62554550, g_loss: 7.68313074\n",
      "Step: [3076] d_loss: 15.16403961, g_loss: 8.07285881\n",
      "Step: [3077] d_loss: 15.29939079, g_loss: 7.74456882\n",
      "Step: [3078] d_loss: 15.43772507, g_loss: 7.71470785\n",
      "Step: [3079] d_loss: 14.76531506, g_loss: 8.21597481\n",
      "Step: [3080] d_loss: 16.17949295, g_loss: 7.90731478\n",
      "Step: [3081] d_loss: 14.93374443, g_loss: 8.46241665\n",
      "Step: [3082] d_loss: 15.40404987, g_loss: 8.38953209\n",
      "Step: [3083] d_loss: 15.03262138, g_loss: 8.52656078\n",
      "Step: [3084] d_loss: 15.63838482, g_loss: 8.26325035\n",
      "Step: [3085] d_loss: 14.67785454, g_loss: 8.37015152\n",
      "Step: [3086] d_loss: 14.99240303, g_loss: 8.53385353\n",
      "Step: [3087] d_loss: 15.63789177, g_loss: 8.00362778\n",
      "Step: [3088] d_loss: 14.61870384, g_loss: 8.17151260\n",
      "Step: [3089] d_loss: 15.03194618, g_loss: 8.30327034\n",
      "Step: [3090] d_loss: 15.55668926, g_loss: 8.13935757\n",
      "Step: [3091] d_loss: 14.94772625, g_loss: 8.50734138\n",
      "Step: [3092] d_loss: 15.05612755, g_loss: 8.44400978\n",
      "Step: [3093] d_loss: 15.00526428, g_loss: 7.92950106\n",
      "Step: [3094] d_loss: 15.84717178, g_loss: 7.80934811\n",
      "Step: [3095] d_loss: 15.31152058, g_loss: 8.03220463\n",
      "Step: [3096] d_loss: 14.65365982, g_loss: 8.24346733\n",
      "Step: [3097] d_loss: 14.80039787, g_loss: 8.15242577\n",
      "Step: [3098] d_loss: 15.54281807, g_loss: 8.47550583\n",
      "Step: [3099] d_loss: 15.64371109, g_loss: 7.57618952\n",
      "Step: [3100] d_loss: 15.63309288, g_loss: 8.13874054\n",
      "Step: [3101] d_loss: 16.01750946, g_loss: 7.68644047\n",
      "Step: [3102] d_loss: 15.21498871, g_loss: 8.10199451\n",
      "Step: [3103] d_loss: 15.49756050, g_loss: 8.61414719\n",
      "Step: [3104] d_loss: 14.86977768, g_loss: 8.19649887\n",
      "Step: [3105] d_loss: 14.50181198, g_loss: 8.16259193\n",
      "Step: [3106] d_loss: 15.13235664, g_loss: 7.71449184\n",
      "Step: [3107] d_loss: 14.43658447, g_loss: 8.34813786\n",
      "Step: [3108] d_loss: 16.91118622, g_loss: 7.43208790\n",
      "Step: [3109] d_loss: 15.20913315, g_loss: 8.12413216\n",
      "Step: [3110] d_loss: 15.47614288, g_loss: 8.35595322\n",
      "Step: [3111] d_loss: 15.56800365, g_loss: 8.17412663\n",
      "Step: [3112] d_loss: 15.69087887, g_loss: 7.53949356\n",
      "Step: [3113] d_loss: 16.09736443, g_loss: 7.89997149\n",
      "Step: [3114] d_loss: 15.89007568, g_loss: 8.19198799\n",
      "Step: [3115] d_loss: 15.94455147, g_loss: 8.33620358\n",
      "Step: [3116] d_loss: 14.93490601, g_loss: 8.69960785\n",
      "Step: [3117] d_loss: 14.89717197, g_loss: 8.79609203\n",
      "Step: [3118] d_loss: 15.24518681, g_loss: 8.25815773\n",
      "Step: [3119] d_loss: 15.59232616, g_loss: 8.71705055\n",
      "Step: [3120] d_loss: 14.98424816, g_loss: 8.45354271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3121] d_loss: 15.13109398, g_loss: 8.18837738\n",
      "Step: [3122] d_loss: 15.16705513, g_loss: 7.83666468\n",
      "Step: [3123] d_loss: 14.80226135, g_loss: 8.09801960\n",
      "Step: [3124] d_loss: 14.87146187, g_loss: 8.29040527\n",
      "Step: [3125] d_loss: 15.09896564, g_loss: 8.72513008\n",
      "Step: [3126] d_loss: 15.21020126, g_loss: 8.40773201\n",
      "Step: [3127] d_loss: 14.72792053, g_loss: 8.07328224\n",
      "Step: [3128] d_loss: 14.75482368, g_loss: 8.29771900\n",
      "Step: [3129] d_loss: 15.19664764, g_loss: 9.05560303\n",
      "Step: [3130] d_loss: 16.10687637, g_loss: 8.26265621\n",
      "Step: [3131] d_loss: 14.94718361, g_loss: 8.14743519\n",
      "Step: [3132] d_loss: 14.93882179, g_loss: 8.43783092\n",
      "Step: [3133] d_loss: 14.76722813, g_loss: 8.52234459\n",
      "Step: [3134] d_loss: 15.54276848, g_loss: 8.40721512\n",
      "Step: [3135] d_loss: 15.02667618, g_loss: 7.90543365\n",
      "Step: [3136] d_loss: 15.30680275, g_loss: 8.02291489\n",
      "Step: [3137] d_loss: 14.07469654, g_loss: 8.42293930\n",
      "Step: [3138] d_loss: 14.55643272, g_loss: 8.46585178\n",
      "Step: [3139] d_loss: 14.64643860, g_loss: 8.67176151\n",
      "Step: [3140] d_loss: 15.16387367, g_loss: 8.05737686\n",
      "Step: [3141] d_loss: 15.54380226, g_loss: 8.12557125\n",
      "Step: [3142] d_loss: 14.82793140, g_loss: 8.41371346\n",
      "Step: [3143] d_loss: 14.60557175, g_loss: 8.37818718\n",
      "Step: [3144] d_loss: 15.16240883, g_loss: 8.33114338\n",
      "Step: [3145] d_loss: 15.64598465, g_loss: 8.13188362\n",
      "Step: [3146] d_loss: 15.23881721, g_loss: 7.79933834\n",
      "Step: [3147] d_loss: 15.18238735, g_loss: 7.75792694\n",
      "Step: [3148] d_loss: 15.59024239, g_loss: 8.10459709\n",
      "Step: [3149] d_loss: 14.65491486, g_loss: 8.49012947\n",
      "Step: [3150] d_loss: 15.21405983, g_loss: 7.49777555\n",
      "Step: [3151] d_loss: 15.67746353, g_loss: 7.45941257\n",
      "Step: [3152] d_loss: 16.03562927, g_loss: 7.39334393\n",
      "Step: [3153] d_loss: 14.97589302, g_loss: 8.26315689\n",
      "Step: [3154] d_loss: 16.09302330, g_loss: 7.59525681\n",
      "Step: [3155] d_loss: 15.04422951, g_loss: 8.04593468\n",
      "Step: [3156] d_loss: 15.16075897, g_loss: 7.97234774\n",
      "Step: [3157] d_loss: 15.36649132, g_loss: 7.55707645\n",
      "Step: [3158] d_loss: 15.44022560, g_loss: 8.00006104\n",
      "Step: [3159] d_loss: 15.63297653, g_loss: 7.99306202\n",
      "Step: [3160] d_loss: 16.46929550, g_loss: 7.48162556\n",
      "Step: [3161] d_loss: 15.12825108, g_loss: 8.01553345\n",
      "Step: [3162] d_loss: 15.38821030, g_loss: 7.76508999\n",
      "Step: [3163] d_loss: 16.48501968, g_loss: 8.06489849\n",
      "Step: [3164] d_loss: 15.97497368, g_loss: 7.69106150\n",
      "Step: [3165] d_loss: 15.39488983, g_loss: 7.58018875\n",
      "Step: [3166] d_loss: 15.44581890, g_loss: 7.93106318\n",
      "Step: [3167] d_loss: 15.01920795, g_loss: 8.33317375\n",
      "Step: [3168] d_loss: 14.98925114, g_loss: 8.41752243\n",
      "Step: [3169] d_loss: 14.67863655, g_loss: 8.46375465\n",
      "Step: [3170] d_loss: 15.19356918, g_loss: 8.13770866\n",
      "Step: [3171] d_loss: 16.26585388, g_loss: 8.32408333\n",
      "Step: [3172] d_loss: 16.37711906, g_loss: 8.46467113\n",
      "Step: [3173] d_loss: 15.62870026, g_loss: 8.33162785\n",
      "Step: [3174] d_loss: 14.88518143, g_loss: 8.00237274\n",
      "Step: [3175] d_loss: 15.21468925, g_loss: 8.43785667\n",
      "Step: [3176] d_loss: 15.31251335, g_loss: 8.37879467\n",
      "Step: [3177] d_loss: 15.45764923, g_loss: 7.26677465\n",
      "Step: [3178] d_loss: 14.72927094, g_loss: 8.31645012\n",
      "Step: [3179] d_loss: 15.28390026, g_loss: 8.44799614\n",
      "Step: [3180] d_loss: 15.41881943, g_loss: 9.06777000\n",
      "Step: [3181] d_loss: 16.05398560, g_loss: 8.64942741\n",
      "Step: [3182] d_loss: 15.34236908, g_loss: 7.61654377\n",
      "Step: [3183] d_loss: 14.77209949, g_loss: 8.41234684\n",
      "Step: [3184] d_loss: 15.94686508, g_loss: 8.99877167\n",
      "Step: [3185] d_loss: 14.74927139, g_loss: 7.72187710\n",
      "Step: [3186] d_loss: 15.14178658, g_loss: 7.70360565\n",
      "Step: [3187] d_loss: 15.24516487, g_loss: 8.20960045\n",
      "Step: [3188] d_loss: 15.23786545, g_loss: 8.17471313\n",
      "Step: [3189] d_loss: 14.75005531, g_loss: 8.13712311\n",
      "Step: [3190] d_loss: 14.47993183, g_loss: 8.20744610\n",
      "Step: [3191] d_loss: 14.89836216, g_loss: 8.16974354\n",
      "Step: [3192] d_loss: 14.88512802, g_loss: 8.23336029\n",
      "Step: [3193] d_loss: 16.39286804, g_loss: 7.70868444\n",
      "Step: [3194] d_loss: 15.47088432, g_loss: 8.34720230\n",
      "Step: [3195] d_loss: 15.71210670, g_loss: 7.94252729\n",
      "Step: [3196] d_loss: 14.77678585, g_loss: 7.98082590\n",
      "Step: [3197] d_loss: 15.44467354, g_loss: 7.99532366\n",
      "Step: [3198] d_loss: 16.16385460, g_loss: 9.27426529\n",
      "Step: [3199] d_loss: 18.04278374, g_loss: 9.08827019\n",
      "Step: [3200] d_loss: 16.85126877, g_loss: 8.14538193\n",
      "Step: [3201] d_loss: 16.70486450, g_loss: 8.63198662\n",
      "Step: [3202] d_loss: 16.45290565, g_loss: 7.90645647\n",
      "Step: [3203] d_loss: 15.79271507, g_loss: 8.07729340\n",
      "Step: [3204] d_loss: 15.69701672, g_loss: 7.91675520\n",
      "Step: [3205] d_loss: 15.21440029, g_loss: 7.98427868\n",
      "Step: [3206] d_loss: 16.25291824, g_loss: 8.03026295\n",
      "Step: [3207] d_loss: 15.24659061, g_loss: 7.87405968\n",
      "Step: [3208] d_loss: 14.88680649, g_loss: 8.32562542\n",
      "Step: [3209] d_loss: 15.18742371, g_loss: 8.07487297\n",
      "Step: [3210] d_loss: 14.98820019, g_loss: 8.37402725\n",
      "Step: [3211] d_loss: 14.42744637, g_loss: 8.63248634\n",
      "Step: [3212] d_loss: 14.98186302, g_loss: 8.05366039\n",
      "Step: [3213] d_loss: 14.29927349, g_loss: 8.21396255\n",
      "Step: [3214] d_loss: 15.19869232, g_loss: 7.75873804\n",
      "Step: [3215] d_loss: 15.16134739, g_loss: 8.15289974\n",
      "Step: [3216] d_loss: 14.53925705, g_loss: 8.30512333\n",
      "Step: [3217] d_loss: 15.29673481, g_loss: 7.90440178\n",
      "Step: [3218] d_loss: 14.87416363, g_loss: 8.27862930\n",
      "Step: [3219] d_loss: 15.28319740, g_loss: 8.47427559\n",
      "Step: [3220] d_loss: 14.59255981, g_loss: 8.36879730\n",
      "Step: [3221] d_loss: 14.84617710, g_loss: 8.53863525\n",
      "Step: [3222] d_loss: 15.95600700, g_loss: 7.69618797\n",
      "Step: [3223] d_loss: 15.81170177, g_loss: 7.97418404\n",
      "Step: [3224] d_loss: 14.80524445, g_loss: 8.26420593\n",
      "Step: [3225] d_loss: 15.15177727, g_loss: 7.76737309\n",
      "Step: [3226] d_loss: 15.19850731, g_loss: 8.27269840\n",
      "Step: [3227] d_loss: 14.96795464, g_loss: 7.94210339\n",
      "Step: [3228] d_loss: 15.22751999, g_loss: 8.21063805\n",
      "Step: [3229] d_loss: 15.78171730, g_loss: 7.46859312\n",
      "Step: [3230] d_loss: 15.30939674, g_loss: 7.75136328\n",
      "Step: [3231] d_loss: 15.60526848, g_loss: 8.04624557\n",
      "Step: [3232] d_loss: 14.80522537, g_loss: 8.35977364\n",
      "Step: [3233] d_loss: 15.22734451, g_loss: 7.77117491\n",
      "Step: [3234] d_loss: 15.06640625, g_loss: 7.86501503\n",
      "Step: [3235] d_loss: 15.20695877, g_loss: 7.98071003\n",
      "Step: [3236] d_loss: 15.31216621, g_loss: 8.02217674\n",
      "Step: [3237] d_loss: 14.63991737, g_loss: 8.23609161\n",
      "Step: [3238] d_loss: 15.42910957, g_loss: 8.42771721\n",
      "Step: [3239] d_loss: 15.84687996, g_loss: 8.03815365\n",
      "Step: [3240] d_loss: 14.79487038, g_loss: 8.19831371\n",
      "Step: [3241] d_loss: 15.71182728, g_loss: 7.99693871\n",
      "Step: [3242] d_loss: 14.51215839, g_loss: 7.95221901\n",
      "Step: [3243] d_loss: 15.70622063, g_loss: 7.86648512\n",
      "Step: [3244] d_loss: 15.83041191, g_loss: 7.89919806\n",
      "Step: [3245] d_loss: 15.89933205, g_loss: 7.94450712\n",
      "Step: [3246] d_loss: 14.92927361, g_loss: 7.91842508\n",
      "Step: [3247] d_loss: 15.36402702, g_loss: 8.00398827\n",
      "Step: [3248] d_loss: 14.71415806, g_loss: 8.29984093\n",
      "Step: [3249] d_loss: 15.37458515, g_loss: 7.74916267\n",
      "Step: [3250] d_loss: 15.07408524, g_loss: 8.36217308\n",
      "Step: [3251] d_loss: 14.45848656, g_loss: 8.27115154\n",
      "Step: [3252] d_loss: 14.82681179, g_loss: 8.24615955\n",
      "Step: [3253] d_loss: 14.78838539, g_loss: 7.53627872\n",
      "Step: [3254] d_loss: 14.95485878, g_loss: 8.04207230\n",
      "Step: [3255] d_loss: 15.77771568, g_loss: 8.77359104\n",
      "Step: [3256] d_loss: 16.42795753, g_loss: 8.08537674\n",
      "Step: [3257] d_loss: 14.87695885, g_loss: 8.18339729\n",
      "Step: [3258] d_loss: 15.44500542, g_loss: 7.75384283\n",
      "Step: [3259] d_loss: 15.03113174, g_loss: 7.36053371\n",
      "Step: [3260] d_loss: 14.61975288, g_loss: 7.91061974\n",
      "Step: [3261] d_loss: 15.31552410, g_loss: 7.90714312\n",
      "Step: [3262] d_loss: 14.95644188, g_loss: 7.73901892\n",
      "Step: [3263] d_loss: 14.57098389, g_loss: 8.14231491\n",
      "Step: [3264] d_loss: 15.50240993, g_loss: 7.97435665\n",
      "Step: [3265] d_loss: 16.37821198, g_loss: 7.77384281\n",
      "Step: [3266] d_loss: 15.65734768, g_loss: 8.17218018\n",
      "Step: [3267] d_loss: 15.71955585, g_loss: 7.67554617\n",
      "Step: [3268] d_loss: 15.73764420, g_loss: 7.73597002\n",
      "Step: [3269] d_loss: 14.81933784, g_loss: 8.01165199\n",
      "Step: [3270] d_loss: 15.50374794, g_loss: 8.04591560\n",
      "Step: [3271] d_loss: 15.40621758, g_loss: 8.40443039\n",
      "Step: [3272] d_loss: 14.94541264, g_loss: 7.80942965\n",
      "Step: [3273] d_loss: 14.76605606, g_loss: 8.35354233\n",
      "Step: [3274] d_loss: 15.71256638, g_loss: 7.57974768\n",
      "Step: [3275] d_loss: 15.29743767, g_loss: 8.30718803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3276] d_loss: 15.46169281, g_loss: 7.75211716\n",
      "Step: [3277] d_loss: 15.42194748, g_loss: 8.04976654\n",
      "Step: [3278] d_loss: 15.21266556, g_loss: 7.72156477\n",
      "Step: [3279] d_loss: 15.22696400, g_loss: 8.46161079\n",
      "Step: [3280] d_loss: 15.74856472, g_loss: 8.14811897\n",
      "Step: [3281] d_loss: 15.05677986, g_loss: 7.73296547\n",
      "Step: [3282] d_loss: 14.66836548, g_loss: 8.35695839\n",
      "Step: [3283] d_loss: 15.60420799, g_loss: 7.37376595\n",
      "Step: [3284] d_loss: 15.64185715, g_loss: 7.35850573\n",
      "Step: [3285] d_loss: 14.32859993, g_loss: 8.79332733\n",
      "Step: [3286] d_loss: 15.22418976, g_loss: 8.29764175\n",
      "Step: [3287] d_loss: 15.87106228, g_loss: 8.04516029\n",
      "Step: [3288] d_loss: 15.16284180, g_loss: 8.25758648\n",
      "Step: [3289] d_loss: 15.11781120, g_loss: 7.87896919\n",
      "Step: [3290] d_loss: 14.88245010, g_loss: 7.69530582\n",
      "Step: [3291] d_loss: 15.40637589, g_loss: 7.97959518\n",
      "Step: [3292] d_loss: 15.18181419, g_loss: 8.63588905\n",
      "Step: [3293] d_loss: 14.78735924, g_loss: 8.24862576\n",
      "Step: [3294] d_loss: 14.73544788, g_loss: 8.42353821\n",
      "Step: [3295] d_loss: 15.73657703, g_loss: 7.43476486\n",
      "Step: [3296] d_loss: 15.49436378, g_loss: 7.88040113\n",
      "Step: [3297] d_loss: 15.17781162, g_loss: 8.29169083\n",
      "Step: [3298] d_loss: 15.50194550, g_loss: 8.15461159\n",
      "Step: [3299] d_loss: 14.62786674, g_loss: 8.49286461\n",
      "Step: [3300] d_loss: 15.00465488, g_loss: 8.08656502\n",
      "Step: [3301] d_loss: 14.89634037, g_loss: 8.11866570\n",
      "Step: [3302] d_loss: 15.02289867, g_loss: 7.90622807\n",
      "Step: [3303] d_loss: 14.84571457, g_loss: 8.04730988\n",
      "Step: [3304] d_loss: 14.81039143, g_loss: 8.12269115\n",
      "Step: [3305] d_loss: 15.22754574, g_loss: 8.74637508\n",
      "Step: [3306] d_loss: 15.69809151, g_loss: 8.43228054\n",
      "Step: [3307] d_loss: 15.20882034, g_loss: 8.52919579\n",
      "Step: [3308] d_loss: 14.79601383, g_loss: 8.54190254\n",
      "Step: [3309] d_loss: 15.35569096, g_loss: 8.01567268\n",
      "Step: [3310] d_loss: 14.89491463, g_loss: 8.47603512\n",
      "Step: [3311] d_loss: 15.50269413, g_loss: 7.86915970\n",
      "Step: [3312] d_loss: 15.47226429, g_loss: 8.44820499\n",
      "Step: [3313] d_loss: 15.33828640, g_loss: 7.89766121\n",
      "Step: [3314] d_loss: 15.33723831, g_loss: 7.97213268\n",
      "Step: [3315] d_loss: 14.76511574, g_loss: 8.62648010\n",
      "Step: [3316] d_loss: 14.96708965, g_loss: 8.19197369\n",
      "Step: [3317] d_loss: 14.84460449, g_loss: 9.10197067\n",
      "Step: [3318] d_loss: 15.54538536, g_loss: 8.62365150\n",
      "Step: [3319] d_loss: 16.14764786, g_loss: 8.30774879\n",
      "Step: [3320] d_loss: 15.98874474, g_loss: 8.67422771\n",
      "Step: [3321] d_loss: 16.16167831, g_loss: 8.34870720\n",
      "Step: [3322] d_loss: 15.08661270, g_loss: 8.02652550\n",
      "Step: [3323] d_loss: 15.37292099, g_loss: 7.85316277\n",
      "Step: [3324] d_loss: 14.81898308, g_loss: 8.33636951\n",
      "Step: [3325] d_loss: 14.94477177, g_loss: 8.21110916\n",
      "Step: [3326] d_loss: 15.51288700, g_loss: 7.88909483\n",
      "Step: [3327] d_loss: 15.79315090, g_loss: 7.68428612\n",
      "Step: [3328] d_loss: 15.99488449, g_loss: 7.68767452\n",
      "Step: [3329] d_loss: 15.51867294, g_loss: 7.67227697\n",
      "Step: [3330] d_loss: 15.22081280, g_loss: 8.45917034\n",
      "Step: [3331] d_loss: 14.83874798, g_loss: 8.43209076\n",
      "Step: [3332] d_loss: 14.98938370, g_loss: 8.33834267\n",
      "Step: [3333] d_loss: 14.32545280, g_loss: 8.54514790\n",
      "Step: [3334] d_loss: 15.29061985, g_loss: 7.90169144\n",
      "Step: [3335] d_loss: 15.79025364, g_loss: 8.78944969\n",
      "Step: [3336] d_loss: 15.89315414, g_loss: 8.65268898\n",
      "Step: [3337] d_loss: 17.36645889, g_loss: 8.35003090\n",
      "Step: [3338] d_loss: 15.62248898, g_loss: 8.20228958\n",
      "Step: [3339] d_loss: 15.12847900, g_loss: 8.55519485\n",
      "Step: [3340] d_loss: 15.00724030, g_loss: 8.00405025\n",
      "Step: [3341] d_loss: 15.64507294, g_loss: 7.85543013\n",
      "Step: [3342] d_loss: 14.80461121, g_loss: 8.68565845\n",
      "Step: [3343] d_loss: 14.75844860, g_loss: 8.66723156\n",
      "Step: [3344] d_loss: 14.59864426, g_loss: 7.95497608\n",
      "Step: [3345] d_loss: 15.36590767, g_loss: 8.51046371\n",
      "Step: [3346] d_loss: 14.96123123, g_loss: 8.44082069\n",
      "Step: [3347] d_loss: 15.61900520, g_loss: 8.00403214\n",
      "Step: [3348] d_loss: 14.74624825, g_loss: 8.02267075\n",
      "Step: [3349] d_loss: 15.08721161, g_loss: 7.81715870\n",
      "Step: [3350] d_loss: 14.89062786, g_loss: 8.51137352\n",
      "Step: [3351] d_loss: 15.06451702, g_loss: 8.04804516\n",
      "Step: [3352] d_loss: 14.23223114, g_loss: 8.74551392\n",
      "Step: [3353] d_loss: 15.19553185, g_loss: 8.17608070\n",
      "Step: [3354] d_loss: 16.01150894, g_loss: 8.42573929\n",
      "Step: [3355] d_loss: 15.97428322, g_loss: 8.57794094\n",
      "Step: [3356] d_loss: 15.45427513, g_loss: 8.05190659\n",
      "Step: [3357] d_loss: 15.86643982, g_loss: 7.93498611\n",
      "Step: [3358] d_loss: 16.59125519, g_loss: 7.84642410\n",
      "Step: [3359] d_loss: 15.62376881, g_loss: 8.20010948\n",
      "Step: [3360] d_loss: 16.54035378, g_loss: 8.30773354\n",
      "Step: [3361] d_loss: 16.49407959, g_loss: 7.82889366\n",
      "Step: [3362] d_loss: 14.99448395, g_loss: 8.12196922\n",
      "Step: [3363] d_loss: 15.79981613, g_loss: 7.52943850\n",
      "Step: [3364] d_loss: 15.85054016, g_loss: 8.24561214\n",
      "Step: [3365] d_loss: 16.35110283, g_loss: 8.21947670\n",
      "Step: [3366] d_loss: 14.77225590, g_loss: 8.88324928\n",
      "Step: [3367] d_loss: 16.60139084, g_loss: 8.66017818\n",
      "Step: [3368] d_loss: 16.29852676, g_loss: 8.82239723\n",
      "Step: [3369] d_loss: 16.45815086, g_loss: 8.48809719\n",
      "Step: [3370] d_loss: 14.93013000, g_loss: 8.25322628\n",
      "Step: [3371] d_loss: 15.40601540, g_loss: 8.04136086\n",
      "Step: [3372] d_loss: 15.06377411, g_loss: 8.02054787\n",
      "Step: [3373] d_loss: 15.50946999, g_loss: 8.12586975\n",
      "Step: [3374] d_loss: 14.68966389, g_loss: 8.74206161\n",
      "Step: [3375] d_loss: 15.42216492, g_loss: 9.09211349\n",
      "Step: [3376] d_loss: 16.47978973, g_loss: 7.82899904\n",
      "Step: [3377] d_loss: 16.21890259, g_loss: 7.36300373\n",
      "Step: [3378] d_loss: 14.84827709, g_loss: 7.85999107\n",
      "Step: [3379] d_loss: 15.54398346, g_loss: 8.13140965\n",
      "Step: [3380] d_loss: 15.47421455, g_loss: 8.19964218\n",
      "Step: [3381] d_loss: 15.67753601, g_loss: 7.68285847\n",
      "Step: [3382] d_loss: 15.32340240, g_loss: 8.02273178\n",
      "Step: [3383] d_loss: 15.34147644, g_loss: 8.18935108\n",
      "Step: [3384] d_loss: 15.13973045, g_loss: 7.84037924\n",
      "Step: [3385] d_loss: 15.08235741, g_loss: 7.80122185\n",
      "Step: [3386] d_loss: 14.65376091, g_loss: 8.44234467\n",
      "Step: [3387] d_loss: 15.50054646, g_loss: 7.87720203\n",
      "Step: [3388] d_loss: 14.92434025, g_loss: 8.00866508\n",
      "Step: [3389] d_loss: 15.37509823, g_loss: 7.67007351\n",
      "Step: [3390] d_loss: 15.46351433, g_loss: 8.37497520\n",
      "Step: [3391] d_loss: 16.55346870, g_loss: 8.64203262\n",
      "Step: [3392] d_loss: 16.09418678, g_loss: 7.92607069\n",
      "Step: [3393] d_loss: 15.50090122, g_loss: 7.85893917\n",
      "Step: [3394] d_loss: 14.40207291, g_loss: 8.72425175\n",
      "Step: [3395] d_loss: 15.94680214, g_loss: 8.16450882\n",
      "Step: [3396] d_loss: 15.84610748, g_loss: 8.43685627\n",
      "Step: [3397] d_loss: 15.70473480, g_loss: 8.04937363\n",
      "Step: [3398] d_loss: 15.38497353, g_loss: 8.05540085\n",
      "Step: [3399] d_loss: 14.77483559, g_loss: 8.01632690\n",
      "Step: [3400] d_loss: 15.48892593, g_loss: 7.78904057\n",
      "Step: [3401] d_loss: 15.35703754, g_loss: 8.34765339\n",
      "Step: [3402] d_loss: 16.15935707, g_loss: 8.91787910\n",
      "Step: [3403] d_loss: 16.19488144, g_loss: 8.42354584\n",
      "Step: [3404] d_loss: 14.95096016, g_loss: 8.20431614\n",
      "Step: [3405] d_loss: 14.87904263, g_loss: 8.01480961\n",
      "Step: [3406] d_loss: 14.82190132, g_loss: 8.54841995\n",
      "Step: [3407] d_loss: 15.22158432, g_loss: 7.95615625\n",
      "Step: [3408] d_loss: 15.24249363, g_loss: 8.39505672\n",
      "Step: [3409] d_loss: 15.37883377, g_loss: 8.03548050\n",
      "Step: [3410] d_loss: 15.35596848, g_loss: 8.24294090\n",
      "Step: [3411] d_loss: 15.22114944, g_loss: 7.88841772\n",
      "Step: [3412] d_loss: 15.32361031, g_loss: 8.04924297\n",
      "Step: [3413] d_loss: 15.15826130, g_loss: 7.53947783\n",
      "Step: [3414] d_loss: 14.86584473, g_loss: 7.68496323\n",
      "Step: [3415] d_loss: 15.65617466, g_loss: 8.10821819\n",
      "Step: [3416] d_loss: 15.27864933, g_loss: 8.46297169\n",
      "Step: [3417] d_loss: 15.20705605, g_loss: 8.03378677\n",
      "Step: [3418] d_loss: 15.08451080, g_loss: 8.34690952\n",
      "Step: [3419] d_loss: 14.93744659, g_loss: 8.20296955\n",
      "Step: [3420] d_loss: 15.11082363, g_loss: 7.68476295\n",
      "Step: [3421] d_loss: 15.47621346, g_loss: 8.00780678\n",
      "Step: [3422] d_loss: 15.00114441, g_loss: 8.01784992\n",
      "Step: [3423] d_loss: 15.10021305, g_loss: 7.72138882\n",
      "Step: [3424] d_loss: 15.01901436, g_loss: 8.13237858\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "    if not y_flag:\n",
    "        # update D network\n",
    "        _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, z: batch_z})\n",
    "        # update G network\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "    else:\n",
    "        _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, y:batch_labels, z: batch_z})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z,y:batch_labels})\n",
    "        _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z,y:batch_labels})\n",
    "        \n",
    "        \n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "        if not y_flag:\n",
    "            samples = sess.run(fake_images, feed_dict={z: sample_z})\n",
    "        else:\n",
    "            samples = sess.run(fake_images, feed_dict={z: sample_z,y:test_labels_onehot})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :, :, :].squeeze()\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
