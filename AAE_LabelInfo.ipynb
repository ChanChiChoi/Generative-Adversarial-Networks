{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the network structure borrow from https://github.com/hwalsuklee/tensorflow-mnist-AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "from math import sin,cos,sqrt\n",
    "from functools import reduce\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net,wscope,bscope,output_depth):\n",
    "    w_init = tf.contrib.layers.xavier_initializer()\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], initializer=w_init)\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=b_init)\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Distribution Functions\n",
    "here define four prior distribution, which will impose onto the hidden code vector of AAE.   \n",
    "Most codes from https://github.com/musyoku/adversarial-autoencoder/blob/master/aae/sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def uniform(batch_size, n_dim, n_labels=10, minv=-1, maxv=1, label_indices=None):\n",
    "    '''if label_indices is None, then uniform will call numpy.random.uniform()'''\n",
    "    if label_indices is not None:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(label, n_labels):\n",
    "            num = int(np.ceil(np.sqrt(n_labels)))\n",
    "            size = (maxv-minv)*1.0/num\n",
    "            x, y = np.random.uniform(-size/2, size/2, (2,))\n",
    "            i = label / num\n",
    "            j = label % num\n",
    "            x += j*size+minv+0.5*size\n",
    "            y += i*size+minv+0.5*size\n",
    "            return np.array([x, y]).reshape((2,))\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "    else:\n",
    "        z = np.random.uniform(minv, maxv, (batch_size, n_dim)).astype(np.float32)\n",
    "    return z\n",
    "\n",
    "def gaussian(batch_size, n_dim, mean=0, var=1, n_labels=10, use_label_info=False):\n",
    "    if use_label_info:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(n_labels):\n",
    "            x, y = np.random.normal(mean, var, (2,))\n",
    "            angle = np.angle((x-mean) + 1j*(y-mean), deg=True)\n",
    "            dist = np.sqrt((x-mean)**2+(y-mean)**2)\n",
    "\n",
    "            # label 0\n",
    "            if dist <1.0:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = ((int)((n_labels-1)*angle))//360\n",
    "\n",
    "                if label<0:\n",
    "                    label+=n_labels-1\n",
    "\n",
    "                label += 1\n",
    "\n",
    "            return np.array([x, y]).reshape((2,)), label\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        z_id = np.empty((batch_size), dtype=np.int32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    a_sample, a_label = sample(n_labels)\n",
    "                    z[batch, zi*2:zi*2+2] = a_sample\n",
    "                    z_id[batch] = a_label\n",
    "        return z, z_id\n",
    "    else:\n",
    "        z = np.random.normal(mean, var, (batch_size, n_dim)).astype(np.float32)\n",
    "        return z\n",
    "\n",
    "def gaussian_mixture(batch_size, n_dim=2, n_labels=10, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z\n",
    "\n",
    "def swiss_roll(batch_size, n_dim=2, n_labels=10, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(label, n_labels):\n",
    "        uni = np.random.uniform(0.0, 1.0) / float(n_labels) + float(label) / float(n_labels)\n",
    "        r = sqrt(uni) * 3.0\n",
    "        rad = np.pi * 4.0 * sqrt(uni)\n",
    "        x = r * cos(rad)\n",
    "        y = r * sin(rad)\n",
    "        return np.array([x, y]).reshape((2,))\n",
    "\n",
    "    z = np.zeros((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(np.random.randint(0, n_labels), n_labels)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the discriminator part of the AAE is one MLP network, two hidden fully connected layers, which input is [batch_size, z] matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout''' \n",
    "        net = linear(z, 'd_wlinear0', 'd_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'd_wlinear1', 'd_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'd_wlinear2', 'd_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_encoder(x, n_hidden, n_output, keep_prob, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"AE_encoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout'''\n",
    "        net = linear(x, 'aeE_wlinear0', 'aeE_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeE_wlinear1', 'aeE_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        z = out_logit = linear(net, 'aeE_wlinear2', 'aeE_blinear2', n_output)\n",
    "\n",
    "    return z\n",
    "\n",
    "def AE_decoder(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"AE_decoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropput'''\n",
    "        net = linear(z, 'aeD_wlinear0', 'aeD_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeD_wlinear1', 'aeD_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'aeD_wlinear2', 'aeD_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        x_pred = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, img_dim, n_hidden):\n",
    "\n",
    "    y = AE_decoder(z, n_hidden, img_dim, 1.0, reuse=True)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "results_path = './result'\n",
    "n_hidden = 1000  # number of hidden units in MLP\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.5\n",
    "n_epochs = 20\n",
    "prior_type = 'mixGaussian' # one of ['mixGaussian', 'swiss_roll', 'normal']\n",
    "batch_size = 64\n",
    "min_tot_loss = 1e99\n",
    "\n",
    "image_dims = [28, 28, 1]\n",
    "img_dim = reduce(lambda x,y:x*y, image_dims)\n",
    "z_dim = 2\n",
    "y_dim = 10\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# In denoising-autoencoder, x_hat == x + noise; otherwise x_hat == x\n",
    "x_hat = tf.placeholder(tf.float32, shape=[None, img_dim], name='input_img')\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_dim], name='target_img')\n",
    "x_id = tf.placeholder(tf.float32, shape=[None, y_dim], name='input_img_label')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# input for PMLR\n",
    "z_in = tf.placeholder(tf.float32, shape=[None, z_dim], name='latent_variable')\n",
    "\n",
    "# samples drawn from prior distribution\n",
    "z_sample = tf.placeholder(tf.float32, shape=[None, z_dim], name='prior_sample')\n",
    "z_id = tf.placeholder(tf.float32, shape=[None, y_dim], name='prior_sample_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "'''Reconstruction Loss''' \n",
    "\n",
    "# encoding\n",
    "z = AE_encoder(x_hat, n_hidden, z_dim, keep_prob)\n",
    "# decoding\n",
    "x_pred = AE_decoder(z, n_hidden, img_dim, keep_prob)\n",
    "# loss\n",
    "marginal_likelihood = -tf.reduce_mean(tf.reduce_mean(tf.squared_difference(x, x_pred)))\n",
    "\n",
    "'''GAN Loss'''\n",
    "z_real = tf.concat([z_sample, z_id],1)\n",
    "z_fake = tf.concat([z, x_id],1)\n",
    "\n",
    "D_real, D_real_logits = discriminator(z_real, n_hidden, 1, keep_prob)\n",
    "D_fake, D_fake_logits = discriminator(z_fake, n_hidden, 1, keep_prob, reuse=True)\n",
    "\n",
    "# discriminator loss\n",
    "D_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n",
    "D_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n",
    "D_loss = D_loss_real+D_loss_fake\n",
    "\n",
    "# generator loss\n",
    "G_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))\n",
    "\n",
    "marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "D_loss = tf.reduce_mean(D_loss)\n",
    "G_loss = tf.reduce_mean(G_loss)\n",
    "\n",
    "neg_marginal_likelihood = -1*marginal_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G and a group for ae\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "g_vars = [var for var in t_vars if 'AE_encoder' in var.name]\n",
    "ae_vars = [var for var in t_vars if 'AE_encoder' in var.name or 'AE_decoder' in var.name]\n",
    "\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate/5, beta1=beta1).minimize(D_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(G_loss, var_list=g_vars)\n",
    "ae_optim = tf.train.AdamOptimizer(learning_rate).minimize(neg_marginal_likelihood, var_list=ae_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "\n",
    "# for test\n",
    "test_image_batch = mnist.test.next_batch(batch_size)\n",
    "test_batch_images,test_batch_labels = test_image_batch\n",
    "test_batch_images = test_batch_images.astype(np.float32)\n",
    "fake_images = x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] total_loss: 2.29625130 d_loss: 1.40348506, g_loss: 0.66248554, ae_loss: 0.23028077\n",
      "Step: [1] total_loss: 2.52903676 d_loss: 1.57928801, g_loss: 0.73006737, ae_loss: 0.21968141\n",
      "Step: [2] total_loss: 2.40931654 d_loss: 1.40159893, g_loss: 0.80839920, ae_loss: 0.19931845\n",
      "Step: [3] total_loss: 2.31808829 d_loss: 1.32486629, g_loss: 0.79757977, ae_loss: 0.19564222\n",
      "Step: [4] total_loss: 2.26216650 d_loss: 1.31306601, g_loss: 0.74330103, ae_loss: 0.20579936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3/lib/python3.6/site-packages/ipykernel_launcher.py:65: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5] total_loss: 2.27918625 d_loss: 1.35693765, g_loss: 0.70432574, ae_loss: 0.21792276\n",
      "Step: [6] total_loss: 2.23101330 d_loss: 1.40176880, g_loss: 0.61029613, ae_loss: 0.21894826\n",
      "Step: [7] total_loss: 2.30303645 d_loss: 1.67069876, g_loss: 0.45909807, ae_loss: 0.17323953\n",
      "Step: [8] total_loss: 2.88357735 d_loss: 2.19079661, g_loss: 0.60505724, ae_loss: 0.08772358\n",
      "Step: [9] total_loss: 3.31606770 d_loss: 1.64511609, g_loss: 1.56656361, ae_loss: 0.10438785\n",
      "Step: [10] total_loss: 2.22205544 d_loss: 1.15573168, g_loss: 0.98725367, ae_loss: 0.07907015\n",
      "Step: [11] total_loss: 2.20523119 d_loss: 1.34847641, g_loss: 0.72013080, ae_loss: 0.13662404\n",
      "Step: [12] total_loss: 2.30001020 d_loss: 1.53762305, g_loss: 0.65759659, ae_loss: 0.10479064\n",
      "Step: [13] total_loss: 2.44295168 d_loss: 1.54072571, g_loss: 0.82907319, ae_loss: 0.07315284\n",
      "Step: [14] total_loss: 2.28643394 d_loss: 1.42079663, g_loss: 0.78589058, ae_loss: 0.07974673\n",
      "Step: [15] total_loss: 2.36841869 d_loss: 1.44962740, g_loss: 0.76504099, ae_loss: 0.15375029\n",
      "Step: [16] total_loss: 2.34807253 d_loss: 1.44484890, g_loss: 0.76729989, ae_loss: 0.13592376\n",
      "Step: [17] total_loss: 2.31638908 d_loss: 1.44074178, g_loss: 0.75694084, ae_loss: 0.11870655\n",
      "Step: [18] total_loss: 2.31390071 d_loss: 1.44186151, g_loss: 0.75611943, ae_loss: 0.11591965\n",
      "Step: [19] total_loss: 2.28315449 d_loss: 1.42931485, g_loss: 0.74762481, ae_loss: 0.10621487\n",
      "Step: [20] total_loss: 2.25785923 d_loss: 1.42113304, g_loss: 0.74168849, ae_loss: 0.09503768\n",
      "Step: [21] total_loss: 2.22054911 d_loss: 1.40336740, g_loss: 0.73667204, ae_loss: 0.08050963\n",
      "Step: [22] total_loss: 2.23027515 d_loss: 1.41652250, g_loss: 0.73404270, ae_loss: 0.07970990\n",
      "Step: [23] total_loss: 2.20595741 d_loss: 1.39901328, g_loss: 0.73279560, ae_loss: 0.07414842\n",
      "Step: [24] total_loss: 2.20415521 d_loss: 1.40731418, g_loss: 0.72534800, ae_loss: 0.07149299\n",
      "Step: [25] total_loss: 2.19577932 d_loss: 1.39497316, g_loss: 0.73006415, ae_loss: 0.07074208\n",
      "Step: [26] total_loss: 2.16742945 d_loss: 1.38326573, g_loss: 0.72119892, ae_loss: 0.06296483\n",
      "Step: [27] total_loss: 2.16480017 d_loss: 1.38576734, g_loss: 0.70831215, ae_loss: 0.07072078\n",
      "Step: [28] total_loss: 2.18592858 d_loss: 1.39751434, g_loss: 0.71717381, ae_loss: 0.07124037\n",
      "Step: [29] total_loss: 2.15751147 d_loss: 1.37796664, g_loss: 0.70721620, ae_loss: 0.07232863\n",
      "Step: [30] total_loss: 2.14865923 d_loss: 1.36532569, g_loss: 0.71134317, ae_loss: 0.07199036\n",
      "Step: [31] total_loss: 2.14652300 d_loss: 1.37131524, g_loss: 0.69838077, ae_loss: 0.07682697\n",
      "Step: [32] total_loss: 2.13329959 d_loss: 1.36652374, g_loss: 0.69732535, ae_loss: 0.06945048\n",
      "Step: [33] total_loss: 2.12548828 d_loss: 1.35694623, g_loss: 0.69246328, ae_loss: 0.07607866\n",
      "Step: [34] total_loss: 2.11988306 d_loss: 1.36061382, g_loss: 0.68970597, ae_loss: 0.06956338\n",
      "Step: [35] total_loss: 2.12767053 d_loss: 1.36288214, g_loss: 0.69533610, ae_loss: 0.06945229\n",
      "Step: [36] total_loss: 2.11324143 d_loss: 1.35858703, g_loss: 0.68078852, ae_loss: 0.07386587\n",
      "Step: [37] total_loss: 2.11657381 d_loss: 1.36028409, g_loss: 0.69221461, ae_loss: 0.06407511\n",
      "Step: [38] total_loss: 2.13943815 d_loss: 1.36239779, g_loss: 0.70153570, ae_loss: 0.07550452\n",
      "Step: [39] total_loss: 2.10914969 d_loss: 1.34574389, g_loss: 0.69258064, ae_loss: 0.07082511\n",
      "Step: [40] total_loss: 2.11779547 d_loss: 1.36043906, g_loss: 0.68717527, ae_loss: 0.07018103\n",
      "Step: [41] total_loss: 2.12961721 d_loss: 1.37698472, g_loss: 0.68000317, ae_loss: 0.07262921\n",
      "Step: [42] total_loss: 2.11295438 d_loss: 1.37041545, g_loss: 0.67424709, ae_loss: 0.06829188\n",
      "Step: [43] total_loss: 2.11778879 d_loss: 1.36022305, g_loss: 0.68009067, ae_loss: 0.07747519\n",
      "Step: [44] total_loss: 2.11584234 d_loss: 1.35072303, g_loss: 0.69278210, ae_loss: 0.07233709\n",
      "Step: [45] total_loss: 2.11891937 d_loss: 1.36596334, g_loss: 0.68334502, ae_loss: 0.06961096\n",
      "Step: [46] total_loss: 2.13262796 d_loss: 1.36723816, g_loss: 0.69453502, ae_loss: 0.07085467\n",
      "Step: [47] total_loss: 2.10122919 d_loss: 1.35390425, g_loss: 0.67781079, ae_loss: 0.06951431\n",
      "Step: [48] total_loss: 2.10639668 d_loss: 1.36456966, g_loss: 0.67370439, ae_loss: 0.06812279\n",
      "Step: [49] total_loss: 2.08836317 d_loss: 1.35289931, g_loss: 0.66909277, ae_loss: 0.06637109\n",
      "Step: [50] total_loss: 2.13082218 d_loss: 1.36880326, g_loss: 0.69061089, ae_loss: 0.07140797\n",
      "Step: [51] total_loss: 2.12024546 d_loss: 1.36433971, g_loss: 0.68542284, ae_loss: 0.07048296\n",
      "Step: [52] total_loss: 2.11000776 d_loss: 1.36032677, g_loss: 0.68076587, ae_loss: 0.06891505\n",
      "Step: [53] total_loss: 2.11508179 d_loss: 1.36884654, g_loss: 0.67606688, ae_loss: 0.07016853\n",
      "Step: [54] total_loss: 2.13886309 d_loss: 1.37880445, g_loss: 0.68784177, ae_loss: 0.07221674\n",
      "Step: [55] total_loss: 2.11482906 d_loss: 1.36995554, g_loss: 0.67916000, ae_loss: 0.06571347\n",
      "Step: [56] total_loss: 2.12130547 d_loss: 1.37419534, g_loss: 0.67792791, ae_loss: 0.06918231\n",
      "Step: [57] total_loss: 2.11670399 d_loss: 1.35465157, g_loss: 0.69162261, ae_loss: 0.07042971\n",
      "Step: [58] total_loss: 2.12558055 d_loss: 1.36316419, g_loss: 0.69138217, ae_loss: 0.07103417\n",
      "Step: [59] total_loss: 2.11518168 d_loss: 1.35365105, g_loss: 0.69555140, ae_loss: 0.06597922\n",
      "Step: [60] total_loss: 2.11806464 d_loss: 1.36241341, g_loss: 0.68743140, ae_loss: 0.06821991\n",
      "Step: [61] total_loss: 2.11621141 d_loss: 1.36024785, g_loss: 0.68607867, ae_loss: 0.06988491\n",
      "Step: [62] total_loss: 2.11193085 d_loss: 1.36932087, g_loss: 0.67821050, ae_loss: 0.06439954\n",
      "Step: [63] total_loss: 2.12699795 d_loss: 1.37457407, g_loss: 0.68671286, ae_loss: 0.06571106\n",
      "Step: [64] total_loss: 2.12971807 d_loss: 1.36217892, g_loss: 0.70103151, ae_loss: 0.06650760\n",
      "Step: [65] total_loss: 2.10910535 d_loss: 1.34696388, g_loss: 0.69414192, ae_loss: 0.06799956\n",
      "Step: [66] total_loss: 2.13213062 d_loss: 1.38615918, g_loss: 0.67765570, ae_loss: 0.06831577\n",
      "Step: [67] total_loss: 2.12049294 d_loss: 1.36309326, g_loss: 0.69052786, ae_loss: 0.06687190\n",
      "Step: [68] total_loss: 2.13362765 d_loss: 1.37431002, g_loss: 0.69033676, ae_loss: 0.06898085\n",
      "Step: [69] total_loss: 2.13786483 d_loss: 1.37794697, g_loss: 0.68934977, ae_loss: 0.07056808\n",
      "Step: [70] total_loss: 2.13318133 d_loss: 1.37081909, g_loss: 0.69871914, ae_loss: 0.06364314\n",
      "Step: [71] total_loss: 2.11775994 d_loss: 1.35724187, g_loss: 0.69544089, ae_loss: 0.06507716\n",
      "Step: [72] total_loss: 2.12671328 d_loss: 1.36761856, g_loss: 0.69453597, ae_loss: 0.06455858\n",
      "Step: [73] total_loss: 2.12722516 d_loss: 1.35359073, g_loss: 0.70924336, ae_loss: 0.06439099\n",
      "Step: [74] total_loss: 2.12913370 d_loss: 1.37076581, g_loss: 0.69101381, ae_loss: 0.06735399\n",
      "Step: [75] total_loss: 2.11928153 d_loss: 1.35881591, g_loss: 0.69089508, ae_loss: 0.06957056\n",
      "Step: [76] total_loss: 2.13609028 d_loss: 1.37467408, g_loss: 0.69596851, ae_loss: 0.06544776\n",
      "Step: [77] total_loss: 2.12268257 d_loss: 1.36711240, g_loss: 0.68809104, ae_loss: 0.06747901\n",
      "Step: [78] total_loss: 2.11738658 d_loss: 1.36235619, g_loss: 0.69176823, ae_loss: 0.06326211\n",
      "Step: [79] total_loss: 2.12182570 d_loss: 1.36982918, g_loss: 0.68237913, ae_loss: 0.06961749\n",
      "Step: [80] total_loss: 2.14107776 d_loss: 1.37313604, g_loss: 0.69490391, ae_loss: 0.07303777\n",
      "Step: [81] total_loss: 2.12312984 d_loss: 1.36580014, g_loss: 0.68143487, ae_loss: 0.07589471\n",
      "Step: [82] total_loss: 2.13148212 d_loss: 1.37500775, g_loss: 0.68604958, ae_loss: 0.07042463\n",
      "Step: [83] total_loss: 2.12645364 d_loss: 1.37894797, g_loss: 0.67919260, ae_loss: 0.06831317\n",
      "Step: [84] total_loss: 2.11975527 d_loss: 1.35368085, g_loss: 0.69906062, ae_loss: 0.06701372\n",
      "Step: [85] total_loss: 2.11011052 d_loss: 1.35620356, g_loss: 0.68530393, ae_loss: 0.06860310\n",
      "Step: [86] total_loss: 2.10981631 d_loss: 1.35130990, g_loss: 0.68950945, ae_loss: 0.06899699\n",
      "Step: [87] total_loss: 2.13965845 d_loss: 1.37501395, g_loss: 0.69203234, ae_loss: 0.07261204\n",
      "Step: [88] total_loss: 2.12788963 d_loss: 1.37405026, g_loss: 0.68754923, ae_loss: 0.06629018\n",
      "Step: [89] total_loss: 2.11247182 d_loss: 1.34432459, g_loss: 0.70007801, ae_loss: 0.06806923\n",
      "Step: [90] total_loss: 2.13163137 d_loss: 1.38258708, g_loss: 0.68139935, ae_loss: 0.06764507\n",
      "Step: [91] total_loss: 2.13921070 d_loss: 1.38296247, g_loss: 0.68982625, ae_loss: 0.06642182\n",
      "Step: [92] total_loss: 2.14354396 d_loss: 1.38174951, g_loss: 0.69492072, ae_loss: 0.06687373\n",
      "Step: [93] total_loss: 2.14241123 d_loss: 1.37422836, g_loss: 0.69831234, ae_loss: 0.06987056\n",
      "Step: [94] total_loss: 2.13397694 d_loss: 1.36835408, g_loss: 0.70156801, ae_loss: 0.06405476\n",
      "Step: [95] total_loss: 2.13187718 d_loss: 1.36515069, g_loss: 0.70527726, ae_loss: 0.06144927\n",
      "Step: [96] total_loss: 2.12536931 d_loss: 1.37491417, g_loss: 0.68675882, ae_loss: 0.06369622\n",
      "Step: [97] total_loss: 2.13553619 d_loss: 1.38224912, g_loss: 0.68693292, ae_loss: 0.06635427\n",
      "Step: [98] total_loss: 2.12320423 d_loss: 1.36879098, g_loss: 0.69026268, ae_loss: 0.06415056\n",
      "Step: [99] total_loss: 2.11743569 d_loss: 1.35871172, g_loss: 0.69394821, ae_loss: 0.06477565\n",
      "Step: [100] total_loss: 2.12638903 d_loss: 1.37433577, g_loss: 0.68469954, ae_loss: 0.06735361\n",
      "Step: [101] total_loss: 2.12386656 d_loss: 1.36447585, g_loss: 0.68943322, ae_loss: 0.06995732\n",
      "Step: [102] total_loss: 2.11665416 d_loss: 1.35806346, g_loss: 0.69286263, ae_loss: 0.06572812\n",
      "Step: [103] total_loss: 2.11438346 d_loss: 1.36217868, g_loss: 0.68545872, ae_loss: 0.06674615\n",
      "Step: [104] total_loss: 2.13897610 d_loss: 1.37802911, g_loss: 0.69592386, ae_loss: 0.06502312\n",
      "Step: [105] total_loss: 2.13265014 d_loss: 1.37755609, g_loss: 0.68913746, ae_loss: 0.06595665\n",
      "Step: [106] total_loss: 2.13194799 d_loss: 1.35811818, g_loss: 0.70652086, ae_loss: 0.06730885\n",
      "Step: [107] total_loss: 2.13784003 d_loss: 1.36537504, g_loss: 0.70438915, ae_loss: 0.06807588\n",
      "Step: [108] total_loss: 2.10991406 d_loss: 1.35992002, g_loss: 0.68775845, ae_loss: 0.06223559\n",
      "Step: [109] total_loss: 2.11578417 d_loss: 1.36291885, g_loss: 0.68661600, ae_loss: 0.06624924\n",
      "Step: [110] total_loss: 2.11686134 d_loss: 1.37761557, g_loss: 0.67278451, ae_loss: 0.06646127\n",
      "Step: [111] total_loss: 2.10752845 d_loss: 1.35350037, g_loss: 0.68954068, ae_loss: 0.06448730\n",
      "Step: [112] total_loss: 2.12466741 d_loss: 1.36554885, g_loss: 0.69203025, ae_loss: 0.06708833\n",
      "Step: [113] total_loss: 2.13058043 d_loss: 1.35490608, g_loss: 0.71007389, ae_loss: 0.06560054\n",
      "Step: [114] total_loss: 2.12278032 d_loss: 1.35625792, g_loss: 0.70102084, ae_loss: 0.06550173\n",
      "Step: [115] total_loss: 2.12188983 d_loss: 1.35655475, g_loss: 0.69888717, ae_loss: 0.06644785\n",
      "Step: [116] total_loss: 2.12363911 d_loss: 1.36227369, g_loss: 0.68982792, ae_loss: 0.07153738\n",
      "Step: [117] total_loss: 2.13043594 d_loss: 1.37617254, g_loss: 0.69042861, ae_loss: 0.06383473\n",
      "Step: [118] total_loss: 2.12714720 d_loss: 1.36360788, g_loss: 0.69663936, ae_loss: 0.06690005\n",
      "Step: [119] total_loss: 2.11980391 d_loss: 1.37290287, g_loss: 0.67873919, ae_loss: 0.06816183\n",
      "Step: [120] total_loss: 2.09467125 d_loss: 1.34203517, g_loss: 0.68818653, ae_loss: 0.06444972\n",
      "Step: [121] total_loss: 2.13145232 d_loss: 1.36950469, g_loss: 0.69605380, ae_loss: 0.06589393\n",
      "Step: [122] total_loss: 2.11197472 d_loss: 1.34762180, g_loss: 0.69559008, ae_loss: 0.06876294\n",
      "Step: [123] total_loss: 2.12249422 d_loss: 1.36492944, g_loss: 0.69106960, ae_loss: 0.06649519\n",
      "Step: [124] total_loss: 2.11061263 d_loss: 1.34299695, g_loss: 0.70618981, ae_loss: 0.06142595\n",
      "Step: [125] total_loss: 2.12833810 d_loss: 1.37452459, g_loss: 0.68712556, ae_loss: 0.06668797\n",
      "Step: [126] total_loss: 2.12436104 d_loss: 1.37301493, g_loss: 0.68727487, ae_loss: 0.06407133\n",
      "Step: [127] total_loss: 2.12832403 d_loss: 1.35659301, g_loss: 0.70754671, ae_loss: 0.06418417\n",
      "Step: [128] total_loss: 2.12402678 d_loss: 1.35543275, g_loss: 0.70200098, ae_loss: 0.06659299\n",
      "Step: [129] total_loss: 2.14370465 d_loss: 1.37965190, g_loss: 0.69829088, ae_loss: 0.06576192\n",
      "Step: [130] total_loss: 2.11545920 d_loss: 1.35611212, g_loss: 0.69199723, ae_loss: 0.06734977\n",
      "Step: [131] total_loss: 2.14176702 d_loss: 1.38895488, g_loss: 0.68751907, ae_loss: 0.06529296\n",
      "Step: [132] total_loss: 2.14105988 d_loss: 1.37170446, g_loss: 0.69967723, ae_loss: 0.06967806\n",
      "Step: [133] total_loss: 2.12205100 d_loss: 1.36445057, g_loss: 0.69269800, ae_loss: 0.06490244\n",
      "Step: [134] total_loss: 2.11286449 d_loss: 1.36143064, g_loss: 0.68918002, ae_loss: 0.06225396\n",
      "Step: [135] total_loss: 2.10759735 d_loss: 1.34964943, g_loss: 0.69275022, ae_loss: 0.06519780\n",
      "Step: [136] total_loss: 2.12739611 d_loss: 1.37324238, g_loss: 0.69510925, ae_loss: 0.05904434\n",
      "Step: [137] total_loss: 2.12125039 d_loss: 1.36770272, g_loss: 0.68948221, ae_loss: 0.06406551\n",
      "Step: [138] total_loss: 2.11628366 d_loss: 1.36314714, g_loss: 0.68675816, ae_loss: 0.06637838\n",
      "Step: [139] total_loss: 2.12553358 d_loss: 1.35363913, g_loss: 0.70977044, ae_loss: 0.06212407\n",
      "Step: [140] total_loss: 2.12329173 d_loss: 1.36361289, g_loss: 0.69779634, ae_loss: 0.06188250\n",
      "Step: [141] total_loss: 2.12365961 d_loss: 1.37098944, g_loss: 0.68909502, ae_loss: 0.06357531\n",
      "Step: [142] total_loss: 2.12542248 d_loss: 1.36404109, g_loss: 0.69286084, ae_loss: 0.06852053\n",
      "Step: [143] total_loss: 2.14130878 d_loss: 1.38454866, g_loss: 0.69086027, ae_loss: 0.06589997\n",
      "Step: [144] total_loss: 2.11737895 d_loss: 1.35968137, g_loss: 0.69310510, ae_loss: 0.06459243\n",
      "Step: [145] total_loss: 2.14815950 d_loss: 1.39988518, g_loss: 0.68310130, ae_loss: 0.06517314\n",
      "Step: [146] total_loss: 2.11404157 d_loss: 1.36018729, g_loss: 0.69026136, ae_loss: 0.06359291\n",
      "Step: [147] total_loss: 2.13171434 d_loss: 1.37579751, g_loss: 0.69391203, ae_loss: 0.06200471\n",
      "Step: [148] total_loss: 2.11729813 d_loss: 1.36106491, g_loss: 0.69549918, ae_loss: 0.06073407\n",
      "Step: [149] total_loss: 2.12141943 d_loss: 1.37968874, g_loss: 0.67727017, ae_loss: 0.06446066\n",
      "Step: [150] total_loss: 2.11951685 d_loss: 1.37930417, g_loss: 0.67706072, ae_loss: 0.06315203\n",
      "Step: [151] total_loss: 2.11948991 d_loss: 1.35920262, g_loss: 0.69650418, ae_loss: 0.06378306\n",
      "Step: [152] total_loss: 2.12636566 d_loss: 1.37412226, g_loss: 0.68758869, ae_loss: 0.06465460\n",
      "Step: [153] total_loss: 2.11284065 d_loss: 1.35152090, g_loss: 0.70353997, ae_loss: 0.05777977\n",
      "Step: [154] total_loss: 2.13499117 d_loss: 1.37712955, g_loss: 0.69286823, ae_loss: 0.06499352\n",
      "Step: [155] total_loss: 2.12081385 d_loss: 1.35704112, g_loss: 0.69976890, ae_loss: 0.06400394\n",
      "Step: [156] total_loss: 2.14044881 d_loss: 1.38012922, g_loss: 0.69758713, ae_loss: 0.06273242\n",
      "Step: [157] total_loss: 2.11064672 d_loss: 1.36625350, g_loss: 0.68363434, ae_loss: 0.06075883\n",
      "Step: [158] total_loss: 2.11601639 d_loss: 1.36818814, g_loss: 0.68593740, ae_loss: 0.06189071\n",
      "Step: [159] total_loss: 2.12074161 d_loss: 1.37771606, g_loss: 0.67997879, ae_loss: 0.06304678\n",
      "Step: [160] total_loss: 2.12774134 d_loss: 1.37205815, g_loss: 0.69088113, ae_loss: 0.06480187\n",
      "Step: [161] total_loss: 2.13899064 d_loss: 1.39998651, g_loss: 0.67343128, ae_loss: 0.06557286\n",
      "Step: [162] total_loss: 2.11496735 d_loss: 1.35984981, g_loss: 0.69228125, ae_loss: 0.06283619\n",
      "Step: [163] total_loss: 2.11015916 d_loss: 1.35931599, g_loss: 0.68609929, ae_loss: 0.06474393\n",
      "Step: [164] total_loss: 2.15021920 d_loss: 1.38478804, g_loss: 0.70380658, ae_loss: 0.06162467\n",
      "Step: [165] total_loss: 2.12231541 d_loss: 1.37804270, g_loss: 0.68333697, ae_loss: 0.06093584\n",
      "Step: [166] total_loss: 2.13590527 d_loss: 1.38178587, g_loss: 0.69154620, ae_loss: 0.06257316\n",
      "Step: [167] total_loss: 2.10873175 d_loss: 1.35731328, g_loss: 0.68881792, ae_loss: 0.06260061\n",
      "Step: [168] total_loss: 2.10426092 d_loss: 1.35943365, g_loss: 0.68025970, ae_loss: 0.06456770\n",
      "Step: [169] total_loss: 2.12031889 d_loss: 1.37158215, g_loss: 0.68579942, ae_loss: 0.06293731\n",
      "Step: [170] total_loss: 2.11638355 d_loss: 1.36106515, g_loss: 0.69136524, ae_loss: 0.06395309\n",
      "Step: [171] total_loss: 2.13144255 d_loss: 1.37894559, g_loss: 0.68925899, ae_loss: 0.06323787\n",
      "Step: [172] total_loss: 2.11605501 d_loss: 1.36231327, g_loss: 0.69065428, ae_loss: 0.06308762\n",
      "Step: [173] total_loss: 2.11774874 d_loss: 1.35246515, g_loss: 0.69871557, ae_loss: 0.06656791\n",
      "Step: [174] total_loss: 2.11618137 d_loss: 1.37216806, g_loss: 0.68025720, ae_loss: 0.06375623\n",
      "Step: [175] total_loss: 2.12762594 d_loss: 1.37627316, g_loss: 0.68463492, ae_loss: 0.06671782\n",
      "Step: [176] total_loss: 2.12114882 d_loss: 1.37506652, g_loss: 0.68529302, ae_loss: 0.06078923\n",
      "Step: [177] total_loss: 2.13389587 d_loss: 1.38468337, g_loss: 0.68509007, ae_loss: 0.06412260\n",
      "Step: [178] total_loss: 2.13082886 d_loss: 1.37861955, g_loss: 0.69094872, ae_loss: 0.06126059\n",
      "Step: [179] total_loss: 2.10822487 d_loss: 1.36665940, g_loss: 0.68056405, ae_loss: 0.06100128\n",
      "Step: [180] total_loss: 2.14239836 d_loss: 1.38210535, g_loss: 0.69453239, ae_loss: 0.06576067\n",
      "Step: [181] total_loss: 2.13359571 d_loss: 1.37630129, g_loss: 0.69101071, ae_loss: 0.06628366\n",
      "Step: [182] total_loss: 2.12042236 d_loss: 1.35431075, g_loss: 0.70183861, ae_loss: 0.06427307\n",
      "Step: [183] total_loss: 2.12871933 d_loss: 1.40055501, g_loss: 0.66134936, ae_loss: 0.06681491\n",
      "Step: [184] total_loss: 2.10656452 d_loss: 1.37425470, g_loss: 0.67189527, ae_loss: 0.06041449\n",
      "Step: [185] total_loss: 2.10865402 d_loss: 1.36565137, g_loss: 0.67673969, ae_loss: 0.06626304\n",
      "Step: [186] total_loss: 2.12499142 d_loss: 1.37519836, g_loss: 0.68657821, ae_loss: 0.06321493\n",
      "Step: [187] total_loss: 2.12132430 d_loss: 1.37218380, g_loss: 0.68741763, ae_loss: 0.06172284\n",
      "Step: [188] total_loss: 2.12280560 d_loss: 1.35972929, g_loss: 0.69954598, ae_loss: 0.06353039\n",
      "Step: [189] total_loss: 2.12332535 d_loss: 1.37551641, g_loss: 0.68330246, ae_loss: 0.06450657\n",
      "Step: [190] total_loss: 2.11601067 d_loss: 1.37444496, g_loss: 0.68128616, ae_loss: 0.06027951\n",
      "Step: [191] total_loss: 2.11957502 d_loss: 1.37713814, g_loss: 0.67910254, ae_loss: 0.06333435\n",
      "Step: [192] total_loss: 2.11358547 d_loss: 1.35629201, g_loss: 0.69286489, ae_loss: 0.06442851\n",
      "Step: [193] total_loss: 2.12725067 d_loss: 1.37262774, g_loss: 0.68990266, ae_loss: 0.06472029\n",
      "Step: [194] total_loss: 2.10504198 d_loss: 1.35367727, g_loss: 0.68613744, ae_loss: 0.06522738\n",
      "Step: [195] total_loss: 2.13340807 d_loss: 1.37292731, g_loss: 0.69354653, ae_loss: 0.06693412\n",
      "Step: [196] total_loss: 2.14086390 d_loss: 1.38112664, g_loss: 0.69468307, ae_loss: 0.06505404\n",
      "Step: [197] total_loss: 2.13351989 d_loss: 1.37625003, g_loss: 0.69559443, ae_loss: 0.06167547\n",
      "Step: [198] total_loss: 2.11139345 d_loss: 1.36881518, g_loss: 0.68230569, ae_loss: 0.06027257\n",
      "Step: [199] total_loss: 2.10654879 d_loss: 1.34407139, g_loss: 0.69825625, ae_loss: 0.06422129\n",
      "Step: [200] total_loss: 2.11869812 d_loss: 1.37433970, g_loss: 0.68344724, ae_loss: 0.06091121\n",
      "Step: [201] total_loss: 2.11352634 d_loss: 1.36062181, g_loss: 0.69064617, ae_loss: 0.06225827\n",
      "Step: [202] total_loss: 2.10506630 d_loss: 1.36057854, g_loss: 0.68485737, ae_loss: 0.05963052\n",
      "Step: [203] total_loss: 2.12331510 d_loss: 1.37102723, g_loss: 0.69100565, ae_loss: 0.06128230\n",
      "Step: [204] total_loss: 2.12710762 d_loss: 1.36270905, g_loss: 0.70380676, ae_loss: 0.06059197\n",
      "Step: [205] total_loss: 2.10981369 d_loss: 1.36913705, g_loss: 0.68084812, ae_loss: 0.05982837\n",
      "Step: [206] total_loss: 2.13663530 d_loss: 1.38228691, g_loss: 0.69452590, ae_loss: 0.05982246\n",
      "Step: [207] total_loss: 2.14217067 d_loss: 1.37452197, g_loss: 0.70210290, ae_loss: 0.06554579\n",
      "Step: [208] total_loss: 2.13598442 d_loss: 1.38478696, g_loss: 0.68857270, ae_loss: 0.06262465\n",
      "Step: [209] total_loss: 2.13493514 d_loss: 1.37139297, g_loss: 0.69914997, ae_loss: 0.06439216\n",
      "Step: [210] total_loss: 2.13559771 d_loss: 1.38385415, g_loss: 0.68453789, ae_loss: 0.06720555\n",
      "Step: [211] total_loss: 2.11527348 d_loss: 1.36956692, g_loss: 0.68227363, ae_loss: 0.06343291\n",
      "Step: [212] total_loss: 2.11383605 d_loss: 1.36901867, g_loss: 0.68363529, ae_loss: 0.06118208\n",
      "Step: [213] total_loss: 2.12035012 d_loss: 1.38460469, g_loss: 0.67399973, ae_loss: 0.06174558\n",
      "Step: [214] total_loss: 2.11477470 d_loss: 1.36119318, g_loss: 0.69180119, ae_loss: 0.06178038\n",
      "Step: [215] total_loss: 2.14015961 d_loss: 1.38303542, g_loss: 0.69671416, ae_loss: 0.06040999\n",
      "Step: [216] total_loss: 2.15656972 d_loss: 1.39924169, g_loss: 0.69296527, ae_loss: 0.06436281\n",
      "Step: [217] total_loss: 2.12480354 d_loss: 1.38202834, g_loss: 0.68164766, ae_loss: 0.06112737\n",
      "Step: [218] total_loss: 2.12689281 d_loss: 1.37062454, g_loss: 0.69121867, ae_loss: 0.06504971\n",
      "Step: [219] total_loss: 2.11388254 d_loss: 1.36781633, g_loss: 0.68317056, ae_loss: 0.06289577\n",
      "Step: [220] total_loss: 2.12190104 d_loss: 1.36810493, g_loss: 0.69228953, ae_loss: 0.06150648\n",
      "Step: [221] total_loss: 2.11672235 d_loss: 1.36476481, g_loss: 0.69188720, ae_loss: 0.06007045\n",
      "Step: [222] total_loss: 2.11739397 d_loss: 1.36869955, g_loss: 0.68640661, ae_loss: 0.06228786\n",
      "Step: [223] total_loss: 2.11803937 d_loss: 1.37995648, g_loss: 0.67802870, ae_loss: 0.06005419\n",
      "Step: [224] total_loss: 2.10964918 d_loss: 1.36161709, g_loss: 0.68450367, ae_loss: 0.06352847\n",
      "Step: [225] total_loss: 2.11805964 d_loss: 1.37160039, g_loss: 0.68390572, ae_loss: 0.06255366\n",
      "Step: [226] total_loss: 2.13225222 d_loss: 1.37990117, g_loss: 0.69265771, ae_loss: 0.05969327\n",
      "Step: [227] total_loss: 2.10319924 d_loss: 1.35659242, g_loss: 0.68466371, ae_loss: 0.06194304\n",
      "Step: [228] total_loss: 2.12351394 d_loss: 1.36592841, g_loss: 0.69974083, ae_loss: 0.05784468\n",
      "Step: [229] total_loss: 2.12903380 d_loss: 1.38136029, g_loss: 0.68267167, ae_loss: 0.06500183\n",
      "Step: [230] total_loss: 2.12599087 d_loss: 1.38623202, g_loss: 0.67872643, ae_loss: 0.06103231\n",
      "Step: [231] total_loss: 2.09024501 d_loss: 1.34459102, g_loss: 0.68309450, ae_loss: 0.06255944\n",
      "Step: [232] total_loss: 2.12858009 d_loss: 1.38338947, g_loss: 0.68492460, ae_loss: 0.06026613\n",
      "Step: [233] total_loss: 2.12656760 d_loss: 1.36952209, g_loss: 0.69047165, ae_loss: 0.06657381\n",
      "Step: [234] total_loss: 2.12924170 d_loss: 1.35462010, g_loss: 0.70991796, ae_loss: 0.06470370\n",
      "Step: [235] total_loss: 2.12649488 d_loss: 1.36601734, g_loss: 0.69882965, ae_loss: 0.06164799\n",
      "Step: [236] total_loss: 2.09622383 d_loss: 1.35224867, g_loss: 0.68263698, ae_loss: 0.06133824\n",
      "Step: [237] total_loss: 2.10366106 d_loss: 1.35253799, g_loss: 0.69042516, ae_loss: 0.06069777\n",
      "Step: [238] total_loss: 2.11927080 d_loss: 1.38555241, g_loss: 0.66996396, ae_loss: 0.06375432\n",
      "Step: [239] total_loss: 2.13215876 d_loss: 1.37572467, g_loss: 0.69340551, ae_loss: 0.06302849\n",
      "Step: [240] total_loss: 2.12496662 d_loss: 1.36962318, g_loss: 0.68915236, ae_loss: 0.06619097\n",
      "Step: [241] total_loss: 2.13479280 d_loss: 1.38517332, g_loss: 0.68620139, ae_loss: 0.06341816\n",
      "Step: [242] total_loss: 2.12423229 d_loss: 1.36921954, g_loss: 0.69296467, ae_loss: 0.06204805\n",
      "Step: [243] total_loss: 2.10712671 d_loss: 1.33718145, g_loss: 0.70939386, ae_loss: 0.06055158\n",
      "Step: [244] total_loss: 2.12504411 d_loss: 1.37551999, g_loss: 0.68430799, ae_loss: 0.06521607\n",
      "Step: [245] total_loss: 2.09290385 d_loss: 1.35497475, g_loss: 0.67829078, ae_loss: 0.05963823\n",
      "Step: [246] total_loss: 2.11736608 d_loss: 1.37022328, g_loss: 0.68300307, ae_loss: 0.06413972\n",
      "Step: [247] total_loss: 2.10378098 d_loss: 1.36130500, g_loss: 0.68106604, ae_loss: 0.06140997\n",
      "Step: [248] total_loss: 2.10031438 d_loss: 1.34368515, g_loss: 0.69273376, ae_loss: 0.06389543\n",
      "Step: [249] total_loss: 2.11615562 d_loss: 1.37665558, g_loss: 0.67496300, ae_loss: 0.06453704\n",
      "Step: [250] total_loss: 2.12075090 d_loss: 1.38338506, g_loss: 0.67796737, ae_loss: 0.05939841\n",
      "Step: [251] total_loss: 2.12479949 d_loss: 1.36405873, g_loss: 0.70035720, ae_loss: 0.06038354\n",
      "Step: [252] total_loss: 2.11755681 d_loss: 1.37082016, g_loss: 0.68746388, ae_loss: 0.05927278\n",
      "Step: [253] total_loss: 2.12519836 d_loss: 1.37753177, g_loss: 0.68800032, ae_loss: 0.05966644\n",
      "Step: [254] total_loss: 2.10571623 d_loss: 1.36433983, g_loss: 0.67720091, ae_loss: 0.06417535\n",
      "Step: [255] total_loss: 2.12222266 d_loss: 1.35961413, g_loss: 0.69913483, ae_loss: 0.06347364\n",
      "Step: [256] total_loss: 2.12672472 d_loss: 1.36916876, g_loss: 0.69057280, ae_loss: 0.06698323\n",
      "Step: [257] total_loss: 2.11609840 d_loss: 1.36368895, g_loss: 0.69227356, ae_loss: 0.06013588\n",
      "Step: [258] total_loss: 2.11842632 d_loss: 1.35447490, g_loss: 0.70008755, ae_loss: 0.06386402\n",
      "Step: [259] total_loss: 2.13389444 d_loss: 1.39073253, g_loss: 0.67734635, ae_loss: 0.06581560\n",
      "Step: [260] total_loss: 2.11077237 d_loss: 1.38365674, g_loss: 0.66633940, ae_loss: 0.06077627\n",
      "Step: [261] total_loss: 2.13282084 d_loss: 1.37497258, g_loss: 0.69762218, ae_loss: 0.06022614\n",
      "Step: [262] total_loss: 2.13701272 d_loss: 1.38554978, g_loss: 0.68841398, ae_loss: 0.06304893\n",
      "Step: [263] total_loss: 2.14345884 d_loss: 1.39758432, g_loss: 0.68491530, ae_loss: 0.06095926\n",
      "Step: [264] total_loss: 2.13445163 d_loss: 1.37839699, g_loss: 0.69227666, ae_loss: 0.06377798\n",
      "Step: [265] total_loss: 2.11912155 d_loss: 1.35688639, g_loss: 0.69989842, ae_loss: 0.06233678\n",
      "Step: [266] total_loss: 2.14307213 d_loss: 1.37387466, g_loss: 0.71153283, ae_loss: 0.05766476\n",
      "Step: [267] total_loss: 2.12891579 d_loss: 1.37374759, g_loss: 0.69222039, ae_loss: 0.06294782\n",
      "Step: [268] total_loss: 2.12275505 d_loss: 1.36194789, g_loss: 0.69877434, ae_loss: 0.06203279\n",
      "Step: [269] total_loss: 2.12925792 d_loss: 1.37330103, g_loss: 0.69407237, ae_loss: 0.06188449\n",
      "Step: [270] total_loss: 2.12426805 d_loss: 1.38149273, g_loss: 0.67867780, ae_loss: 0.06409738\n",
      "Step: [271] total_loss: 2.09928989 d_loss: 1.37683952, g_loss: 0.66512054, ae_loss: 0.05732988\n",
      "Step: [272] total_loss: 2.11959934 d_loss: 1.37914503, g_loss: 0.67873144, ae_loss: 0.06172287\n",
      "Step: [273] total_loss: 2.14412546 d_loss: 1.39820123, g_loss: 0.68105131, ae_loss: 0.06487292\n",
      "Step: [274] total_loss: 2.12233615 d_loss: 1.36449909, g_loss: 0.69731444, ae_loss: 0.06052259\n",
      "Step: [275] total_loss: 2.13210320 d_loss: 1.36886585, g_loss: 0.69787735, ae_loss: 0.06535996\n",
      "Step: [276] total_loss: 2.14087224 d_loss: 1.38298345, g_loss: 0.69717431, ae_loss: 0.06071449\n",
      "Step: [277] total_loss: 2.11255121 d_loss: 1.35557914, g_loss: 0.69800365, ae_loss: 0.05896826\n",
      "Step: [278] total_loss: 2.11764097 d_loss: 1.36709595, g_loss: 0.69211996, ae_loss: 0.05842494\n",
      "Step: [279] total_loss: 2.12168932 d_loss: 1.37180126, g_loss: 0.68707490, ae_loss: 0.06281313\n",
      "Step: [280] total_loss: 2.12017727 d_loss: 1.37572145, g_loss: 0.68367147, ae_loss: 0.06078423\n",
      "Step: [281] total_loss: 2.11386776 d_loss: 1.35196590, g_loss: 0.69984138, ae_loss: 0.06206049\n",
      "Step: [282] total_loss: 2.10355043 d_loss: 1.37393641, g_loss: 0.67062163, ae_loss: 0.05899227\n",
      "Step: [283] total_loss: 2.10440135 d_loss: 1.37192607, g_loss: 0.67148542, ae_loss: 0.06098984\n",
      "Step: [284] total_loss: 2.14520764 d_loss: 1.38639688, g_loss: 0.69881707, ae_loss: 0.05999368\n",
      "Step: [285] total_loss: 2.12277269 d_loss: 1.38432896, g_loss: 0.68038094, ae_loss: 0.05806261\n",
      "Step: [286] total_loss: 2.11379957 d_loss: 1.35249460, g_loss: 0.69916195, ae_loss: 0.06214309\n",
      "Step: [287] total_loss: 2.12147403 d_loss: 1.37344813, g_loss: 0.68356490, ae_loss: 0.06446097\n",
      "Step: [288] total_loss: 2.13442969 d_loss: 1.39864790, g_loss: 0.67480910, ae_loss: 0.06097269\n",
      "Step: [289] total_loss: 2.10960722 d_loss: 1.38780689, g_loss: 0.65989780, ae_loss: 0.06190242\n",
      "Step: [290] total_loss: 2.09689999 d_loss: 1.36486626, g_loss: 0.67304099, ae_loss: 0.05899290\n",
      "Step: [291] total_loss: 2.11914492 d_loss: 1.37893105, g_loss: 0.67647660, ae_loss: 0.06373716\n",
      "Step: [292] total_loss: 2.12161493 d_loss: 1.37933838, g_loss: 0.68215132, ae_loss: 0.06012513\n",
      "Step: [293] total_loss: 2.11845994 d_loss: 1.37372088, g_loss: 0.68926805, ae_loss: 0.05547104\n",
      "Step: [294] total_loss: 2.13477230 d_loss: 1.38511193, g_loss: 0.68392801, ae_loss: 0.06573219\n",
      "Step: [295] total_loss: 2.10266733 d_loss: 1.34214544, g_loss: 0.70316917, ae_loss: 0.05735270\n",
      "Step: [296] total_loss: 2.13758969 d_loss: 1.38664865, g_loss: 0.69093376, ae_loss: 0.06000733\n",
      "Step: [297] total_loss: 2.11231852 d_loss: 1.36154985, g_loss: 0.68544728, ae_loss: 0.06532137\n",
      "Step: [298] total_loss: 2.14474654 d_loss: 1.37580621, g_loss: 0.70668638, ae_loss: 0.06225391\n",
      "Step: [299] total_loss: 2.11857581 d_loss: 1.37028837, g_loss: 0.68765533, ae_loss: 0.06063205\n",
      "Step: [300] total_loss: 2.11925697 d_loss: 1.37234378, g_loss: 0.68474960, ae_loss: 0.06216370\n",
      "Step: [301] total_loss: 2.10931253 d_loss: 1.37651384, g_loss: 0.67360002, ae_loss: 0.05919860\n",
      "Step: [302] total_loss: 2.11839104 d_loss: 1.38245308, g_loss: 0.67295158, ae_loss: 0.06298638\n",
      "Step: [303] total_loss: 2.13308835 d_loss: 1.36878467, g_loss: 0.70327419, ae_loss: 0.06102951\n",
      "Step: [304] total_loss: 2.12224817 d_loss: 1.37136602, g_loss: 0.69231296, ae_loss: 0.05856924\n",
      "Step: [305] total_loss: 2.12510538 d_loss: 1.37766623, g_loss: 0.68460721, ae_loss: 0.06283198\n",
      "Step: [306] total_loss: 2.10785556 d_loss: 1.38795304, g_loss: 0.66018361, ae_loss: 0.05971890\n",
      "Step: [307] total_loss: 2.11571622 d_loss: 1.37864995, g_loss: 0.67707080, ae_loss: 0.05999548\n",
      "Step: [308] total_loss: 2.12461829 d_loss: 1.37781274, g_loss: 0.68835360, ae_loss: 0.05845194\n",
      "Step: [309] total_loss: 2.12136698 d_loss: 1.37569988, g_loss: 0.68416345, ae_loss: 0.06150375\n",
      "Step: [310] total_loss: 2.13303995 d_loss: 1.36900675, g_loss: 0.70349932, ae_loss: 0.06053400\n",
      "Step: [311] total_loss: 2.14382648 d_loss: 1.39505124, g_loss: 0.68373686, ae_loss: 0.06503846\n",
      "Step: [312] total_loss: 2.13205862 d_loss: 1.38632870, g_loss: 0.68491447, ae_loss: 0.06081546\n",
      "Step: [313] total_loss: 2.12058115 d_loss: 1.38623691, g_loss: 0.67756593, ae_loss: 0.05677821\n",
      "Step: [314] total_loss: 2.12582707 d_loss: 1.37659645, g_loss: 0.68856597, ae_loss: 0.06066466\n",
      "Step: [315] total_loss: 2.13394403 d_loss: 1.38439000, g_loss: 0.68111956, ae_loss: 0.06843430\n",
      "Step: [316] total_loss: 2.10869980 d_loss: 1.36723351, g_loss: 0.68608147, ae_loss: 0.05538484\n",
      "Step: [317] total_loss: 2.13429213 d_loss: 1.39650321, g_loss: 0.68156332, ae_loss: 0.05622550\n",
      "Step: [318] total_loss: 2.12109637 d_loss: 1.37335896, g_loss: 0.68761683, ae_loss: 0.06012058\n",
      "Step: [319] total_loss: 2.13544869 d_loss: 1.39393544, g_loss: 0.67781162, ae_loss: 0.06370169\n",
      "Step: [320] total_loss: 2.10163188 d_loss: 1.35292459, g_loss: 0.68468934, ae_loss: 0.06401786\n",
      "Step: [321] total_loss: 2.10085535 d_loss: 1.36946011, g_loss: 0.67069423, ae_loss: 0.06070085\n",
      "Step: [322] total_loss: 2.11590743 d_loss: 1.37690902, g_loss: 0.68053102, ae_loss: 0.05846737\n",
      "Step: [323] total_loss: 2.12133455 d_loss: 1.37447667, g_loss: 0.68660069, ae_loss: 0.06025721\n",
      "Step: [324] total_loss: 2.11803222 d_loss: 1.37837088, g_loss: 0.67931795, ae_loss: 0.06034337\n",
      "Step: [325] total_loss: 2.12055683 d_loss: 1.37315977, g_loss: 0.68922532, ae_loss: 0.05817164\n",
      "Step: [326] total_loss: 2.11631107 d_loss: 1.35551310, g_loss: 0.69974232, ae_loss: 0.06105564\n",
      "Step: [327] total_loss: 2.11603785 d_loss: 1.36687768, g_loss: 0.68770039, ae_loss: 0.06145963\n",
      "Step: [328] total_loss: 2.12248373 d_loss: 1.38213205, g_loss: 0.68171525, ae_loss: 0.05863649\n",
      "Step: [329] total_loss: 2.13723564 d_loss: 1.38495910, g_loss: 0.68832719, ae_loss: 0.06394933\n",
      "Step: [330] total_loss: 2.12088585 d_loss: 1.35266423, g_loss: 0.70657539, ae_loss: 0.06164614\n",
      "Step: [331] total_loss: 2.12067080 d_loss: 1.35633826, g_loss: 0.70553994, ae_loss: 0.05879252\n",
      "Step: [332] total_loss: 2.12877131 d_loss: 1.38408554, g_loss: 0.68586010, ae_loss: 0.05882555\n",
      "Step: [333] total_loss: 2.11776304 d_loss: 1.36216843, g_loss: 0.69261980, ae_loss: 0.06297474\n",
      "Step: [334] total_loss: 2.14067793 d_loss: 1.39101124, g_loss: 0.68644047, ae_loss: 0.06322623\n",
      "Step: [335] total_loss: 2.12857246 d_loss: 1.35968387, g_loss: 0.70690519, ae_loss: 0.06198331\n",
      "Step: [336] total_loss: 2.11348009 d_loss: 1.34927857, g_loss: 0.70496774, ae_loss: 0.05923364\n",
      "Step: [337] total_loss: 2.14925051 d_loss: 1.41480458, g_loss: 0.67415833, ae_loss: 0.06028758\n",
      "Step: [338] total_loss: 2.13540936 d_loss: 1.39045608, g_loss: 0.68384802, ae_loss: 0.06110518\n",
      "Step: [339] total_loss: 2.10166121 d_loss: 1.35034776, g_loss: 0.69221401, ae_loss: 0.05909941\n",
      "Step: [340] total_loss: 2.12535477 d_loss: 1.39898396, g_loss: 0.65859199, ae_loss: 0.06777889\n",
      "Step: [341] total_loss: 2.10282922 d_loss: 1.37381983, g_loss: 0.66757143, ae_loss: 0.06143797\n",
      "Step: [342] total_loss: 2.08868980 d_loss: 1.34981894, g_loss: 0.68460822, ae_loss: 0.05426260\n",
      "Step: [343] total_loss: 2.14662957 d_loss: 1.39534688, g_loss: 0.69068766, ae_loss: 0.06059509\n",
      "Step: [344] total_loss: 2.13565207 d_loss: 1.36403561, g_loss: 0.71552324, ae_loss: 0.05609338\n",
      "Step: [345] total_loss: 2.12363005 d_loss: 1.37304235, g_loss: 0.69589394, ae_loss: 0.05469380\n",
      "Step: [346] total_loss: 2.14640927 d_loss: 1.37897408, g_loss: 0.70440537, ae_loss: 0.06302972\n",
      "Step: [347] total_loss: 2.12019110 d_loss: 1.36004460, g_loss: 0.69971585, ae_loss: 0.06043078\n",
      "Step: [348] total_loss: 2.16528487 d_loss: 1.39326370, g_loss: 0.71386480, ae_loss: 0.05815637\n",
      "Step: [349] total_loss: 2.09870410 d_loss: 1.33080101, g_loss: 0.70556641, ae_loss: 0.06233672\n",
      "Step: [350] total_loss: 2.12240028 d_loss: 1.38562036, g_loss: 0.67381668, ae_loss: 0.06296322\n",
      "Step: [351] total_loss: 2.11082649 d_loss: 1.36378527, g_loss: 0.68675834, ae_loss: 0.06028277\n",
      "Step: [352] total_loss: 2.11202407 d_loss: 1.36339664, g_loss: 0.68556428, ae_loss: 0.06306311\n",
      "Step: [353] total_loss: 2.10609531 d_loss: 1.33896422, g_loss: 0.70713997, ae_loss: 0.05999117\n",
      "Step: [354] total_loss: 2.11437035 d_loss: 1.37334251, g_loss: 0.68084478, ae_loss: 0.06018297\n",
      "Step: [355] total_loss: 2.11826849 d_loss: 1.37589717, g_loss: 0.68364847, ae_loss: 0.05872294\n",
      "Step: [356] total_loss: 2.11226606 d_loss: 1.34296525, g_loss: 0.71064615, ae_loss: 0.05865461\n",
      "Step: [357] total_loss: 2.15031624 d_loss: 1.40895653, g_loss: 0.68386507, ae_loss: 0.05749447\n",
      "Step: [358] total_loss: 2.13248539 d_loss: 1.36107528, g_loss: 0.71502936, ae_loss: 0.05638063\n",
      "Step: [359] total_loss: 2.12302446 d_loss: 1.34852457, g_loss: 0.71366501, ae_loss: 0.06083475\n",
      "Step: [360] total_loss: 2.10208893 d_loss: 1.37468052, g_loss: 0.67117864, ae_loss: 0.05622969\n",
      "Step: [361] total_loss: 2.12896132 d_loss: 1.38952255, g_loss: 0.67865998, ae_loss: 0.06077873\n",
      "Step: [362] total_loss: 2.11795044 d_loss: 1.37126136, g_loss: 0.68540221, ae_loss: 0.06128676\n",
      "Step: [363] total_loss: 2.10877109 d_loss: 1.35306919, g_loss: 0.69962144, ae_loss: 0.05608051\n",
      "Step: [364] total_loss: 2.12488031 d_loss: 1.38176870, g_loss: 0.68169469, ae_loss: 0.06141689\n",
      "Step: [365] total_loss: 2.13034487 d_loss: 1.36700797, g_loss: 0.70271349, ae_loss: 0.06062358\n",
      "Step: [366] total_loss: 2.12911916 d_loss: 1.37042403, g_loss: 0.70126683, ae_loss: 0.05742822\n",
      "Step: [367] total_loss: 2.15174580 d_loss: 1.40987062, g_loss: 0.67982429, ae_loss: 0.06205088\n",
      "Step: [368] total_loss: 2.14746976 d_loss: 1.38606083, g_loss: 0.70568931, ae_loss: 0.05571963\n",
      "Step: [369] total_loss: 2.14430285 d_loss: 1.39215589, g_loss: 0.69449782, ae_loss: 0.05764923\n",
      "Step: [370] total_loss: 2.12800980 d_loss: 1.36751413, g_loss: 0.70311022, ae_loss: 0.05738539\n",
      "Step: [371] total_loss: 2.13081956 d_loss: 1.38335013, g_loss: 0.68893832, ae_loss: 0.05853103\n",
      "Step: [372] total_loss: 2.12667418 d_loss: 1.37250113, g_loss: 0.69278455, ae_loss: 0.06138854\n",
      "Step: [373] total_loss: 2.13068771 d_loss: 1.39363456, g_loss: 0.68187082, ae_loss: 0.05518224\n",
      "Step: [374] total_loss: 2.13021708 d_loss: 1.38469744, g_loss: 0.68396688, ae_loss: 0.06155280\n",
      "Step: [375] total_loss: 2.09919357 d_loss: 1.36471200, g_loss: 0.67681623, ae_loss: 0.05766541\n",
      "Step: [376] total_loss: 2.12825298 d_loss: 1.35692024, g_loss: 0.71122801, ae_loss: 0.06010472\n",
      "Step: [377] total_loss: 2.14018607 d_loss: 1.40229619, g_loss: 0.67254663, ae_loss: 0.06534328\n",
      "Step: [378] total_loss: 2.10734892 d_loss: 1.36853647, g_loss: 0.68115318, ae_loss: 0.05765929\n",
      "Step: [379] total_loss: 2.11251736 d_loss: 1.38334560, g_loss: 0.66933358, ae_loss: 0.05983808\n",
      "Step: [380] total_loss: 2.10885429 d_loss: 1.38241732, g_loss: 0.66557717, ae_loss: 0.06085994\n",
      "Step: [381] total_loss: 2.09887862 d_loss: 1.35700858, g_loss: 0.68071789, ae_loss: 0.06115227\n",
      "Step: [382] total_loss: 2.10405111 d_loss: 1.36234021, g_loss: 0.68515360, ae_loss: 0.05655716\n",
      "Step: [383] total_loss: 2.12352729 d_loss: 1.37445498, g_loss: 0.69396883, ae_loss: 0.05510354\n",
      "Step: [384] total_loss: 2.13600731 d_loss: 1.39831424, g_loss: 0.67512983, ae_loss: 0.06256323\n",
      "Step: [385] total_loss: 2.11912251 d_loss: 1.37358427, g_loss: 0.68841052, ae_loss: 0.05712789\n",
      "Step: [386] total_loss: 2.12156057 d_loss: 1.38093209, g_loss: 0.68319881, ae_loss: 0.05742963\n",
      "Step: [387] total_loss: 2.10771775 d_loss: 1.38808513, g_loss: 0.65943485, ae_loss: 0.06019769\n",
      "Step: [388] total_loss: 2.09622312 d_loss: 1.35148036, g_loss: 0.68231153, ae_loss: 0.06243121\n",
      "Step: [389] total_loss: 2.10454607 d_loss: 1.36400294, g_loss: 0.68196493, ae_loss: 0.05857809\n",
      "Step: [390] total_loss: 2.09448767 d_loss: 1.38271165, g_loss: 0.65753591, ae_loss: 0.05424011\n",
      "Step: [391] total_loss: 2.11930180 d_loss: 1.37371826, g_loss: 0.68770736, ae_loss: 0.05787627\n",
      "Step: [392] total_loss: 2.13249183 d_loss: 1.40095651, g_loss: 0.67524534, ae_loss: 0.05628990\n",
      "Step: [393] total_loss: 2.11978960 d_loss: 1.36808443, g_loss: 0.69112742, ae_loss: 0.06057771\n",
      "Step: [394] total_loss: 2.11863470 d_loss: 1.38103139, g_loss: 0.67995405, ae_loss: 0.05764909\n",
      "Step: [395] total_loss: 2.12709093 d_loss: 1.38714480, g_loss: 0.68314540, ae_loss: 0.05680088\n",
      "Step: [396] total_loss: 2.13713598 d_loss: 1.37834764, g_loss: 0.69958180, ae_loss: 0.05920654\n",
      "Step: [397] total_loss: 2.11341763 d_loss: 1.35503817, g_loss: 0.69927561, ae_loss: 0.05910373\n",
      "Step: [398] total_loss: 2.13073540 d_loss: 1.37362266, g_loss: 0.69599509, ae_loss: 0.06111768\n",
      "Step: [399] total_loss: 2.12024140 d_loss: 1.37098372, g_loss: 0.68636578, ae_loss: 0.06289182\n",
      "Step: [400] total_loss: 2.12390780 d_loss: 1.37185621, g_loss: 0.69162035, ae_loss: 0.06043128\n",
      "Step: [401] total_loss: 2.10533810 d_loss: 1.33923864, g_loss: 0.70826983, ae_loss: 0.05782949\n",
      "Step: [402] total_loss: 2.13024497 d_loss: 1.36856651, g_loss: 0.70494384, ae_loss: 0.05673463\n",
      "Step: [403] total_loss: 2.12009645 d_loss: 1.35547161, g_loss: 0.70357025, ae_loss: 0.06105454\n",
      "Step: [404] total_loss: 2.12021136 d_loss: 1.36478364, g_loss: 0.69707769, ae_loss: 0.05834997\n",
      "Step: [405] total_loss: 2.15649962 d_loss: 1.39615405, g_loss: 0.69878197, ae_loss: 0.06156366\n",
      "Step: [406] total_loss: 2.09948063 d_loss: 1.35983920, g_loss: 0.68203044, ae_loss: 0.05761100\n",
      "Step: [407] total_loss: 2.10117531 d_loss: 1.37073255, g_loss: 0.67069757, ae_loss: 0.05974511\n",
      "Step: [408] total_loss: 2.11996961 d_loss: 1.37348115, g_loss: 0.68932939, ae_loss: 0.05715909\n",
      "Step: [409] total_loss: 2.09600735 d_loss: 1.35773790, g_loss: 0.67631495, ae_loss: 0.06195439\n",
      "Step: [410] total_loss: 2.10020685 d_loss: 1.34178734, g_loss: 0.70320857, ae_loss: 0.05521078\n",
      "Step: [411] total_loss: 2.12473726 d_loss: 1.39018011, g_loss: 0.67582470, ae_loss: 0.05873251\n",
      "Step: [412] total_loss: 2.12168527 d_loss: 1.38330412, g_loss: 0.68215758, ae_loss: 0.05622368\n",
      "Step: [413] total_loss: 2.09218979 d_loss: 1.35161448, g_loss: 0.68073010, ae_loss: 0.05984534\n",
      "Step: [414] total_loss: 2.11329746 d_loss: 1.37168336, g_loss: 0.68440711, ae_loss: 0.05720706\n",
      "Step: [415] total_loss: 2.13876104 d_loss: 1.38561678, g_loss: 0.69135112, ae_loss: 0.06179321\n",
      "Step: [416] total_loss: 2.12085962 d_loss: 1.37016201, g_loss: 0.69142735, ae_loss: 0.05927041\n",
      "Step: [417] total_loss: 2.12448645 d_loss: 1.36873460, g_loss: 0.69672203, ae_loss: 0.05902975\n",
      "Step: [418] total_loss: 2.12466192 d_loss: 1.37166965, g_loss: 0.69269621, ae_loss: 0.06029608\n",
      "Step: [419] total_loss: 2.10717916 d_loss: 1.35890496, g_loss: 0.68220031, ae_loss: 0.06607392\n",
      "Step: [420] total_loss: 2.12350035 d_loss: 1.39761245, g_loss: 0.66561544, ae_loss: 0.06027261\n",
      "Step: [421] total_loss: 2.10708189 d_loss: 1.36976004, g_loss: 0.67996275, ae_loss: 0.05735897\n",
      "Step: [422] total_loss: 2.13744831 d_loss: 1.39235735, g_loss: 0.68763304, ae_loss: 0.05745778\n",
      "Step: [423] total_loss: 2.12944841 d_loss: 1.38372338, g_loss: 0.68568724, ae_loss: 0.06003768\n",
      "Step: [424] total_loss: 2.11468911 d_loss: 1.38105965, g_loss: 0.67585450, ae_loss: 0.05777499\n",
      "Step: [425] total_loss: 2.11724544 d_loss: 1.35163617, g_loss: 0.70619464, ae_loss: 0.05941465\n",
      "Step: [426] total_loss: 2.13250685 d_loss: 1.39824069, g_loss: 0.67344409, ae_loss: 0.06082196\n",
      "Step: [427] total_loss: 2.10386944 d_loss: 1.36335111, g_loss: 0.68346059, ae_loss: 0.05705789\n",
      "Step: [428] total_loss: 2.13199973 d_loss: 1.37629068, g_loss: 0.69759274, ae_loss: 0.05811630\n",
      "Step: [429] total_loss: 2.12136650 d_loss: 1.36389184, g_loss: 0.70390511, ae_loss: 0.05356938\n",
      "Step: [430] total_loss: 2.12841272 d_loss: 1.39013982, g_loss: 0.68383020, ae_loss: 0.05444272\n",
      "Step: [431] total_loss: 2.12276316 d_loss: 1.35869741, g_loss: 0.70997834, ae_loss: 0.05408724\n",
      "Step: [432] total_loss: 2.12461829 d_loss: 1.39062786, g_loss: 0.67590749, ae_loss: 0.05808288\n",
      "Step: [433] total_loss: 2.13197684 d_loss: 1.39429212, g_loss: 0.68141669, ae_loss: 0.05626801\n",
      "Step: [434] total_loss: 2.11383343 d_loss: 1.38947952, g_loss: 0.66980696, ae_loss: 0.05454680\n",
      "Step: [435] total_loss: 2.10109282 d_loss: 1.35759735, g_loss: 0.68345201, ae_loss: 0.06004359\n",
      "Step: [436] total_loss: 2.11056924 d_loss: 1.36020958, g_loss: 0.69173676, ae_loss: 0.05862282\n",
      "Step: [437] total_loss: 2.13557863 d_loss: 1.38785684, g_loss: 0.68716687, ae_loss: 0.06055497\n",
      "Step: [438] total_loss: 2.11123276 d_loss: 1.37040997, g_loss: 0.68486118, ae_loss: 0.05596172\n",
      "Step: [439] total_loss: 2.13046503 d_loss: 1.38478327, g_loss: 0.68553275, ae_loss: 0.06014911\n",
      "Step: [440] total_loss: 2.13471913 d_loss: 1.38205409, g_loss: 0.69239497, ae_loss: 0.06027004\n",
      "Step: [441] total_loss: 2.10626554 d_loss: 1.36972237, g_loss: 0.67872095, ae_loss: 0.05782208\n",
      "Step: [442] total_loss: 2.12500811 d_loss: 1.37838054, g_loss: 0.69020379, ae_loss: 0.05642372\n",
      "Step: [443] total_loss: 2.11462164 d_loss: 1.38198781, g_loss: 0.67750120, ae_loss: 0.05513265\n",
      "Step: [444] total_loss: 2.11493683 d_loss: 1.37383318, g_loss: 0.68540418, ae_loss: 0.05569939\n",
      "Step: [445] total_loss: 2.14416742 d_loss: 1.40431321, g_loss: 0.68377066, ae_loss: 0.05608360\n",
      "Step: [446] total_loss: 2.12574506 d_loss: 1.38752127, g_loss: 0.67840976, ae_loss: 0.05981407\n",
      "Step: [447] total_loss: 2.11610651 d_loss: 1.36806321, g_loss: 0.69279492, ae_loss: 0.05524842\n",
      "Step: [448] total_loss: 2.09046173 d_loss: 1.36120927, g_loss: 0.67587316, ae_loss: 0.05337917\n",
      "Step: [449] total_loss: 2.10675669 d_loss: 1.36253023, g_loss: 0.68847787, ae_loss: 0.05574857\n",
      "Step: [450] total_loss: 2.13334131 d_loss: 1.38099170, g_loss: 0.69512320, ae_loss: 0.05722654\n",
      "Step: [451] total_loss: 2.13727641 d_loss: 1.39147091, g_loss: 0.68738925, ae_loss: 0.05841622\n",
      "Step: [452] total_loss: 2.14565945 d_loss: 1.39588439, g_loss: 0.69406903, ae_loss: 0.05570614\n",
      "Step: [453] total_loss: 2.13637853 d_loss: 1.38058424, g_loss: 0.69805783, ae_loss: 0.05773635\n",
      "Step: [454] total_loss: 2.12887621 d_loss: 1.38067937, g_loss: 0.69045603, ae_loss: 0.05774066\n",
      "Step: [455] total_loss: 2.14610672 d_loss: 1.38140702, g_loss: 0.70198023, ae_loss: 0.06271950\n",
      "Step: [456] total_loss: 2.12779713 d_loss: 1.39116645, g_loss: 0.67569041, ae_loss: 0.06094034\n",
      "Step: [457] total_loss: 2.12458491 d_loss: 1.37416816, g_loss: 0.69366527, ae_loss: 0.05675151\n",
      "Step: [458] total_loss: 2.13749838 d_loss: 1.37098742, g_loss: 0.70672727, ae_loss: 0.05978378\n",
      "Step: [459] total_loss: 2.12733984 d_loss: 1.39320636, g_loss: 0.68072593, ae_loss: 0.05340765\n",
      "Step: [460] total_loss: 2.12206125 d_loss: 1.37599015, g_loss: 0.68922007, ae_loss: 0.05685101\n",
      "Step: [461] total_loss: 2.12798786 d_loss: 1.36596894, g_loss: 0.70582640, ae_loss: 0.05619240\n",
      "Step: [462] total_loss: 2.12327623 d_loss: 1.37339592, g_loss: 0.69302261, ae_loss: 0.05685757\n",
      "Step: [463] total_loss: 2.13653636 d_loss: 1.40272129, g_loss: 0.67931241, ae_loss: 0.05450259\n",
      "Step: [464] total_loss: 2.13401484 d_loss: 1.38115597, g_loss: 0.69520473, ae_loss: 0.05765411\n",
      "Step: [465] total_loss: 2.11277175 d_loss: 1.37971139, g_loss: 0.67687160, ae_loss: 0.05618884\n",
      "Step: [466] total_loss: 2.14085650 d_loss: 1.40779233, g_loss: 0.67434525, ae_loss: 0.05871892\n",
      "Step: [467] total_loss: 2.11650300 d_loss: 1.37621593, g_loss: 0.68426418, ae_loss: 0.05602290\n",
      "Step: [468] total_loss: 2.12032223 d_loss: 1.37140751, g_loss: 0.68843597, ae_loss: 0.06047869\n",
      "Step: [469] total_loss: 2.13177896 d_loss: 1.39583671, g_loss: 0.67735171, ae_loss: 0.05859059\n",
      "Step: [470] total_loss: 2.10833168 d_loss: 1.37362134, g_loss: 0.67415762, ae_loss: 0.06055274\n",
      "Step: [471] total_loss: 2.12466168 d_loss: 1.37480688, g_loss: 0.69045377, ae_loss: 0.05940101\n",
      "Step: [472] total_loss: 2.11648035 d_loss: 1.37064767, g_loss: 0.68749166, ae_loss: 0.05834085\n",
      "Step: [473] total_loss: 2.12113023 d_loss: 1.37464142, g_loss: 0.69023347, ae_loss: 0.05625528\n",
      "Step: [474] total_loss: 2.12394404 d_loss: 1.38111746, g_loss: 0.68655103, ae_loss: 0.05627551\n",
      "Step: [475] total_loss: 2.12597370 d_loss: 1.35621500, g_loss: 0.71182871, ae_loss: 0.05793002\n",
      "Step: [476] total_loss: 2.10642576 d_loss: 1.37313986, g_loss: 0.67915142, ae_loss: 0.05413451\n",
      "Step: [477] total_loss: 2.13149786 d_loss: 1.39332938, g_loss: 0.68509197, ae_loss: 0.05307642\n",
      "Step: [478] total_loss: 2.11994076 d_loss: 1.37499046, g_loss: 0.68855894, ae_loss: 0.05639152\n",
      "Step: [479] total_loss: 2.12094355 d_loss: 1.37472749, g_loss: 0.68916225, ae_loss: 0.05705393\n",
      "Step: [480] total_loss: 2.10783815 d_loss: 1.37128329, g_loss: 0.68471956, ae_loss: 0.05183521\n",
      "Step: [481] total_loss: 2.13926840 d_loss: 1.38757885, g_loss: 0.68842876, ae_loss: 0.06326076\n",
      "Step: [482] total_loss: 2.11598420 d_loss: 1.35765576, g_loss: 0.70180970, ae_loss: 0.05651884\n",
      "Step: [483] total_loss: 2.11619234 d_loss: 1.37636840, g_loss: 0.68225563, ae_loss: 0.05756828\n",
      "Step: [484] total_loss: 2.09464788 d_loss: 1.36095810, g_loss: 0.67643708, ae_loss: 0.05725270\n",
      "Step: [485] total_loss: 2.10281682 d_loss: 1.36296642, g_loss: 0.68229157, ae_loss: 0.05755875\n",
      "Step: [486] total_loss: 2.09971714 d_loss: 1.35574913, g_loss: 0.68910187, ae_loss: 0.05486612\n",
      "Step: [487] total_loss: 2.12178230 d_loss: 1.40039897, g_loss: 0.66348445, ae_loss: 0.05789895\n",
      "Step: [488] total_loss: 2.11052275 d_loss: 1.36805296, g_loss: 0.68040252, ae_loss: 0.06206719\n",
      "Step: [489] total_loss: 2.10329747 d_loss: 1.36131215, g_loss: 0.68304896, ae_loss: 0.05893634\n",
      "Step: [490] total_loss: 2.10821557 d_loss: 1.34671974, g_loss: 0.70282972, ae_loss: 0.05866611\n",
      "Step: [491] total_loss: 2.12279749 d_loss: 1.37631321, g_loss: 0.69098794, ae_loss: 0.05549651\n",
      "Step: [492] total_loss: 2.12893844 d_loss: 1.38838577, g_loss: 0.68095416, ae_loss: 0.05959843\n",
      "Step: [493] total_loss: 2.12083340 d_loss: 1.40249181, g_loss: 0.66362250, ae_loss: 0.05471894\n",
      "Step: [494] total_loss: 2.12573051 d_loss: 1.37280631, g_loss: 0.69316125, ae_loss: 0.05976300\n",
      "Step: [495] total_loss: 2.10199666 d_loss: 1.36816359, g_loss: 0.67673773, ae_loss: 0.05709542\n",
      "Step: [496] total_loss: 2.11235380 d_loss: 1.37257612, g_loss: 0.68637311, ae_loss: 0.05340460\n",
      "Step: [497] total_loss: 2.11972284 d_loss: 1.36736608, g_loss: 0.69930178, ae_loss: 0.05305509\n",
      "Step: [498] total_loss: 2.12208986 d_loss: 1.36963069, g_loss: 0.69523036, ae_loss: 0.05722893\n",
      "Step: [499] total_loss: 2.12688971 d_loss: 1.38515186, g_loss: 0.68444991, ae_loss: 0.05728778\n",
      "Step: [500] total_loss: 2.12903500 d_loss: 1.37100661, g_loss: 0.69955051, ae_loss: 0.05847776\n",
      "Step: [501] total_loss: 2.11190605 d_loss: 1.38202786, g_loss: 0.66871417, ae_loss: 0.06116388\n",
      "Step: [502] total_loss: 2.11512828 d_loss: 1.35990953, g_loss: 0.69596916, ae_loss: 0.05924964\n",
      "Step: [503] total_loss: 2.13464069 d_loss: 1.39321256, g_loss: 0.68327284, ae_loss: 0.05815523\n",
      "Step: [504] total_loss: 2.13965368 d_loss: 1.39773154, g_loss: 0.68538791, ae_loss: 0.05653423\n",
      "Step: [505] total_loss: 2.10603380 d_loss: 1.39048469, g_loss: 0.65744376, ae_loss: 0.05810551\n",
      "Step: [506] total_loss: 2.11566639 d_loss: 1.37519610, g_loss: 0.68271136, ae_loss: 0.05775909\n",
      "Step: [507] total_loss: 2.12515688 d_loss: 1.37846231, g_loss: 0.68580395, ae_loss: 0.06089053\n",
      "Step: [508] total_loss: 2.14775753 d_loss: 1.38281858, g_loss: 0.71063137, ae_loss: 0.05430753\n",
      "Step: [509] total_loss: 2.13184547 d_loss: 1.36664927, g_loss: 0.70744282, ae_loss: 0.05775347\n",
      "Step: [510] total_loss: 2.12846613 d_loss: 1.38349128, g_loss: 0.69146836, ae_loss: 0.05350654\n",
      "Step: [511] total_loss: 2.14040232 d_loss: 1.40766978, g_loss: 0.67684889, ae_loss: 0.05588361\n",
      "Step: [512] total_loss: 2.12003231 d_loss: 1.37566710, g_loss: 0.68655324, ae_loss: 0.05781195\n",
      "Step: [513] total_loss: 2.12915373 d_loss: 1.37630403, g_loss: 0.69281805, ae_loss: 0.06003180\n",
      "Step: [514] total_loss: 2.10640240 d_loss: 1.38002300, g_loss: 0.66895592, ae_loss: 0.05742357\n",
      "Step: [515] total_loss: 2.10227013 d_loss: 1.35255659, g_loss: 0.68994379, ae_loss: 0.05976964\n",
      "Step: [516] total_loss: 2.12352848 d_loss: 1.36466038, g_loss: 0.70125747, ae_loss: 0.05761060\n",
      "Step: [517] total_loss: 2.11853719 d_loss: 1.37511551, g_loss: 0.68512440, ae_loss: 0.05829728\n",
      "Step: [518] total_loss: 2.14388990 d_loss: 1.39306474, g_loss: 0.68480146, ae_loss: 0.06602379\n",
      "Step: [519] total_loss: 2.10463238 d_loss: 1.34486270, g_loss: 0.70759320, ae_loss: 0.05217639\n",
      "Step: [520] total_loss: 2.14293718 d_loss: 1.40057397, g_loss: 0.68424773, ae_loss: 0.05811534\n",
      "Step: [521] total_loss: 2.13065100 d_loss: 1.37040937, g_loss: 0.70522624, ae_loss: 0.05501539\n",
      "Step: [522] total_loss: 2.11441660 d_loss: 1.36858797, g_loss: 0.68711156, ae_loss: 0.05871696\n",
      "Step: [523] total_loss: 2.09701490 d_loss: 1.36873126, g_loss: 0.67540067, ae_loss: 0.05288287\n",
      "Step: [524] total_loss: 2.12323952 d_loss: 1.37710118, g_loss: 0.69279933, ae_loss: 0.05333915\n",
      "Step: [525] total_loss: 2.13537049 d_loss: 1.37580836, g_loss: 0.69975966, ae_loss: 0.05980238\n",
      "Step: [526] total_loss: 2.09400988 d_loss: 1.36223865, g_loss: 0.67676145, ae_loss: 0.05500981\n",
      "Step: [527] total_loss: 2.11698461 d_loss: 1.37791276, g_loss: 0.68399084, ae_loss: 0.05508098\n",
      "Step: [528] total_loss: 2.09569836 d_loss: 1.36864913, g_loss: 0.66821551, ae_loss: 0.05883369\n",
      "Step: [529] total_loss: 2.12086105 d_loss: 1.39197111, g_loss: 0.67429292, ae_loss: 0.05459714\n",
      "Step: [530] total_loss: 2.11691356 d_loss: 1.37720871, g_loss: 0.68280643, ae_loss: 0.05689850\n",
      "Step: [531] total_loss: 2.11207175 d_loss: 1.35983324, g_loss: 0.69608283, ae_loss: 0.05615567\n",
      "Step: [532] total_loss: 2.10342741 d_loss: 1.36032605, g_loss: 0.68151164, ae_loss: 0.06158969\n",
      "Step: [533] total_loss: 2.12835407 d_loss: 1.37395883, g_loss: 0.70034969, ae_loss: 0.05404569\n",
      "Step: [534] total_loss: 2.11206532 d_loss: 1.35920978, g_loss: 0.69822103, ae_loss: 0.05463458\n",
      "Step: [535] total_loss: 2.12576413 d_loss: 1.38595665, g_loss: 0.67894244, ae_loss: 0.06086507\n",
      "Step: [536] total_loss: 2.11618805 d_loss: 1.37203097, g_loss: 0.68820596, ae_loss: 0.05595098\n",
      "Step: [537] total_loss: 2.09026408 d_loss: 1.33900952, g_loss: 0.69678652, ae_loss: 0.05446808\n",
      "Step: [538] total_loss: 2.11681795 d_loss: 1.38945127, g_loss: 0.66938519, ae_loss: 0.05798132\n",
      "Step: [539] total_loss: 2.11390781 d_loss: 1.37982583, g_loss: 0.67670548, ae_loss: 0.05737668\n",
      "Step: [540] total_loss: 2.12493372 d_loss: 1.39825273, g_loss: 0.66937172, ae_loss: 0.05730939\n",
      "Step: [541] total_loss: 2.12520790 d_loss: 1.37061548, g_loss: 0.69988143, ae_loss: 0.05471103\n",
      "Step: [542] total_loss: 2.11371279 d_loss: 1.36710572, g_loss: 0.69034725, ae_loss: 0.05625990\n",
      "Step: [543] total_loss: 2.11031389 d_loss: 1.36454272, g_loss: 0.68898606, ae_loss: 0.05678523\n",
      "Step: [544] total_loss: 2.12741709 d_loss: 1.38244891, g_loss: 0.68647701, ae_loss: 0.05849107\n",
      "Step: [545] total_loss: 2.13593531 d_loss: 1.38373852, g_loss: 0.69930023, ae_loss: 0.05289664\n",
      "Step: [546] total_loss: 2.13733149 d_loss: 1.39775193, g_loss: 0.68212456, ae_loss: 0.05745511\n",
      "Step: [547] total_loss: 2.13032746 d_loss: 1.36466432, g_loss: 0.70662087, ae_loss: 0.05904231\n",
      "Step: [548] total_loss: 2.09885144 d_loss: 1.35606301, g_loss: 0.68952793, ae_loss: 0.05326049\n",
      "Step: [549] total_loss: 2.12402964 d_loss: 1.35715532, g_loss: 0.71374893, ae_loss: 0.05312530\n",
      "Step: [550] total_loss: 2.14654303 d_loss: 1.40512156, g_loss: 0.68810356, ae_loss: 0.05331804\n",
      "Step: [551] total_loss: 2.11778975 d_loss: 1.37549877, g_loss: 0.68741441, ae_loss: 0.05487657\n",
      "Step: [552] total_loss: 2.11195469 d_loss: 1.36636567, g_loss: 0.68729907, ae_loss: 0.05828999\n",
      "Step: [553] total_loss: 2.12520599 d_loss: 1.36233342, g_loss: 0.70715880, ae_loss: 0.05571380\n",
      "Step: [554] total_loss: 2.12636471 d_loss: 1.36903477, g_loss: 0.70102203, ae_loss: 0.05630792\n",
      "Step: [555] total_loss: 2.08834672 d_loss: 1.34711421, g_loss: 0.68774658, ae_loss: 0.05348583\n",
      "Step: [556] total_loss: 2.10287523 d_loss: 1.37305403, g_loss: 0.67175984, ae_loss: 0.05806152\n",
      "Step: [557] total_loss: 2.10759473 d_loss: 1.38339591, g_loss: 0.66327208, ae_loss: 0.06092668\n",
      "Step: [558] total_loss: 2.08716965 d_loss: 1.35311818, g_loss: 0.67771852, ae_loss: 0.05633301\n",
      "Step: [559] total_loss: 2.11568403 d_loss: 1.37096190, g_loss: 0.68871558, ae_loss: 0.05600670\n",
      "Step: [560] total_loss: 2.13887405 d_loss: 1.37227559, g_loss: 0.70480782, ae_loss: 0.06179060\n",
      "Step: [561] total_loss: 2.12677836 d_loss: 1.35773158, g_loss: 0.71106261, ae_loss: 0.05798415\n",
      "Step: [562] total_loss: 2.13256693 d_loss: 1.39116871, g_loss: 0.68802452, ae_loss: 0.05337378\n",
      "Step: [563] total_loss: 2.12245989 d_loss: 1.37607408, g_loss: 0.68857044, ae_loss: 0.05781529\n",
      "Step: [564] total_loss: 2.13432932 d_loss: 1.40219963, g_loss: 0.67437768, ae_loss: 0.05775214\n",
      "Step: [565] total_loss: 2.13637304 d_loss: 1.40096438, g_loss: 0.67788255, ae_loss: 0.05752627\n",
      "Step: [566] total_loss: 2.15043354 d_loss: 1.38951325, g_loss: 0.70239639, ae_loss: 0.05852387\n",
      "Step: [567] total_loss: 2.12957859 d_loss: 1.39624679, g_loss: 0.68102539, ae_loss: 0.05230654\n",
      "Step: [568] total_loss: 2.12173700 d_loss: 1.37847936, g_loss: 0.68562376, ae_loss: 0.05763403\n",
      "Step: [569] total_loss: 2.11379266 d_loss: 1.37479937, g_loss: 0.68368286, ae_loss: 0.05531041\n",
      "Step: [570] total_loss: 2.12738895 d_loss: 1.37990797, g_loss: 0.69325161, ae_loss: 0.05422947\n",
      "Step: [571] total_loss: 2.14183950 d_loss: 1.37342584, g_loss: 0.71348488, ae_loss: 0.05492871\n",
      "Step: [572] total_loss: 2.14195180 d_loss: 1.36105061, g_loss: 0.72418076, ae_loss: 0.05672053\n",
      "Step: [573] total_loss: 2.14485025 d_loss: 1.38298786, g_loss: 0.70811856, ae_loss: 0.05374368\n",
      "Step: [574] total_loss: 2.12930679 d_loss: 1.37113392, g_loss: 0.70448232, ae_loss: 0.05369044\n",
      "Step: [575] total_loss: 2.13237953 d_loss: 1.36820281, g_loss: 0.70816576, ae_loss: 0.05601107\n",
      "Step: [576] total_loss: 2.10252666 d_loss: 1.38268077, g_loss: 0.66407043, ae_loss: 0.05577538\n",
      "Step: [577] total_loss: 2.10281539 d_loss: 1.37983680, g_loss: 0.66800869, ae_loss: 0.05496993\n",
      "Step: [578] total_loss: 2.10558438 d_loss: 1.39337659, g_loss: 0.65595174, ae_loss: 0.05625604\n",
      "Step: [579] total_loss: 2.09602070 d_loss: 1.35812604, g_loss: 0.68474221, ae_loss: 0.05315257\n",
      "Step: [580] total_loss: 2.11591768 d_loss: 1.39095080, g_loss: 0.66625273, ae_loss: 0.05871409\n",
      "Step: [581] total_loss: 2.12798262 d_loss: 1.38833046, g_loss: 0.68504548, ae_loss: 0.05460671\n",
      "Step: [582] total_loss: 2.12702179 d_loss: 1.37900352, g_loss: 0.69032884, ae_loss: 0.05768926\n",
      "Step: [583] total_loss: 2.13748407 d_loss: 1.36802292, g_loss: 0.70677304, ae_loss: 0.06268820\n",
      "Step: [584] total_loss: 2.11601162 d_loss: 1.38877082, g_loss: 0.67067486, ae_loss: 0.05656596\n",
      "Step: [585] total_loss: 2.12648129 d_loss: 1.39957654, g_loss: 0.67087483, ae_loss: 0.05602989\n",
      "Step: [586] total_loss: 2.11466169 d_loss: 1.37809038, g_loss: 0.68012381, ae_loss: 0.05644765\n",
      "Step: [587] total_loss: 2.13243961 d_loss: 1.40171385, g_loss: 0.67473483, ae_loss: 0.05599092\n",
      "Step: [588] total_loss: 2.12813687 d_loss: 1.38655019, g_loss: 0.68792862, ae_loss: 0.05365802\n",
      "Step: [589] total_loss: 2.11960387 d_loss: 1.39175236, g_loss: 0.66466290, ae_loss: 0.06318853\n",
      "Step: [590] total_loss: 2.10870409 d_loss: 1.36394024, g_loss: 0.69209909, ae_loss: 0.05266485\n",
      "Step: [591] total_loss: 2.12912989 d_loss: 1.38921297, g_loss: 0.68567443, ae_loss: 0.05424264\n",
      "Step: [592] total_loss: 2.13335180 d_loss: 1.39668036, g_loss: 0.68592799, ae_loss: 0.05074344\n",
      "Step: [593] total_loss: 2.11732435 d_loss: 1.37621772, g_loss: 0.68506789, ae_loss: 0.05603878\n",
      "Step: [594] total_loss: 2.11336517 d_loss: 1.35567820, g_loss: 0.70266151, ae_loss: 0.05502554\n",
      "Step: [595] total_loss: 2.13145304 d_loss: 1.40549731, g_loss: 0.67239171, ae_loss: 0.05356398\n",
      "Step: [596] total_loss: 2.13394570 d_loss: 1.39162827, g_loss: 0.68892419, ae_loss: 0.05339320\n",
      "Step: [597] total_loss: 2.11913919 d_loss: 1.38288879, g_loss: 0.67825729, ae_loss: 0.05799301\n",
      "Step: [598] total_loss: 2.11914873 d_loss: 1.35435581, g_loss: 0.70909286, ae_loss: 0.05570007\n",
      "Step: [599] total_loss: 2.09775019 d_loss: 1.35428858, g_loss: 0.68878484, ae_loss: 0.05467662\n",
      "Step: [600] total_loss: 2.15232182 d_loss: 1.39077568, g_loss: 0.70128131, ae_loss: 0.06026470\n",
      "Step: [601] total_loss: 2.14153433 d_loss: 1.38869286, g_loss: 0.69413239, ae_loss: 0.05870913\n",
      "Step: [602] total_loss: 2.12772274 d_loss: 1.38447261, g_loss: 0.68835646, ae_loss: 0.05489375\n",
      "Step: [603] total_loss: 2.13092136 d_loss: 1.36870146, g_loss: 0.70441163, ae_loss: 0.05780828\n",
      "Step: [604] total_loss: 2.13491488 d_loss: 1.37337804, g_loss: 0.70574033, ae_loss: 0.05579656\n",
      "Step: [605] total_loss: 2.14266038 d_loss: 1.37392986, g_loss: 0.71256405, ae_loss: 0.05616648\n",
      "Step: [606] total_loss: 2.12675476 d_loss: 1.38157630, g_loss: 0.68467927, ae_loss: 0.06049918\n",
      "Step: [607] total_loss: 2.11137772 d_loss: 1.36151052, g_loss: 0.69195354, ae_loss: 0.05791350\n",
      "Step: [608] total_loss: 2.13684034 d_loss: 1.36073530, g_loss: 0.71905494, ae_loss: 0.05705007\n",
      "Step: [609] total_loss: 2.12781978 d_loss: 1.39841533, g_loss: 0.67282373, ae_loss: 0.05658066\n",
      "Step: [610] total_loss: 2.12595510 d_loss: 1.38100576, g_loss: 0.68897557, ae_loss: 0.05597393\n",
      "Step: [611] total_loss: 2.14248800 d_loss: 1.41288495, g_loss: 0.67345524, ae_loss: 0.05614772\n",
      "Step: [612] total_loss: 2.13487530 d_loss: 1.37312829, g_loss: 0.70902115, ae_loss: 0.05272577\n",
      "Step: [613] total_loss: 2.12472677 d_loss: 1.37567115, g_loss: 0.69292712, ae_loss: 0.05612862\n",
      "Step: [614] total_loss: 2.13184118 d_loss: 1.39743614, g_loss: 0.67899507, ae_loss: 0.05540985\n",
      "Step: [615] total_loss: 2.12441111 d_loss: 1.40955949, g_loss: 0.65676486, ae_loss: 0.05808669\n",
      "Step: [616] total_loss: 2.11977577 d_loss: 1.37926841, g_loss: 0.68201935, ae_loss: 0.05848799\n",
      "Step: [617] total_loss: 2.12618399 d_loss: 1.38776076, g_loss: 0.68541813, ae_loss: 0.05300504\n",
      "Step: [618] total_loss: 2.12829733 d_loss: 1.36549401, g_loss: 0.70816803, ae_loss: 0.05463520\n",
      "Step: [619] total_loss: 2.11085129 d_loss: 1.37764609, g_loss: 0.67789495, ae_loss: 0.05531014\n",
      "Step: [620] total_loss: 2.09576297 d_loss: 1.35161889, g_loss: 0.68874848, ae_loss: 0.05539559\n",
      "Step: [621] total_loss: 2.12142992 d_loss: 1.38824320, g_loss: 0.67761374, ae_loss: 0.05557289\n",
      "Step: [622] total_loss: 2.09951353 d_loss: 1.36711252, g_loss: 0.67649937, ae_loss: 0.05590177\n",
      "Step: [623] total_loss: 2.11694336 d_loss: 1.37059832, g_loss: 0.68914676, ae_loss: 0.05719829\n",
      "Step: [624] total_loss: 2.12358332 d_loss: 1.35802150, g_loss: 0.71439624, ae_loss: 0.05116542\n",
      "Step: [625] total_loss: 2.14011669 d_loss: 1.38268971, g_loss: 0.70160818, ae_loss: 0.05581880\n",
      "Step: [626] total_loss: 2.13108683 d_loss: 1.37432027, g_loss: 0.69888210, ae_loss: 0.05788437\n",
      "Step: [627] total_loss: 2.14004731 d_loss: 1.37683117, g_loss: 0.70608330, ae_loss: 0.05713278\n",
      "Step: [628] total_loss: 2.12434435 d_loss: 1.36310816, g_loss: 0.70553493, ae_loss: 0.05570114\n",
      "Step: [629] total_loss: 2.12333703 d_loss: 1.38527799, g_loss: 0.68254781, ae_loss: 0.05551118\n",
      "Step: [630] total_loss: 2.11463690 d_loss: 1.35936666, g_loss: 0.70199227, ae_loss: 0.05327788\n",
      "Step: [631] total_loss: 2.14239120 d_loss: 1.38594174, g_loss: 0.69937372, ae_loss: 0.05707584\n",
      "Step: [632] total_loss: 2.14476514 d_loss: 1.40470791, g_loss: 0.68689984, ae_loss: 0.05315749\n",
      "Step: [633] total_loss: 2.13179612 d_loss: 1.39674640, g_loss: 0.68428886, ae_loss: 0.05076087\n",
      "Step: [634] total_loss: 2.09890842 d_loss: 1.36051798, g_loss: 0.68577790, ae_loss: 0.05261242\n",
      "Step: [635] total_loss: 2.12748718 d_loss: 1.39084625, g_loss: 0.67895055, ae_loss: 0.05769022\n",
      "Step: [636] total_loss: 2.11467528 d_loss: 1.36322844, g_loss: 0.69497353, ae_loss: 0.05647335\n",
      "Step: [637] total_loss: 2.12476158 d_loss: 1.38656580, g_loss: 0.68415785, ae_loss: 0.05403781\n",
      "Step: [638] total_loss: 2.12999296 d_loss: 1.37234139, g_loss: 0.70234030, ae_loss: 0.05531117\n",
      "Step: [639] total_loss: 2.10947561 d_loss: 1.38389277, g_loss: 0.66819739, ae_loss: 0.05738544\n",
      "Step: [640] total_loss: 2.11971140 d_loss: 1.37112045, g_loss: 0.69245428, ae_loss: 0.05613656\n",
      "Step: [641] total_loss: 2.13801861 d_loss: 1.37539673, g_loss: 0.71064961, ae_loss: 0.05197223\n",
      "Step: [642] total_loss: 2.12048197 d_loss: 1.38412762, g_loss: 0.68224347, ae_loss: 0.05411073\n",
      "Step: [643] total_loss: 2.12548232 d_loss: 1.38407874, g_loss: 0.68444365, ae_loss: 0.05695992\n",
      "Step: [644] total_loss: 2.11633158 d_loss: 1.37863111, g_loss: 0.68446958, ae_loss: 0.05323081\n",
      "Step: [645] total_loss: 2.11123657 d_loss: 1.37084746, g_loss: 0.68147725, ae_loss: 0.05891175\n",
      "Step: [646] total_loss: 2.16067839 d_loss: 1.39299750, g_loss: 0.70849919, ae_loss: 0.05918181\n",
      "Step: [647] total_loss: 2.10996747 d_loss: 1.36068177, g_loss: 0.69396198, ae_loss: 0.05532373\n",
      "Step: [648] total_loss: 2.14812279 d_loss: 1.41292167, g_loss: 0.68297285, ae_loss: 0.05222819\n",
      "Step: [649] total_loss: 2.09516430 d_loss: 1.34653246, g_loss: 0.69231915, ae_loss: 0.05631261\n",
      "Step: [650] total_loss: 2.10837889 d_loss: 1.37101245, g_loss: 0.68056536, ae_loss: 0.05680116\n",
      "Step: [651] total_loss: 2.11451435 d_loss: 1.37590718, g_loss: 0.68785912, ae_loss: 0.05074804\n",
      "Step: [652] total_loss: 2.13072610 d_loss: 1.38302672, g_loss: 0.69298011, ae_loss: 0.05471918\n",
      "Step: [653] total_loss: 2.12490273 d_loss: 1.39529753, g_loss: 0.67655754, ae_loss: 0.05304761\n",
      "Step: [654] total_loss: 2.12888265 d_loss: 1.38919640, g_loss: 0.68565893, ae_loss: 0.05402732\n",
      "Step: [655] total_loss: 2.11404228 d_loss: 1.37294543, g_loss: 0.68460310, ae_loss: 0.05649387\n",
      "Step: [656] total_loss: 2.10945702 d_loss: 1.36160183, g_loss: 0.69254637, ae_loss: 0.05530884\n",
      "Step: [657] total_loss: 2.12716746 d_loss: 1.39384699, g_loss: 0.68407071, ae_loss: 0.04924972\n",
      "Step: [658] total_loss: 2.14277124 d_loss: 1.38674092, g_loss: 0.70334876, ae_loss: 0.05268159\n",
      "Step: [659] total_loss: 2.13630581 d_loss: 1.38012922, g_loss: 0.70166183, ae_loss: 0.05451468\n",
      "Step: [660] total_loss: 2.13516378 d_loss: 1.38922215, g_loss: 0.69434738, ae_loss: 0.05159434\n",
      "Step: [661] total_loss: 2.13753533 d_loss: 1.37571371, g_loss: 0.70698714, ae_loss: 0.05483447\n",
      "Step: [662] total_loss: 2.12986827 d_loss: 1.38203120, g_loss: 0.69264692, ae_loss: 0.05519024\n",
      "Step: [663] total_loss: 2.12104321 d_loss: 1.38206995, g_loss: 0.68439353, ae_loss: 0.05457957\n",
      "Step: [664] total_loss: 2.09711838 d_loss: 1.35807991, g_loss: 0.68460310, ae_loss: 0.05443551\n",
      "Step: [665] total_loss: 2.11048985 d_loss: 1.36999297, g_loss: 0.68617511, ae_loss: 0.05432173\n",
      "Step: [666] total_loss: 2.10477972 d_loss: 1.37189388, g_loss: 0.68112957, ae_loss: 0.05175611\n",
      "Step: [667] total_loss: 2.11297393 d_loss: 1.36991668, g_loss: 0.68819517, ae_loss: 0.05486208\n",
      "Step: [668] total_loss: 2.11827993 d_loss: 1.37613010, g_loss: 0.68754470, ae_loss: 0.05460521\n",
      "Step: [669] total_loss: 2.11375666 d_loss: 1.36907613, g_loss: 0.68690133, ae_loss: 0.05777907\n",
      "Step: [670] total_loss: 2.11332083 d_loss: 1.37746787, g_loss: 0.68303752, ae_loss: 0.05281538\n",
      "Step: [671] total_loss: 2.11379051 d_loss: 1.37149978, g_loss: 0.68603170, ae_loss: 0.05625891\n",
      "Step: [672] total_loss: 2.14239979 d_loss: 1.39544654, g_loss: 0.69416070, ae_loss: 0.05279239\n",
      "Step: [673] total_loss: 2.13634944 d_loss: 1.39789319, g_loss: 0.68505329, ae_loss: 0.05340289\n",
      "Step: [674] total_loss: 2.14003801 d_loss: 1.38654518, g_loss: 0.69882017, ae_loss: 0.05467264\n",
      "Step: [675] total_loss: 2.15964317 d_loss: 1.39426112, g_loss: 0.71115410, ae_loss: 0.05422786\n",
      "Step: [676] total_loss: 2.13126183 d_loss: 1.37336218, g_loss: 0.70327401, ae_loss: 0.05462546\n",
      "Step: [677] total_loss: 2.11779785 d_loss: 1.35908675, g_loss: 0.70659411, ae_loss: 0.05211706\n",
      "Step: [678] total_loss: 2.14362502 d_loss: 1.37621999, g_loss: 0.70902073, ae_loss: 0.05838428\n",
      "Step: [679] total_loss: 2.12978053 d_loss: 1.40053129, g_loss: 0.67605972, ae_loss: 0.05318953\n",
      "Step: [680] total_loss: 2.11481547 d_loss: 1.36567760, g_loss: 0.69583452, ae_loss: 0.05330331\n",
      "Step: [681] total_loss: 2.13224697 d_loss: 1.37650156, g_loss: 0.69984114, ae_loss: 0.05590431\n",
      "Step: [682] total_loss: 2.14610338 d_loss: 1.37286305, g_loss: 0.71903741, ae_loss: 0.05420282\n",
      "Step: [683] total_loss: 2.14528894 d_loss: 1.40597010, g_loss: 0.68902409, ae_loss: 0.05029467\n",
      "Step: [684] total_loss: 2.10597277 d_loss: 1.37773979, g_loss: 0.67670935, ae_loss: 0.05152353\n",
      "Step: [685] total_loss: 2.09701061 d_loss: 1.36285233, g_loss: 0.67991424, ae_loss: 0.05424387\n",
      "Step: [686] total_loss: 2.10184050 d_loss: 1.37111926, g_loss: 0.67840087, ae_loss: 0.05232054\n",
      "Step: [687] total_loss: 2.13680816 d_loss: 1.38681853, g_loss: 0.69587058, ae_loss: 0.05411911\n",
      "Step: [688] total_loss: 2.15161872 d_loss: 1.41059005, g_loss: 0.69045705, ae_loss: 0.05057151\n",
      "Step: [689] total_loss: 2.14252114 d_loss: 1.39248824, g_loss: 0.69591331, ae_loss: 0.05411961\n",
      "Step: [690] total_loss: 2.13190031 d_loss: 1.37179077, g_loss: 0.70511395, ae_loss: 0.05499553\n",
      "Step: [691] total_loss: 2.12088490 d_loss: 1.38286352, g_loss: 0.68413174, ae_loss: 0.05388980\n",
      "Step: [692] total_loss: 2.13731718 d_loss: 1.41132283, g_loss: 0.67204845, ae_loss: 0.05394595\n",
      "Step: [693] total_loss: 2.14018106 d_loss: 1.41089547, g_loss: 0.67775577, ae_loss: 0.05152987\n",
      "Step: [694] total_loss: 2.12266898 d_loss: 1.40032983, g_loss: 0.66672313, ae_loss: 0.05561605\n",
      "Step: [695] total_loss: 2.11551070 d_loss: 1.36681962, g_loss: 0.69124860, ae_loss: 0.05744253\n",
      "Step: [696] total_loss: 2.10984087 d_loss: 1.37502265, g_loss: 0.68673217, ae_loss: 0.04808619\n",
      "Step: [697] total_loss: 2.11754227 d_loss: 1.37697232, g_loss: 0.68278754, ae_loss: 0.05778247\n",
      "Step: [698] total_loss: 2.13555002 d_loss: 1.39505267, g_loss: 0.68054020, ae_loss: 0.05995704\n",
      "Step: [699] total_loss: 2.12443638 d_loss: 1.35279560, g_loss: 0.71606839, ae_loss: 0.05557255\n",
      "Step: [700] total_loss: 2.11365891 d_loss: 1.35708761, g_loss: 0.70486879, ae_loss: 0.05170252\n",
      "Step: [701] total_loss: 2.08990240 d_loss: 1.35940373, g_loss: 0.67523110, ae_loss: 0.05526745\n",
      "Step: [702] total_loss: 2.10590339 d_loss: 1.37482035, g_loss: 0.67885983, ae_loss: 0.05222321\n",
      "Step: [703] total_loss: 2.09461164 d_loss: 1.35513318, g_loss: 0.68719292, ae_loss: 0.05228569\n",
      "Step: [704] total_loss: 2.13644314 d_loss: 1.40365636, g_loss: 0.68151212, ae_loss: 0.05127480\n",
      "Step: [705] total_loss: 2.15340447 d_loss: 1.38878179, g_loss: 0.70591062, ae_loss: 0.05871204\n",
      "Step: [706] total_loss: 2.15884614 d_loss: 1.40923214, g_loss: 0.69595402, ae_loss: 0.05365999\n",
      "Step: [707] total_loss: 2.11499405 d_loss: 1.36648870, g_loss: 0.69244391, ae_loss: 0.05606139\n",
      "Step: [708] total_loss: 2.10945511 d_loss: 1.34192646, g_loss: 0.71307296, ae_loss: 0.05445563\n",
      "Step: [709] total_loss: 2.09731340 d_loss: 1.37410045, g_loss: 0.67046863, ae_loss: 0.05274426\n",
      "Step: [710] total_loss: 2.11937237 d_loss: 1.39206922, g_loss: 0.67365599, ae_loss: 0.05364711\n",
      "Step: [711] total_loss: 2.11944818 d_loss: 1.38295507, g_loss: 0.68290257, ae_loss: 0.05359064\n",
      "Step: [712] total_loss: 2.14011979 d_loss: 1.37423897, g_loss: 0.71132445, ae_loss: 0.05455641\n",
      "Step: [713] total_loss: 2.11692858 d_loss: 1.35754478, g_loss: 0.70935476, ae_loss: 0.05002912\n",
      "Step: [714] total_loss: 2.13023353 d_loss: 1.39025712, g_loss: 0.68741757, ae_loss: 0.05255885\n",
      "Step: [715] total_loss: 2.13346720 d_loss: 1.39266825, g_loss: 0.68630683, ae_loss: 0.05449226\n",
      "Step: [716] total_loss: 2.14267731 d_loss: 1.40874553, g_loss: 0.67994088, ae_loss: 0.05399094\n",
      "Step: [717] total_loss: 2.10228062 d_loss: 1.37331557, g_loss: 0.67487562, ae_loss: 0.05408954\n",
      "Step: [718] total_loss: 2.11408281 d_loss: 1.38083255, g_loss: 0.67908752, ae_loss: 0.05416258\n",
      "Step: [719] total_loss: 2.11925459 d_loss: 1.37930214, g_loss: 0.68691850, ae_loss: 0.05303401\n",
      "Step: [720] total_loss: 2.09834433 d_loss: 1.36497021, g_loss: 0.68263543, ae_loss: 0.05073870\n",
      "Step: [721] total_loss: 2.11081934 d_loss: 1.38500929, g_loss: 0.66835868, ae_loss: 0.05745128\n",
      "Step: [722] total_loss: 2.10716963 d_loss: 1.35839736, g_loss: 0.69654405, ae_loss: 0.05222804\n",
      "Step: [723] total_loss: 2.13731074 d_loss: 1.38322806, g_loss: 0.69852412, ae_loss: 0.05555853\n",
      "Step: [724] total_loss: 2.12679911 d_loss: 1.38434696, g_loss: 0.68450570, ae_loss: 0.05794657\n",
      "Step: [725] total_loss: 2.13686895 d_loss: 1.39246142, g_loss: 0.68977082, ae_loss: 0.05463676\n",
      "Step: [726] total_loss: 2.14013219 d_loss: 1.38380396, g_loss: 0.70365602, ae_loss: 0.05267215\n",
      "Step: [727] total_loss: 2.11397409 d_loss: 1.37075090, g_loss: 0.68601716, ae_loss: 0.05720593\n",
      "Step: [728] total_loss: 2.14607334 d_loss: 1.39532125, g_loss: 0.69730037, ae_loss: 0.05345165\n",
      "Step: [729] total_loss: 2.13253307 d_loss: 1.39260209, g_loss: 0.68418360, ae_loss: 0.05574724\n",
      "Step: [730] total_loss: 2.13913774 d_loss: 1.38776815, g_loss: 0.70046389, ae_loss: 0.05090560\n",
      "Step: [731] total_loss: 2.14280701 d_loss: 1.39637136, g_loss: 0.69431734, ae_loss: 0.05211821\n",
      "Step: [732] total_loss: 2.13320112 d_loss: 1.39064813, g_loss: 0.68772757, ae_loss: 0.05482542\n",
      "Step: [733] total_loss: 2.12336636 d_loss: 1.38474035, g_loss: 0.68496239, ae_loss: 0.05366371\n",
      "Step: [734] total_loss: 2.12118268 d_loss: 1.38888788, g_loss: 0.68063724, ae_loss: 0.05165760\n",
      "Step: [735] total_loss: 2.13383222 d_loss: 1.37968457, g_loss: 0.70008457, ae_loss: 0.05406310\n",
      "Step: [736] total_loss: 2.11538744 d_loss: 1.37957144, g_loss: 0.68432915, ae_loss: 0.05148690\n",
      "Step: [737] total_loss: 2.14402509 d_loss: 1.39429200, g_loss: 0.69265747, ae_loss: 0.05707558\n",
      "Step: [738] total_loss: 2.13217139 d_loss: 1.38695657, g_loss: 0.68734765, ae_loss: 0.05786719\n",
      "Step: [739] total_loss: 2.12358046 d_loss: 1.38462210, g_loss: 0.68906629, ae_loss: 0.04989203\n",
      "Step: [740] total_loss: 2.11515999 d_loss: 1.37392426, g_loss: 0.68434727, ae_loss: 0.05688837\n",
      "Step: [741] total_loss: 2.10614371 d_loss: 1.37829399, g_loss: 0.67802876, ae_loss: 0.04982103\n",
      "Step: [742] total_loss: 2.11978602 d_loss: 1.36710715, g_loss: 0.69720435, ae_loss: 0.05547455\n",
      "Step: [743] total_loss: 2.15207934 d_loss: 1.39932323, g_loss: 0.69898546, ae_loss: 0.05377064\n",
      "Step: [744] total_loss: 2.12331820 d_loss: 1.36993337, g_loss: 0.69798946, ae_loss: 0.05539554\n",
      "Step: [745] total_loss: 2.14558458 d_loss: 1.39549184, g_loss: 0.69800514, ae_loss: 0.05208749\n",
      "Step: [746] total_loss: 2.13101792 d_loss: 1.38889050, g_loss: 0.68622541, ae_loss: 0.05590202\n",
      "Step: [747] total_loss: 2.11564541 d_loss: 1.37017834, g_loss: 0.69429606, ae_loss: 0.05117090\n",
      "Step: [748] total_loss: 2.09327078 d_loss: 1.36774004, g_loss: 0.67446172, ae_loss: 0.05106886\n",
      "Step: [749] total_loss: 2.10915899 d_loss: 1.37432909, g_loss: 0.67835176, ae_loss: 0.05647811\n",
      "Step: [750] total_loss: 2.11838675 d_loss: 1.35229182, g_loss: 0.70952529, ae_loss: 0.05656955\n",
      "Step: [751] total_loss: 2.12619615 d_loss: 1.39301240, g_loss: 0.67633736, ae_loss: 0.05684641\n",
      "Step: [752] total_loss: 2.11625195 d_loss: 1.39538121, g_loss: 0.66849649, ae_loss: 0.05237427\n",
      "Step: [753] total_loss: 2.12844419 d_loss: 1.38748312, g_loss: 0.68459809, ae_loss: 0.05636304\n",
      "Step: [754] total_loss: 2.14241886 d_loss: 1.38359523, g_loss: 0.70166808, ae_loss: 0.05715547\n",
      "Step: [755] total_loss: 2.14074254 d_loss: 1.36521375, g_loss: 0.72781861, ae_loss: 0.04771023\n",
      "Step: [756] total_loss: 2.14372158 d_loss: 1.40481985, g_loss: 0.68423116, ae_loss: 0.05467064\n",
      "Step: [757] total_loss: 2.12463427 d_loss: 1.37472665, g_loss: 0.69837612, ae_loss: 0.05153141\n",
      "Step: [758] total_loss: 2.15086365 d_loss: 1.39710236, g_loss: 0.69913018, ae_loss: 0.05463112\n",
      "Step: [759] total_loss: 2.14252377 d_loss: 1.39571834, g_loss: 0.69714868, ae_loss: 0.04965685\n",
      "Step: [760] total_loss: 2.10557652 d_loss: 1.37152970, g_loss: 0.68211621, ae_loss: 0.05193061\n",
      "Step: [761] total_loss: 2.08636880 d_loss: 1.35032177, g_loss: 0.69125170, ae_loss: 0.04479527\n",
      "Step: [762] total_loss: 2.12423563 d_loss: 1.39323449, g_loss: 0.67868602, ae_loss: 0.05231512\n",
      "Step: [763] total_loss: 2.14088488 d_loss: 1.38687396, g_loss: 0.70174837, ae_loss: 0.05226267\n",
      "Step: [764] total_loss: 2.15120840 d_loss: 1.41215920, g_loss: 0.68630815, ae_loss: 0.05274113\n",
      "Step: [765] total_loss: 2.10733199 d_loss: 1.35051894, g_loss: 0.70348394, ae_loss: 0.05332909\n",
      "Step: [766] total_loss: 2.15363765 d_loss: 1.38767624, g_loss: 0.70813060, ae_loss: 0.05783081\n",
      "Step: [767] total_loss: 2.13297915 d_loss: 1.39271760, g_loss: 0.68422920, ae_loss: 0.05603226\n",
      "Step: [768] total_loss: 2.12430382 d_loss: 1.38540614, g_loss: 0.68272865, ae_loss: 0.05616906\n",
      "Step: [769] total_loss: 2.13984036 d_loss: 1.39140308, g_loss: 0.69459307, ae_loss: 0.05384420\n",
      "Step: [770] total_loss: 2.13888550 d_loss: 1.37109137, g_loss: 0.71389675, ae_loss: 0.05389746\n",
      "Step: [771] total_loss: 2.12466240 d_loss: 1.37587976, g_loss: 0.69532263, ae_loss: 0.05346014\n",
      "Step: [772] total_loss: 2.10560775 d_loss: 1.37257373, g_loss: 0.68178278, ae_loss: 0.05125128\n",
      "Step: [773] total_loss: 2.11965656 d_loss: 1.38979363, g_loss: 0.67416066, ae_loss: 0.05570225\n",
      "Step: [774] total_loss: 2.13257241 d_loss: 1.39490294, g_loss: 0.68271273, ae_loss: 0.05495670\n",
      "Step: [775] total_loss: 2.12702727 d_loss: 1.38520968, g_loss: 0.68505490, ae_loss: 0.05676274\n",
      "Step: [776] total_loss: 2.12750816 d_loss: 1.36366034, g_loss: 0.71067679, ae_loss: 0.05317097\n",
      "Step: [777] total_loss: 2.11204934 d_loss: 1.38226342, g_loss: 0.67699105, ae_loss: 0.05279479\n",
      "Step: [778] total_loss: 2.11106968 d_loss: 1.37568212, g_loss: 0.68483829, ae_loss: 0.05054912\n",
      "Step: [779] total_loss: 2.13653064 d_loss: 1.40128565, g_loss: 0.68551898, ae_loss: 0.04972595\n",
      "Step: [780] total_loss: 2.13563347 d_loss: 1.39024448, g_loss: 0.69010913, ae_loss: 0.05527970\n",
      "Step: [781] total_loss: 2.12934160 d_loss: 1.36469364, g_loss: 0.70991462, ae_loss: 0.05473343\n",
      "Step: [782] total_loss: 2.13118529 d_loss: 1.38303375, g_loss: 0.69260687, ae_loss: 0.05554467\n",
      "Step: [783] total_loss: 2.14491296 d_loss: 1.37995017, g_loss: 0.71095788, ae_loss: 0.05400493\n",
      "Step: [784] total_loss: 2.11891174 d_loss: 1.36486816, g_loss: 0.69825488, ae_loss: 0.05578881\n",
      "Step: [785] total_loss: 2.11011791 d_loss: 1.36636043, g_loss: 0.69129938, ae_loss: 0.05245820\n",
      "Step: [786] total_loss: 2.12373281 d_loss: 1.40466762, g_loss: 0.66353607, ae_loss: 0.05552907\n",
      "Step: [787] total_loss: 2.11703444 d_loss: 1.37467337, g_loss: 0.68962681, ae_loss: 0.05273432\n",
      "Step: [788] total_loss: 2.10271692 d_loss: 1.36116433, g_loss: 0.69044590, ae_loss: 0.05110675\n",
      "Step: [789] total_loss: 2.12075472 d_loss: 1.38475549, g_loss: 0.68481398, ae_loss: 0.05118520\n",
      "Step: [790] total_loss: 2.12638903 d_loss: 1.36643314, g_loss: 0.70272028, ae_loss: 0.05723576\n",
      "Step: [791] total_loss: 2.10374117 d_loss: 1.36698055, g_loss: 0.67894590, ae_loss: 0.05781455\n",
      "Step: [792] total_loss: 2.11230659 d_loss: 1.38917923, g_loss: 0.67074311, ae_loss: 0.05238438\n",
      "Step: [793] total_loss: 2.13826275 d_loss: 1.36274683, g_loss: 0.71717930, ae_loss: 0.05833653\n",
      "Step: [794] total_loss: 2.13469315 d_loss: 1.36883569, g_loss: 0.70990258, ae_loss: 0.05595478\n",
      "Step: [795] total_loss: 2.14216971 d_loss: 1.37027907, g_loss: 0.72055584, ae_loss: 0.05133491\n",
      "Step: [796] total_loss: 2.14126873 d_loss: 1.38836515, g_loss: 0.69971812, ae_loss: 0.05318556\n",
      "Step: [797] total_loss: 2.12226105 d_loss: 1.38674140, g_loss: 0.68318152, ae_loss: 0.05233820\n",
      "Step: [798] total_loss: 2.11755562 d_loss: 1.37309480, g_loss: 0.68938506, ae_loss: 0.05507569\n",
      "Step: [799] total_loss: 2.10275364 d_loss: 1.36831582, g_loss: 0.67879140, ae_loss: 0.05564629\n",
      "Step: [800] total_loss: 2.12517929 d_loss: 1.39974022, g_loss: 0.67374170, ae_loss: 0.05169737\n",
      "Step: [801] total_loss: 2.12385321 d_loss: 1.38149858, g_loss: 0.68822014, ae_loss: 0.05413439\n",
      "Step: [802] total_loss: 2.13203192 d_loss: 1.38764942, g_loss: 0.69000810, ae_loss: 0.05437447\n",
      "Step: [803] total_loss: 2.11379862 d_loss: 1.38554561, g_loss: 0.67665333, ae_loss: 0.05159962\n",
      "Step: [804] total_loss: 2.14078426 d_loss: 1.39225650, g_loss: 0.69509280, ae_loss: 0.05343506\n",
      "Step: [805] total_loss: 2.14013958 d_loss: 1.40362263, g_loss: 0.68188131, ae_loss: 0.05463556\n",
      "Step: [806] total_loss: 2.11978269 d_loss: 1.35267770, g_loss: 0.71102440, ae_loss: 0.05608058\n",
      "Step: [807] total_loss: 2.16204596 d_loss: 1.41422904, g_loss: 0.69338119, ae_loss: 0.05443579\n",
      "Step: [808] total_loss: 2.14456439 d_loss: 1.38083291, g_loss: 0.71090877, ae_loss: 0.05282265\n",
      "Step: [809] total_loss: 2.15086842 d_loss: 1.39687347, g_loss: 0.70146924, ae_loss: 0.05252568\n",
      "Step: [810] total_loss: 2.12972307 d_loss: 1.37422109, g_loss: 0.70517743, ae_loss: 0.05032460\n",
      "Step: [811] total_loss: 2.15124273 d_loss: 1.39630747, g_loss: 0.70608807, ae_loss: 0.04884708\n",
      "Step: [812] total_loss: 2.10638714 d_loss: 1.37299752, g_loss: 0.68057346, ae_loss: 0.05281615\n",
      "Step: [813] total_loss: 2.14333296 d_loss: 1.39301729, g_loss: 0.69388008, ae_loss: 0.05643555\n",
      "Step: [814] total_loss: 2.10762739 d_loss: 1.36446428, g_loss: 0.68757385, ae_loss: 0.05558934\n",
      "Step: [815] total_loss: 2.10959864 d_loss: 1.38327432, g_loss: 0.67530859, ae_loss: 0.05101563\n",
      "Step: [816] total_loss: 2.14390111 d_loss: 1.39427400, g_loss: 0.69222480, ae_loss: 0.05740237\n",
      "Step: [817] total_loss: 2.12254810 d_loss: 1.38292122, g_loss: 0.68691456, ae_loss: 0.05271215\n",
      "Step: [818] total_loss: 2.12975669 d_loss: 1.37394929, g_loss: 0.70050120, ae_loss: 0.05530616\n",
      "Step: [819] total_loss: 2.13890815 d_loss: 1.37925231, g_loss: 0.70591909, ae_loss: 0.05373674\n",
      "Step: [820] total_loss: 2.12382746 d_loss: 1.37928414, g_loss: 0.69051278, ae_loss: 0.05403039\n",
      "Step: [821] total_loss: 2.11271906 d_loss: 1.38350177, g_loss: 0.67749691, ae_loss: 0.05172055\n",
      "Step: [822] total_loss: 2.11548471 d_loss: 1.39412236, g_loss: 0.66921949, ae_loss: 0.05214295\n",
      "Step: [823] total_loss: 2.09335256 d_loss: 1.35848880, g_loss: 0.68208456, ae_loss: 0.05277917\n",
      "Step: [824] total_loss: 2.12959003 d_loss: 1.39245236, g_loss: 0.68210781, ae_loss: 0.05503004\n",
      "Step: [825] total_loss: 2.10822582 d_loss: 1.38481164, g_loss: 0.66994774, ae_loss: 0.05346648\n",
      "Step: [826] total_loss: 2.11306000 d_loss: 1.38996923, g_loss: 0.67068428, ae_loss: 0.05240638\n",
      "Step: [827] total_loss: 2.13808155 d_loss: 1.38688898, g_loss: 0.69769681, ae_loss: 0.05349585\n",
      "Step: [828] total_loss: 2.12363410 d_loss: 1.38661921, g_loss: 0.68313301, ae_loss: 0.05388187\n",
      "Step: [829] total_loss: 2.11513948 d_loss: 1.36391163, g_loss: 0.70063686, ae_loss: 0.05059109\n",
      "Step: [830] total_loss: 2.14470720 d_loss: 1.38411570, g_loss: 0.70876527, ae_loss: 0.05182636\n",
      "Step: [831] total_loss: 2.14916849 d_loss: 1.39178109, g_loss: 0.70842052, ae_loss: 0.04896675\n",
      "Step: [832] total_loss: 2.14595318 d_loss: 1.39181650, g_loss: 0.70293510, ae_loss: 0.05120155\n",
      "Step: [833] total_loss: 2.12526369 d_loss: 1.38305271, g_loss: 0.69016600, ae_loss: 0.05204487\n",
      "Step: [834] total_loss: 2.12087870 d_loss: 1.36619854, g_loss: 0.70596778, ae_loss: 0.04871229\n",
      "Step: [835] total_loss: 2.11565351 d_loss: 1.36563110, g_loss: 0.69810092, ae_loss: 0.05192142\n",
      "Step: [836] total_loss: 2.11312437 d_loss: 1.36446607, g_loss: 0.69737184, ae_loss: 0.05128663\n",
      "Step: [837] total_loss: 2.12509918 d_loss: 1.38495493, g_loss: 0.68943775, ae_loss: 0.05070635\n",
      "Step: [838] total_loss: 2.14750338 d_loss: 1.40040076, g_loss: 0.69782603, ae_loss: 0.04927650\n",
      "Step: [839] total_loss: 2.11763692 d_loss: 1.36863101, g_loss: 0.69371217, ae_loss: 0.05529364\n",
      "Step: [840] total_loss: 2.13173914 d_loss: 1.40213108, g_loss: 0.67647493, ae_loss: 0.05313296\n",
      "Step: [841] total_loss: 2.11184192 d_loss: 1.37903607, g_loss: 0.68221754, ae_loss: 0.05058829\n",
      "Step: [842] total_loss: 2.11928916 d_loss: 1.38517916, g_loss: 0.67697608, ae_loss: 0.05713397\n",
      "Step: [843] total_loss: 2.12150121 d_loss: 1.38657176, g_loss: 0.67838353, ae_loss: 0.05654592\n",
      "Step: [844] total_loss: 2.13055825 d_loss: 1.37459242, g_loss: 0.70300496, ae_loss: 0.05296085\n",
      "Step: [845] total_loss: 2.17261934 d_loss: 1.42808294, g_loss: 0.69174564, ae_loss: 0.05279077\n",
      "Step: [846] total_loss: 2.15044332 d_loss: 1.39936101, g_loss: 0.69829851, ae_loss: 0.05278385\n",
      "Step: [847] total_loss: 2.12972093 d_loss: 1.39254880, g_loss: 0.68462431, ae_loss: 0.05254776\n",
      "Step: [848] total_loss: 2.11565399 d_loss: 1.36566722, g_loss: 0.69720060, ae_loss: 0.05278615\n",
      "Step: [849] total_loss: 2.13007021 d_loss: 1.38432324, g_loss: 0.68948293, ae_loss: 0.05626404\n",
      "Step: [850] total_loss: 2.12425709 d_loss: 1.39466465, g_loss: 0.67738473, ae_loss: 0.05220757\n",
      "Step: [851] total_loss: 2.15136385 d_loss: 1.41552174, g_loss: 0.68115789, ae_loss: 0.05468415\n",
      "Step: [852] total_loss: 2.14038587 d_loss: 1.39732313, g_loss: 0.68918389, ae_loss: 0.05387881\n",
      "Step: [853] total_loss: 2.14379644 d_loss: 1.38087487, g_loss: 0.70918822, ae_loss: 0.05373330\n",
      "Step: [854] total_loss: 2.13312984 d_loss: 1.37638080, g_loss: 0.69830304, ae_loss: 0.05844604\n",
      "Step: [855] total_loss: 2.13142991 d_loss: 1.36097050, g_loss: 0.71544904, ae_loss: 0.05501043\n",
      "Step: [856] total_loss: 2.12793827 d_loss: 1.37585902, g_loss: 0.70166373, ae_loss: 0.05041542\n",
      "Step: [857] total_loss: 2.12483692 d_loss: 1.39302564, g_loss: 0.68097347, ae_loss: 0.05083790\n",
      "Step: [858] total_loss: 2.14245820 d_loss: 1.39004433, g_loss: 0.69637823, ae_loss: 0.05603563\n",
      "Step: [859] total_loss: 2.15515423 d_loss: 1.39240146, g_loss: 0.71051228, ae_loss: 0.05224065\n",
      "Step: [860] total_loss: 2.11895561 d_loss: 1.38094425, g_loss: 0.68457961, ae_loss: 0.05343172\n",
      "Step: [861] total_loss: 2.11448097 d_loss: 1.36703789, g_loss: 0.69769418, ae_loss: 0.04974881\n",
      "Step: [862] total_loss: 2.11400795 d_loss: 1.36148083, g_loss: 0.69890749, ae_loss: 0.05361968\n",
      "Step: [863] total_loss: 2.10521221 d_loss: 1.37501919, g_loss: 0.67469370, ae_loss: 0.05549945\n",
      "Step: [864] total_loss: 2.11047578 d_loss: 1.36996460, g_loss: 0.69114548, ae_loss: 0.04936578\n",
      "Step: [865] total_loss: 2.11856866 d_loss: 1.38668168, g_loss: 0.68168491, ae_loss: 0.05020209\n",
      "Step: [866] total_loss: 2.13052082 d_loss: 1.37551403, g_loss: 0.70075822, ae_loss: 0.05424865\n",
      "Step: [867] total_loss: 2.12074423 d_loss: 1.37818861, g_loss: 0.68988389, ae_loss: 0.05267176\n",
      "Step: [868] total_loss: 2.11640787 d_loss: 1.37299061, g_loss: 0.68716967, ae_loss: 0.05624762\n",
      "Step: [869] total_loss: 2.12630010 d_loss: 1.38301992, g_loss: 0.68684024, ae_loss: 0.05643989\n",
      "Step: [870] total_loss: 2.13860798 d_loss: 1.38898933, g_loss: 0.69733274, ae_loss: 0.05228599\n",
      "Step: [871] total_loss: 2.13657808 d_loss: 1.38295889, g_loss: 0.69505143, ae_loss: 0.05856764\n",
      "Step: [872] total_loss: 2.14142179 d_loss: 1.40028226, g_loss: 0.69008148, ae_loss: 0.05105799\n",
      "Step: [873] total_loss: 2.14685369 d_loss: 1.39746749, g_loss: 0.69499469, ae_loss: 0.05439155\n",
      "Step: [874] total_loss: 2.13087177 d_loss: 1.36727095, g_loss: 0.70989883, ae_loss: 0.05370188\n",
      "Step: [875] total_loss: 2.13915205 d_loss: 1.34881008, g_loss: 0.73581529, ae_loss: 0.05452668\n",
      "Step: [876] total_loss: 2.14130783 d_loss: 1.38365054, g_loss: 0.70369321, ae_loss: 0.05396403\n",
      "Step: [877] total_loss: 2.13360310 d_loss: 1.36070240, g_loss: 0.72041512, ae_loss: 0.05248553\n",
      "Step: [878] total_loss: 2.12812662 d_loss: 1.40013969, g_loss: 0.67422342, ae_loss: 0.05376334\n",
      "Step: [879] total_loss: 2.12314558 d_loss: 1.40436959, g_loss: 0.67002130, ae_loss: 0.04875460\n",
      "Step: [880] total_loss: 2.12401700 d_loss: 1.36948586, g_loss: 0.70164734, ae_loss: 0.05288386\n",
      "Step: [881] total_loss: 2.14134073 d_loss: 1.41076231, g_loss: 0.67794567, ae_loss: 0.05263265\n",
      "Step: [882] total_loss: 2.12103772 d_loss: 1.37358117, g_loss: 0.69485337, ae_loss: 0.05260324\n",
      "Step: [883] total_loss: 2.11890411 d_loss: 1.38732481, g_loss: 0.67819160, ae_loss: 0.05338774\n",
      "Step: [884] total_loss: 2.13356495 d_loss: 1.39490128, g_loss: 0.68428564, ae_loss: 0.05437810\n",
      "Step: [885] total_loss: 2.11913228 d_loss: 1.39443350, g_loss: 0.67225921, ae_loss: 0.05243959\n",
      "Step: [886] total_loss: 2.13132882 d_loss: 1.38860202, g_loss: 0.68935841, ae_loss: 0.05336841\n",
      "Step: [887] total_loss: 2.13338470 d_loss: 1.37740982, g_loss: 0.70520109, ae_loss: 0.05077388\n",
      "Step: [888] total_loss: 2.13040733 d_loss: 1.38146496, g_loss: 0.69460678, ae_loss: 0.05433573\n",
      "Step: [889] total_loss: 2.15311337 d_loss: 1.38844275, g_loss: 0.71361148, ae_loss: 0.05105897\n",
      "Step: [890] total_loss: 2.14464927 d_loss: 1.38653541, g_loss: 0.70804507, ae_loss: 0.05006877\n",
      "Step: [891] total_loss: 2.13211179 d_loss: 1.37351036, g_loss: 0.70979887, ae_loss: 0.04880244\n",
      "Step: [892] total_loss: 2.13675690 d_loss: 1.39113986, g_loss: 0.69285655, ae_loss: 0.05276043\n",
      "Step: [893] total_loss: 2.11660719 d_loss: 1.37859607, g_loss: 0.68223798, ae_loss: 0.05577306\n",
      "Step: [894] total_loss: 2.11262465 d_loss: 1.37836039, g_loss: 0.68147933, ae_loss: 0.05278504\n",
      "Step: [895] total_loss: 2.10425115 d_loss: 1.38567924, g_loss: 0.66480678, ae_loss: 0.05376510\n",
      "Step: [896] total_loss: 2.10541439 d_loss: 1.37231612, g_loss: 0.68112755, ae_loss: 0.05197081\n",
      "Step: [897] total_loss: 2.13463497 d_loss: 1.39153433, g_loss: 0.69134861, ae_loss: 0.05175207\n",
      "Step: [898] total_loss: 2.13464975 d_loss: 1.37972164, g_loss: 0.70061958, ae_loss: 0.05430852\n",
      "Step: [899] total_loss: 2.14383101 d_loss: 1.37668705, g_loss: 0.71421683, ae_loss: 0.05292710\n",
      "Step: [900] total_loss: 2.12498116 d_loss: 1.37347627, g_loss: 0.70038676, ae_loss: 0.05111809\n",
      "Step: [901] total_loss: 2.14576912 d_loss: 1.37061882, g_loss: 0.71803904, ae_loss: 0.05711126\n",
      "Step: [902] total_loss: 2.12150860 d_loss: 1.39029860, g_loss: 0.67617774, ae_loss: 0.05503235\n",
      "Step: [903] total_loss: 2.09681368 d_loss: 1.36140537, g_loss: 0.68112779, ae_loss: 0.05428055\n",
      "Step: [904] total_loss: 2.11265945 d_loss: 1.36321044, g_loss: 0.69460857, ae_loss: 0.05484052\n",
      "Step: [905] total_loss: 2.13596535 d_loss: 1.40194011, g_loss: 0.67836714, ae_loss: 0.05565802\n",
      "Step: [906] total_loss: 2.12279892 d_loss: 1.37980556, g_loss: 0.69183141, ae_loss: 0.05116189\n",
      "Step: [907] total_loss: 2.11225510 d_loss: 1.35404706, g_loss: 0.70174444, ae_loss: 0.05646375\n",
      "Step: [908] total_loss: 2.12480116 d_loss: 1.37465167, g_loss: 0.69745165, ae_loss: 0.05269790\n",
      "Step: [909] total_loss: 2.14704871 d_loss: 1.40513277, g_loss: 0.68769979, ae_loss: 0.05421612\n",
      "Step: [910] total_loss: 2.13468838 d_loss: 1.36856246, g_loss: 0.71133244, ae_loss: 0.05479336\n",
      "Step: [911] total_loss: 2.14456224 d_loss: 1.37984204, g_loss: 0.71494287, ae_loss: 0.04977730\n",
      "Step: [912] total_loss: 2.12707615 d_loss: 1.37301087, g_loss: 0.70060921, ae_loss: 0.05345617\n",
      "Step: [913] total_loss: 2.15845537 d_loss: 1.40862107, g_loss: 0.69871783, ae_loss: 0.05111649\n",
      "Step: [914] total_loss: 2.11989093 d_loss: 1.36381459, g_loss: 0.70262796, ae_loss: 0.05344844\n",
      "Step: [915] total_loss: 2.12095308 d_loss: 1.34368610, g_loss: 0.72743720, ae_loss: 0.04982980\n",
      "Step: [916] total_loss: 2.10934043 d_loss: 1.36686158, g_loss: 0.69377375, ae_loss: 0.04870509\n",
      "Step: [917] total_loss: 2.12661242 d_loss: 1.38999343, g_loss: 0.68186474, ae_loss: 0.05475427\n",
      "Step: [918] total_loss: 2.10596085 d_loss: 1.35555291, g_loss: 0.69990921, ae_loss: 0.05049860\n",
      "Step: [919] total_loss: 2.11123800 d_loss: 1.36574781, g_loss: 0.69471645, ae_loss: 0.05077385\n",
      "Step: [920] total_loss: 2.12724638 d_loss: 1.38852465, g_loss: 0.68770951, ae_loss: 0.05101221\n",
      "Step: [921] total_loss: 2.14210129 d_loss: 1.37552094, g_loss: 0.71496046, ae_loss: 0.05162002\n",
      "Step: [922] total_loss: 2.13365388 d_loss: 1.39645994, g_loss: 0.68369406, ae_loss: 0.05349999\n",
      "Step: [923] total_loss: 2.12799382 d_loss: 1.37332606, g_loss: 0.69763792, ae_loss: 0.05702979\n",
      "Step: [924] total_loss: 2.13000798 d_loss: 1.38392234, g_loss: 0.69518793, ae_loss: 0.05089774\n",
      "Step: [925] total_loss: 2.14994025 d_loss: 1.39267778, g_loss: 0.70082158, ae_loss: 0.05644086\n",
      "Step: [926] total_loss: 2.12439537 d_loss: 1.37333906, g_loss: 0.69819874, ae_loss: 0.05285750\n",
      "Step: [927] total_loss: 2.14389372 d_loss: 1.38362932, g_loss: 0.70601881, ae_loss: 0.05424541\n",
      "Step: [928] total_loss: 2.11046958 d_loss: 1.36201644, g_loss: 0.69386798, ae_loss: 0.05458513\n",
      "Step: [929] total_loss: 2.13612413 d_loss: 1.41447496, g_loss: 0.66966778, ae_loss: 0.05198127\n",
      "Step: [930] total_loss: 2.14059973 d_loss: 1.39651620, g_loss: 0.68669683, ae_loss: 0.05738661\n",
      "Step: [931] total_loss: 2.16478944 d_loss: 1.41254735, g_loss: 0.69447434, ae_loss: 0.05776777\n",
      "Step: [932] total_loss: 2.14115596 d_loss: 1.38565361, g_loss: 0.70299387, ae_loss: 0.05250850\n",
      "Step: [933] total_loss: 2.15225577 d_loss: 1.36300242, g_loss: 0.73240411, ae_loss: 0.05684927\n",
      "Step: [934] total_loss: 2.14039803 d_loss: 1.38604403, g_loss: 0.69473505, ae_loss: 0.05961877\n",
      "Step: [935] total_loss: 2.13551569 d_loss: 1.39144814, g_loss: 0.68815076, ae_loss: 0.05591669\n",
      "Step: [936] total_loss: 2.13728952 d_loss: 1.36143756, g_loss: 0.72140372, ae_loss: 0.05444835\n",
      "Step: [937] total_loss: 2.13799644 d_loss: 1.38084364, g_loss: 0.70189327, ae_loss: 0.05525964\n",
      "Step: [938] total_loss: 2.12762189 d_loss: 1.39992213, g_loss: 0.67430335, ae_loss: 0.05339643\n",
      "Step: [939] total_loss: 2.15385628 d_loss: 1.41854668, g_loss: 0.67998654, ae_loss: 0.05532309\n",
      "Step: [940] total_loss: 2.15236807 d_loss: 1.41378331, g_loss: 0.68258178, ae_loss: 0.05600314\n",
      "Step: [941] total_loss: 2.12992334 d_loss: 1.37399912, g_loss: 0.70113003, ae_loss: 0.05479423\n",
      "Step: [942] total_loss: 2.12940025 d_loss: 1.37885630, g_loss: 0.69297993, ae_loss: 0.05756405\n",
      "Step: [943] total_loss: 2.15623617 d_loss: 1.39390779, g_loss: 0.71029079, ae_loss: 0.05203743\n",
      "Step: [944] total_loss: 2.11951566 d_loss: 1.38289964, g_loss: 0.68216586, ae_loss: 0.05445014\n",
      "Step: [945] total_loss: 2.13354611 d_loss: 1.37876284, g_loss: 0.70160908, ae_loss: 0.05317418\n",
      "Step: [946] total_loss: 2.11439419 d_loss: 1.37614775, g_loss: 0.68514991, ae_loss: 0.05309642\n",
      "Step: [947] total_loss: 2.09428978 d_loss: 1.38232601, g_loss: 0.65890181, ae_loss: 0.05306182\n",
      "Step: [948] total_loss: 2.13044643 d_loss: 1.40193892, g_loss: 0.67321849, ae_loss: 0.05528916\n",
      "Step: [949] total_loss: 2.15538359 d_loss: 1.40055192, g_loss: 0.69793296, ae_loss: 0.05689882\n",
      "Step: [950] total_loss: 2.13122511 d_loss: 1.39208233, g_loss: 0.68503416, ae_loss: 0.05410852\n",
      "Step: [951] total_loss: 2.13443518 d_loss: 1.37723422, g_loss: 0.70135027, ae_loss: 0.05585076\n",
      "Step: [952] total_loss: 2.13399386 d_loss: 1.38140035, g_loss: 0.70045280, ae_loss: 0.05214066\n",
      "Step: [953] total_loss: 2.13312531 d_loss: 1.38740718, g_loss: 0.68884015, ae_loss: 0.05687802\n",
      "Step: [954] total_loss: 2.13774109 d_loss: 1.39985800, g_loss: 0.67939699, ae_loss: 0.05848595\n",
      "Step: [955] total_loss: 2.14250541 d_loss: 1.38731003, g_loss: 0.70086056, ae_loss: 0.05433491\n",
      "Step: [956] total_loss: 2.13068628 d_loss: 1.37680817, g_loss: 0.69976246, ae_loss: 0.05411568\n",
      "Step: [957] total_loss: 2.14700127 d_loss: 1.40251684, g_loss: 0.68971384, ae_loss: 0.05477057\n",
      "Step: [958] total_loss: 2.12626982 d_loss: 1.39158571, g_loss: 0.67980164, ae_loss: 0.05488257\n",
      "Step: [959] total_loss: 2.13106751 d_loss: 1.38799524, g_loss: 0.68425196, ae_loss: 0.05882036\n",
      "Step: [960] total_loss: 2.13931274 d_loss: 1.39336240, g_loss: 0.69413316, ae_loss: 0.05181721\n",
      "Step: [961] total_loss: 2.12343001 d_loss: 1.38026738, g_loss: 0.68845701, ae_loss: 0.05470564\n",
      "Step: [962] total_loss: 2.11645818 d_loss: 1.38033557, g_loss: 0.68490982, ae_loss: 0.05121285\n",
      "Step: [963] total_loss: 2.12062716 d_loss: 1.37942195, g_loss: 0.68743169, ae_loss: 0.05377354\n",
      "Step: [964] total_loss: 2.11343145 d_loss: 1.37272286, g_loss: 0.68471313, ae_loss: 0.05599531\n",
      "Step: [965] total_loss: 2.13242960 d_loss: 1.37567222, g_loss: 0.70189893, ae_loss: 0.05485860\n",
      "Step: [966] total_loss: 2.14502859 d_loss: 1.37586379, g_loss: 0.71531260, ae_loss: 0.05385216\n",
      "Step: [967] total_loss: 2.14723754 d_loss: 1.39791989, g_loss: 0.69718671, ae_loss: 0.05213089\n",
      "Step: [968] total_loss: 2.12735891 d_loss: 1.39986372, g_loss: 0.66971827, ae_loss: 0.05777688\n",
      "Step: [969] total_loss: 2.12951183 d_loss: 1.37334120, g_loss: 0.70306814, ae_loss: 0.05310262\n",
      "Step: [970] total_loss: 2.13410378 d_loss: 1.40190864, g_loss: 0.67972171, ae_loss: 0.05247350\n",
      "Step: [971] total_loss: 2.11481547 d_loss: 1.38214135, g_loss: 0.67959666, ae_loss: 0.05307749\n",
      "Step: [972] total_loss: 2.08399844 d_loss: 1.34957409, g_loss: 0.68154520, ae_loss: 0.05287910\n",
      "Step: [973] total_loss: 2.11078882 d_loss: 1.36678553, g_loss: 0.68912977, ae_loss: 0.05487347\n",
      "Step: [974] total_loss: 2.13950753 d_loss: 1.38264298, g_loss: 0.70384282, ae_loss: 0.05302184\n",
      "Step: [975] total_loss: 2.13789082 d_loss: 1.36706066, g_loss: 0.71698040, ae_loss: 0.05384982\n",
      "Step: [976] total_loss: 2.11265373 d_loss: 1.37255466, g_loss: 0.68640351, ae_loss: 0.05369560\n",
      "Step: [977] total_loss: 2.12190485 d_loss: 1.37976122, g_loss: 0.68791437, ae_loss: 0.05422933\n",
      "Step: [978] total_loss: 2.10188365 d_loss: 1.37116098, g_loss: 0.67636096, ae_loss: 0.05436172\n",
      "Step: [979] total_loss: 2.11109805 d_loss: 1.40400147, g_loss: 0.65413976, ae_loss: 0.05295682\n",
      "Step: [980] total_loss: 2.11141825 d_loss: 1.38039565, g_loss: 0.67201495, ae_loss: 0.05900772\n",
      "Step: [981] total_loss: 2.11531258 d_loss: 1.38482606, g_loss: 0.67849797, ae_loss: 0.05198852\n",
      "Step: [982] total_loss: 2.12541294 d_loss: 1.37808383, g_loss: 0.69579673, ae_loss: 0.05153228\n",
      "Step: [983] total_loss: 2.14404655 d_loss: 1.39083123, g_loss: 0.69950271, ae_loss: 0.05371264\n",
      "Step: [984] total_loss: 2.15840149 d_loss: 1.39254141, g_loss: 0.71032155, ae_loss: 0.05553870\n",
      "Step: [985] total_loss: 2.14008164 d_loss: 1.36396408, g_loss: 0.72207940, ae_loss: 0.05403813\n",
      "Step: [986] total_loss: 2.16429901 d_loss: 1.39585543, g_loss: 0.71757436, ae_loss: 0.05086921\n",
      "Step: [987] total_loss: 2.12741613 d_loss: 1.37180269, g_loss: 0.70298302, ae_loss: 0.05263048\n",
      "Step: [988] total_loss: 2.11676598 d_loss: 1.38782287, g_loss: 0.67344117, ae_loss: 0.05550206\n",
      "Step: [989] total_loss: 2.10431433 d_loss: 1.38112199, g_loss: 0.66367912, ae_loss: 0.05951311\n",
      "Step: [990] total_loss: 2.13186431 d_loss: 1.39137316, g_loss: 0.68657613, ae_loss: 0.05391498\n",
      "Step: [991] total_loss: 2.10869169 d_loss: 1.38120461, g_loss: 0.67337382, ae_loss: 0.05411318\n",
      "Step: [992] total_loss: 2.10800529 d_loss: 1.37908721, g_loss: 0.67389619, ae_loss: 0.05502185\n",
      "Step: [993] total_loss: 2.15694523 d_loss: 1.41919422, g_loss: 0.68839347, ae_loss: 0.04935755\n",
      "Step: [994] total_loss: 2.14075089 d_loss: 1.39171684, g_loss: 0.69593596, ae_loss: 0.05309804\n",
      "Step: [995] total_loss: 2.13975120 d_loss: 1.37473464, g_loss: 0.71578479, ae_loss: 0.04923180\n",
      "Step: [996] total_loss: 2.16372299 d_loss: 1.41845703, g_loss: 0.69219583, ae_loss: 0.05307024\n",
      "Step: [997] total_loss: 2.12487626 d_loss: 1.39349711, g_loss: 0.68040717, ae_loss: 0.05097203\n",
      "Step: [998] total_loss: 2.12756777 d_loss: 1.39007330, g_loss: 0.68392611, ae_loss: 0.05356844\n",
      "Step: [999] total_loss: 2.13424826 d_loss: 1.38291609, g_loss: 0.69704401, ae_loss: 0.05428815\n",
      "Step: [1000] total_loss: 2.11442947 d_loss: 1.35075736, g_loss: 0.70846575, ae_loss: 0.05520624\n",
      "Step: [1001] total_loss: 2.14097571 d_loss: 1.37524867, g_loss: 0.71451157, ae_loss: 0.05121546\n",
      "Step: [1002] total_loss: 2.13806200 d_loss: 1.38384545, g_loss: 0.70222998, ae_loss: 0.05198643\n",
      "Step: [1003] total_loss: 2.14849472 d_loss: 1.39375377, g_loss: 0.69916701, ae_loss: 0.05557397\n",
      "Step: [1004] total_loss: 2.12722874 d_loss: 1.37569714, g_loss: 0.70425218, ae_loss: 0.04727942\n",
      "Step: [1005] total_loss: 2.11797762 d_loss: 1.38770437, g_loss: 0.67688179, ae_loss: 0.05339145\n",
      "Step: [1006] total_loss: 2.10817671 d_loss: 1.35404837, g_loss: 0.69682842, ae_loss: 0.05729984\n",
      "Step: [1007] total_loss: 2.14544630 d_loss: 1.38899589, g_loss: 0.70154357, ae_loss: 0.05490668\n",
      "Step: [1008] total_loss: 2.11285210 d_loss: 1.35834634, g_loss: 0.69849944, ae_loss: 0.05600616\n",
      "Step: [1009] total_loss: 2.11449790 d_loss: 1.38199615, g_loss: 0.67866063, ae_loss: 0.05384111\n",
      "Step: [1010] total_loss: 2.11322498 d_loss: 1.37520659, g_loss: 0.68315458, ae_loss: 0.05486378\n",
      "Step: [1011] total_loss: 2.15548587 d_loss: 1.41318679, g_loss: 0.69132608, ae_loss: 0.05097302\n",
      "Step: [1012] total_loss: 2.14218569 d_loss: 1.38272119, g_loss: 0.70636278, ae_loss: 0.05310174\n",
      "Step: [1013] total_loss: 2.13629150 d_loss: 1.39892232, g_loss: 0.68368387, ae_loss: 0.05368538\n",
      "Step: [1014] total_loss: 2.12331820 d_loss: 1.36038733, g_loss: 0.71089482, ae_loss: 0.05203611\n",
      "Step: [1015] total_loss: 2.15892768 d_loss: 1.38524306, g_loss: 0.71823651, ae_loss: 0.05544804\n",
      "Step: [1016] total_loss: 2.14058733 d_loss: 1.36917996, g_loss: 0.71623743, ae_loss: 0.05516994\n",
      "Step: [1017] total_loss: 2.11154795 d_loss: 1.34724450, g_loss: 0.71170378, ae_loss: 0.05259949\n",
      "Step: [1018] total_loss: 2.13560224 d_loss: 1.41040242, g_loss: 0.67232448, ae_loss: 0.05287531\n",
      "Step: [1019] total_loss: 2.12380910 d_loss: 1.37848806, g_loss: 0.69107836, ae_loss: 0.05424273\n",
      "Step: [1020] total_loss: 2.12003517 d_loss: 1.40252900, g_loss: 0.66527313, ae_loss: 0.05223313\n",
      "Step: [1021] total_loss: 2.11375189 d_loss: 1.39265227, g_loss: 0.66680288, ae_loss: 0.05429681\n",
      "Step: [1022] total_loss: 2.12309837 d_loss: 1.39343727, g_loss: 0.67720038, ae_loss: 0.05246079\n",
      "Step: [1023] total_loss: 2.13864470 d_loss: 1.39824522, g_loss: 0.68512994, ae_loss: 0.05526948\n",
      "Step: [1024] total_loss: 2.13676763 d_loss: 1.38355827, g_loss: 0.70386100, ae_loss: 0.04934835\n",
      "Step: [1025] total_loss: 2.13680100 d_loss: 1.38420677, g_loss: 0.69833362, ae_loss: 0.05426063\n",
      "Step: [1026] total_loss: 2.12113833 d_loss: 1.38543344, g_loss: 0.68085665, ae_loss: 0.05484816\n",
      "Step: [1027] total_loss: 2.14768386 d_loss: 1.38809752, g_loss: 0.70925862, ae_loss: 0.05032783\n",
      "Step: [1028] total_loss: 2.13549066 d_loss: 1.38707495, g_loss: 0.69604313, ae_loss: 0.05237255\n",
      "Step: [1029] total_loss: 2.12676907 d_loss: 1.39093161, g_loss: 0.68122423, ae_loss: 0.05461308\n",
      "Step: [1030] total_loss: 2.13716984 d_loss: 1.37845469, g_loss: 0.70497000, ae_loss: 0.05374523\n",
      "Step: [1031] total_loss: 2.10350227 d_loss: 1.36859238, g_loss: 0.68553030, ae_loss: 0.04937961\n",
      "Step: [1032] total_loss: 2.12476993 d_loss: 1.38812590, g_loss: 0.68443584, ae_loss: 0.05220818\n",
      "Step: [1033] total_loss: 2.11828661 d_loss: 1.37383270, g_loss: 0.68824649, ae_loss: 0.05620756\n",
      "Step: [1034] total_loss: 2.12820005 d_loss: 1.38737452, g_loss: 0.69121718, ae_loss: 0.04960822\n",
      "Step: [1035] total_loss: 2.11180758 d_loss: 1.37326372, g_loss: 0.68720531, ae_loss: 0.05133858\n",
      "Step: [1036] total_loss: 2.15854740 d_loss: 1.42000318, g_loss: 0.68646014, ae_loss: 0.05208399\n",
      "Step: [1037] total_loss: 2.12805009 d_loss: 1.37461376, g_loss: 0.70213681, ae_loss: 0.05129963\n",
      "Step: [1038] total_loss: 2.14453077 d_loss: 1.38072228, g_loss: 0.70897180, ae_loss: 0.05483659\n",
      "Step: [1039] total_loss: 2.13034415 d_loss: 1.38789988, g_loss: 0.68845636, ae_loss: 0.05398794\n",
      "Step: [1040] total_loss: 2.12732649 d_loss: 1.38118720, g_loss: 0.69426435, ae_loss: 0.05187494\n",
      "Step: [1041] total_loss: 2.12746620 d_loss: 1.38813603, g_loss: 0.68486011, ae_loss: 0.05447019\n",
      "Step: [1042] total_loss: 2.14895964 d_loss: 1.40732980, g_loss: 0.68656337, ae_loss: 0.05506642\n",
      "Step: [1043] total_loss: 2.13480949 d_loss: 1.38222694, g_loss: 0.70131087, ae_loss: 0.05127174\n",
      "Step: [1044] total_loss: 2.13772893 d_loss: 1.37746227, g_loss: 0.70606947, ae_loss: 0.05419719\n",
      "Step: [1045] total_loss: 2.14232826 d_loss: 1.39673924, g_loss: 0.69592094, ae_loss: 0.04966796\n",
      "Step: [1046] total_loss: 2.11368895 d_loss: 1.37573469, g_loss: 0.68839669, ae_loss: 0.04955759\n",
      "Step: [1047] total_loss: 2.11174297 d_loss: 1.38097715, g_loss: 0.68077695, ae_loss: 0.04998874\n",
      "Step: [1048] total_loss: 2.15274715 d_loss: 1.39974570, g_loss: 0.70120215, ae_loss: 0.05179920\n",
      "Step: [1049] total_loss: 2.12447619 d_loss: 1.38791013, g_loss: 0.68036187, ae_loss: 0.05620421\n",
      "Step: [1050] total_loss: 2.13157058 d_loss: 1.39542055, g_loss: 0.68629330, ae_loss: 0.04985680\n",
      "Step: [1051] total_loss: 2.12332201 d_loss: 1.37389565, g_loss: 0.69572383, ae_loss: 0.05370254\n",
      "Step: [1052] total_loss: 2.13329816 d_loss: 1.37726712, g_loss: 0.70368141, ae_loss: 0.05234959\n",
      "Step: [1053] total_loss: 2.14409304 d_loss: 1.40272355, g_loss: 0.68568027, ae_loss: 0.05568904\n",
      "Step: [1054] total_loss: 2.12411118 d_loss: 1.37611830, g_loss: 0.69392395, ae_loss: 0.05406893\n",
      "Step: [1055] total_loss: 2.11826849 d_loss: 1.36669016, g_loss: 0.70052278, ae_loss: 0.05105545\n",
      "Step: [1056] total_loss: 2.11980343 d_loss: 1.38583899, g_loss: 0.68059593, ae_loss: 0.05336852\n",
      "Step: [1057] total_loss: 2.11908197 d_loss: 1.38546944, g_loss: 0.68490160, ae_loss: 0.04871083\n",
      "Step: [1058] total_loss: 2.13490582 d_loss: 1.38450384, g_loss: 0.69860375, ae_loss: 0.05179840\n",
      "Step: [1059] total_loss: 2.13342857 d_loss: 1.39884639, g_loss: 0.67859054, ae_loss: 0.05599160\n",
      "Step: [1060] total_loss: 2.11916542 d_loss: 1.37813258, g_loss: 0.68710375, ae_loss: 0.05392896\n",
      "Step: [1061] total_loss: 2.13846827 d_loss: 1.39955449, g_loss: 0.68474710, ae_loss: 0.05416656\n",
      "Step: [1062] total_loss: 2.14611006 d_loss: 1.39562488, g_loss: 0.69325775, ae_loss: 0.05722743\n",
      "Step: [1063] total_loss: 2.16288185 d_loss: 1.39147043, g_loss: 0.71974015, ae_loss: 0.05167136\n",
      "Step: [1064] total_loss: 2.14615965 d_loss: 1.39397645, g_loss: 0.69810843, ae_loss: 0.05407478\n",
      "Step: [1065] total_loss: 2.17733359 d_loss: 1.40918827, g_loss: 0.71569312, ae_loss: 0.05245217\n",
      "Step: [1066] total_loss: 2.13951826 d_loss: 1.38192225, g_loss: 0.70466554, ae_loss: 0.05293046\n",
      "Step: [1067] total_loss: 2.13271809 d_loss: 1.39349604, g_loss: 0.68727171, ae_loss: 0.05195033\n",
      "Step: [1068] total_loss: 2.10783148 d_loss: 1.36968923, g_loss: 0.68686104, ae_loss: 0.05128131\n",
      "Step: [1069] total_loss: 2.12647486 d_loss: 1.37285030, g_loss: 0.69895965, ae_loss: 0.05466485\n",
      "Step: [1070] total_loss: 2.13195348 d_loss: 1.39763331, g_loss: 0.68317944, ae_loss: 0.05114077\n",
      "Step: [1071] total_loss: 2.11462927 d_loss: 1.36876225, g_loss: 0.69434011, ae_loss: 0.05152686\n",
      "Step: [1072] total_loss: 2.14372587 d_loss: 1.38168085, g_loss: 0.70712477, ae_loss: 0.05492023\n",
      "Step: [1073] total_loss: 2.13424778 d_loss: 1.39334369, g_loss: 0.68827629, ae_loss: 0.05262771\n",
      "Step: [1074] total_loss: 2.10995817 d_loss: 1.36418641, g_loss: 0.68970096, ae_loss: 0.05607093\n",
      "Step: [1075] total_loss: 2.12486339 d_loss: 1.39365971, g_loss: 0.67648852, ae_loss: 0.05471514\n",
      "Step: [1076] total_loss: 2.10761547 d_loss: 1.38783622, g_loss: 0.66460788, ae_loss: 0.05517123\n",
      "Step: [1077] total_loss: 2.10142565 d_loss: 1.36732531, g_loss: 0.67788076, ae_loss: 0.05621960\n",
      "Step: [1078] total_loss: 2.12649918 d_loss: 1.38590431, g_loss: 0.68423533, ae_loss: 0.05635954\n",
      "Step: [1079] total_loss: 2.14608169 d_loss: 1.40132141, g_loss: 0.69600803, ae_loss: 0.04875231\n",
      "Step: [1080] total_loss: 2.15355682 d_loss: 1.39255238, g_loss: 0.70815313, ae_loss: 0.05285144\n",
      "Step: [1081] total_loss: 2.13866019 d_loss: 1.37438214, g_loss: 0.71482295, ae_loss: 0.04945514\n",
      "Step: [1082] total_loss: 2.14783192 d_loss: 1.38300300, g_loss: 0.71093941, ae_loss: 0.05388947\n",
      "Step: [1083] total_loss: 2.12381935 d_loss: 1.37386537, g_loss: 0.69629395, ae_loss: 0.05365989\n",
      "Step: [1084] total_loss: 2.11771369 d_loss: 1.36175334, g_loss: 0.70400172, ae_loss: 0.05195869\n",
      "Step: [1085] total_loss: 2.12431383 d_loss: 1.38299441, g_loss: 0.68879157, ae_loss: 0.05252778\n",
      "Step: [1086] total_loss: 2.11696053 d_loss: 1.36264932, g_loss: 0.69880128, ae_loss: 0.05550995\n",
      "Step: [1087] total_loss: 2.12097573 d_loss: 1.37194395, g_loss: 0.69316149, ae_loss: 0.05587027\n",
      "Step: [1088] total_loss: 2.12790227 d_loss: 1.38048947, g_loss: 0.69371641, ae_loss: 0.05369638\n",
      "Step: [1089] total_loss: 2.13731122 d_loss: 1.40066969, g_loss: 0.68119365, ae_loss: 0.05544798\n",
      "Step: [1090] total_loss: 2.13337827 d_loss: 1.37706065, g_loss: 0.70155495, ae_loss: 0.05476264\n",
      "Step: [1091] total_loss: 2.14076042 d_loss: 1.37166023, g_loss: 0.70875013, ae_loss: 0.06035016\n",
      "Step: [1092] total_loss: 2.12516546 d_loss: 1.38643956, g_loss: 0.68536520, ae_loss: 0.05336069\n",
      "Step: [1093] total_loss: 2.13765860 d_loss: 1.38055778, g_loss: 0.69916970, ae_loss: 0.05793121\n",
      "Step: [1094] total_loss: 2.12835479 d_loss: 1.39285755, g_loss: 0.68271333, ae_loss: 0.05278398\n",
      "Step: [1095] total_loss: 2.11927128 d_loss: 1.38341475, g_loss: 0.68365407, ae_loss: 0.05220240\n",
      "Step: [1096] total_loss: 2.13335085 d_loss: 1.38005137, g_loss: 0.70179200, ae_loss: 0.05150734\n",
      "Step: [1097] total_loss: 2.13622808 d_loss: 1.38960350, g_loss: 0.69514155, ae_loss: 0.05148298\n",
      "Step: [1098] total_loss: 2.12357831 d_loss: 1.37717950, g_loss: 0.68915975, ae_loss: 0.05723907\n",
      "Step: [1099] total_loss: 2.13201094 d_loss: 1.38540006, g_loss: 0.69214880, ae_loss: 0.05446215\n",
      "Step: [1100] total_loss: 2.13547993 d_loss: 1.37136412, g_loss: 0.70937186, ae_loss: 0.05474401\n",
      "Step: [1101] total_loss: 2.11400557 d_loss: 1.36664867, g_loss: 0.69663525, ae_loss: 0.05072151\n",
      "Step: [1102] total_loss: 2.11906338 d_loss: 1.39179981, g_loss: 0.67467517, ae_loss: 0.05258837\n",
      "Step: [1103] total_loss: 2.17598176 d_loss: 1.40931845, g_loss: 0.71420437, ae_loss: 0.05245900\n",
      "Step: [1104] total_loss: 2.13862085 d_loss: 1.38271236, g_loss: 0.70151997, ae_loss: 0.05438855\n",
      "Step: [1105] total_loss: 2.13284516 d_loss: 1.38343930, g_loss: 0.69182688, ae_loss: 0.05757894\n",
      "Step: [1106] total_loss: 2.14537430 d_loss: 1.39889991, g_loss: 0.69298732, ae_loss: 0.05348711\n",
      "Step: [1107] total_loss: 2.14036083 d_loss: 1.39853144, g_loss: 0.68247652, ae_loss: 0.05935282\n",
      "Step: [1108] total_loss: 2.13014150 d_loss: 1.37730670, g_loss: 0.69534427, ae_loss: 0.05749055\n",
      "Step: [1109] total_loss: 2.12193441 d_loss: 1.38487399, g_loss: 0.68737698, ae_loss: 0.04968347\n",
      "Step: [1110] total_loss: 2.15016317 d_loss: 1.40451217, g_loss: 0.69150138, ae_loss: 0.05414969\n",
      "Step: [1111] total_loss: 2.14223385 d_loss: 1.37691963, g_loss: 0.71216357, ae_loss: 0.05315068\n",
      "Step: [1112] total_loss: 2.12154722 d_loss: 1.37664557, g_loss: 0.69444877, ae_loss: 0.05045279\n",
      "Step: [1113] total_loss: 2.13275576 d_loss: 1.39937043, g_loss: 0.68364704, ae_loss: 0.04973820\n",
      "Step: [1114] total_loss: 2.12033892 d_loss: 1.36308432, g_loss: 0.70079851, ae_loss: 0.05645598\n",
      "Step: [1115] total_loss: 2.15712094 d_loss: 1.40139365, g_loss: 0.69842929, ae_loss: 0.05729803\n",
      "Step: [1116] total_loss: 2.12755466 d_loss: 1.39034390, g_loss: 0.68482774, ae_loss: 0.05238304\n",
      "Step: [1117] total_loss: 2.11047864 d_loss: 1.36128521, g_loss: 0.69173938, ae_loss: 0.05745393\n",
      "Step: [1118] total_loss: 2.12514353 d_loss: 1.38232255, g_loss: 0.68850154, ae_loss: 0.05431956\n",
      "Step: [1119] total_loss: 2.14190626 d_loss: 1.39522815, g_loss: 0.69288731, ae_loss: 0.05379074\n",
      "Step: [1120] total_loss: 2.16921544 d_loss: 1.41874051, g_loss: 0.69797003, ae_loss: 0.05250492\n",
      "Step: [1121] total_loss: 2.13528013 d_loss: 1.39873743, g_loss: 0.68709719, ae_loss: 0.04944569\n",
      "Step: [1122] total_loss: 2.13946199 d_loss: 1.38941765, g_loss: 0.69574964, ae_loss: 0.05429453\n",
      "Step: [1123] total_loss: 2.13864493 d_loss: 1.38004363, g_loss: 0.70628858, ae_loss: 0.05231268\n",
      "Step: [1124] total_loss: 2.14020967 d_loss: 1.39266968, g_loss: 0.69453096, ae_loss: 0.05300886\n",
      "Step: [1125] total_loss: 2.11391354 d_loss: 1.36529851, g_loss: 0.69433498, ae_loss: 0.05428001\n",
      "Step: [1126] total_loss: 2.13606477 d_loss: 1.39098048, g_loss: 0.69363761, ae_loss: 0.05144671\n",
      "Step: [1127] total_loss: 2.15221930 d_loss: 1.37497485, g_loss: 0.72306955, ae_loss: 0.05417508\n",
      "Step: [1128] total_loss: 2.12925053 d_loss: 1.38023329, g_loss: 0.69570649, ae_loss: 0.05331060\n",
      "Step: [1129] total_loss: 2.13342047 d_loss: 1.37959301, g_loss: 0.70196164, ae_loss: 0.05186582\n",
      "Step: [1130] total_loss: 2.13530231 d_loss: 1.37471950, g_loss: 0.70693880, ae_loss: 0.05364392\n",
      "Step: [1131] total_loss: 2.10953856 d_loss: 1.36845112, g_loss: 0.68577915, ae_loss: 0.05530825\n",
      "Step: [1132] total_loss: 2.12416220 d_loss: 1.37439215, g_loss: 0.69779432, ae_loss: 0.05197581\n",
      "Step: [1133] total_loss: 2.13011599 d_loss: 1.40509892, g_loss: 0.67134869, ae_loss: 0.05366826\n",
      "Step: [1134] total_loss: 2.10229206 d_loss: 1.36143911, g_loss: 0.68656111, ae_loss: 0.05429195\n",
      "Step: [1135] total_loss: 2.14600587 d_loss: 1.38322163, g_loss: 0.70819545, ae_loss: 0.05458876\n",
      "Step: [1136] total_loss: 2.11826754 d_loss: 1.36523223, g_loss: 0.69719684, ae_loss: 0.05583844\n",
      "Step: [1137] total_loss: 2.12030816 d_loss: 1.38643217, g_loss: 0.68079334, ae_loss: 0.05308260\n",
      "Step: [1138] total_loss: 2.14551544 d_loss: 1.40939569, g_loss: 0.68088281, ae_loss: 0.05523676\n",
      "Step: [1139] total_loss: 2.13707352 d_loss: 1.39041555, g_loss: 0.68771762, ae_loss: 0.05894035\n",
      "Step: [1140] total_loss: 2.13904619 d_loss: 1.39245987, g_loss: 0.69783741, ae_loss: 0.04874900\n",
      "Step: [1141] total_loss: 2.13922215 d_loss: 1.37653375, g_loss: 0.70891118, ae_loss: 0.05377704\n",
      "Step: [1142] total_loss: 2.11825156 d_loss: 1.37613225, g_loss: 0.69015968, ae_loss: 0.05195960\n",
      "Step: [1143] total_loss: 2.12223935 d_loss: 1.38623524, g_loss: 0.68393725, ae_loss: 0.05206693\n",
      "Step: [1144] total_loss: 2.13505554 d_loss: 1.39935827, g_loss: 0.68466604, ae_loss: 0.05103119\n",
      "Step: [1145] total_loss: 2.13194537 d_loss: 1.36796355, g_loss: 0.70917755, ae_loss: 0.05480436\n",
      "Step: [1146] total_loss: 2.13867640 d_loss: 1.38691485, g_loss: 0.69825256, ae_loss: 0.05350903\n",
      "Step: [1147] total_loss: 2.14037490 d_loss: 1.37213147, g_loss: 0.71057665, ae_loss: 0.05766675\n",
      "Step: [1148] total_loss: 2.14007521 d_loss: 1.39845884, g_loss: 0.68748438, ae_loss: 0.05413208\n",
      "Step: [1149] total_loss: 2.13170910 d_loss: 1.39012647, g_loss: 0.68596017, ae_loss: 0.05562240\n",
      "Step: [1150] total_loss: 2.10299134 d_loss: 1.36613405, g_loss: 0.68649995, ae_loss: 0.05035735\n",
      "Step: [1151] total_loss: 2.10919666 d_loss: 1.39062715, g_loss: 0.66573453, ae_loss: 0.05283483\n",
      "Step: [1152] total_loss: 2.12170458 d_loss: 1.36828828, g_loss: 0.70051241, ae_loss: 0.05290373\n",
      "Step: [1153] total_loss: 2.14733124 d_loss: 1.38997173, g_loss: 0.70247948, ae_loss: 0.05488003\n",
      "Step: [1154] total_loss: 2.14499831 d_loss: 1.39834166, g_loss: 0.68962699, ae_loss: 0.05702970\n",
      "Step: [1155] total_loss: 2.12033129 d_loss: 1.36094308, g_loss: 0.71146017, ae_loss: 0.04792811\n",
      "Step: [1156] total_loss: 2.13580370 d_loss: 1.37907672, g_loss: 0.70431191, ae_loss: 0.05241513\n",
      "Step: [1157] total_loss: 2.12284422 d_loss: 1.37013483, g_loss: 0.69911528, ae_loss: 0.05359402\n",
      "Step: [1158] total_loss: 2.10553646 d_loss: 1.36087680, g_loss: 0.69156718, ae_loss: 0.05309234\n",
      "Step: [1159] total_loss: 2.13927269 d_loss: 1.38260543, g_loss: 0.70081282, ae_loss: 0.05585445\n",
      "Step: [1160] total_loss: 2.12998581 d_loss: 1.38692987, g_loss: 0.68813002, ae_loss: 0.05492582\n",
      "Step: [1161] total_loss: 2.14120817 d_loss: 1.40305161, g_loss: 0.69135368, ae_loss: 0.04680306\n",
      "Step: [1162] total_loss: 2.13180470 d_loss: 1.39063740, g_loss: 0.69200903, ae_loss: 0.04915831\n",
      "Step: [1163] total_loss: 2.12265038 d_loss: 1.39650035, g_loss: 0.67564636, ae_loss: 0.05050359\n",
      "Step: [1164] total_loss: 2.14174175 d_loss: 1.39170086, g_loss: 0.69436193, ae_loss: 0.05567897\n",
      "Step: [1165] total_loss: 2.11578870 d_loss: 1.38118589, g_loss: 0.68424428, ae_loss: 0.05035853\n",
      "Step: [1166] total_loss: 2.09825325 d_loss: 1.36519945, g_loss: 0.68013138, ae_loss: 0.05292236\n",
      "Step: [1167] total_loss: 2.12416267 d_loss: 1.38925242, g_loss: 0.68379772, ae_loss: 0.05111266\n",
      "Step: [1168] total_loss: 2.10821915 d_loss: 1.36352205, g_loss: 0.69472802, ae_loss: 0.04996920\n",
      "Step: [1169] total_loss: 2.13850498 d_loss: 1.37771225, g_loss: 0.70782983, ae_loss: 0.05296300\n",
      "Step: [1170] total_loss: 2.14343524 d_loss: 1.40887690, g_loss: 0.68231076, ae_loss: 0.05224768\n",
      "Step: [1171] total_loss: 2.12004399 d_loss: 1.38286018, g_loss: 0.68671441, ae_loss: 0.05046942\n",
      "Step: [1172] total_loss: 2.13978648 d_loss: 1.37933397, g_loss: 0.70800126, ae_loss: 0.05245129\n",
      "Step: [1173] total_loss: 2.13204241 d_loss: 1.36902773, g_loss: 0.70970798, ae_loss: 0.05330685\n",
      "Step: [1174] total_loss: 2.12655926 d_loss: 1.37390983, g_loss: 0.69581211, ae_loss: 0.05683734\n",
      "Step: [1175] total_loss: 2.14676666 d_loss: 1.39520884, g_loss: 0.69867969, ae_loss: 0.05287804\n",
      "Step: [1176] total_loss: 2.13094163 d_loss: 1.37124467, g_loss: 0.70566410, ae_loss: 0.05403280\n",
      "Step: [1177] total_loss: 2.12357950 d_loss: 1.38784456, g_loss: 0.68168223, ae_loss: 0.05405255\n",
      "Step: [1178] total_loss: 2.15392780 d_loss: 1.40623009, g_loss: 0.69601738, ae_loss: 0.05168017\n",
      "Step: [1179] total_loss: 2.13209343 d_loss: 1.39034462, g_loss: 0.68724000, ae_loss: 0.05450881\n",
      "Step: [1180] total_loss: 2.13038135 d_loss: 1.38738608, g_loss: 0.69468164, ae_loss: 0.04831365\n",
      "Step: [1181] total_loss: 2.13968563 d_loss: 1.38464892, g_loss: 0.70163220, ae_loss: 0.05340448\n",
      "Step: [1182] total_loss: 2.14392710 d_loss: 1.37953722, g_loss: 0.70859802, ae_loss: 0.05579182\n",
      "Step: [1183] total_loss: 2.14102793 d_loss: 1.38176560, g_loss: 0.70225853, ae_loss: 0.05700390\n",
      "Step: [1184] total_loss: 2.11635733 d_loss: 1.37796474, g_loss: 0.68381852, ae_loss: 0.05457409\n",
      "Step: [1185] total_loss: 2.10165596 d_loss: 1.38042951, g_loss: 0.67221498, ae_loss: 0.04901155\n",
      "Step: [1186] total_loss: 2.13225055 d_loss: 1.39405262, g_loss: 0.68116534, ae_loss: 0.05703255\n",
      "Step: [1187] total_loss: 2.13044024 d_loss: 1.37537754, g_loss: 0.70156646, ae_loss: 0.05349610\n",
      "Step: [1188] total_loss: 2.14327049 d_loss: 1.36935675, g_loss: 0.72174352, ae_loss: 0.05217025\n",
      "Step: [1189] total_loss: 2.13846159 d_loss: 1.36855459, g_loss: 0.71302366, ae_loss: 0.05688327\n",
      "Step: [1190] total_loss: 2.14503145 d_loss: 1.40006042, g_loss: 0.69261408, ae_loss: 0.05235703\n",
      "Step: [1191] total_loss: 2.13556385 d_loss: 1.39389169, g_loss: 0.68568617, ae_loss: 0.05598604\n",
      "Step: [1192] total_loss: 2.12425137 d_loss: 1.40163529, g_loss: 0.66729414, ae_loss: 0.05532185\n",
      "Step: [1193] total_loss: 2.11247015 d_loss: 1.37611175, g_loss: 0.68436348, ae_loss: 0.05199507\n",
      "Step: [1194] total_loss: 2.10611629 d_loss: 1.35996842, g_loss: 0.69087017, ae_loss: 0.05527784\n",
      "Step: [1195] total_loss: 2.12338448 d_loss: 1.39108562, g_loss: 0.68043971, ae_loss: 0.05185924\n",
      "Step: [1196] total_loss: 2.14145613 d_loss: 1.38511693, g_loss: 0.70212877, ae_loss: 0.05421046\n",
      "Step: [1197] total_loss: 2.13512850 d_loss: 1.36995411, g_loss: 0.71306348, ae_loss: 0.05211098\n",
      "Step: [1198] total_loss: 2.13710690 d_loss: 1.37864184, g_loss: 0.70534861, ae_loss: 0.05311651\n",
      "Step: [1199] total_loss: 2.10289621 d_loss: 1.36097407, g_loss: 0.69092941, ae_loss: 0.05099284\n",
      "Step: [1200] total_loss: 2.13468528 d_loss: 1.39424109, g_loss: 0.68730223, ae_loss: 0.05314191\n",
      "Step: [1201] total_loss: 2.10965061 d_loss: 1.36395025, g_loss: 0.69229090, ae_loss: 0.05340964\n",
      "Step: [1202] total_loss: 2.12590551 d_loss: 1.40037632, g_loss: 0.67141831, ae_loss: 0.05411085\n",
      "Step: [1203] total_loss: 2.11036158 d_loss: 1.36779714, g_loss: 0.68857968, ae_loss: 0.05398493\n",
      "Step: [1204] total_loss: 2.12612462 d_loss: 1.38615549, g_loss: 0.68729526, ae_loss: 0.05267391\n",
      "Step: [1205] total_loss: 2.12550902 d_loss: 1.37910402, g_loss: 0.69699222, ae_loss: 0.04941276\n",
      "Step: [1206] total_loss: 2.13229609 d_loss: 1.37868834, g_loss: 0.70363671, ae_loss: 0.04997098\n",
      "Step: [1207] total_loss: 2.16981936 d_loss: 1.36496663, g_loss: 0.74969411, ae_loss: 0.05515865\n",
      "Step: [1208] total_loss: 2.14453793 d_loss: 1.38187861, g_loss: 0.70856893, ae_loss: 0.05409050\n",
      "Step: [1209] total_loss: 2.15061903 d_loss: 1.39130056, g_loss: 0.70959067, ae_loss: 0.04972765\n",
      "Step: [1210] total_loss: 2.14215803 d_loss: 1.37418866, g_loss: 0.71270955, ae_loss: 0.05525995\n",
      "Step: [1211] total_loss: 2.12883615 d_loss: 1.36425066, g_loss: 0.71308923, ae_loss: 0.05149630\n",
      "Step: [1212] total_loss: 2.13790464 d_loss: 1.40122342, g_loss: 0.67875016, ae_loss: 0.05793098\n",
      "Step: [1213] total_loss: 2.10154724 d_loss: 1.36944759, g_loss: 0.67901373, ae_loss: 0.05308604\n",
      "Step: [1214] total_loss: 2.11764932 d_loss: 1.38051498, g_loss: 0.68355864, ae_loss: 0.05357574\n",
      "Step: [1215] total_loss: 2.12604666 d_loss: 1.37796497, g_loss: 0.69570571, ae_loss: 0.05237608\n",
      "Step: [1216] total_loss: 2.11243773 d_loss: 1.37796235, g_loss: 0.68149149, ae_loss: 0.05298375\n",
      "Step: [1217] total_loss: 2.12694287 d_loss: 1.37796807, g_loss: 0.70070124, ae_loss: 0.04827359\n",
      "Step: [1218] total_loss: 2.12941027 d_loss: 1.38651371, g_loss: 0.69020128, ae_loss: 0.05269528\n",
      "Step: [1219] total_loss: 2.10709953 d_loss: 1.35725665, g_loss: 0.69810343, ae_loss: 0.05173949\n",
      "Step: [1220] total_loss: 2.13116193 d_loss: 1.38181663, g_loss: 0.69353867, ae_loss: 0.05580669\n",
      "Step: [1221] total_loss: 2.11895084 d_loss: 1.37803423, g_loss: 0.68486321, ae_loss: 0.05605333\n",
      "Step: [1222] total_loss: 2.12546849 d_loss: 1.40358806, g_loss: 0.66790122, ae_loss: 0.05397926\n",
      "Step: [1223] total_loss: 2.10062933 d_loss: 1.36327267, g_loss: 0.68435955, ae_loss: 0.05299717\n",
      "Step: [1224] total_loss: 2.11565304 d_loss: 1.38576555, g_loss: 0.67161441, ae_loss: 0.05827304\n",
      "Step: [1225] total_loss: 2.11652994 d_loss: 1.34874749, g_loss: 0.71182960, ae_loss: 0.05595294\n",
      "Step: [1226] total_loss: 2.13772202 d_loss: 1.39302409, g_loss: 0.69637525, ae_loss: 0.04832271\n",
      "Step: [1227] total_loss: 2.12413454 d_loss: 1.36080337, g_loss: 0.70978403, ae_loss: 0.05354703\n",
      "Step: [1228] total_loss: 2.15087175 d_loss: 1.36943412, g_loss: 0.72938192, ae_loss: 0.05205577\n",
      "Step: [1229] total_loss: 2.14001465 d_loss: 1.38149548, g_loss: 0.70705330, ae_loss: 0.05146572\n",
      "Step: [1230] total_loss: 2.15539551 d_loss: 1.38037181, g_loss: 0.72073483, ae_loss: 0.05428895\n",
      "Step: [1231] total_loss: 2.12490368 d_loss: 1.38846922, g_loss: 0.68161839, ae_loss: 0.05481612\n",
      "Step: [1232] total_loss: 2.11493993 d_loss: 1.37684965, g_loss: 0.68481201, ae_loss: 0.05327826\n",
      "Step: [1233] total_loss: 2.12073421 d_loss: 1.38199437, g_loss: 0.68447006, ae_loss: 0.05426965\n",
      "Step: [1234] total_loss: 2.10611701 d_loss: 1.35708940, g_loss: 0.69169545, ae_loss: 0.05733221\n",
      "Step: [1235] total_loss: 2.12911081 d_loss: 1.36936355, g_loss: 0.70469463, ae_loss: 0.05505248\n",
      "Step: [1236] total_loss: 2.14742303 d_loss: 1.35549855, g_loss: 0.73995090, ae_loss: 0.05197355\n",
      "Step: [1237] total_loss: 2.15206909 d_loss: 1.37935877, g_loss: 0.71946549, ae_loss: 0.05324500\n",
      "Step: [1238] total_loss: 2.12512827 d_loss: 1.36448336, g_loss: 0.70893431, ae_loss: 0.05171059\n",
      "Step: [1239] total_loss: 2.13594389 d_loss: 1.37836313, g_loss: 0.70363021, ae_loss: 0.05395070\n",
      "Step: [1240] total_loss: 2.14555430 d_loss: 1.39851201, g_loss: 0.69394553, ae_loss: 0.05309672\n",
      "Step: [1241] total_loss: 2.13052225 d_loss: 1.39212847, g_loss: 0.68670094, ae_loss: 0.05169282\n",
      "Step: [1242] total_loss: 2.13596702 d_loss: 1.36986113, g_loss: 0.71369213, ae_loss: 0.05241375\n",
      "Step: [1243] total_loss: 2.13115454 d_loss: 1.38243759, g_loss: 0.69454420, ae_loss: 0.05417276\n",
      "Step: [1244] total_loss: 2.11812544 d_loss: 1.39039588, g_loss: 0.67874348, ae_loss: 0.04898602\n",
      "Step: [1245] total_loss: 2.10059094 d_loss: 1.36272645, g_loss: 0.68639433, ae_loss: 0.05147012\n",
      "Step: [1246] total_loss: 2.11118937 d_loss: 1.37961876, g_loss: 0.68225729, ae_loss: 0.04931333\n",
      "Step: [1247] total_loss: 2.12384319 d_loss: 1.38498068, g_loss: 0.68674183, ae_loss: 0.05212084\n",
      "Step: [1248] total_loss: 2.14101338 d_loss: 1.39313602, g_loss: 0.69402856, ae_loss: 0.05384871\n",
      "Step: [1249] total_loss: 2.15646768 d_loss: 1.38056791, g_loss: 0.73124200, ae_loss: 0.04465769\n",
      "Step: [1250] total_loss: 2.13850713 d_loss: 1.36723363, g_loss: 0.71956366, ae_loss: 0.05170985\n",
      "Step: [1251] total_loss: 2.11591959 d_loss: 1.38135505, g_loss: 0.68143588, ae_loss: 0.05312857\n",
      "Step: [1252] total_loss: 2.12711477 d_loss: 1.38348794, g_loss: 0.69155240, ae_loss: 0.05207438\n",
      "Step: [1253] total_loss: 2.11364865 d_loss: 1.37161922, g_loss: 0.68924791, ae_loss: 0.05278163\n",
      "Step: [1254] total_loss: 2.12153387 d_loss: 1.37221181, g_loss: 0.69661349, ae_loss: 0.05270861\n",
      "Step: [1255] total_loss: 2.12934947 d_loss: 1.39773142, g_loss: 0.67826843, ae_loss: 0.05334963\n",
      "Step: [1256] total_loss: 2.13510084 d_loss: 1.37099123, g_loss: 0.71702003, ae_loss: 0.04708942\n",
      "Step: [1257] total_loss: 2.13829374 d_loss: 1.38344312, g_loss: 0.70260453, ae_loss: 0.05224612\n",
      "Step: [1258] total_loss: 2.12744260 d_loss: 1.38215137, g_loss: 0.69333684, ae_loss: 0.05195440\n",
      "Step: [1259] total_loss: 2.14374542 d_loss: 1.38222706, g_loss: 0.70582104, ae_loss: 0.05569738\n",
      "Step: [1260] total_loss: 2.13489676 d_loss: 1.39334488, g_loss: 0.68812382, ae_loss: 0.05342808\n",
      "Step: [1261] total_loss: 2.12422562 d_loss: 1.37482119, g_loss: 0.69669467, ae_loss: 0.05270982\n",
      "Step: [1262] total_loss: 2.13462901 d_loss: 1.39943719, g_loss: 0.68275619, ae_loss: 0.05243565\n",
      "Step: [1263] total_loss: 2.11074519 d_loss: 1.35887003, g_loss: 0.70416731, ae_loss: 0.04770782\n",
      "Step: [1264] total_loss: 2.13080025 d_loss: 1.38400137, g_loss: 0.69238663, ae_loss: 0.05441232\n",
      "Step: [1265] total_loss: 2.11351871 d_loss: 1.36731851, g_loss: 0.69574457, ae_loss: 0.05045563\n",
      "Step: [1266] total_loss: 2.09466028 d_loss: 1.34159493, g_loss: 0.70024765, ae_loss: 0.05281761\n",
      "Step: [1267] total_loss: 2.13690615 d_loss: 1.39937305, g_loss: 0.68188810, ae_loss: 0.05564507\n",
      "Step: [1268] total_loss: 2.13037682 d_loss: 1.39112091, g_loss: 0.68933052, ae_loss: 0.04992536\n",
      "Step: [1269] total_loss: 2.14196610 d_loss: 1.40716088, g_loss: 0.68137693, ae_loss: 0.05342831\n",
      "Step: [1270] total_loss: 2.14574933 d_loss: 1.40472627, g_loss: 0.68571299, ae_loss: 0.05531009\n",
      "Step: [1271] total_loss: 2.13112807 d_loss: 1.38313818, g_loss: 0.69497633, ae_loss: 0.05301361\n",
      "Step: [1272] total_loss: 2.15103388 d_loss: 1.39167047, g_loss: 0.70375466, ae_loss: 0.05560877\n",
      "Step: [1273] total_loss: 2.13022089 d_loss: 1.39417219, g_loss: 0.68355781, ae_loss: 0.05249090\n",
      "Step: [1274] total_loss: 2.13151336 d_loss: 1.38953543, g_loss: 0.68830311, ae_loss: 0.05367476\n",
      "Step: [1275] total_loss: 2.14421201 d_loss: 1.40145564, g_loss: 0.68850851, ae_loss: 0.05424788\n",
      "Step: [1276] total_loss: 2.13028455 d_loss: 1.39404964, g_loss: 0.68228853, ae_loss: 0.05394642\n",
      "Step: [1277] total_loss: 2.16963434 d_loss: 1.39158559, g_loss: 0.72036409, ae_loss: 0.05768455\n",
      "Step: [1278] total_loss: 2.13556194 d_loss: 1.38893986, g_loss: 0.69373238, ae_loss: 0.05288954\n",
      "Step: [1279] total_loss: 2.13039017 d_loss: 1.36849761, g_loss: 0.70958555, ae_loss: 0.05230685\n",
      "Step: [1280] total_loss: 2.12832880 d_loss: 1.39844429, g_loss: 0.68015057, ae_loss: 0.04973400\n",
      "Step: [1281] total_loss: 2.12478685 d_loss: 1.38500690, g_loss: 0.69146097, ae_loss: 0.04831884\n",
      "Step: [1282] total_loss: 2.11157656 d_loss: 1.38879633, g_loss: 0.66807723, ae_loss: 0.05470317\n",
      "Step: [1283] total_loss: 2.11376047 d_loss: 1.36637926, g_loss: 0.69593859, ae_loss: 0.05144249\n",
      "Step: [1284] total_loss: 2.12619925 d_loss: 1.36971116, g_loss: 0.70614779, ae_loss: 0.05034023\n",
      "Step: [1285] total_loss: 2.12810445 d_loss: 1.38754296, g_loss: 0.69243932, ae_loss: 0.04812220\n",
      "Step: [1286] total_loss: 2.15459204 d_loss: 1.38775611, g_loss: 0.71553254, ae_loss: 0.05130346\n",
      "Step: [1287] total_loss: 2.12988067 d_loss: 1.38489258, g_loss: 0.69196725, ae_loss: 0.05302089\n",
      "Step: [1288] total_loss: 2.14642406 d_loss: 1.38057709, g_loss: 0.71268523, ae_loss: 0.05316170\n",
      "Step: [1289] total_loss: 2.14389467 d_loss: 1.37957680, g_loss: 0.70850718, ae_loss: 0.05581074\n",
      "Step: [1290] total_loss: 2.12910080 d_loss: 1.39831018, g_loss: 0.67846847, ae_loss: 0.05232205\n",
      "Step: [1291] total_loss: 2.10356593 d_loss: 1.37459874, g_loss: 0.67346960, ae_loss: 0.05549761\n",
      "Step: [1292] total_loss: 2.11367416 d_loss: 1.35914755, g_loss: 0.70481497, ae_loss: 0.04971175\n",
      "Step: [1293] total_loss: 2.11097908 d_loss: 1.38722253, g_loss: 0.67454159, ae_loss: 0.04921512\n",
      "Step: [1294] total_loss: 2.11000109 d_loss: 1.37848973, g_loss: 0.67988634, ae_loss: 0.05162486\n",
      "Step: [1295] total_loss: 2.13061571 d_loss: 1.40605760, g_loss: 0.67382270, ae_loss: 0.05073538\n",
      "Step: [1296] total_loss: 2.12497163 d_loss: 1.38757610, g_loss: 0.68435824, ae_loss: 0.05303727\n",
      "Step: [1297] total_loss: 2.14336300 d_loss: 1.39747095, g_loss: 0.68939030, ae_loss: 0.05650160\n",
      "Step: [1298] total_loss: 2.12095070 d_loss: 1.39225495, g_loss: 0.67384517, ae_loss: 0.05485069\n",
      "Step: [1299] total_loss: 2.12937546 d_loss: 1.38813663, g_loss: 0.68648696, ae_loss: 0.05475189\n",
      "Step: [1300] total_loss: 2.14499855 d_loss: 1.40643382, g_loss: 0.68421769, ae_loss: 0.05434708\n",
      "Step: [1301] total_loss: 2.12594509 d_loss: 1.37745309, g_loss: 0.69512522, ae_loss: 0.05336663\n",
      "Step: [1302] total_loss: 2.14298916 d_loss: 1.40885448, g_loss: 0.68146074, ae_loss: 0.05267395\n",
      "Step: [1303] total_loss: 2.14401627 d_loss: 1.39673400, g_loss: 0.69562334, ae_loss: 0.05165898\n",
      "Step: [1304] total_loss: 2.14046550 d_loss: 1.38091266, g_loss: 0.70691323, ae_loss: 0.05263959\n",
      "Step: [1305] total_loss: 2.11307859 d_loss: 1.36992276, g_loss: 0.69280481, ae_loss: 0.05035109\n",
      "Step: [1306] total_loss: 2.11869645 d_loss: 1.37214828, g_loss: 0.69119948, ae_loss: 0.05534862\n",
      "Step: [1307] total_loss: 2.13264513 d_loss: 1.39121747, g_loss: 0.68559480, ae_loss: 0.05583285\n",
      "Step: [1308] total_loss: 2.13508606 d_loss: 1.36780763, g_loss: 0.71234602, ae_loss: 0.05493232\n",
      "Step: [1309] total_loss: 2.12886882 d_loss: 1.37400496, g_loss: 0.70398319, ae_loss: 0.05088067\n",
      "Step: [1310] total_loss: 2.14857173 d_loss: 1.41528463, g_loss: 0.67793536, ae_loss: 0.05535167\n",
      "Step: [1311] total_loss: 2.12528396 d_loss: 1.38279331, g_loss: 0.68820459, ae_loss: 0.05428597\n",
      "Step: [1312] total_loss: 2.10770941 d_loss: 1.38394618, g_loss: 0.67176056, ae_loss: 0.05200284\n",
      "Step: [1313] total_loss: 2.11822629 d_loss: 1.38576114, g_loss: 0.67763692, ae_loss: 0.05482828\n",
      "Step: [1314] total_loss: 2.15055466 d_loss: 1.42063653, g_loss: 0.67824858, ae_loss: 0.05166961\n",
      "Step: [1315] total_loss: 2.14376593 d_loss: 1.40270936, g_loss: 0.68768215, ae_loss: 0.05337436\n",
      "Step: [1316] total_loss: 2.14983225 d_loss: 1.40035045, g_loss: 0.70003915, ae_loss: 0.04944256\n",
      "Step: [1317] total_loss: 2.10184050 d_loss: 1.36026716, g_loss: 0.68885356, ae_loss: 0.05271970\n",
      "Step: [1318] total_loss: 2.14436436 d_loss: 1.39235783, g_loss: 0.69945234, ae_loss: 0.05255418\n",
      "Step: [1319] total_loss: 2.13654995 d_loss: 1.35753083, g_loss: 0.72627437, ae_loss: 0.05274482\n",
      "Step: [1320] total_loss: 2.14701152 d_loss: 1.39240229, g_loss: 0.70525748, ae_loss: 0.04935177\n",
      "Step: [1321] total_loss: 2.13471651 d_loss: 1.37777603, g_loss: 0.70372331, ae_loss: 0.05321728\n",
      "Step: [1322] total_loss: 2.12661409 d_loss: 1.37024057, g_loss: 0.70346886, ae_loss: 0.05290457\n",
      "Step: [1323] total_loss: 2.13584089 d_loss: 1.38361418, g_loss: 0.69848734, ae_loss: 0.05373945\n",
      "Step: [1324] total_loss: 2.11759710 d_loss: 1.38495398, g_loss: 0.68136466, ae_loss: 0.05127856\n",
      "Step: [1325] total_loss: 2.11814809 d_loss: 1.39376593, g_loss: 0.67335325, ae_loss: 0.05102880\n",
      "Step: [1326] total_loss: 2.12738442 d_loss: 1.39275944, g_loss: 0.67657727, ae_loss: 0.05804763\n",
      "Step: [1327] total_loss: 2.10282803 d_loss: 1.37085414, g_loss: 0.68324459, ae_loss: 0.04872937\n",
      "Step: [1328] total_loss: 2.14060259 d_loss: 1.38245964, g_loss: 0.70430481, ae_loss: 0.05383822\n",
      "Step: [1329] total_loss: 2.14566207 d_loss: 1.39262927, g_loss: 0.69633704, ae_loss: 0.05669580\n",
      "Step: [1330] total_loss: 2.13467145 d_loss: 1.39913082, g_loss: 0.68084931, ae_loss: 0.05469127\n",
      "Step: [1331] total_loss: 2.16147208 d_loss: 1.41682661, g_loss: 0.69315624, ae_loss: 0.05148924\n",
      "Step: [1332] total_loss: 2.13032579 d_loss: 1.38679790, g_loss: 0.69151318, ae_loss: 0.05201487\n",
      "Step: [1333] total_loss: 2.14467168 d_loss: 1.40585411, g_loss: 0.68771398, ae_loss: 0.05110361\n",
      "Step: [1334] total_loss: 2.12972999 d_loss: 1.38013530, g_loss: 0.69945914, ae_loss: 0.05013545\n",
      "Step: [1335] total_loss: 2.13828611 d_loss: 1.40144539, g_loss: 0.68262935, ae_loss: 0.05421137\n",
      "Step: [1336] total_loss: 2.13083410 d_loss: 1.39333665, g_loss: 0.68370736, ae_loss: 0.05379023\n",
      "Step: [1337] total_loss: 2.14022827 d_loss: 1.38946581, g_loss: 0.69315529, ae_loss: 0.05760705\n",
      "Step: [1338] total_loss: 2.13531590 d_loss: 1.38352120, g_loss: 0.70124954, ae_loss: 0.05054509\n",
      "Step: [1339] total_loss: 2.10623002 d_loss: 1.38053441, g_loss: 0.67136866, ae_loss: 0.05432693\n",
      "Step: [1340] total_loss: 2.12522149 d_loss: 1.39736116, g_loss: 0.67807621, ae_loss: 0.04978406\n",
      "Step: [1341] total_loss: 2.13384676 d_loss: 1.37450719, g_loss: 0.70576489, ae_loss: 0.05357471\n",
      "Step: [1342] total_loss: 2.12957883 d_loss: 1.38983011, g_loss: 0.68704736, ae_loss: 0.05270136\n",
      "Step: [1343] total_loss: 2.13015795 d_loss: 1.38127494, g_loss: 0.69314450, ae_loss: 0.05573855\n",
      "Step: [1344] total_loss: 2.14660454 d_loss: 1.39630139, g_loss: 0.69836098, ae_loss: 0.05194212\n",
      "Step: [1345] total_loss: 2.13585210 d_loss: 1.38853621, g_loss: 0.69533163, ae_loss: 0.05198427\n",
      "Step: [1346] total_loss: 2.10019255 d_loss: 1.36265814, g_loss: 0.68011373, ae_loss: 0.05742062\n",
      "Step: [1347] total_loss: 2.11409569 d_loss: 1.37677383, g_loss: 0.68685234, ae_loss: 0.05046967\n",
      "Step: [1348] total_loss: 2.11858153 d_loss: 1.39671564, g_loss: 0.67076385, ae_loss: 0.05110208\n",
      "Step: [1349] total_loss: 2.14183331 d_loss: 1.40758252, g_loss: 0.68494034, ae_loss: 0.04931050\n",
      "Step: [1350] total_loss: 2.12661409 d_loss: 1.38892341, g_loss: 0.68681300, ae_loss: 0.05087767\n",
      "Step: [1351] total_loss: 2.13625193 d_loss: 1.38908434, g_loss: 0.69725144, ae_loss: 0.04991625\n",
      "Step: [1352] total_loss: 2.12183523 d_loss: 1.37323821, g_loss: 0.69451267, ae_loss: 0.05408429\n",
      "Step: [1353] total_loss: 2.13881469 d_loss: 1.38503289, g_loss: 0.69924194, ae_loss: 0.05453989\n",
      "Step: [1354] total_loss: 2.12658834 d_loss: 1.37897921, g_loss: 0.69085538, ae_loss: 0.05675389\n",
      "Step: [1355] total_loss: 2.13613248 d_loss: 1.39226890, g_loss: 0.69279230, ae_loss: 0.05107128\n",
      "Step: [1356] total_loss: 2.12260866 d_loss: 1.37665081, g_loss: 0.69364774, ae_loss: 0.05230999\n",
      "Step: [1357] total_loss: 2.13935876 d_loss: 1.38361859, g_loss: 0.70126086, ae_loss: 0.05447920\n",
      "Step: [1358] total_loss: 2.12677979 d_loss: 1.38554382, g_loss: 0.68831605, ae_loss: 0.05291991\n",
      "Step: [1359] total_loss: 2.12502241 d_loss: 1.38413596, g_loss: 0.68643689, ae_loss: 0.05444946\n",
      "Step: [1360] total_loss: 2.11231256 d_loss: 1.36222827, g_loss: 0.69439763, ae_loss: 0.05568664\n",
      "Step: [1361] total_loss: 2.13489723 d_loss: 1.37563848, g_loss: 0.70488918, ae_loss: 0.05436955\n",
      "Step: [1362] total_loss: 2.12214899 d_loss: 1.38771653, g_loss: 0.67918062, ae_loss: 0.05525194\n",
      "Step: [1363] total_loss: 2.13562059 d_loss: 1.38991594, g_loss: 0.69265187, ae_loss: 0.05305287\n",
      "Step: [1364] total_loss: 2.12800717 d_loss: 1.38696694, g_loss: 0.68823504, ae_loss: 0.05280524\n",
      "Step: [1365] total_loss: 2.12898827 d_loss: 1.37797034, g_loss: 0.69843960, ae_loss: 0.05257824\n",
      "Step: [1366] total_loss: 2.13223290 d_loss: 1.38045466, g_loss: 0.69826007, ae_loss: 0.05351821\n",
      "Step: [1367] total_loss: 2.11259413 d_loss: 1.37075138, g_loss: 0.69023311, ae_loss: 0.05160974\n",
      "Step: [1368] total_loss: 2.11796260 d_loss: 1.37797761, g_loss: 0.68909544, ae_loss: 0.05088956\n",
      "Step: [1369] total_loss: 2.11987472 d_loss: 1.38243127, g_loss: 0.68594098, ae_loss: 0.05150244\n",
      "Step: [1370] total_loss: 2.12666273 d_loss: 1.39989972, g_loss: 0.67329001, ae_loss: 0.05347313\n",
      "Step: [1371] total_loss: 2.10455084 d_loss: 1.37256193, g_loss: 0.68136716, ae_loss: 0.05062161\n",
      "Step: [1372] total_loss: 2.11119509 d_loss: 1.36882007, g_loss: 0.69181848, ae_loss: 0.05055669\n",
      "Step: [1373] total_loss: 2.10913658 d_loss: 1.37537599, g_loss: 0.67999351, ae_loss: 0.05376701\n",
      "Step: [1374] total_loss: 2.12336516 d_loss: 1.39276910, g_loss: 0.67687279, ae_loss: 0.05372335\n",
      "Step: [1375] total_loss: 2.12751889 d_loss: 1.37701178, g_loss: 0.69634444, ae_loss: 0.05416267\n",
      "Step: [1376] total_loss: 2.14434290 d_loss: 1.40036535, g_loss: 0.69210529, ae_loss: 0.05187241\n",
      "Step: [1377] total_loss: 2.16173863 d_loss: 1.39278221, g_loss: 0.71895450, ae_loss: 0.05000202\n",
      "Step: [1378] total_loss: 2.13756800 d_loss: 1.37853265, g_loss: 0.70673978, ae_loss: 0.05229570\n",
      "Step: [1379] total_loss: 2.13258362 d_loss: 1.37139082, g_loss: 0.70682847, ae_loss: 0.05436433\n",
      "Step: [1380] total_loss: 2.14023304 d_loss: 1.37742066, g_loss: 0.70870292, ae_loss: 0.05410935\n",
      "Step: [1381] total_loss: 2.14164686 d_loss: 1.38494134, g_loss: 0.70530176, ae_loss: 0.05140363\n",
      "Step: [1382] total_loss: 2.15079975 d_loss: 1.37704635, g_loss: 0.71970761, ae_loss: 0.05404579\n",
      "Step: [1383] total_loss: 2.13547444 d_loss: 1.38739550, g_loss: 0.69378543, ae_loss: 0.05429346\n",
      "Step: [1384] total_loss: 2.11854362 d_loss: 1.36678410, g_loss: 0.69670206, ae_loss: 0.05505753\n",
      "Step: [1385] total_loss: 2.13858795 d_loss: 1.38762581, g_loss: 0.69615710, ae_loss: 0.05480521\n",
      "Step: [1386] total_loss: 2.16334009 d_loss: 1.41459394, g_loss: 0.69356585, ae_loss: 0.05518024\n",
      "Step: [1387] total_loss: 2.12683392 d_loss: 1.37661004, g_loss: 0.69366348, ae_loss: 0.05656040\n",
      "Step: [1388] total_loss: 2.12950611 d_loss: 1.39434242, g_loss: 0.68319285, ae_loss: 0.05197085\n",
      "Step: [1389] total_loss: 2.12228179 d_loss: 1.38839483, g_loss: 0.68025702, ae_loss: 0.05363001\n",
      "Step: [1390] total_loss: 2.12323689 d_loss: 1.39281297, g_loss: 0.67936963, ae_loss: 0.05105434\n",
      "Step: [1391] total_loss: 2.13307548 d_loss: 1.39434540, g_loss: 0.68311822, ae_loss: 0.05561180\n",
      "Step: [1392] total_loss: 2.12464213 d_loss: 1.38087928, g_loss: 0.69347954, ae_loss: 0.05028329\n",
      "Step: [1393] total_loss: 2.13550687 d_loss: 1.39702880, g_loss: 0.68545198, ae_loss: 0.05302610\n",
      "Step: [1394] total_loss: 2.14031744 d_loss: 1.38853526, g_loss: 0.69816768, ae_loss: 0.05361446\n",
      "Step: [1395] total_loss: 2.10935593 d_loss: 1.36404574, g_loss: 0.69558865, ae_loss: 0.04972158\n",
      "Step: [1396] total_loss: 2.11510086 d_loss: 1.37552786, g_loss: 0.68915439, ae_loss: 0.05041866\n",
      "Step: [1397] total_loss: 2.11906362 d_loss: 1.36363673, g_loss: 0.70097995, ae_loss: 0.05444694\n",
      "Step: [1398] total_loss: 2.12063074 d_loss: 1.40069139, g_loss: 0.66564810, ae_loss: 0.05429117\n",
      "Step: [1399] total_loss: 2.12271738 d_loss: 1.37857366, g_loss: 0.69450653, ae_loss: 0.04963706\n",
      "Step: [1400] total_loss: 2.10786986 d_loss: 1.36188018, g_loss: 0.68964130, ae_loss: 0.05634839\n",
      "Step: [1401] total_loss: 2.09962940 d_loss: 1.37303591, g_loss: 0.67794359, ae_loss: 0.04864990\n",
      "Step: [1402] total_loss: 2.15516949 d_loss: 1.42385030, g_loss: 0.67849773, ae_loss: 0.05282141\n",
      "Step: [1403] total_loss: 2.12347555 d_loss: 1.37977827, g_loss: 0.69058442, ae_loss: 0.05311291\n",
      "Step: [1404] total_loss: 2.12721038 d_loss: 1.37461674, g_loss: 0.69906873, ae_loss: 0.05352481\n",
      "Step: [1405] total_loss: 2.13051534 d_loss: 1.38785362, g_loss: 0.69009113, ae_loss: 0.05257063\n",
      "Step: [1406] total_loss: 2.15486097 d_loss: 1.40485168, g_loss: 0.69414794, ae_loss: 0.05586153\n",
      "Step: [1407] total_loss: 2.12925100 d_loss: 1.37485111, g_loss: 0.70403826, ae_loss: 0.05036156\n",
      "Step: [1408] total_loss: 2.11842513 d_loss: 1.36742914, g_loss: 0.69958103, ae_loss: 0.05141496\n",
      "Step: [1409] total_loss: 2.13673902 d_loss: 1.38470423, g_loss: 0.70118129, ae_loss: 0.05085351\n",
      "Step: [1410] total_loss: 2.13177872 d_loss: 1.39021873, g_loss: 0.68842739, ae_loss: 0.05313258\n",
      "Step: [1411] total_loss: 2.12608552 d_loss: 1.38740087, g_loss: 0.68613374, ae_loss: 0.05255089\n",
      "Step: [1412] total_loss: 2.14935446 d_loss: 1.39395154, g_loss: 0.70221311, ae_loss: 0.05318982\n",
      "Step: [1413] total_loss: 2.15763450 d_loss: 1.41619921, g_loss: 0.68999809, ae_loss: 0.05143712\n",
      "Step: [1414] total_loss: 2.11489320 d_loss: 1.36827612, g_loss: 0.69220889, ae_loss: 0.05440825\n",
      "Step: [1415] total_loss: 2.11108994 d_loss: 1.39116406, g_loss: 0.66892874, ae_loss: 0.05099716\n",
      "Step: [1416] total_loss: 2.13413477 d_loss: 1.40256083, g_loss: 0.67849326, ae_loss: 0.05308059\n",
      "Step: [1417] total_loss: 2.14174008 d_loss: 1.39485669, g_loss: 0.69343919, ae_loss: 0.05344409\n",
      "Step: [1418] total_loss: 2.12540531 d_loss: 1.38373637, g_loss: 0.69140005, ae_loss: 0.05026898\n",
      "Step: [1419] total_loss: 2.13954949 d_loss: 1.36515188, g_loss: 0.72372365, ae_loss: 0.05067396\n",
      "Step: [1420] total_loss: 2.13044691 d_loss: 1.37921596, g_loss: 0.69669354, ae_loss: 0.05453744\n",
      "Step: [1421] total_loss: 2.15756822 d_loss: 1.40232778, g_loss: 0.70200402, ae_loss: 0.05323632\n",
      "Step: [1422] total_loss: 2.11415100 d_loss: 1.36034334, g_loss: 0.69924545, ae_loss: 0.05456207\n",
      "Step: [1423] total_loss: 2.12977266 d_loss: 1.38976943, g_loss: 0.68628758, ae_loss: 0.05371574\n",
      "Step: [1424] total_loss: 2.14736867 d_loss: 1.40208006, g_loss: 0.68885607, ae_loss: 0.05643259\n",
      "Step: [1425] total_loss: 2.12329912 d_loss: 1.38358545, g_loss: 0.68714976, ae_loss: 0.05256399\n",
      "Step: [1426] total_loss: 2.14109039 d_loss: 1.38174105, g_loss: 0.70470786, ae_loss: 0.05464160\n",
      "Step: [1427] total_loss: 2.13981199 d_loss: 1.39362597, g_loss: 0.69399691, ae_loss: 0.05218918\n",
      "Step: [1428] total_loss: 2.11683178 d_loss: 1.38418102, g_loss: 0.67971516, ae_loss: 0.05293546\n",
      "Step: [1429] total_loss: 2.11487651 d_loss: 1.36495709, g_loss: 0.70001042, ae_loss: 0.04990902\n",
      "Step: [1430] total_loss: 2.14057493 d_loss: 1.38302052, g_loss: 0.70528150, ae_loss: 0.05227286\n",
      "Step: [1431] total_loss: 2.14343905 d_loss: 1.39656126, g_loss: 0.69597507, ae_loss: 0.05090277\n",
      "Step: [1432] total_loss: 2.13880754 d_loss: 1.38122129, g_loss: 0.70296705, ae_loss: 0.05461915\n",
      "Step: [1433] total_loss: 2.15532947 d_loss: 1.39008808, g_loss: 0.71565539, ae_loss: 0.04958589\n",
      "Step: [1434] total_loss: 2.13417220 d_loss: 1.38118088, g_loss: 0.69752836, ae_loss: 0.05546296\n",
      "Step: [1435] total_loss: 2.14036322 d_loss: 1.36370540, g_loss: 0.72337842, ae_loss: 0.05327949\n",
      "Step: [1436] total_loss: 2.11377215 d_loss: 1.38092208, g_loss: 0.68072629, ae_loss: 0.05212379\n",
      "Step: [1437] total_loss: 2.12509227 d_loss: 1.40482712, g_loss: 0.67019790, ae_loss: 0.05006719\n",
      "Step: [1438] total_loss: 2.12872672 d_loss: 1.39456177, g_loss: 0.68581843, ae_loss: 0.04834653\n",
      "Step: [1439] total_loss: 2.13926983 d_loss: 1.39502239, g_loss: 0.69271803, ae_loss: 0.05152950\n",
      "Step: [1440] total_loss: 2.13215160 d_loss: 1.38293576, g_loss: 0.69939691, ae_loss: 0.04981885\n",
      "Step: [1441] total_loss: 2.13711667 d_loss: 1.38691425, g_loss: 0.69929594, ae_loss: 0.05090652\n",
      "Step: [1442] total_loss: 2.12332726 d_loss: 1.37943840, g_loss: 0.69103253, ae_loss: 0.05285630\n",
      "Step: [1443] total_loss: 2.14136100 d_loss: 1.41141784, g_loss: 0.67668408, ae_loss: 0.05325896\n",
      "Step: [1444] total_loss: 2.12585640 d_loss: 1.37426114, g_loss: 0.69777048, ae_loss: 0.05382492\n",
      "Step: [1445] total_loss: 2.12569857 d_loss: 1.37707353, g_loss: 0.69633657, ae_loss: 0.05228850\n",
      "Step: [1446] total_loss: 2.11756945 d_loss: 1.37048125, g_loss: 0.69543093, ae_loss: 0.05165715\n",
      "Step: [1447] total_loss: 2.12263966 d_loss: 1.36371756, g_loss: 0.70610774, ae_loss: 0.05281442\n",
      "Step: [1448] total_loss: 2.11662197 d_loss: 1.36475945, g_loss: 0.69721645, ae_loss: 0.05464618\n",
      "Step: [1449] total_loss: 2.12987161 d_loss: 1.39884520, g_loss: 0.67679530, ae_loss: 0.05423118\n",
      "Step: [1450] total_loss: 2.13467765 d_loss: 1.39319122, g_loss: 0.69374406, ae_loss: 0.04774242\n",
      "Step: [1451] total_loss: 2.13094687 d_loss: 1.37317038, g_loss: 0.70456910, ae_loss: 0.05320746\n",
      "Step: [1452] total_loss: 2.13532257 d_loss: 1.38451684, g_loss: 0.70240545, ae_loss: 0.04840013\n",
      "Step: [1453] total_loss: 2.13623524 d_loss: 1.36684287, g_loss: 0.71739995, ae_loss: 0.05199237\n",
      "Step: [1454] total_loss: 2.12879467 d_loss: 1.37050009, g_loss: 0.70184380, ae_loss: 0.05645083\n",
      "Step: [1455] total_loss: 2.15700912 d_loss: 1.42754614, g_loss: 0.67933691, ae_loss: 0.05012606\n",
      "Step: [1456] total_loss: 2.11919451 d_loss: 1.36225045, g_loss: 0.70439351, ae_loss: 0.05255038\n",
      "Step: [1457] total_loss: 2.12299776 d_loss: 1.37484622, g_loss: 0.69762665, ae_loss: 0.05052484\n",
      "Step: [1458] total_loss: 2.13579321 d_loss: 1.38323951, g_loss: 0.69450700, ae_loss: 0.05804658\n",
      "Step: [1459] total_loss: 2.12222147 d_loss: 1.38913834, g_loss: 0.67835546, ae_loss: 0.05472763\n",
      "Step: [1460] total_loss: 2.12576222 d_loss: 1.38695383, g_loss: 0.68975484, ae_loss: 0.04905351\n",
      "Step: [1461] total_loss: 2.11232543 d_loss: 1.37266278, g_loss: 0.68368578, ae_loss: 0.05597681\n",
      "Step: [1462] total_loss: 2.13637853 d_loss: 1.40193391, g_loss: 0.68713206, ae_loss: 0.04731250\n",
      "Step: [1463] total_loss: 2.13442421 d_loss: 1.38563752, g_loss: 0.69454181, ae_loss: 0.05424482\n",
      "Step: [1464] total_loss: 2.11318994 d_loss: 1.37951088, g_loss: 0.68494195, ae_loss: 0.04873720\n",
      "Step: [1465] total_loss: 2.13027143 d_loss: 1.39423954, g_loss: 0.68564129, ae_loss: 0.05039062\n",
      "Step: [1466] total_loss: 2.12497711 d_loss: 1.39001596, g_loss: 0.68157220, ae_loss: 0.05338891\n",
      "Step: [1467] total_loss: 2.11403799 d_loss: 1.36403906, g_loss: 0.69892871, ae_loss: 0.05107022\n",
      "Step: [1468] total_loss: 2.14215970 d_loss: 1.40056086, g_loss: 0.68461424, ae_loss: 0.05698467\n",
      "Step: [1469] total_loss: 2.16996455 d_loss: 1.40491796, g_loss: 0.70564801, ae_loss: 0.05939848\n",
      "Step: [1470] total_loss: 2.15089011 d_loss: 1.40072393, g_loss: 0.69687265, ae_loss: 0.05329361\n",
      "Step: [1471] total_loss: 2.14379549 d_loss: 1.38608170, g_loss: 0.70536387, ae_loss: 0.05234983\n",
      "Step: [1472] total_loss: 2.14945698 d_loss: 1.38559484, g_loss: 0.71011668, ae_loss: 0.05374548\n",
      "Step: [1473] total_loss: 2.13840365 d_loss: 1.37922287, g_loss: 0.70600688, ae_loss: 0.05317394\n",
      "Step: [1474] total_loss: 2.13247585 d_loss: 1.39379215, g_loss: 0.68239927, ae_loss: 0.05628449\n",
      "Step: [1475] total_loss: 2.11861968 d_loss: 1.36460352, g_loss: 0.70420897, ae_loss: 0.04980716\n",
      "Step: [1476] total_loss: 2.12345886 d_loss: 1.38381863, g_loss: 0.68979037, ae_loss: 0.04984988\n",
      "Step: [1477] total_loss: 2.13380527 d_loss: 1.40618539, g_loss: 0.67567134, ae_loss: 0.05194842\n",
      "Step: [1478] total_loss: 2.12639451 d_loss: 1.38668275, g_loss: 0.68758088, ae_loss: 0.05213086\n",
      "Step: [1479] total_loss: 2.13179970 d_loss: 1.40194678, g_loss: 0.67843807, ae_loss: 0.05141470\n",
      "Step: [1480] total_loss: 2.13237953 d_loss: 1.37746906, g_loss: 0.69782919, ae_loss: 0.05708121\n",
      "Step: [1481] total_loss: 2.14714766 d_loss: 1.36807728, g_loss: 0.72697508, ae_loss: 0.05209532\n",
      "Step: [1482] total_loss: 2.13292146 d_loss: 1.39626420, g_loss: 0.68668139, ae_loss: 0.04997583\n",
      "Step: [1483] total_loss: 2.13251638 d_loss: 1.39102471, g_loss: 0.68522513, ae_loss: 0.05626642\n",
      "Step: [1484] total_loss: 2.12848377 d_loss: 1.39839673, g_loss: 0.67677826, ae_loss: 0.05330887\n",
      "Step: [1485] total_loss: 2.12182546 d_loss: 1.38812172, g_loss: 0.68111265, ae_loss: 0.05259112\n",
      "Step: [1486] total_loss: 2.14389753 d_loss: 1.39057922, g_loss: 0.69668448, ae_loss: 0.05663371\n",
      "Step: [1487] total_loss: 2.13191891 d_loss: 1.37525010, g_loss: 0.70568299, ae_loss: 0.05098576\n",
      "Step: [1488] total_loss: 2.13285208 d_loss: 1.38781023, g_loss: 0.69530702, ae_loss: 0.04973479\n",
      "Step: [1489] total_loss: 2.11338401 d_loss: 1.37680078, g_loss: 0.68650639, ae_loss: 0.05007687\n",
      "Step: [1490] total_loss: 2.09334779 d_loss: 1.36253834, g_loss: 0.67795670, ae_loss: 0.05285272\n",
      "Step: [1491] total_loss: 2.12380075 d_loss: 1.37352061, g_loss: 0.69542831, ae_loss: 0.05485191\n",
      "Step: [1492] total_loss: 2.11335993 d_loss: 1.38090265, g_loss: 0.68107349, ae_loss: 0.05138376\n",
      "Step: [1493] total_loss: 2.11462069 d_loss: 1.36795688, g_loss: 0.69274271, ae_loss: 0.05392097\n",
      "Step: [1494] total_loss: 2.12563276 d_loss: 1.38412046, g_loss: 0.68863094, ae_loss: 0.05288129\n",
      "Step: [1495] total_loss: 2.14004731 d_loss: 1.40141320, g_loss: 0.68679386, ae_loss: 0.05184017\n",
      "Step: [1496] total_loss: 2.15725279 d_loss: 1.41421747, g_loss: 0.68922925, ae_loss: 0.05380601\n",
      "Step: [1497] total_loss: 2.14169383 d_loss: 1.38756871, g_loss: 0.70418662, ae_loss: 0.04993853\n",
      "Step: [1498] total_loss: 2.13473487 d_loss: 1.37504196, g_loss: 0.70521152, ae_loss: 0.05448135\n",
      "Step: [1499] total_loss: 2.11395073 d_loss: 1.37545109, g_loss: 0.68470132, ae_loss: 0.05379830\n",
      "Step: [1500] total_loss: 2.12021995 d_loss: 1.38929367, g_loss: 0.67913699, ae_loss: 0.05178927\n",
      "Step: [1501] total_loss: 2.11446047 d_loss: 1.36907911, g_loss: 0.68968719, ae_loss: 0.05569417\n",
      "Step: [1502] total_loss: 2.13475800 d_loss: 1.40207314, g_loss: 0.67959762, ae_loss: 0.05308720\n",
      "Step: [1503] total_loss: 2.13145542 d_loss: 1.39091349, g_loss: 0.69206810, ae_loss: 0.04847397\n",
      "Step: [1504] total_loss: 2.14939952 d_loss: 1.39194691, g_loss: 0.70452142, ae_loss: 0.05293113\n",
      "Step: [1505] total_loss: 2.14776635 d_loss: 1.38231301, g_loss: 0.71623898, ae_loss: 0.04921435\n",
      "Step: [1506] total_loss: 2.15495038 d_loss: 1.39593673, g_loss: 0.70050323, ae_loss: 0.05851047\n",
      "Step: [1507] total_loss: 2.14162636 d_loss: 1.39558411, g_loss: 0.69259232, ae_loss: 0.05344998\n",
      "Step: [1508] total_loss: 2.13609362 d_loss: 1.38078070, g_loss: 0.70347226, ae_loss: 0.05184075\n",
      "Step: [1509] total_loss: 2.11816835 d_loss: 1.37755716, g_loss: 0.68942463, ae_loss: 0.05118638\n",
      "Step: [1510] total_loss: 2.13198185 d_loss: 1.38451743, g_loss: 0.68917191, ae_loss: 0.05829264\n",
      "Step: [1511] total_loss: 2.13626838 d_loss: 1.39224327, g_loss: 0.68766189, ae_loss: 0.05636324\n",
      "Step: [1512] total_loss: 2.13728881 d_loss: 1.38709188, g_loss: 0.69885635, ae_loss: 0.05134061\n",
      "Step: [1513] total_loss: 2.14360428 d_loss: 1.39380836, g_loss: 0.69985688, ae_loss: 0.04993913\n",
      "Step: [1514] total_loss: 2.12250710 d_loss: 1.37464905, g_loss: 0.69194889, ae_loss: 0.05590900\n",
      "Step: [1515] total_loss: 2.13532829 d_loss: 1.37383890, g_loss: 0.70321447, ae_loss: 0.05827501\n",
      "Step: [1516] total_loss: 2.10395718 d_loss: 1.34927201, g_loss: 0.70394540, ae_loss: 0.05073982\n",
      "Step: [1517] total_loss: 2.14836478 d_loss: 1.40856552, g_loss: 0.68527395, ae_loss: 0.05452539\n",
      "Step: [1518] total_loss: 2.13272858 d_loss: 1.39927089, g_loss: 0.68050516, ae_loss: 0.05295259\n",
      "Step: [1519] total_loss: 2.12303972 d_loss: 1.38401425, g_loss: 0.68734491, ae_loss: 0.05168074\n",
      "Step: [1520] total_loss: 2.10915565 d_loss: 1.37560582, g_loss: 0.67780268, ae_loss: 0.05574720\n",
      "Step: [1521] total_loss: 2.12311935 d_loss: 1.36365676, g_loss: 0.70647097, ae_loss: 0.05299160\n",
      "Step: [1522] total_loss: 2.12057734 d_loss: 1.38302398, g_loss: 0.68574804, ae_loss: 0.05180523\n",
      "Step: [1523] total_loss: 2.16088843 d_loss: 1.40852284, g_loss: 0.69834852, ae_loss: 0.05401709\n",
      "Step: [1524] total_loss: 2.12560368 d_loss: 1.36639917, g_loss: 0.71063375, ae_loss: 0.04857064\n",
      "Step: [1525] total_loss: 2.14687443 d_loss: 1.40506911, g_loss: 0.69044048, ae_loss: 0.05136477\n",
      "Step: [1526] total_loss: 2.15146470 d_loss: 1.39673877, g_loss: 0.69979078, ae_loss: 0.05493512\n",
      "Step: [1527] total_loss: 2.14243174 d_loss: 1.38391268, g_loss: 0.70163053, ae_loss: 0.05688847\n",
      "Step: [1528] total_loss: 2.12551785 d_loss: 1.39033949, g_loss: 0.68477643, ae_loss: 0.05040189\n",
      "Step: [1529] total_loss: 2.13626337 d_loss: 1.39017677, g_loss: 0.69021857, ae_loss: 0.05586796\n",
      "Step: [1530] total_loss: 2.13275886 d_loss: 1.37208319, g_loss: 0.71234512, ae_loss: 0.04833057\n",
      "Step: [1531] total_loss: 2.13295937 d_loss: 1.38420963, g_loss: 0.69622546, ae_loss: 0.05252417\n",
      "Step: [1532] total_loss: 2.11995435 d_loss: 1.36928928, g_loss: 0.69800746, ae_loss: 0.05265757\n",
      "Step: [1533] total_loss: 2.12577152 d_loss: 1.39520049, g_loss: 0.67762125, ae_loss: 0.05294967\n",
      "Step: [1534] total_loss: 2.12642574 d_loss: 1.39346218, g_loss: 0.67962468, ae_loss: 0.05333872\n",
      "Step: [1535] total_loss: 2.10127282 d_loss: 1.36815214, g_loss: 0.68044478, ae_loss: 0.05267590\n",
      "Step: [1536] total_loss: 2.10457945 d_loss: 1.36337256, g_loss: 0.68835497, ae_loss: 0.05285185\n",
      "Step: [1537] total_loss: 2.12315106 d_loss: 1.36284757, g_loss: 0.70633644, ae_loss: 0.05396708\n",
      "Step: [1538] total_loss: 2.13005447 d_loss: 1.37373674, g_loss: 0.70271957, ae_loss: 0.05359818\n",
      "Step: [1539] total_loss: 2.14067626 d_loss: 1.38229358, g_loss: 0.70396101, ae_loss: 0.05442171\n",
      "Step: [1540] total_loss: 2.12680006 d_loss: 1.37586689, g_loss: 0.69896626, ae_loss: 0.05196685\n",
      "Step: [1541] total_loss: 2.12737513 d_loss: 1.38221896, g_loss: 0.68907118, ae_loss: 0.05608490\n",
      "Step: [1542] total_loss: 2.11989307 d_loss: 1.38828158, g_loss: 0.68109655, ae_loss: 0.05051478\n",
      "Step: [1543] total_loss: 2.12329888 d_loss: 1.39887929, g_loss: 0.67020166, ae_loss: 0.05421791\n",
      "Step: [1544] total_loss: 2.12851882 d_loss: 1.38011932, g_loss: 0.69099778, ae_loss: 0.05740177\n",
      "Step: [1545] total_loss: 2.12051702 d_loss: 1.38903654, g_loss: 0.67926389, ae_loss: 0.05221664\n",
      "Step: [1546] total_loss: 2.13839865 d_loss: 1.39625561, g_loss: 0.69138497, ae_loss: 0.05075802\n",
      "Step: [1547] total_loss: 2.14855909 d_loss: 1.39313316, g_loss: 0.70518732, ae_loss: 0.05023865\n",
      "Step: [1548] total_loss: 2.11699915 d_loss: 1.35719693, g_loss: 0.70680583, ae_loss: 0.05299654\n",
      "Step: [1549] total_loss: 2.11978912 d_loss: 1.38938534, g_loss: 0.68254697, ae_loss: 0.04785670\n",
      "Step: [1550] total_loss: 2.11693692 d_loss: 1.37282348, g_loss: 0.69011992, ae_loss: 0.05399362\n",
      "Step: [1551] total_loss: 2.12886906 d_loss: 1.38237369, g_loss: 0.69141495, ae_loss: 0.05508050\n",
      "Step: [1552] total_loss: 2.13926268 d_loss: 1.38330626, g_loss: 0.70748127, ae_loss: 0.04847524\n",
      "Step: [1553] total_loss: 2.12383747 d_loss: 1.38639963, g_loss: 0.68754721, ae_loss: 0.04989070\n",
      "Step: [1554] total_loss: 2.12092209 d_loss: 1.36765540, g_loss: 0.69945574, ae_loss: 0.05381091\n",
      "Step: [1555] total_loss: 2.14508533 d_loss: 1.39850509, g_loss: 0.69245446, ae_loss: 0.05412575\n",
      "Step: [1556] total_loss: 2.15464807 d_loss: 1.38960004, g_loss: 0.71074295, ae_loss: 0.05430513\n",
      "Step: [1557] total_loss: 2.12307143 d_loss: 1.37395298, g_loss: 0.69842750, ae_loss: 0.05069096\n",
      "Step: [1558] total_loss: 2.12045717 d_loss: 1.38003051, g_loss: 0.68946087, ae_loss: 0.05096577\n",
      "Step: [1559] total_loss: 2.12591553 d_loss: 1.39153314, g_loss: 0.68163431, ae_loss: 0.05274823\n",
      "Step: [1560] total_loss: 2.12737322 d_loss: 1.36831427, g_loss: 0.70461380, ae_loss: 0.05444533\n",
      "Step: [1561] total_loss: 2.10642529 d_loss: 1.34352827, g_loss: 0.70880264, ae_loss: 0.05409435\n",
      "Step: [1562] total_loss: 2.11895037 d_loss: 1.37995660, g_loss: 0.68474913, ae_loss: 0.05424473\n",
      "Step: [1563] total_loss: 2.13336873 d_loss: 1.39557052, g_loss: 0.68279076, ae_loss: 0.05500742\n",
      "Step: [1564] total_loss: 2.12284374 d_loss: 1.38625145, g_loss: 0.68552327, ae_loss: 0.05106905\n",
      "Step: [1565] total_loss: 2.12149620 d_loss: 1.38239861, g_loss: 0.68447423, ae_loss: 0.05462351\n",
      "Step: [1566] total_loss: 2.13383007 d_loss: 1.39173794, g_loss: 0.68889439, ae_loss: 0.05319783\n",
      "Step: [1567] total_loss: 2.13600492 d_loss: 1.36928880, g_loss: 0.71292806, ae_loss: 0.05378800\n",
      "Step: [1568] total_loss: 2.12340498 d_loss: 1.36766481, g_loss: 0.70229101, ae_loss: 0.05344918\n",
      "Step: [1569] total_loss: 2.12365460 d_loss: 1.38015199, g_loss: 0.69208932, ae_loss: 0.05141331\n",
      "Step: [1570] total_loss: 2.12580228 d_loss: 1.37828052, g_loss: 0.69537652, ae_loss: 0.05214523\n",
      "Step: [1571] total_loss: 2.12022543 d_loss: 1.37473524, g_loss: 0.69146705, ae_loss: 0.05402319\n",
      "Step: [1572] total_loss: 2.13848114 d_loss: 1.38654315, g_loss: 0.69665575, ae_loss: 0.05528241\n",
      "Step: [1573] total_loss: 2.10538960 d_loss: 1.34393358, g_loss: 0.71006083, ae_loss: 0.05139522\n",
      "Step: [1574] total_loss: 2.12358189 d_loss: 1.37726927, g_loss: 0.69372284, ae_loss: 0.05258986\n",
      "Step: [1575] total_loss: 2.12482643 d_loss: 1.38175476, g_loss: 0.68685961, ae_loss: 0.05621217\n",
      "Step: [1576] total_loss: 2.12691975 d_loss: 1.37623298, g_loss: 0.69602531, ae_loss: 0.05466153\n",
      "Step: [1577] total_loss: 2.10782480 d_loss: 1.35962987, g_loss: 0.69106847, ae_loss: 0.05712657\n",
      "Step: [1578] total_loss: 2.10298491 d_loss: 1.36688209, g_loss: 0.68432301, ae_loss: 0.05177991\n",
      "Step: [1579] total_loss: 2.12871122 d_loss: 1.41951394, g_loss: 0.65665364, ae_loss: 0.05254376\n",
      "Step: [1580] total_loss: 2.10457373 d_loss: 1.37040651, g_loss: 0.67785501, ae_loss: 0.05631206\n",
      "Step: [1581] total_loss: 2.13330603 d_loss: 1.38610816, g_loss: 0.69630891, ae_loss: 0.05088889\n",
      "Step: [1582] total_loss: 2.12569904 d_loss: 1.39515281, g_loss: 0.67480183, ae_loss: 0.05574449\n",
      "Step: [1583] total_loss: 2.13109446 d_loss: 1.37516117, g_loss: 0.70448202, ae_loss: 0.05145128\n",
      "Step: [1584] total_loss: 2.13165617 d_loss: 1.36334062, g_loss: 0.71147746, ae_loss: 0.05683816\n",
      "Step: [1585] total_loss: 2.12777710 d_loss: 1.36844766, g_loss: 0.70703030, ae_loss: 0.05229900\n",
      "Step: [1586] total_loss: 2.13542032 d_loss: 1.38223600, g_loss: 0.70367777, ae_loss: 0.04950653\n",
      "Step: [1587] total_loss: 2.13558245 d_loss: 1.40054679, g_loss: 0.68166113, ae_loss: 0.05337440\n",
      "Step: [1588] total_loss: 2.13489175 d_loss: 1.37851739, g_loss: 0.70611775, ae_loss: 0.05025660\n",
      "Step: [1589] total_loss: 2.13026071 d_loss: 1.38934755, g_loss: 0.68596685, ae_loss: 0.05494636\n",
      "Step: [1590] total_loss: 2.11528325 d_loss: 1.37071979, g_loss: 0.69122863, ae_loss: 0.05333482\n",
      "Step: [1591] total_loss: 2.10872173 d_loss: 1.37200809, g_loss: 0.68266535, ae_loss: 0.05404814\n",
      "Step: [1592] total_loss: 2.11520576 d_loss: 1.36555910, g_loss: 0.69723547, ae_loss: 0.05241115\n",
      "Step: [1593] total_loss: 2.10320163 d_loss: 1.36902118, g_loss: 0.67897600, ae_loss: 0.05520452\n",
      "Step: [1594] total_loss: 2.12364578 d_loss: 1.38241100, g_loss: 0.68384778, ae_loss: 0.05738697\n",
      "Step: [1595] total_loss: 2.15155768 d_loss: 1.39865041, g_loss: 0.69608605, ae_loss: 0.05682126\n",
      "Step: [1596] total_loss: 2.12637043 d_loss: 1.36890686, g_loss: 0.70902848, ae_loss: 0.04843497\n",
      "Step: [1597] total_loss: 2.12120175 d_loss: 1.38497460, g_loss: 0.68392181, ae_loss: 0.05230535\n",
      "Step: [1598] total_loss: 2.12764335 d_loss: 1.38218808, g_loss: 0.69532514, ae_loss: 0.05013008\n",
      "Step: [1599] total_loss: 2.14316130 d_loss: 1.40824080, g_loss: 0.68619168, ae_loss: 0.04872877\n",
      "Step: [1600] total_loss: 2.16032338 d_loss: 1.42026544, g_loss: 0.68796885, ae_loss: 0.05208914\n",
      "Step: [1601] total_loss: 2.15242624 d_loss: 1.38979030, g_loss: 0.71012676, ae_loss: 0.05250922\n",
      "Step: [1602] total_loss: 2.11569190 d_loss: 1.37766600, g_loss: 0.68510306, ae_loss: 0.05292290\n",
      "Step: [1603] total_loss: 2.12707663 d_loss: 1.38155317, g_loss: 0.69274837, ae_loss: 0.05277504\n",
      "Step: [1604] total_loss: 2.12455082 d_loss: 1.37659168, g_loss: 0.69338262, ae_loss: 0.05457648\n",
      "Step: [1605] total_loss: 2.13673592 d_loss: 1.37875688, g_loss: 0.70479453, ae_loss: 0.05318441\n",
      "Step: [1606] total_loss: 2.13177896 d_loss: 1.39700103, g_loss: 0.68192309, ae_loss: 0.05285473\n",
      "Step: [1607] total_loss: 2.12073421 d_loss: 1.37520146, g_loss: 0.69469202, ae_loss: 0.05084072\n",
      "Step: [1608] total_loss: 2.12509823 d_loss: 1.37655449, g_loss: 0.69550079, ae_loss: 0.05304284\n",
      "Step: [1609] total_loss: 2.13903475 d_loss: 1.38380992, g_loss: 0.70233285, ae_loss: 0.05289179\n",
      "Step: [1610] total_loss: 2.12665510 d_loss: 1.38742948, g_loss: 0.68296778, ae_loss: 0.05625768\n",
      "Step: [1611] total_loss: 2.12507844 d_loss: 1.34785509, g_loss: 0.72085613, ae_loss: 0.05636733\n",
      "Step: [1612] total_loss: 2.13076925 d_loss: 1.39863396, g_loss: 0.68302953, ae_loss: 0.04910588\n",
      "Step: [1613] total_loss: 2.13455462 d_loss: 1.37683570, g_loss: 0.70504314, ae_loss: 0.05267585\n",
      "Step: [1614] total_loss: 2.12010264 d_loss: 1.38559175, g_loss: 0.68611175, ae_loss: 0.04839908\n",
      "Step: [1615] total_loss: 2.15456343 d_loss: 1.39010835, g_loss: 0.71332121, ae_loss: 0.05113393\n",
      "Step: [1616] total_loss: 2.11923409 d_loss: 1.38014376, g_loss: 0.68696082, ae_loss: 0.05212941\n",
      "Step: [1617] total_loss: 2.10096788 d_loss: 1.37340593, g_loss: 0.67639971, ae_loss: 0.05116207\n",
      "Step: [1618] total_loss: 2.09399796 d_loss: 1.36532927, g_loss: 0.67724919, ae_loss: 0.05141948\n",
      "Step: [1619] total_loss: 2.11542988 d_loss: 1.37484884, g_loss: 0.69158912, ae_loss: 0.04899184\n",
      "Step: [1620] total_loss: 2.12559700 d_loss: 1.38350999, g_loss: 0.69242454, ae_loss: 0.04966250\n",
      "Step: [1621] total_loss: 2.13167667 d_loss: 1.40119410, g_loss: 0.68326908, ae_loss: 0.04721345\n",
      "Step: [1622] total_loss: 2.12873363 d_loss: 1.37197185, g_loss: 0.70447415, ae_loss: 0.05228772\n",
      "Step: [1623] total_loss: 2.13314152 d_loss: 1.38310695, g_loss: 0.69661224, ae_loss: 0.05342218\n",
      "Step: [1624] total_loss: 2.12649012 d_loss: 1.37976539, g_loss: 0.69799614, ae_loss: 0.04872863\n",
      "Step: [1625] total_loss: 2.11337852 d_loss: 1.38168550, g_loss: 0.67696857, ae_loss: 0.05472448\n",
      "Step: [1626] total_loss: 2.11756229 d_loss: 1.38828111, g_loss: 0.68020153, ae_loss: 0.04907969\n",
      "Step: [1627] total_loss: 2.11788797 d_loss: 1.37911201, g_loss: 0.68794143, ae_loss: 0.05083452\n",
      "Step: [1628] total_loss: 2.14295387 d_loss: 1.39426851, g_loss: 0.69252455, ae_loss: 0.05616084\n",
      "Step: [1629] total_loss: 2.10215998 d_loss: 1.33090520, g_loss: 0.71960783, ae_loss: 0.05164703\n",
      "Step: [1630] total_loss: 2.14652824 d_loss: 1.40172935, g_loss: 0.68950927, ae_loss: 0.05528966\n",
      "Step: [1631] total_loss: 2.11388588 d_loss: 1.37174904, g_loss: 0.68936145, ae_loss: 0.05277555\n",
      "Step: [1632] total_loss: 2.14560390 d_loss: 1.38730979, g_loss: 0.70471847, ae_loss: 0.05357561\n",
      "Step: [1633] total_loss: 2.15443945 d_loss: 1.39584017, g_loss: 0.70484960, ae_loss: 0.05374985\n",
      "Step: [1634] total_loss: 2.12510490 d_loss: 1.39503598, g_loss: 0.67831695, ae_loss: 0.05175185\n",
      "Step: [1635] total_loss: 2.11003757 d_loss: 1.37295103, g_loss: 0.68739390, ae_loss: 0.04969266\n",
      "Step: [1636] total_loss: 2.10050464 d_loss: 1.35872245, g_loss: 0.68764681, ae_loss: 0.05413544\n",
      "Step: [1637] total_loss: 2.13409400 d_loss: 1.38962483, g_loss: 0.69459969, ae_loss: 0.04986937\n",
      "Step: [1638] total_loss: 2.14461780 d_loss: 1.39437914, g_loss: 0.69567186, ae_loss: 0.05456676\n",
      "Step: [1639] total_loss: 2.11480236 d_loss: 1.35590863, g_loss: 0.70578420, ae_loss: 0.05310967\n",
      "Step: [1640] total_loss: 2.15676928 d_loss: 1.40128350, g_loss: 0.70438862, ae_loss: 0.05109731\n",
      "Step: [1641] total_loss: 2.15100145 d_loss: 1.42048526, g_loss: 0.67720741, ae_loss: 0.05330872\n",
      "Step: [1642] total_loss: 2.13071609 d_loss: 1.39950919, g_loss: 0.68192339, ae_loss: 0.04928353\n",
      "Step: [1643] total_loss: 2.13651824 d_loss: 1.38199389, g_loss: 0.70021129, ae_loss: 0.05431310\n",
      "Step: [1644] total_loss: 2.11570978 d_loss: 1.36611176, g_loss: 0.69509041, ae_loss: 0.05450764\n",
      "Step: [1645] total_loss: 2.12440753 d_loss: 1.39593220, g_loss: 0.67428863, ae_loss: 0.05418665\n",
      "Step: [1646] total_loss: 2.13108587 d_loss: 1.39625597, g_loss: 0.68257111, ae_loss: 0.05225877\n",
      "Step: [1647] total_loss: 2.11320829 d_loss: 1.36522996, g_loss: 0.69781220, ae_loss: 0.05016610\n",
      "Step: [1648] total_loss: 2.10961008 d_loss: 1.36890471, g_loss: 0.68899560, ae_loss: 0.05170968\n",
      "Step: [1649] total_loss: 2.11550021 d_loss: 1.37314153, g_loss: 0.69188350, ae_loss: 0.05047508\n",
      "Step: [1650] total_loss: 2.13964128 d_loss: 1.39863443, g_loss: 0.68771595, ae_loss: 0.05329097\n",
      "Step: [1651] total_loss: 2.13339949 d_loss: 1.38296866, g_loss: 0.69926524, ae_loss: 0.05116566\n",
      "Step: [1652] total_loss: 2.14631701 d_loss: 1.39891887, g_loss: 0.69585347, ae_loss: 0.05154461\n",
      "Step: [1653] total_loss: 2.12374616 d_loss: 1.37568998, g_loss: 0.69347048, ae_loss: 0.05458572\n",
      "Step: [1654] total_loss: 2.12871552 d_loss: 1.38147128, g_loss: 0.69451272, ae_loss: 0.05273145\n",
      "Step: [1655] total_loss: 2.11460423 d_loss: 1.37358725, g_loss: 0.69065785, ae_loss: 0.05035916\n",
      "Step: [1656] total_loss: 2.12068510 d_loss: 1.38762140, g_loss: 0.68096530, ae_loss: 0.05209824\n",
      "Step: [1657] total_loss: 2.10642529 d_loss: 1.36710811, g_loss: 0.68631274, ae_loss: 0.05300453\n",
      "Step: [1658] total_loss: 2.14352703 d_loss: 1.40974653, g_loss: 0.67772627, ae_loss: 0.05605436\n",
      "Step: [1659] total_loss: 2.13477230 d_loss: 1.39600015, g_loss: 0.68957913, ae_loss: 0.04919315\n",
      "Step: [1660] total_loss: 2.13848734 d_loss: 1.39954233, g_loss: 0.68939304, ae_loss: 0.04955209\n",
      "Step: [1661] total_loss: 2.16570878 d_loss: 1.40471661, g_loss: 0.70883608, ae_loss: 0.05215605\n",
      "Step: [1662] total_loss: 2.13427258 d_loss: 1.37408888, g_loss: 0.70581502, ae_loss: 0.05436859\n",
      "Step: [1663] total_loss: 2.15110016 d_loss: 1.40404153, g_loss: 0.69335669, ae_loss: 0.05370197\n",
      "Step: [1664] total_loss: 2.12534833 d_loss: 1.38256419, g_loss: 0.69228059, ae_loss: 0.05050363\n",
      "Step: [1665] total_loss: 2.12436581 d_loss: 1.39107656, g_loss: 0.68129003, ae_loss: 0.05199919\n",
      "Step: [1666] total_loss: 2.11517525 d_loss: 1.36824632, g_loss: 0.69476342, ae_loss: 0.05216547\n",
      "Step: [1667] total_loss: 2.12419605 d_loss: 1.37603021, g_loss: 0.69656050, ae_loss: 0.05160535\n",
      "Step: [1668] total_loss: 2.10760379 d_loss: 1.36546493, g_loss: 0.69044054, ae_loss: 0.05169830\n",
      "Step: [1669] total_loss: 2.11408329 d_loss: 1.37025797, g_loss: 0.69469088, ae_loss: 0.04913438\n",
      "Step: [1670] total_loss: 2.13110685 d_loss: 1.38734031, g_loss: 0.68989289, ae_loss: 0.05387348\n",
      "Step: [1671] total_loss: 2.13992572 d_loss: 1.39858031, g_loss: 0.69220322, ae_loss: 0.04914229\n",
      "Step: [1672] total_loss: 2.14323330 d_loss: 1.39763618, g_loss: 0.68998450, ae_loss: 0.05561255\n",
      "Step: [1673] total_loss: 2.11858439 d_loss: 1.37533522, g_loss: 0.69248831, ae_loss: 0.05076091\n",
      "Step: [1674] total_loss: 2.14194822 d_loss: 1.37893200, g_loss: 0.71214437, ae_loss: 0.05087176\n",
      "Step: [1675] total_loss: 2.12828422 d_loss: 1.37742889, g_loss: 0.69945449, ae_loss: 0.05140080\n",
      "Step: [1676] total_loss: 2.15524387 d_loss: 1.40103030, g_loss: 0.70043820, ae_loss: 0.05377531\n",
      "Step: [1677] total_loss: 2.13614416 d_loss: 1.39094043, g_loss: 0.69263077, ae_loss: 0.05257294\n",
      "Step: [1678] total_loss: 2.09739065 d_loss: 1.36079454, g_loss: 0.68438649, ae_loss: 0.05220964\n",
      "Step: [1679] total_loss: 2.11204314 d_loss: 1.37265825, g_loss: 0.69049937, ae_loss: 0.04888542\n",
      "Step: [1680] total_loss: 2.10793352 d_loss: 1.37704158, g_loss: 0.68048739, ae_loss: 0.05040464\n",
      "Step: [1681] total_loss: 2.13448644 d_loss: 1.40735054, g_loss: 0.67608100, ae_loss: 0.05105492\n",
      "Step: [1682] total_loss: 2.14660025 d_loss: 1.40205097, g_loss: 0.69112474, ae_loss: 0.05342456\n",
      "Step: [1683] total_loss: 2.11891174 d_loss: 1.38341379, g_loss: 0.68539882, ae_loss: 0.05009912\n",
      "Step: [1684] total_loss: 2.11201429 d_loss: 1.39477110, g_loss: 0.67046666, ae_loss: 0.04677647\n",
      "Step: [1685] total_loss: 2.14361691 d_loss: 1.39244461, g_loss: 0.69370031, ae_loss: 0.05747204\n",
      "Step: [1686] total_loss: 2.12657785 d_loss: 1.39027524, g_loss: 0.68468297, ae_loss: 0.05161972\n",
      "Step: [1687] total_loss: 2.11750603 d_loss: 1.37072265, g_loss: 0.69634783, ae_loss: 0.05043555\n",
      "Step: [1688] total_loss: 2.15949154 d_loss: 1.39581764, g_loss: 0.71037644, ae_loss: 0.05329751\n",
      "Step: [1689] total_loss: 2.14067912 d_loss: 1.40063632, g_loss: 0.68599826, ae_loss: 0.05404466\n",
      "Step: [1690] total_loss: 2.12948942 d_loss: 1.39657211, g_loss: 0.68284357, ae_loss: 0.05007387\n",
      "Step: [1691] total_loss: 2.12913394 d_loss: 1.39254546, g_loss: 0.68425614, ae_loss: 0.05233230\n",
      "Step: [1692] total_loss: 2.10768771 d_loss: 1.38283920, g_loss: 0.67702758, ae_loss: 0.04782093\n",
      "Step: [1693] total_loss: 2.12558794 d_loss: 1.39068568, g_loss: 0.68068659, ae_loss: 0.05421575\n",
      "Step: [1694] total_loss: 2.13540840 d_loss: 1.39911640, g_loss: 0.68236434, ae_loss: 0.05392748\n",
      "Step: [1695] total_loss: 2.14368701 d_loss: 1.39815116, g_loss: 0.69001043, ae_loss: 0.05552537\n",
      "Step: [1696] total_loss: 2.14118338 d_loss: 1.40034986, g_loss: 0.68760622, ae_loss: 0.05322714\n",
      "Step: [1697] total_loss: 2.12732053 d_loss: 1.38776445, g_loss: 0.68652451, ae_loss: 0.05303159\n",
      "Step: [1698] total_loss: 2.12265825 d_loss: 1.38560700, g_loss: 0.68907905, ae_loss: 0.04797208\n",
      "Step: [1699] total_loss: 2.12893748 d_loss: 1.37312806, g_loss: 0.69878173, ae_loss: 0.05702767\n",
      "Step: [1700] total_loss: 2.11449218 d_loss: 1.38530040, g_loss: 0.67578608, ae_loss: 0.05340567\n",
      "Step: [1701] total_loss: 2.12438297 d_loss: 1.38655555, g_loss: 0.68633473, ae_loss: 0.05149268\n",
      "Step: [1702] total_loss: 2.09771204 d_loss: 1.36013663, g_loss: 0.68032730, ae_loss: 0.05724799\n",
      "Step: [1703] total_loss: 2.12731647 d_loss: 1.39574242, g_loss: 0.68088990, ae_loss: 0.05068425\n",
      "Step: [1704] total_loss: 2.12671065 d_loss: 1.38134480, g_loss: 0.69390005, ae_loss: 0.05146588\n",
      "Step: [1705] total_loss: 2.12095404 d_loss: 1.38378131, g_loss: 0.68710172, ae_loss: 0.05007091\n",
      "Step: [1706] total_loss: 2.12286234 d_loss: 1.38101721, g_loss: 0.69428742, ae_loss: 0.04755770\n",
      "Step: [1707] total_loss: 2.12924933 d_loss: 1.38492668, g_loss: 0.69267458, ae_loss: 0.05164813\n",
      "Step: [1708] total_loss: 2.12953663 d_loss: 1.37130880, g_loss: 0.70420247, ae_loss: 0.05402531\n",
      "Step: [1709] total_loss: 2.12539411 d_loss: 1.38805902, g_loss: 0.68703789, ae_loss: 0.05029726\n",
      "Step: [1710] total_loss: 2.12540579 d_loss: 1.38961339, g_loss: 0.68271172, ae_loss: 0.05308078\n",
      "Step: [1711] total_loss: 2.13966012 d_loss: 1.38660741, g_loss: 0.70049918, ae_loss: 0.05255359\n",
      "Step: [1712] total_loss: 2.14994669 d_loss: 1.38960791, g_loss: 0.70324552, ae_loss: 0.05709312\n",
      "Step: [1713] total_loss: 2.12632561 d_loss: 1.37956941, g_loss: 0.69201112, ae_loss: 0.05474525\n",
      "Step: [1714] total_loss: 2.13155556 d_loss: 1.38808227, g_loss: 0.69055283, ae_loss: 0.05292046\n",
      "Step: [1715] total_loss: 2.08956861 d_loss: 1.36053395, g_loss: 0.67751777, ae_loss: 0.05151701\n",
      "Step: [1716] total_loss: 2.12302947 d_loss: 1.39796710, g_loss: 0.67769021, ae_loss: 0.04737215\n",
      "Step: [1717] total_loss: 2.11238122 d_loss: 1.37162638, g_loss: 0.69145858, ae_loss: 0.04929631\n",
      "Step: [1718] total_loss: 2.12367439 d_loss: 1.39747989, g_loss: 0.67049456, ae_loss: 0.05569995\n",
      "Step: [1719] total_loss: 2.10602593 d_loss: 1.37767708, g_loss: 0.67618936, ae_loss: 0.05215942\n",
      "Step: [1720] total_loss: 2.12145638 d_loss: 1.37721109, g_loss: 0.68765843, ae_loss: 0.05658684\n",
      "Step: [1721] total_loss: 2.13302612 d_loss: 1.38449049, g_loss: 0.69799805, ae_loss: 0.05053755\n",
      "Step: [1722] total_loss: 2.12786746 d_loss: 1.37612343, g_loss: 0.69980907, ae_loss: 0.05193496\n",
      "Step: [1723] total_loss: 2.12973785 d_loss: 1.39175630, g_loss: 0.68710369, ae_loss: 0.05087795\n",
      "Step: [1724] total_loss: 2.14058208 d_loss: 1.38615060, g_loss: 0.70402914, ae_loss: 0.05040225\n",
      "Step: [1725] total_loss: 2.13018632 d_loss: 1.38705337, g_loss: 0.69046974, ae_loss: 0.05266325\n",
      "Step: [1726] total_loss: 2.13884830 d_loss: 1.39743662, g_loss: 0.69212532, ae_loss: 0.04928648\n",
      "Step: [1727] total_loss: 2.12582064 d_loss: 1.38373923, g_loss: 0.68885761, ae_loss: 0.05322389\n",
      "Step: [1728] total_loss: 2.11568356 d_loss: 1.38638568, g_loss: 0.67299151, ae_loss: 0.05630621\n",
      "Step: [1729] total_loss: 2.11652851 d_loss: 1.38481224, g_loss: 0.67785513, ae_loss: 0.05386108\n",
      "Step: [1730] total_loss: 2.10466790 d_loss: 1.38341773, g_loss: 0.67317522, ae_loss: 0.04807491\n",
      "Step: [1731] total_loss: 2.13293505 d_loss: 1.38379288, g_loss: 0.69763696, ae_loss: 0.05150520\n",
      "Step: [1732] total_loss: 2.12432122 d_loss: 1.37374687, g_loss: 0.70269066, ae_loss: 0.04788366\n",
      "Step: [1733] total_loss: 2.13614440 d_loss: 1.38045144, g_loss: 0.70358080, ae_loss: 0.05211214\n",
      "Step: [1734] total_loss: 2.14637661 d_loss: 1.38685071, g_loss: 0.70476866, ae_loss: 0.05475714\n",
      "Step: [1735] total_loss: 2.10936689 d_loss: 1.36530232, g_loss: 0.68914211, ae_loss: 0.05492260\n",
      "Step: [1736] total_loss: 2.13515234 d_loss: 1.38593531, g_loss: 0.69594598, ae_loss: 0.05327091\n",
      "Step: [1737] total_loss: 2.12436438 d_loss: 1.36732459, g_loss: 0.70652127, ae_loss: 0.05051834\n",
      "Step: [1738] total_loss: 2.13390779 d_loss: 1.38225555, g_loss: 0.69885707, ae_loss: 0.05279506\n",
      "Step: [1739] total_loss: 2.12690544 d_loss: 1.38269639, g_loss: 0.69117284, ae_loss: 0.05303636\n",
      "Step: [1740] total_loss: 2.11702681 d_loss: 1.37589359, g_loss: 0.69001555, ae_loss: 0.05111772\n",
      "Step: [1741] total_loss: 2.12693453 d_loss: 1.38719511, g_loss: 0.68654197, ae_loss: 0.05319749\n",
      "Step: [1742] total_loss: 2.14273739 d_loss: 1.39683032, g_loss: 0.69288617, ae_loss: 0.05302100\n",
      "Step: [1743] total_loss: 2.13242960 d_loss: 1.38805532, g_loss: 0.68854630, ae_loss: 0.05582784\n",
      "Step: [1744] total_loss: 2.13398051 d_loss: 1.39178967, g_loss: 0.68550920, ae_loss: 0.05668158\n",
      "Step: [1745] total_loss: 2.11919355 d_loss: 1.37042820, g_loss: 0.69883996, ae_loss: 0.04992550\n",
      "Step: [1746] total_loss: 2.12474489 d_loss: 1.38643217, g_loss: 0.68856806, ae_loss: 0.04974466\n",
      "Step: [1747] total_loss: 2.12386894 d_loss: 1.37224746, g_loss: 0.70473111, ae_loss: 0.04689020\n",
      "Step: [1748] total_loss: 2.14781332 d_loss: 1.41085410, g_loss: 0.68320119, ae_loss: 0.05375810\n",
      "Step: [1749] total_loss: 2.12842536 d_loss: 1.38600695, g_loss: 0.69134641, ae_loss: 0.05107205\n",
      "Step: [1750] total_loss: 2.12233782 d_loss: 1.36418569, g_loss: 0.70616543, ae_loss: 0.05198655\n",
      "Step: [1751] total_loss: 2.13963127 d_loss: 1.38227141, g_loss: 0.70317483, ae_loss: 0.05418507\n",
      "Step: [1752] total_loss: 2.13863969 d_loss: 1.39001465, g_loss: 0.69872022, ae_loss: 0.04990488\n",
      "Step: [1753] total_loss: 2.13383770 d_loss: 1.37746084, g_loss: 0.70276666, ae_loss: 0.05361029\n",
      "Step: [1754] total_loss: 2.12930202 d_loss: 1.37367868, g_loss: 0.70468104, ae_loss: 0.05094229\n",
      "Step: [1755] total_loss: 2.12602592 d_loss: 1.38558590, g_loss: 0.68816006, ae_loss: 0.05228000\n",
      "Step: [1756] total_loss: 2.13712311 d_loss: 1.39731407, g_loss: 0.68511140, ae_loss: 0.05469771\n",
      "Step: [1757] total_loss: 2.11304855 d_loss: 1.37096310, g_loss: 0.69121236, ae_loss: 0.05087307\n",
      "Step: [1758] total_loss: 2.16032934 d_loss: 1.42128396, g_loss: 0.68740058, ae_loss: 0.05164490\n",
      "Step: [1759] total_loss: 2.11050558 d_loss: 1.37332225, g_loss: 0.68443435, ae_loss: 0.05274899\n",
      "Step: [1760] total_loss: 2.14480567 d_loss: 1.41180539, g_loss: 0.68006051, ae_loss: 0.05293975\n",
      "Step: [1761] total_loss: 2.14207077 d_loss: 1.39345407, g_loss: 0.69614053, ae_loss: 0.05247619\n",
      "Step: [1762] total_loss: 2.10700417 d_loss: 1.36004770, g_loss: 0.68671417, ae_loss: 0.06024221\n",
      "Step: [1763] total_loss: 2.13973546 d_loss: 1.39633501, g_loss: 0.68800646, ae_loss: 0.05539388\n",
      "Step: [1764] total_loss: 2.11912727 d_loss: 1.37692690, g_loss: 0.68569690, ae_loss: 0.05650341\n",
      "Step: [1765] total_loss: 2.12750769 d_loss: 1.38855052, g_loss: 0.68665665, ae_loss: 0.05230042\n",
      "Step: [1766] total_loss: 2.12334919 d_loss: 1.37540495, g_loss: 0.69529915, ae_loss: 0.05264505\n",
      "Step: [1767] total_loss: 2.14853239 d_loss: 1.38039112, g_loss: 0.71395183, ae_loss: 0.05418947\n",
      "Step: [1768] total_loss: 2.15024710 d_loss: 1.41113353, g_loss: 0.68910968, ae_loss: 0.05000404\n",
      "Step: [1769] total_loss: 2.12768722 d_loss: 1.38891745, g_loss: 0.68984252, ae_loss: 0.04892730\n",
      "Step: [1770] total_loss: 2.15444303 d_loss: 1.39862549, g_loss: 0.69784778, ae_loss: 0.05796973\n",
      "Step: [1771] total_loss: 2.10719252 d_loss: 1.37573171, g_loss: 0.68248391, ae_loss: 0.04897683\n",
      "Step: [1772] total_loss: 2.11984158 d_loss: 1.36922514, g_loss: 0.69760954, ae_loss: 0.05300687\n",
      "Step: [1773] total_loss: 2.11717272 d_loss: 1.37347031, g_loss: 0.68914402, ae_loss: 0.05455834\n",
      "Step: [1774] total_loss: 2.13488412 d_loss: 1.39155710, g_loss: 0.69129813, ae_loss: 0.05202895\n",
      "Step: [1775] total_loss: 2.13623428 d_loss: 1.38495708, g_loss: 0.69874465, ae_loss: 0.05253267\n",
      "Step: [1776] total_loss: 2.14599371 d_loss: 1.39842939, g_loss: 0.69400036, ae_loss: 0.05356398\n",
      "Step: [1777] total_loss: 2.15026903 d_loss: 1.40027380, g_loss: 0.69641542, ae_loss: 0.05357988\n",
      "Step: [1778] total_loss: 2.11187029 d_loss: 1.37677503, g_loss: 0.68343806, ae_loss: 0.05165721\n",
      "Step: [1779] total_loss: 2.12681270 d_loss: 1.36518919, g_loss: 0.71144140, ae_loss: 0.05018215\n",
      "Step: [1780] total_loss: 2.12159491 d_loss: 1.38687062, g_loss: 0.68107307, ae_loss: 0.05365138\n",
      "Step: [1781] total_loss: 2.13257933 d_loss: 1.39727235, g_loss: 0.68177783, ae_loss: 0.05352932\n",
      "Step: [1782] total_loss: 2.13273382 d_loss: 1.37797427, g_loss: 0.70570755, ae_loss: 0.04905198\n",
      "Step: [1783] total_loss: 2.11507559 d_loss: 1.38550222, g_loss: 0.67803323, ae_loss: 0.05154007\n",
      "Step: [1784] total_loss: 2.12941957 d_loss: 1.37247908, g_loss: 0.70817292, ae_loss: 0.04876756\n",
      "Step: [1785] total_loss: 2.13657355 d_loss: 1.36417770, g_loss: 0.72085923, ae_loss: 0.05153661\n",
      "Step: [1786] total_loss: 2.13259768 d_loss: 1.36933684, g_loss: 0.71166658, ae_loss: 0.05159421\n",
      "Step: [1787] total_loss: 2.10688138 d_loss: 1.37284112, g_loss: 0.68166298, ae_loss: 0.05237723\n",
      "Step: [1788] total_loss: 2.13603878 d_loss: 1.40183723, g_loss: 0.67973518, ae_loss: 0.05446622\n",
      "Step: [1789] total_loss: 2.11755753 d_loss: 1.37644887, g_loss: 0.68998909, ae_loss: 0.05111949\n",
      "Step: [1790] total_loss: 2.13779449 d_loss: 1.38163936, g_loss: 0.70650703, ae_loss: 0.04964808\n",
      "Step: [1791] total_loss: 2.13661242 d_loss: 1.36940336, g_loss: 0.71587706, ae_loss: 0.05133194\n",
      "Step: [1792] total_loss: 2.12800026 d_loss: 1.38450313, g_loss: 0.69348860, ae_loss: 0.05000848\n",
      "Step: [1793] total_loss: 2.12202716 d_loss: 1.38072681, g_loss: 0.68432719, ae_loss: 0.05697326\n",
      "Step: [1794] total_loss: 2.14186263 d_loss: 1.40147257, g_loss: 0.68809861, ae_loss: 0.05229147\n",
      "Step: [1795] total_loss: 2.13733149 d_loss: 1.39489532, g_loss: 0.69205642, ae_loss: 0.05037965\n",
      "Step: [1796] total_loss: 2.11985922 d_loss: 1.37334609, g_loss: 0.69381648, ae_loss: 0.05269669\n",
      "Step: [1797] total_loss: 2.13865185 d_loss: 1.39129078, g_loss: 0.69637835, ae_loss: 0.05098289\n",
      "Step: [1798] total_loss: 2.10937142 d_loss: 1.38260102, g_loss: 0.67879128, ae_loss: 0.04797914\n",
      "Step: [1799] total_loss: 2.13615656 d_loss: 1.37164545, g_loss: 0.71334809, ae_loss: 0.05116309\n",
      "Step: [1800] total_loss: 2.12554908 d_loss: 1.37218237, g_loss: 0.70062524, ae_loss: 0.05274142\n",
      "Step: [1801] total_loss: 2.12959051 d_loss: 1.39331388, g_loss: 0.67858881, ae_loss: 0.05768790\n",
      "Step: [1802] total_loss: 2.16420722 d_loss: 1.39632440, g_loss: 0.71868753, ae_loss: 0.04919530\n",
      "Step: [1803] total_loss: 2.16120362 d_loss: 1.41380250, g_loss: 0.69699609, ae_loss: 0.05040500\n",
      "Step: [1804] total_loss: 2.15642715 d_loss: 1.40309334, g_loss: 0.70148396, ae_loss: 0.05184981\n",
      "Step: [1805] total_loss: 2.15653253 d_loss: 1.39335060, g_loss: 0.71113932, ae_loss: 0.05204258\n",
      "Step: [1806] total_loss: 2.11566496 d_loss: 1.36967432, g_loss: 0.69324768, ae_loss: 0.05274294\n",
      "Step: [1807] total_loss: 2.12587118 d_loss: 1.40568256, g_loss: 0.66208386, ae_loss: 0.05810488\n",
      "Step: [1808] total_loss: 2.12158203 d_loss: 1.35207677, g_loss: 0.71811438, ae_loss: 0.05139097\n",
      "Step: [1809] total_loss: 2.11547136 d_loss: 1.36830735, g_loss: 0.69857168, ae_loss: 0.04859217\n",
      "Step: [1810] total_loss: 2.12697053 d_loss: 1.38286877, g_loss: 0.69174528, ae_loss: 0.05235650\n",
      "Step: [1811] total_loss: 2.13089085 d_loss: 1.37959957, g_loss: 0.69983464, ae_loss: 0.05145669\n",
      "Step: [1812] total_loss: 2.13819718 d_loss: 1.36748791, g_loss: 0.71881658, ae_loss: 0.05189266\n",
      "Step: [1813] total_loss: 2.15825248 d_loss: 1.38836360, g_loss: 0.71532267, ae_loss: 0.05456613\n",
      "Step: [1814] total_loss: 2.14922714 d_loss: 1.37986624, g_loss: 0.71509635, ae_loss: 0.05426453\n",
      "Step: [1815] total_loss: 2.15241742 d_loss: 1.40704119, g_loss: 0.69043636, ae_loss: 0.05493985\n",
      "Step: [1816] total_loss: 2.14133525 d_loss: 1.37691629, g_loss: 0.71156031, ae_loss: 0.05285862\n",
      "Step: [1817] total_loss: 2.14414477 d_loss: 1.39156938, g_loss: 0.69712818, ae_loss: 0.05544717\n",
      "Step: [1818] total_loss: 2.15236998 d_loss: 1.41251636, g_loss: 0.68663538, ae_loss: 0.05321813\n",
      "Step: [1819] total_loss: 2.12890959 d_loss: 1.38654459, g_loss: 0.68977857, ae_loss: 0.05258648\n",
      "Step: [1820] total_loss: 2.14938545 d_loss: 1.40141213, g_loss: 0.69489318, ae_loss: 0.05308023\n",
      "Step: [1821] total_loss: 2.14368677 d_loss: 1.38150668, g_loss: 0.70709550, ae_loss: 0.05508441\n",
      "Step: [1822] total_loss: 2.13765812 d_loss: 1.39650083, g_loss: 0.69167000, ae_loss: 0.04948733\n",
      "Step: [1823] total_loss: 2.13439989 d_loss: 1.38902962, g_loss: 0.69256026, ae_loss: 0.05280993\n",
      "Step: [1824] total_loss: 2.12831092 d_loss: 1.37655187, g_loss: 0.70138639, ae_loss: 0.05037256\n",
      "Step: [1825] total_loss: 2.11837959 d_loss: 1.36968708, g_loss: 0.69271255, ae_loss: 0.05597996\n",
      "Step: [1826] total_loss: 2.10297537 d_loss: 1.36823034, g_loss: 0.68434358, ae_loss: 0.05040135\n",
      "Step: [1827] total_loss: 2.11539984 d_loss: 1.37896240, g_loss: 0.68583596, ae_loss: 0.05060146\n",
      "Step: [1828] total_loss: 2.12771988 d_loss: 1.37860417, g_loss: 0.69758511, ae_loss: 0.05153048\n",
      "Step: [1829] total_loss: 2.11309981 d_loss: 1.36573613, g_loss: 0.69611579, ae_loss: 0.05124785\n",
      "Step: [1830] total_loss: 2.13804889 d_loss: 1.39331675, g_loss: 0.69187802, ae_loss: 0.05285406\n",
      "Step: [1831] total_loss: 2.13569069 d_loss: 1.39137220, g_loss: 0.69434404, ae_loss: 0.04997447\n",
      "Step: [1832] total_loss: 2.13475394 d_loss: 1.39115870, g_loss: 0.69178367, ae_loss: 0.05181162\n",
      "Step: [1833] total_loss: 2.13193226 d_loss: 1.38697171, g_loss: 0.69271201, ae_loss: 0.05224865\n",
      "Step: [1834] total_loss: 2.10510969 d_loss: 1.36668515, g_loss: 0.68724942, ae_loss: 0.05117495\n",
      "Step: [1835] total_loss: 2.12303782 d_loss: 1.38671041, g_loss: 0.68479902, ae_loss: 0.05152834\n",
      "Step: [1836] total_loss: 2.11750627 d_loss: 1.37848639, g_loss: 0.68626261, ae_loss: 0.05275729\n",
      "Step: [1837] total_loss: 2.12652850 d_loss: 1.37893438, g_loss: 0.69859838, ae_loss: 0.04899579\n",
      "Step: [1838] total_loss: 2.11264896 d_loss: 1.36651552, g_loss: 0.69359672, ae_loss: 0.05253682\n",
      "Step: [1839] total_loss: 2.12753010 d_loss: 1.38683558, g_loss: 0.69011927, ae_loss: 0.05057535\n",
      "Step: [1840] total_loss: 2.14074087 d_loss: 1.37654531, g_loss: 0.71276975, ae_loss: 0.05142573\n",
      "Step: [1841] total_loss: 2.14279747 d_loss: 1.37325549, g_loss: 0.71488082, ae_loss: 0.05466126\n",
      "Step: [1842] total_loss: 2.14369059 d_loss: 1.39500356, g_loss: 0.69596207, ae_loss: 0.05272479\n",
      "Step: [1843] total_loss: 2.16304159 d_loss: 1.40763307, g_loss: 0.70125163, ae_loss: 0.05415686\n",
      "Step: [1844] total_loss: 2.14252520 d_loss: 1.38985789, g_loss: 0.70163977, ae_loss: 0.05102739\n",
      "Step: [1845] total_loss: 2.13827372 d_loss: 1.39065373, g_loss: 0.69468796, ae_loss: 0.05293215\n",
      "Step: [1846] total_loss: 2.13717699 d_loss: 1.39737666, g_loss: 0.68565619, ae_loss: 0.05414413\n",
      "Step: [1847] total_loss: 2.11499882 d_loss: 1.37422752, g_loss: 0.69030315, ae_loss: 0.05046815\n",
      "Step: [1848] total_loss: 2.11161828 d_loss: 1.36553299, g_loss: 0.69610840, ae_loss: 0.04997698\n",
      "Step: [1849] total_loss: 2.13404727 d_loss: 1.39141655, g_loss: 0.69253498, ae_loss: 0.05009563\n",
      "Step: [1850] total_loss: 2.12807989 d_loss: 1.38904667, g_loss: 0.68867719, ae_loss: 0.05035597\n",
      "Step: [1851] total_loss: 2.15566897 d_loss: 1.39084411, g_loss: 0.71200794, ae_loss: 0.05281681\n",
      "Step: [1852] total_loss: 2.13427424 d_loss: 1.38763833, g_loss: 0.69552636, ae_loss: 0.05110954\n",
      "Step: [1853] total_loss: 2.13365293 d_loss: 1.37203014, g_loss: 0.70811272, ae_loss: 0.05351011\n",
      "Step: [1854] total_loss: 2.12484431 d_loss: 1.35812259, g_loss: 0.71429241, ae_loss: 0.05242931\n",
      "Step: [1855] total_loss: 2.14392710 d_loss: 1.38802624, g_loss: 0.70143473, ae_loss: 0.05446602\n",
      "Step: [1856] total_loss: 2.12853241 d_loss: 1.37694919, g_loss: 0.69748187, ae_loss: 0.05410126\n",
      "Step: [1857] total_loss: 2.11139917 d_loss: 1.38872814, g_loss: 0.67324299, ae_loss: 0.04942812\n",
      "Step: [1858] total_loss: 2.11479688 d_loss: 1.38477433, g_loss: 0.67315942, ae_loss: 0.05686312\n",
      "Step: [1859] total_loss: 2.10431528 d_loss: 1.38483667, g_loss: 0.66995370, ae_loss: 0.04952474\n",
      "Step: [1860] total_loss: 2.12458038 d_loss: 1.39217556, g_loss: 0.67791051, ae_loss: 0.05449424\n",
      "Step: [1861] total_loss: 2.10408473 d_loss: 1.36990452, g_loss: 0.68227613, ae_loss: 0.05190409\n",
      "Step: [1862] total_loss: 2.13579702 d_loss: 1.39788270, g_loss: 0.68450832, ae_loss: 0.05340595\n",
      "Step: [1863] total_loss: 2.13694572 d_loss: 1.38922465, g_loss: 0.69480753, ae_loss: 0.05291358\n",
      "Step: [1864] total_loss: 2.15865040 d_loss: 1.41411662, g_loss: 0.69151986, ae_loss: 0.05301380\n",
      "Step: [1865] total_loss: 2.14348221 d_loss: 1.37234187, g_loss: 0.71908987, ae_loss: 0.05205052\n",
      "Step: [1866] total_loss: 2.13448238 d_loss: 1.37318945, g_loss: 0.70934522, ae_loss: 0.05194758\n",
      "Step: [1867] total_loss: 2.13885736 d_loss: 1.37181115, g_loss: 0.71502507, ae_loss: 0.05202106\n",
      "Step: [1868] total_loss: 2.12906861 d_loss: 1.38009167, g_loss: 0.69375831, ae_loss: 0.05521876\n",
      "Step: [1869] total_loss: 2.12432647 d_loss: 1.38707268, g_loss: 0.68685263, ae_loss: 0.05040115\n",
      "Step: [1870] total_loss: 2.11932111 d_loss: 1.39242029, g_loss: 0.67350042, ae_loss: 0.05340039\n",
      "Step: [1871] total_loss: 2.12465763 d_loss: 1.39147973, g_loss: 0.68297982, ae_loss: 0.05019800\n",
      "Step: [1872] total_loss: 2.11159587 d_loss: 1.36998987, g_loss: 0.68705744, ae_loss: 0.05454849\n",
      "Step: [1873] total_loss: 2.10818958 d_loss: 1.37711811, g_loss: 0.67984670, ae_loss: 0.05122466\n",
      "Step: [1874] total_loss: 2.13424206 d_loss: 1.35784674, g_loss: 0.72639751, ae_loss: 0.04999771\n",
      "Step: [1875] total_loss: 2.14030194 d_loss: 1.40106976, g_loss: 0.69276345, ae_loss: 0.04646870\n",
      "Step: [1876] total_loss: 2.13408303 d_loss: 1.37764645, g_loss: 0.69716251, ae_loss: 0.05927405\n",
      "Step: [1877] total_loss: 2.15798497 d_loss: 1.41413975, g_loss: 0.69371104, ae_loss: 0.05013424\n",
      "Step: [1878] total_loss: 2.12713027 d_loss: 1.37550914, g_loss: 0.69980657, ae_loss: 0.05181457\n",
      "Step: [1879] total_loss: 2.10773253 d_loss: 1.36888790, g_loss: 0.68601865, ae_loss: 0.05282596\n",
      "Step: [1880] total_loss: 2.11733580 d_loss: 1.38610387, g_loss: 0.67944181, ae_loss: 0.05179027\n",
      "Step: [1881] total_loss: 2.11742330 d_loss: 1.38514650, g_loss: 0.68141741, ae_loss: 0.05085936\n",
      "Step: [1882] total_loss: 2.13177299 d_loss: 1.39807773, g_loss: 0.68018830, ae_loss: 0.05350689\n",
      "Step: [1883] total_loss: 2.14139223 d_loss: 1.35952377, g_loss: 0.72780240, ae_loss: 0.05406623\n",
      "Step: [1884] total_loss: 2.15392685 d_loss: 1.39919436, g_loss: 0.69899809, ae_loss: 0.05573424\n",
      "Step: [1885] total_loss: 2.14713669 d_loss: 1.37304425, g_loss: 0.72124064, ae_loss: 0.05285188\n",
      "Step: [1886] total_loss: 2.13858128 d_loss: 1.38409173, g_loss: 0.70250750, ae_loss: 0.05198196\n",
      "Step: [1887] total_loss: 2.13243937 d_loss: 1.40809762, g_loss: 0.67151362, ae_loss: 0.05282805\n",
      "Step: [1888] total_loss: 2.13570118 d_loss: 1.38606787, g_loss: 0.70018089, ae_loss: 0.04945242\n",
      "Step: [1889] total_loss: 2.12618160 d_loss: 1.37971258, g_loss: 0.69344568, ae_loss: 0.05302335\n",
      "Step: [1890] total_loss: 2.14011574 d_loss: 1.38739586, g_loss: 0.69959527, ae_loss: 0.05312467\n",
      "Step: [1891] total_loss: 2.12034035 d_loss: 1.37887335, g_loss: 0.68761230, ae_loss: 0.05385480\n",
      "Step: [1892] total_loss: 2.13716173 d_loss: 1.37928414, g_loss: 0.70482051, ae_loss: 0.05305694\n",
      "Step: [1893] total_loss: 2.13646150 d_loss: 1.39172494, g_loss: 0.69458616, ae_loss: 0.05015036\n",
      "Step: [1894] total_loss: 2.10518813 d_loss: 1.37077236, g_loss: 0.68515539, ae_loss: 0.04926033\n",
      "Step: [1895] total_loss: 2.12367296 d_loss: 1.38267565, g_loss: 0.69073164, ae_loss: 0.05026550\n",
      "Step: [1896] total_loss: 2.13628721 d_loss: 1.39253104, g_loss: 0.69080162, ae_loss: 0.05295440\n",
      "Step: [1897] total_loss: 2.12911820 d_loss: 1.37816370, g_loss: 0.69500721, ae_loss: 0.05594733\n",
      "Step: [1898] total_loss: 2.12199020 d_loss: 1.38019168, g_loss: 0.68952346, ae_loss: 0.05227509\n",
      "Step: [1899] total_loss: 2.11094809 d_loss: 1.36161482, g_loss: 0.69836622, ae_loss: 0.05096700\n",
      "Step: [1900] total_loss: 2.15592384 d_loss: 1.40612555, g_loss: 0.69416380, ae_loss: 0.05563451\n",
      "Step: [1901] total_loss: 2.14982200 d_loss: 1.38741207, g_loss: 0.70919359, ae_loss: 0.05321628\n",
      "Step: [1902] total_loss: 2.12878132 d_loss: 1.37398350, g_loss: 0.70511568, ae_loss: 0.04968200\n",
      "Step: [1903] total_loss: 2.12728548 d_loss: 1.36827588, g_loss: 0.70999283, ae_loss: 0.04901682\n",
      "Step: [1904] total_loss: 2.11310720 d_loss: 1.37578201, g_loss: 0.68680304, ae_loss: 0.05052217\n",
      "Step: [1905] total_loss: 2.11355710 d_loss: 1.36761117, g_loss: 0.69505614, ae_loss: 0.05088989\n",
      "Step: [1906] total_loss: 2.12857461 d_loss: 1.39825165, g_loss: 0.67800981, ae_loss: 0.05231323\n",
      "Step: [1907] total_loss: 2.13108730 d_loss: 1.39557910, g_loss: 0.68106496, ae_loss: 0.05444327\n",
      "Step: [1908] total_loss: 2.11729240 d_loss: 1.37119460, g_loss: 0.69202065, ae_loss: 0.05407705\n",
      "Step: [1909] total_loss: 2.11754966 d_loss: 1.38084877, g_loss: 0.68013805, ae_loss: 0.05656280\n",
      "Step: [1910] total_loss: 2.13025999 d_loss: 1.38311338, g_loss: 0.69694912, ae_loss: 0.05019749\n",
      "Step: [1911] total_loss: 2.11885262 d_loss: 1.37708688, g_loss: 0.68884110, ae_loss: 0.05292476\n",
      "Step: [1912] total_loss: 2.12948537 d_loss: 1.38629401, g_loss: 0.69451559, ae_loss: 0.04867576\n",
      "Step: [1913] total_loss: 2.11296654 d_loss: 1.37209046, g_loss: 0.69127560, ae_loss: 0.04960041\n",
      "Step: [1914] total_loss: 2.11585641 d_loss: 1.38337016, g_loss: 0.68468058, ae_loss: 0.04780572\n",
      "Step: [1915] total_loss: 2.13725042 d_loss: 1.40293074, g_loss: 0.68508130, ae_loss: 0.04923830\n",
      "Step: [1916] total_loss: 2.14822698 d_loss: 1.38500762, g_loss: 0.71084821, ae_loss: 0.05237110\n",
      "Step: [1917] total_loss: 2.13952208 d_loss: 1.39371014, g_loss: 0.69332039, ae_loss: 0.05249138\n",
      "Step: [1918] total_loss: 2.12038946 d_loss: 1.38034415, g_loss: 0.68410987, ae_loss: 0.05593540\n",
      "Step: [1919] total_loss: 2.14216638 d_loss: 1.39542508, g_loss: 0.69599169, ae_loss: 0.05074966\n",
      "Step: [1920] total_loss: 2.13962913 d_loss: 1.39178371, g_loss: 0.69543159, ae_loss: 0.05241387\n",
      "Step: [1921] total_loss: 2.14353418 d_loss: 1.38068724, g_loss: 0.71032119, ae_loss: 0.05252571\n",
      "Step: [1922] total_loss: 2.14193463 d_loss: 1.39892185, g_loss: 0.69037998, ae_loss: 0.05263280\n",
      "Step: [1923] total_loss: 2.14192629 d_loss: 1.39122272, g_loss: 0.69724524, ae_loss: 0.05345832\n",
      "Step: [1924] total_loss: 2.12190819 d_loss: 1.38248181, g_loss: 0.68879747, ae_loss: 0.05062906\n",
      "Step: [1925] total_loss: 2.12517667 d_loss: 1.38288844, g_loss: 0.69140863, ae_loss: 0.05087964\n",
      "Step: [1926] total_loss: 2.11683989 d_loss: 1.37058139, g_loss: 0.69369578, ae_loss: 0.05256255\n",
      "Step: [1927] total_loss: 2.12121153 d_loss: 1.36640668, g_loss: 0.69744009, ae_loss: 0.05736470\n",
      "Step: [1928] total_loss: 2.16831446 d_loss: 1.39364541, g_loss: 0.71750045, ae_loss: 0.05716870\n",
      "Step: [1929] total_loss: 2.12920761 d_loss: 1.38363481, g_loss: 0.68872529, ae_loss: 0.05684757\n",
      "Step: [1930] total_loss: 2.14086246 d_loss: 1.39262366, g_loss: 0.69304001, ae_loss: 0.05519895\n",
      "Step: [1931] total_loss: 2.14781427 d_loss: 1.39823568, g_loss: 0.69486922, ae_loss: 0.05470926\n",
      "Step: [1932] total_loss: 2.16125202 d_loss: 1.39132786, g_loss: 0.71493322, ae_loss: 0.05499100\n",
      "Step: [1933] total_loss: 2.14665270 d_loss: 1.40687633, g_loss: 0.68274713, ae_loss: 0.05702918\n",
      "Step: [1934] total_loss: 2.13839102 d_loss: 1.38371837, g_loss: 0.70004380, ae_loss: 0.05462871\n",
      "Step: [1935] total_loss: 2.13988137 d_loss: 1.38750136, g_loss: 0.69812387, ae_loss: 0.05425620\n",
      "Step: [1936] total_loss: 2.12770414 d_loss: 1.37598896, g_loss: 0.69780958, ae_loss: 0.05390558\n",
      "Step: [1937] total_loss: 2.12157798 d_loss: 1.37801397, g_loss: 0.69436234, ae_loss: 0.04920166\n",
      "Step: [1938] total_loss: 2.12447476 d_loss: 1.38559723, g_loss: 0.68514508, ae_loss: 0.05373236\n",
      "Step: [1939] total_loss: 2.12417746 d_loss: 1.38646960, g_loss: 0.68640685, ae_loss: 0.05130086\n",
      "Step: [1940] total_loss: 2.10549402 d_loss: 1.36343241, g_loss: 0.69044226, ae_loss: 0.05161943\n",
      "Step: [1941] total_loss: 2.10903931 d_loss: 1.38303947, g_loss: 0.67353916, ae_loss: 0.05246085\n",
      "Step: [1942] total_loss: 2.12530470 d_loss: 1.38706434, g_loss: 0.68709475, ae_loss: 0.05114552\n",
      "Step: [1943] total_loss: 2.13281703 d_loss: 1.38695765, g_loss: 0.69314659, ae_loss: 0.05271279\n",
      "Step: [1944] total_loss: 2.11434412 d_loss: 1.37173295, g_loss: 0.68882668, ae_loss: 0.05378439\n",
      "Step: [1945] total_loss: 2.13311720 d_loss: 1.39166331, g_loss: 0.68968737, ae_loss: 0.05176637\n",
      "Step: [1946] total_loss: 2.14359760 d_loss: 1.38899159, g_loss: 0.70100546, ae_loss: 0.05360065\n",
      "Step: [1947] total_loss: 2.13423157 d_loss: 1.38004911, g_loss: 0.69958866, ae_loss: 0.05459395\n",
      "Step: [1948] total_loss: 2.14848781 d_loss: 1.39117157, g_loss: 0.70792431, ae_loss: 0.04939181\n",
      "Step: [1949] total_loss: 2.11905432 d_loss: 1.37381053, g_loss: 0.69312078, ae_loss: 0.05212302\n",
      "Step: [1950] total_loss: 2.10341787 d_loss: 1.36924481, g_loss: 0.68221450, ae_loss: 0.05195859\n",
      "Step: [1951] total_loss: 2.12921619 d_loss: 1.38617778, g_loss: 0.68792325, ae_loss: 0.05511527\n",
      "Step: [1952] total_loss: 2.12258768 d_loss: 1.38954878, g_loss: 0.68122375, ae_loss: 0.05181529\n",
      "Step: [1953] total_loss: 2.12226391 d_loss: 1.39158750, g_loss: 0.67990106, ae_loss: 0.05077537\n",
      "Step: [1954] total_loss: 2.11167336 d_loss: 1.37891197, g_loss: 0.68008912, ae_loss: 0.05267216\n",
      "Step: [1955] total_loss: 2.12187958 d_loss: 1.37041330, g_loss: 0.69800627, ae_loss: 0.05346010\n",
      "Step: [1956] total_loss: 2.12622738 d_loss: 1.37134910, g_loss: 0.70198363, ae_loss: 0.05289457\n",
      "Step: [1957] total_loss: 2.13464499 d_loss: 1.38070071, g_loss: 0.69975871, ae_loss: 0.05418566\n",
      "Step: [1958] total_loss: 2.13109541 d_loss: 1.37902975, g_loss: 0.69700485, ae_loss: 0.05506069\n",
      "Step: [1959] total_loss: 2.12539458 d_loss: 1.38089752, g_loss: 0.69340825, ae_loss: 0.05108885\n",
      "Step: [1960] total_loss: 2.11936641 d_loss: 1.37242651, g_loss: 0.69603974, ae_loss: 0.05090015\n",
      "Step: [1961] total_loss: 2.14945889 d_loss: 1.39299965, g_loss: 0.70026708, ae_loss: 0.05619203\n",
      "Step: [1962] total_loss: 2.14245319 d_loss: 1.39666772, g_loss: 0.69308192, ae_loss: 0.05270346\n",
      "Step: [1963] total_loss: 2.14202738 d_loss: 1.39734054, g_loss: 0.69192863, ae_loss: 0.05275810\n",
      "Step: [1964] total_loss: 2.12230778 d_loss: 1.38044453, g_loss: 0.69067848, ae_loss: 0.05118464\n",
      "Step: [1965] total_loss: 2.12704849 d_loss: 1.38693857, g_loss: 0.68601120, ae_loss: 0.05409855\n",
      "Step: [1966] total_loss: 2.13134980 d_loss: 1.38004673, g_loss: 0.69849002, ae_loss: 0.05281311\n",
      "Step: [1967] total_loss: 2.12087870 d_loss: 1.36416197, g_loss: 0.70596194, ae_loss: 0.05075480\n",
      "Step: [1968] total_loss: 2.10880470 d_loss: 1.37503517, g_loss: 0.67978138, ae_loss: 0.05398823\n",
      "Step: [1969] total_loss: 2.14727330 d_loss: 1.40256286, g_loss: 0.69294828, ae_loss: 0.05176211\n",
      "Step: [1970] total_loss: 2.13593698 d_loss: 1.38497162, g_loss: 0.70165622, ae_loss: 0.04930916\n",
      "Step: [1971] total_loss: 2.12591410 d_loss: 1.38299394, g_loss: 0.68975294, ae_loss: 0.05316726\n",
      "Step: [1972] total_loss: 2.14396262 d_loss: 1.38076210, g_loss: 0.70781785, ae_loss: 0.05538265\n",
      "Step: [1973] total_loss: 2.13374424 d_loss: 1.39362276, g_loss: 0.68605834, ae_loss: 0.05406326\n",
      "Step: [1974] total_loss: 2.14063072 d_loss: 1.39970624, g_loss: 0.68373406, ae_loss: 0.05719032\n",
      "Step: [1975] total_loss: 2.11365557 d_loss: 1.36818862, g_loss: 0.69842786, ae_loss: 0.04703900\n",
      "Step: [1976] total_loss: 2.12012815 d_loss: 1.39059758, g_loss: 0.67480499, ae_loss: 0.05472549\n",
      "Step: [1977] total_loss: 2.13089800 d_loss: 1.39419794, g_loss: 0.68511719, ae_loss: 0.05158280\n",
      "Step: [1978] total_loss: 2.10767460 d_loss: 1.38158584, g_loss: 0.67185146, ae_loss: 0.05423721\n",
      "Step: [1979] total_loss: 2.14331961 d_loss: 1.40135288, g_loss: 0.68816233, ae_loss: 0.05380453\n",
      "Step: [1980] total_loss: 2.12528872 d_loss: 1.37761045, g_loss: 0.69576854, ae_loss: 0.05190982\n",
      "Step: [1981] total_loss: 2.12152576 d_loss: 1.37380660, g_loss: 0.69459534, ae_loss: 0.05312390\n",
      "Step: [1982] total_loss: 2.09727693 d_loss: 1.35801542, g_loss: 0.69066155, ae_loss: 0.04859992\n",
      "Step: [1983] total_loss: 2.12110686 d_loss: 1.37902939, g_loss: 0.69160724, ae_loss: 0.05047029\n",
      "Step: [1984] total_loss: 2.11619663 d_loss: 1.38180053, g_loss: 0.68480080, ae_loss: 0.04959521\n",
      "Step: [1985] total_loss: 2.15470123 d_loss: 1.39381957, g_loss: 0.71001691, ae_loss: 0.05086476\n",
      "Step: [1986] total_loss: 2.16100550 d_loss: 1.41804266, g_loss: 0.69060481, ae_loss: 0.05235812\n",
      "Step: [1987] total_loss: 2.13540721 d_loss: 1.36851430, g_loss: 0.71748072, ae_loss: 0.04941212\n",
      "Step: [1988] total_loss: 2.13626981 d_loss: 1.38176990, g_loss: 0.70056683, ae_loss: 0.05393303\n",
      "Step: [1989] total_loss: 2.14767957 d_loss: 1.39225936, g_loss: 0.70070392, ae_loss: 0.05471636\n",
      "Step: [1990] total_loss: 2.13851261 d_loss: 1.38216841, g_loss: 0.70774150, ae_loss: 0.04860257\n",
      "Step: [1991] total_loss: 2.13716054 d_loss: 1.39848173, g_loss: 0.68865037, ae_loss: 0.05002842\n",
      "Step: [1992] total_loss: 2.15328217 d_loss: 1.37730026, g_loss: 0.72061670, ae_loss: 0.05536521\n",
      "Step: [1993] total_loss: 2.11676836 d_loss: 1.38021779, g_loss: 0.68292236, ae_loss: 0.05362808\n",
      "Step: [1994] total_loss: 2.14483547 d_loss: 1.40240908, g_loss: 0.69096172, ae_loss: 0.05146460\n",
      "Step: [1995] total_loss: 2.12893724 d_loss: 1.37736917, g_loss: 0.69508129, ae_loss: 0.05648670\n",
      "Step: [1996] total_loss: 2.14639950 d_loss: 1.38791180, g_loss: 0.70578623, ae_loss: 0.05270137\n",
      "Step: [1997] total_loss: 2.15620089 d_loss: 1.39957380, g_loss: 0.70441580, ae_loss: 0.05221135\n",
      "Step: [1998] total_loss: 2.13803482 d_loss: 1.38970268, g_loss: 0.69589996, ae_loss: 0.05243228\n",
      "Step: [1999] total_loss: 2.13379574 d_loss: 1.39641285, g_loss: 0.68821591, ae_loss: 0.04916703\n",
      "Step: [2000] total_loss: 2.12738967 d_loss: 1.38680625, g_loss: 0.68709546, ae_loss: 0.05348786\n",
      "Step: [2001] total_loss: 2.13963032 d_loss: 1.39095354, g_loss: 0.69933695, ae_loss: 0.04933994\n",
      "Step: [2002] total_loss: 2.13714004 d_loss: 1.37372017, g_loss: 0.71035999, ae_loss: 0.05305983\n",
      "Step: [2003] total_loss: 2.11861277 d_loss: 1.36794400, g_loss: 0.69823998, ae_loss: 0.05242886\n",
      "Step: [2004] total_loss: 2.11966085 d_loss: 1.39075053, g_loss: 0.68080097, ae_loss: 0.04810928\n",
      "Step: [2005] total_loss: 2.12542677 d_loss: 1.37649429, g_loss: 0.70041084, ae_loss: 0.04852160\n",
      "Step: [2006] total_loss: 2.13703346 d_loss: 1.39758265, g_loss: 0.68770659, ae_loss: 0.05174417\n",
      "Step: [2007] total_loss: 2.10102773 d_loss: 1.35342908, g_loss: 0.69969380, ae_loss: 0.04790479\n",
      "Step: [2008] total_loss: 2.10690808 d_loss: 1.37096798, g_loss: 0.68280733, ae_loss: 0.05313272\n",
      "Step: [2009] total_loss: 2.10805273 d_loss: 1.37875295, g_loss: 0.67370808, ae_loss: 0.05559178\n",
      "Step: [2010] total_loss: 2.12755251 d_loss: 1.39139140, g_loss: 0.68612677, ae_loss: 0.05003439\n",
      "Step: [2011] total_loss: 2.13251925 d_loss: 1.39722490, g_loss: 0.68654716, ae_loss: 0.04874710\n",
      "Step: [2012] total_loss: 2.12068486 d_loss: 1.37214577, g_loss: 0.69624954, ae_loss: 0.05228949\n",
      "Step: [2013] total_loss: 2.11696672 d_loss: 1.36133170, g_loss: 0.70369709, ae_loss: 0.05193792\n",
      "Step: [2014] total_loss: 2.14413595 d_loss: 1.38962233, g_loss: 0.70251971, ae_loss: 0.05199399\n",
      "Step: [2015] total_loss: 2.11777234 d_loss: 1.37021673, g_loss: 0.69497633, ae_loss: 0.05257931\n",
      "Step: [2016] total_loss: 2.12642550 d_loss: 1.37650156, g_loss: 0.69345242, ae_loss: 0.05647161\n",
      "Step: [2017] total_loss: 2.13032484 d_loss: 1.39440703, g_loss: 0.68035328, ae_loss: 0.05556441\n",
      "Step: [2018] total_loss: 2.12278414 d_loss: 1.38699222, g_loss: 0.68090451, ae_loss: 0.05488740\n",
      "Step: [2019] total_loss: 2.11669779 d_loss: 1.38256061, g_loss: 0.68725216, ae_loss: 0.04688492\n",
      "Step: [2020] total_loss: 2.12745237 d_loss: 1.38517487, g_loss: 0.68919301, ae_loss: 0.05308462\n",
      "Step: [2021] total_loss: 2.12842488 d_loss: 1.39245653, g_loss: 0.67918569, ae_loss: 0.05678266\n",
      "Step: [2022] total_loss: 2.12499189 d_loss: 1.39055920, g_loss: 0.67892802, ae_loss: 0.05550484\n",
      "Step: [2023] total_loss: 2.12593269 d_loss: 1.39622116, g_loss: 0.67940986, ae_loss: 0.05030156\n",
      "Step: [2024] total_loss: 2.14380884 d_loss: 1.40114284, g_loss: 0.68706715, ae_loss: 0.05559874\n",
      "Step: [2025] total_loss: 2.13678074 d_loss: 1.39242864, g_loss: 0.69599676, ae_loss: 0.04835531\n",
      "Step: [2026] total_loss: 2.12369442 d_loss: 1.36903250, g_loss: 0.70158195, ae_loss: 0.05307997\n",
      "Step: [2027] total_loss: 2.12997341 d_loss: 1.38423395, g_loss: 0.69405413, ae_loss: 0.05168547\n",
      "Step: [2028] total_loss: 2.13547039 d_loss: 1.39176345, g_loss: 0.68923390, ae_loss: 0.05447319\n",
      "Step: [2029] total_loss: 2.13326979 d_loss: 1.39035714, g_loss: 0.69405663, ae_loss: 0.04885619\n",
      "Step: [2030] total_loss: 2.13005209 d_loss: 1.38331676, g_loss: 0.69578797, ae_loss: 0.05094728\n",
      "Step: [2031] total_loss: 2.11421347 d_loss: 1.37486267, g_loss: 0.68746871, ae_loss: 0.05188208\n",
      "Step: [2032] total_loss: 2.12946939 d_loss: 1.38959146, g_loss: 0.68572718, ae_loss: 0.05415085\n",
      "Step: [2033] total_loss: 2.12788248 d_loss: 1.38796747, g_loss: 0.68443006, ae_loss: 0.05548504\n",
      "Step: [2034] total_loss: 2.13434219 d_loss: 1.39603627, g_loss: 0.68412638, ae_loss: 0.05417945\n",
      "Step: [2035] total_loss: 2.13700557 d_loss: 1.39822197, g_loss: 0.68713230, ae_loss: 0.05165124\n",
      "Step: [2036] total_loss: 2.13412762 d_loss: 1.38407326, g_loss: 0.69871932, ae_loss: 0.05133500\n",
      "Step: [2037] total_loss: 2.13588428 d_loss: 1.39163220, g_loss: 0.69275284, ae_loss: 0.05149926\n",
      "Step: [2038] total_loss: 2.14460731 d_loss: 1.39253521, g_loss: 0.70003724, ae_loss: 0.05203482\n",
      "Step: [2039] total_loss: 2.12918448 d_loss: 1.37558246, g_loss: 0.70175332, ae_loss: 0.05184881\n",
      "Step: [2040] total_loss: 2.15489674 d_loss: 1.41066086, g_loss: 0.69184864, ae_loss: 0.05238712\n",
      "Step: [2041] total_loss: 2.13009119 d_loss: 1.38224280, g_loss: 0.69764388, ae_loss: 0.05020456\n",
      "Step: [2042] total_loss: 2.11036396 d_loss: 1.36593008, g_loss: 0.69414753, ae_loss: 0.05028624\n",
      "Step: [2043] total_loss: 2.14993691 d_loss: 1.40982842, g_loss: 0.68717420, ae_loss: 0.05293432\n",
      "Step: [2044] total_loss: 2.12276363 d_loss: 1.38075185, g_loss: 0.68866080, ae_loss: 0.05335097\n",
      "Step: [2045] total_loss: 2.13076663 d_loss: 1.38278127, g_loss: 0.69647741, ae_loss: 0.05150799\n",
      "Step: [2046] total_loss: 2.15648913 d_loss: 1.39068866, g_loss: 0.71014297, ae_loss: 0.05565749\n",
      "Step: [2047] total_loss: 2.13794231 d_loss: 1.38786650, g_loss: 0.69778514, ae_loss: 0.05229074\n",
      "Step: [2048] total_loss: 2.13318467 d_loss: 1.39043367, g_loss: 0.69283772, ae_loss: 0.04991334\n",
      "Step: [2049] total_loss: 2.11674905 d_loss: 1.37090611, g_loss: 0.69626784, ae_loss: 0.04957506\n",
      "Step: [2050] total_loss: 2.13150072 d_loss: 1.38974953, g_loss: 0.68989247, ae_loss: 0.05185876\n",
      "Step: [2051] total_loss: 2.12881994 d_loss: 1.38454199, g_loss: 0.69192410, ae_loss: 0.05235369\n",
      "Step: [2052] total_loss: 2.12361574 d_loss: 1.36967444, g_loss: 0.70400763, ae_loss: 0.04993357\n",
      "Step: [2053] total_loss: 2.12580061 d_loss: 1.39538610, g_loss: 0.67946643, ae_loss: 0.05094818\n",
      "Step: [2054] total_loss: 2.12258005 d_loss: 1.37741756, g_loss: 0.69267195, ae_loss: 0.05249064\n",
      "Step: [2055] total_loss: 2.14623785 d_loss: 1.41591394, g_loss: 0.67375165, ae_loss: 0.05657217\n",
      "Step: [2056] total_loss: 2.12267351 d_loss: 1.37028754, g_loss: 0.69923353, ae_loss: 0.05315246\n",
      "Step: [2057] total_loss: 2.09998322 d_loss: 1.37181795, g_loss: 0.67567813, ae_loss: 0.05248697\n",
      "Step: [2058] total_loss: 2.11724830 d_loss: 1.38715029, g_loss: 0.67952120, ae_loss: 0.05057682\n",
      "Step: [2059] total_loss: 2.11057687 d_loss: 1.38724613, g_loss: 0.66815734, ae_loss: 0.05517345\n",
      "Step: [2060] total_loss: 2.12226319 d_loss: 1.38831711, g_loss: 0.68279195, ae_loss: 0.05115417\n",
      "Step: [2061] total_loss: 2.11822152 d_loss: 1.38277721, g_loss: 0.68234390, ae_loss: 0.05310032\n",
      "Step: [2062] total_loss: 2.11747837 d_loss: 1.37989402, g_loss: 0.68832660, ae_loss: 0.04925767\n",
      "Step: [2063] total_loss: 2.12515759 d_loss: 1.38186836, g_loss: 0.69279939, ae_loss: 0.05048979\n",
      "Step: [2064] total_loss: 2.14158678 d_loss: 1.40199137, g_loss: 0.68543750, ae_loss: 0.05415793\n",
      "Step: [2065] total_loss: 2.13879418 d_loss: 1.39407313, g_loss: 0.69108391, ae_loss: 0.05363714\n",
      "Step: [2066] total_loss: 2.13582754 d_loss: 1.39367461, g_loss: 0.69178921, ae_loss: 0.05036362\n",
      "Step: [2067] total_loss: 2.11952782 d_loss: 1.38688385, g_loss: 0.67818522, ae_loss: 0.05445887\n",
      "Step: [2068] total_loss: 2.13780880 d_loss: 1.38507795, g_loss: 0.70105118, ae_loss: 0.05167959\n",
      "Step: [2069] total_loss: 2.12655330 d_loss: 1.38370728, g_loss: 0.68966800, ae_loss: 0.05317796\n",
      "Step: [2070] total_loss: 2.14063859 d_loss: 1.40116096, g_loss: 0.68607223, ae_loss: 0.05340539\n",
      "Step: [2071] total_loss: 2.10851026 d_loss: 1.37320721, g_loss: 0.68840820, ae_loss: 0.04689480\n",
      "Step: [2072] total_loss: 2.11082602 d_loss: 1.38475120, g_loss: 0.67468143, ae_loss: 0.05139332\n",
      "Step: [2073] total_loss: 2.11872053 d_loss: 1.36498010, g_loss: 0.70489311, ae_loss: 0.04884740\n",
      "Step: [2074] total_loss: 2.12873840 d_loss: 1.37250674, g_loss: 0.70276755, ae_loss: 0.05346423\n",
      "Step: [2075] total_loss: 2.13209343 d_loss: 1.38797188, g_loss: 0.69213271, ae_loss: 0.05198878\n",
      "Step: [2076] total_loss: 2.12687302 d_loss: 1.38886225, g_loss: 0.68587750, ae_loss: 0.05213331\n",
      "Step: [2077] total_loss: 2.11677146 d_loss: 1.37723172, g_loss: 0.69005901, ae_loss: 0.04948071\n",
      "Step: [2078] total_loss: 2.12343407 d_loss: 1.37618423, g_loss: 0.69257843, ae_loss: 0.05467139\n",
      "Step: [2079] total_loss: 2.12005591 d_loss: 1.35944247, g_loss: 0.71229625, ae_loss: 0.04831718\n",
      "Step: [2080] total_loss: 2.12112904 d_loss: 1.37971210, g_loss: 0.69255722, ae_loss: 0.04885966\n",
      "Step: [2081] total_loss: 2.14262462 d_loss: 1.41371810, g_loss: 0.67777675, ae_loss: 0.05112983\n",
      "Step: [2082] total_loss: 2.11855698 d_loss: 1.38068259, g_loss: 0.68790138, ae_loss: 0.04997301\n",
      "Step: [2083] total_loss: 2.14131737 d_loss: 1.39490843, g_loss: 0.69592762, ae_loss: 0.05048119\n",
      "Step: [2084] total_loss: 2.12946272 d_loss: 1.38116360, g_loss: 0.69887972, ae_loss: 0.04941930\n",
      "Step: [2085] total_loss: 2.13536263 d_loss: 1.36616516, g_loss: 0.71413732, ae_loss: 0.05506022\n",
      "Step: [2086] total_loss: 2.14386106 d_loss: 1.38087511, g_loss: 0.71359968, ae_loss: 0.04938630\n",
      "Step: [2087] total_loss: 2.15161324 d_loss: 1.37362361, g_loss: 0.72856927, ae_loss: 0.04942051\n",
      "Step: [2088] total_loss: 2.12441874 d_loss: 1.37087393, g_loss: 0.70669043, ae_loss: 0.04685451\n",
      "Step: [2089] total_loss: 2.13726473 d_loss: 1.38600087, g_loss: 0.69655001, ae_loss: 0.05471395\n",
      "Step: [2090] total_loss: 2.12008643 d_loss: 1.36225581, g_loss: 0.70607078, ae_loss: 0.05175985\n",
      "Step: [2091] total_loss: 2.12009907 d_loss: 1.37246478, g_loss: 0.69656932, ae_loss: 0.05106489\n",
      "Step: [2092] total_loss: 2.12870145 d_loss: 1.37508202, g_loss: 0.70123208, ae_loss: 0.05238738\n",
      "Step: [2093] total_loss: 2.16158772 d_loss: 1.41602516, g_loss: 0.69105804, ae_loss: 0.05450444\n",
      "Step: [2094] total_loss: 2.13809562 d_loss: 1.40009260, g_loss: 0.68638408, ae_loss: 0.05161889\n",
      "Step: [2095] total_loss: 2.11795759 d_loss: 1.37718284, g_loss: 0.68797940, ae_loss: 0.05279531\n",
      "Step: [2096] total_loss: 2.12225008 d_loss: 1.37551093, g_loss: 0.69639432, ae_loss: 0.05034491\n",
      "Step: [2097] total_loss: 2.13436484 d_loss: 1.39757681, g_loss: 0.68649232, ae_loss: 0.05029568\n",
      "Step: [2098] total_loss: 2.15305877 d_loss: 1.41144037, g_loss: 0.68841541, ae_loss: 0.05320293\n",
      "Step: [2099] total_loss: 2.13645148 d_loss: 1.38294077, g_loss: 0.69819796, ae_loss: 0.05531273\n",
      "Step: [2100] total_loss: 2.14927506 d_loss: 1.40403283, g_loss: 0.69271290, ae_loss: 0.05252929\n",
      "Step: [2101] total_loss: 2.12490368 d_loss: 1.36760485, g_loss: 0.70406806, ae_loss: 0.05323067\n",
      "Step: [2102] total_loss: 2.15933251 d_loss: 1.40199327, g_loss: 0.70437837, ae_loss: 0.05296088\n",
      "Step: [2103] total_loss: 2.11972928 d_loss: 1.37998879, g_loss: 0.69170922, ae_loss: 0.04803119\n",
      "Step: [2104] total_loss: 2.15911913 d_loss: 1.40490448, g_loss: 0.69721341, ae_loss: 0.05700137\n",
      "Step: [2105] total_loss: 2.12795591 d_loss: 1.38784623, g_loss: 0.68814278, ae_loss: 0.05196678\n",
      "Step: [2106] total_loss: 2.13092136 d_loss: 1.39615679, g_loss: 0.68214381, ae_loss: 0.05262072\n",
      "Step: [2107] total_loss: 2.13653994 d_loss: 1.39331079, g_loss: 0.68704271, ae_loss: 0.05618653\n",
      "Step: [2108] total_loss: 2.11803222 d_loss: 1.36484146, g_loss: 0.70052040, ae_loss: 0.05267031\n",
      "Step: [2109] total_loss: 2.13651943 d_loss: 1.40746367, g_loss: 0.68100810, ae_loss: 0.04804781\n",
      "Step: [2110] total_loss: 2.13383460 d_loss: 1.39257169, g_loss: 0.69258571, ae_loss: 0.04867716\n",
      "Step: [2111] total_loss: 2.12781191 d_loss: 1.37570238, g_loss: 0.69388175, ae_loss: 0.05822775\n",
      "Step: [2112] total_loss: 2.13067627 d_loss: 1.37945664, g_loss: 0.69883996, ae_loss: 0.05237956\n",
      "Step: [2113] total_loss: 2.13849831 d_loss: 1.38249588, g_loss: 0.70403516, ae_loss: 0.05196717\n",
      "Step: [2114] total_loss: 2.12211132 d_loss: 1.37584829, g_loss: 0.69152498, ae_loss: 0.05473818\n",
      "Step: [2115] total_loss: 2.12785125 d_loss: 1.39609694, g_loss: 0.67834258, ae_loss: 0.05341170\n",
      "Step: [2116] total_loss: 2.11992836 d_loss: 1.39521122, g_loss: 0.67453277, ae_loss: 0.05018420\n",
      "Step: [2117] total_loss: 2.10780382 d_loss: 1.37240124, g_loss: 0.68354404, ae_loss: 0.05185844\n",
      "Step: [2118] total_loss: 2.11430311 d_loss: 1.36318541, g_loss: 0.69750357, ae_loss: 0.05361410\n",
      "Step: [2119] total_loss: 2.12561893 d_loss: 1.39140296, g_loss: 0.68436563, ae_loss: 0.04985021\n",
      "Step: [2120] total_loss: 2.14224935 d_loss: 1.39793634, g_loss: 0.69184077, ae_loss: 0.05247220\n",
      "Step: [2121] total_loss: 2.13191080 d_loss: 1.38343525, g_loss: 0.69656873, ae_loss: 0.05190676\n",
      "Step: [2122] total_loss: 2.13404298 d_loss: 1.39093208, g_loss: 0.69010478, ae_loss: 0.05300614\n",
      "Step: [2123] total_loss: 2.15520787 d_loss: 1.41241455, g_loss: 0.69100106, ae_loss: 0.05179229\n",
      "Step: [2124] total_loss: 2.11817932 d_loss: 1.37674284, g_loss: 0.69106126, ae_loss: 0.05037517\n",
      "Step: [2125] total_loss: 2.11860943 d_loss: 1.38418937, g_loss: 0.68576169, ae_loss: 0.04865837\n",
      "Step: [2126] total_loss: 2.13171005 d_loss: 1.39971995, g_loss: 0.68642402, ae_loss: 0.04556621\n",
      "Step: [2127] total_loss: 2.11764622 d_loss: 1.37257624, g_loss: 0.69197112, ae_loss: 0.05309895\n",
      "Step: [2128] total_loss: 2.12138128 d_loss: 1.36967957, g_loss: 0.70033979, ae_loss: 0.05136199\n",
      "Step: [2129] total_loss: 2.13760352 d_loss: 1.38489699, g_loss: 0.69778252, ae_loss: 0.05492404\n",
      "Step: [2130] total_loss: 2.14421058 d_loss: 1.37657547, g_loss: 0.71507967, ae_loss: 0.05255546\n",
      "Step: [2131] total_loss: 2.13610244 d_loss: 1.37419581, g_loss: 0.70800155, ae_loss: 0.05390517\n",
      "Step: [2132] total_loss: 2.12573957 d_loss: 1.39122629, g_loss: 0.68356574, ae_loss: 0.05094752\n",
      "Step: [2133] total_loss: 2.12844610 d_loss: 1.39469385, g_loss: 0.68003792, ae_loss: 0.05371422\n",
      "Step: [2134] total_loss: 2.11666226 d_loss: 1.37777376, g_loss: 0.69011605, ae_loss: 0.04877247\n",
      "Step: [2135] total_loss: 2.10848665 d_loss: 1.37326443, g_loss: 0.68362439, ae_loss: 0.05159782\n",
      "Step: [2136] total_loss: 2.11189032 d_loss: 1.37289476, g_loss: 0.68811107, ae_loss: 0.05088462\n",
      "Step: [2137] total_loss: 2.11362791 d_loss: 1.38379145, g_loss: 0.68040526, ae_loss: 0.04943129\n",
      "Step: [2138] total_loss: 2.11871552 d_loss: 1.37106109, g_loss: 0.69797188, ae_loss: 0.04968257\n",
      "Step: [2139] total_loss: 2.13162088 d_loss: 1.39014387, g_loss: 0.68484211, ae_loss: 0.05663495\n",
      "Step: [2140] total_loss: 2.14394808 d_loss: 1.40727925, g_loss: 0.68705022, ae_loss: 0.04961869\n",
      "Step: [2141] total_loss: 2.12469912 d_loss: 1.37504983, g_loss: 0.69764280, ae_loss: 0.05200648\n",
      "Step: [2142] total_loss: 2.13845086 d_loss: 1.35898030, g_loss: 0.72972667, ae_loss: 0.04974392\n",
      "Step: [2143] total_loss: 2.14926004 d_loss: 1.40333581, g_loss: 0.69266975, ae_loss: 0.05325454\n",
      "Step: [2144] total_loss: 2.11239052 d_loss: 1.37086010, g_loss: 0.68789595, ae_loss: 0.05363439\n",
      "Step: [2145] total_loss: 2.13088751 d_loss: 1.37115324, g_loss: 0.70735931, ae_loss: 0.05237491\n",
      "Step: [2146] total_loss: 2.11822343 d_loss: 1.37688279, g_loss: 0.68930525, ae_loss: 0.05203533\n",
      "Step: [2147] total_loss: 2.12735319 d_loss: 1.38720918, g_loss: 0.68311208, ae_loss: 0.05703187\n",
      "Step: [2148] total_loss: 2.14753032 d_loss: 1.40455854, g_loss: 0.68957937, ae_loss: 0.05339238\n",
      "Step: [2149] total_loss: 2.12178040 d_loss: 1.37721133, g_loss: 0.69293141, ae_loss: 0.05163782\n",
      "Step: [2150] total_loss: 2.13072252 d_loss: 1.37945783, g_loss: 0.70076346, ae_loss: 0.05050135\n",
      "Step: [2151] total_loss: 2.11237741 d_loss: 1.36522579, g_loss: 0.69392776, ae_loss: 0.05322383\n",
      "Step: [2152] total_loss: 2.11768770 d_loss: 1.38116455, g_loss: 0.68530977, ae_loss: 0.05121328\n",
      "Step: [2153] total_loss: 2.11777949 d_loss: 1.36815023, g_loss: 0.69639200, ae_loss: 0.05323723\n",
      "Step: [2154] total_loss: 2.10402441 d_loss: 1.34821904, g_loss: 0.70012683, ae_loss: 0.05567861\n",
      "Step: [2155] total_loss: 2.13073826 d_loss: 1.36329675, g_loss: 0.71571672, ae_loss: 0.05172485\n",
      "Step: [2156] total_loss: 2.11992216 d_loss: 1.37174642, g_loss: 0.69446468, ae_loss: 0.05371089\n",
      "Step: [2157] total_loss: 2.13883805 d_loss: 1.38072407, g_loss: 0.70775366, ae_loss: 0.05036037\n",
      "Step: [2158] total_loss: 2.13223600 d_loss: 1.38814569, g_loss: 0.69058847, ae_loss: 0.05350176\n",
      "Step: [2159] total_loss: 2.12169743 d_loss: 1.36347437, g_loss: 0.70629251, ae_loss: 0.05193065\n",
      "Step: [2160] total_loss: 2.11326718 d_loss: 1.36741471, g_loss: 0.69242442, ae_loss: 0.05342807\n",
      "Step: [2161] total_loss: 2.11819935 d_loss: 1.35610080, g_loss: 0.71222627, ae_loss: 0.04987216\n",
      "Step: [2162] total_loss: 2.15016794 d_loss: 1.40843451, g_loss: 0.68780929, ae_loss: 0.05392414\n",
      "Step: [2163] total_loss: 2.13408947 d_loss: 1.39075708, g_loss: 0.69164705, ae_loss: 0.05168538\n",
      "Step: [2164] total_loss: 2.14953184 d_loss: 1.39187312, g_loss: 0.70174229, ae_loss: 0.05591656\n",
      "Step: [2165] total_loss: 2.14205098 d_loss: 1.38846326, g_loss: 0.70703197, ae_loss: 0.04655577\n",
      "Step: [2166] total_loss: 2.14330816 d_loss: 1.38571930, g_loss: 0.70474350, ae_loss: 0.05284543\n",
      "Step: [2167] total_loss: 2.11903119 d_loss: 1.38793290, g_loss: 0.67622560, ae_loss: 0.05487266\n",
      "Step: [2168] total_loss: 2.12823272 d_loss: 1.38296258, g_loss: 0.69589239, ae_loss: 0.04937784\n",
      "Step: [2169] total_loss: 2.12758994 d_loss: 1.39522934, g_loss: 0.68069136, ae_loss: 0.05166927\n",
      "Step: [2170] total_loss: 2.11438417 d_loss: 1.38015676, g_loss: 0.68503666, ae_loss: 0.04919061\n",
      "Step: [2171] total_loss: 2.14031553 d_loss: 1.39348638, g_loss: 0.69107842, ae_loss: 0.05575080\n",
      "Step: [2172] total_loss: 2.08624935 d_loss: 1.35270393, g_loss: 0.68273622, ae_loss: 0.05080918\n",
      "Step: [2173] total_loss: 2.10095620 d_loss: 1.38422120, g_loss: 0.66742760, ae_loss: 0.04930736\n",
      "Step: [2174] total_loss: 2.11522722 d_loss: 1.39534831, g_loss: 0.66925526, ae_loss: 0.05062362\n",
      "Step: [2175] total_loss: 2.12183380 d_loss: 1.39323163, g_loss: 0.67745173, ae_loss: 0.05115059\n",
      "Step: [2176] total_loss: 2.13141108 d_loss: 1.37239611, g_loss: 0.70777929, ae_loss: 0.05123569\n",
      "Step: [2177] total_loss: 2.14189172 d_loss: 1.38026738, g_loss: 0.70987034, ae_loss: 0.05175397\n",
      "Step: [2178] total_loss: 2.15074205 d_loss: 1.40167904, g_loss: 0.69080365, ae_loss: 0.05825939\n",
      "Step: [2179] total_loss: 2.15417981 d_loss: 1.43240392, g_loss: 0.66886652, ae_loss: 0.05290934\n",
      "Step: [2180] total_loss: 2.14319015 d_loss: 1.37844408, g_loss: 0.71330881, ae_loss: 0.05143731\n",
      "Step: [2181] total_loss: 2.12942600 d_loss: 1.38072789, g_loss: 0.69600368, ae_loss: 0.05269445\n",
      "Step: [2182] total_loss: 2.12732720 d_loss: 1.38889921, g_loss: 0.68393177, ae_loss: 0.05449619\n",
      "Step: [2183] total_loss: 2.12368059 d_loss: 1.38185692, g_loss: 0.68782443, ae_loss: 0.05399917\n",
      "Step: [2184] total_loss: 2.15022755 d_loss: 1.39378905, g_loss: 0.70601463, ae_loss: 0.05042383\n",
      "Step: [2185] total_loss: 2.14923334 d_loss: 1.40521598, g_loss: 0.69009262, ae_loss: 0.05392484\n",
      "Step: [2186] total_loss: 2.13147259 d_loss: 1.38957429, g_loss: 0.68865436, ae_loss: 0.05324405\n",
      "Step: [2187] total_loss: 2.13192558 d_loss: 1.38663328, g_loss: 0.69243348, ae_loss: 0.05285887\n",
      "Step: [2188] total_loss: 2.12311697 d_loss: 1.37513447, g_loss: 0.69178665, ae_loss: 0.05619575\n",
      "Step: [2189] total_loss: 2.11656952 d_loss: 1.37161601, g_loss: 0.69492722, ae_loss: 0.05002638\n",
      "Step: [2190] total_loss: 2.13312197 d_loss: 1.39050841, g_loss: 0.68907523, ae_loss: 0.05353829\n",
      "Step: [2191] total_loss: 2.11861038 d_loss: 1.37092257, g_loss: 0.69677043, ae_loss: 0.05091730\n",
      "Step: [2192] total_loss: 2.12557840 d_loss: 1.37278247, g_loss: 0.69896770, ae_loss: 0.05382823\n",
      "Step: [2193] total_loss: 2.12375498 d_loss: 1.38675141, g_loss: 0.68414152, ae_loss: 0.05286207\n",
      "Step: [2194] total_loss: 2.10273457 d_loss: 1.36352158, g_loss: 0.68605322, ae_loss: 0.05315987\n",
      "Step: [2195] total_loss: 2.11432099 d_loss: 1.37674356, g_loss: 0.68391210, ae_loss: 0.05366534\n",
      "Step: [2196] total_loss: 2.10650349 d_loss: 1.36709857, g_loss: 0.68933702, ae_loss: 0.05006788\n",
      "Step: [2197] total_loss: 2.13304090 d_loss: 1.39528906, g_loss: 0.68917286, ae_loss: 0.04857890\n",
      "Step: [2198] total_loss: 2.12335658 d_loss: 1.36640120, g_loss: 0.70943069, ae_loss: 0.04752469\n",
      "Step: [2199] total_loss: 2.13432121 d_loss: 1.39278769, g_loss: 0.68880510, ae_loss: 0.05272843\n",
      "Step: [2200] total_loss: 2.12843561 d_loss: 1.39066458, g_loss: 0.68710893, ae_loss: 0.05066202\n",
      "Step: [2201] total_loss: 2.14602804 d_loss: 1.40420234, g_loss: 0.68895209, ae_loss: 0.05287347\n",
      "Step: [2202] total_loss: 2.12562108 d_loss: 1.38171256, g_loss: 0.69137919, ae_loss: 0.05252932\n",
      "Step: [2203] total_loss: 2.13093138 d_loss: 1.40188932, g_loss: 0.67802310, ae_loss: 0.05101890\n",
      "Step: [2204] total_loss: 2.14354467 d_loss: 1.40303874, g_loss: 0.68509996, ae_loss: 0.05540598\n",
      "Step: [2205] total_loss: 2.13453555 d_loss: 1.38139749, g_loss: 0.70100373, ae_loss: 0.05213429\n",
      "Step: [2206] total_loss: 2.13061571 d_loss: 1.37230921, g_loss: 0.70826840, ae_loss: 0.05003826\n",
      "Step: [2207] total_loss: 2.13766909 d_loss: 1.37320662, g_loss: 0.70826697, ae_loss: 0.05619545\n",
      "Step: [2208] total_loss: 2.13256860 d_loss: 1.39348865, g_loss: 0.68819016, ae_loss: 0.05088976\n",
      "Step: [2209] total_loss: 2.11670256 d_loss: 1.38731194, g_loss: 0.67681742, ae_loss: 0.05257334\n",
      "Step: [2210] total_loss: 2.10987806 d_loss: 1.36246121, g_loss: 0.69292843, ae_loss: 0.05448857\n",
      "Step: [2211] total_loss: 2.12035441 d_loss: 1.38380325, g_loss: 0.68657541, ae_loss: 0.04997578\n",
      "Step: [2212] total_loss: 2.13987327 d_loss: 1.39527750, g_loss: 0.68895787, ae_loss: 0.05563784\n",
      "Step: [2213] total_loss: 2.14839745 d_loss: 1.39522278, g_loss: 0.70358384, ae_loss: 0.04959077\n",
      "Step: [2214] total_loss: 2.12912941 d_loss: 1.38883305, g_loss: 0.68754113, ae_loss: 0.05275533\n",
      "Step: [2215] total_loss: 2.10509729 d_loss: 1.37209272, g_loss: 0.68388093, ae_loss: 0.04912367\n",
      "Step: [2216] total_loss: 2.13787723 d_loss: 1.39519000, g_loss: 0.68732160, ae_loss: 0.05536566\n",
      "Step: [2217] total_loss: 2.11610103 d_loss: 1.36704063, g_loss: 0.69525778, ae_loss: 0.05380264\n",
      "Step: [2218] total_loss: 2.11252761 d_loss: 1.37314081, g_loss: 0.68791062, ae_loss: 0.05147612\n",
      "Step: [2219] total_loss: 2.14541197 d_loss: 1.39589572, g_loss: 0.69703847, ae_loss: 0.05247790\n",
      "Step: [2220] total_loss: 2.13854599 d_loss: 1.39326620, g_loss: 0.69594276, ae_loss: 0.04933696\n",
      "Step: [2221] total_loss: 2.11785269 d_loss: 1.36638904, g_loss: 0.69586408, ae_loss: 0.05559950\n",
      "Step: [2222] total_loss: 2.14741278 d_loss: 1.39968753, g_loss: 0.69623601, ae_loss: 0.05148913\n",
      "Step: [2223] total_loss: 2.13584232 d_loss: 1.38848615, g_loss: 0.69436049, ae_loss: 0.05299554\n",
      "Step: [2224] total_loss: 2.12601304 d_loss: 1.39461327, g_loss: 0.68186116, ae_loss: 0.04953865\n",
      "Step: [2225] total_loss: 2.13978577 d_loss: 1.39396107, g_loss: 0.69315779, ae_loss: 0.05266692\n",
      "Step: [2226] total_loss: 2.14227867 d_loss: 1.39083922, g_loss: 0.70192689, ae_loss: 0.04951256\n",
      "Step: [2227] total_loss: 2.10724688 d_loss: 1.36511207, g_loss: 0.69286895, ae_loss: 0.04926594\n",
      "Step: [2228] total_loss: 2.11688042 d_loss: 1.39246893, g_loss: 0.67376745, ae_loss: 0.05064393\n",
      "Step: [2229] total_loss: 2.12073040 d_loss: 1.38179827, g_loss: 0.68751085, ae_loss: 0.05142127\n",
      "Step: [2230] total_loss: 2.12839055 d_loss: 1.39007533, g_loss: 0.68411392, ae_loss: 0.05420122\n",
      "Step: [2231] total_loss: 2.12856531 d_loss: 1.36941338, g_loss: 0.70377243, ae_loss: 0.05537946\n",
      "Step: [2232] total_loss: 2.13783979 d_loss: 1.39677954, g_loss: 0.69186771, ae_loss: 0.04919247\n",
      "Step: [2233] total_loss: 2.12133598 d_loss: 1.37564576, g_loss: 0.69914591, ae_loss: 0.04654428\n",
      "Step: [2234] total_loss: 2.12094712 d_loss: 1.37965763, g_loss: 0.68720609, ae_loss: 0.05408347\n",
      "Step: [2235] total_loss: 2.13363028 d_loss: 1.39844227, g_loss: 0.68312150, ae_loss: 0.05206654\n",
      "Step: [2236] total_loss: 2.14163446 d_loss: 1.39435029, g_loss: 0.69731021, ae_loss: 0.04997411\n",
      "Step: [2237] total_loss: 2.12538576 d_loss: 1.38007784, g_loss: 0.69064724, ae_loss: 0.05466056\n",
      "Step: [2238] total_loss: 2.11332297 d_loss: 1.37886930, g_loss: 0.68332368, ae_loss: 0.05113008\n",
      "Step: [2239] total_loss: 2.12218046 d_loss: 1.37042785, g_loss: 0.70038974, ae_loss: 0.05136285\n",
      "Step: [2240] total_loss: 2.13046360 d_loss: 1.39902401, g_loss: 0.67709577, ae_loss: 0.05434377\n",
      "Step: [2241] total_loss: 2.12170506 d_loss: 1.39033329, g_loss: 0.67904210, ae_loss: 0.05232957\n",
      "Step: [2242] total_loss: 2.14525175 d_loss: 1.40390587, g_loss: 0.68275583, ae_loss: 0.05859009\n",
      "Step: [2243] total_loss: 2.10619974 d_loss: 1.36652446, g_loss: 0.69203645, ae_loss: 0.04763874\n",
      "Step: [2244] total_loss: 2.13552594 d_loss: 1.38920045, g_loss: 0.68945593, ae_loss: 0.05686951\n",
      "Step: [2245] total_loss: 2.12132072 d_loss: 1.38271654, g_loss: 0.68899727, ae_loss: 0.04960706\n",
      "Step: [2246] total_loss: 2.13796091 d_loss: 1.37215686, g_loss: 0.71407485, ae_loss: 0.05172915\n",
      "Step: [2247] total_loss: 2.12651443 d_loss: 1.38290989, g_loss: 0.68966985, ae_loss: 0.05393460\n",
      "Step: [2248] total_loss: 2.12208796 d_loss: 1.37915778, g_loss: 0.69146538, ae_loss: 0.05146487\n",
      "Step: [2249] total_loss: 2.13143897 d_loss: 1.38751316, g_loss: 0.69410288, ae_loss: 0.04982295\n",
      "Step: [2250] total_loss: 2.14209914 d_loss: 1.38685060, g_loss: 0.70340538, ae_loss: 0.05184311\n",
      "Step: [2251] total_loss: 2.13007641 d_loss: 1.38183177, g_loss: 0.69483024, ae_loss: 0.05341443\n",
      "Step: [2252] total_loss: 2.14654589 d_loss: 1.39661050, g_loss: 0.69420910, ae_loss: 0.05572631\n",
      "Step: [2253] total_loss: 2.13392305 d_loss: 1.37598419, g_loss: 0.70557082, ae_loss: 0.05236820\n",
      "Step: [2254] total_loss: 2.12848568 d_loss: 1.37381661, g_loss: 0.70406663, ae_loss: 0.05060250\n",
      "Step: [2255] total_loss: 2.11727095 d_loss: 1.36592937, g_loss: 0.69840860, ae_loss: 0.05293310\n",
      "Step: [2256] total_loss: 2.15630627 d_loss: 1.42135811, g_loss: 0.68311989, ae_loss: 0.05182835\n",
      "Step: [2257] total_loss: 2.12706184 d_loss: 1.37369037, g_loss: 0.70023811, ae_loss: 0.05313324\n",
      "Step: [2258] total_loss: 2.10815620 d_loss: 1.35746038, g_loss: 0.70276248, ae_loss: 0.04793322\n",
      "Step: [2259] total_loss: 2.12926960 d_loss: 1.40527630, g_loss: 0.67338914, ae_loss: 0.05060423\n",
      "Step: [2260] total_loss: 2.14270878 d_loss: 1.40426362, g_loss: 0.68561172, ae_loss: 0.05283357\n",
      "Step: [2261] total_loss: 2.13394070 d_loss: 1.38980937, g_loss: 0.69205081, ae_loss: 0.05208038\n",
      "Step: [2262] total_loss: 2.14140439 d_loss: 1.38490498, g_loss: 0.70498693, ae_loss: 0.05151248\n",
      "Step: [2263] total_loss: 2.13781786 d_loss: 1.39297223, g_loss: 0.69463646, ae_loss: 0.05020919\n",
      "Step: [2264] total_loss: 2.12339282 d_loss: 1.37263536, g_loss: 0.70057803, ae_loss: 0.05017948\n",
      "Step: [2265] total_loss: 2.15234756 d_loss: 1.39021015, g_loss: 0.70732462, ae_loss: 0.05481284\n",
      "Step: [2266] total_loss: 2.13097191 d_loss: 1.38150620, g_loss: 0.70054829, ae_loss: 0.04891739\n",
      "Step: [2267] total_loss: 2.10538578 d_loss: 1.37388742, g_loss: 0.68156433, ae_loss: 0.04993405\n",
      "Step: [2268] total_loss: 2.15478563 d_loss: 1.41906190, g_loss: 0.68209004, ae_loss: 0.05363366\n",
      "Step: [2269] total_loss: 2.12482071 d_loss: 1.37152743, g_loss: 0.69847274, ae_loss: 0.05482045\n",
      "Step: [2270] total_loss: 2.12385559 d_loss: 1.39633274, g_loss: 0.67302418, ae_loss: 0.05449864\n",
      "Step: [2271] total_loss: 2.11714339 d_loss: 1.38753366, g_loss: 0.67910552, ae_loss: 0.05050422\n",
      "Step: [2272] total_loss: 2.12784982 d_loss: 1.38958645, g_loss: 0.68622047, ae_loss: 0.05204295\n",
      "Step: [2273] total_loss: 2.13067365 d_loss: 1.39635766, g_loss: 0.68165404, ae_loss: 0.05266196\n",
      "Step: [2274] total_loss: 2.14233398 d_loss: 1.39960527, g_loss: 0.69013900, ae_loss: 0.05258973\n",
      "Step: [2275] total_loss: 2.12204027 d_loss: 1.38281870, g_loss: 0.68312478, ae_loss: 0.05609662\n",
      "Step: [2276] total_loss: 2.12084317 d_loss: 1.38801479, g_loss: 0.68358874, ae_loss: 0.04923960\n",
      "Step: [2277] total_loss: 2.11880970 d_loss: 1.37556648, g_loss: 0.69180804, ae_loss: 0.05143528\n",
      "Step: [2278] total_loss: 2.14294624 d_loss: 1.39738226, g_loss: 0.69320261, ae_loss: 0.05236144\n",
      "Step: [2279] total_loss: 2.14679718 d_loss: 1.37831044, g_loss: 0.71750700, ae_loss: 0.05097978\n",
      "Step: [2280] total_loss: 2.12855291 d_loss: 1.37399161, g_loss: 0.70487678, ae_loss: 0.04968448\n",
      "Step: [2281] total_loss: 2.13836312 d_loss: 1.39339280, g_loss: 0.69212854, ae_loss: 0.05284176\n",
      "Step: [2282] total_loss: 2.12011242 d_loss: 1.37412965, g_loss: 0.69097102, ae_loss: 0.05501161\n",
      "Step: [2283] total_loss: 2.14128709 d_loss: 1.36324811, g_loss: 0.72268420, ae_loss: 0.05535482\n",
      "Step: [2284] total_loss: 2.12849998 d_loss: 1.38754988, g_loss: 0.69019485, ae_loss: 0.05075539\n",
      "Step: [2285] total_loss: 2.12063456 d_loss: 1.37620401, g_loss: 0.69466203, ae_loss: 0.04976841\n",
      "Step: [2286] total_loss: 2.12983894 d_loss: 1.37662578, g_loss: 0.70240724, ae_loss: 0.05080575\n",
      "Step: [2287] total_loss: 2.11661959 d_loss: 1.37626362, g_loss: 0.69396508, ae_loss: 0.04639076\n",
      "Step: [2288] total_loss: 2.10943079 d_loss: 1.37335467, g_loss: 0.68592370, ae_loss: 0.05015254\n",
      "Step: [2289] total_loss: 2.10806084 d_loss: 1.35912168, g_loss: 0.69860899, ae_loss: 0.05033007\n",
      "Step: [2290] total_loss: 2.11097527 d_loss: 1.36844826, g_loss: 0.68952948, ae_loss: 0.05299751\n",
      "Step: [2291] total_loss: 2.14627528 d_loss: 1.40541363, g_loss: 0.68623388, ae_loss: 0.05462774\n",
      "Step: [2292] total_loss: 2.11650872 d_loss: 1.38528943, g_loss: 0.68128479, ae_loss: 0.04993449\n",
      "Step: [2293] total_loss: 2.12796974 d_loss: 1.39080644, g_loss: 0.68849540, ae_loss: 0.04866774\n",
      "Step: [2294] total_loss: 2.13106441 d_loss: 1.38711774, g_loss: 0.69372153, ae_loss: 0.05022516\n",
      "Step: [2295] total_loss: 2.12853479 d_loss: 1.38726258, g_loss: 0.69252622, ae_loss: 0.04874606\n",
      "Step: [2296] total_loss: 2.14777255 d_loss: 1.39984250, g_loss: 0.69337225, ae_loss: 0.05455775\n",
      "Step: [2297] total_loss: 2.13102341 d_loss: 1.38644576, g_loss: 0.68866229, ae_loss: 0.05591529\n",
      "Step: [2298] total_loss: 2.13609457 d_loss: 1.39331365, g_loss: 0.69054854, ae_loss: 0.05223245\n",
      "Step: [2299] total_loss: 2.14430285 d_loss: 1.39170158, g_loss: 0.69684088, ae_loss: 0.05576027\n",
      "Step: [2300] total_loss: 2.11798191 d_loss: 1.38544357, g_loss: 0.67879558, ae_loss: 0.05374275\n",
      "Step: [2301] total_loss: 2.12588525 d_loss: 1.39037418, g_loss: 0.68326360, ae_loss: 0.05224738\n",
      "Step: [2302] total_loss: 2.12279272 d_loss: 1.38722432, g_loss: 0.68386102, ae_loss: 0.05170724\n",
      "Step: [2303] total_loss: 2.12695694 d_loss: 1.38241935, g_loss: 0.69080460, ae_loss: 0.05373282\n",
      "Step: [2304] total_loss: 2.12088418 d_loss: 1.37075090, g_loss: 0.69944543, ae_loss: 0.05068774\n",
      "Step: [2305] total_loss: 2.12309313 d_loss: 1.38126349, g_loss: 0.69010770, ae_loss: 0.05172202\n",
      "Step: [2306] total_loss: 2.13669944 d_loss: 1.37684035, g_loss: 0.70775676, ae_loss: 0.05210239\n",
      "Step: [2307] total_loss: 2.14500093 d_loss: 1.40816689, g_loss: 0.68170738, ae_loss: 0.05512656\n",
      "Step: [2308] total_loss: 2.14107442 d_loss: 1.40686035, g_loss: 0.68161947, ae_loss: 0.05259450\n",
      "Step: [2309] total_loss: 2.12586212 d_loss: 1.38661802, g_loss: 0.68739861, ae_loss: 0.05184548\n",
      "Step: [2310] total_loss: 2.12010813 d_loss: 1.38451493, g_loss: 0.68648732, ae_loss: 0.04910574\n",
      "Step: [2311] total_loss: 2.11570120 d_loss: 1.37672305, g_loss: 0.68754458, ae_loss: 0.05143355\n",
      "Step: [2312] total_loss: 2.15674257 d_loss: 1.40004599, g_loss: 0.69951004, ae_loss: 0.05718654\n",
      "Step: [2313] total_loss: 2.12016416 d_loss: 1.36370313, g_loss: 0.70373815, ae_loss: 0.05272292\n",
      "Step: [2314] total_loss: 2.13490462 d_loss: 1.37652349, g_loss: 0.70847279, ae_loss: 0.04990835\n",
      "Step: [2315] total_loss: 2.12615800 d_loss: 1.38427043, g_loss: 0.69331497, ae_loss: 0.04857260\n",
      "Step: [2316] total_loss: 2.13406229 d_loss: 1.38920391, g_loss: 0.69526863, ae_loss: 0.04958973\n",
      "Step: [2317] total_loss: 2.15174294 d_loss: 1.41835666, g_loss: 0.68241876, ae_loss: 0.05096760\n",
      "Step: [2318] total_loss: 2.12580967 d_loss: 1.37245440, g_loss: 0.70116323, ae_loss: 0.05219207\n",
      "Step: [2319] total_loss: 2.12471223 d_loss: 1.38592720, g_loss: 0.68852258, ae_loss: 0.05026240\n",
      "Step: [2320] total_loss: 2.12215900 d_loss: 1.36909890, g_loss: 0.70072782, ae_loss: 0.05233246\n",
      "Step: [2321] total_loss: 2.13618708 d_loss: 1.36589503, g_loss: 0.72232294, ae_loss: 0.04796923\n",
      "Step: [2322] total_loss: 2.14215660 d_loss: 1.39878714, g_loss: 0.69533336, ae_loss: 0.04803624\n",
      "Step: [2323] total_loss: 2.12507486 d_loss: 1.38510501, g_loss: 0.68571150, ae_loss: 0.05425817\n",
      "Step: [2324] total_loss: 2.11813927 d_loss: 1.37361479, g_loss: 0.69070888, ae_loss: 0.05381574\n",
      "Step: [2325] total_loss: 2.11903620 d_loss: 1.35777712, g_loss: 0.70993000, ae_loss: 0.05132918\n",
      "Step: [2326] total_loss: 2.12762976 d_loss: 1.37674332, g_loss: 0.69831914, ae_loss: 0.05256722\n",
      "Step: [2327] total_loss: 2.09760475 d_loss: 1.36199129, g_loss: 0.68465209, ae_loss: 0.05096152\n",
      "Step: [2328] total_loss: 2.10403609 d_loss: 1.37052941, g_loss: 0.68436575, ae_loss: 0.04914096\n",
      "Step: [2329] total_loss: 2.12405133 d_loss: 1.39694953, g_loss: 0.67232460, ae_loss: 0.05477729\n",
      "Step: [2330] total_loss: 2.10929441 d_loss: 1.37555003, g_loss: 0.68061829, ae_loss: 0.05312609\n",
      "Step: [2331] total_loss: 2.12416697 d_loss: 1.36603391, g_loss: 0.70665669, ae_loss: 0.05147629\n",
      "Step: [2332] total_loss: 2.13562131 d_loss: 1.38996685, g_loss: 0.69679677, ae_loss: 0.04885773\n",
      "Step: [2333] total_loss: 2.14487886 d_loss: 1.40517974, g_loss: 0.69120198, ae_loss: 0.04849709\n",
      "Step: [2334] total_loss: 2.15363860 d_loss: 1.41466892, g_loss: 0.68428248, ae_loss: 0.05468723\n",
      "Step: [2335] total_loss: 2.12751722 d_loss: 1.38835144, g_loss: 0.68446863, ae_loss: 0.05469699\n",
      "Step: [2336] total_loss: 2.12849283 d_loss: 1.38936639, g_loss: 0.68326056, ae_loss: 0.05586594\n",
      "Step: [2337] total_loss: 2.10974050 d_loss: 1.36863220, g_loss: 0.68795753, ae_loss: 0.05315079\n",
      "Step: [2338] total_loss: 2.11693192 d_loss: 1.37623048, g_loss: 0.68652546, ae_loss: 0.05417593\n",
      "Step: [2339] total_loss: 2.11068821 d_loss: 1.36533463, g_loss: 0.69315886, ae_loss: 0.05219485\n",
      "Step: [2340] total_loss: 2.12227392 d_loss: 1.38121104, g_loss: 0.68924379, ae_loss: 0.05181899\n",
      "Step: [2341] total_loss: 2.11353731 d_loss: 1.37240887, g_loss: 0.69111276, ae_loss: 0.05001552\n",
      "Step: [2342] total_loss: 2.10640335 d_loss: 1.36929870, g_loss: 0.68444091, ae_loss: 0.05266374\n",
      "Step: [2343] total_loss: 2.14037752 d_loss: 1.39018941, g_loss: 0.69839466, ae_loss: 0.05179337\n",
      "Step: [2344] total_loss: 2.14293551 d_loss: 1.39507246, g_loss: 0.69700974, ae_loss: 0.05085339\n",
      "Step: [2345] total_loss: 2.11986876 d_loss: 1.39104784, g_loss: 0.67957187, ae_loss: 0.04924915\n",
      "Step: [2346] total_loss: 2.13642693 d_loss: 1.39523101, g_loss: 0.69037259, ae_loss: 0.05082322\n",
      "Step: [2347] total_loss: 2.15135646 d_loss: 1.38645244, g_loss: 0.71363342, ae_loss: 0.05127063\n",
      "Step: [2348] total_loss: 2.12673283 d_loss: 1.37622404, g_loss: 0.69609118, ae_loss: 0.05441752\n",
      "Step: [2349] total_loss: 2.12091565 d_loss: 1.37090635, g_loss: 0.69607651, ae_loss: 0.05393279\n",
      "Step: [2350] total_loss: 2.11449265 d_loss: 1.37006903, g_loss: 0.69058073, ae_loss: 0.05384291\n",
      "Step: [2351] total_loss: 2.12391615 d_loss: 1.38885093, g_loss: 0.68389094, ae_loss: 0.05117425\n",
      "Step: [2352] total_loss: 2.13334084 d_loss: 1.38262248, g_loss: 0.69767666, ae_loss: 0.05304179\n",
      "Step: [2353] total_loss: 2.11104488 d_loss: 1.37375224, g_loss: 0.68875837, ae_loss: 0.04853432\n",
      "Step: [2354] total_loss: 2.13574982 d_loss: 1.38641691, g_loss: 0.69726121, ae_loss: 0.05207168\n",
      "Step: [2355] total_loss: 2.12471318 d_loss: 1.37369740, g_loss: 0.69992608, ae_loss: 0.05108977\n",
      "Step: [2356] total_loss: 2.14457226 d_loss: 1.39086020, g_loss: 0.69772857, ae_loss: 0.05598337\n",
      "Step: [2357] total_loss: 2.13400221 d_loss: 1.37481332, g_loss: 0.70510495, ae_loss: 0.05408398\n",
      "Step: [2358] total_loss: 2.14252758 d_loss: 1.41548574, g_loss: 0.67609978, ae_loss: 0.05094189\n",
      "Step: [2359] total_loss: 2.11843848 d_loss: 1.37511504, g_loss: 0.68953812, ae_loss: 0.05378531\n",
      "Step: [2360] total_loss: 2.10665131 d_loss: 1.37852097, g_loss: 0.67575103, ae_loss: 0.05237934\n",
      "Step: [2361] total_loss: 2.12753987 d_loss: 1.40655053, g_loss: 0.66841167, ae_loss: 0.05257776\n",
      "Step: [2362] total_loss: 2.12133741 d_loss: 1.38156176, g_loss: 0.68819880, ae_loss: 0.05157698\n",
      "Step: [2363] total_loss: 2.12085366 d_loss: 1.36926162, g_loss: 0.69926858, ae_loss: 0.05232351\n",
      "Step: [2364] total_loss: 2.13522410 d_loss: 1.40497851, g_loss: 0.68075967, ae_loss: 0.04948596\n",
      "Step: [2365] total_loss: 2.12317324 d_loss: 1.38674521, g_loss: 0.68218017, ae_loss: 0.05424776\n",
      "Step: [2366] total_loss: 2.11692047 d_loss: 1.36994278, g_loss: 0.69481099, ae_loss: 0.05216684\n",
      "Step: [2367] total_loss: 2.10827780 d_loss: 1.36059856, g_loss: 0.69648373, ae_loss: 0.05119559\n",
      "Step: [2368] total_loss: 2.12344718 d_loss: 1.38015747, g_loss: 0.68857986, ae_loss: 0.05470983\n",
      "Step: [2369] total_loss: 2.13078737 d_loss: 1.37471855, g_loss: 0.70300901, ae_loss: 0.05305986\n",
      "Step: [2370] total_loss: 2.11238670 d_loss: 1.37363052, g_loss: 0.68524885, ae_loss: 0.05350735\n",
      "Step: [2371] total_loss: 2.12214184 d_loss: 1.35652804, g_loss: 0.71503079, ae_loss: 0.05058292\n",
      "Step: [2372] total_loss: 2.13846159 d_loss: 1.38627839, g_loss: 0.69820184, ae_loss: 0.05398146\n",
      "Step: [2373] total_loss: 2.15772152 d_loss: 1.38942003, g_loss: 0.71609521, ae_loss: 0.05220624\n",
      "Step: [2374] total_loss: 2.13606405 d_loss: 1.38936710, g_loss: 0.69744176, ae_loss: 0.04925509\n",
      "Step: [2375] total_loss: 2.11418557 d_loss: 1.38072383, g_loss: 0.68252313, ae_loss: 0.05093865\n",
      "Step: [2376] total_loss: 2.11827588 d_loss: 1.35779798, g_loss: 0.70710856, ae_loss: 0.05336935\n",
      "Step: [2377] total_loss: 2.14196348 d_loss: 1.39056706, g_loss: 0.69706339, ae_loss: 0.05433313\n",
      "Step: [2378] total_loss: 2.13077497 d_loss: 1.37308860, g_loss: 0.70425022, ae_loss: 0.05343621\n",
      "Step: [2379] total_loss: 2.11700392 d_loss: 1.38257897, g_loss: 0.68504477, ae_loss: 0.04938004\n",
      "Step: [2380] total_loss: 2.13161469 d_loss: 1.38828743, g_loss: 0.69022334, ae_loss: 0.05310397\n",
      "Step: [2381] total_loss: 2.13057351 d_loss: 1.38358843, g_loss: 0.69596595, ae_loss: 0.05101905\n",
      "Step: [2382] total_loss: 2.12086964 d_loss: 1.37345278, g_loss: 0.69279861, ae_loss: 0.05461823\n",
      "Step: [2383] total_loss: 2.13058710 d_loss: 1.40028751, g_loss: 0.67893469, ae_loss: 0.05136488\n",
      "Step: [2384] total_loss: 2.12739754 d_loss: 1.38595271, g_loss: 0.68929678, ae_loss: 0.05214812\n",
      "Step: [2385] total_loss: 2.12834406 d_loss: 1.37939489, g_loss: 0.69786286, ae_loss: 0.05108646\n",
      "Step: [2386] total_loss: 2.11867309 d_loss: 1.38575840, g_loss: 0.68134844, ae_loss: 0.05156627\n",
      "Step: [2387] total_loss: 2.13181949 d_loss: 1.39868188, g_loss: 0.67877078, ae_loss: 0.05436680\n",
      "Step: [2388] total_loss: 2.12436056 d_loss: 1.37261689, g_loss: 0.70172256, ae_loss: 0.05002110\n",
      "Step: [2389] total_loss: 2.13129330 d_loss: 1.37396860, g_loss: 0.70440978, ae_loss: 0.05291486\n",
      "Step: [2390] total_loss: 2.12144375 d_loss: 1.37586510, g_loss: 0.69442463, ae_loss: 0.05115415\n",
      "Step: [2391] total_loss: 2.13307619 d_loss: 1.38394618, g_loss: 0.69834316, ae_loss: 0.05078674\n",
      "Step: [2392] total_loss: 2.13602757 d_loss: 1.39518785, g_loss: 0.68718213, ae_loss: 0.05365760\n",
      "Step: [2393] total_loss: 2.13255262 d_loss: 1.38828969, g_loss: 0.69174862, ae_loss: 0.05251443\n",
      "Step: [2394] total_loss: 2.12373447 d_loss: 1.36679399, g_loss: 0.69882035, ae_loss: 0.05812006\n",
      "Step: [2395] total_loss: 2.14449072 d_loss: 1.40559030, g_loss: 0.68490577, ae_loss: 0.05399466\n",
      "Step: [2396] total_loss: 2.12663364 d_loss: 1.38008070, g_loss: 0.69628853, ae_loss: 0.05026442\n",
      "Step: [2397] total_loss: 2.13820434 d_loss: 1.37761736, g_loss: 0.70828199, ae_loss: 0.05230496\n",
      "Step: [2398] total_loss: 2.12274313 d_loss: 1.39527106, g_loss: 0.67355955, ae_loss: 0.05391248\n",
      "Step: [2399] total_loss: 2.12794280 d_loss: 1.38999557, g_loss: 0.68585670, ae_loss: 0.05209052\n",
      "Step: [2400] total_loss: 2.15564489 d_loss: 1.39125133, g_loss: 0.71026862, ae_loss: 0.05412507\n",
      "Step: [2401] total_loss: 2.13417768 d_loss: 1.38898861, g_loss: 0.69353479, ae_loss: 0.05165435\n",
      "Step: [2402] total_loss: 2.13904715 d_loss: 1.38568020, g_loss: 0.69496214, ae_loss: 0.05840491\n",
      "Step: [2403] total_loss: 2.13664675 d_loss: 1.38915968, g_loss: 0.69433439, ae_loss: 0.05315261\n",
      "Step: [2404] total_loss: 2.12583590 d_loss: 1.38211107, g_loss: 0.68835330, ae_loss: 0.05537167\n",
      "Step: [2405] total_loss: 2.11424708 d_loss: 1.38279045, g_loss: 0.68217051, ae_loss: 0.04928612\n",
      "Step: [2406] total_loss: 2.12703943 d_loss: 1.39072859, g_loss: 0.68119073, ae_loss: 0.05512026\n",
      "Step: [2407] total_loss: 2.12077546 d_loss: 1.37613964, g_loss: 0.69658828, ae_loss: 0.04804751\n",
      "Step: [2408] total_loss: 2.13931990 d_loss: 1.37290835, g_loss: 0.71622992, ae_loss: 0.05018179\n",
      "Step: [2409] total_loss: 2.11211205 d_loss: 1.36506081, g_loss: 0.69499367, ae_loss: 0.05205768\n",
      "Step: [2410] total_loss: 2.12201977 d_loss: 1.36469603, g_loss: 0.70390624, ae_loss: 0.05341754\n",
      "Step: [2411] total_loss: 2.12788749 d_loss: 1.40120363, g_loss: 0.67751741, ae_loss: 0.04916646\n",
      "Step: [2412] total_loss: 2.12076235 d_loss: 1.36022735, g_loss: 0.70759344, ae_loss: 0.05294147\n",
      "Step: [2413] total_loss: 2.10486650 d_loss: 1.36394715, g_loss: 0.68633509, ae_loss: 0.05458412\n",
      "Step: [2414] total_loss: 2.11981273 d_loss: 1.38292742, g_loss: 0.68387288, ae_loss: 0.05301232\n",
      "Step: [2415] total_loss: 2.16597414 d_loss: 1.43354356, g_loss: 0.68319893, ae_loss: 0.04923170\n",
      "Step: [2416] total_loss: 2.15346479 d_loss: 1.40491939, g_loss: 0.69827926, ae_loss: 0.05026627\n",
      "Step: [2417] total_loss: 2.13113141 d_loss: 1.38219798, g_loss: 0.69797677, ae_loss: 0.05095655\n",
      "Step: [2418] total_loss: 2.12901878 d_loss: 1.37003684, g_loss: 0.70644617, ae_loss: 0.05253572\n",
      "Step: [2419] total_loss: 2.14685869 d_loss: 1.38178110, g_loss: 0.71196771, ae_loss: 0.05310997\n",
      "Step: [2420] total_loss: 2.13229942 d_loss: 1.35986912, g_loss: 0.71466458, ae_loss: 0.05776557\n",
      "Step: [2421] total_loss: 2.12796021 d_loss: 1.39120436, g_loss: 0.68731606, ae_loss: 0.04943965\n",
      "Step: [2422] total_loss: 2.12753057 d_loss: 1.39069724, g_loss: 0.68603766, ae_loss: 0.05079570\n",
      "Step: [2423] total_loss: 2.14860392 d_loss: 1.39488435, g_loss: 0.70507920, ae_loss: 0.04864022\n",
      "Step: [2424] total_loss: 2.11963654 d_loss: 1.37479877, g_loss: 0.69696820, ae_loss: 0.04786966\n",
      "Step: [2425] total_loss: 2.10838652 d_loss: 1.37383175, g_loss: 0.68509823, ae_loss: 0.04945655\n",
      "Step: [2426] total_loss: 2.13574505 d_loss: 1.37948668, g_loss: 0.69976449, ae_loss: 0.05649401\n",
      "Step: [2427] total_loss: 2.13323832 d_loss: 1.35828102, g_loss: 0.72424650, ae_loss: 0.05071070\n",
      "Step: [2428] total_loss: 2.13363838 d_loss: 1.36290157, g_loss: 0.72485542, ae_loss: 0.04588125\n",
      "Step: [2429] total_loss: 2.13627481 d_loss: 1.37407899, g_loss: 0.70905745, ae_loss: 0.05313824\n",
      "Step: [2430] total_loss: 2.13143253 d_loss: 1.37486553, g_loss: 0.70440763, ae_loss: 0.05215927\n",
      "Step: [2431] total_loss: 2.13167500 d_loss: 1.36419594, g_loss: 0.71655959, ae_loss: 0.05091941\n",
      "Step: [2432] total_loss: 2.12445116 d_loss: 1.39142036, g_loss: 0.67971551, ae_loss: 0.05331532\n",
      "Step: [2433] total_loss: 2.12748313 d_loss: 1.36059070, g_loss: 0.71036637, ae_loss: 0.05652602\n",
      "Step: [2434] total_loss: 2.15119553 d_loss: 1.41988683, g_loss: 0.67987132, ae_loss: 0.05143721\n",
      "Step: [2435] total_loss: 2.12631059 d_loss: 1.37727237, g_loss: 0.69816947, ae_loss: 0.05086872\n",
      "Step: [2436] total_loss: 2.14154625 d_loss: 1.38684177, g_loss: 0.70172465, ae_loss: 0.05297995\n",
      "Step: [2437] total_loss: 2.13959408 d_loss: 1.40460253, g_loss: 0.68513852, ae_loss: 0.04985296\n",
      "Step: [2438] total_loss: 2.12823820 d_loss: 1.37556243, g_loss: 0.70067543, ae_loss: 0.05200040\n",
      "Step: [2439] total_loss: 2.11742949 d_loss: 1.35423684, g_loss: 0.71033370, ae_loss: 0.05285892\n",
      "Step: [2440] total_loss: 2.13356900 d_loss: 1.39304304, g_loss: 0.68489063, ae_loss: 0.05563533\n",
      "Step: [2441] total_loss: 2.13121700 d_loss: 1.37928140, g_loss: 0.70097506, ae_loss: 0.05096050\n",
      "Step: [2442] total_loss: 2.12326288 d_loss: 1.39424241, g_loss: 0.67586398, ae_loss: 0.05315646\n",
      "Step: [2443] total_loss: 2.11254382 d_loss: 1.37067294, g_loss: 0.68631035, ae_loss: 0.05556064\n",
      "Step: [2444] total_loss: 2.12147188 d_loss: 1.39016414, g_loss: 0.68273336, ae_loss: 0.04857434\n",
      "Step: [2445] total_loss: 2.13844991 d_loss: 1.40144539, g_loss: 0.68820727, ae_loss: 0.04879727\n",
      "Step: [2446] total_loss: 2.14302397 d_loss: 1.38312864, g_loss: 0.70664829, ae_loss: 0.05324694\n",
      "Step: [2447] total_loss: 2.12002087 d_loss: 1.38045740, g_loss: 0.68654704, ae_loss: 0.05301647\n",
      "Step: [2448] total_loss: 2.13620424 d_loss: 1.38061106, g_loss: 0.69972920, ae_loss: 0.05586410\n",
      "Step: [2449] total_loss: 2.12025881 d_loss: 1.36171186, g_loss: 0.70451397, ae_loss: 0.05403303\n",
      "Step: [2450] total_loss: 2.12644863 d_loss: 1.38978195, g_loss: 0.68754524, ae_loss: 0.04912142\n",
      "Step: [2451] total_loss: 2.12187910 d_loss: 1.36686432, g_loss: 0.70500588, ae_loss: 0.05000895\n",
      "Step: [2452] total_loss: 2.10500288 d_loss: 1.36122859, g_loss: 0.69354165, ae_loss: 0.05023259\n",
      "Step: [2453] total_loss: 2.11201167 d_loss: 1.36046147, g_loss: 0.69749576, ae_loss: 0.05405436\n",
      "Step: [2454] total_loss: 2.11750412 d_loss: 1.37440991, g_loss: 0.68946505, ae_loss: 0.05362907\n",
      "Step: [2455] total_loss: 2.13238859 d_loss: 1.37541199, g_loss: 0.70315480, ae_loss: 0.05382178\n",
      "Step: [2456] total_loss: 2.13582730 d_loss: 1.37057686, g_loss: 0.71123165, ae_loss: 0.05401881\n",
      "Step: [2457] total_loss: 2.13226247 d_loss: 1.40203190, g_loss: 0.67610770, ae_loss: 0.05412292\n",
      "Step: [2458] total_loss: 2.15616322 d_loss: 1.40048838, g_loss: 0.70194995, ae_loss: 0.05372492\n",
      "Step: [2459] total_loss: 2.12051344 d_loss: 1.38314605, g_loss: 0.68089306, ae_loss: 0.05647435\n",
      "Step: [2460] total_loss: 2.14113545 d_loss: 1.41256237, g_loss: 0.67809051, ae_loss: 0.05048251\n",
      "Step: [2461] total_loss: 2.11199284 d_loss: 1.36832893, g_loss: 0.69101429, ae_loss: 0.05264969\n",
      "Step: [2462] total_loss: 2.11875272 d_loss: 1.38312054, g_loss: 0.68665266, ae_loss: 0.04897948\n",
      "Step: [2463] total_loss: 2.11871958 d_loss: 1.38850439, g_loss: 0.68112886, ae_loss: 0.04908637\n",
      "Step: [2464] total_loss: 2.12896824 d_loss: 1.37196100, g_loss: 0.70189273, ae_loss: 0.05511462\n",
      "Step: [2465] total_loss: 2.13542986 d_loss: 1.36291885, g_loss: 0.71857417, ae_loss: 0.05393694\n",
      "Step: [2466] total_loss: 2.14839911 d_loss: 1.40099609, g_loss: 0.69392163, ae_loss: 0.05348140\n",
      "Step: [2467] total_loss: 2.11805749 d_loss: 1.37704539, g_loss: 0.68443459, ae_loss: 0.05657762\n",
      "Step: [2468] total_loss: 2.11523247 d_loss: 1.38936448, g_loss: 0.67342591, ae_loss: 0.05244202\n",
      "Step: [2469] total_loss: 2.12502527 d_loss: 1.37668300, g_loss: 0.69675827, ae_loss: 0.05158393\n",
      "Step: [2470] total_loss: 2.10532999 d_loss: 1.36799991, g_loss: 0.68397641, ae_loss: 0.05335359\n",
      "Step: [2471] total_loss: 2.10296559 d_loss: 1.36786675, g_loss: 0.68078756, ae_loss: 0.05431126\n",
      "Step: [2472] total_loss: 2.12741137 d_loss: 1.39615595, g_loss: 0.68045616, ae_loss: 0.05079912\n",
      "Step: [2473] total_loss: 2.14616632 d_loss: 1.38775146, g_loss: 0.70069861, ae_loss: 0.05771622\n",
      "Step: [2474] total_loss: 2.12316585 d_loss: 1.36209512, g_loss: 0.70630765, ae_loss: 0.05476311\n",
      "Step: [2475] total_loss: 2.13621879 d_loss: 1.39437056, g_loss: 0.68983603, ae_loss: 0.05201223\n",
      "Step: [2476] total_loss: 2.14906979 d_loss: 1.39200222, g_loss: 0.70350647, ae_loss: 0.05356099\n",
      "Step: [2477] total_loss: 2.12116575 d_loss: 1.37543249, g_loss: 0.69552046, ae_loss: 0.05021287\n",
      "Step: [2478] total_loss: 2.15238810 d_loss: 1.40356958, g_loss: 0.69367242, ae_loss: 0.05514609\n",
      "Step: [2479] total_loss: 2.12958598 d_loss: 1.37646270, g_loss: 0.70257157, ae_loss: 0.05055162\n",
      "Step: [2480] total_loss: 2.12567401 d_loss: 1.37684250, g_loss: 0.69413537, ae_loss: 0.05469612\n",
      "Step: [2481] total_loss: 2.13239169 d_loss: 1.40037179, g_loss: 0.68132234, ae_loss: 0.05069754\n",
      "Step: [2482] total_loss: 2.11673141 d_loss: 1.36306310, g_loss: 0.70134413, ae_loss: 0.05232420\n",
      "Step: [2483] total_loss: 2.10724354 d_loss: 1.35679340, g_loss: 0.69873857, ae_loss: 0.05171150\n",
      "Step: [2484] total_loss: 2.11569047 d_loss: 1.37771356, g_loss: 0.68525910, ae_loss: 0.05271778\n",
      "Step: [2485] total_loss: 2.11215091 d_loss: 1.36821079, g_loss: 0.69278294, ae_loss: 0.05115712\n",
      "Step: [2486] total_loss: 2.12311602 d_loss: 1.39046812, g_loss: 0.68079817, ae_loss: 0.05184987\n",
      "Step: [2487] total_loss: 2.12988305 d_loss: 1.38266516, g_loss: 0.69404399, ae_loss: 0.05317396\n",
      "Step: [2488] total_loss: 2.13025832 d_loss: 1.38825679, g_loss: 0.68575531, ae_loss: 0.05624630\n",
      "Step: [2489] total_loss: 2.10686255 d_loss: 1.35663533, g_loss: 0.69514942, ae_loss: 0.05507796\n",
      "Step: [2490] total_loss: 2.12992477 d_loss: 1.38202655, g_loss: 0.69152224, ae_loss: 0.05637601\n",
      "Step: [2491] total_loss: 2.12217379 d_loss: 1.38890922, g_loss: 0.68281639, ae_loss: 0.05044835\n",
      "Step: [2492] total_loss: 2.12748194 d_loss: 1.37520409, g_loss: 0.70106262, ae_loss: 0.05121513\n",
      "Step: [2493] total_loss: 2.12041807 d_loss: 1.38899302, g_loss: 0.68095207, ae_loss: 0.05047293\n",
      "Step: [2494] total_loss: 2.12308073 d_loss: 1.37064600, g_loss: 0.70214617, ae_loss: 0.05028840\n",
      "Step: [2495] total_loss: 2.14918804 d_loss: 1.39196634, g_loss: 0.70500380, ae_loss: 0.05221780\n",
      "Step: [2496] total_loss: 2.12629652 d_loss: 1.36622310, g_loss: 0.70873177, ae_loss: 0.05134182\n",
      "Step: [2497] total_loss: 2.12039709 d_loss: 1.37149394, g_loss: 0.69354033, ae_loss: 0.05536290\n",
      "Step: [2498] total_loss: 2.11754704 d_loss: 1.37942410, g_loss: 0.68688363, ae_loss: 0.05123920\n",
      "Step: [2499] total_loss: 2.13907743 d_loss: 1.39676702, g_loss: 0.69076794, ae_loss: 0.05154235\n",
      "Step: [2500] total_loss: 2.11604881 d_loss: 1.38738203, g_loss: 0.67729187, ae_loss: 0.05137488\n",
      "Step: [2501] total_loss: 2.12714100 d_loss: 1.38680387, g_loss: 0.68785810, ae_loss: 0.05247897\n",
      "Step: [2502] total_loss: 2.12596750 d_loss: 1.38243294, g_loss: 0.69216359, ae_loss: 0.05137107\n",
      "Step: [2503] total_loss: 2.11992550 d_loss: 1.35346746, g_loss: 0.71532726, ae_loss: 0.05113083\n",
      "Step: [2504] total_loss: 2.15936637 d_loss: 1.41492248, g_loss: 0.69323057, ae_loss: 0.05121322\n",
      "Step: [2505] total_loss: 2.12378883 d_loss: 1.37425780, g_loss: 0.70151579, ae_loss: 0.04801514\n",
      "Step: [2506] total_loss: 2.15322065 d_loss: 1.41084623, g_loss: 0.69111538, ae_loss: 0.05125898\n",
      "Step: [2507] total_loss: 2.12621689 d_loss: 1.39040160, g_loss: 0.68450230, ae_loss: 0.05131293\n",
      "Step: [2508] total_loss: 2.15783978 d_loss: 1.40200734, g_loss: 0.69924891, ae_loss: 0.05658352\n",
      "Step: [2509] total_loss: 2.12489223 d_loss: 1.38508058, g_loss: 0.68812001, ae_loss: 0.05169167\n",
      "Step: [2510] total_loss: 2.13286209 d_loss: 1.39354706, g_loss: 0.68416500, ae_loss: 0.05514994\n",
      "Step: [2511] total_loss: 2.11323166 d_loss: 1.36995816, g_loss: 0.69141781, ae_loss: 0.05185575\n",
      "Step: [2512] total_loss: 2.12069535 d_loss: 1.39337707, g_loss: 0.67644936, ae_loss: 0.05086890\n",
      "Step: [2513] total_loss: 2.14316750 d_loss: 1.40515041, g_loss: 0.68661058, ae_loss: 0.05140667\n",
      "Step: [2514] total_loss: 2.11867642 d_loss: 1.38196075, g_loss: 0.68561637, ae_loss: 0.05109929\n",
      "Step: [2515] total_loss: 2.11208749 d_loss: 1.38537157, g_loss: 0.67214441, ae_loss: 0.05457151\n",
      "Step: [2516] total_loss: 2.12108898 d_loss: 1.37208974, g_loss: 0.69804430, ae_loss: 0.05095497\n",
      "Step: [2517] total_loss: 2.13155413 d_loss: 1.39495122, g_loss: 0.68654764, ae_loss: 0.05005512\n",
      "Step: [2518] total_loss: 2.13304305 d_loss: 1.38714647, g_loss: 0.69425416, ae_loss: 0.05164244\n",
      "Step: [2519] total_loss: 2.13855362 d_loss: 1.38310635, g_loss: 0.70452607, ae_loss: 0.05092129\n",
      "Step: [2520] total_loss: 2.12923837 d_loss: 1.36933351, g_loss: 0.70844483, ae_loss: 0.05145998\n",
      "Step: [2521] total_loss: 2.13859868 d_loss: 1.38108635, g_loss: 0.70507455, ae_loss: 0.05243772\n",
      "Step: [2522] total_loss: 2.16339612 d_loss: 1.41227937, g_loss: 0.70077848, ae_loss: 0.05033828\n",
      "Step: [2523] total_loss: 2.12857962 d_loss: 1.38276696, g_loss: 0.69430315, ae_loss: 0.05150941\n",
      "Step: [2524] total_loss: 2.12175560 d_loss: 1.37464523, g_loss: 0.69581926, ae_loss: 0.05129116\n",
      "Step: [2525] total_loss: 2.12525511 d_loss: 1.38383675, g_loss: 0.68799257, ae_loss: 0.05342561\n",
      "Step: [2526] total_loss: 2.11785603 d_loss: 1.36597371, g_loss: 0.69988984, ae_loss: 0.05199237\n",
      "Step: [2527] total_loss: 2.12902975 d_loss: 1.38325286, g_loss: 0.69160330, ae_loss: 0.05417363\n",
      "Step: [2528] total_loss: 2.14790964 d_loss: 1.40260744, g_loss: 0.69186175, ae_loss: 0.05344059\n",
      "Step: [2529] total_loss: 2.12698460 d_loss: 1.38719368, g_loss: 0.68289793, ae_loss: 0.05689288\n",
      "Step: [2530] total_loss: 2.11118913 d_loss: 1.37403917, g_loss: 0.68303305, ae_loss: 0.05411682\n",
      "Step: [2531] total_loss: 2.13815451 d_loss: 1.38745677, g_loss: 0.69484460, ae_loss: 0.05585303\n",
      "Step: [2532] total_loss: 2.14063334 d_loss: 1.40464640, g_loss: 0.68149990, ae_loss: 0.05448700\n",
      "Step: [2533] total_loss: 2.11565399 d_loss: 1.36549509, g_loss: 0.69894731, ae_loss: 0.05121174\n",
      "Step: [2534] total_loss: 2.11870956 d_loss: 1.37149739, g_loss: 0.69639611, ae_loss: 0.05081593\n",
      "Step: [2535] total_loss: 2.11482334 d_loss: 1.38099265, g_loss: 0.68149942, ae_loss: 0.05233127\n",
      "Step: [2536] total_loss: 2.12204671 d_loss: 1.36941457, g_loss: 0.69891155, ae_loss: 0.05372060\n",
      "Step: [2537] total_loss: 2.14212346 d_loss: 1.40391088, g_loss: 0.68821132, ae_loss: 0.05000122\n",
      "Step: [2538] total_loss: 2.11230850 d_loss: 1.35940766, g_loss: 0.70026624, ae_loss: 0.05263455\n",
      "Step: [2539] total_loss: 2.14372015 d_loss: 1.40186441, g_loss: 0.68963850, ae_loss: 0.05221742\n",
      "Step: [2540] total_loss: 2.13491440 d_loss: 1.37917089, g_loss: 0.70202267, ae_loss: 0.05372098\n",
      "Step: [2541] total_loss: 2.12843895 d_loss: 1.38381910, g_loss: 0.69312763, ae_loss: 0.05149237\n",
      "Step: [2542] total_loss: 2.11267471 d_loss: 1.37002802, g_loss: 0.69332182, ae_loss: 0.04932480\n",
      "Step: [2543] total_loss: 2.11054420 d_loss: 1.38072515, g_loss: 0.68125510, ae_loss: 0.04856399\n",
      "Step: [2544] total_loss: 2.13311481 d_loss: 1.37870884, g_loss: 0.70619655, ae_loss: 0.04820936\n",
      "Step: [2545] total_loss: 2.11897039 d_loss: 1.37638283, g_loss: 0.68771327, ae_loss: 0.05487436\n",
      "Step: [2546] total_loss: 2.13044453 d_loss: 1.37187028, g_loss: 0.70776641, ae_loss: 0.05080785\n",
      "Step: [2547] total_loss: 2.14247203 d_loss: 1.39579082, g_loss: 0.69322586, ae_loss: 0.05345529\n",
      "Step: [2548] total_loss: 2.13681364 d_loss: 1.38248873, g_loss: 0.70503008, ae_loss: 0.04929474\n",
      "Step: [2549] total_loss: 2.13217235 d_loss: 1.38299644, g_loss: 0.69924563, ae_loss: 0.04993036\n",
      "Step: [2550] total_loss: 2.11375785 d_loss: 1.37804043, g_loss: 0.68256533, ae_loss: 0.05315208\n",
      "Step: [2551] total_loss: 2.09501052 d_loss: 1.36950314, g_loss: 0.67670834, ae_loss: 0.04879905\n",
      "Step: [2552] total_loss: 2.12475681 d_loss: 1.40199888, g_loss: 0.67435408, ae_loss: 0.04840393\n",
      "Step: [2553] total_loss: 2.12516141 d_loss: 1.36842549, g_loss: 0.70317209, ae_loss: 0.05356380\n",
      "Step: [2554] total_loss: 2.14075089 d_loss: 1.37173474, g_loss: 0.71472603, ae_loss: 0.05429022\n",
      "Step: [2555] total_loss: 2.13213968 d_loss: 1.39308667, g_loss: 0.68481070, ae_loss: 0.05424241\n",
      "Step: [2556] total_loss: 2.13437557 d_loss: 1.38522124, g_loss: 0.69692862, ae_loss: 0.05222578\n",
      "Step: [2557] total_loss: 2.12277079 d_loss: 1.37306404, g_loss: 0.69561982, ae_loss: 0.05408686\n",
      "Step: [2558] total_loss: 2.14043570 d_loss: 1.40382981, g_loss: 0.68620813, ae_loss: 0.05039778\n",
      "Step: [2559] total_loss: 2.10490274 d_loss: 1.36810458, g_loss: 0.68643802, ae_loss: 0.05036025\n",
      "Step: [2560] total_loss: 2.16209722 d_loss: 1.40529919, g_loss: 0.70456684, ae_loss: 0.05223124\n",
      "Step: [2561] total_loss: 2.11529803 d_loss: 1.37228096, g_loss: 0.69553012, ae_loss: 0.04748695\n",
      "Step: [2562] total_loss: 2.09680700 d_loss: 1.35470915, g_loss: 0.68921924, ae_loss: 0.05287852\n",
      "Step: [2563] total_loss: 2.13357115 d_loss: 1.39433599, g_loss: 0.68769568, ae_loss: 0.05153947\n",
      "Step: [2564] total_loss: 2.13176751 d_loss: 1.36893392, g_loss: 0.71095884, ae_loss: 0.05187481\n",
      "Step: [2565] total_loss: 2.15457726 d_loss: 1.38832033, g_loss: 0.70922428, ae_loss: 0.05703276\n",
      "Step: [2566] total_loss: 2.11266279 d_loss: 1.38333297, g_loss: 0.67748463, ae_loss: 0.05184504\n",
      "Step: [2567] total_loss: 2.14133430 d_loss: 1.38319516, g_loss: 0.70234114, ae_loss: 0.05579801\n",
      "Step: [2568] total_loss: 2.12636995 d_loss: 1.37980545, g_loss: 0.69589794, ae_loss: 0.05066650\n",
      "Step: [2569] total_loss: 2.13718438 d_loss: 1.41109967, g_loss: 0.67535645, ae_loss: 0.05072825\n",
      "Step: [2570] total_loss: 2.10496044 d_loss: 1.36478209, g_loss: 0.68743420, ae_loss: 0.05274420\n",
      "Step: [2571] total_loss: 2.11132097 d_loss: 1.38342786, g_loss: 0.67524648, ae_loss: 0.05264665\n",
      "Step: [2572] total_loss: 2.13815761 d_loss: 1.39947724, g_loss: 0.68539095, ae_loss: 0.05328945\n",
      "Step: [2573] total_loss: 2.12768030 d_loss: 1.38151765, g_loss: 0.69399810, ae_loss: 0.05216438\n",
      "Step: [2574] total_loss: 2.12556934 d_loss: 1.37689042, g_loss: 0.69233745, ae_loss: 0.05634151\n",
      "Step: [2575] total_loss: 2.11436176 d_loss: 1.36044693, g_loss: 0.70606911, ae_loss: 0.04784556\n",
      "Step: [2576] total_loss: 2.11716032 d_loss: 1.39050364, g_loss: 0.67671162, ae_loss: 0.04994499\n",
      "Step: [2577] total_loss: 2.11770964 d_loss: 1.38206100, g_loss: 0.68266964, ae_loss: 0.05297888\n",
      "Step: [2578] total_loss: 2.10705209 d_loss: 1.37191296, g_loss: 0.68335450, ae_loss: 0.05178462\n",
      "Step: [2579] total_loss: 2.13209891 d_loss: 1.39727163, g_loss: 0.68022603, ae_loss: 0.05460120\n",
      "Step: [2580] total_loss: 2.13122535 d_loss: 1.38527632, g_loss: 0.69590622, ae_loss: 0.05004292\n",
      "Step: [2581] total_loss: 2.12648964 d_loss: 1.38271904, g_loss: 0.69033402, ae_loss: 0.05343656\n",
      "Step: [2582] total_loss: 2.11707544 d_loss: 1.37745762, g_loss: 0.68839753, ae_loss: 0.05122015\n",
      "Step: [2583] total_loss: 2.12844849 d_loss: 1.39001179, g_loss: 0.68787658, ae_loss: 0.05056015\n",
      "Step: [2584] total_loss: 2.15753841 d_loss: 1.41567612, g_loss: 0.68895090, ae_loss: 0.05291125\n",
      "Step: [2585] total_loss: 2.14237833 d_loss: 1.38564169, g_loss: 0.70072508, ae_loss: 0.05601171\n",
      "Step: [2586] total_loss: 2.11244965 d_loss: 1.36819589, g_loss: 0.69169438, ae_loss: 0.05255926\n",
      "Step: [2587] total_loss: 2.13202095 d_loss: 1.38750303, g_loss: 0.69019723, ae_loss: 0.05432085\n",
      "Step: [2588] total_loss: 2.14721489 d_loss: 1.39506567, g_loss: 0.69954276, ae_loss: 0.05260652\n",
      "Step: [2589] total_loss: 2.12891340 d_loss: 1.38103342, g_loss: 0.69700813, ae_loss: 0.05087196\n",
      "Step: [2590] total_loss: 2.12979245 d_loss: 1.39130020, g_loss: 0.68569398, ae_loss: 0.05279832\n",
      "Step: [2591] total_loss: 2.13128567 d_loss: 1.37871408, g_loss: 0.69942546, ae_loss: 0.05314604\n",
      "Step: [2592] total_loss: 2.11038923 d_loss: 1.36273360, g_loss: 0.69707751, ae_loss: 0.05057798\n",
      "Step: [2593] total_loss: 2.12679100 d_loss: 1.38110042, g_loss: 0.69142127, ae_loss: 0.05426921\n",
      "Step: [2594] total_loss: 2.11403060 d_loss: 1.37461567, g_loss: 0.68769389, ae_loss: 0.05172111\n",
      "Step: [2595] total_loss: 2.14621210 d_loss: 1.38991463, g_loss: 0.70567685, ae_loss: 0.05062067\n",
      "Step: [2596] total_loss: 2.11647582 d_loss: 1.37908196, g_loss: 0.68926501, ae_loss: 0.04812890\n",
      "Step: [2597] total_loss: 2.12908411 d_loss: 1.38635778, g_loss: 0.68803692, ae_loss: 0.05468926\n",
      "Step: [2598] total_loss: 2.09400487 d_loss: 1.36685669, g_loss: 0.67323542, ae_loss: 0.05391280\n",
      "Step: [2599] total_loss: 2.14035225 d_loss: 1.39396477, g_loss: 0.69364625, ae_loss: 0.05274116\n",
      "Step: [2600] total_loss: 2.11361289 d_loss: 1.35330248, g_loss: 0.70365572, ae_loss: 0.05665474\n",
      "Step: [2601] total_loss: 2.14389801 d_loss: 1.39983916, g_loss: 0.69025815, ae_loss: 0.05380069\n",
      "Step: [2602] total_loss: 2.12293959 d_loss: 1.37703156, g_loss: 0.69251692, ae_loss: 0.05339104\n",
      "Step: [2603] total_loss: 2.13160229 d_loss: 1.39454210, g_loss: 0.68409884, ae_loss: 0.05296125\n",
      "Step: [2604] total_loss: 2.13595366 d_loss: 1.39045930, g_loss: 0.69137859, ae_loss: 0.05411576\n",
      "Step: [2605] total_loss: 2.12798738 d_loss: 1.38650775, g_loss: 0.69407564, ae_loss: 0.04740389\n",
      "Step: [2606] total_loss: 2.14865637 d_loss: 1.38722348, g_loss: 0.70712245, ae_loss: 0.05431037\n",
      "Step: [2607] total_loss: 2.14276266 d_loss: 1.38708735, g_loss: 0.70721620, ae_loss: 0.04845919\n",
      "Step: [2608] total_loss: 2.13140249 d_loss: 1.37573910, g_loss: 0.70474362, ae_loss: 0.05091985\n",
      "Step: [2609] total_loss: 2.11038971 d_loss: 1.37063587, g_loss: 0.68576789, ae_loss: 0.05398583\n",
      "Step: [2610] total_loss: 2.11670208 d_loss: 1.38440859, g_loss: 0.68561387, ae_loss: 0.04667978\n",
      "Step: [2611] total_loss: 2.14018631 d_loss: 1.40306640, g_loss: 0.68730497, ae_loss: 0.04981498\n",
      "Step: [2612] total_loss: 2.13489628 d_loss: 1.40499163, g_loss: 0.67643040, ae_loss: 0.05347435\n",
      "Step: [2613] total_loss: 2.12152147 d_loss: 1.40088224, g_loss: 0.66958809, ae_loss: 0.05105101\n",
      "Step: [2614] total_loss: 2.13074446 d_loss: 1.38732946, g_loss: 0.69250292, ae_loss: 0.05091206\n",
      "Step: [2615] total_loss: 2.12317753 d_loss: 1.36307192, g_loss: 0.70647854, ae_loss: 0.05362717\n",
      "Step: [2616] total_loss: 2.14630795 d_loss: 1.39544177, g_loss: 0.69806105, ae_loss: 0.05280520\n",
      "Step: [2617] total_loss: 2.11565900 d_loss: 1.38741469, g_loss: 0.68217731, ae_loss: 0.04606703\n",
      "Step: [2618] total_loss: 2.13490844 d_loss: 1.39517927, g_loss: 0.68604809, ae_loss: 0.05368106\n",
      "Step: [2619] total_loss: 2.11444044 d_loss: 1.38438916, g_loss: 0.67973065, ae_loss: 0.05032077\n",
      "Step: [2620] total_loss: 2.12209320 d_loss: 1.38065112, g_loss: 0.69001549, ae_loss: 0.05142667\n",
      "Step: [2621] total_loss: 2.13145208 d_loss: 1.39316511, g_loss: 0.68641710, ae_loss: 0.05186976\n",
      "Step: [2622] total_loss: 2.10929203 d_loss: 1.37294221, g_loss: 0.68395770, ae_loss: 0.05239224\n",
      "Step: [2623] total_loss: 2.11008191 d_loss: 1.37949800, g_loss: 0.67797065, ae_loss: 0.05261330\n",
      "Step: [2624] total_loss: 2.12180662 d_loss: 1.38031018, g_loss: 0.69315189, ae_loss: 0.04834455\n",
      "Step: [2625] total_loss: 2.11361766 d_loss: 1.37654674, g_loss: 0.68914616, ae_loss: 0.04792476\n",
      "Step: [2626] total_loss: 2.13247633 d_loss: 1.39260602, g_loss: 0.68607771, ae_loss: 0.05379273\n",
      "Step: [2627] total_loss: 2.13130903 d_loss: 1.37792182, g_loss: 0.70554918, ae_loss: 0.04783807\n",
      "Step: [2628] total_loss: 2.12883139 d_loss: 1.39008653, g_loss: 0.68896157, ae_loss: 0.04978337\n",
      "Step: [2629] total_loss: 2.14389968 d_loss: 1.39559233, g_loss: 0.69806778, ae_loss: 0.05023957\n",
      "Step: [2630] total_loss: 2.12232947 d_loss: 1.38312697, g_loss: 0.68693334, ae_loss: 0.05226927\n",
      "Step: [2631] total_loss: 2.14494824 d_loss: 1.40643597, g_loss: 0.68520123, ae_loss: 0.05331098\n",
      "Step: [2632] total_loss: 2.14359665 d_loss: 1.39669311, g_loss: 0.69537145, ae_loss: 0.05153203\n",
      "Step: [2633] total_loss: 2.14650345 d_loss: 1.37760353, g_loss: 0.71799922, ae_loss: 0.05090068\n",
      "Step: [2634] total_loss: 2.11591816 d_loss: 1.38529277, g_loss: 0.67995787, ae_loss: 0.05066759\n",
      "Step: [2635] total_loss: 2.11679363 d_loss: 1.37765956, g_loss: 0.68703198, ae_loss: 0.05210196\n",
      "Step: [2636] total_loss: 2.11437798 d_loss: 1.38604355, g_loss: 0.67097878, ae_loss: 0.05735563\n",
      "Step: [2637] total_loss: 2.11937690 d_loss: 1.39475679, g_loss: 0.67312783, ae_loss: 0.05149239\n",
      "Step: [2638] total_loss: 2.12536621 d_loss: 1.39427507, g_loss: 0.67838252, ae_loss: 0.05270856\n",
      "Step: [2639] total_loss: 2.13927770 d_loss: 1.39014697, g_loss: 0.69263124, ae_loss: 0.05649943\n",
      "Step: [2640] total_loss: 2.11457682 d_loss: 1.36660218, g_loss: 0.69722378, ae_loss: 0.05075092\n",
      "Step: [2641] total_loss: 2.12346029 d_loss: 1.38406634, g_loss: 0.68816149, ae_loss: 0.05123263\n",
      "Step: [2642] total_loss: 2.10781431 d_loss: 1.37723601, g_loss: 0.68343842, ae_loss: 0.04713975\n",
      "Step: [2643] total_loss: 2.12256479 d_loss: 1.37769318, g_loss: 0.68737173, ae_loss: 0.05749998\n",
      "Step: [2644] total_loss: 2.10909343 d_loss: 1.37493479, g_loss: 0.68200994, ae_loss: 0.05214875\n",
      "Step: [2645] total_loss: 2.11255479 d_loss: 1.37695336, g_loss: 0.68376935, ae_loss: 0.05183210\n",
      "Step: [2646] total_loss: 2.13273931 d_loss: 1.38723314, g_loss: 0.69329405, ae_loss: 0.05221211\n",
      "Step: [2647] total_loss: 2.13438225 d_loss: 1.39443922, g_loss: 0.68600941, ae_loss: 0.05393354\n",
      "Step: [2648] total_loss: 2.11903071 d_loss: 1.37698865, g_loss: 0.69241965, ae_loss: 0.04962236\n",
      "Step: [2649] total_loss: 2.13175201 d_loss: 1.38477802, g_loss: 0.69452798, ae_loss: 0.05244597\n",
      "Step: [2650] total_loss: 2.13663340 d_loss: 1.40513134, g_loss: 0.67844558, ae_loss: 0.05305643\n",
      "Step: [2651] total_loss: 2.12379265 d_loss: 1.37287271, g_loss: 0.69920516, ae_loss: 0.05171466\n",
      "Step: [2652] total_loss: 2.12271190 d_loss: 1.36555886, g_loss: 0.70455033, ae_loss: 0.05260269\n",
      "Step: [2653] total_loss: 2.14078856 d_loss: 1.38829327, g_loss: 0.70283425, ae_loss: 0.04966100\n",
      "Step: [2654] total_loss: 2.14118123 d_loss: 1.38401961, g_loss: 0.70603251, ae_loss: 0.05112911\n",
      "Step: [2655] total_loss: 2.12952662 d_loss: 1.36185646, g_loss: 0.71150672, ae_loss: 0.05616330\n",
      "Step: [2656] total_loss: 2.13059092 d_loss: 1.38029718, g_loss: 0.69342446, ae_loss: 0.05686927\n",
      "Step: [2657] total_loss: 2.13318396 d_loss: 1.36623430, g_loss: 0.71210980, ae_loss: 0.05483968\n",
      "Step: [2658] total_loss: 2.12626314 d_loss: 1.38493848, g_loss: 0.68808389, ae_loss: 0.05324084\n",
      "Step: [2659] total_loss: 2.12157011 d_loss: 1.37755048, g_loss: 0.69209218, ae_loss: 0.05192739\n",
      "Step: [2660] total_loss: 2.13507032 d_loss: 1.38248229, g_loss: 0.70159614, ae_loss: 0.05099186\n",
      "Step: [2661] total_loss: 2.13485432 d_loss: 1.40067101, g_loss: 0.67977202, ae_loss: 0.05441117\n",
      "Step: [2662] total_loss: 2.15306830 d_loss: 1.41749549, g_loss: 0.68615639, ae_loss: 0.04941642\n",
      "Step: [2663] total_loss: 2.14196110 d_loss: 1.39280474, g_loss: 0.70040286, ae_loss: 0.04875345\n",
      "Step: [2664] total_loss: 2.13857889 d_loss: 1.38382435, g_loss: 0.70663512, ae_loss: 0.04811959\n",
      "Step: [2665] total_loss: 2.14907217 d_loss: 1.40382290, g_loss: 0.69297683, ae_loss: 0.05227241\n",
      "Step: [2666] total_loss: 2.11951256 d_loss: 1.35925961, g_loss: 0.70697051, ae_loss: 0.05328238\n",
      "Step: [2667] total_loss: 2.10779119 d_loss: 1.37081313, g_loss: 0.68678755, ae_loss: 0.05019061\n",
      "Step: [2668] total_loss: 2.13045621 d_loss: 1.38718331, g_loss: 0.69215715, ae_loss: 0.05111575\n",
      "Step: [2669] total_loss: 2.14726400 d_loss: 1.39843512, g_loss: 0.69444603, ae_loss: 0.05438275\n",
      "Step: [2670] total_loss: 2.13005424 d_loss: 1.37790608, g_loss: 0.69837183, ae_loss: 0.05377632\n",
      "Step: [2671] total_loss: 2.11881781 d_loss: 1.37076545, g_loss: 0.69860101, ae_loss: 0.04945133\n",
      "Step: [2672] total_loss: 2.12805223 d_loss: 1.38240671, g_loss: 0.69354451, ae_loss: 0.05210089\n",
      "Step: [2673] total_loss: 2.12401438 d_loss: 1.38831997, g_loss: 0.68624198, ae_loss: 0.04945242\n",
      "Step: [2674] total_loss: 2.11880493 d_loss: 1.38764977, g_loss: 0.67697680, ae_loss: 0.05417844\n",
      "Step: [2675] total_loss: 2.10728765 d_loss: 1.37713504, g_loss: 0.68122327, ae_loss: 0.04892931\n",
      "Step: [2676] total_loss: 2.11238050 d_loss: 1.37984407, g_loss: 0.68214309, ae_loss: 0.05039352\n",
      "Step: [2677] total_loss: 2.13228226 d_loss: 1.37360430, g_loss: 0.70672214, ae_loss: 0.05195574\n",
      "Step: [2678] total_loss: 2.14177394 d_loss: 1.40297973, g_loss: 0.68824953, ae_loss: 0.05054476\n",
      "Step: [2679] total_loss: 2.12931895 d_loss: 1.38377905, g_loss: 0.69265950, ae_loss: 0.05288044\n",
      "Step: [2680] total_loss: 2.13061833 d_loss: 1.38445866, g_loss: 0.69566333, ae_loss: 0.05049638\n",
      "Step: [2681] total_loss: 2.12360930 d_loss: 1.37692475, g_loss: 0.69445109, ae_loss: 0.05223349\n",
      "Step: [2682] total_loss: 2.14362216 d_loss: 1.40590882, g_loss: 0.68597490, ae_loss: 0.05173835\n",
      "Step: [2683] total_loss: 2.11187863 d_loss: 1.37591982, g_loss: 0.68430746, ae_loss: 0.05165141\n",
      "Step: [2684] total_loss: 2.13624430 d_loss: 1.40181160, g_loss: 0.68023968, ae_loss: 0.05419317\n",
      "Step: [2685] total_loss: 2.11739707 d_loss: 1.38268661, g_loss: 0.68644178, ae_loss: 0.04826872\n",
      "Step: [2686] total_loss: 2.12367392 d_loss: 1.37803352, g_loss: 0.69053352, ae_loss: 0.05510699\n",
      "Step: [2687] total_loss: 2.12368202 d_loss: 1.37801600, g_loss: 0.69362432, ae_loss: 0.05204166\n",
      "Step: [2688] total_loss: 2.15078259 d_loss: 1.40243709, g_loss: 0.69737124, ae_loss: 0.05097428\n",
      "Step: [2689] total_loss: 2.13793015 d_loss: 1.39111972, g_loss: 0.69364172, ae_loss: 0.05316866\n",
      "Step: [2690] total_loss: 2.13839531 d_loss: 1.39784575, g_loss: 0.69141495, ae_loss: 0.04913448\n",
      "Step: [2691] total_loss: 2.12103224 d_loss: 1.38305652, g_loss: 0.68726337, ae_loss: 0.05071227\n",
      "Step: [2692] total_loss: 2.12406445 d_loss: 1.37222314, g_loss: 0.69943297, ae_loss: 0.05240823\n",
      "Step: [2693] total_loss: 2.12838936 d_loss: 1.38667929, g_loss: 0.68952125, ae_loss: 0.05218886\n",
      "Step: [2694] total_loss: 2.12633896 d_loss: 1.39549243, g_loss: 0.68195760, ae_loss: 0.04888885\n",
      "Step: [2695] total_loss: 2.12531471 d_loss: 1.37182581, g_loss: 0.70435822, ae_loss: 0.04913073\n",
      "Step: [2696] total_loss: 2.13247252 d_loss: 1.38348460, g_loss: 0.69529402, ae_loss: 0.05369383\n",
      "Step: [2697] total_loss: 2.12375975 d_loss: 1.39093566, g_loss: 0.68218851, ae_loss: 0.05063558\n",
      "Step: [2698] total_loss: 2.12179041 d_loss: 1.36245167, g_loss: 0.70574510, ae_loss: 0.05359375\n",
      "Step: [2699] total_loss: 2.13328195 d_loss: 1.39500630, g_loss: 0.68502200, ae_loss: 0.05325365\n",
      "Step: [2700] total_loss: 2.14431167 d_loss: 1.39937139, g_loss: 0.68935233, ae_loss: 0.05558791\n",
      "Step: [2701] total_loss: 2.11465359 d_loss: 1.39244819, g_loss: 0.66814739, ae_loss: 0.05405791\n",
      "Step: [2702] total_loss: 2.13061309 d_loss: 1.38727403, g_loss: 0.69512284, ae_loss: 0.04821622\n",
      "Step: [2703] total_loss: 2.11680031 d_loss: 1.38171387, g_loss: 0.68389350, ae_loss: 0.05119303\n",
      "Step: [2704] total_loss: 2.12789679 d_loss: 1.39407992, g_loss: 0.68280578, ae_loss: 0.05101112\n",
      "Step: [2705] total_loss: 2.13310933 d_loss: 1.40053034, g_loss: 0.68166459, ae_loss: 0.05091443\n",
      "Step: [2706] total_loss: 2.13858652 d_loss: 1.40548873, g_loss: 0.68297851, ae_loss: 0.05011917\n",
      "Step: [2707] total_loss: 2.13709474 d_loss: 1.38864541, g_loss: 0.69424438, ae_loss: 0.05420498\n",
      "Step: [2708] total_loss: 2.11583710 d_loss: 1.36779189, g_loss: 0.70013559, ae_loss: 0.04790963\n",
      "Step: [2709] total_loss: 2.13173008 d_loss: 1.38371110, g_loss: 0.69419837, ae_loss: 0.05382063\n",
      "Step: [2710] total_loss: 2.12958407 d_loss: 1.38294375, g_loss: 0.69226354, ae_loss: 0.05437674\n",
      "Step: [2711] total_loss: 2.13200283 d_loss: 1.36958325, g_loss: 0.71177149, ae_loss: 0.05064809\n",
      "Step: [2712] total_loss: 2.14307690 d_loss: 1.38659728, g_loss: 0.70629531, ae_loss: 0.05018425\n",
      "Step: [2713] total_loss: 2.11712861 d_loss: 1.36706185, g_loss: 0.69882226, ae_loss: 0.05124448\n",
      "Step: [2714] total_loss: 2.14062691 d_loss: 1.39322865, g_loss: 0.69664639, ae_loss: 0.05075182\n",
      "Step: [2715] total_loss: 2.12297392 d_loss: 1.35963035, g_loss: 0.71372026, ae_loss: 0.04962333\n",
      "Step: [2716] total_loss: 2.12794590 d_loss: 1.39148426, g_loss: 0.68366945, ae_loss: 0.05279217\n",
      "Step: [2717] total_loss: 2.12541676 d_loss: 1.38391900, g_loss: 0.68600070, ae_loss: 0.05549690\n",
      "Step: [2718] total_loss: 2.12324858 d_loss: 1.37313664, g_loss: 0.69321990, ae_loss: 0.05689195\n",
      "Step: [2719] total_loss: 2.08969355 d_loss: 1.35646796, g_loss: 0.68246204, ae_loss: 0.05076356\n",
      "Step: [2720] total_loss: 2.11091590 d_loss: 1.38177705, g_loss: 0.67648864, ae_loss: 0.05265017\n",
      "Step: [2721] total_loss: 2.11267710 d_loss: 1.37611127, g_loss: 0.68292367, ae_loss: 0.05364203\n",
      "Step: [2722] total_loss: 2.14425588 d_loss: 1.40965414, g_loss: 0.68064064, ae_loss: 0.05396110\n",
      "Step: [2723] total_loss: 2.12290597 d_loss: 1.37678099, g_loss: 0.69762516, ae_loss: 0.04849982\n",
      "Step: [2724] total_loss: 2.14445972 d_loss: 1.40583432, g_loss: 0.68739903, ae_loss: 0.05122644\n",
      "Step: [2725] total_loss: 2.11611652 d_loss: 1.37454438, g_loss: 0.68959880, ae_loss: 0.05197340\n",
      "Step: [2726] total_loss: 2.15057349 d_loss: 1.38921571, g_loss: 0.70647776, ae_loss: 0.05488005\n",
      "Step: [2727] total_loss: 2.13904929 d_loss: 1.38231492, g_loss: 0.69824612, ae_loss: 0.05848827\n",
      "Step: [2728] total_loss: 2.13092899 d_loss: 1.38966441, g_loss: 0.69546950, ae_loss: 0.04579504\n",
      "Step: [2729] total_loss: 2.13394833 d_loss: 1.38148832, g_loss: 0.70382965, ae_loss: 0.04863051\n",
      "Step: [2730] total_loss: 2.12910080 d_loss: 1.37533569, g_loss: 0.70194632, ae_loss: 0.05181888\n",
      "Step: [2731] total_loss: 2.10967875 d_loss: 1.36255181, g_loss: 0.69424391, ae_loss: 0.05288289\n",
      "Step: [2732] total_loss: 2.12240624 d_loss: 1.38908696, g_loss: 0.68060398, ae_loss: 0.05271535\n",
      "Step: [2733] total_loss: 2.10893321 d_loss: 1.38045692, g_loss: 0.67650199, ae_loss: 0.05197432\n",
      "Step: [2734] total_loss: 2.13322377 d_loss: 1.39985013, g_loss: 0.67773950, ae_loss: 0.05563412\n",
      "Step: [2735] total_loss: 2.14977694 d_loss: 1.41182351, g_loss: 0.68647909, ae_loss: 0.05147448\n",
      "Step: [2736] total_loss: 2.14063931 d_loss: 1.38867950, g_loss: 0.70385313, ae_loss: 0.04810679\n",
      "Step: [2737] total_loss: 2.13261986 d_loss: 1.38767695, g_loss: 0.69263333, ae_loss: 0.05230959\n",
      "Step: [2738] total_loss: 2.12489915 d_loss: 1.37854886, g_loss: 0.69307870, ae_loss: 0.05327170\n",
      "Step: [2739] total_loss: 2.10823250 d_loss: 1.36191845, g_loss: 0.68951857, ae_loss: 0.05679561\n",
      "Step: [2740] total_loss: 2.11520052 d_loss: 1.37712955, g_loss: 0.68483400, ae_loss: 0.05323697\n",
      "Step: [2741] total_loss: 2.12239289 d_loss: 1.37743592, g_loss: 0.69441473, ae_loss: 0.05054227\n",
      "Step: [2742] total_loss: 2.13135195 d_loss: 1.39180923, g_loss: 0.68615514, ae_loss: 0.05338750\n",
      "Step: [2743] total_loss: 2.13025928 d_loss: 1.37192917, g_loss: 0.70458120, ae_loss: 0.05374879\n",
      "Step: [2744] total_loss: 2.13618088 d_loss: 1.38381457, g_loss: 0.69818938, ae_loss: 0.05417684\n",
      "Step: [2745] total_loss: 2.13214517 d_loss: 1.37084627, g_loss: 0.70927876, ae_loss: 0.05202020\n",
      "Step: [2746] total_loss: 2.13801599 d_loss: 1.40007973, g_loss: 0.68627423, ae_loss: 0.05166201\n",
      "Step: [2747] total_loss: 2.14131927 d_loss: 1.40589857, g_loss: 0.68515849, ae_loss: 0.05026234\n",
      "Step: [2748] total_loss: 2.11918664 d_loss: 1.37817061, g_loss: 0.68964583, ae_loss: 0.05137022\n",
      "Step: [2749] total_loss: 2.12287784 d_loss: 1.38014090, g_loss: 0.69312876, ae_loss: 0.04960818\n",
      "Step: [2750] total_loss: 2.10155439 d_loss: 1.35283327, g_loss: 0.69508886, ae_loss: 0.05363231\n",
      "Step: [2751] total_loss: 2.11790156 d_loss: 1.37527442, g_loss: 0.68769586, ae_loss: 0.05493130\n",
      "Step: [2752] total_loss: 2.11539364 d_loss: 1.38511443, g_loss: 0.68262851, ae_loss: 0.04765056\n",
      "Step: [2753] total_loss: 2.15628767 d_loss: 1.42221069, g_loss: 0.68403792, ae_loss: 0.05003899\n",
      "Step: [2754] total_loss: 2.11749029 d_loss: 1.37600684, g_loss: 0.68907976, ae_loss: 0.05240365\n",
      "Step: [2755] total_loss: 2.13612938 d_loss: 1.39894676, g_loss: 0.68316543, ae_loss: 0.05401734\n",
      "Step: [2756] total_loss: 2.11930704 d_loss: 1.38445652, g_loss: 0.68524897, ae_loss: 0.04960157\n",
      "Step: [2757] total_loss: 2.13431025 d_loss: 1.38226795, g_loss: 0.69993138, ae_loss: 0.05211076\n",
      "Step: [2758] total_loss: 2.13797045 d_loss: 1.38656473, g_loss: 0.69657624, ae_loss: 0.05482961\n",
      "Step: [2759] total_loss: 2.13167000 d_loss: 1.37809515, g_loss: 0.70474231, ae_loss: 0.04883237\n",
      "Step: [2760] total_loss: 2.14652872 d_loss: 1.39489985, g_loss: 0.70199716, ae_loss: 0.04963168\n",
      "Step: [2761] total_loss: 2.14186811 d_loss: 1.39388430, g_loss: 0.69584072, ae_loss: 0.05214300\n",
      "Step: [2762] total_loss: 2.13551569 d_loss: 1.39022851, g_loss: 0.69337416, ae_loss: 0.05191287\n",
      "Step: [2763] total_loss: 2.12372780 d_loss: 1.38548231, g_loss: 0.68892026, ae_loss: 0.04932529\n",
      "Step: [2764] total_loss: 2.12864399 d_loss: 1.39051747, g_loss: 0.69045985, ae_loss: 0.04766657\n",
      "Step: [2765] total_loss: 2.13371277 d_loss: 1.39461875, g_loss: 0.68658888, ae_loss: 0.05250526\n",
      "Step: [2766] total_loss: 2.11196709 d_loss: 1.37309515, g_loss: 0.68579495, ae_loss: 0.05307694\n",
      "Step: [2767] total_loss: 2.13197494 d_loss: 1.39010382, g_loss: 0.69240725, ae_loss: 0.04946383\n",
      "Step: [2768] total_loss: 2.14463043 d_loss: 1.38405275, g_loss: 0.70751846, ae_loss: 0.05305929\n",
      "Step: [2769] total_loss: 2.13765097 d_loss: 1.39285779, g_loss: 0.69200945, ae_loss: 0.05278378\n",
      "Step: [2770] total_loss: 2.13362169 d_loss: 1.38877034, g_loss: 0.69371700, ae_loss: 0.05113440\n",
      "Step: [2771] total_loss: 2.11570644 d_loss: 1.37945867, g_loss: 0.68381113, ae_loss: 0.05243663\n",
      "Step: [2772] total_loss: 2.11212873 d_loss: 1.37132120, g_loss: 0.68794799, ae_loss: 0.05285970\n",
      "Step: [2773] total_loss: 2.11388278 d_loss: 1.38431001, g_loss: 0.67976516, ae_loss: 0.04980771\n",
      "Step: [2774] total_loss: 2.12018800 d_loss: 1.38129687, g_loss: 0.68262947, ae_loss: 0.05626161\n",
      "Step: [2775] total_loss: 2.09998178 d_loss: 1.38361001, g_loss: 0.66531998, ae_loss: 0.05105184\n",
      "Step: [2776] total_loss: 2.10736442 d_loss: 1.38330507, g_loss: 0.67374635, ae_loss: 0.05031301\n",
      "Step: [2777] total_loss: 2.12578201 d_loss: 1.38429284, g_loss: 0.68955284, ae_loss: 0.05193623\n",
      "Step: [2778] total_loss: 2.15468025 d_loss: 1.39825058, g_loss: 0.70512629, ae_loss: 0.05130321\n",
      "Step: [2779] total_loss: 2.12940478 d_loss: 1.39302349, g_loss: 0.68759859, ae_loss: 0.04878273\n",
      "Step: [2780] total_loss: 2.13701248 d_loss: 1.38500392, g_loss: 0.70072216, ae_loss: 0.05128636\n",
      "Step: [2781] total_loss: 2.11946416 d_loss: 1.38980532, g_loss: 0.68018633, ae_loss: 0.04947245\n",
      "Step: [2782] total_loss: 2.13749576 d_loss: 1.39709008, g_loss: 0.68601203, ae_loss: 0.05439371\n",
      "Step: [2783] total_loss: 2.12307167 d_loss: 1.38201749, g_loss: 0.68963563, ae_loss: 0.05141838\n",
      "Step: [2784] total_loss: 2.11787605 d_loss: 1.38364506, g_loss: 0.68684733, ae_loss: 0.04738358\n",
      "Step: [2785] total_loss: 2.11680746 d_loss: 1.37616682, g_loss: 0.68941283, ae_loss: 0.05122766\n",
      "Step: [2786] total_loss: 2.13065863 d_loss: 1.37300086, g_loss: 0.70655888, ae_loss: 0.05109880\n",
      "Step: [2787] total_loss: 2.11788082 d_loss: 1.40224564, g_loss: 0.66283989, ae_loss: 0.05279520\n",
      "Step: [2788] total_loss: 2.11841893 d_loss: 1.37825418, g_loss: 0.69368571, ae_loss: 0.04647914\n",
      "Step: [2789] total_loss: 2.11721635 d_loss: 1.38750505, g_loss: 0.67747486, ae_loss: 0.05223640\n",
      "Step: [2790] total_loss: 2.11988878 d_loss: 1.38276601, g_loss: 0.68098664, ae_loss: 0.05613612\n",
      "Step: [2791] total_loss: 2.10394287 d_loss: 1.37573040, g_loss: 0.67941773, ae_loss: 0.04879488\n",
      "Step: [2792] total_loss: 2.12891388 d_loss: 1.38759303, g_loss: 0.69364679, ae_loss: 0.04767414\n",
      "Step: [2793] total_loss: 2.11691141 d_loss: 1.36919522, g_loss: 0.70009017, ae_loss: 0.04762604\n",
      "Step: [2794] total_loss: 2.13315535 d_loss: 1.38182890, g_loss: 0.70325994, ae_loss: 0.04806656\n",
      "Step: [2795] total_loss: 2.15585518 d_loss: 1.39008927, g_loss: 0.71511042, ae_loss: 0.05065531\n",
      "Step: [2796] total_loss: 2.13404918 d_loss: 1.37493110, g_loss: 0.70623207, ae_loss: 0.05288605\n",
      "Step: [2797] total_loss: 2.13332558 d_loss: 1.38378024, g_loss: 0.69816327, ae_loss: 0.05138211\n",
      "Step: [2798] total_loss: 2.13387060 d_loss: 1.37932098, g_loss: 0.70148337, ae_loss: 0.05306633\n",
      "Step: [2799] total_loss: 2.14045429 d_loss: 1.38775420, g_loss: 0.69986182, ae_loss: 0.05283825\n",
      "Step: [2800] total_loss: 2.12562609 d_loss: 1.36909223, g_loss: 0.70217586, ae_loss: 0.05435790\n",
      "Step: [2801] total_loss: 2.14700270 d_loss: 1.40086985, g_loss: 0.69929892, ae_loss: 0.04683385\n",
      "Step: [2802] total_loss: 2.11759281 d_loss: 1.36466527, g_loss: 0.69722611, ae_loss: 0.05570132\n",
      "Step: [2803] total_loss: 2.14662910 d_loss: 1.37794113, g_loss: 0.71714586, ae_loss: 0.05154200\n",
      "Step: [2804] total_loss: 2.13534117 d_loss: 1.36418509, g_loss: 0.71823335, ae_loss: 0.05292280\n",
      "Step: [2805] total_loss: 2.14820075 d_loss: 1.40608299, g_loss: 0.69224006, ae_loss: 0.04987764\n",
      "Step: [2806] total_loss: 2.13671708 d_loss: 1.38021934, g_loss: 0.70557320, ae_loss: 0.05092449\n",
      "Step: [2807] total_loss: 2.11891556 d_loss: 1.38548303, g_loss: 0.68303204, ae_loss: 0.05040057\n",
      "Step: [2808] total_loss: 2.12339211 d_loss: 1.36995459, g_loss: 0.70268452, ae_loss: 0.05075290\n",
      "Step: [2809] total_loss: 2.13385582 d_loss: 1.40400028, g_loss: 0.67977077, ae_loss: 0.05008467\n",
      "Step: [2810] total_loss: 2.14221025 d_loss: 1.38286602, g_loss: 0.71438205, ae_loss: 0.04496218\n",
      "Step: [2811] total_loss: 2.11856222 d_loss: 1.38311505, g_loss: 0.68575883, ae_loss: 0.04968832\n",
      "Step: [2812] total_loss: 2.11919308 d_loss: 1.38350928, g_loss: 0.68508482, ae_loss: 0.05059881\n",
      "Step: [2813] total_loss: 2.11679554 d_loss: 1.38274670, g_loss: 0.68409777, ae_loss: 0.04995092\n",
      "Step: [2814] total_loss: 2.13525295 d_loss: 1.39253461, g_loss: 0.68980831, ae_loss: 0.05291000\n",
      "Step: [2815] total_loss: 2.11818504 d_loss: 1.36317158, g_loss: 0.70478582, ae_loss: 0.05022764\n",
      "Step: [2816] total_loss: 2.13105392 d_loss: 1.39245033, g_loss: 0.68678296, ae_loss: 0.05182072\n",
      "Step: [2817] total_loss: 2.15051556 d_loss: 1.41132474, g_loss: 0.68459952, ae_loss: 0.05459141\n",
      "Step: [2818] total_loss: 2.14699173 d_loss: 1.38708758, g_loss: 0.70623052, ae_loss: 0.05367361\n",
      "Step: [2819] total_loss: 2.13498950 d_loss: 1.38595593, g_loss: 0.69635111, ae_loss: 0.05268255\n",
      "Step: [2820] total_loss: 2.13918066 d_loss: 1.38448381, g_loss: 0.70354724, ae_loss: 0.05114957\n",
      "Step: [2821] total_loss: 2.11549520 d_loss: 1.37411022, g_loss: 0.69125676, ae_loss: 0.05012839\n",
      "Step: [2822] total_loss: 2.13967037 d_loss: 1.39401472, g_loss: 0.69607621, ae_loss: 0.04957933\n",
      "Step: [2823] total_loss: 2.11911345 d_loss: 1.38290358, g_loss: 0.68263078, ae_loss: 0.05357908\n",
      "Step: [2824] total_loss: 2.12012482 d_loss: 1.37981534, g_loss: 0.68972188, ae_loss: 0.05058764\n",
      "Step: [2825] total_loss: 2.12294340 d_loss: 1.36891341, g_loss: 0.70288879, ae_loss: 0.05114122\n",
      "Step: [2826] total_loss: 2.13060999 d_loss: 1.37784779, g_loss: 0.69760489, ae_loss: 0.05515717\n",
      "Step: [2827] total_loss: 2.13450980 d_loss: 1.39969277, g_loss: 0.68610764, ae_loss: 0.04870936\n",
      "Step: [2828] total_loss: 2.13699746 d_loss: 1.39278603, g_loss: 0.68829012, ae_loss: 0.05592127\n",
      "Step: [2829] total_loss: 2.14323258 d_loss: 1.40957427, g_loss: 0.68355215, ae_loss: 0.05010616\n",
      "Step: [2830] total_loss: 2.12732792 d_loss: 1.37435055, g_loss: 0.70353103, ae_loss: 0.04944628\n",
      "Step: [2831] total_loss: 2.12655497 d_loss: 1.39469564, g_loss: 0.67906737, ae_loss: 0.05279204\n",
      "Step: [2832] total_loss: 2.12506533 d_loss: 1.38235545, g_loss: 0.69024944, ae_loss: 0.05246027\n",
      "Step: [2833] total_loss: 2.13182902 d_loss: 1.38507080, g_loss: 0.69453245, ae_loss: 0.05222584\n",
      "Step: [2834] total_loss: 2.11526155 d_loss: 1.38436747, g_loss: 0.67742598, ae_loss: 0.05346796\n",
      "Step: [2835] total_loss: 2.12380719 d_loss: 1.37687469, g_loss: 0.69576907, ae_loss: 0.05116346\n",
      "Step: [2836] total_loss: 2.11672306 d_loss: 1.38373542, g_loss: 0.68585336, ae_loss: 0.04713436\n",
      "Step: [2837] total_loss: 2.13320589 d_loss: 1.38909280, g_loss: 0.68555975, ae_loss: 0.05855319\n",
      "Step: [2838] total_loss: 2.11478114 d_loss: 1.38046730, g_loss: 0.68480754, ae_loss: 0.04950634\n",
      "Step: [2839] total_loss: 2.11102057 d_loss: 1.37117255, g_loss: 0.68529594, ae_loss: 0.05455218\n",
      "Step: [2840] total_loss: 2.11812901 d_loss: 1.37802458, g_loss: 0.69135684, ae_loss: 0.04874752\n",
      "Step: [2841] total_loss: 2.12387562 d_loss: 1.37533486, g_loss: 0.69554889, ae_loss: 0.05299169\n",
      "Step: [2842] total_loss: 2.11558437 d_loss: 1.36755955, g_loss: 0.69952285, ae_loss: 0.04850212\n",
      "Step: [2843] total_loss: 2.12613893 d_loss: 1.36535072, g_loss: 0.70981985, ae_loss: 0.05096842\n",
      "Step: [2844] total_loss: 2.12825632 d_loss: 1.38092005, g_loss: 0.69438493, ae_loss: 0.05295151\n",
      "Step: [2845] total_loss: 2.12546682 d_loss: 1.37182498, g_loss: 0.69940484, ae_loss: 0.05423686\n",
      "Step: [2846] total_loss: 2.13354445 d_loss: 1.38232708, g_loss: 0.70360053, ae_loss: 0.04761675\n",
      "Step: [2847] total_loss: 2.11450052 d_loss: 1.37695909, g_loss: 0.68693691, ae_loss: 0.05060446\n",
      "Step: [2848] total_loss: 2.12401891 d_loss: 1.37237000, g_loss: 0.70034039, ae_loss: 0.05130851\n",
      "Step: [2849] total_loss: 2.12159538 d_loss: 1.37277246, g_loss: 0.69540954, ae_loss: 0.05341322\n",
      "Step: [2850] total_loss: 2.14332652 d_loss: 1.39563942, g_loss: 0.69464564, ae_loss: 0.05304140\n",
      "Step: [2851] total_loss: 2.14686584 d_loss: 1.39821362, g_loss: 0.69330955, ae_loss: 0.05534270\n",
      "Step: [2852] total_loss: 2.15082431 d_loss: 1.40207529, g_loss: 0.69997299, ae_loss: 0.04877600\n",
      "Step: [2853] total_loss: 2.13795233 d_loss: 1.39407837, g_loss: 0.69101429, ae_loss: 0.05285950\n",
      "Step: [2854] total_loss: 2.12330484 d_loss: 1.37756085, g_loss: 0.69750661, ae_loss: 0.04823742\n",
      "Step: [2855] total_loss: 2.12591481 d_loss: 1.37581205, g_loss: 0.69543940, ae_loss: 0.05466345\n",
      "Step: [2856] total_loss: 2.12542272 d_loss: 1.37120199, g_loss: 0.70552385, ae_loss: 0.04869682\n",
      "Step: [2857] total_loss: 2.12877750 d_loss: 1.39184618, g_loss: 0.68580395, ae_loss: 0.05112744\n",
      "Step: [2858] total_loss: 2.11739588 d_loss: 1.38692427, g_loss: 0.68271697, ae_loss: 0.04775478\n",
      "Step: [2859] total_loss: 2.10777497 d_loss: 1.35643387, g_loss: 0.70277673, ae_loss: 0.04856431\n",
      "Step: [2860] total_loss: 2.14589953 d_loss: 1.39006281, g_loss: 0.70440555, ae_loss: 0.05143120\n",
      "Step: [2861] total_loss: 2.14623642 d_loss: 1.37062907, g_loss: 0.72156096, ae_loss: 0.05404622\n",
      "Step: [2862] total_loss: 2.13374424 d_loss: 1.37986088, g_loss: 0.70333362, ae_loss: 0.05054972\n",
      "Step: [2863] total_loss: 2.15216255 d_loss: 1.39221573, g_loss: 0.70308399, ae_loss: 0.05686274\n",
      "Step: [2864] total_loss: 2.13295150 d_loss: 1.38145947, g_loss: 0.69898045, ae_loss: 0.05251160\n",
      "Step: [2865] total_loss: 2.12633276 d_loss: 1.37624741, g_loss: 0.70032853, ae_loss: 0.04975681\n",
      "Step: [2866] total_loss: 2.11336803 d_loss: 1.38867664, g_loss: 0.67013836, ae_loss: 0.05455317\n",
      "Step: [2867] total_loss: 2.11676431 d_loss: 1.38850260, g_loss: 0.67263305, ae_loss: 0.05562865\n",
      "Step: [2868] total_loss: 2.11993742 d_loss: 1.37893820, g_loss: 0.68788230, ae_loss: 0.05311700\n",
      "Step: [2869] total_loss: 2.10744715 d_loss: 1.35381651, g_loss: 0.70685852, ae_loss: 0.04677201\n",
      "Step: [2870] total_loss: 2.12085533 d_loss: 1.35683692, g_loss: 0.70646131, ae_loss: 0.05755701\n",
      "Step: [2871] total_loss: 2.12702823 d_loss: 1.38053358, g_loss: 0.69524872, ae_loss: 0.05124598\n",
      "Step: [2872] total_loss: 2.12833047 d_loss: 1.37160230, g_loss: 0.70609951, ae_loss: 0.05062862\n",
      "Step: [2873] total_loss: 2.13743782 d_loss: 1.38505042, g_loss: 0.70071155, ae_loss: 0.05167583\n",
      "Step: [2874] total_loss: 2.12893748 d_loss: 1.39282286, g_loss: 0.68409795, ae_loss: 0.05201670\n",
      "Step: [2875] total_loss: 2.12680101 d_loss: 1.38662219, g_loss: 0.69067997, ae_loss: 0.04949875\n",
      "Step: [2876] total_loss: 2.14145803 d_loss: 1.40184093, g_loss: 0.68600768, ae_loss: 0.05360945\n",
      "Step: [2877] total_loss: 2.10585594 d_loss: 1.35525000, g_loss: 0.70076716, ae_loss: 0.04983882\n",
      "Step: [2878] total_loss: 2.11480904 d_loss: 1.38052356, g_loss: 0.68170702, ae_loss: 0.05257846\n",
      "Step: [2879] total_loss: 2.10497665 d_loss: 1.37090433, g_loss: 0.68046850, ae_loss: 0.05360385\n",
      "Step: [2880] total_loss: 2.10480165 d_loss: 1.35640740, g_loss: 0.69914824, ae_loss: 0.04924606\n",
      "Step: [2881] total_loss: 2.13948083 d_loss: 1.36946654, g_loss: 0.71242982, ae_loss: 0.05758456\n",
      "Step: [2882] total_loss: 2.13022184 d_loss: 1.36132574, g_loss: 0.71787548, ae_loss: 0.05102072\n",
      "Step: [2883] total_loss: 2.13958812 d_loss: 1.39729929, g_loss: 0.69241333, ae_loss: 0.04987554\n",
      "Step: [2884] total_loss: 2.13747311 d_loss: 1.39456892, g_loss: 0.69230509, ae_loss: 0.05059924\n",
      "Step: [2885] total_loss: 2.11530256 d_loss: 1.39005816, g_loss: 0.67194164, ae_loss: 0.05330270\n",
      "Step: [2886] total_loss: 2.13716722 d_loss: 1.38676500, g_loss: 0.70154446, ae_loss: 0.04885766\n",
      "Step: [2887] total_loss: 2.12448812 d_loss: 1.38688886, g_loss: 0.68614650, ae_loss: 0.05145273\n",
      "Step: [2888] total_loss: 2.14139557 d_loss: 1.37926006, g_loss: 0.70845866, ae_loss: 0.05367667\n",
      "Step: [2889] total_loss: 2.10611582 d_loss: 1.37311840, g_loss: 0.68398404, ae_loss: 0.04901325\n",
      "Step: [2890] total_loss: 2.13333058 d_loss: 1.40680242, g_loss: 0.67410070, ae_loss: 0.05242748\n",
      "Step: [2891] total_loss: 2.12213778 d_loss: 1.36776447, g_loss: 0.70391607, ae_loss: 0.05045719\n",
      "Step: [2892] total_loss: 2.13960218 d_loss: 1.39453328, g_loss: 0.69084418, ae_loss: 0.05422482\n",
      "Step: [2893] total_loss: 2.11811519 d_loss: 1.37002075, g_loss: 0.69616741, ae_loss: 0.05192713\n",
      "Step: [2894] total_loss: 2.12872362 d_loss: 1.39142692, g_loss: 0.68367338, ae_loss: 0.05362316\n",
      "Step: [2895] total_loss: 2.16354990 d_loss: 1.43180287, g_loss: 0.67703855, ae_loss: 0.05470848\n",
      "Step: [2896] total_loss: 2.12259769 d_loss: 1.36727691, g_loss: 0.70135993, ae_loss: 0.05396081\n",
      "Step: [2897] total_loss: 2.12516975 d_loss: 1.38271630, g_loss: 0.69064796, ae_loss: 0.05180554\n",
      "Step: [2898] total_loss: 2.13797331 d_loss: 1.37770379, g_loss: 0.70747054, ae_loss: 0.05279890\n",
      "Step: [2899] total_loss: 2.12677646 d_loss: 1.39339614, g_loss: 0.68217492, ae_loss: 0.05120537\n",
      "Step: [2900] total_loss: 2.13316321 d_loss: 1.39961445, g_loss: 0.68076545, ae_loss: 0.05278334\n",
      "Step: [2901] total_loss: 2.11912727 d_loss: 1.37161005, g_loss: 0.69678909, ae_loss: 0.05072808\n",
      "Step: [2902] total_loss: 2.11759877 d_loss: 1.37838292, g_loss: 0.68987513, ae_loss: 0.04934074\n",
      "Step: [2903] total_loss: 2.13848019 d_loss: 1.39612770, g_loss: 0.68977320, ae_loss: 0.05257935\n",
      "Step: [2904] total_loss: 2.12299633 d_loss: 1.39055538, g_loss: 0.68524259, ae_loss: 0.04719833\n",
      "Step: [2905] total_loss: 2.13835716 d_loss: 1.41365182, g_loss: 0.67330039, ae_loss: 0.05140494\n",
      "Step: [2906] total_loss: 2.12444186 d_loss: 1.39599085, g_loss: 0.68166250, ae_loss: 0.04678850\n",
      "Step: [2907] total_loss: 2.11840534 d_loss: 1.37693477, g_loss: 0.69147956, ae_loss: 0.04999090\n",
      "Step: [2908] total_loss: 2.12922215 d_loss: 1.39403677, g_loss: 0.68186772, ae_loss: 0.05331766\n",
      "Step: [2909] total_loss: 2.12599182 d_loss: 1.38256907, g_loss: 0.69299257, ae_loss: 0.05043009\n",
      "Step: [2910] total_loss: 2.13735151 d_loss: 1.37438226, g_loss: 0.71283436, ae_loss: 0.05013490\n",
      "Step: [2911] total_loss: 2.11306429 d_loss: 1.37664437, g_loss: 0.68264103, ae_loss: 0.05377879\n",
      "Step: [2912] total_loss: 2.10642672 d_loss: 1.37095475, g_loss: 0.68701178, ae_loss: 0.04846017\n",
      "Step: [2913] total_loss: 2.12458229 d_loss: 1.37601018, g_loss: 0.69729716, ae_loss: 0.05127490\n",
      "Step: [2914] total_loss: 2.11404991 d_loss: 1.37246108, g_loss: 0.68469608, ae_loss: 0.05689286\n",
      "Step: [2915] total_loss: 2.13806391 d_loss: 1.39071596, g_loss: 0.69358647, ae_loss: 0.05376131\n",
      "Step: [2916] total_loss: 2.13998413 d_loss: 1.40361714, g_loss: 0.68479514, ae_loss: 0.05157194\n",
      "Step: [2917] total_loss: 2.12399602 d_loss: 1.37737751, g_loss: 0.69343531, ae_loss: 0.05318315\n",
      "Step: [2918] total_loss: 2.12459803 d_loss: 1.36675787, g_loss: 0.70497620, ae_loss: 0.05286385\n",
      "Step: [2919] total_loss: 2.13556814 d_loss: 1.39415741, g_loss: 0.68647337, ae_loss: 0.05493754\n",
      "Step: [2920] total_loss: 2.11077356 d_loss: 1.37484252, g_loss: 0.68500865, ae_loss: 0.05092242\n",
      "Step: [2921] total_loss: 2.12060213 d_loss: 1.37238872, g_loss: 0.69565439, ae_loss: 0.05255901\n",
      "Step: [2922] total_loss: 2.12519693 d_loss: 1.38351810, g_loss: 0.69132984, ae_loss: 0.05034900\n",
      "Step: [2923] total_loss: 2.12339497 d_loss: 1.36299586, g_loss: 0.70587921, ae_loss: 0.05451999\n",
      "Step: [2924] total_loss: 2.12941694 d_loss: 1.38154411, g_loss: 0.69667798, ae_loss: 0.05119476\n",
      "Step: [2925] total_loss: 2.12860942 d_loss: 1.39622569, g_loss: 0.68081391, ae_loss: 0.05156986\n",
      "Step: [2926] total_loss: 2.10717773 d_loss: 1.37644291, g_loss: 0.67868459, ae_loss: 0.05205008\n",
      "Step: [2927] total_loss: 2.13418102 d_loss: 1.37538075, g_loss: 0.70698190, ae_loss: 0.05181855\n",
      "Step: [2928] total_loss: 2.11727405 d_loss: 1.37546468, g_loss: 0.69029152, ae_loss: 0.05151780\n",
      "Step: [2929] total_loss: 2.13074541 d_loss: 1.38412642, g_loss: 0.69168770, ae_loss: 0.05493142\n",
      "Step: [2930] total_loss: 2.11796856 d_loss: 1.38372540, g_loss: 0.68284810, ae_loss: 0.05139497\n",
      "Step: [2931] total_loss: 2.11494040 d_loss: 1.37171757, g_loss: 0.69633347, ae_loss: 0.04688935\n",
      "Step: [2932] total_loss: 2.12328243 d_loss: 1.38094807, g_loss: 0.68836725, ae_loss: 0.05396728\n",
      "Step: [2933] total_loss: 2.12235594 d_loss: 1.39703369, g_loss: 0.67622745, ae_loss: 0.04909474\n",
      "Step: [2934] total_loss: 2.11641288 d_loss: 1.37573171, g_loss: 0.68647027, ae_loss: 0.05421090\n",
      "Step: [2935] total_loss: 2.14266348 d_loss: 1.40666699, g_loss: 0.68462586, ae_loss: 0.05137054\n",
      "Step: [2936] total_loss: 2.13105059 d_loss: 1.39103699, g_loss: 0.68835783, ae_loss: 0.05165580\n",
      "Step: [2937] total_loss: 2.12936068 d_loss: 1.37632680, g_loss: 0.69976354, ae_loss: 0.05327026\n",
      "Step: [2938] total_loss: 2.14198637 d_loss: 1.37310171, g_loss: 0.71745884, ae_loss: 0.05142584\n",
      "Step: [2939] total_loss: 2.12936449 d_loss: 1.37616539, g_loss: 0.69941926, ae_loss: 0.05377968\n",
      "Step: [2940] total_loss: 2.12475538 d_loss: 1.39431381, g_loss: 0.67936784, ae_loss: 0.05107369\n",
      "Step: [2941] total_loss: 2.14433360 d_loss: 1.40169060, g_loss: 0.69234443, ae_loss: 0.05029852\n",
      "Step: [2942] total_loss: 2.12514091 d_loss: 1.36893940, g_loss: 0.70533174, ae_loss: 0.05086975\n",
      "Step: [2943] total_loss: 2.14114165 d_loss: 1.35909510, g_loss: 0.72709876, ae_loss: 0.05494788\n",
      "Step: [2944] total_loss: 2.13289499 d_loss: 1.39958441, g_loss: 0.68014991, ae_loss: 0.05316059\n",
      "Step: [2945] total_loss: 2.12806368 d_loss: 1.36591077, g_loss: 0.70896888, ae_loss: 0.05318406\n",
      "Step: [2946] total_loss: 2.12473965 d_loss: 1.38489389, g_loss: 0.69214046, ae_loss: 0.04770534\n",
      "Step: [2947] total_loss: 2.12703538 d_loss: 1.37855887, g_loss: 0.69623536, ae_loss: 0.05224123\n",
      "Step: [2948] total_loss: 2.11080694 d_loss: 1.38211679, g_loss: 0.67820990, ae_loss: 0.05048015\n",
      "Step: [2949] total_loss: 2.14705944 d_loss: 1.40397215, g_loss: 0.68862087, ae_loss: 0.05446643\n",
      "Step: [2950] total_loss: 2.13787889 d_loss: 1.39279163, g_loss: 0.68904805, ae_loss: 0.05603906\n",
      "Step: [2951] total_loss: 2.14396000 d_loss: 1.40157664, g_loss: 0.68972898, ae_loss: 0.05265422\n",
      "Step: [2952] total_loss: 2.13251972 d_loss: 1.38519335, g_loss: 0.69157374, ae_loss: 0.05575264\n",
      "Step: [2953] total_loss: 2.12433434 d_loss: 1.36596417, g_loss: 0.70212406, ae_loss: 0.05624610\n",
      "Step: [2954] total_loss: 2.16404819 d_loss: 1.42833328, g_loss: 0.68568265, ae_loss: 0.05003230\n",
      "Step: [2955] total_loss: 2.13226867 d_loss: 1.39345622, g_loss: 0.68620753, ae_loss: 0.05260493\n",
      "Step: [2956] total_loss: 2.10979319 d_loss: 1.38326716, g_loss: 0.67369187, ae_loss: 0.05283422\n",
      "Step: [2957] total_loss: 2.10977364 d_loss: 1.36934733, g_loss: 0.68708396, ae_loss: 0.05334235\n",
      "Step: [2958] total_loss: 2.13161230 d_loss: 1.38088393, g_loss: 0.70005059, ae_loss: 0.05067794\n",
      "Step: [2959] total_loss: 2.13952065 d_loss: 1.39952970, g_loss: 0.68901491, ae_loss: 0.05097621\n",
      "Step: [2960] total_loss: 2.13336182 d_loss: 1.39638758, g_loss: 0.68314821, ae_loss: 0.05382601\n",
      "Step: [2961] total_loss: 2.13678980 d_loss: 1.40502477, g_loss: 0.68381786, ae_loss: 0.04794723\n",
      "Step: [2962] total_loss: 2.14642262 d_loss: 1.39828110, g_loss: 0.69916946, ae_loss: 0.04897215\n",
      "Step: [2963] total_loss: 2.13357592 d_loss: 1.37548161, g_loss: 0.70692390, ae_loss: 0.05117035\n",
      "Step: [2964] total_loss: 2.13309336 d_loss: 1.37595332, g_loss: 0.70496953, ae_loss: 0.05217041\n",
      "Step: [2965] total_loss: 2.11063933 d_loss: 1.37197447, g_loss: 0.68486512, ae_loss: 0.05379980\n",
      "Step: [2966] total_loss: 2.13389587 d_loss: 1.38145423, g_loss: 0.70241952, ae_loss: 0.05002207\n",
      "Step: [2967] total_loss: 2.12412405 d_loss: 1.36709619, g_loss: 0.70006168, ae_loss: 0.05696632\n",
      "Step: [2968] total_loss: 2.13118315 d_loss: 1.37689590, g_loss: 0.69938451, ae_loss: 0.05490266\n",
      "Step: [2969] total_loss: 2.14295745 d_loss: 1.39799452, g_loss: 0.69331104, ae_loss: 0.05165187\n",
      "Step: [2970] total_loss: 2.12529325 d_loss: 1.38007891, g_loss: 0.69630915, ae_loss: 0.04890511\n",
      "Step: [2971] total_loss: 2.11245632 d_loss: 1.36608076, g_loss: 0.69787306, ae_loss: 0.04850248\n",
      "Step: [2972] total_loss: 2.10329342 d_loss: 1.35996175, g_loss: 0.69591796, ae_loss: 0.04741355\n",
      "Step: [2973] total_loss: 2.12228346 d_loss: 1.39515138, g_loss: 0.67690206, ae_loss: 0.05022997\n",
      "Step: [2974] total_loss: 2.11988354 d_loss: 1.38176453, g_loss: 0.68403298, ae_loss: 0.05408595\n",
      "Step: [2975] total_loss: 2.12349033 d_loss: 1.39218080, g_loss: 0.68221438, ae_loss: 0.04909502\n",
      "Step: [2976] total_loss: 2.10418129 d_loss: 1.37140679, g_loss: 0.68283814, ae_loss: 0.04993626\n",
      "Step: [2977] total_loss: 2.12721205 d_loss: 1.39127922, g_loss: 0.68514049, ae_loss: 0.05079243\n",
      "Step: [2978] total_loss: 2.12650251 d_loss: 1.37776625, g_loss: 0.69646740, ae_loss: 0.05226876\n",
      "Step: [2979] total_loss: 2.13587713 d_loss: 1.39551651, g_loss: 0.68892980, ae_loss: 0.05143069\n",
      "Step: [2980] total_loss: 2.14382434 d_loss: 1.39035845, g_loss: 0.70220107, ae_loss: 0.05126489\n",
      "Step: [2981] total_loss: 2.12824178 d_loss: 1.39735103, g_loss: 0.67843080, ae_loss: 0.05245995\n",
      "Step: [2982] total_loss: 2.13201404 d_loss: 1.39435196, g_loss: 0.68701053, ae_loss: 0.05065152\n",
      "Step: [2983] total_loss: 2.13163877 d_loss: 1.37107790, g_loss: 0.70520246, ae_loss: 0.05535845\n",
      "Step: [2984] total_loss: 2.11631584 d_loss: 1.37433839, g_loss: 0.68985105, ae_loss: 0.05212649\n",
      "Step: [2985] total_loss: 2.11911964 d_loss: 1.38611507, g_loss: 0.68071270, ae_loss: 0.05229186\n",
      "Step: [2986] total_loss: 2.13070536 d_loss: 1.37808573, g_loss: 0.69838274, ae_loss: 0.05423700\n",
      "Step: [2987] total_loss: 2.13181877 d_loss: 1.38991821, g_loss: 0.68699574, ae_loss: 0.05490473\n",
      "Step: [2988] total_loss: 2.11622906 d_loss: 1.37978172, g_loss: 0.68711495, ae_loss: 0.04933238\n",
      "Step: [2989] total_loss: 2.12809658 d_loss: 1.39313126, g_loss: 0.68582922, ae_loss: 0.04913605\n",
      "Step: [2990] total_loss: 2.11676025 d_loss: 1.37650955, g_loss: 0.69198501, ae_loss: 0.04826587\n",
      "Step: [2991] total_loss: 2.13489485 d_loss: 1.38951850, g_loss: 0.69512761, ae_loss: 0.05024867\n",
      "Step: [2992] total_loss: 2.11997843 d_loss: 1.37889004, g_loss: 0.69633138, ae_loss: 0.04475706\n",
      "Step: [2993] total_loss: 2.13300037 d_loss: 1.39101863, g_loss: 0.68908858, ae_loss: 0.05289324\n",
      "Step: [2994] total_loss: 2.13563395 d_loss: 1.39585853, g_loss: 0.68809009, ae_loss: 0.05168519\n",
      "Step: [2995] total_loss: 2.10679102 d_loss: 1.36251473, g_loss: 0.69453025, ae_loss: 0.04974607\n",
      "Step: [2996] total_loss: 2.13247919 d_loss: 1.38994122, g_loss: 0.69286138, ae_loss: 0.04967661\n",
      "Step: [2997] total_loss: 2.12340641 d_loss: 1.39015818, g_loss: 0.68295264, ae_loss: 0.05029570\n",
      "Step: [2998] total_loss: 2.12589264 d_loss: 1.38568449, g_loss: 0.68977380, ae_loss: 0.05043426\n",
      "Step: [2999] total_loss: 2.12686443 d_loss: 1.38713169, g_loss: 0.68909788, ae_loss: 0.05063501\n",
      "Step: [3000] total_loss: 2.13899279 d_loss: 1.40229666, g_loss: 0.68845910, ae_loss: 0.04823697\n",
      "Step: [3001] total_loss: 2.13200498 d_loss: 1.37424254, g_loss: 0.70889610, ae_loss: 0.04886628\n",
      "Step: [3002] total_loss: 2.12143755 d_loss: 1.38376057, g_loss: 0.68468356, ae_loss: 0.05299352\n",
      "Step: [3003] total_loss: 2.11437607 d_loss: 1.37912655, g_loss: 0.68420547, ae_loss: 0.05104400\n",
      "Step: [3004] total_loss: 2.12702703 d_loss: 1.39423716, g_loss: 0.68271101, ae_loss: 0.05007880\n",
      "Step: [3005] total_loss: 2.11179924 d_loss: 1.37030768, g_loss: 0.69282079, ae_loss: 0.04867093\n",
      "Step: [3006] total_loss: 2.14913654 d_loss: 1.39937234, g_loss: 0.69919217, ae_loss: 0.05057212\n",
      "Step: [3007] total_loss: 2.13238811 d_loss: 1.38084531, g_loss: 0.70245093, ae_loss: 0.04909189\n",
      "Step: [3008] total_loss: 2.13612652 d_loss: 1.38361669, g_loss: 0.70126808, ae_loss: 0.05124171\n",
      "Step: [3009] total_loss: 2.14675403 d_loss: 1.40056753, g_loss: 0.69592339, ae_loss: 0.05026307\n",
      "Step: [3010] total_loss: 2.14594316 d_loss: 1.39688694, g_loss: 0.69635117, ae_loss: 0.05270502\n",
      "Step: [3011] total_loss: 2.12935925 d_loss: 1.38750744, g_loss: 0.69213414, ae_loss: 0.04971760\n",
      "Step: [3012] total_loss: 2.11723232 d_loss: 1.38806415, g_loss: 0.68121028, ae_loss: 0.04795802\n",
      "Step: [3013] total_loss: 2.12392855 d_loss: 1.39292526, g_loss: 0.67849010, ae_loss: 0.05251315\n",
      "Step: [3014] total_loss: 2.12053204 d_loss: 1.38369966, g_loss: 0.68258905, ae_loss: 0.05424320\n",
      "Step: [3015] total_loss: 2.10980082 d_loss: 1.36504626, g_loss: 0.69210005, ae_loss: 0.05265466\n",
      "Step: [3016] total_loss: 2.13653684 d_loss: 1.38740802, g_loss: 0.69619912, ae_loss: 0.05292959\n",
      "Step: [3017] total_loss: 2.11141109 d_loss: 1.38184929, g_loss: 0.68037212, ae_loss: 0.04918977\n",
      "Step: [3018] total_loss: 2.12714434 d_loss: 1.38376999, g_loss: 0.69394135, ae_loss: 0.04943290\n",
      "Step: [3019] total_loss: 2.12643242 d_loss: 1.38239098, g_loss: 0.69059515, ae_loss: 0.05344620\n",
      "Step: [3020] total_loss: 2.12797689 d_loss: 1.40068030, g_loss: 0.67658913, ae_loss: 0.05070744\n",
      "Step: [3021] total_loss: 2.11724997 d_loss: 1.38132393, g_loss: 0.68459511, ae_loss: 0.05133093\n",
      "Step: [3022] total_loss: 2.10987520 d_loss: 1.37924814, g_loss: 0.67677689, ae_loss: 0.05385032\n",
      "Step: [3023] total_loss: 2.11561012 d_loss: 1.38508666, g_loss: 0.67862719, ae_loss: 0.05189620\n",
      "Step: [3024] total_loss: 2.11146617 d_loss: 1.37429476, g_loss: 0.68620592, ae_loss: 0.05096550\n",
      "Step: [3025] total_loss: 2.11983585 d_loss: 1.38661361, g_loss: 0.68066496, ae_loss: 0.05255724\n",
      "Step: [3026] total_loss: 2.09530234 d_loss: 1.37007976, g_loss: 0.67453450, ae_loss: 0.05068820\n",
      "Step: [3027] total_loss: 2.11666870 d_loss: 1.39238453, g_loss: 0.67309231, ae_loss: 0.05119198\n",
      "Step: [3028] total_loss: 2.13546658 d_loss: 1.39254594, g_loss: 0.69424903, ae_loss: 0.04867171\n",
      "Step: [3029] total_loss: 2.09241772 d_loss: 1.36004829, g_loss: 0.68081695, ae_loss: 0.05155254\n",
      "Step: [3030] total_loss: 2.10350728 d_loss: 1.37542307, g_loss: 0.67601728, ae_loss: 0.05206696\n",
      "Step: [3031] total_loss: 2.10555840 d_loss: 1.37526512, g_loss: 0.68050766, ae_loss: 0.04978552\n",
      "Step: [3032] total_loss: 2.11976814 d_loss: 1.37854934, g_loss: 0.69187999, ae_loss: 0.04933896\n",
      "Step: [3033] total_loss: 2.12529135 d_loss: 1.37975156, g_loss: 0.69858134, ae_loss: 0.04695858\n",
      "Step: [3034] total_loss: 2.14051247 d_loss: 1.39591002, g_loss: 0.69383419, ae_loss: 0.05076808\n",
      "Step: [3035] total_loss: 2.14617491 d_loss: 1.39743423, g_loss: 0.69574171, ae_loss: 0.05299905\n",
      "Step: [3036] total_loss: 2.12853479 d_loss: 1.39310861, g_loss: 0.68491328, ae_loss: 0.05051289\n",
      "Step: [3037] total_loss: 2.13898754 d_loss: 1.39332581, g_loss: 0.69755602, ae_loss: 0.04810584\n",
      "Step: [3038] total_loss: 2.13160896 d_loss: 1.38033164, g_loss: 0.69667554, ae_loss: 0.05460186\n",
      "Step: [3039] total_loss: 2.14050627 d_loss: 1.38758695, g_loss: 0.70369023, ae_loss: 0.04922902\n",
      "Step: [3040] total_loss: 2.12334728 d_loss: 1.38327646, g_loss: 0.68952191, ae_loss: 0.05054875\n",
      "Step: [3041] total_loss: 2.14059067 d_loss: 1.40260017, g_loss: 0.68348074, ae_loss: 0.05450967\n",
      "Step: [3042] total_loss: 2.11380148 d_loss: 1.36691248, g_loss: 0.69487429, ae_loss: 0.05201471\n",
      "Step: [3043] total_loss: 2.11781526 d_loss: 1.37817216, g_loss: 0.68949908, ae_loss: 0.05014399\n",
      "Step: [3044] total_loss: 2.13749599 d_loss: 1.39559650, g_loss: 0.69086057, ae_loss: 0.05103881\n",
      "Step: [3045] total_loss: 2.11567187 d_loss: 1.36244106, g_loss: 0.70648772, ae_loss: 0.04674304\n",
      "Step: [3046] total_loss: 2.13200521 d_loss: 1.38699579, g_loss: 0.69083595, ae_loss: 0.05417345\n",
      "Step: [3047] total_loss: 2.14348149 d_loss: 1.40249109, g_loss: 0.69158506, ae_loss: 0.04940537\n",
      "Step: [3048] total_loss: 2.14428830 d_loss: 1.39940500, g_loss: 0.68957996, ae_loss: 0.05530329\n",
      "Step: [3049] total_loss: 2.14593172 d_loss: 1.39670384, g_loss: 0.69862914, ae_loss: 0.05059881\n",
      "Step: [3050] total_loss: 2.12955642 d_loss: 1.39668870, g_loss: 0.68305713, ae_loss: 0.04981060\n",
      "Step: [3051] total_loss: 2.13132310 d_loss: 1.37971735, g_loss: 0.69969201, ae_loss: 0.05191377\n",
      "Step: [3052] total_loss: 2.14057660 d_loss: 1.39065576, g_loss: 0.69299585, ae_loss: 0.05692501\n",
      "Step: [3053] total_loss: 2.10580730 d_loss: 1.35269117, g_loss: 0.70347142, ae_loss: 0.04964475\n",
      "Step: [3054] total_loss: 2.12362981 d_loss: 1.38667595, g_loss: 0.68247157, ae_loss: 0.05448224\n",
      "Step: [3055] total_loss: 2.11904931 d_loss: 1.37824500, g_loss: 0.69006240, ae_loss: 0.05074191\n",
      "Step: [3056] total_loss: 2.11892700 d_loss: 1.38507962, g_loss: 0.68006247, ae_loss: 0.05378482\n",
      "Step: [3057] total_loss: 2.11800098 d_loss: 1.38236451, g_loss: 0.68255144, ae_loss: 0.05308493\n",
      "Step: [3058] total_loss: 2.14061165 d_loss: 1.38160753, g_loss: 0.70915043, ae_loss: 0.04985369\n",
      "Step: [3059] total_loss: 2.15164232 d_loss: 1.38864708, g_loss: 0.70875067, ae_loss: 0.05424458\n",
      "Step: [3060] total_loss: 2.13598537 d_loss: 1.37405062, g_loss: 0.70716065, ae_loss: 0.05477422\n",
      "Step: [3061] total_loss: 2.14154887 d_loss: 1.40030503, g_loss: 0.69239700, ae_loss: 0.04884684\n",
      "Step: [3062] total_loss: 2.14252615 d_loss: 1.37674618, g_loss: 0.71396601, ae_loss: 0.05181405\n",
      "Step: [3063] total_loss: 2.13818312 d_loss: 1.38748729, g_loss: 0.69943500, ae_loss: 0.05126076\n",
      "Step: [3064] total_loss: 2.14202380 d_loss: 1.39569163, g_loss: 0.69708508, ae_loss: 0.04924704\n",
      "Step: [3065] total_loss: 2.13151860 d_loss: 1.37975931, g_loss: 0.69939440, ae_loss: 0.05236498\n",
      "Step: [3066] total_loss: 2.11781955 d_loss: 1.37909424, g_loss: 0.68826795, ae_loss: 0.05045734\n",
      "Step: [3067] total_loss: 2.12726355 d_loss: 1.36630738, g_loss: 0.70764792, ae_loss: 0.05330816\n",
      "Step: [3068] total_loss: 2.12014556 d_loss: 1.38991737, g_loss: 0.67288351, ae_loss: 0.05734464\n",
      "Step: [3069] total_loss: 2.10702801 d_loss: 1.37384164, g_loss: 0.68517745, ae_loss: 0.04800906\n",
      "Step: [3070] total_loss: 2.11814046 d_loss: 1.38759696, g_loss: 0.67722499, ae_loss: 0.05331848\n",
      "Step: [3071] total_loss: 2.13807893 d_loss: 1.36001015, g_loss: 0.72604215, ae_loss: 0.05202664\n",
      "Step: [3072] total_loss: 2.11532450 d_loss: 1.35664725, g_loss: 0.70951557, ae_loss: 0.04916182\n",
      "Step: [3073] total_loss: 2.13967991 d_loss: 1.38875246, g_loss: 0.69698477, ae_loss: 0.05394263\n",
      "Step: [3074] total_loss: 2.12661457 d_loss: 1.36842537, g_loss: 0.70706844, ae_loss: 0.05112077\n",
      "Step: [3075] total_loss: 2.12479067 d_loss: 1.38631272, g_loss: 0.68767220, ae_loss: 0.05080565\n",
      "Step: [3076] total_loss: 2.11242318 d_loss: 1.37577486, g_loss: 0.68508297, ae_loss: 0.05156530\n",
      "Step: [3077] total_loss: 2.13363528 d_loss: 1.40012169, g_loss: 0.68380821, ae_loss: 0.04970539\n",
      "Step: [3078] total_loss: 2.11227107 d_loss: 1.36435151, g_loss: 0.69561732, ae_loss: 0.05230224\n",
      "Step: [3079] total_loss: 2.12866211 d_loss: 1.37716508, g_loss: 0.69842660, ae_loss: 0.05307029\n",
      "Step: [3080] total_loss: 2.11590815 d_loss: 1.36135483, g_loss: 0.70320678, ae_loss: 0.05134659\n",
      "Step: [3081] total_loss: 2.14532089 d_loss: 1.38730669, g_loss: 0.70286262, ae_loss: 0.05515162\n",
      "Step: [3082] total_loss: 2.13064003 d_loss: 1.39452624, g_loss: 0.68423492, ae_loss: 0.05187876\n",
      "Step: [3083] total_loss: 2.12278914 d_loss: 1.37380290, g_loss: 0.69398242, ae_loss: 0.05500387\n",
      "Step: [3084] total_loss: 2.14711094 d_loss: 1.40753222, g_loss: 0.68860197, ae_loss: 0.05097660\n",
      "Step: [3085] total_loss: 2.11011863 d_loss: 1.38058114, g_loss: 0.67726380, ae_loss: 0.05227358\n",
      "Step: [3086] total_loss: 2.11227655 d_loss: 1.37086236, g_loss: 0.68886149, ae_loss: 0.05255253\n",
      "Step: [3087] total_loss: 2.11719704 d_loss: 1.38776588, g_loss: 0.67566812, ae_loss: 0.05376291\n",
      "Step: [3088] total_loss: 2.13802028 d_loss: 1.37837565, g_loss: 0.70957786, ae_loss: 0.05006682\n",
      "Step: [3089] total_loss: 2.12680101 d_loss: 1.38129854, g_loss: 0.69144905, ae_loss: 0.05405348\n",
      "Step: [3090] total_loss: 2.11338782 d_loss: 1.36710024, g_loss: 0.69477737, ae_loss: 0.05151019\n",
      "Step: [3091] total_loss: 2.12340641 d_loss: 1.37914062, g_loss: 0.69299853, ae_loss: 0.05126710\n",
      "Step: [3092] total_loss: 2.11527562 d_loss: 1.37151599, g_loss: 0.69163114, ae_loss: 0.05212843\n",
      "Step: [3093] total_loss: 2.11735010 d_loss: 1.37795901, g_loss: 0.69198644, ae_loss: 0.04740448\n",
      "Step: [3094] total_loss: 2.11758757 d_loss: 1.39011967, g_loss: 0.67664325, ae_loss: 0.05082480\n",
      "Step: [3095] total_loss: 2.11748171 d_loss: 1.37636566, g_loss: 0.69099551, ae_loss: 0.05012060\n",
      "Step: [3096] total_loss: 2.12396002 d_loss: 1.38521695, g_loss: 0.68800122, ae_loss: 0.05074177\n",
      "Step: [3097] total_loss: 2.13002133 d_loss: 1.39151168, g_loss: 0.68820953, ae_loss: 0.05030012\n",
      "Step: [3098] total_loss: 2.09958792 d_loss: 1.35844326, g_loss: 0.69120395, ae_loss: 0.04994065\n",
      "Step: [3099] total_loss: 2.13597107 d_loss: 1.40050662, g_loss: 0.68518996, ae_loss: 0.05027464\n",
      "Step: [3100] total_loss: 2.10588694 d_loss: 1.36054182, g_loss: 0.69273043, ae_loss: 0.05261468\n",
      "Step: [3101] total_loss: 2.12765098 d_loss: 1.37169063, g_loss: 0.70307529, ae_loss: 0.05288502\n",
      "Step: [3102] total_loss: 2.13061523 d_loss: 1.37337208, g_loss: 0.70357776, ae_loss: 0.05366552\n",
      "Step: [3103] total_loss: 2.13194942 d_loss: 1.38353896, g_loss: 0.69442916, ae_loss: 0.05398120\n",
      "Step: [3104] total_loss: 2.13356590 d_loss: 1.38402355, g_loss: 0.69543570, ae_loss: 0.05410670\n",
      "Step: [3105] total_loss: 2.11291766 d_loss: 1.37211823, g_loss: 0.68983930, ae_loss: 0.05096003\n",
      "Step: [3106] total_loss: 2.11871147 d_loss: 1.37962174, g_loss: 0.68746263, ae_loss: 0.05162708\n",
      "Step: [3107] total_loss: 2.15190959 d_loss: 1.39801764, g_loss: 0.70182580, ae_loss: 0.05206607\n",
      "Step: [3108] total_loss: 2.13296556 d_loss: 1.39036417, g_loss: 0.68910140, ae_loss: 0.05349988\n",
      "Step: [3109] total_loss: 2.13280582 d_loss: 1.38546884, g_loss: 0.69452775, ae_loss: 0.05280941\n",
      "Step: [3110] total_loss: 2.12409973 d_loss: 1.38050222, g_loss: 0.69259971, ae_loss: 0.05099775\n",
      "Step: [3111] total_loss: 2.11721635 d_loss: 1.36730456, g_loss: 0.69439930, ae_loss: 0.05551243\n",
      "Step: [3112] total_loss: 2.12476110 d_loss: 1.39053428, g_loss: 0.68137157, ae_loss: 0.05285527\n",
      "Step: [3113] total_loss: 2.12668204 d_loss: 1.39355874, g_loss: 0.67968631, ae_loss: 0.05343705\n",
      "Step: [3114] total_loss: 2.09659004 d_loss: 1.36639082, g_loss: 0.67946529, ae_loss: 0.05073395\n",
      "Step: [3115] total_loss: 2.10488105 d_loss: 1.37357271, g_loss: 0.68038565, ae_loss: 0.05092265\n",
      "Step: [3116] total_loss: 2.11456895 d_loss: 1.39419174, g_loss: 0.66481704, ae_loss: 0.05556019\n",
      "Step: [3117] total_loss: 2.12667131 d_loss: 1.38907564, g_loss: 0.68711460, ae_loss: 0.05048105\n",
      "Step: [3118] total_loss: 2.13608027 d_loss: 1.37958145, g_loss: 0.70648861, ae_loss: 0.05001009\n",
      "Step: [3119] total_loss: 2.11811161 d_loss: 1.36738694, g_loss: 0.69828725, ae_loss: 0.05243753\n",
      "Step: [3120] total_loss: 2.14588308 d_loss: 1.39860559, g_loss: 0.69525552, ae_loss: 0.05202194\n",
      "Step: [3121] total_loss: 2.13669968 d_loss: 1.37479722, g_loss: 0.70975745, ae_loss: 0.05214508\n",
      "Step: [3122] total_loss: 2.11182094 d_loss: 1.36128211, g_loss: 0.70135099, ae_loss: 0.04918777\n",
      "Step: [3123] total_loss: 2.13415241 d_loss: 1.39663923, g_loss: 0.68721199, ae_loss: 0.05030109\n",
      "Step: [3124] total_loss: 2.10905361 d_loss: 1.36393416, g_loss: 0.69343233, ae_loss: 0.05168695\n",
      "Step: [3125] total_loss: 2.13244152 d_loss: 1.39491487, g_loss: 0.68681407, ae_loss: 0.05071269\n",
      "Step: [3126] total_loss: 2.12832451 d_loss: 1.39484596, g_loss: 0.68544853, ae_loss: 0.04802984\n",
      "Step: [3127] total_loss: 2.12549090 d_loss: 1.38482618, g_loss: 0.68792856, ae_loss: 0.05273617\n",
      "Step: [3128] total_loss: 2.13165092 d_loss: 1.38501573, g_loss: 0.69297016, ae_loss: 0.05366499\n",
      "Step: [3129] total_loss: 2.13048935 d_loss: 1.37781167, g_loss: 0.69844890, ae_loss: 0.05422862\n",
      "Step: [3130] total_loss: 2.12452269 d_loss: 1.36541915, g_loss: 0.70571518, ae_loss: 0.05338842\n",
      "Step: [3131] total_loss: 2.11579347 d_loss: 1.37781513, g_loss: 0.68663567, ae_loss: 0.05134277\n",
      "Step: [3132] total_loss: 2.12483239 d_loss: 1.38907003, g_loss: 0.68505877, ae_loss: 0.05070359\n",
      "Step: [3133] total_loss: 2.13678646 d_loss: 1.39262938, g_loss: 0.69710791, ae_loss: 0.04704908\n",
      "Step: [3134] total_loss: 2.13380456 d_loss: 1.38276839, g_loss: 0.69664639, ae_loss: 0.05438966\n",
      "Step: [3135] total_loss: 2.14168119 d_loss: 1.37246442, g_loss: 0.71766096, ae_loss: 0.05155576\n",
      "Step: [3136] total_loss: 2.12927151 d_loss: 1.37282717, g_loss: 0.70890212, ae_loss: 0.04754218\n",
      "Step: [3137] total_loss: 2.10395956 d_loss: 1.36333203, g_loss: 0.68782508, ae_loss: 0.05280234\n",
      "Step: [3138] total_loss: 2.11458659 d_loss: 1.39834523, g_loss: 0.66637164, ae_loss: 0.04986982\n",
      "Step: [3139] total_loss: 2.15185785 d_loss: 1.40385926, g_loss: 0.69127071, ae_loss: 0.05672789\n",
      "Step: [3140] total_loss: 2.13147998 d_loss: 1.38088036, g_loss: 0.70454925, ae_loss: 0.04605040\n",
      "Step: [3141] total_loss: 2.14898634 d_loss: 1.39031553, g_loss: 0.70086539, ae_loss: 0.05780556\n",
      "Step: [3142] total_loss: 2.14126730 d_loss: 1.39091349, g_loss: 0.70290583, ae_loss: 0.04744793\n",
      "Step: [3143] total_loss: 2.11718345 d_loss: 1.36135530, g_loss: 0.70895034, ae_loss: 0.04687792\n",
      "Step: [3144] total_loss: 2.14354086 d_loss: 1.39801884, g_loss: 0.69109845, ae_loss: 0.05442369\n",
      "Step: [3145] total_loss: 2.11544251 d_loss: 1.36929131, g_loss: 0.69552916, ae_loss: 0.05062196\n",
      "Step: [3146] total_loss: 2.12730789 d_loss: 1.39804411, g_loss: 0.67674708, ae_loss: 0.05251662\n",
      "Step: [3147] total_loss: 2.11806774 d_loss: 1.35721946, g_loss: 0.70235479, ae_loss: 0.05849355\n",
      "Step: [3148] total_loss: 2.13414383 d_loss: 1.38879347, g_loss: 0.69378912, ae_loss: 0.05156141\n",
      "Step: [3149] total_loss: 2.12894273 d_loss: 1.37740183, g_loss: 0.69836348, ae_loss: 0.05317731\n",
      "Step: [3150] total_loss: 2.12847471 d_loss: 1.38340139, g_loss: 0.68901765, ae_loss: 0.05605559\n",
      "Step: [3151] total_loss: 2.13876152 d_loss: 1.38578308, g_loss: 0.69942379, ae_loss: 0.05355466\n",
      "Step: [3152] total_loss: 2.12865305 d_loss: 1.38720465, g_loss: 0.68779230, ae_loss: 0.05365600\n",
      "Step: [3153] total_loss: 2.12279415 d_loss: 1.38067174, g_loss: 0.68925202, ae_loss: 0.05287055\n",
      "Step: [3154] total_loss: 2.13510990 d_loss: 1.38071644, g_loss: 0.70150065, ae_loss: 0.05289283\n",
      "Step: [3155] total_loss: 2.11648774 d_loss: 1.38229525, g_loss: 0.68361664, ae_loss: 0.05057582\n",
      "Step: [3156] total_loss: 2.13280249 d_loss: 1.39110351, g_loss: 0.68868649, ae_loss: 0.05301231\n",
      "Step: [3157] total_loss: 2.12492800 d_loss: 1.39252448, g_loss: 0.67977214, ae_loss: 0.05263150\n",
      "Step: [3158] total_loss: 2.12180734 d_loss: 1.37621033, g_loss: 0.69202989, ae_loss: 0.05356719\n",
      "Step: [3159] total_loss: 2.13145590 d_loss: 1.38330698, g_loss: 0.69801396, ae_loss: 0.05013485\n",
      "Step: [3160] total_loss: 2.13500357 d_loss: 1.38667524, g_loss: 0.69668555, ae_loss: 0.05164261\n",
      "Step: [3161] total_loss: 2.12483597 d_loss: 1.38831806, g_loss: 0.68321180, ae_loss: 0.05330594\n",
      "Step: [3162] total_loss: 2.12747312 d_loss: 1.38280082, g_loss: 0.69157106, ae_loss: 0.05310132\n",
      "Step: [3163] total_loss: 2.11919928 d_loss: 1.39314675, g_loss: 0.67680776, ae_loss: 0.04924487\n",
      "Step: [3164] total_loss: 2.12666559 d_loss: 1.39736557, g_loss: 0.68007183, ae_loss: 0.04922811\n",
      "Step: [3165] total_loss: 2.12171221 d_loss: 1.38358390, g_loss: 0.68694532, ae_loss: 0.05118284\n",
      "Step: [3166] total_loss: 2.09104633 d_loss: 1.36772776, g_loss: 0.67089349, ae_loss: 0.05242516\n",
      "Step: [3167] total_loss: 2.11575031 d_loss: 1.38452983, g_loss: 0.68268836, ae_loss: 0.04853223\n",
      "Step: [3168] total_loss: 2.12761784 d_loss: 1.36270392, g_loss: 0.71183312, ae_loss: 0.05308095\n",
      "Step: [3169] total_loss: 2.09374714 d_loss: 1.33728409, g_loss: 0.70586514, ae_loss: 0.05059781\n",
      "Step: [3170] total_loss: 2.13187623 d_loss: 1.37773252, g_loss: 0.69768125, ae_loss: 0.05646248\n",
      "Step: [3171] total_loss: 2.13337445 d_loss: 1.37764847, g_loss: 0.70066589, ae_loss: 0.05506008\n",
      "Step: [3172] total_loss: 2.11167049 d_loss: 1.37587023, g_loss: 0.69034648, ae_loss: 0.04545390\n",
      "Step: [3173] total_loss: 2.15573597 d_loss: 1.39173508, g_loss: 0.70953721, ae_loss: 0.05446374\n",
      "Step: [3174] total_loss: 2.12458301 d_loss: 1.37962937, g_loss: 0.69309449, ae_loss: 0.05185917\n",
      "Step: [3175] total_loss: 2.11746979 d_loss: 1.38112903, g_loss: 0.68480694, ae_loss: 0.05153391\n",
      "Step: [3176] total_loss: 2.12385130 d_loss: 1.36961877, g_loss: 0.70719087, ae_loss: 0.04704152\n",
      "Step: [3177] total_loss: 2.13524485 d_loss: 1.38809872, g_loss: 0.69026470, ae_loss: 0.05688127\n",
      "Step: [3178] total_loss: 2.12536407 d_loss: 1.36279202, g_loss: 0.70827919, ae_loss: 0.05429275\n",
      "Step: [3179] total_loss: 2.13381743 d_loss: 1.38794208, g_loss: 0.69608474, ae_loss: 0.04979061\n",
      "Step: [3180] total_loss: 2.12072682 d_loss: 1.37955499, g_loss: 0.69096249, ae_loss: 0.05020938\n",
      "Step: [3181] total_loss: 2.15742826 d_loss: 1.40131450, g_loss: 0.70549476, ae_loss: 0.05061903\n",
      "Step: [3182] total_loss: 2.13609076 d_loss: 1.38445973, g_loss: 0.70146239, ae_loss: 0.05016857\n",
      "Step: [3183] total_loss: 2.11468005 d_loss: 1.36795461, g_loss: 0.69499552, ae_loss: 0.05172997\n",
      "Step: [3184] total_loss: 2.13267016 d_loss: 1.38078642, g_loss: 0.70062268, ae_loss: 0.05126104\n",
      "Step: [3185] total_loss: 2.11385322 d_loss: 1.36901999, g_loss: 0.69780397, ae_loss: 0.04702930\n",
      "Step: [3186] total_loss: 2.13205719 d_loss: 1.39423347, g_loss: 0.68468034, ae_loss: 0.05314350\n",
      "Step: [3187] total_loss: 2.12835455 d_loss: 1.38531506, g_loss: 0.68875057, ae_loss: 0.05428895\n",
      "Step: [3188] total_loss: 2.10693216 d_loss: 1.36290717, g_loss: 0.69624603, ae_loss: 0.04777913\n",
      "Step: [3189] total_loss: 2.13282537 d_loss: 1.40536416, g_loss: 0.67228627, ae_loss: 0.05517485\n",
      "Step: [3190] total_loss: 2.11496139 d_loss: 1.38060880, g_loss: 0.68542147, ae_loss: 0.04893109\n",
      "Step: [3191] total_loss: 2.11017299 d_loss: 1.36808228, g_loss: 0.68749225, ae_loss: 0.05459845\n",
      "Step: [3192] total_loss: 2.12837195 d_loss: 1.37585187, g_loss: 0.70085067, ae_loss: 0.05166939\n",
      "Step: [3193] total_loss: 2.12264991 d_loss: 1.36146486, g_loss: 0.70846725, ae_loss: 0.05271775\n",
      "Step: [3194] total_loss: 2.12803888 d_loss: 1.37430286, g_loss: 0.70051873, ae_loss: 0.05321727\n",
      "Step: [3195] total_loss: 2.13673687 d_loss: 1.37562513, g_loss: 0.70970881, ae_loss: 0.05140279\n",
      "Step: [3196] total_loss: 2.15413332 d_loss: 1.40863085, g_loss: 0.69337940, ae_loss: 0.05212321\n",
      "Step: [3197] total_loss: 2.13397169 d_loss: 1.38382864, g_loss: 0.69674575, ae_loss: 0.05339723\n",
      "Step: [3198] total_loss: 2.14335155 d_loss: 1.39065790, g_loss: 0.70332110, ae_loss: 0.04937250\n",
      "Step: [3199] total_loss: 2.13110733 d_loss: 1.37295985, g_loss: 0.70474988, ae_loss: 0.05339748\n",
      "Step: [3200] total_loss: 2.10792089 d_loss: 1.37745500, g_loss: 0.67678934, ae_loss: 0.05367646\n",
      "Step: [3201] total_loss: 2.13385749 d_loss: 1.39696372, g_loss: 0.68436277, ae_loss: 0.05253101\n",
      "Step: [3202] total_loss: 2.12302661 d_loss: 1.38656962, g_loss: 0.68218666, ae_loss: 0.05427036\n",
      "Step: [3203] total_loss: 2.12676120 d_loss: 1.39780855, g_loss: 0.67541075, ae_loss: 0.05354192\n",
      "Step: [3204] total_loss: 2.13065767 d_loss: 1.38977504, g_loss: 0.69111264, ae_loss: 0.04977008\n",
      "Step: [3205] total_loss: 2.12160516 d_loss: 1.37159657, g_loss: 0.70063955, ae_loss: 0.04936913\n",
      "Step: [3206] total_loss: 2.13139343 d_loss: 1.38429046, g_loss: 0.69540071, ae_loss: 0.05170211\n",
      "Step: [3207] total_loss: 2.15032339 d_loss: 1.39578319, g_loss: 0.69897020, ae_loss: 0.05557007\n",
      "Step: [3208] total_loss: 2.13922501 d_loss: 1.36992049, g_loss: 0.71643680, ae_loss: 0.05286764\n",
      "Step: [3209] total_loss: 2.10628510 d_loss: 1.37522340, g_loss: 0.67926955, ae_loss: 0.05179214\n",
      "Step: [3210] total_loss: 2.14946055 d_loss: 1.40784383, g_loss: 0.68881238, ae_loss: 0.05280433\n",
      "Step: [3211] total_loss: 2.12259722 d_loss: 1.38826275, g_loss: 0.68275166, ae_loss: 0.05158265\n",
      "Step: [3212] total_loss: 2.12969255 d_loss: 1.39002001, g_loss: 0.68567562, ae_loss: 0.05399682\n",
      "Step: [3213] total_loss: 2.12452602 d_loss: 1.38534713, g_loss: 0.68839216, ae_loss: 0.05078669\n",
      "Step: [3214] total_loss: 2.12740827 d_loss: 1.38300180, g_loss: 0.69334173, ae_loss: 0.05106478\n",
      "Step: [3215] total_loss: 2.12030935 d_loss: 1.38238442, g_loss: 0.68760073, ae_loss: 0.05032406\n",
      "Step: [3216] total_loss: 2.12401128 d_loss: 1.37909245, g_loss: 0.68894172, ae_loss: 0.05597713\n",
      "Step: [3217] total_loss: 2.11631727 d_loss: 1.38417971, g_loss: 0.68173468, ae_loss: 0.05040288\n",
      "Step: [3218] total_loss: 2.13672519 d_loss: 1.38892484, g_loss: 0.69562989, ae_loss: 0.05217042\n",
      "Step: [3219] total_loss: 2.12350178 d_loss: 1.37965751, g_loss: 0.69024253, ae_loss: 0.05360164\n",
      "Step: [3220] total_loss: 2.12536144 d_loss: 1.37603593, g_loss: 0.69308555, ae_loss: 0.05623979\n",
      "Step: [3221] total_loss: 2.12065744 d_loss: 1.37293136, g_loss: 0.69630909, ae_loss: 0.05141705\n",
      "Step: [3222] total_loss: 2.12410164 d_loss: 1.37543154, g_loss: 0.69568419, ae_loss: 0.05298588\n",
      "Step: [3223] total_loss: 2.14325762 d_loss: 1.40175021, g_loss: 0.69079131, ae_loss: 0.05071604\n",
      "Step: [3224] total_loss: 2.13980818 d_loss: 1.36168885, g_loss: 0.72291112, ae_loss: 0.05520817\n",
      "Step: [3225] total_loss: 2.12777734 d_loss: 1.38153851, g_loss: 0.69212890, ae_loss: 0.05410993\n",
      "Step: [3226] total_loss: 2.12419772 d_loss: 1.37756705, g_loss: 0.69841194, ae_loss: 0.04821870\n",
      "Step: [3227] total_loss: 2.11882138 d_loss: 1.37311935, g_loss: 0.69685292, ae_loss: 0.04884916\n",
      "Step: [3228] total_loss: 2.12567401 d_loss: 1.37249446, g_loss: 0.70297194, ae_loss: 0.05020757\n",
      "Step: [3229] total_loss: 2.13032126 d_loss: 1.39342999, g_loss: 0.68848646, ae_loss: 0.04840480\n",
      "Step: [3230] total_loss: 2.12423062 d_loss: 1.38983440, g_loss: 0.67738730, ae_loss: 0.05700903\n",
      "Step: [3231] total_loss: 2.11041689 d_loss: 1.37127435, g_loss: 0.69158685, ae_loss: 0.04755567\n",
      "Step: [3232] total_loss: 2.10602999 d_loss: 1.37636971, g_loss: 0.67875773, ae_loss: 0.05090246\n",
      "Step: [3233] total_loss: 2.11953306 d_loss: 1.40316272, g_loss: 0.66481811, ae_loss: 0.05155218\n",
      "Step: [3234] total_loss: 2.11689520 d_loss: 1.37824893, g_loss: 0.69204301, ae_loss: 0.04660324\n",
      "Step: [3235] total_loss: 2.13200140 d_loss: 1.38433552, g_loss: 0.69509113, ae_loss: 0.05257483\n",
      "Step: [3236] total_loss: 2.12837386 d_loss: 1.39099419, g_loss: 0.68542743, ae_loss: 0.05195222\n",
      "Step: [3237] total_loss: 2.12311292 d_loss: 1.38355553, g_loss: 0.68827081, ae_loss: 0.05128660\n",
      "Step: [3238] total_loss: 2.13865733 d_loss: 1.38704169, g_loss: 0.69859004, ae_loss: 0.05302563\n",
      "Step: [3239] total_loss: 2.12863636 d_loss: 1.38269091, g_loss: 0.68822217, ae_loss: 0.05772311\n",
      "Step: [3240] total_loss: 2.13445520 d_loss: 1.39960575, g_loss: 0.68484557, ae_loss: 0.05000401\n",
      "Step: [3241] total_loss: 2.14568281 d_loss: 1.38456452, g_loss: 0.70532429, ae_loss: 0.05579396\n",
      "Step: [3242] total_loss: 2.12487364 d_loss: 1.38726044, g_loss: 0.68496835, ae_loss: 0.05264471\n",
      "Step: [3243] total_loss: 2.13339067 d_loss: 1.38033533, g_loss: 0.69870597, ae_loss: 0.05434937\n",
      "Step: [3244] total_loss: 2.12166023 d_loss: 1.38932216, g_loss: 0.67978817, ae_loss: 0.05254988\n",
      "Step: [3245] total_loss: 2.12521052 d_loss: 1.38184476, g_loss: 0.69249153, ae_loss: 0.05087428\n",
      "Step: [3246] total_loss: 2.09773088 d_loss: 1.35477161, g_loss: 0.69058526, ae_loss: 0.05237403\n",
      "Step: [3247] total_loss: 2.12651277 d_loss: 1.36997795, g_loss: 0.70887995, ae_loss: 0.04765490\n",
      "Step: [3248] total_loss: 2.10954499 d_loss: 1.37310362, g_loss: 0.68186468, ae_loss: 0.05457659\n",
      "Step: [3249] total_loss: 2.12834096 d_loss: 1.37167263, g_loss: 0.70652437, ae_loss: 0.05014390\n",
      "Step: [3250] total_loss: 2.12871456 d_loss: 1.38559127, g_loss: 0.68904710, ae_loss: 0.05407605\n",
      "Step: [3251] total_loss: 2.09698081 d_loss: 1.36543870, g_loss: 0.68169123, ae_loss: 0.04985096\n",
      "Step: [3252] total_loss: 2.11845493 d_loss: 1.37659883, g_loss: 0.69141519, ae_loss: 0.05044087\n",
      "Step: [3253] total_loss: 2.12634754 d_loss: 1.38755977, g_loss: 0.68770266, ae_loss: 0.05108498\n",
      "Step: [3254] total_loss: 2.13368511 d_loss: 1.38812935, g_loss: 0.69199014, ae_loss: 0.05356570\n",
      "Step: [3255] total_loss: 2.13124752 d_loss: 1.38336456, g_loss: 0.69347727, ae_loss: 0.05440582\n",
      "Step: [3256] total_loss: 2.13539124 d_loss: 1.38029635, g_loss: 0.70518410, ae_loss: 0.04991067\n",
      "Step: [3257] total_loss: 2.13153696 d_loss: 1.37350059, g_loss: 0.70399785, ae_loss: 0.05403859\n",
      "Step: [3258] total_loss: 2.13457799 d_loss: 1.39151561, g_loss: 0.69404638, ae_loss: 0.04901603\n",
      "Step: [3259] total_loss: 2.12018776 d_loss: 1.37783289, g_loss: 0.68722129, ae_loss: 0.05513343\n",
      "Step: [3260] total_loss: 2.13568950 d_loss: 1.40190542, g_loss: 0.68337572, ae_loss: 0.05040839\n",
      "Step: [3261] total_loss: 2.13478994 d_loss: 1.39420688, g_loss: 0.68820530, ae_loss: 0.05237783\n",
      "Step: [3262] total_loss: 2.11801910 d_loss: 1.38563454, g_loss: 0.68031782, ae_loss: 0.05206682\n",
      "Step: [3263] total_loss: 2.13231421 d_loss: 1.39391065, g_loss: 0.68748564, ae_loss: 0.05091801\n",
      "Step: [3264] total_loss: 2.11822081 d_loss: 1.37767625, g_loss: 0.68814582, ae_loss: 0.05239864\n",
      "Step: [3265] total_loss: 2.13778925 d_loss: 1.38172460, g_loss: 0.70616078, ae_loss: 0.04990374\n",
      "Step: [3266] total_loss: 2.13627267 d_loss: 1.38672400, g_loss: 0.69772172, ae_loss: 0.05182694\n",
      "Step: [3267] total_loss: 2.13777089 d_loss: 1.38871574, g_loss: 0.69857287, ae_loss: 0.05048229\n",
      "Step: [3268] total_loss: 2.12157679 d_loss: 1.34990501, g_loss: 0.71600819, ae_loss: 0.05566354\n",
      "Step: [3269] total_loss: 2.15387464 d_loss: 1.39943862, g_loss: 0.70242918, ae_loss: 0.05200679\n",
      "Step: [3270] total_loss: 2.11136270 d_loss: 1.34574771, g_loss: 0.71424943, ae_loss: 0.05136546\n",
      "Step: [3271] total_loss: 2.12420130 d_loss: 1.40925586, g_loss: 0.66249156, ae_loss: 0.05245401\n",
      "Step: [3272] total_loss: 2.10973263 d_loss: 1.38105571, g_loss: 0.67757857, ae_loss: 0.05109818\n",
      "Step: [3273] total_loss: 2.10912251 d_loss: 1.34738696, g_loss: 0.70926660, ae_loss: 0.05246890\n",
      "Step: [3274] total_loss: 2.12532735 d_loss: 1.36886430, g_loss: 0.70614100, ae_loss: 0.05032202\n",
      "Step: [3275] total_loss: 2.12729549 d_loss: 1.38083684, g_loss: 0.69184327, ae_loss: 0.05461529\n",
      "Step: [3276] total_loss: 2.13385034 d_loss: 1.36914945, g_loss: 0.70854753, ae_loss: 0.05615338\n",
      "Step: [3277] total_loss: 2.13463330 d_loss: 1.40453243, g_loss: 0.68082964, ae_loss: 0.04927118\n",
      "Step: [3278] total_loss: 2.12699032 d_loss: 1.38090611, g_loss: 0.69638157, ae_loss: 0.04970282\n",
      "Step: [3279] total_loss: 2.12562180 d_loss: 1.36138153, g_loss: 0.71327126, ae_loss: 0.05096906\n",
      "Step: [3280] total_loss: 2.12398791 d_loss: 1.38352656, g_loss: 0.68679094, ae_loss: 0.05367042\n",
      "Step: [3281] total_loss: 2.11533952 d_loss: 1.38635290, g_loss: 0.67609727, ae_loss: 0.05288935\n",
      "Step: [3282] total_loss: 2.12273431 d_loss: 1.39857674, g_loss: 0.67407340, ae_loss: 0.05008424\n",
      "Step: [3283] total_loss: 2.11771917 d_loss: 1.35495508, g_loss: 0.70838594, ae_loss: 0.05437832\n",
      "Step: [3284] total_loss: 2.12909698 d_loss: 1.39733326, g_loss: 0.68328476, ae_loss: 0.04847882\n",
      "Step: [3285] total_loss: 2.14273477 d_loss: 1.41682148, g_loss: 0.67425865, ae_loss: 0.05165470\n",
      "Step: [3286] total_loss: 2.13219213 d_loss: 1.39070034, g_loss: 0.68506300, ae_loss: 0.05642880\n",
      "Step: [3287] total_loss: 2.11138010 d_loss: 1.36214352, g_loss: 0.69702351, ae_loss: 0.05221316\n",
      "Step: [3288] total_loss: 2.12070084 d_loss: 1.36903811, g_loss: 0.69920409, ae_loss: 0.05245848\n",
      "Step: [3289] total_loss: 2.13462615 d_loss: 1.39555168, g_loss: 0.68581784, ae_loss: 0.05325666\n",
      "Step: [3290] total_loss: 2.12633038 d_loss: 1.37382483, g_loss: 0.70253277, ae_loss: 0.04997295\n",
      "Step: [3291] total_loss: 2.10945845 d_loss: 1.37920022, g_loss: 0.67785895, ae_loss: 0.05239945\n",
      "Step: [3292] total_loss: 2.11103296 d_loss: 1.37153471, g_loss: 0.68703008, ae_loss: 0.05246830\n",
      "Step: [3293] total_loss: 2.13979959 d_loss: 1.40104425, g_loss: 0.68889427, ae_loss: 0.04986112\n",
      "Step: [3294] total_loss: 2.12274742 d_loss: 1.37211907, g_loss: 0.69967341, ae_loss: 0.05095482\n",
      "Step: [3295] total_loss: 2.12740326 d_loss: 1.39692450, g_loss: 0.68072128, ae_loss: 0.04975766\n",
      "Step: [3296] total_loss: 2.14225292 d_loss: 1.40601683, g_loss: 0.68189728, ae_loss: 0.05433889\n",
      "Step: [3297] total_loss: 2.12004495 d_loss: 1.39385831, g_loss: 0.67432183, ae_loss: 0.05186474\n",
      "Step: [3298] total_loss: 2.12530041 d_loss: 1.39149272, g_loss: 0.68407822, ae_loss: 0.04972952\n",
      "Step: [3299] total_loss: 2.14806986 d_loss: 1.41205633, g_loss: 0.68632150, ae_loss: 0.04969186\n",
      "Step: [3300] total_loss: 2.14523554 d_loss: 1.38946795, g_loss: 0.70640165, ae_loss: 0.04936597\n",
      "Step: [3301] total_loss: 2.14311600 d_loss: 1.38083804, g_loss: 0.70980680, ae_loss: 0.05247127\n",
      "Step: [3302] total_loss: 2.11445832 d_loss: 1.38008308, g_loss: 0.68373662, ae_loss: 0.05063862\n",
      "Step: [3303] total_loss: 2.10640717 d_loss: 1.36232328, g_loss: 0.69759965, ae_loss: 0.04648424\n",
      "Step: [3304] total_loss: 2.12730217 d_loss: 1.39592719, g_loss: 0.68077993, ae_loss: 0.05059516\n",
      "Step: [3305] total_loss: 2.10724974 d_loss: 1.36768210, g_loss: 0.68454218, ae_loss: 0.05502545\n",
      "Step: [3306] total_loss: 2.11456394 d_loss: 1.37131584, g_loss: 0.69433331, ae_loss: 0.04891466\n",
      "Step: [3307] total_loss: 2.12261891 d_loss: 1.38461614, g_loss: 0.69050521, ae_loss: 0.04749762\n",
      "Step: [3308] total_loss: 2.11941910 d_loss: 1.37975550, g_loss: 0.68634945, ae_loss: 0.05331406\n",
      "Step: [3309] total_loss: 2.13987255 d_loss: 1.37354970, g_loss: 0.70868933, ae_loss: 0.05763358\n",
      "Step: [3310] total_loss: 2.12402391 d_loss: 1.38959908, g_loss: 0.68520582, ae_loss: 0.04921895\n",
      "Step: [3311] total_loss: 2.14011097 d_loss: 1.39149499, g_loss: 0.69771421, ae_loss: 0.05090166\n",
      "Step: [3312] total_loss: 2.14284372 d_loss: 1.40552366, g_loss: 0.68580294, ae_loss: 0.05151705\n",
      "Step: [3313] total_loss: 2.12552190 d_loss: 1.40379465, g_loss: 0.66862553, ae_loss: 0.05310183\n",
      "Step: [3314] total_loss: 2.11717463 d_loss: 1.37942743, g_loss: 0.68911815, ae_loss: 0.04862905\n",
      "Step: [3315] total_loss: 2.13408875 d_loss: 1.38783097, g_loss: 0.69434059, ae_loss: 0.05191717\n",
      "Step: [3316] total_loss: 2.11467075 d_loss: 1.37470388, g_loss: 0.68786782, ae_loss: 0.05209905\n",
      "Step: [3317] total_loss: 2.11302352 d_loss: 1.37199819, g_loss: 0.69010472, ae_loss: 0.05092055\n",
      "Step: [3318] total_loss: 2.11090612 d_loss: 1.37886882, g_loss: 0.67989123, ae_loss: 0.05214620\n",
      "Step: [3319] total_loss: 2.12546563 d_loss: 1.39153981, g_loss: 0.68639404, ae_loss: 0.04753170\n",
      "Step: [3320] total_loss: 2.11884212 d_loss: 1.37529254, g_loss: 0.69095862, ae_loss: 0.05259098\n",
      "Step: [3321] total_loss: 2.13694954 d_loss: 1.39351213, g_loss: 0.68945956, ae_loss: 0.05397781\n",
      "Step: [3322] total_loss: 2.10743666 d_loss: 1.36234856, g_loss: 0.69193286, ae_loss: 0.05315528\n",
      "Step: [3323] total_loss: 2.12585711 d_loss: 1.38706315, g_loss: 0.68832701, ae_loss: 0.05046692\n",
      "Step: [3324] total_loss: 2.11595654 d_loss: 1.36689758, g_loss: 0.69714028, ae_loss: 0.05191872\n",
      "Step: [3325] total_loss: 2.12657452 d_loss: 1.37953305, g_loss: 0.70062190, ae_loss: 0.04641952\n",
      "Step: [3326] total_loss: 2.09956861 d_loss: 1.36805892, g_loss: 0.68070239, ae_loss: 0.05080741\n",
      "Step: [3327] total_loss: 2.09486151 d_loss: 1.37893403, g_loss: 0.66584635, ae_loss: 0.05008097\n",
      "Step: [3328] total_loss: 2.10546160 d_loss: 1.36967802, g_loss: 0.68200576, ae_loss: 0.05377782\n",
      "Step: [3329] total_loss: 2.11654782 d_loss: 1.38216364, g_loss: 0.68868202, ae_loss: 0.04570206\n",
      "Step: [3330] total_loss: 2.11372232 d_loss: 1.38051331, g_loss: 0.68165278, ae_loss: 0.05155624\n",
      "Step: [3331] total_loss: 2.13739181 d_loss: 1.40333486, g_loss: 0.68245387, ae_loss: 0.05160303\n",
      "Step: [3332] total_loss: 2.12133431 d_loss: 1.37119317, g_loss: 0.70113075, ae_loss: 0.04901037\n",
      "Step: [3333] total_loss: 2.12321806 d_loss: 1.38089204, g_loss: 0.69244218, ae_loss: 0.04988397\n",
      "Step: [3334] total_loss: 2.14956164 d_loss: 1.39269161, g_loss: 0.70190316, ae_loss: 0.05496694\n",
      "Step: [3335] total_loss: 2.13264489 d_loss: 1.38952279, g_loss: 0.69272113, ae_loss: 0.05040097\n",
      "Step: [3336] total_loss: 2.13626981 d_loss: 1.37909842, g_loss: 0.70376217, ae_loss: 0.05340918\n",
      "Step: [3337] total_loss: 2.12485123 d_loss: 1.38882422, g_loss: 0.68680763, ae_loss: 0.04921949\n",
      "Step: [3338] total_loss: 2.12667799 d_loss: 1.39439869, g_loss: 0.67967987, ae_loss: 0.05259951\n",
      "Step: [3339] total_loss: 2.13066578 d_loss: 1.39315069, g_loss: 0.68538368, ae_loss: 0.05213130\n",
      "Step: [3340] total_loss: 2.11423349 d_loss: 1.36739826, g_loss: 0.69298905, ae_loss: 0.05384625\n",
      "Step: [3341] total_loss: 2.12953401 d_loss: 1.38247132, g_loss: 0.69563270, ae_loss: 0.05143001\n",
      "Step: [3342] total_loss: 2.11978674 d_loss: 1.37954617, g_loss: 0.68630731, ae_loss: 0.05393322\n",
      "Step: [3343] total_loss: 2.12199426 d_loss: 1.36911726, g_loss: 0.70022529, ae_loss: 0.05265173\n",
      "Step: [3344] total_loss: 2.12878346 d_loss: 1.37449455, g_loss: 0.70213783, ae_loss: 0.05215111\n",
      "Step: [3345] total_loss: 2.12712479 d_loss: 1.35499704, g_loss: 0.72140694, ae_loss: 0.05072073\n",
      "Step: [3346] total_loss: 2.12690330 d_loss: 1.39289510, g_loss: 0.68357849, ae_loss: 0.05042974\n",
      "Step: [3347] total_loss: 2.10599780 d_loss: 1.36353493, g_loss: 0.69146699, ae_loss: 0.05099577\n",
      "Step: [3348] total_loss: 2.11496568 d_loss: 1.38489652, g_loss: 0.67596352, ae_loss: 0.05410563\n",
      "Step: [3349] total_loss: 2.10558844 d_loss: 1.38190079, g_loss: 0.67171383, ae_loss: 0.05197387\n",
      "Step: [3350] total_loss: 2.13632059 d_loss: 1.40041065, g_loss: 0.68476599, ae_loss: 0.05114403\n",
      "Step: [3351] total_loss: 2.12255335 d_loss: 1.38006020, g_loss: 0.68975395, ae_loss: 0.05273918\n",
      "Step: [3352] total_loss: 2.13076448 d_loss: 1.38146698, g_loss: 0.69948661, ae_loss: 0.04981076\n",
      "Step: [3353] total_loss: 2.10870337 d_loss: 1.37142718, g_loss: 0.68762195, ae_loss: 0.04965425\n",
      "Step: [3354] total_loss: 2.12646484 d_loss: 1.37190652, g_loss: 0.70118177, ae_loss: 0.05337673\n",
      "Step: [3355] total_loss: 2.11908484 d_loss: 1.36022902, g_loss: 0.70805413, ae_loss: 0.05080175\n",
      "Step: [3356] total_loss: 2.13521051 d_loss: 1.37713444, g_loss: 0.70803684, ae_loss: 0.05003916\n",
      "Step: [3357] total_loss: 2.12117147 d_loss: 1.38594723, g_loss: 0.68544781, ae_loss: 0.04977633\n",
      "Step: [3358] total_loss: 2.12195015 d_loss: 1.38732767, g_loss: 0.67149025, ae_loss: 0.06313214\n",
      "Step: [3359] total_loss: 2.12060499 d_loss: 1.38122857, g_loss: 0.68507969, ae_loss: 0.05429678\n",
      "Step: [3360] total_loss: 2.10352945 d_loss: 1.36186624, g_loss: 0.68965077, ae_loss: 0.05201249\n",
      "Step: [3361] total_loss: 2.11787558 d_loss: 1.37702239, g_loss: 0.69301546, ae_loss: 0.04783781\n",
      "Step: [3362] total_loss: 2.12819314 d_loss: 1.38819361, g_loss: 0.68804264, ae_loss: 0.05195685\n",
      "Step: [3363] total_loss: 2.11094189 d_loss: 1.36065340, g_loss: 0.69468522, ae_loss: 0.05560328\n",
      "Step: [3364] total_loss: 2.11960292 d_loss: 1.36725402, g_loss: 0.69726121, ae_loss: 0.05508773\n",
      "Step: [3365] total_loss: 2.13289404 d_loss: 1.38919187, g_loss: 0.69075668, ae_loss: 0.05294536\n",
      "Step: [3366] total_loss: 2.12739801 d_loss: 1.38093603, g_loss: 0.69462782, ae_loss: 0.05183408\n",
      "Step: [3367] total_loss: 2.16173077 d_loss: 1.39324117, g_loss: 0.71248865, ae_loss: 0.05600091\n",
      "Step: [3368] total_loss: 2.15079141 d_loss: 1.39936805, g_loss: 0.70016241, ae_loss: 0.05126097\n",
      "Step: [3369] total_loss: 2.13588619 d_loss: 1.38544989, g_loss: 0.70210809, ae_loss: 0.04832818\n",
      "Step: [3370] total_loss: 2.11179686 d_loss: 1.36230636, g_loss: 0.69801056, ae_loss: 0.05148008\n",
      "Step: [3371] total_loss: 2.12345099 d_loss: 1.36182117, g_loss: 0.71349728, ae_loss: 0.04813250\n",
      "Step: [3372] total_loss: 2.12357187 d_loss: 1.38506413, g_loss: 0.68661636, ae_loss: 0.05189127\n",
      "Step: [3373] total_loss: 2.11570668 d_loss: 1.37814963, g_loss: 0.68498200, ae_loss: 0.05257514\n",
      "Step: [3374] total_loss: 2.12721777 d_loss: 1.40277886, g_loss: 0.67672873, ae_loss: 0.04771029\n",
      "Step: [3375] total_loss: 2.13202906 d_loss: 1.38652325, g_loss: 0.69338179, ae_loss: 0.05212418\n",
      "Step: [3376] total_loss: 2.13471556 d_loss: 1.36568141, g_loss: 0.71588874, ae_loss: 0.05314552\n",
      "Step: [3377] total_loss: 2.10903668 d_loss: 1.38021421, g_loss: 0.68054134, ae_loss: 0.04828125\n",
      "Step: [3378] total_loss: 2.12680936 d_loss: 1.38862693, g_loss: 0.68715906, ae_loss: 0.05102333\n",
      "Step: [3379] total_loss: 2.11875081 d_loss: 1.38182700, g_loss: 0.68420786, ae_loss: 0.05271596\n",
      "Step: [3380] total_loss: 2.12671614 d_loss: 1.38107228, g_loss: 0.69315654, ae_loss: 0.05248741\n",
      "Step: [3381] total_loss: 2.13720965 d_loss: 1.40006769, g_loss: 0.68650359, ae_loss: 0.05063846\n",
      "Step: [3382] total_loss: 2.13445306 d_loss: 1.37758124, g_loss: 0.70689231, ae_loss: 0.04997952\n",
      "Step: [3383] total_loss: 2.13195038 d_loss: 1.38248348, g_loss: 0.70208776, ae_loss: 0.04737900\n",
      "Step: [3384] total_loss: 2.11592388 d_loss: 1.35432291, g_loss: 0.70947099, ae_loss: 0.05213004\n",
      "Step: [3385] total_loss: 2.12565589 d_loss: 1.39035046, g_loss: 0.68080854, ae_loss: 0.05449683\n",
      "Step: [3386] total_loss: 2.13043118 d_loss: 1.39254665, g_loss: 0.68632913, ae_loss: 0.05155554\n",
      "Step: [3387] total_loss: 2.11954021 d_loss: 1.38406658, g_loss: 0.68393075, ae_loss: 0.05154295\n",
      "Step: [3388] total_loss: 2.11268091 d_loss: 1.38717413, g_loss: 0.67480338, ae_loss: 0.05070343\n",
      "Step: [3389] total_loss: 2.13971519 d_loss: 1.39803684, g_loss: 0.69079220, ae_loss: 0.05088606\n",
      "Step: [3390] total_loss: 2.11700869 d_loss: 1.36705470, g_loss: 0.69494450, ae_loss: 0.05500931\n",
      "Step: [3391] total_loss: 2.10223722 d_loss: 1.36676979, g_loss: 0.68745530, ae_loss: 0.04801221\n",
      "Step: [3392] total_loss: 2.13642406 d_loss: 1.39585125, g_loss: 0.68960583, ae_loss: 0.05096685\n",
      "Step: [3393] total_loss: 2.10641313 d_loss: 1.35881829, g_loss: 0.69697809, ae_loss: 0.05061674\n",
      "Step: [3394] total_loss: 2.09670949 d_loss: 1.35964620, g_loss: 0.68647557, ae_loss: 0.05058777\n",
      "Step: [3395] total_loss: 2.11329055 d_loss: 1.38352275, g_loss: 0.67570508, ae_loss: 0.05406278\n",
      "Step: [3396] total_loss: 2.12819386 d_loss: 1.38898671, g_loss: 0.68755507, ae_loss: 0.05165224\n",
      "Step: [3397] total_loss: 2.12000775 d_loss: 1.38168931, g_loss: 0.69450432, ae_loss: 0.04381420\n",
      "Step: [3398] total_loss: 2.12088728 d_loss: 1.37336135, g_loss: 0.69752330, ae_loss: 0.05000255\n",
      "Step: [3399] total_loss: 2.12589550 d_loss: 1.37536526, g_loss: 0.69842160, ae_loss: 0.05210876\n",
      "Step: [3400] total_loss: 2.12754393 d_loss: 1.37405670, g_loss: 0.70029420, ae_loss: 0.05319311\n",
      "Step: [3401] total_loss: 2.12855244 d_loss: 1.37124431, g_loss: 0.70477867, ae_loss: 0.05252936\n",
      "Step: [3402] total_loss: 2.12685847 d_loss: 1.37601221, g_loss: 0.70023388, ae_loss: 0.05061249\n",
      "Step: [3403] total_loss: 2.12661099 d_loss: 1.36531556, g_loss: 0.70848286, ae_loss: 0.05281259\n",
      "Step: [3404] total_loss: 2.11882544 d_loss: 1.38558269, g_loss: 0.68068099, ae_loss: 0.05256187\n",
      "Step: [3405] total_loss: 2.12310243 d_loss: 1.38425314, g_loss: 0.68313116, ae_loss: 0.05571822\n",
      "Step: [3406] total_loss: 2.12518501 d_loss: 1.36071062, g_loss: 0.71216536, ae_loss: 0.05230914\n",
      "Step: [3407] total_loss: 2.16449022 d_loss: 1.43366694, g_loss: 0.68398750, ae_loss: 0.04683571\n",
      "Step: [3408] total_loss: 2.12118244 d_loss: 1.39119136, g_loss: 0.67598104, ae_loss: 0.05401020\n",
      "Step: [3409] total_loss: 2.11386347 d_loss: 1.36266208, g_loss: 0.69979417, ae_loss: 0.05140716\n",
      "Step: [3410] total_loss: 2.12223291 d_loss: 1.40702748, g_loss: 0.66234797, ae_loss: 0.05285749\n",
      "Step: [3411] total_loss: 2.09162903 d_loss: 1.34816170, g_loss: 0.69254565, ae_loss: 0.05092166\n",
      "Step: [3412] total_loss: 2.12756133 d_loss: 1.37151730, g_loss: 0.70786721, ae_loss: 0.04817686\n",
      "Step: [3413] total_loss: 2.12292433 d_loss: 1.37659562, g_loss: 0.69488788, ae_loss: 0.05144095\n",
      "Step: [3414] total_loss: 2.11936092 d_loss: 1.36196208, g_loss: 0.70420885, ae_loss: 0.05318990\n",
      "Step: [3415] total_loss: 2.14167023 d_loss: 1.36665964, g_loss: 0.72477472, ae_loss: 0.05023595\n",
      "Step: [3416] total_loss: 2.12952709 d_loss: 1.39085162, g_loss: 0.68951070, ae_loss: 0.04916493\n",
      "Step: [3417] total_loss: 2.12633514 d_loss: 1.38976061, g_loss: 0.68438482, ae_loss: 0.05218977\n",
      "Step: [3418] total_loss: 2.11850858 d_loss: 1.39070535, g_loss: 0.67848450, ae_loss: 0.04931877\n",
      "Step: [3419] total_loss: 2.11409473 d_loss: 1.39229250, g_loss: 0.66893864, ae_loss: 0.05286366\n",
      "Step: [3420] total_loss: 2.11568117 d_loss: 1.38324440, g_loss: 0.68224633, ae_loss: 0.05019044\n",
      "Step: [3421] total_loss: 2.12638211 d_loss: 1.38979149, g_loss: 0.68263733, ae_loss: 0.05395329\n",
      "Step: [3422] total_loss: 2.12617302 d_loss: 1.38289797, g_loss: 0.69041938, ae_loss: 0.05285557\n",
      "Step: [3423] total_loss: 2.12827873 d_loss: 1.39723659, g_loss: 0.67790341, ae_loss: 0.05313890\n",
      "Step: [3424] total_loss: 2.10056448 d_loss: 1.34798896, g_loss: 0.70426250, ae_loss: 0.04831296\n",
      "Step: [3425] total_loss: 2.11063600 d_loss: 1.37712717, g_loss: 0.68386978, ae_loss: 0.04963899\n",
      "Step: [3426] total_loss: 2.10515428 d_loss: 1.35859048, g_loss: 0.69689006, ae_loss: 0.04967378\n",
      "Step: [3427] total_loss: 2.13848162 d_loss: 1.39846539, g_loss: 0.68493283, ae_loss: 0.05508329\n",
      "Step: [3428] total_loss: 2.12154841 d_loss: 1.37952054, g_loss: 0.69155735, ae_loss: 0.05047061\n",
      "Step: [3429] total_loss: 2.14112473 d_loss: 1.38234591, g_loss: 0.70473230, ae_loss: 0.05404647\n",
      "Step: [3430] total_loss: 2.12489963 d_loss: 1.38388658, g_loss: 0.69138509, ae_loss: 0.04962798\n",
      "Step: [3431] total_loss: 2.12664080 d_loss: 1.38998973, g_loss: 0.68921840, ae_loss: 0.04743253\n",
      "Step: [3432] total_loss: 2.11702752 d_loss: 1.38839507, g_loss: 0.67767340, ae_loss: 0.05095914\n",
      "Step: [3433] total_loss: 2.12639189 d_loss: 1.36251783, g_loss: 0.71023601, ae_loss: 0.05363815\n",
      "Step: [3434] total_loss: 2.15180755 d_loss: 1.40094805, g_loss: 0.69849223, ae_loss: 0.05236724\n",
      "Step: [3435] total_loss: 2.11999655 d_loss: 1.38285947, g_loss: 0.68729299, ae_loss: 0.04984401\n",
      "Step: [3436] total_loss: 2.13739371 d_loss: 1.39618587, g_loss: 0.68483967, ae_loss: 0.05636810\n",
      "Step: [3437] total_loss: 2.12143207 d_loss: 1.38641083, g_loss: 0.68485993, ae_loss: 0.05016124\n",
      "Step: [3438] total_loss: 2.10946417 d_loss: 1.37850475, g_loss: 0.68035495, ae_loss: 0.05060444\n",
      "Step: [3439] total_loss: 2.12861085 d_loss: 1.38107991, g_loss: 0.69740474, ae_loss: 0.05012616\n",
      "Step: [3440] total_loss: 2.10592198 d_loss: 1.36564696, g_loss: 0.69103295, ae_loss: 0.04924212\n",
      "Step: [3441] total_loss: 2.12973523 d_loss: 1.38880646, g_loss: 0.69116414, ae_loss: 0.04976461\n",
      "Step: [3442] total_loss: 2.15313530 d_loss: 1.39443016, g_loss: 0.70538557, ae_loss: 0.05331942\n",
      "Step: [3443] total_loss: 2.14063311 d_loss: 1.38184929, g_loss: 0.70876646, ae_loss: 0.05001729\n",
      "Step: [3444] total_loss: 2.13077044 d_loss: 1.37261891, g_loss: 0.71021932, ae_loss: 0.04793232\n",
      "Step: [3445] total_loss: 2.13093638 d_loss: 1.36936736, g_loss: 0.71454501, ae_loss: 0.04702402\n",
      "Step: [3446] total_loss: 2.12898445 d_loss: 1.37907910, g_loss: 0.69764483, ae_loss: 0.05226038\n",
      "Step: [3447] total_loss: 2.13002419 d_loss: 1.39190149, g_loss: 0.68658715, ae_loss: 0.05153551\n",
      "Step: [3448] total_loss: 2.12267423 d_loss: 1.38449383, g_loss: 0.68564558, ae_loss: 0.05253480\n",
      "Step: [3449] total_loss: 2.13469696 d_loss: 1.39156675, g_loss: 0.69201726, ae_loss: 0.05111305\n",
      "Step: [3450] total_loss: 2.10633969 d_loss: 1.36798275, g_loss: 0.69049793, ae_loss: 0.04785908\n",
      "Step: [3451] total_loss: 2.13424492 d_loss: 1.40118372, g_loss: 0.68110090, ae_loss: 0.05196026\n",
      "Step: [3452] total_loss: 2.12272263 d_loss: 1.37906051, g_loss: 0.69231629, ae_loss: 0.05134597\n",
      "Step: [3453] total_loss: 2.13355350 d_loss: 1.39077425, g_loss: 0.69208062, ae_loss: 0.05069861\n",
      "Step: [3454] total_loss: 2.12255526 d_loss: 1.37233090, g_loss: 0.70108998, ae_loss: 0.04913438\n",
      "Step: [3455] total_loss: 2.15627813 d_loss: 1.39562345, g_loss: 0.71038836, ae_loss: 0.05026644\n",
      "Step: [3456] total_loss: 2.16596603 d_loss: 1.42161441, g_loss: 0.69296789, ae_loss: 0.05138383\n",
      "Step: [3457] total_loss: 2.13604403 d_loss: 1.37257648, g_loss: 0.71438539, ae_loss: 0.04908207\n",
      "Step: [3458] total_loss: 2.14045668 d_loss: 1.38125229, g_loss: 0.70458490, ae_loss: 0.05461939\n",
      "Step: [3459] total_loss: 2.12248373 d_loss: 1.37806499, g_loss: 0.68974358, ae_loss: 0.05467518\n",
      "Step: [3460] total_loss: 2.12856007 d_loss: 1.39782619, g_loss: 0.67972696, ae_loss: 0.05100685\n",
      "Step: [3461] total_loss: 2.13485885 d_loss: 1.39167035, g_loss: 0.69182086, ae_loss: 0.05136761\n",
      "Step: [3462] total_loss: 2.14173317 d_loss: 1.40136909, g_loss: 0.68856335, ae_loss: 0.05180067\n",
      "Step: [3463] total_loss: 2.13904691 d_loss: 1.38434315, g_loss: 0.70344007, ae_loss: 0.05126370\n",
      "Step: [3464] total_loss: 2.12524009 d_loss: 1.36523461, g_loss: 0.70775783, ae_loss: 0.05224768\n",
      "Step: [3465] total_loss: 2.12747574 d_loss: 1.37578404, g_loss: 0.70404112, ae_loss: 0.04765045\n",
      "Step: [3466] total_loss: 2.15816545 d_loss: 1.42000008, g_loss: 0.68382335, ae_loss: 0.05434220\n",
      "Step: [3467] total_loss: 2.13645554 d_loss: 1.38484073, g_loss: 0.70048195, ae_loss: 0.05113292\n",
      "Step: [3468] total_loss: 2.13187170 d_loss: 1.38274539, g_loss: 0.70062220, ae_loss: 0.04850405\n",
      "Step: [3469] total_loss: 2.13762975 d_loss: 1.39402640, g_loss: 0.69586754, ae_loss: 0.04773579\n",
      "Step: [3470] total_loss: 2.11928177 d_loss: 1.36597180, g_loss: 0.70281506, ae_loss: 0.05049492\n",
      "Step: [3471] total_loss: 2.12528825 d_loss: 1.38091087, g_loss: 0.68998086, ae_loss: 0.05439645\n",
      "Step: [3472] total_loss: 2.13489938 d_loss: 1.40082455, g_loss: 0.68453729, ae_loss: 0.04953748\n",
      "Step: [3473] total_loss: 2.12748957 d_loss: 1.37115455, g_loss: 0.70509529, ae_loss: 0.05123959\n",
      "Step: [3474] total_loss: 2.12730026 d_loss: 1.37794125, g_loss: 0.70462811, ae_loss: 0.04473101\n",
      "Step: [3475] total_loss: 2.12075329 d_loss: 1.36378193, g_loss: 0.70426595, ae_loss: 0.05270554\n",
      "Step: [3476] total_loss: 2.12843704 d_loss: 1.36605835, g_loss: 0.71488941, ae_loss: 0.04748936\n",
      "Step: [3477] total_loss: 2.10965395 d_loss: 1.37809896, g_loss: 0.68318880, ae_loss: 0.04836614\n",
      "Step: [3478] total_loss: 2.10779691 d_loss: 1.35652351, g_loss: 0.70280093, ae_loss: 0.04847242\n",
      "Step: [3479] total_loss: 2.11797857 d_loss: 1.36519170, g_loss: 0.70472258, ae_loss: 0.04806430\n",
      "Step: [3480] total_loss: 2.13851261 d_loss: 1.39754593, g_loss: 0.69108665, ae_loss: 0.04988002\n",
      "Step: [3481] total_loss: 2.12179756 d_loss: 1.38546252, g_loss: 0.68655431, ae_loss: 0.04978063\n",
      "Step: [3482] total_loss: 2.13154793 d_loss: 1.39536345, g_loss: 0.68466175, ae_loss: 0.05152263\n",
      "Step: [3483] total_loss: 2.10512781 d_loss: 1.38243985, g_loss: 0.67618847, ae_loss: 0.04649935\n",
      "Step: [3484] total_loss: 2.14572835 d_loss: 1.40936804, g_loss: 0.68357784, ae_loss: 0.05278250\n",
      "Step: [3485] total_loss: 2.13616800 d_loss: 1.39179826, g_loss: 0.69599164, ae_loss: 0.04837821\n",
      "Step: [3486] total_loss: 2.13001490 d_loss: 1.39455032, g_loss: 0.68333757, ae_loss: 0.05212712\n",
      "Step: [3487] total_loss: 2.13461733 d_loss: 1.39186120, g_loss: 0.69012064, ae_loss: 0.05263551\n",
      "Step: [3488] total_loss: 2.13229084 d_loss: 1.38485360, g_loss: 0.69917226, ae_loss: 0.04826508\n",
      "Step: [3489] total_loss: 2.12531519 d_loss: 1.37784398, g_loss: 0.69743907, ae_loss: 0.05003223\n",
      "Step: [3490] total_loss: 2.13200569 d_loss: 1.37753272, g_loss: 0.70393378, ae_loss: 0.05053911\n",
      "Step: [3491] total_loss: 2.13481522 d_loss: 1.38383436, g_loss: 0.69957733, ae_loss: 0.05140362\n",
      "Step: [3492] total_loss: 2.11485338 d_loss: 1.36451375, g_loss: 0.69980633, ae_loss: 0.05053328\n",
      "Step: [3493] total_loss: 2.12659621 d_loss: 1.38301766, g_loss: 0.69079494, ae_loss: 0.05278363\n",
      "Step: [3494] total_loss: 2.13135624 d_loss: 1.39023244, g_loss: 0.68758190, ae_loss: 0.05354203\n",
      "Step: [3495] total_loss: 2.14534068 d_loss: 1.37885427, g_loss: 0.71362644, ae_loss: 0.05285999\n",
      "Step: [3496] total_loss: 2.13096762 d_loss: 1.38242531, g_loss: 0.69852579, ae_loss: 0.05001663\n",
      "Step: [3497] total_loss: 2.14041567 d_loss: 1.38636231, g_loss: 0.70195240, ae_loss: 0.05210090\n",
      "Step: [3498] total_loss: 2.14986944 d_loss: 1.38377368, g_loss: 0.71085846, ae_loss: 0.05523711\n",
      "Step: [3499] total_loss: 2.14108372 d_loss: 1.39529169, g_loss: 0.69018555, ae_loss: 0.05560662\n",
      "Step: [3500] total_loss: 2.13402510 d_loss: 1.39314628, g_loss: 0.68924057, ae_loss: 0.05163808\n",
      "Step: [3501] total_loss: 2.13744926 d_loss: 1.37381363, g_loss: 0.70635235, ae_loss: 0.05728322\n",
      "Step: [3502] total_loss: 2.13054800 d_loss: 1.38581884, g_loss: 0.69124919, ae_loss: 0.05348007\n",
      "Step: [3503] total_loss: 2.11701107 d_loss: 1.37766445, g_loss: 0.68981230, ae_loss: 0.04953437\n",
      "Step: [3504] total_loss: 2.11979389 d_loss: 1.38337827, g_loss: 0.68523508, ae_loss: 0.05118057\n",
      "Step: [3505] total_loss: 2.12978649 d_loss: 1.38443756, g_loss: 0.69643199, ae_loss: 0.04891707\n",
      "Step: [3506] total_loss: 2.13080335 d_loss: 1.38733411, g_loss: 0.68912655, ae_loss: 0.05434263\n",
      "Step: [3507] total_loss: 2.12492943 d_loss: 1.37508631, g_loss: 0.69933367, ae_loss: 0.05050943\n",
      "Step: [3508] total_loss: 2.13408947 d_loss: 1.40078282, g_loss: 0.68153954, ae_loss: 0.05176698\n",
      "Step: [3509] total_loss: 2.13297844 d_loss: 1.38771105, g_loss: 0.69729364, ae_loss: 0.04797376\n",
      "Step: [3510] total_loss: 2.11859465 d_loss: 1.38011265, g_loss: 0.68672013, ae_loss: 0.05176193\n",
      "Step: [3511] total_loss: 2.13681221 d_loss: 1.39729881, g_loss: 0.68894958, ae_loss: 0.05056368\n",
      "Step: [3512] total_loss: 2.14605951 d_loss: 1.38033676, g_loss: 0.71736169, ae_loss: 0.04836089\n",
      "Step: [3513] total_loss: 2.14475727 d_loss: 1.40254283, g_loss: 0.69113088, ae_loss: 0.05108346\n",
      "Step: [3514] total_loss: 2.14851141 d_loss: 1.38996756, g_loss: 0.71032405, ae_loss: 0.04821975\n",
      "Step: [3515] total_loss: 2.10865879 d_loss: 1.35822165, g_loss: 0.70201886, ae_loss: 0.04841831\n",
      "Step: [3516] total_loss: 2.12372971 d_loss: 1.38979506, g_loss: 0.68660319, ae_loss: 0.04733130\n",
      "Step: [3517] total_loss: 2.13035440 d_loss: 1.39698076, g_loss: 0.68009531, ae_loss: 0.05327835\n",
      "Step: [3518] total_loss: 2.11201692 d_loss: 1.37546337, g_loss: 0.68175679, ae_loss: 0.05479673\n",
      "Step: [3519] total_loss: 2.13484716 d_loss: 1.40415883, g_loss: 0.67598021, ae_loss: 0.05470824\n",
      "Step: [3520] total_loss: 2.11663365 d_loss: 1.39154601, g_loss: 0.68156016, ae_loss: 0.04352751\n",
      "Step: [3521] total_loss: 2.12281871 d_loss: 1.38360476, g_loss: 0.68447852, ae_loss: 0.05473546\n",
      "Step: [3522] total_loss: 2.13526034 d_loss: 1.38600135, g_loss: 0.69614351, ae_loss: 0.05311551\n",
      "Step: [3523] total_loss: 2.13123393 d_loss: 1.38100648, g_loss: 0.69564545, ae_loss: 0.05458201\n",
      "Step: [3524] total_loss: 2.11824298 d_loss: 1.37990582, g_loss: 0.68790329, ae_loss: 0.05043387\n",
      "Step: [3525] total_loss: 2.13040996 d_loss: 1.38750792, g_loss: 0.69517875, ae_loss: 0.04772327\n",
      "Step: [3526] total_loss: 2.12243986 d_loss: 1.38002312, g_loss: 0.69103420, ae_loss: 0.05138246\n",
      "Step: [3527] total_loss: 2.12068534 d_loss: 1.37950981, g_loss: 0.68900108, ae_loss: 0.05217449\n",
      "Step: [3528] total_loss: 2.12598872 d_loss: 1.37261152, g_loss: 0.70265627, ae_loss: 0.05072087\n",
      "Step: [3529] total_loss: 2.11207724 d_loss: 1.37487483, g_loss: 0.69031060, ae_loss: 0.04689166\n",
      "Step: [3530] total_loss: 2.11744380 d_loss: 1.37941504, g_loss: 0.68801838, ae_loss: 0.05001038\n",
      "Step: [3531] total_loss: 2.11263680 d_loss: 1.36631632, g_loss: 0.69359517, ae_loss: 0.05272532\n",
      "Step: [3532] total_loss: 2.11159182 d_loss: 1.37627447, g_loss: 0.68660486, ae_loss: 0.04871259\n",
      "Step: [3533] total_loss: 2.11372066 d_loss: 1.38242006, g_loss: 0.68088603, ae_loss: 0.05041454\n",
      "Step: [3534] total_loss: 2.11272049 d_loss: 1.37657464, g_loss: 0.69007552, ae_loss: 0.04607036\n",
      "Step: [3535] total_loss: 2.11851740 d_loss: 1.38152778, g_loss: 0.68905795, ae_loss: 0.04793178\n",
      "Step: [3536] total_loss: 2.13039255 d_loss: 1.40986013, g_loss: 0.66614813, ae_loss: 0.05438430\n",
      "Step: [3537] total_loss: 2.10116124 d_loss: 1.37163651, g_loss: 0.67969328, ae_loss: 0.04983143\n",
      "Step: [3538] total_loss: 2.15728354 d_loss: 1.39883041, g_loss: 0.70676178, ae_loss: 0.05169130\n",
      "Step: [3539] total_loss: 2.11827421 d_loss: 1.38024998, g_loss: 0.68807256, ae_loss: 0.04995162\n",
      "Step: [3540] total_loss: 2.12293386 d_loss: 1.36794055, g_loss: 0.69735104, ae_loss: 0.05764219\n",
      "Step: [3541] total_loss: 2.13288879 d_loss: 1.37671137, g_loss: 0.71092153, ae_loss: 0.04525588\n",
      "Step: [3542] total_loss: 2.15134478 d_loss: 1.42262888, g_loss: 0.67613053, ae_loss: 0.05258532\n",
      "Step: [3543] total_loss: 2.13131857 d_loss: 1.38432860, g_loss: 0.69554615, ae_loss: 0.05144395\n",
      "Step: [3544] total_loss: 2.11929417 d_loss: 1.38491321, g_loss: 0.68012881, ae_loss: 0.05425201\n",
      "Step: [3545] total_loss: 2.14261842 d_loss: 1.38447189, g_loss: 0.70134306, ae_loss: 0.05680343\n",
      "Step: [3546] total_loss: 2.11887884 d_loss: 1.37019706, g_loss: 0.69773734, ae_loss: 0.05094452\n",
      "Step: [3547] total_loss: 2.13172102 d_loss: 1.36873496, g_loss: 0.71351218, ae_loss: 0.04947373\n",
      "Step: [3548] total_loss: 2.13287878 d_loss: 1.39396954, g_loss: 0.68726838, ae_loss: 0.05164081\n",
      "Step: [3549] total_loss: 2.14035249 d_loss: 1.38421392, g_loss: 0.70615989, ae_loss: 0.04997876\n",
      "Step: [3550] total_loss: 2.12641382 d_loss: 1.38799405, g_loss: 0.68690455, ae_loss: 0.05151531\n",
      "Step: [3551] total_loss: 2.14768219 d_loss: 1.40664852, g_loss: 0.68949234, ae_loss: 0.05154123\n",
      "Step: [3552] total_loss: 2.12374210 d_loss: 1.36625385, g_loss: 0.70411307, ae_loss: 0.05337511\n",
      "Step: [3553] total_loss: 2.14511395 d_loss: 1.39819360, g_loss: 0.69325149, ae_loss: 0.05366878\n",
      "Step: [3554] total_loss: 2.13702846 d_loss: 1.39238977, g_loss: 0.69071937, ae_loss: 0.05391933\n",
      "Step: [3555] total_loss: 2.11738753 d_loss: 1.37484050, g_loss: 0.68681324, ae_loss: 0.05573385\n",
      "Step: [3556] total_loss: 2.09981203 d_loss: 1.37270200, g_loss: 0.67244744, ae_loss: 0.05466267\n",
      "Step: [3557] total_loss: 2.09909177 d_loss: 1.37354851, g_loss: 0.67692822, ae_loss: 0.04861512\n",
      "Step: [3558] total_loss: 2.12593579 d_loss: 1.38811040, g_loss: 0.68642253, ae_loss: 0.05140282\n",
      "Step: [3559] total_loss: 2.13932133 d_loss: 1.40470910, g_loss: 0.68397260, ae_loss: 0.05063957\n",
      "Step: [3560] total_loss: 2.13525581 d_loss: 1.38348854, g_loss: 0.70190459, ae_loss: 0.04986266\n",
      "Step: [3561] total_loss: 2.12829328 d_loss: 1.38175368, g_loss: 0.69460911, ae_loss: 0.05193052\n",
      "Step: [3562] total_loss: 2.13427162 d_loss: 1.39259911, g_loss: 0.69079542, ae_loss: 0.05087711\n",
      "Step: [3563] total_loss: 2.13418269 d_loss: 1.39134574, g_loss: 0.69085997, ae_loss: 0.05197697\n",
      "Step: [3564] total_loss: 2.12920785 d_loss: 1.38356614, g_loss: 0.69275898, ae_loss: 0.05288263\n",
      "Step: [3565] total_loss: 2.14185715 d_loss: 1.40573740, g_loss: 0.68415350, ae_loss: 0.05196629\n",
      "Step: [3566] total_loss: 2.11148310 d_loss: 1.37682343, g_loss: 0.68837655, ae_loss: 0.04628299\n",
      "Step: [3567] total_loss: 2.11929822 d_loss: 1.39720631, g_loss: 0.67252618, ae_loss: 0.04956564\n",
      "Step: [3568] total_loss: 2.11504579 d_loss: 1.37197566, g_loss: 0.69234431, ae_loss: 0.05072587\n",
      "Step: [3569] total_loss: 2.12444997 d_loss: 1.38766718, g_loss: 0.68489277, ae_loss: 0.05188999\n",
      "Step: [3570] total_loss: 2.14079142 d_loss: 1.39179051, g_loss: 0.69929218, ae_loss: 0.04970884\n",
      "Step: [3571] total_loss: 2.13164997 d_loss: 1.39496911, g_loss: 0.68772745, ae_loss: 0.04895324\n",
      "Step: [3572] total_loss: 2.11430740 d_loss: 1.36978984, g_loss: 0.69510108, ae_loss: 0.04941651\n",
      "Step: [3573] total_loss: 2.11900759 d_loss: 1.37529528, g_loss: 0.69211656, ae_loss: 0.05159571\n",
      "Step: [3574] total_loss: 2.14159060 d_loss: 1.40396690, g_loss: 0.68701172, ae_loss: 0.05061196\n",
      "Step: [3575] total_loss: 2.11759329 d_loss: 1.38489223, g_loss: 0.68171692, ae_loss: 0.05098404\n",
      "Step: [3576] total_loss: 2.12945127 d_loss: 1.38524234, g_loss: 0.69478351, ae_loss: 0.04942533\n",
      "Step: [3577] total_loss: 2.10976982 d_loss: 1.36278582, g_loss: 0.69579291, ae_loss: 0.05119109\n",
      "Step: [3578] total_loss: 2.13349652 d_loss: 1.38761020, g_loss: 0.69577748, ae_loss: 0.05010876\n",
      "Step: [3579] total_loss: 2.11954951 d_loss: 1.36188877, g_loss: 0.70508420, ae_loss: 0.05257658\n",
      "Step: [3580] total_loss: 2.11564064 d_loss: 1.38126612, g_loss: 0.68399203, ae_loss: 0.05038239\n",
      "Step: [3581] total_loss: 2.11348772 d_loss: 1.37425017, g_loss: 0.68942845, ae_loss: 0.04980928\n",
      "Step: [3582] total_loss: 2.11165762 d_loss: 1.34670615, g_loss: 0.71684510, ae_loss: 0.04810635\n",
      "Step: [3583] total_loss: 2.11132193 d_loss: 1.38301873, g_loss: 0.68067372, ae_loss: 0.04762945\n",
      "Step: [3584] total_loss: 2.13010049 d_loss: 1.39390695, g_loss: 0.68666154, ae_loss: 0.04953200\n",
      "Step: [3585] total_loss: 2.13350582 d_loss: 1.38711309, g_loss: 0.69250286, ae_loss: 0.05388972\n",
      "Step: [3586] total_loss: 2.12235713 d_loss: 1.37071085, g_loss: 0.70081139, ae_loss: 0.05083493\n",
      "Step: [3587] total_loss: 2.12655497 d_loss: 1.37243891, g_loss: 0.70137715, ae_loss: 0.05273905\n",
      "Step: [3588] total_loss: 2.13072062 d_loss: 1.38745296, g_loss: 0.69044322, ae_loss: 0.05282447\n",
      "Step: [3589] total_loss: 2.14131546 d_loss: 1.39300811, g_loss: 0.69787991, ae_loss: 0.05042732\n",
      "Step: [3590] total_loss: 2.11874032 d_loss: 1.36101675, g_loss: 0.70664108, ae_loss: 0.05108247\n",
      "Step: [3591] total_loss: 2.13424635 d_loss: 1.37739182, g_loss: 0.70454180, ae_loss: 0.05231275\n",
      "Step: [3592] total_loss: 2.12560821 d_loss: 1.37497783, g_loss: 0.70240611, ae_loss: 0.04822424\n",
      "Step: [3593] total_loss: 2.12194967 d_loss: 1.38300467, g_loss: 0.68788624, ae_loss: 0.05105886\n",
      "Step: [3594] total_loss: 2.15219069 d_loss: 1.40881693, g_loss: 0.68887430, ae_loss: 0.05449947\n",
      "Step: [3595] total_loss: 2.12504101 d_loss: 1.37550724, g_loss: 0.70075196, ae_loss: 0.04878189\n",
      "Step: [3596] total_loss: 2.12904763 d_loss: 1.38778055, g_loss: 0.69067228, ae_loss: 0.05059478\n",
      "Step: [3597] total_loss: 2.13152361 d_loss: 1.39494336, g_loss: 0.68425214, ae_loss: 0.05232811\n",
      "Step: [3598] total_loss: 2.13478374 d_loss: 1.40374291, g_loss: 0.68007427, ae_loss: 0.05096664\n",
      "Step: [3599] total_loss: 2.11842108 d_loss: 1.37785482, g_loss: 0.68506753, ae_loss: 0.05549859\n",
      "Step: [3600] total_loss: 2.12132215 d_loss: 1.39091992, g_loss: 0.68257892, ae_loss: 0.04782330\n",
      "Step: [3601] total_loss: 2.13187575 d_loss: 1.37022924, g_loss: 0.71177632, ae_loss: 0.04987009\n",
      "Step: [3602] total_loss: 2.12703562 d_loss: 1.39939630, g_loss: 0.67618239, ae_loss: 0.05145698\n",
      "Step: [3603] total_loss: 2.12190199 d_loss: 1.38622141, g_loss: 0.68397915, ae_loss: 0.05170145\n",
      "Step: [3604] total_loss: 2.10205936 d_loss: 1.37227249, g_loss: 0.67870307, ae_loss: 0.05108370\n",
      "Step: [3605] total_loss: 2.12594604 d_loss: 1.39377451, g_loss: 0.68269098, ae_loss: 0.04948069\n",
      "Step: [3606] total_loss: 2.13658857 d_loss: 1.38646555, g_loss: 0.70656395, ae_loss: 0.04355893\n",
      "Step: [3607] total_loss: 2.12898445 d_loss: 1.38554120, g_loss: 0.69197762, ae_loss: 0.05146578\n",
      "Step: [3608] total_loss: 2.13771772 d_loss: 1.39791608, g_loss: 0.69045818, ae_loss: 0.04934363\n",
      "Step: [3609] total_loss: 2.15716791 d_loss: 1.39140773, g_loss: 0.71305799, ae_loss: 0.05270228\n",
      "Step: [3610] total_loss: 2.14823580 d_loss: 1.38492179, g_loss: 0.70708066, ae_loss: 0.05623341\n",
      "Step: [3611] total_loss: 2.13109183 d_loss: 1.38005722, g_loss: 0.70000452, ae_loss: 0.05103003\n",
      "Step: [3612] total_loss: 2.12519312 d_loss: 1.38231003, g_loss: 0.69158554, ae_loss: 0.05129741\n",
      "Step: [3613] total_loss: 2.14360118 d_loss: 1.38917172, g_loss: 0.70142144, ae_loss: 0.05300804\n",
      "Step: [3614] total_loss: 2.13868690 d_loss: 1.39776230, g_loss: 0.68782723, ae_loss: 0.05309739\n",
      "Step: [3615] total_loss: 2.12510538 d_loss: 1.38286150, g_loss: 0.68800235, ae_loss: 0.05424143\n",
      "Step: [3616] total_loss: 2.12930107 d_loss: 1.38234627, g_loss: 0.69536412, ae_loss: 0.05159057\n",
      "Step: [3617] total_loss: 2.14892650 d_loss: 1.39381909, g_loss: 0.70268101, ae_loss: 0.05242632\n",
      "Step: [3618] total_loss: 2.13341188 d_loss: 1.37244892, g_loss: 0.71079099, ae_loss: 0.05017187\n",
      "Step: [3619] total_loss: 2.14097309 d_loss: 1.39703512, g_loss: 0.69227374, ae_loss: 0.05166426\n",
      "Step: [3620] total_loss: 2.12729692 d_loss: 1.38127518, g_loss: 0.69298387, ae_loss: 0.05303804\n",
      "Step: [3621] total_loss: 2.13053799 d_loss: 1.39364350, g_loss: 0.68365824, ae_loss: 0.05323607\n",
      "Step: [3622] total_loss: 2.10097551 d_loss: 1.36254907, g_loss: 0.68904459, ae_loss: 0.04938179\n",
      "Step: [3623] total_loss: 2.13880372 d_loss: 1.40234447, g_loss: 0.68641144, ae_loss: 0.05004780\n",
      "Step: [3624] total_loss: 2.13465738 d_loss: 1.39097917, g_loss: 0.69113541, ae_loss: 0.05254271\n",
      "Step: [3625] total_loss: 2.12894559 d_loss: 1.38235760, g_loss: 0.69269931, ae_loss: 0.05388862\n",
      "Step: [3626] total_loss: 2.12334847 d_loss: 1.37312210, g_loss: 0.69611675, ae_loss: 0.05410959\n",
      "Step: [3627] total_loss: 2.13031292 d_loss: 1.37730813, g_loss: 0.70012653, ae_loss: 0.05287820\n",
      "Step: [3628] total_loss: 2.14595580 d_loss: 1.40236473, g_loss: 0.69629568, ae_loss: 0.04729539\n",
      "Step: [3629] total_loss: 2.13125086 d_loss: 1.38039720, g_loss: 0.70312965, ae_loss: 0.04772396\n",
      "Step: [3630] total_loss: 2.12755728 d_loss: 1.37928557, g_loss: 0.69795179, ae_loss: 0.05031994\n",
      "Step: [3631] total_loss: 2.14336514 d_loss: 1.40403938, g_loss: 0.69124031, ae_loss: 0.04808550\n",
      "Step: [3632] total_loss: 2.13180327 d_loss: 1.39321470, g_loss: 0.68825901, ae_loss: 0.05032952\n",
      "Step: [3633] total_loss: 2.12181187 d_loss: 1.39256167, g_loss: 0.68023312, ae_loss: 0.04901724\n",
      "Step: [3634] total_loss: 2.10784125 d_loss: 1.37307072, g_loss: 0.68483245, ae_loss: 0.04993809\n",
      "Step: [3635] total_loss: 2.11418986 d_loss: 1.37775826, g_loss: 0.68368393, ae_loss: 0.05274775\n",
      "Step: [3636] total_loss: 2.11958194 d_loss: 1.39932013, g_loss: 0.67023486, ae_loss: 0.05002683\n",
      "Step: [3637] total_loss: 2.12956882 d_loss: 1.38914955, g_loss: 0.69221938, ae_loss: 0.04819988\n",
      "Step: [3638] total_loss: 2.12387156 d_loss: 1.39082813, g_loss: 0.68467253, ae_loss: 0.04837096\n",
      "Step: [3639] total_loss: 2.11847043 d_loss: 1.38323450, g_loss: 0.68441027, ae_loss: 0.05082569\n",
      "Step: [3640] total_loss: 2.12284851 d_loss: 1.38547993, g_loss: 0.68841934, ae_loss: 0.04894913\n",
      "Step: [3641] total_loss: 2.12602520 d_loss: 1.38634109, g_loss: 0.68981761, ae_loss: 0.04986659\n",
      "Step: [3642] total_loss: 2.13066292 d_loss: 1.37882054, g_loss: 0.70309842, ae_loss: 0.04874406\n",
      "Step: [3643] total_loss: 2.13867283 d_loss: 1.35776329, g_loss: 0.72532636, ae_loss: 0.05558319\n",
      "Step: [3644] total_loss: 2.13882661 d_loss: 1.38732004, g_loss: 0.70383829, ae_loss: 0.04766821\n",
      "Step: [3645] total_loss: 2.12705016 d_loss: 1.37549186, g_loss: 0.70200264, ae_loss: 0.04955571\n",
      "Step: [3646] total_loss: 2.12213850 d_loss: 1.37036335, g_loss: 0.70339870, ae_loss: 0.04837646\n",
      "Step: [3647] total_loss: 2.12737083 d_loss: 1.37907624, g_loss: 0.70075071, ae_loss: 0.04754385\n",
      "Step: [3648] total_loss: 2.13032818 d_loss: 1.38032365, g_loss: 0.69925094, ae_loss: 0.05075358\n",
      "Step: [3649] total_loss: 2.11452484 d_loss: 1.37827516, g_loss: 0.68361604, ae_loss: 0.05263349\n",
      "Step: [3650] total_loss: 2.12540865 d_loss: 1.38212085, g_loss: 0.69217038, ae_loss: 0.05111738\n",
      "Step: [3651] total_loss: 2.13534808 d_loss: 1.38831890, g_loss: 0.69699198, ae_loss: 0.05003725\n",
      "Step: [3652] total_loss: 2.14971972 d_loss: 1.40921950, g_loss: 0.69117206, ae_loss: 0.04932819\n",
      "Step: [3653] total_loss: 2.14388418 d_loss: 1.39437294, g_loss: 0.70211148, ae_loss: 0.04739988\n",
      "Step: [3654] total_loss: 2.13303471 d_loss: 1.39707887, g_loss: 0.68521571, ae_loss: 0.05074009\n",
      "Step: [3655] total_loss: 2.11383009 d_loss: 1.36993551, g_loss: 0.69639885, ae_loss: 0.04749583\n",
      "Step: [3656] total_loss: 2.13477707 d_loss: 1.38080478, g_loss: 0.70286691, ae_loss: 0.05110553\n",
      "Step: [3657] total_loss: 2.12525702 d_loss: 1.36041808, g_loss: 0.71363544, ae_loss: 0.05120344\n",
      "Step: [3658] total_loss: 2.12224221 d_loss: 1.38680911, g_loss: 0.68821901, ae_loss: 0.04721402\n",
      "Step: [3659] total_loss: 2.13167953 d_loss: 1.39332664, g_loss: 0.69012600, ae_loss: 0.04822692\n",
      "Step: [3660] total_loss: 2.11427975 d_loss: 1.36961794, g_loss: 0.69355226, ae_loss: 0.05110950\n",
      "Step: [3661] total_loss: 2.13674140 d_loss: 1.40533400, g_loss: 0.68187606, ae_loss: 0.04953137\n",
      "Step: [3662] total_loss: 2.12478828 d_loss: 1.38728738, g_loss: 0.68417621, ae_loss: 0.05332468\n",
      "Step: [3663] total_loss: 2.13756871 d_loss: 1.38314998, g_loss: 0.70280588, ae_loss: 0.05161282\n",
      "Step: [3664] total_loss: 2.13350892 d_loss: 1.37833166, g_loss: 0.70561117, ae_loss: 0.04956606\n",
      "Step: [3665] total_loss: 2.12298560 d_loss: 1.37582922, g_loss: 0.69849253, ae_loss: 0.04866384\n",
      "Step: [3666] total_loss: 2.13165045 d_loss: 1.38142991, g_loss: 0.70296931, ae_loss: 0.04725105\n",
      "Step: [3667] total_loss: 2.12753391 d_loss: 1.39187098, g_loss: 0.68569338, ae_loss: 0.04996940\n",
      "Step: [3668] total_loss: 2.14454222 d_loss: 1.37487268, g_loss: 0.72501856, ae_loss: 0.04465089\n",
      "Step: [3669] total_loss: 2.13370323 d_loss: 1.35449290, g_loss: 0.72961301, ae_loss: 0.04959723\n",
      "Step: [3670] total_loss: 2.10540295 d_loss: 1.37181115, g_loss: 0.68047333, ae_loss: 0.05311847\n",
      "Step: [3671] total_loss: 2.12056923 d_loss: 1.37993073, g_loss: 0.69450331, ae_loss: 0.04613528\n",
      "Step: [3672] total_loss: 2.12044787 d_loss: 1.37271953, g_loss: 0.69682682, ae_loss: 0.05090157\n",
      "Step: [3673] total_loss: 2.15559602 d_loss: 1.38898849, g_loss: 0.71774489, ae_loss: 0.04886257\n",
      "Step: [3674] total_loss: 2.13307357 d_loss: 1.38445985, g_loss: 0.69502950, ae_loss: 0.05358425\n",
      "Step: [3675] total_loss: 2.13852406 d_loss: 1.39048505, g_loss: 0.69797313, ae_loss: 0.05006602\n",
      "Step: [3676] total_loss: 2.14493513 d_loss: 1.40511799, g_loss: 0.68710661, ae_loss: 0.05271038\n",
      "Step: [3677] total_loss: 2.13046074 d_loss: 1.38860202, g_loss: 0.68769765, ae_loss: 0.05416090\n",
      "Step: [3678] total_loss: 2.11882639 d_loss: 1.37553287, g_loss: 0.69401067, ae_loss: 0.04928277\n",
      "Step: [3679] total_loss: 2.12520647 d_loss: 1.37140143, g_loss: 0.70324373, ae_loss: 0.05056126\n",
      "Step: [3680] total_loss: 2.12998247 d_loss: 1.39460373, g_loss: 0.68227434, ae_loss: 0.05310426\n",
      "Step: [3681] total_loss: 2.11265469 d_loss: 1.36542642, g_loss: 0.69643122, ae_loss: 0.05079711\n",
      "Step: [3682] total_loss: 2.12357044 d_loss: 1.38913238, g_loss: 0.68603742, ae_loss: 0.04840056\n",
      "Step: [3683] total_loss: 2.12350869 d_loss: 1.38949227, g_loss: 0.68606806, ae_loss: 0.04794834\n",
      "Step: [3684] total_loss: 2.13740754 d_loss: 1.37594271, g_loss: 0.70753282, ae_loss: 0.05393197\n",
      "Step: [3685] total_loss: 2.13042212 d_loss: 1.38455510, g_loss: 0.69223118, ae_loss: 0.05363581\n",
      "Step: [3686] total_loss: 2.13723636 d_loss: 1.38700008, g_loss: 0.70037156, ae_loss: 0.04986472\n",
      "Step: [3687] total_loss: 2.13384533 d_loss: 1.38690305, g_loss: 0.69496524, ae_loss: 0.05197706\n",
      "Step: [3688] total_loss: 2.13929319 d_loss: 1.40459180, g_loss: 0.68053734, ae_loss: 0.05416420\n",
      "Step: [3689] total_loss: 2.13141346 d_loss: 1.38885105, g_loss: 0.69280446, ae_loss: 0.04975798\n",
      "Step: [3690] total_loss: 2.14975047 d_loss: 1.40617979, g_loss: 0.68833131, ae_loss: 0.05523936\n",
      "Step: [3691] total_loss: 2.12143183 d_loss: 1.39120126, g_loss: 0.67817557, ae_loss: 0.05205486\n",
      "Step: [3692] total_loss: 2.13332486 d_loss: 1.37799692, g_loss: 0.70371193, ae_loss: 0.05161599\n",
      "Step: [3693] total_loss: 2.14309621 d_loss: 1.37951207, g_loss: 0.71531618, ae_loss: 0.04826797\n",
      "Step: [3694] total_loss: 2.14347363 d_loss: 1.39256406, g_loss: 0.70014131, ae_loss: 0.05076814\n",
      "Step: [3695] total_loss: 2.12903166 d_loss: 1.37228346, g_loss: 0.70646328, ae_loss: 0.05028488\n",
      "Step: [3696] total_loss: 2.13749027 d_loss: 1.39058721, g_loss: 0.69437200, ae_loss: 0.05253110\n",
      "Step: [3697] total_loss: 2.13713670 d_loss: 1.37921190, g_loss: 0.70554185, ae_loss: 0.05238291\n",
      "Step: [3698] total_loss: 2.13056636 d_loss: 1.39363694, g_loss: 0.68356061, ae_loss: 0.05336884\n",
      "Step: [3699] total_loss: 2.13188839 d_loss: 1.37931037, g_loss: 0.70299214, ae_loss: 0.04958598\n",
      "Step: [3700] total_loss: 2.12858748 d_loss: 1.38471317, g_loss: 0.69299924, ae_loss: 0.05087506\n",
      "Step: [3701] total_loss: 2.12758970 d_loss: 1.37332726, g_loss: 0.70395005, ae_loss: 0.05031238\n",
      "Step: [3702] total_loss: 2.11829305 d_loss: 1.38364172, g_loss: 0.68811166, ae_loss: 0.04653972\n",
      "Step: [3703] total_loss: 2.13612437 d_loss: 1.39925885, g_loss: 0.68637073, ae_loss: 0.05049479\n",
      "Step: [3704] total_loss: 2.11136198 d_loss: 1.36753130, g_loss: 0.69179714, ae_loss: 0.05203345\n",
      "Step: [3705] total_loss: 2.10768676 d_loss: 1.35941291, g_loss: 0.69683450, ae_loss: 0.05143945\n",
      "Step: [3706] total_loss: 2.12239075 d_loss: 1.37787056, g_loss: 0.69444931, ae_loss: 0.05007094\n",
      "Step: [3707] total_loss: 2.12433648 d_loss: 1.39685690, g_loss: 0.67634976, ae_loss: 0.05112986\n",
      "Step: [3708] total_loss: 2.12565422 d_loss: 1.39195609, g_loss: 0.68518329, ae_loss: 0.04851474\n",
      "Step: [3709] total_loss: 2.14400578 d_loss: 1.40311837, g_loss: 0.69101894, ae_loss: 0.04986844\n",
      "Step: [3710] total_loss: 2.12590766 d_loss: 1.38449824, g_loss: 0.68913114, ae_loss: 0.05227833\n",
      "Step: [3711] total_loss: 2.12478447 d_loss: 1.37839842, g_loss: 0.69404674, ae_loss: 0.05233938\n",
      "Step: [3712] total_loss: 2.13122702 d_loss: 1.38917160, g_loss: 0.69243991, ae_loss: 0.04961568\n",
      "Step: [3713] total_loss: 2.13417768 d_loss: 1.40069020, g_loss: 0.68291557, ae_loss: 0.05057201\n",
      "Step: [3714] total_loss: 2.11962700 d_loss: 1.37430072, g_loss: 0.69400215, ae_loss: 0.05132403\n",
      "Step: [3715] total_loss: 2.13528442 d_loss: 1.37844372, g_loss: 0.70734453, ae_loss: 0.04949604\n",
      "Step: [3716] total_loss: 2.11204505 d_loss: 1.36252367, g_loss: 0.69771701, ae_loss: 0.05180428\n",
      "Step: [3717] total_loss: 2.10739279 d_loss: 1.37347960, g_loss: 0.68384051, ae_loss: 0.05007270\n",
      "Step: [3718] total_loss: 2.11454582 d_loss: 1.37897277, g_loss: 0.68819201, ae_loss: 0.04738113\n",
      "Step: [3719] total_loss: 2.11865568 d_loss: 1.37569082, g_loss: 0.69354403, ae_loss: 0.04942098\n",
      "Step: [3720] total_loss: 2.13850021 d_loss: 1.37513542, g_loss: 0.71099854, ae_loss: 0.05236617\n",
      "Step: [3721] total_loss: 2.11723876 d_loss: 1.37003231, g_loss: 0.69728827, ae_loss: 0.04991816\n",
      "Step: [3722] total_loss: 2.14238262 d_loss: 1.40168011, g_loss: 0.69364047, ae_loss: 0.04706213\n",
      "Step: [3723] total_loss: 2.12215185 d_loss: 1.39736259, g_loss: 0.67323339, ae_loss: 0.05155595\n",
      "Step: [3724] total_loss: 2.13088346 d_loss: 1.39775014, g_loss: 0.68382639, ae_loss: 0.04930702\n",
      "Step: [3725] total_loss: 2.12778258 d_loss: 1.38280916, g_loss: 0.69450223, ae_loss: 0.05047116\n",
      "Step: [3726] total_loss: 2.11934733 d_loss: 1.37224126, g_loss: 0.69459218, ae_loss: 0.05251395\n",
      "Step: [3727] total_loss: 2.10133171 d_loss: 1.36609554, g_loss: 0.68703574, ae_loss: 0.04820044\n",
      "Step: [3728] total_loss: 2.11894202 d_loss: 1.37581038, g_loss: 0.68643272, ae_loss: 0.05669894\n",
      "Step: [3729] total_loss: 2.13343906 d_loss: 1.37753248, g_loss: 0.70392168, ae_loss: 0.05198478\n",
      "Step: [3730] total_loss: 2.15622807 d_loss: 1.42337799, g_loss: 0.68250823, ae_loss: 0.05034170\n",
      "Step: [3731] total_loss: 2.11939263 d_loss: 1.37033677, g_loss: 0.69825917, ae_loss: 0.05079675\n",
      "Step: [3732] total_loss: 2.13223505 d_loss: 1.38948536, g_loss: 0.69249970, ae_loss: 0.05024999\n",
      "Step: [3733] total_loss: 2.12776947 d_loss: 1.38686931, g_loss: 0.69184589, ae_loss: 0.04905418\n",
      "Step: [3734] total_loss: 2.12892580 d_loss: 1.38127899, g_loss: 0.70191550, ae_loss: 0.04573136\n",
      "Step: [3735] total_loss: 2.12086034 d_loss: 1.37007844, g_loss: 0.70099717, ae_loss: 0.04978466\n",
      "Step: [3736] total_loss: 2.11231327 d_loss: 1.36228395, g_loss: 0.70161557, ae_loss: 0.04841379\n",
      "Step: [3737] total_loss: 2.12918162 d_loss: 1.38506746, g_loss: 0.69592267, ae_loss: 0.04819150\n",
      "Step: [3738] total_loss: 2.13156271 d_loss: 1.38939679, g_loss: 0.68856066, ae_loss: 0.05360532\n",
      "Step: [3739] total_loss: 2.13913584 d_loss: 1.38590991, g_loss: 0.70231497, ae_loss: 0.05091093\n",
      "Step: [3740] total_loss: 2.13986778 d_loss: 1.39597368, g_loss: 0.69358468, ae_loss: 0.05030931\n",
      "Step: [3741] total_loss: 2.12562180 d_loss: 1.37796092, g_loss: 0.69803417, ae_loss: 0.04962663\n",
      "Step: [3742] total_loss: 2.12306786 d_loss: 1.38469577, g_loss: 0.68935740, ae_loss: 0.04901465\n",
      "Step: [3743] total_loss: 2.11908317 d_loss: 1.37874603, g_loss: 0.69096190, ae_loss: 0.04937521\n",
      "Step: [3744] total_loss: 2.10776854 d_loss: 1.36662829, g_loss: 0.68753117, ae_loss: 0.05360914\n",
      "Step: [3745] total_loss: 2.13008976 d_loss: 1.37226415, g_loss: 0.70893967, ae_loss: 0.04888606\n",
      "Step: [3746] total_loss: 2.12635946 d_loss: 1.39374936, g_loss: 0.68527496, ae_loss: 0.04733511\n",
      "Step: [3747] total_loss: 2.12597322 d_loss: 1.39243960, g_loss: 0.68072736, ae_loss: 0.05280638\n",
      "Step: [3748] total_loss: 2.12774730 d_loss: 1.38932395, g_loss: 0.68658108, ae_loss: 0.05184236\n",
      "Step: [3749] total_loss: 2.13824463 d_loss: 1.39475369, g_loss: 0.69047379, ae_loss: 0.05301709\n",
      "Step: [3750] total_loss: 2.13178158 d_loss: 1.38038504, g_loss: 0.69967163, ae_loss: 0.05172492\n",
      "Step: [3751] total_loss: 2.12984514 d_loss: 1.38140869, g_loss: 0.69766307, ae_loss: 0.05077322\n",
      "Step: [3752] total_loss: 2.11393094 d_loss: 1.37337780, g_loss: 0.69480926, ae_loss: 0.04574378\n",
      "Step: [3753] total_loss: 2.12742281 d_loss: 1.37856257, g_loss: 0.70113623, ae_loss: 0.04772417\n",
      "Step: [3754] total_loss: 2.12585306 d_loss: 1.38937318, g_loss: 0.68227965, ae_loss: 0.05420017\n",
      "Step: [3755] total_loss: 2.13987207 d_loss: 1.38408709, g_loss: 0.70482659, ae_loss: 0.05095833\n",
      "Step: [3756] total_loss: 2.12083602 d_loss: 1.37118220, g_loss: 0.69876742, ae_loss: 0.05088638\n",
      "Step: [3757] total_loss: 2.13420773 d_loss: 1.39872932, g_loss: 0.68713707, ae_loss: 0.04834138\n",
      "Step: [3758] total_loss: 2.14398003 d_loss: 1.40380657, g_loss: 0.69017160, ae_loss: 0.05000177\n",
      "Step: [3759] total_loss: 2.13050270 d_loss: 1.38861012, g_loss: 0.68645000, ae_loss: 0.05544242\n",
      "Step: [3760] total_loss: 2.13203049 d_loss: 1.38622165, g_loss: 0.69217682, ae_loss: 0.05363217\n",
      "Step: [3761] total_loss: 2.13528323 d_loss: 1.38289595, g_loss: 0.70172012, ae_loss: 0.05066719\n",
      "Step: [3762] total_loss: 2.13325644 d_loss: 1.38759804, g_loss: 0.69847322, ae_loss: 0.04718513\n",
      "Step: [3763] total_loss: 2.14153504 d_loss: 1.39349914, g_loss: 0.69949371, ae_loss: 0.04854210\n",
      "Step: [3764] total_loss: 2.13174963 d_loss: 1.38293338, g_loss: 0.69858092, ae_loss: 0.05023530\n",
      "Step: [3765] total_loss: 2.15159631 d_loss: 1.39584315, g_loss: 0.70241714, ae_loss: 0.05333602\n",
      "Step: [3766] total_loss: 2.12632227 d_loss: 1.38286233, g_loss: 0.69171971, ae_loss: 0.05174012\n",
      "Step: [3767] total_loss: 2.13275814 d_loss: 1.37399435, g_loss: 0.70913768, ae_loss: 0.04962628\n",
      "Step: [3768] total_loss: 2.13249564 d_loss: 1.39040446, g_loss: 0.69498521, ae_loss: 0.04710603\n",
      "Step: [3769] total_loss: 2.12187505 d_loss: 1.37346816, g_loss: 0.69561929, ae_loss: 0.05278752\n",
      "Step: [3770] total_loss: 2.13131285 d_loss: 1.37283492, g_loss: 0.70537412, ae_loss: 0.05310391\n",
      "Step: [3771] total_loss: 2.13628769 d_loss: 1.37733364, g_loss: 0.70729518, ae_loss: 0.05165879\n",
      "Step: [3772] total_loss: 2.12392545 d_loss: 1.37269747, g_loss: 0.69874209, ae_loss: 0.05248590\n",
      "Step: [3773] total_loss: 2.14207792 d_loss: 1.39909518, g_loss: 0.68940461, ae_loss: 0.05357800\n",
      "Step: [3774] total_loss: 2.11329365 d_loss: 1.36126089, g_loss: 0.70002490, ae_loss: 0.05200775\n",
      "Step: [3775] total_loss: 2.13589883 d_loss: 1.36575615, g_loss: 0.71512920, ae_loss: 0.05501344\n",
      "Step: [3776] total_loss: 2.11511970 d_loss: 1.37740719, g_loss: 0.69025892, ae_loss: 0.04745363\n",
      "Step: [3777] total_loss: 2.14385796 d_loss: 1.39873195, g_loss: 0.69492209, ae_loss: 0.05020380\n",
      "Step: [3778] total_loss: 2.13414145 d_loss: 1.37809658, g_loss: 0.70730734, ae_loss: 0.04873764\n",
      "Step: [3779] total_loss: 2.13058329 d_loss: 1.39140105, g_loss: 0.68628764, ae_loss: 0.05289460\n",
      "Step: [3780] total_loss: 2.11005950 d_loss: 1.37179732, g_loss: 0.68618935, ae_loss: 0.05207281\n",
      "Step: [3781] total_loss: 2.10841036 d_loss: 1.36462653, g_loss: 0.69018793, ae_loss: 0.05359574\n",
      "Step: [3782] total_loss: 2.13289690 d_loss: 1.38639355, g_loss: 0.69311213, ae_loss: 0.05339111\n",
      "Step: [3783] total_loss: 2.10401821 d_loss: 1.36234784, g_loss: 0.68949151, ae_loss: 0.05217897\n",
      "Step: [3784] total_loss: 2.10931301 d_loss: 1.38387847, g_loss: 0.67488158, ae_loss: 0.05055301\n",
      "Step: [3785] total_loss: 2.13533187 d_loss: 1.38560414, g_loss: 0.69797659, ae_loss: 0.05175117\n",
      "Step: [3786] total_loss: 2.15234470 d_loss: 1.38240981, g_loss: 0.71569431, ae_loss: 0.05424073\n",
      "Step: [3787] total_loss: 2.12468338 d_loss: 1.37670434, g_loss: 0.70222235, ae_loss: 0.04575654\n",
      "Step: [3788] total_loss: 2.12604213 d_loss: 1.36117435, g_loss: 0.71880466, ae_loss: 0.04606323\n",
      "Step: [3789] total_loss: 2.16171455 d_loss: 1.39663887, g_loss: 0.71059525, ae_loss: 0.05448037\n",
      "Step: [3790] total_loss: 2.12645721 d_loss: 1.39984584, g_loss: 0.67720616, ae_loss: 0.04940505\n",
      "Step: [3791] total_loss: 2.13868904 d_loss: 1.41004026, g_loss: 0.67412972, ae_loss: 0.05451893\n",
      "Step: [3792] total_loss: 2.13148403 d_loss: 1.39229786, g_loss: 0.68488151, ae_loss: 0.05430465\n",
      "Step: [3793] total_loss: 2.12084627 d_loss: 1.36304474, g_loss: 0.70820987, ae_loss: 0.04959152\n",
      "Step: [3794] total_loss: 2.11799502 d_loss: 1.36651695, g_loss: 0.70093179, ae_loss: 0.05054631\n",
      "Step: [3795] total_loss: 2.12334943 d_loss: 1.39439893, g_loss: 0.68197262, ae_loss: 0.04697785\n",
      "Step: [3796] total_loss: 2.12832761 d_loss: 1.37501466, g_loss: 0.69928610, ae_loss: 0.05402689\n",
      "Step: [3797] total_loss: 2.13329077 d_loss: 1.39638233, g_loss: 0.68756229, ae_loss: 0.04934616\n",
      "Step: [3798] total_loss: 2.12791705 d_loss: 1.37194061, g_loss: 0.69845819, ae_loss: 0.05751825\n",
      "Step: [3799] total_loss: 2.13277340 d_loss: 1.39134002, g_loss: 0.68917084, ae_loss: 0.05226256\n",
      "Step: [3800] total_loss: 2.11302304 d_loss: 1.37375677, g_loss: 0.68664283, ae_loss: 0.05262345\n",
      "Step: [3801] total_loss: 2.11897135 d_loss: 1.37428427, g_loss: 0.69358289, ae_loss: 0.05110428\n",
      "Step: [3802] total_loss: 2.11830044 d_loss: 1.37826610, g_loss: 0.68740320, ae_loss: 0.05263103\n",
      "Step: [3803] total_loss: 2.14728284 d_loss: 1.40470040, g_loss: 0.69041747, ae_loss: 0.05216487\n",
      "Step: [3804] total_loss: 2.13711643 d_loss: 1.39903092, g_loss: 0.68622637, ae_loss: 0.05185916\n",
      "Step: [3805] total_loss: 2.11872578 d_loss: 1.36794031, g_loss: 0.70000488, ae_loss: 0.05078068\n",
      "Step: [3806] total_loss: 2.10841203 d_loss: 1.37746739, g_loss: 0.68144679, ae_loss: 0.04949787\n",
      "Step: [3807] total_loss: 2.11471701 d_loss: 1.36770904, g_loss: 0.69275022, ae_loss: 0.05425772\n",
      "Step: [3808] total_loss: 2.12349510 d_loss: 1.38867617, g_loss: 0.68334621, ae_loss: 0.05147276\n",
      "Step: [3809] total_loss: 2.12820911 d_loss: 1.38799250, g_loss: 0.68649465, ae_loss: 0.05372208\n",
      "Step: [3810] total_loss: 2.10839367 d_loss: 1.36601198, g_loss: 0.69136679, ae_loss: 0.05101483\n",
      "Step: [3811] total_loss: 2.13842058 d_loss: 1.39034021, g_loss: 0.69739461, ae_loss: 0.05068574\n",
      "Step: [3812] total_loss: 2.13763809 d_loss: 1.38168585, g_loss: 0.70847243, ae_loss: 0.04747990\n",
      "Step: [3813] total_loss: 2.13418865 d_loss: 1.37734985, g_loss: 0.70755035, ae_loss: 0.04928837\n",
      "Step: [3814] total_loss: 2.13398910 d_loss: 1.39888406, g_loss: 0.68492776, ae_loss: 0.05017717\n",
      "Step: [3815] total_loss: 2.12482953 d_loss: 1.38181973, g_loss: 0.69523358, ae_loss: 0.04777624\n",
      "Step: [3816] total_loss: 2.14578819 d_loss: 1.40141380, g_loss: 0.69405991, ae_loss: 0.05031449\n",
      "Step: [3817] total_loss: 2.12709713 d_loss: 1.36978579, g_loss: 0.70701635, ae_loss: 0.05029485\n",
      "Step: [3818] total_loss: 2.12015438 d_loss: 1.37744021, g_loss: 0.69469011, ae_loss: 0.04802414\n",
      "Step: [3819] total_loss: 2.14464474 d_loss: 1.40639961, g_loss: 0.68490374, ae_loss: 0.05334141\n",
      "Step: [3820] total_loss: 2.12870002 d_loss: 1.39085412, g_loss: 0.69049197, ae_loss: 0.04735385\n",
      "Step: [3821] total_loss: 2.12328196 d_loss: 1.38211107, g_loss: 0.68959999, ae_loss: 0.05157089\n",
      "Step: [3822] total_loss: 2.11262608 d_loss: 1.38274932, g_loss: 0.67983866, ae_loss: 0.05003818\n",
      "Step: [3823] total_loss: 2.13689184 d_loss: 1.39033818, g_loss: 0.69029510, ae_loss: 0.05625863\n",
      "Step: [3824] total_loss: 2.14690208 d_loss: 1.39853621, g_loss: 0.69446611, ae_loss: 0.05389972\n",
      "Step: [3825] total_loss: 2.12996125 d_loss: 1.37288427, g_loss: 0.70745999, ae_loss: 0.04961708\n",
      "Step: [3826] total_loss: 2.11678028 d_loss: 1.36539674, g_loss: 0.70404398, ae_loss: 0.04733945\n",
      "Step: [3827] total_loss: 2.13824463 d_loss: 1.39626396, g_loss: 0.68851393, ae_loss: 0.05346680\n",
      "Step: [3828] total_loss: 2.12255955 d_loss: 1.37598825, g_loss: 0.69543856, ae_loss: 0.05113262\n",
      "Step: [3829] total_loss: 2.12262392 d_loss: 1.38215160, g_loss: 0.69223392, ae_loss: 0.04823840\n",
      "Step: [3830] total_loss: 2.11938906 d_loss: 1.39305353, g_loss: 0.67297924, ae_loss: 0.05335620\n",
      "Step: [3831] total_loss: 2.11760020 d_loss: 1.38758731, g_loss: 0.67779005, ae_loss: 0.05222290\n",
      "Step: [3832] total_loss: 2.13682985 d_loss: 1.39321959, g_loss: 0.69525611, ae_loss: 0.04835417\n",
      "Step: [3833] total_loss: 2.14755678 d_loss: 1.38541794, g_loss: 0.70821118, ae_loss: 0.05392761\n",
      "Step: [3834] total_loss: 2.12143183 d_loss: 1.37031126, g_loss: 0.70429456, ae_loss: 0.04682597\n",
      "Step: [3835] total_loss: 2.12086487 d_loss: 1.36599267, g_loss: 0.70315421, ae_loss: 0.05171800\n",
      "Step: [3836] total_loss: 2.13529634 d_loss: 1.38968873, g_loss: 0.69423318, ae_loss: 0.05137439\n",
      "Step: [3837] total_loss: 2.12707877 d_loss: 1.38554287, g_loss: 0.69025505, ae_loss: 0.05128086\n",
      "Step: [3838] total_loss: 2.12226343 d_loss: 1.38966846, g_loss: 0.68157434, ae_loss: 0.05102057\n",
      "Step: [3839] total_loss: 2.11882424 d_loss: 1.38810778, g_loss: 0.68041682, ae_loss: 0.05029966\n",
      "Step: [3840] total_loss: 2.13806581 d_loss: 1.39103019, g_loss: 0.69645405, ae_loss: 0.05058160\n",
      "Step: [3841] total_loss: 2.15430379 d_loss: 1.41798902, g_loss: 0.68818396, ae_loss: 0.04813085\n",
      "Step: [3842] total_loss: 2.12022686 d_loss: 1.37758505, g_loss: 0.69556719, ae_loss: 0.04707453\n",
      "Step: [3843] total_loss: 2.13536954 d_loss: 1.39300990, g_loss: 0.69208741, ae_loss: 0.05027218\n",
      "Step: [3844] total_loss: 2.13175774 d_loss: 1.39569569, g_loss: 0.68382668, ae_loss: 0.05223551\n",
      "Step: [3845] total_loss: 2.11872864 d_loss: 1.38976574, g_loss: 0.68202353, ae_loss: 0.04693932\n",
      "Step: [3846] total_loss: 2.12368917 d_loss: 1.38510847, g_loss: 0.68656564, ae_loss: 0.05201520\n",
      "Step: [3847] total_loss: 2.13214302 d_loss: 1.40036941, g_loss: 0.68332624, ae_loss: 0.04844742\n",
      "Step: [3848] total_loss: 2.12025905 d_loss: 1.38432646, g_loss: 0.68513489, ae_loss: 0.05079769\n",
      "Step: [3849] total_loss: 2.12090659 d_loss: 1.37560916, g_loss: 0.69119340, ae_loss: 0.05410392\n",
      "Step: [3850] total_loss: 2.12900162 d_loss: 1.40238285, g_loss: 0.67476881, ae_loss: 0.05184999\n",
      "Step: [3851] total_loss: 2.13087773 d_loss: 1.38107359, g_loss: 0.69781089, ae_loss: 0.05199326\n",
      "Step: [3852] total_loss: 2.12949085 d_loss: 1.38515770, g_loss: 0.69253349, ae_loss: 0.05179951\n",
      "Step: [3853] total_loss: 2.13238049 d_loss: 1.39493430, g_loss: 0.68724811, ae_loss: 0.05019822\n",
      "Step: [3854] total_loss: 2.12584424 d_loss: 1.37415922, g_loss: 0.70021015, ae_loss: 0.05147496\n",
      "Step: [3855] total_loss: 2.11542201 d_loss: 1.38122725, g_loss: 0.68750751, ae_loss: 0.04668726\n",
      "Step: [3856] total_loss: 2.11846471 d_loss: 1.37223268, g_loss: 0.69480956, ae_loss: 0.05142246\n",
      "Step: [3857] total_loss: 2.12409616 d_loss: 1.37664461, g_loss: 0.69583595, ae_loss: 0.05161556\n",
      "Step: [3858] total_loss: 2.12710619 d_loss: 1.38033128, g_loss: 0.69112074, ae_loss: 0.05565412\n",
      "Step: [3859] total_loss: 2.11384225 d_loss: 1.38657975, g_loss: 0.67796016, ae_loss: 0.04930240\n",
      "Step: [3860] total_loss: 2.13627267 d_loss: 1.39993477, g_loss: 0.68560475, ae_loss: 0.05073307\n",
      "Step: [3861] total_loss: 2.10059166 d_loss: 1.37144041, g_loss: 0.68034422, ae_loss: 0.04880719\n",
      "Step: [3862] total_loss: 2.11086845 d_loss: 1.37851906, g_loss: 0.67861354, ae_loss: 0.05373585\n",
      "Step: [3863] total_loss: 2.12952447 d_loss: 1.39550614, g_loss: 0.67989463, ae_loss: 0.05412375\n",
      "Step: [3864] total_loss: 2.12258101 d_loss: 1.38172317, g_loss: 0.68617344, ae_loss: 0.05468432\n",
      "Step: [3865] total_loss: 2.11220598 d_loss: 1.37281442, g_loss: 0.69260716, ae_loss: 0.04678441\n",
      "Step: [3866] total_loss: 2.12404156 d_loss: 1.37880921, g_loss: 0.69102883, ae_loss: 0.05420345\n",
      "Step: [3867] total_loss: 2.14061594 d_loss: 1.36699080, g_loss: 0.72090340, ae_loss: 0.05272181\n",
      "Step: [3868] total_loss: 2.11418819 d_loss: 1.36667633, g_loss: 0.69581574, ae_loss: 0.05169607\n",
      "Step: [3869] total_loss: 2.11433268 d_loss: 1.37158847, g_loss: 0.69274056, ae_loss: 0.05000350\n",
      "Step: [3870] total_loss: 2.12671804 d_loss: 1.38920212, g_loss: 0.68929917, ae_loss: 0.04821675\n",
      "Step: [3871] total_loss: 2.14401531 d_loss: 1.39886022, g_loss: 0.69552577, ae_loss: 0.04962918\n",
      "Step: [3872] total_loss: 2.14141273 d_loss: 1.38889694, g_loss: 0.69925916, ae_loss: 0.05325659\n",
      "Step: [3873] total_loss: 2.14509988 d_loss: 1.38382137, g_loss: 0.70981777, ae_loss: 0.05146075\n",
      "Step: [3874] total_loss: 2.11743832 d_loss: 1.37540936, g_loss: 0.69142658, ae_loss: 0.05060235\n",
      "Step: [3875] total_loss: 2.11615443 d_loss: 1.36628056, g_loss: 0.70025480, ae_loss: 0.04961913\n",
      "Step: [3876] total_loss: 2.14200044 d_loss: 1.41099286, g_loss: 0.68548089, ae_loss: 0.04552672\n",
      "Step: [3877] total_loss: 2.12265635 d_loss: 1.38987029, g_loss: 0.67980957, ae_loss: 0.05297651\n",
      "Step: [3878] total_loss: 2.10752392 d_loss: 1.36514235, g_loss: 0.68908644, ae_loss: 0.05329511\n",
      "Step: [3879] total_loss: 2.12708068 d_loss: 1.38464546, g_loss: 0.69142812, ae_loss: 0.05100716\n",
      "Step: [3880] total_loss: 2.10822487 d_loss: 1.37448311, g_loss: 0.68889713, ae_loss: 0.04484465\n",
      "Step: [3881] total_loss: 2.12805343 d_loss: 1.39238238, g_loss: 0.68487376, ae_loss: 0.05079735\n",
      "Step: [3882] total_loss: 2.10901403 d_loss: 1.36597395, g_loss: 0.69078732, ae_loss: 0.05225262\n",
      "Step: [3883] total_loss: 2.11550593 d_loss: 1.37822318, g_loss: 0.68780911, ae_loss: 0.04947368\n",
      "Step: [3884] total_loss: 2.13564301 d_loss: 1.39449835, g_loss: 0.69253790, ae_loss: 0.04860685\n",
      "Step: [3885] total_loss: 2.12473035 d_loss: 1.38628554, g_loss: 0.68705052, ae_loss: 0.05139439\n",
      "Step: [3886] total_loss: 2.11557341 d_loss: 1.38338757, g_loss: 0.68287110, ae_loss: 0.04931475\n",
      "Step: [3887] total_loss: 2.12149477 d_loss: 1.38346791, g_loss: 0.68226743, ae_loss: 0.05575939\n",
      "Step: [3888] total_loss: 2.08864617 d_loss: 1.35443068, g_loss: 0.68512321, ae_loss: 0.04909232\n",
      "Step: [3889] total_loss: 2.10561705 d_loss: 1.36879444, g_loss: 0.68368381, ae_loss: 0.05313884\n",
      "Step: [3890] total_loss: 2.13224936 d_loss: 1.38896561, g_loss: 0.68716371, ae_loss: 0.05611995\n",
      "Step: [3891] total_loss: 2.13434911 d_loss: 1.39203012, g_loss: 0.69022143, ae_loss: 0.05209756\n",
      "Step: [3892] total_loss: 2.14089251 d_loss: 1.39753342, g_loss: 0.68873644, ae_loss: 0.05462272\n",
      "Step: [3893] total_loss: 2.12986922 d_loss: 1.39232445, g_loss: 0.68909943, ae_loss: 0.04844532\n",
      "Step: [3894] total_loss: 2.14153075 d_loss: 1.35759997, g_loss: 0.73207152, ae_loss: 0.05185926\n",
      "Step: [3895] total_loss: 2.12016487 d_loss: 1.37083745, g_loss: 0.69933653, ae_loss: 0.04999073\n",
      "Step: [3896] total_loss: 2.12686324 d_loss: 1.37664127, g_loss: 0.69672334, ae_loss: 0.05349857\n",
      "Step: [3897] total_loss: 2.13709450 d_loss: 1.39510489, g_loss: 0.68993640, ae_loss: 0.05205324\n",
      "Step: [3898] total_loss: 2.13604951 d_loss: 1.38302875, g_loss: 0.70553613, ae_loss: 0.04748468\n",
      "Step: [3899] total_loss: 2.12300611 d_loss: 1.39232802, g_loss: 0.67857003, ae_loss: 0.05210800\n",
      "Step: [3900] total_loss: 2.12094784 d_loss: 1.39158690, g_loss: 0.68128824, ae_loss: 0.04807270\n",
      "Step: [3901] total_loss: 2.12580037 d_loss: 1.37999916, g_loss: 0.69508636, ae_loss: 0.05071485\n",
      "Step: [3902] total_loss: 2.12344646 d_loss: 1.37297916, g_loss: 0.70103848, ae_loss: 0.04942884\n",
      "Step: [3903] total_loss: 2.13419461 d_loss: 1.39372516, g_loss: 0.68663186, ae_loss: 0.05383749\n",
      "Step: [3904] total_loss: 2.12612796 d_loss: 1.38825417, g_loss: 0.68651748, ae_loss: 0.05135630\n",
      "Step: [3905] total_loss: 2.13780642 d_loss: 1.39796114, g_loss: 0.69174910, ae_loss: 0.04809630\n",
      "Step: [3906] total_loss: 2.10378408 d_loss: 1.35692430, g_loss: 0.69638538, ae_loss: 0.05047426\n",
      "Step: [3907] total_loss: 2.12443924 d_loss: 1.38491666, g_loss: 0.68472695, ae_loss: 0.05479554\n",
      "Step: [3908] total_loss: 2.11694908 d_loss: 1.38465977, g_loss: 0.67972291, ae_loss: 0.05256658\n",
      "Step: [3909] total_loss: 2.10880327 d_loss: 1.37224317, g_loss: 0.68239683, ae_loss: 0.05416335\n",
      "Step: [3910] total_loss: 2.13905096 d_loss: 1.40090418, g_loss: 0.68488157, ae_loss: 0.05326521\n",
      "Step: [3911] total_loss: 2.10068750 d_loss: 1.37498188, g_loss: 0.67674398, ae_loss: 0.04896163\n",
      "Step: [3912] total_loss: 2.11475611 d_loss: 1.36243808, g_loss: 0.70008981, ae_loss: 0.05222808\n",
      "Step: [3913] total_loss: 2.10878038 d_loss: 1.38049722, g_loss: 0.67977023, ae_loss: 0.04851295\n",
      "Step: [3914] total_loss: 2.15324759 d_loss: 1.37723243, g_loss: 0.72219962, ae_loss: 0.05381565\n",
      "Step: [3915] total_loss: 2.12633634 d_loss: 1.37036991, g_loss: 0.69915378, ae_loss: 0.05681263\n",
      "Step: [3916] total_loss: 2.13999891 d_loss: 1.38349152, g_loss: 0.70369875, ae_loss: 0.05280871\n",
      "Step: [3917] total_loss: 2.12683582 d_loss: 1.38884807, g_loss: 0.68669832, ae_loss: 0.05128943\n",
      "Step: [3918] total_loss: 2.12271643 d_loss: 1.37788153, g_loss: 0.69245315, ae_loss: 0.05238158\n",
      "Step: [3919] total_loss: 2.12245274 d_loss: 1.37775707, g_loss: 0.69488657, ae_loss: 0.04980919\n",
      "Step: [3920] total_loss: 2.13903761 d_loss: 1.40128255, g_loss: 0.68501842, ae_loss: 0.05273651\n",
      "Step: [3921] total_loss: 2.13056993 d_loss: 1.37692165, g_loss: 0.69746304, ae_loss: 0.05618515\n",
      "Step: [3922] total_loss: 2.13205862 d_loss: 1.40070236, g_loss: 0.67995954, ae_loss: 0.05139673\n",
      "Step: [3923] total_loss: 2.13636661 d_loss: 1.39403856, g_loss: 0.68836015, ae_loss: 0.05396793\n",
      "Step: [3924] total_loss: 2.14461517 d_loss: 1.39111352, g_loss: 0.70410466, ae_loss: 0.04939693\n",
      "Step: [3925] total_loss: 2.12822700 d_loss: 1.38277006, g_loss: 0.69485295, ae_loss: 0.05060394\n",
      "Step: [3926] total_loss: 2.13869858 d_loss: 1.39568388, g_loss: 0.69087994, ae_loss: 0.05213485\n",
      "Step: [3927] total_loss: 2.12230206 d_loss: 1.38742089, g_loss: 0.68158948, ae_loss: 0.05329156\n",
      "Step: [3928] total_loss: 2.11379194 d_loss: 1.38167906, g_loss: 0.68118846, ae_loss: 0.05092458\n",
      "Step: [3929] total_loss: 2.12679529 d_loss: 1.35944283, g_loss: 0.71266502, ae_loss: 0.05468737\n",
      "Step: [3930] total_loss: 2.13764811 d_loss: 1.40942419, g_loss: 0.68072999, ae_loss: 0.04749406\n",
      "Step: [3931] total_loss: 2.12202668 d_loss: 1.37861776, g_loss: 0.69384313, ae_loss: 0.04956577\n",
      "Step: [3932] total_loss: 2.12829590 d_loss: 1.37120390, g_loss: 0.70433629, ae_loss: 0.05275583\n",
      "Step: [3933] total_loss: 2.12587261 d_loss: 1.37816703, g_loss: 0.69162154, ae_loss: 0.05608419\n",
      "Step: [3934] total_loss: 2.12026978 d_loss: 1.37362099, g_loss: 0.69557297, ae_loss: 0.05107597\n",
      "Step: [3935] total_loss: 2.11560965 d_loss: 1.37445021, g_loss: 0.69145501, ae_loss: 0.04970429\n",
      "Step: [3936] total_loss: 2.11538601 d_loss: 1.38102114, g_loss: 0.68878913, ae_loss: 0.04557562\n",
      "Step: [3937] total_loss: 2.11196303 d_loss: 1.36666608, g_loss: 0.69326550, ae_loss: 0.05203152\n",
      "Step: [3938] total_loss: 2.11401987 d_loss: 1.37505662, g_loss: 0.68992299, ae_loss: 0.04904034\n",
      "Step: [3939] total_loss: 2.11006594 d_loss: 1.37428308, g_loss: 0.68136001, ae_loss: 0.05442289\n",
      "Step: [3940] total_loss: 2.10629845 d_loss: 1.36961865, g_loss: 0.69138151, ae_loss: 0.04529820\n",
      "Step: [3941] total_loss: 2.12738252 d_loss: 1.38231909, g_loss: 0.69310981, ae_loss: 0.05195362\n",
      "Step: [3942] total_loss: 2.11266804 d_loss: 1.38213146, g_loss: 0.68231624, ae_loss: 0.04822034\n",
      "Step: [3943] total_loss: 2.14642119 d_loss: 1.40275311, g_loss: 0.69409895, ae_loss: 0.04956913\n",
      "Step: [3944] total_loss: 2.13278222 d_loss: 1.37832618, g_loss: 0.70560008, ae_loss: 0.04885589\n",
      "Step: [3945] total_loss: 2.14681721 d_loss: 1.39226222, g_loss: 0.70083851, ae_loss: 0.05371648\n",
      "Step: [3946] total_loss: 2.12789488 d_loss: 1.38424730, g_loss: 0.68843746, ae_loss: 0.05521008\n",
      "Step: [3947] total_loss: 2.13053131 d_loss: 1.39920652, g_loss: 0.67856574, ae_loss: 0.05275903\n",
      "Step: [3948] total_loss: 2.11770868 d_loss: 1.39491642, g_loss: 0.67131305, ae_loss: 0.05147937\n",
      "Step: [3949] total_loss: 2.11622715 d_loss: 1.38355827, g_loss: 0.68001318, ae_loss: 0.05265580\n",
      "Step: [3950] total_loss: 2.12892675 d_loss: 1.39790165, g_loss: 0.67324650, ae_loss: 0.05777849\n",
      "Step: [3951] total_loss: 2.13433695 d_loss: 1.39915299, g_loss: 0.68162507, ae_loss: 0.05355892\n",
      "Step: [3952] total_loss: 2.11741734 d_loss: 1.38898659, g_loss: 0.67832839, ae_loss: 0.05010241\n",
      "Step: [3953] total_loss: 2.13020253 d_loss: 1.39675462, g_loss: 0.68204737, ae_loss: 0.05140049\n",
      "Step: [3954] total_loss: 2.11715579 d_loss: 1.37486303, g_loss: 0.68833041, ae_loss: 0.05396233\n",
      "Step: [3955] total_loss: 2.13989401 d_loss: 1.38571286, g_loss: 0.70088613, ae_loss: 0.05329500\n",
      "Step: [3956] total_loss: 2.13067865 d_loss: 1.39563537, g_loss: 0.68501770, ae_loss: 0.05002552\n",
      "Step: [3957] total_loss: 2.12958860 d_loss: 1.37807345, g_loss: 0.69964594, ae_loss: 0.05186925\n",
      "Step: [3958] total_loss: 2.12627363 d_loss: 1.37706172, g_loss: 0.69898951, ae_loss: 0.05022256\n",
      "Step: [3959] total_loss: 2.13613415 d_loss: 1.39477181, g_loss: 0.69310153, ae_loss: 0.04826091\n",
      "Step: [3960] total_loss: 2.12997031 d_loss: 1.37633932, g_loss: 0.70335793, ae_loss: 0.05027305\n",
      "Step: [3961] total_loss: 2.11887646 d_loss: 1.38128185, g_loss: 0.68721455, ae_loss: 0.05038014\n",
      "Step: [3962] total_loss: 2.12051249 d_loss: 1.39157677, g_loss: 0.67759043, ae_loss: 0.05134520\n",
      "Step: [3963] total_loss: 2.10895252 d_loss: 1.36268651, g_loss: 0.69901073, ae_loss: 0.04725510\n",
      "Step: [3964] total_loss: 2.12549758 d_loss: 1.37943566, g_loss: 0.69509327, ae_loss: 0.05096868\n",
      "Step: [3965] total_loss: 2.14341831 d_loss: 1.41124082, g_loss: 0.68148410, ae_loss: 0.05069348\n",
      "Step: [3966] total_loss: 2.14222908 d_loss: 1.38471138, g_loss: 0.70587099, ae_loss: 0.05164658\n",
      "Step: [3967] total_loss: 2.13669086 d_loss: 1.38695121, g_loss: 0.69726729, ae_loss: 0.05247240\n",
      "Step: [3968] total_loss: 2.13308191 d_loss: 1.38184869, g_loss: 0.69997203, ae_loss: 0.05126123\n",
      "Step: [3969] total_loss: 2.13150096 d_loss: 1.37862992, g_loss: 0.70331192, ae_loss: 0.04955914\n",
      "Step: [3970] total_loss: 2.11396694 d_loss: 1.35650063, g_loss: 0.70370209, ae_loss: 0.05376412\n",
      "Step: [3971] total_loss: 2.13520432 d_loss: 1.37515450, g_loss: 0.70772684, ae_loss: 0.05232291\n",
      "Step: [3972] total_loss: 2.12559319 d_loss: 1.37674963, g_loss: 0.69695175, ae_loss: 0.05189176\n",
      "Step: [3973] total_loss: 2.11008978 d_loss: 1.37152278, g_loss: 0.68704581, ae_loss: 0.05152105\n",
      "Step: [3974] total_loss: 2.12377596 d_loss: 1.38035703, g_loss: 0.69550997, ae_loss: 0.04790891\n",
      "Step: [3975] total_loss: 2.13562965 d_loss: 1.39522123, g_loss: 0.68769646, ae_loss: 0.05271211\n",
      "Step: [3976] total_loss: 2.11305928 d_loss: 1.36102676, g_loss: 0.70527995, ae_loss: 0.04675258\n",
      "Step: [3977] total_loss: 2.13297701 d_loss: 1.38706303, g_loss: 0.69609869, ae_loss: 0.04981538\n",
      "Step: [3978] total_loss: 2.12815762 d_loss: 1.38335121, g_loss: 0.69444287, ae_loss: 0.05036370\n",
      "Step: [3979] total_loss: 2.11784267 d_loss: 1.38198972, g_loss: 0.68406767, ae_loss: 0.05178537\n",
      "Step: [3980] total_loss: 2.13430262 d_loss: 1.39572811, g_loss: 0.68859100, ae_loss: 0.04998344\n",
      "Step: [3981] total_loss: 2.13371825 d_loss: 1.38146305, g_loss: 0.70247167, ae_loss: 0.04978351\n",
      "Step: [3982] total_loss: 2.13472176 d_loss: 1.37454939, g_loss: 0.70887005, ae_loss: 0.05130228\n",
      "Step: [3983] total_loss: 2.13444519 d_loss: 1.38258100, g_loss: 0.70683229, ae_loss: 0.04503183\n",
      "Step: [3984] total_loss: 2.09345531 d_loss: 1.36420226, g_loss: 0.68017524, ae_loss: 0.04907783\n",
      "Step: [3985] total_loss: 2.13589621 d_loss: 1.38864148, g_loss: 0.69017279, ae_loss: 0.05708190\n",
      "Step: [3986] total_loss: 2.14290094 d_loss: 1.39885211, g_loss: 0.69147730, ae_loss: 0.05257159\n",
      "Step: [3987] total_loss: 2.13317299 d_loss: 1.37045968, g_loss: 0.71150470, ae_loss: 0.05120847\n",
      "Step: [3988] total_loss: 2.09813404 d_loss: 1.37426555, g_loss: 0.67705882, ae_loss: 0.04680964\n",
      "Step: [3989] total_loss: 2.11920309 d_loss: 1.38907993, g_loss: 0.67621893, ae_loss: 0.05390422\n",
      "Step: [3990] total_loss: 2.13835192 d_loss: 1.38772392, g_loss: 0.70002127, ae_loss: 0.05060688\n",
      "Step: [3991] total_loss: 2.12295747 d_loss: 1.39020920, g_loss: 0.67975622, ae_loss: 0.05299196\n",
      "Step: [3992] total_loss: 2.11058831 d_loss: 1.36605132, g_loss: 0.69228739, ae_loss: 0.05224962\n",
      "Step: [3993] total_loss: 2.13991952 d_loss: 1.39404178, g_loss: 0.69283855, ae_loss: 0.05303915\n",
      "Step: [3994] total_loss: 2.12352228 d_loss: 1.37711394, g_loss: 0.69180787, ae_loss: 0.05460052\n",
      "Step: [3995] total_loss: 2.12612152 d_loss: 1.38084316, g_loss: 0.69319952, ae_loss: 0.05207878\n",
      "Step: [3996] total_loss: 2.12851071 d_loss: 1.37987578, g_loss: 0.69888341, ae_loss: 0.04975152\n",
      "Step: [3997] total_loss: 2.14174414 d_loss: 1.38360262, g_loss: 0.70505559, ae_loss: 0.05308580\n",
      "Step: [3998] total_loss: 2.12046432 d_loss: 1.37369847, g_loss: 0.69416201, ae_loss: 0.05260390\n",
      "Step: [3999] total_loss: 2.12186766 d_loss: 1.39759374, g_loss: 0.67019528, ae_loss: 0.05407869\n",
      "Step: [4000] total_loss: 2.14030552 d_loss: 1.36639190, g_loss: 0.71928680, ae_loss: 0.05462699\n",
      "Step: [4001] total_loss: 2.11393738 d_loss: 1.38441515, g_loss: 0.67713428, ae_loss: 0.05238794\n",
      "Step: [4002] total_loss: 2.14830065 d_loss: 1.39763331, g_loss: 0.69857025, ae_loss: 0.05209704\n",
      "Step: [4003] total_loss: 2.12760019 d_loss: 1.38594973, g_loss: 0.68696749, ae_loss: 0.05468302\n",
      "Step: [4004] total_loss: 2.13863945 d_loss: 1.39036703, g_loss: 0.69490886, ae_loss: 0.05336368\n",
      "Step: [4005] total_loss: 2.12880659 d_loss: 1.38806295, g_loss: 0.68734407, ae_loss: 0.05339950\n",
      "Step: [4006] total_loss: 2.11566067 d_loss: 1.37984645, g_loss: 0.68333435, ae_loss: 0.05247988\n",
      "Step: [4007] total_loss: 2.13367391 d_loss: 1.38733029, g_loss: 0.69316435, ae_loss: 0.05317922\n",
      "Step: [4008] total_loss: 2.12166166 d_loss: 1.38137674, g_loss: 0.69394892, ae_loss: 0.04633611\n",
      "Step: [4009] total_loss: 2.12746072 d_loss: 1.37291646, g_loss: 0.70083976, ae_loss: 0.05370447\n",
      "Step: [4010] total_loss: 2.12122965 d_loss: 1.39008319, g_loss: 0.68303925, ae_loss: 0.04810714\n",
      "Step: [4011] total_loss: 2.11840630 d_loss: 1.38180900, g_loss: 0.68790781, ae_loss: 0.04868940\n",
      "Step: [4012] total_loss: 2.11484456 d_loss: 1.35220432, g_loss: 0.70881176, ae_loss: 0.05382844\n",
      "Step: [4013] total_loss: 2.13462496 d_loss: 1.38744998, g_loss: 0.69646966, ae_loss: 0.05070516\n",
      "Step: [4014] total_loss: 2.11894536 d_loss: 1.37073565, g_loss: 0.69467616, ae_loss: 0.05353357\n",
      "Step: [4015] total_loss: 2.12260437 d_loss: 1.37928200, g_loss: 0.68764645, ae_loss: 0.05567583\n",
      "Step: [4016] total_loss: 2.10562539 d_loss: 1.37376487, g_loss: 0.67954409, ae_loss: 0.05231643\n",
      "Step: [4017] total_loss: 2.11947036 d_loss: 1.39236307, g_loss: 0.67523026, ae_loss: 0.05187704\n",
      "Step: [4018] total_loss: 2.09746838 d_loss: 1.36794496, g_loss: 0.68148541, ae_loss: 0.04803808\n",
      "Step: [4019] total_loss: 2.13091469 d_loss: 1.39326382, g_loss: 0.68682647, ae_loss: 0.05082445\n",
      "Step: [4020] total_loss: 2.11996651 d_loss: 1.39324880, g_loss: 0.67629105, ae_loss: 0.05042669\n",
      "Step: [4021] total_loss: 2.12996292 d_loss: 1.38747811, g_loss: 0.68825710, ae_loss: 0.05422763\n",
      "Step: [4022] total_loss: 2.12392545 d_loss: 1.37968397, g_loss: 0.69378018, ae_loss: 0.05046129\n",
      "Step: [4023] total_loss: 2.13429260 d_loss: 1.39258885, g_loss: 0.69039977, ae_loss: 0.05130395\n",
      "Step: [4024] total_loss: 2.12973237 d_loss: 1.38547504, g_loss: 0.69733834, ae_loss: 0.04691895\n",
      "Step: [4025] total_loss: 2.13860464 d_loss: 1.38852787, g_loss: 0.69955695, ae_loss: 0.05051972\n",
      "Step: [4026] total_loss: 2.12228775 d_loss: 1.38180220, g_loss: 0.68583697, ae_loss: 0.05464853\n",
      "Step: [4027] total_loss: 2.12126493 d_loss: 1.37484527, g_loss: 0.69314921, ae_loss: 0.05327058\n",
      "Step: [4028] total_loss: 2.14604473 d_loss: 1.38378811, g_loss: 0.70689154, ae_loss: 0.05536501\n",
      "Step: [4029] total_loss: 2.13203573 d_loss: 1.39034152, g_loss: 0.68520641, ae_loss: 0.05648777\n",
      "Step: [4030] total_loss: 2.13675833 d_loss: 1.39704967, g_loss: 0.69444758, ae_loss: 0.04526102\n",
      "Step: [4031] total_loss: 2.13396549 d_loss: 1.38604319, g_loss: 0.69765365, ae_loss: 0.05026854\n",
      "Step: [4032] total_loss: 2.12486815 d_loss: 1.37949741, g_loss: 0.69494027, ae_loss: 0.05043054\n",
      "Step: [4033] total_loss: 2.11041903 d_loss: 1.37654328, g_loss: 0.68325591, ae_loss: 0.05061985\n",
      "Step: [4034] total_loss: 2.10647368 d_loss: 1.36896765, g_loss: 0.68655562, ae_loss: 0.05095041\n",
      "Step: [4035] total_loss: 2.13152313 d_loss: 1.38722146, g_loss: 0.69172668, ae_loss: 0.05257503\n",
      "Step: [4036] total_loss: 2.11471701 d_loss: 1.37305558, g_loss: 0.69188631, ae_loss: 0.04977509\n",
      "Step: [4037] total_loss: 2.13268995 d_loss: 1.39194453, g_loss: 0.68859625, ae_loss: 0.05214919\n",
      "Step: [4038] total_loss: 2.11755753 d_loss: 1.37459648, g_loss: 0.69242859, ae_loss: 0.05053239\n",
      "Step: [4039] total_loss: 2.09784460 d_loss: 1.36049771, g_loss: 0.68504131, ae_loss: 0.05230576\n",
      "Step: [4040] total_loss: 2.12298965 d_loss: 1.37736607, g_loss: 0.69001842, ae_loss: 0.05560508\n",
      "Step: [4041] total_loss: 2.11490631 d_loss: 1.36275721, g_loss: 0.69890845, ae_loss: 0.05324049\n",
      "Step: [4042] total_loss: 2.14491796 d_loss: 1.37404692, g_loss: 0.72073734, ae_loss: 0.05013383\n",
      "Step: [4043] total_loss: 2.13615084 d_loss: 1.39252508, g_loss: 0.69130594, ae_loss: 0.05231980\n",
      "Step: [4044] total_loss: 2.12190247 d_loss: 1.36443245, g_loss: 0.70924968, ae_loss: 0.04822022\n",
      "Step: [4045] total_loss: 2.13076830 d_loss: 1.38189042, g_loss: 0.69939059, ae_loss: 0.04948732\n",
      "Step: [4046] total_loss: 2.12054563 d_loss: 1.38297153, g_loss: 0.68965340, ae_loss: 0.04792072\n",
      "Step: [4047] total_loss: 2.12328053 d_loss: 1.36763966, g_loss: 0.70589674, ae_loss: 0.04974403\n",
      "Step: [4048] total_loss: 2.14239931 d_loss: 1.37113070, g_loss: 0.71891129, ae_loss: 0.05235732\n",
      "Step: [4049] total_loss: 2.12407279 d_loss: 1.38311768, g_loss: 0.69030952, ae_loss: 0.05064559\n",
      "Step: [4050] total_loss: 2.14219475 d_loss: 1.39327455, g_loss: 0.69997990, ae_loss: 0.04894043\n",
      "Step: [4051] total_loss: 2.12807846 d_loss: 1.36398852, g_loss: 0.71431363, ae_loss: 0.04977636\n",
      "Step: [4052] total_loss: 2.13215399 d_loss: 1.38536894, g_loss: 0.70094442, ae_loss: 0.04584047\n",
      "Step: [4053] total_loss: 2.12241697 d_loss: 1.37735641, g_loss: 0.69670051, ae_loss: 0.04835996\n",
      "Step: [4054] total_loss: 2.11926937 d_loss: 1.37794518, g_loss: 0.68881631, ae_loss: 0.05250771\n",
      "Step: [4055] total_loss: 2.12837124 d_loss: 1.39588737, g_loss: 0.68157017, ae_loss: 0.05091384\n",
      "Step: [4056] total_loss: 2.13767076 d_loss: 1.37467945, g_loss: 0.71249682, ae_loss: 0.05049449\n",
      "Step: [4057] total_loss: 2.13679433 d_loss: 1.39850354, g_loss: 0.68904960, ae_loss: 0.04924121\n",
      "Step: [4058] total_loss: 2.12629795 d_loss: 1.38116288, g_loss: 0.69671810, ae_loss: 0.04841710\n",
      "Step: [4059] total_loss: 2.12893534 d_loss: 1.37649548, g_loss: 0.69969690, ae_loss: 0.05274296\n",
      "Step: [4060] total_loss: 2.12128878 d_loss: 1.38698936, g_loss: 0.68213427, ae_loss: 0.05216530\n",
      "Step: [4061] total_loss: 2.14742589 d_loss: 1.39625216, g_loss: 0.70299101, ae_loss: 0.04818273\n",
      "Step: [4062] total_loss: 2.11966348 d_loss: 1.37692857, g_loss: 0.69224948, ae_loss: 0.05048550\n",
      "Step: [4063] total_loss: 2.12353253 d_loss: 1.37310100, g_loss: 0.69708800, ae_loss: 0.05334354\n",
      "Step: [4064] total_loss: 2.12025881 d_loss: 1.38454962, g_loss: 0.68401933, ae_loss: 0.05168995\n",
      "Step: [4065] total_loss: 2.12426233 d_loss: 1.39379120, g_loss: 0.68002462, ae_loss: 0.05044653\n",
      "Step: [4066] total_loss: 2.12380648 d_loss: 1.37363625, g_loss: 0.69662076, ae_loss: 0.05354958\n",
      "Step: [4067] total_loss: 2.12556314 d_loss: 1.39048827, g_loss: 0.68171823, ae_loss: 0.05335663\n",
      "Step: [4068] total_loss: 2.11555243 d_loss: 1.38008451, g_loss: 0.68344879, ae_loss: 0.05201918\n",
      "Step: [4069] total_loss: 2.14587021 d_loss: 1.40244055, g_loss: 0.69284588, ae_loss: 0.05058372\n",
      "Step: [4070] total_loss: 2.11592269 d_loss: 1.37766600, g_loss: 0.68816531, ae_loss: 0.05009143\n",
      "Step: [4071] total_loss: 2.13947868 d_loss: 1.39955330, g_loss: 0.68989116, ae_loss: 0.05003425\n",
      "Step: [4072] total_loss: 2.15106130 d_loss: 1.40391350, g_loss: 0.69452870, ae_loss: 0.05261913\n",
      "Step: [4073] total_loss: 2.11661911 d_loss: 1.38154590, g_loss: 0.68708062, ae_loss: 0.04799274\n",
      "Step: [4074] total_loss: 2.12214470 d_loss: 1.37247539, g_loss: 0.69360995, ae_loss: 0.05605927\n",
      "Step: [4075] total_loss: 2.13410378 d_loss: 1.38472807, g_loss: 0.69401014, ae_loss: 0.05536550\n",
      "Step: [4076] total_loss: 2.13721561 d_loss: 1.40128469, g_loss: 0.68528277, ae_loss: 0.05064806\n",
      "Step: [4077] total_loss: 2.11855984 d_loss: 1.37671113, g_loss: 0.69138896, ae_loss: 0.05045985\n",
      "Step: [4078] total_loss: 2.12930632 d_loss: 1.38871050, g_loss: 0.69556272, ae_loss: 0.04503318\n",
      "Step: [4079] total_loss: 2.11923265 d_loss: 1.38427734, g_loss: 0.68188971, ae_loss: 0.05306549\n",
      "Step: [4080] total_loss: 2.14105058 d_loss: 1.37369990, g_loss: 0.71474433, ae_loss: 0.05260633\n",
      "Step: [4081] total_loss: 2.11261821 d_loss: 1.38642764, g_loss: 0.67420393, ae_loss: 0.05198652\n",
      "Step: [4082] total_loss: 2.11836290 d_loss: 1.38686693, g_loss: 0.67671609, ae_loss: 0.05478002\n",
      "Step: [4083] total_loss: 2.11825204 d_loss: 1.38658738, g_loss: 0.68005979, ae_loss: 0.05160491\n",
      "Step: [4084] total_loss: 2.11129165 d_loss: 1.37683487, g_loss: 0.68391943, ae_loss: 0.05053733\n",
      "Step: [4085] total_loss: 2.10821390 d_loss: 1.36947262, g_loss: 0.68270952, ae_loss: 0.05603174\n",
      "Step: [4086] total_loss: 2.10633397 d_loss: 1.36616600, g_loss: 0.68956095, ae_loss: 0.05060698\n",
      "Step: [4087] total_loss: 2.11848736 d_loss: 1.38849497, g_loss: 0.67928416, ae_loss: 0.05070815\n",
      "Step: [4088] total_loss: 2.12815762 d_loss: 1.40249872, g_loss: 0.67664826, ae_loss: 0.04901049\n",
      "Step: [4089] total_loss: 2.12463665 d_loss: 1.38355398, g_loss: 0.68635750, ae_loss: 0.05472530\n",
      "Step: [4090] total_loss: 2.11368775 d_loss: 1.39041436, g_loss: 0.67660171, ae_loss: 0.04667178\n",
      "Step: [4091] total_loss: 2.12025070 d_loss: 1.38775015, g_loss: 0.68062603, ae_loss: 0.05187456\n",
      "Step: [4092] total_loss: 2.11670804 d_loss: 1.38433075, g_loss: 0.67896903, ae_loss: 0.05340827\n",
      "Step: [4093] total_loss: 2.12221932 d_loss: 1.38565564, g_loss: 0.68601692, ae_loss: 0.05054680\n",
      "Step: [4094] total_loss: 2.11199570 d_loss: 1.38165486, g_loss: 0.67927539, ae_loss: 0.05106539\n",
      "Step: [4095] total_loss: 2.11829066 d_loss: 1.39493990, g_loss: 0.67388856, ae_loss: 0.04946224\n",
      "Step: [4096] total_loss: 2.11816192 d_loss: 1.37810183, g_loss: 0.68708372, ae_loss: 0.05297637\n",
      "Step: [4097] total_loss: 2.12251449 d_loss: 1.39266789, g_loss: 0.68077445, ae_loss: 0.04907209\n",
      "Step: [4098] total_loss: 2.13059974 d_loss: 1.39115953, g_loss: 0.68789017, ae_loss: 0.05155006\n",
      "Step: [4099] total_loss: 2.13139606 d_loss: 1.38923681, g_loss: 0.68973839, ae_loss: 0.05242084\n",
      "Step: [4100] total_loss: 2.14363313 d_loss: 1.39675760, g_loss: 0.69584405, ae_loss: 0.05103141\n",
      "Step: [4101] total_loss: 2.12623739 d_loss: 1.39109349, g_loss: 0.68777728, ae_loss: 0.04736666\n",
      "Step: [4102] total_loss: 2.13554955 d_loss: 1.39561033, g_loss: 0.68782294, ae_loss: 0.05211619\n",
      "Step: [4103] total_loss: 2.14099145 d_loss: 1.39869642, g_loss: 0.69069356, ae_loss: 0.05160149\n",
      "Step: [4104] total_loss: 2.12824249 d_loss: 1.38846111, g_loss: 0.68935263, ae_loss: 0.05042860\n",
      "Step: [4105] total_loss: 2.13089657 d_loss: 1.39030707, g_loss: 0.68825459, ae_loss: 0.05233477\n",
      "Step: [4106] total_loss: 2.11855364 d_loss: 1.36899197, g_loss: 0.69425160, ae_loss: 0.05531009\n",
      "Step: [4107] total_loss: 2.13071537 d_loss: 1.39899111, g_loss: 0.68280458, ae_loss: 0.04891971\n",
      "Step: [4108] total_loss: 2.12940025 d_loss: 1.40014338, g_loss: 0.68225336, ae_loss: 0.04700368\n",
      "Step: [4109] total_loss: 2.12024665 d_loss: 1.38385582, g_loss: 0.68665338, ae_loss: 0.04973748\n",
      "Step: [4110] total_loss: 2.12921381 d_loss: 1.38339615, g_loss: 0.69180179, ae_loss: 0.05401588\n",
      "Step: [4111] total_loss: 2.12517548 d_loss: 1.38660288, g_loss: 0.68740094, ae_loss: 0.05117183\n",
      "Step: [4112] total_loss: 2.12191868 d_loss: 1.37709522, g_loss: 0.69439578, ae_loss: 0.05042781\n",
      "Step: [4113] total_loss: 2.12210751 d_loss: 1.38051808, g_loss: 0.68981147, ae_loss: 0.05177803\n",
      "Step: [4114] total_loss: 2.11248112 d_loss: 1.37998021, g_loss: 0.68188059, ae_loss: 0.05062019\n",
      "Step: [4115] total_loss: 2.11124158 d_loss: 1.37489986, g_loss: 0.68235940, ae_loss: 0.05398225\n",
      "Step: [4116] total_loss: 2.11410284 d_loss: 1.37691557, g_loss: 0.68857342, ae_loss: 0.04861390\n",
      "Step: [4117] total_loss: 2.11493254 d_loss: 1.38248730, g_loss: 0.68040895, ae_loss: 0.05203642\n",
      "Step: [4118] total_loss: 2.13491321 d_loss: 1.40371466, g_loss: 0.68353289, ae_loss: 0.04766556\n",
      "Step: [4119] total_loss: 2.13293839 d_loss: 1.38887680, g_loss: 0.69417238, ae_loss: 0.04988920\n",
      "Step: [4120] total_loss: 2.12897348 d_loss: 1.38199639, g_loss: 0.69401968, ae_loss: 0.05295755\n",
      "Step: [4121] total_loss: 2.13671136 d_loss: 1.38245201, g_loss: 0.70334160, ae_loss: 0.05091772\n",
      "Step: [4122] total_loss: 2.13354874 d_loss: 1.38085902, g_loss: 0.70078391, ae_loss: 0.05190584\n",
      "Step: [4123] total_loss: 2.13031220 d_loss: 1.38257742, g_loss: 0.69654107, ae_loss: 0.05119373\n",
      "Step: [4124] total_loss: 2.13113666 d_loss: 1.39136302, g_loss: 0.69134301, ae_loss: 0.04843053\n",
      "Step: [4125] total_loss: 2.13499689 d_loss: 1.38363910, g_loss: 0.69948208, ae_loss: 0.05187557\n",
      "Step: [4126] total_loss: 2.11399198 d_loss: 1.36555564, g_loss: 0.69566709, ae_loss: 0.05276917\n",
      "Step: [4127] total_loss: 2.12367392 d_loss: 1.38958287, g_loss: 0.68295753, ae_loss: 0.05113349\n",
      "Step: [4128] total_loss: 2.11675930 d_loss: 1.38595366, g_loss: 0.68428957, ae_loss: 0.04651623\n",
      "Step: [4129] total_loss: 2.11831045 d_loss: 1.37544727, g_loss: 0.69214702, ae_loss: 0.05071626\n",
      "Step: [4130] total_loss: 2.13327360 d_loss: 1.38237596, g_loss: 0.69608402, ae_loss: 0.05481358\n",
      "Step: [4131] total_loss: 2.12851524 d_loss: 1.39263201, g_loss: 0.68439531, ae_loss: 0.05148791\n",
      "Step: [4132] total_loss: 2.12413263 d_loss: 1.38110423, g_loss: 0.69484055, ae_loss: 0.04818770\n",
      "Step: [4133] total_loss: 2.12851429 d_loss: 1.38094771, g_loss: 0.69804168, ae_loss: 0.04952504\n",
      "Step: [4134] total_loss: 2.13692141 d_loss: 1.39037323, g_loss: 0.69614214, ae_loss: 0.05040599\n",
      "Step: [4135] total_loss: 2.11810446 d_loss: 1.35711670, g_loss: 0.71193385, ae_loss: 0.04905393\n",
      "Step: [4136] total_loss: 2.12188196 d_loss: 1.36645794, g_loss: 0.70386839, ae_loss: 0.05155546\n",
      "Step: [4137] total_loss: 2.12333560 d_loss: 1.34481168, g_loss: 0.72883129, ae_loss: 0.04969261\n",
      "Step: [4138] total_loss: 2.10999894 d_loss: 1.36620069, g_loss: 0.69545150, ae_loss: 0.04834679\n",
      "Step: [4139] total_loss: 2.11865735 d_loss: 1.37600660, g_loss: 0.68822098, ae_loss: 0.05442980\n",
      "Step: [4140] total_loss: 2.14211607 d_loss: 1.40698719, g_loss: 0.68960100, ae_loss: 0.04552777\n",
      "Step: [4141] total_loss: 2.16108394 d_loss: 1.40412474, g_loss: 0.70183396, ae_loss: 0.05512528\n",
      "Step: [4142] total_loss: 2.11294460 d_loss: 1.37430167, g_loss: 0.69117308, ae_loss: 0.04746981\n",
      "Step: [4143] total_loss: 2.13263965 d_loss: 1.36165881, g_loss: 0.72023404, ae_loss: 0.05074684\n",
      "Step: [4144] total_loss: 2.14195633 d_loss: 1.38285995, g_loss: 0.70560753, ae_loss: 0.05348882\n",
      "Step: [4145] total_loss: 2.12768602 d_loss: 1.38025236, g_loss: 0.69825459, ae_loss: 0.04917893\n",
      "Step: [4146] total_loss: 2.10760331 d_loss: 1.37528944, g_loss: 0.68163049, ae_loss: 0.05068342\n",
      "Step: [4147] total_loss: 2.13298631 d_loss: 1.37886047, g_loss: 0.70395482, ae_loss: 0.05017104\n",
      "Step: [4148] total_loss: 2.12792468 d_loss: 1.36629534, g_loss: 0.70732218, ae_loss: 0.05430711\n",
      "Step: [4149] total_loss: 2.12974977 d_loss: 1.39113879, g_loss: 0.68851674, ae_loss: 0.05009411\n",
      "Step: [4150] total_loss: 2.11574650 d_loss: 1.39607811, g_loss: 0.66726589, ae_loss: 0.05240240\n",
      "Step: [4151] total_loss: 2.11664367 d_loss: 1.39066148, g_loss: 0.67424637, ae_loss: 0.05173590\n",
      "Step: [4152] total_loss: 2.11867952 d_loss: 1.36578381, g_loss: 0.70279253, ae_loss: 0.05010301\n",
      "Step: [4153] total_loss: 2.11031866 d_loss: 1.36882365, g_loss: 0.69009054, ae_loss: 0.05140443\n",
      "Step: [4154] total_loss: 2.13032937 d_loss: 1.39719152, g_loss: 0.67797965, ae_loss: 0.05515816\n",
      "Step: [4155] total_loss: 2.11710548 d_loss: 1.37754869, g_loss: 0.68928254, ae_loss: 0.05027443\n",
      "Step: [4156] total_loss: 2.12885690 d_loss: 1.37736988, g_loss: 0.69895583, ae_loss: 0.05253129\n",
      "Step: [4157] total_loss: 2.13817120 d_loss: 1.40303588, g_loss: 0.68284702, ae_loss: 0.05228812\n",
      "Step: [4158] total_loss: 2.12659359 d_loss: 1.37101591, g_loss: 0.70643383, ae_loss: 0.04914384\n",
      "Step: [4159] total_loss: 2.12505484 d_loss: 1.38362789, g_loss: 0.68663180, ae_loss: 0.05479525\n",
      "Step: [4160] total_loss: 2.12479234 d_loss: 1.39079690, g_loss: 0.68627238, ae_loss: 0.04772308\n",
      "Step: [4161] total_loss: 2.13615441 d_loss: 1.38177764, g_loss: 0.70411801, ae_loss: 0.05025875\n",
      "Step: [4162] total_loss: 2.13269663 d_loss: 1.37495899, g_loss: 0.70319611, ae_loss: 0.05454149\n",
      "Step: [4163] total_loss: 2.12829494 d_loss: 1.38882613, g_loss: 0.68912131, ae_loss: 0.05034745\n",
      "Step: [4164] total_loss: 2.11969185 d_loss: 1.38053060, g_loss: 0.68252009, ae_loss: 0.05664112\n",
      "Step: [4165] total_loss: 2.12865257 d_loss: 1.40066874, g_loss: 0.67911041, ae_loss: 0.04887336\n",
      "Step: [4166] total_loss: 2.13631105 d_loss: 1.38383353, g_loss: 0.70162284, ae_loss: 0.05085459\n",
      "Step: [4167] total_loss: 2.10999036 d_loss: 1.36960459, g_loss: 0.68973082, ae_loss: 0.05065505\n",
      "Step: [4168] total_loss: 2.13053536 d_loss: 1.38604212, g_loss: 0.69552755, ae_loss: 0.04896571\n",
      "Step: [4169] total_loss: 2.12264109 d_loss: 1.38119841, g_loss: 0.69240963, ae_loss: 0.04903296\n",
      "Step: [4170] total_loss: 2.11710715 d_loss: 1.38843155, g_loss: 0.67946446, ae_loss: 0.04921119\n",
      "Step: [4171] total_loss: 2.11616135 d_loss: 1.37130940, g_loss: 0.69397259, ae_loss: 0.05087919\n",
      "Step: [4172] total_loss: 2.12438846 d_loss: 1.39107490, g_loss: 0.68040490, ae_loss: 0.05290869\n",
      "Step: [4173] total_loss: 2.12868547 d_loss: 1.38348782, g_loss: 0.69471020, ae_loss: 0.05048747\n",
      "Step: [4174] total_loss: 2.11885190 d_loss: 1.37241101, g_loss: 0.69510210, ae_loss: 0.05133876\n",
      "Step: [4175] total_loss: 2.12380290 d_loss: 1.37590218, g_loss: 0.69622940, ae_loss: 0.05167123\n",
      "Step: [4176] total_loss: 2.14078927 d_loss: 1.39058065, g_loss: 0.69629657, ae_loss: 0.05391207\n",
      "Step: [4177] total_loss: 2.13049459 d_loss: 1.39328790, g_loss: 0.68656057, ae_loss: 0.05064604\n",
      "Step: [4178] total_loss: 2.12082148 d_loss: 1.37226772, g_loss: 0.69558990, ae_loss: 0.05296374\n",
      "Step: [4179] total_loss: 2.13958120 d_loss: 1.38519597, g_loss: 0.70092869, ae_loss: 0.05345649\n",
      "Step: [4180] total_loss: 2.13861036 d_loss: 1.38467538, g_loss: 0.70043510, ae_loss: 0.05349989\n",
      "Step: [4181] total_loss: 2.12166405 d_loss: 1.38314724, g_loss: 0.68804842, ae_loss: 0.05046836\n",
      "Step: [4182] total_loss: 2.11469221 d_loss: 1.37310541, g_loss: 0.68543160, ae_loss: 0.05615517\n",
      "Step: [4183] total_loss: 2.11369753 d_loss: 1.38971961, g_loss: 0.67431247, ae_loss: 0.04966558\n",
      "Step: [4184] total_loss: 2.13262916 d_loss: 1.37695825, g_loss: 0.70431173, ae_loss: 0.05135921\n",
      "Step: [4185] total_loss: 2.11328411 d_loss: 1.38913536, g_loss: 0.67335820, ae_loss: 0.05079063\n",
      "Step: [4186] total_loss: 2.11483264 d_loss: 1.38437915, g_loss: 0.67804652, ae_loss: 0.05240688\n",
      "Step: [4187] total_loss: 2.09834385 d_loss: 1.37569022, g_loss: 0.66947865, ae_loss: 0.05317510\n",
      "Step: [4188] total_loss: 2.10350227 d_loss: 1.37214851, g_loss: 0.68148649, ae_loss: 0.04986731\n",
      "Step: [4189] total_loss: 2.11773133 d_loss: 1.36603224, g_loss: 0.69866723, ae_loss: 0.05303180\n",
      "Step: [4190] total_loss: 2.14134836 d_loss: 1.38912868, g_loss: 0.70153773, ae_loss: 0.05068208\n",
      "Step: [4191] total_loss: 2.13240242 d_loss: 1.39438355, g_loss: 0.68531251, ae_loss: 0.05270651\n",
      "Step: [4192] total_loss: 2.14381123 d_loss: 1.39928055, g_loss: 0.69701308, ae_loss: 0.04751749\n",
      "Step: [4193] total_loss: 2.13076305 d_loss: 1.38581252, g_loss: 0.69332254, ae_loss: 0.05162798\n",
      "Step: [4194] total_loss: 2.12928104 d_loss: 1.36987805, g_loss: 0.70469356, ae_loss: 0.05470942\n",
      "Step: [4195] total_loss: 2.15154004 d_loss: 1.40795422, g_loss: 0.69410998, ae_loss: 0.04947575\n",
      "Step: [4196] total_loss: 2.13728237 d_loss: 1.39198518, g_loss: 0.69413364, ae_loss: 0.05116367\n",
      "Step: [4197] total_loss: 2.14835167 d_loss: 1.40652037, g_loss: 0.69114244, ae_loss: 0.05068898\n",
      "Step: [4198] total_loss: 2.12036967 d_loss: 1.38194227, g_loss: 0.68609786, ae_loss: 0.05232956\n",
      "Step: [4199] total_loss: 2.11150885 d_loss: 1.37866402, g_loss: 0.68245000, ae_loss: 0.05039479\n",
      "Step: [4200] total_loss: 2.13324714 d_loss: 1.39606857, g_loss: 0.68777269, ae_loss: 0.04940588\n",
      "Step: [4201] total_loss: 2.12585878 d_loss: 1.38675821, g_loss: 0.68958127, ae_loss: 0.04951929\n",
      "Step: [4202] total_loss: 2.14336681 d_loss: 1.40477681, g_loss: 0.68724293, ae_loss: 0.05134706\n",
      "Step: [4203] total_loss: 2.15825772 d_loss: 1.41288364, g_loss: 0.69467998, ae_loss: 0.05069417\n",
      "Step: [4204] total_loss: 2.13690925 d_loss: 1.39078963, g_loss: 0.69420284, ae_loss: 0.05191674\n",
      "Step: [4205] total_loss: 2.11601830 d_loss: 1.37007356, g_loss: 0.69050086, ae_loss: 0.05544390\n",
      "Step: [4206] total_loss: 2.12617159 d_loss: 1.38689744, g_loss: 0.68676603, ae_loss: 0.05250818\n",
      "Step: [4207] total_loss: 2.12583518 d_loss: 1.37137008, g_loss: 0.70151407, ae_loss: 0.05295103\n",
      "Step: [4208] total_loss: 2.12729287 d_loss: 1.39084399, g_loss: 0.68653649, ae_loss: 0.04991240\n",
      "Step: [4209] total_loss: 2.10469222 d_loss: 1.36552930, g_loss: 0.68917698, ae_loss: 0.04998589\n",
      "Step: [4210] total_loss: 2.10725880 d_loss: 1.38861418, g_loss: 0.66834319, ae_loss: 0.05030127\n",
      "Step: [4211] total_loss: 2.12169409 d_loss: 1.38078332, g_loss: 0.68982112, ae_loss: 0.05108969\n",
      "Step: [4212] total_loss: 2.13330245 d_loss: 1.39612079, g_loss: 0.68586069, ae_loss: 0.05132087\n",
      "Step: [4213] total_loss: 2.12001085 d_loss: 1.37111270, g_loss: 0.69338739, ae_loss: 0.05551061\n",
      "Step: [4214] total_loss: 2.14453840 d_loss: 1.41015589, g_loss: 0.68574965, ae_loss: 0.04863279\n",
      "Step: [4215] total_loss: 2.12456918 d_loss: 1.37374592, g_loss: 0.70188379, ae_loss: 0.04893945\n",
      "Step: [4216] total_loss: 2.12901330 d_loss: 1.37901211, g_loss: 0.69659948, ae_loss: 0.05340168\n",
      "Step: [4217] total_loss: 2.11580610 d_loss: 1.37080264, g_loss: 0.69062746, ae_loss: 0.05437605\n",
      "Step: [4218] total_loss: 2.13267660 d_loss: 1.38149953, g_loss: 0.70294297, ae_loss: 0.04823422\n",
      "Step: [4219] total_loss: 2.12319517 d_loss: 1.38044000, g_loss: 0.69107413, ae_loss: 0.05168093\n",
      "Step: [4220] total_loss: 2.12504482 d_loss: 1.37069106, g_loss: 0.70085275, ae_loss: 0.05350109\n",
      "Step: [4221] total_loss: 2.10965347 d_loss: 1.36855972, g_loss: 0.69252729, ae_loss: 0.04856648\n",
      "Step: [4222] total_loss: 2.11714840 d_loss: 1.38719046, g_loss: 0.68421662, ae_loss: 0.04574141\n",
      "Step: [4223] total_loss: 2.12826467 d_loss: 1.40266693, g_loss: 0.67947620, ae_loss: 0.04612162\n",
      "Step: [4224] total_loss: 2.10947752 d_loss: 1.38200319, g_loss: 0.68160236, ae_loss: 0.04587214\n",
      "Step: [4225] total_loss: 2.12893295 d_loss: 1.39666247, g_loss: 0.68111241, ae_loss: 0.05115823\n",
      "Step: [4226] total_loss: 2.11851025 d_loss: 1.39146304, g_loss: 0.67632210, ae_loss: 0.05072496\n",
      "Step: [4227] total_loss: 2.12142920 d_loss: 1.38959599, g_loss: 0.68366766, ae_loss: 0.04816557\n",
      "Step: [4228] total_loss: 2.13057041 d_loss: 1.38657117, g_loss: 0.69306445, ae_loss: 0.05093492\n",
      "Step: [4229] total_loss: 2.11838055 d_loss: 1.38207865, g_loss: 0.68571419, ae_loss: 0.05058764\n",
      "Step: [4230] total_loss: 2.12757087 d_loss: 1.38262010, g_loss: 0.69613987, ae_loss: 0.04881100\n",
      "Step: [4231] total_loss: 2.11144018 d_loss: 1.38022482, g_loss: 0.68670160, ae_loss: 0.04451387\n",
      "Step: [4232] total_loss: 2.12664175 d_loss: 1.37798095, g_loss: 0.69936156, ae_loss: 0.04929908\n",
      "Step: [4233] total_loss: 2.09955120 d_loss: 1.35696936, g_loss: 0.69450665, ae_loss: 0.04807531\n",
      "Step: [4234] total_loss: 2.12517977 d_loss: 1.37231576, g_loss: 0.70039904, ae_loss: 0.05246506\n",
      "Step: [4235] total_loss: 2.09855723 d_loss: 1.36737704, g_loss: 0.67917544, ae_loss: 0.05200483\n",
      "Step: [4236] total_loss: 2.12288213 d_loss: 1.39097023, g_loss: 0.68392181, ae_loss: 0.04799006\n",
      "Step: [4237] total_loss: 2.12101364 d_loss: 1.35812521, g_loss: 0.71248651, ae_loss: 0.05040206\n",
      "Step: [4238] total_loss: 2.12590599 d_loss: 1.40332985, g_loss: 0.67378992, ae_loss: 0.04878611\n",
      "Step: [4239] total_loss: 2.11931419 d_loss: 1.36730039, g_loss: 0.70237595, ae_loss: 0.04963797\n",
      "Step: [4240] total_loss: 2.12529635 d_loss: 1.39470661, g_loss: 0.67982870, ae_loss: 0.05076094\n",
      "Step: [4241] total_loss: 2.10555983 d_loss: 1.37211752, g_loss: 0.68413544, ae_loss: 0.04930698\n",
      "Step: [4242] total_loss: 2.10877562 d_loss: 1.36980128, g_loss: 0.68930382, ae_loss: 0.04967051\n",
      "Step: [4243] total_loss: 2.11532235 d_loss: 1.37013090, g_loss: 0.69464606, ae_loss: 0.05054548\n",
      "Step: [4244] total_loss: 2.13054395 d_loss: 1.38301504, g_loss: 0.69561070, ae_loss: 0.05191820\n",
      "Step: [4245] total_loss: 2.11651182 d_loss: 1.37429452, g_loss: 0.69390547, ae_loss: 0.04831192\n",
      "Step: [4246] total_loss: 2.14430237 d_loss: 1.38302302, g_loss: 0.71062517, ae_loss: 0.05065434\n",
      "Step: [4247] total_loss: 2.13351917 d_loss: 1.40344799, g_loss: 0.67499363, ae_loss: 0.05507765\n",
      "Step: [4248] total_loss: 2.12980270 d_loss: 1.37826109, g_loss: 0.70372021, ae_loss: 0.04782153\n",
      "Step: [4249] total_loss: 2.13803244 d_loss: 1.39673567, g_loss: 0.68921846, ae_loss: 0.05207819\n",
      "Step: [4250] total_loss: 2.13496327 d_loss: 1.39405382, g_loss: 0.68715000, ae_loss: 0.05375942\n",
      "Step: [4251] total_loss: 2.13654327 d_loss: 1.38894129, g_loss: 0.69351238, ae_loss: 0.05408962\n",
      "Step: [4252] total_loss: 2.11554193 d_loss: 1.39491701, g_loss: 0.67005718, ae_loss: 0.05056781\n",
      "Step: [4253] total_loss: 2.13528156 d_loss: 1.38142121, g_loss: 0.70522571, ae_loss: 0.04863480\n",
      "Step: [4254] total_loss: 2.12273836 d_loss: 1.38969958, g_loss: 0.68364370, ae_loss: 0.04939497\n",
      "Step: [4255] total_loss: 2.11835909 d_loss: 1.37452614, g_loss: 0.69579822, ae_loss: 0.04803482\n",
      "Step: [4256] total_loss: 2.12261581 d_loss: 1.38680410, g_loss: 0.68420017, ae_loss: 0.05161171\n",
      "Step: [4257] total_loss: 2.12911558 d_loss: 1.38864565, g_loss: 0.68891823, ae_loss: 0.05155187\n",
      "Step: [4258] total_loss: 2.11885619 d_loss: 1.38202715, g_loss: 0.68878549, ae_loss: 0.04804347\n",
      "Step: [4259] total_loss: 2.12602758 d_loss: 1.40083838, g_loss: 0.67476571, ae_loss: 0.05042348\n",
      "Step: [4260] total_loss: 2.12273550 d_loss: 1.37289333, g_loss: 0.69815767, ae_loss: 0.05168450\n",
      "Step: [4261] total_loss: 2.14026308 d_loss: 1.39862478, g_loss: 0.68923700, ae_loss: 0.05240117\n",
      "Step: [4262] total_loss: 2.13654423 d_loss: 1.40284872, g_loss: 0.68249273, ae_loss: 0.05120288\n",
      "Step: [4263] total_loss: 2.13084507 d_loss: 1.39076447, g_loss: 0.68807340, ae_loss: 0.05200733\n",
      "Step: [4264] total_loss: 2.10798788 d_loss: 1.35805309, g_loss: 0.70155591, ae_loss: 0.04837888\n",
      "Step: [4265] total_loss: 2.13279223 d_loss: 1.38580585, g_loss: 0.69265956, ae_loss: 0.05432677\n",
      "Step: [4266] total_loss: 2.12662578 d_loss: 1.38155615, g_loss: 0.69754040, ae_loss: 0.04752919\n",
      "Step: [4267] total_loss: 2.13632011 d_loss: 1.39398813, g_loss: 0.69441968, ae_loss: 0.04791236\n",
      "Step: [4268] total_loss: 2.12785244 d_loss: 1.36901426, g_loss: 0.70736057, ae_loss: 0.05147754\n",
      "Step: [4269] total_loss: 2.14910460 d_loss: 1.39101267, g_loss: 0.70719624, ae_loss: 0.05089565\n",
      "Step: [4270] total_loss: 2.13987303 d_loss: 1.39566135, g_loss: 0.69468492, ae_loss: 0.04952686\n",
      "Step: [4271] total_loss: 2.11964726 d_loss: 1.37939823, g_loss: 0.69170243, ae_loss: 0.04854655\n",
      "Step: [4272] total_loss: 2.14027023 d_loss: 1.39011705, g_loss: 0.70244938, ae_loss: 0.04770377\n",
      "Step: [4273] total_loss: 2.13392925 d_loss: 1.39002609, g_loss: 0.69463825, ae_loss: 0.04926507\n",
      "Step: [4274] total_loss: 2.12875438 d_loss: 1.39088631, g_loss: 0.68827558, ae_loss: 0.04959248\n",
      "Step: [4275] total_loss: 2.12354684 d_loss: 1.37681568, g_loss: 0.69430029, ae_loss: 0.05243083\n",
      "Step: [4276] total_loss: 2.13957596 d_loss: 1.39970315, g_loss: 0.68898106, ae_loss: 0.05089176\n",
      "Step: [4277] total_loss: 2.13325453 d_loss: 1.38754964, g_loss: 0.69533014, ae_loss: 0.05037482\n",
      "Step: [4278] total_loss: 2.12297344 d_loss: 1.39143658, g_loss: 0.68651080, ae_loss: 0.04502608\n",
      "Step: [4279] total_loss: 2.12989664 d_loss: 1.39031291, g_loss: 0.68703580, ae_loss: 0.05254807\n",
      "Step: [4280] total_loss: 2.12435699 d_loss: 1.38542151, g_loss: 0.68841052, ae_loss: 0.05052496\n",
      "Step: [4281] total_loss: 2.11913562 d_loss: 1.38259435, g_loss: 0.68496358, ae_loss: 0.05157768\n",
      "Step: [4282] total_loss: 2.12739873 d_loss: 1.38321733, g_loss: 0.69613439, ae_loss: 0.04804694\n",
      "Step: [4283] total_loss: 2.11756039 d_loss: 1.37712634, g_loss: 0.69027001, ae_loss: 0.05016403\n",
      "Step: [4284] total_loss: 2.11623001 d_loss: 1.38350224, g_loss: 0.68877763, ae_loss: 0.04395011\n",
      "Step: [4285] total_loss: 2.14654636 d_loss: 1.40240479, g_loss: 0.69448417, ae_loss: 0.04965741\n",
      "Step: [4286] total_loss: 2.11322284 d_loss: 1.37984538, g_loss: 0.68137461, ae_loss: 0.05200297\n",
      "Step: [4287] total_loss: 2.12165713 d_loss: 1.36521912, g_loss: 0.70513189, ae_loss: 0.05130609\n",
      "Step: [4288] total_loss: 2.12323785 d_loss: 1.37483299, g_loss: 0.69507617, ae_loss: 0.05332864\n",
      "Step: [4289] total_loss: 2.12548399 d_loss: 1.39168811, g_loss: 0.68333817, ae_loss: 0.05045771\n",
      "Step: [4290] total_loss: 2.13504648 d_loss: 1.39875150, g_loss: 0.68858957, ae_loss: 0.04770526\n",
      "Step: [4291] total_loss: 2.13818288 d_loss: 1.39493299, g_loss: 0.68901825, ae_loss: 0.05423169\n",
      "Step: [4292] total_loss: 2.13281131 d_loss: 1.38654327, g_loss: 0.69325423, ae_loss: 0.05301380\n",
      "Step: [4293] total_loss: 2.11231947 d_loss: 1.37805104, g_loss: 0.68256104, ae_loss: 0.05170728\n",
      "Step: [4294] total_loss: 2.12768793 d_loss: 1.38541114, g_loss: 0.69037127, ae_loss: 0.05190542\n",
      "Step: [4295] total_loss: 2.11205649 d_loss: 1.37023807, g_loss: 0.69424021, ae_loss: 0.04757817\n",
      "Step: [4296] total_loss: 2.12259650 d_loss: 1.38799703, g_loss: 0.68576503, ae_loss: 0.04883449\n",
      "Step: [4297] total_loss: 2.14048982 d_loss: 1.40302920, g_loss: 0.68925774, ae_loss: 0.04820292\n",
      "Step: [4298] total_loss: 2.11689329 d_loss: 1.37847638, g_loss: 0.68670154, ae_loss: 0.05171534\n",
      "Step: [4299] total_loss: 2.13094926 d_loss: 1.38347661, g_loss: 0.69519621, ae_loss: 0.05227651\n",
      "Step: [4300] total_loss: 2.12846136 d_loss: 1.38281298, g_loss: 0.69285363, ae_loss: 0.05279474\n",
      "Step: [4301] total_loss: 2.13536692 d_loss: 1.38719869, g_loss: 0.69725728, ae_loss: 0.05091109\n",
      "Step: [4302] total_loss: 2.14233613 d_loss: 1.39576948, g_loss: 0.69514436, ae_loss: 0.05142238\n",
      "Step: [4303] total_loss: 2.12639284 d_loss: 1.37917268, g_loss: 0.70090020, ae_loss: 0.04632008\n",
      "Step: [4304] total_loss: 2.12481618 d_loss: 1.38977408, g_loss: 0.68122387, ae_loss: 0.05381829\n",
      "Step: [4305] total_loss: 2.12247372 d_loss: 1.38940597, g_loss: 0.68497455, ae_loss: 0.04809319\n",
      "Step: [4306] total_loss: 2.13286233 d_loss: 1.39244318, g_loss: 0.68993956, ae_loss: 0.05047968\n",
      "Step: [4307] total_loss: 2.12715816 d_loss: 1.38176680, g_loss: 0.69371611, ae_loss: 0.05167529\n",
      "Step: [4308] total_loss: 2.12904072 d_loss: 1.38577533, g_loss: 0.69307315, ae_loss: 0.05019232\n",
      "Step: [4309] total_loss: 2.13110518 d_loss: 1.38614011, g_loss: 0.69488794, ae_loss: 0.05007724\n",
      "Step: [4310] total_loss: 2.13174319 d_loss: 1.37814176, g_loss: 0.70351446, ae_loss: 0.05008703\n",
      "Step: [4311] total_loss: 2.14298844 d_loss: 1.39967155, g_loss: 0.69379318, ae_loss: 0.04952373\n",
      "Step: [4312] total_loss: 2.13994551 d_loss: 1.39616251, g_loss: 0.69132316, ae_loss: 0.05245969\n",
      "Step: [4313] total_loss: 2.12074041 d_loss: 1.38077283, g_loss: 0.69383371, ae_loss: 0.04613398\n",
      "Step: [4314] total_loss: 2.12534094 d_loss: 1.38554645, g_loss: 0.69030738, ae_loss: 0.04948696\n",
      "Step: [4315] total_loss: 2.12448835 d_loss: 1.37984085, g_loss: 0.69582999, ae_loss: 0.04881755\n",
      "Step: [4316] total_loss: 2.11353731 d_loss: 1.36850095, g_loss: 0.69469404, ae_loss: 0.05034234\n",
      "Step: [4317] total_loss: 2.11718345 d_loss: 1.38758469, g_loss: 0.67651606, ae_loss: 0.05308276\n",
      "Step: [4318] total_loss: 2.10921383 d_loss: 1.38452601, g_loss: 0.67129588, ae_loss: 0.05339190\n",
      "Step: [4319] total_loss: 2.11101055 d_loss: 1.37560463, g_loss: 0.68787122, ae_loss: 0.04753477\n",
      "Step: [4320] total_loss: 2.12283182 d_loss: 1.38695240, g_loss: 0.68416178, ae_loss: 0.05171758\n",
      "Step: [4321] total_loss: 2.10834002 d_loss: 1.37707138, g_loss: 0.68198419, ae_loss: 0.04928443\n",
      "Step: [4322] total_loss: 2.12463522 d_loss: 1.38091421, g_loss: 0.69042349, ae_loss: 0.05329755\n",
      "Step: [4323] total_loss: 2.11808395 d_loss: 1.38976514, g_loss: 0.67396736, ae_loss: 0.05435162\n",
      "Step: [4324] total_loss: 2.13793945 d_loss: 1.38522005, g_loss: 0.70180088, ae_loss: 0.05091856\n",
      "Step: [4325] total_loss: 2.14760447 d_loss: 1.39050436, g_loss: 0.70273578, ae_loss: 0.05436446\n",
      "Step: [4326] total_loss: 2.12844229 d_loss: 1.38658011, g_loss: 0.69341397, ae_loss: 0.04844815\n",
      "Step: [4327] total_loss: 2.14387703 d_loss: 1.38652134, g_loss: 0.70590043, ae_loss: 0.05145526\n",
      "Step: [4328] total_loss: 2.13567686 d_loss: 1.38500595, g_loss: 0.69748676, ae_loss: 0.05318417\n",
      "Step: [4329] total_loss: 2.14810491 d_loss: 1.40280664, g_loss: 0.69328803, ae_loss: 0.05201017\n",
      "Step: [4330] total_loss: 2.12507939 d_loss: 1.38029337, g_loss: 0.69761860, ae_loss: 0.04716738\n",
      "Step: [4331] total_loss: 2.13890553 d_loss: 1.39744020, g_loss: 0.68814611, ae_loss: 0.05331935\n",
      "Step: [4332] total_loss: 2.13670087 d_loss: 1.38497901, g_loss: 0.69784909, ae_loss: 0.05387272\n",
      "Step: [4333] total_loss: 2.13378048 d_loss: 1.38614750, g_loss: 0.69715244, ae_loss: 0.05048050\n",
      "Step: [4334] total_loss: 2.14362144 d_loss: 1.39848876, g_loss: 0.69365370, ae_loss: 0.05147912\n",
      "Step: [4335] total_loss: 2.12918258 d_loss: 1.39243174, g_loss: 0.68638825, ae_loss: 0.05036258\n",
      "Step: [4336] total_loss: 2.12436247 d_loss: 1.38373446, g_loss: 0.68738425, ae_loss: 0.05324392\n",
      "Step: [4337] total_loss: 2.12545133 d_loss: 1.38682234, g_loss: 0.68804812, ae_loss: 0.05058085\n",
      "Step: [4338] total_loss: 2.12360406 d_loss: 1.39041114, g_loss: 0.68397105, ae_loss: 0.04922184\n",
      "Step: [4339] total_loss: 2.13645577 d_loss: 1.38367271, g_loss: 0.69915611, ae_loss: 0.05362705\n",
      "Step: [4340] total_loss: 2.12813330 d_loss: 1.37960720, g_loss: 0.69792688, ae_loss: 0.05059920\n",
      "Step: [4341] total_loss: 2.13459754 d_loss: 1.38909698, g_loss: 0.70031035, ae_loss: 0.04519021\n",
      "Step: [4342] total_loss: 2.13996434 d_loss: 1.38484037, g_loss: 0.70339799, ae_loss: 0.05172594\n",
      "Step: [4343] total_loss: 2.12800694 d_loss: 1.38769794, g_loss: 0.69128108, ae_loss: 0.04902792\n",
      "Step: [4344] total_loss: 2.11651683 d_loss: 1.37360692, g_loss: 0.68922156, ae_loss: 0.05368834\n",
      "Step: [4345] total_loss: 2.14356208 d_loss: 1.39312863, g_loss: 0.69945025, ae_loss: 0.05098319\n",
      "Step: [4346] total_loss: 2.11095810 d_loss: 1.37845874, g_loss: 0.68678391, ae_loss: 0.04571557\n",
      "Step: [4347] total_loss: 2.12261963 d_loss: 1.37977600, g_loss: 0.69432086, ae_loss: 0.04852268\n",
      "Step: [4348] total_loss: 2.11204481 d_loss: 1.36812532, g_loss: 0.69619942, ae_loss: 0.04772021\n",
      "Step: [4349] total_loss: 2.11519885 d_loss: 1.37853634, g_loss: 0.68669844, ae_loss: 0.04996409\n",
      "Step: [4350] total_loss: 2.11135244 d_loss: 1.37425411, g_loss: 0.68067646, ae_loss: 0.05642176\n",
      "Step: [4351] total_loss: 2.11037636 d_loss: 1.38680315, g_loss: 0.67396933, ae_loss: 0.04960390\n",
      "Step: [4352] total_loss: 2.11476612 d_loss: 1.37852645, g_loss: 0.68218219, ae_loss: 0.05405741\n",
      "Step: [4353] total_loss: 2.11602259 d_loss: 1.38846982, g_loss: 0.67682397, ae_loss: 0.05072876\n",
      "Step: [4354] total_loss: 2.11642075 d_loss: 1.38168144, g_loss: 0.68141919, ae_loss: 0.05332015\n",
      "Step: [4355] total_loss: 2.12761259 d_loss: 1.39189792, g_loss: 0.68290019, ae_loss: 0.05281465\n",
      "Step: [4356] total_loss: 2.11045599 d_loss: 1.36542344, g_loss: 0.69102359, ae_loss: 0.05400904\n",
      "Step: [4357] total_loss: 2.12603712 d_loss: 1.38682032, g_loss: 0.68788695, ae_loss: 0.05132985\n",
      "Step: [4358] total_loss: 2.12733960 d_loss: 1.38128614, g_loss: 0.69616300, ae_loss: 0.04989039\n",
      "Step: [4359] total_loss: 2.13205242 d_loss: 1.39034438, g_loss: 0.69341958, ae_loss: 0.04828833\n",
      "Step: [4360] total_loss: 2.11291003 d_loss: 1.36700761, g_loss: 0.69477814, ae_loss: 0.05112420\n",
      "Step: [4361] total_loss: 2.12865734 d_loss: 1.38824081, g_loss: 0.68729126, ae_loss: 0.05312534\n",
      "Step: [4362] total_loss: 2.12820816 d_loss: 1.39581263, g_loss: 0.68130243, ae_loss: 0.05109299\n",
      "Step: [4363] total_loss: 2.12712765 d_loss: 1.38164449, g_loss: 0.69676328, ae_loss: 0.04871999\n",
      "Step: [4364] total_loss: 2.11112309 d_loss: 1.37777424, g_loss: 0.67862773, ae_loss: 0.05472097\n",
      "Step: [4365] total_loss: 2.13023710 d_loss: 1.38647246, g_loss: 0.69329202, ae_loss: 0.05047269\n",
      "Step: [4366] total_loss: 2.12965083 d_loss: 1.39231062, g_loss: 0.68347245, ae_loss: 0.05386772\n",
      "Step: [4367] total_loss: 2.10936975 d_loss: 1.37875938, g_loss: 0.68134135, ae_loss: 0.04926910\n",
      "Step: [4368] total_loss: 2.11488724 d_loss: 1.37324810, g_loss: 0.69110978, ae_loss: 0.05052925\n",
      "Step: [4369] total_loss: 2.12513232 d_loss: 1.38094103, g_loss: 0.69269276, ae_loss: 0.05149854\n",
      "Step: [4370] total_loss: 2.10592508 d_loss: 1.35284054, g_loss: 0.70594335, ae_loss: 0.04714132\n",
      "Step: [4371] total_loss: 2.13022470 d_loss: 1.38363159, g_loss: 0.69419098, ae_loss: 0.05240207\n",
      "Step: [4372] total_loss: 2.11636639 d_loss: 1.38360405, g_loss: 0.68154538, ae_loss: 0.05121700\n",
      "Step: [4373] total_loss: 2.11194944 d_loss: 1.38158703, g_loss: 0.68073165, ae_loss: 0.04963080\n",
      "Step: [4374] total_loss: 2.13113451 d_loss: 1.39235544, g_loss: 0.68260670, ae_loss: 0.05617238\n",
      "Step: [4375] total_loss: 2.11189175 d_loss: 1.36863589, g_loss: 0.69374549, ae_loss: 0.04951047\n",
      "Step: [4376] total_loss: 2.12388134 d_loss: 1.37802088, g_loss: 0.69800776, ae_loss: 0.04785277\n",
      "Step: [4377] total_loss: 2.11947775 d_loss: 1.38003159, g_loss: 0.69022453, ae_loss: 0.04922178\n",
      "Step: [4378] total_loss: 2.13623714 d_loss: 1.39730310, g_loss: 0.68769765, ae_loss: 0.05123649\n",
      "Step: [4379] total_loss: 2.14071941 d_loss: 1.39478254, g_loss: 0.69525659, ae_loss: 0.05068032\n",
      "Step: [4380] total_loss: 2.13754034 d_loss: 1.38447273, g_loss: 0.70728195, ae_loss: 0.04578549\n",
      "Step: [4381] total_loss: 2.12331390 d_loss: 1.38105702, g_loss: 0.69618881, ae_loss: 0.04606794\n",
      "Step: [4382] total_loss: 2.13476586 d_loss: 1.39470541, g_loss: 0.68687004, ae_loss: 0.05319051\n",
      "Step: [4383] total_loss: 2.13028002 d_loss: 1.39566553, g_loss: 0.68462932, ae_loss: 0.04998526\n",
      "Step: [4384] total_loss: 2.11007810 d_loss: 1.36402392, g_loss: 0.70040083, ae_loss: 0.04565340\n",
      "Step: [4385] total_loss: 2.12499714 d_loss: 1.37914562, g_loss: 0.70009089, ae_loss: 0.04576064\n",
      "Step: [4386] total_loss: 2.12936449 d_loss: 1.39495254, g_loss: 0.68002927, ae_loss: 0.05438272\n",
      "Step: [4387] total_loss: 2.12619090 d_loss: 1.39585054, g_loss: 0.68036908, ae_loss: 0.04997137\n",
      "Step: [4388] total_loss: 2.12854052 d_loss: 1.39280725, g_loss: 0.68401790, ae_loss: 0.05171520\n",
      "Step: [4389] total_loss: 2.12161303 d_loss: 1.37887931, g_loss: 0.69056076, ae_loss: 0.05217299\n",
      "Step: [4390] total_loss: 2.11854649 d_loss: 1.36998880, g_loss: 0.69919217, ae_loss: 0.04936552\n",
      "Step: [4391] total_loss: 2.13955402 d_loss: 1.38809443, g_loss: 0.69938254, ae_loss: 0.05207720\n",
      "Step: [4392] total_loss: 2.12114930 d_loss: 1.38052058, g_loss: 0.68964928, ae_loss: 0.05097940\n",
      "Step: [4393] total_loss: 2.14463758 d_loss: 1.39861071, g_loss: 0.69231516, ae_loss: 0.05371160\n",
      "Step: [4394] total_loss: 2.11207390 d_loss: 1.37240458, g_loss: 0.69161630, ae_loss: 0.04805291\n",
      "Step: [4395] total_loss: 2.11169958 d_loss: 1.36185694, g_loss: 0.69953835, ae_loss: 0.05030435\n",
      "Step: [4396] total_loss: 2.12752461 d_loss: 1.37626898, g_loss: 0.69819587, ae_loss: 0.05305974\n",
      "Step: [4397] total_loss: 2.12073874 d_loss: 1.38677359, g_loss: 0.68560231, ae_loss: 0.04836289\n",
      "Step: [4398] total_loss: 2.15785480 d_loss: 1.39246380, g_loss: 0.71471101, ae_loss: 0.05068003\n",
      "Step: [4399] total_loss: 2.15400958 d_loss: 1.41139638, g_loss: 0.69093555, ae_loss: 0.05167757\n",
      "Step: [4400] total_loss: 2.12684393 d_loss: 1.36738801, g_loss: 0.70596868, ae_loss: 0.05348722\n",
      "Step: [4401] total_loss: 2.11221480 d_loss: 1.37736034, g_loss: 0.68344843, ae_loss: 0.05140608\n",
      "Step: [4402] total_loss: 2.11960626 d_loss: 1.39166760, g_loss: 0.67878860, ae_loss: 0.04914999\n",
      "Step: [4403] total_loss: 2.13265538 d_loss: 1.39239049, g_loss: 0.68963748, ae_loss: 0.05062743\n",
      "Step: [4404] total_loss: 2.13441539 d_loss: 1.37909174, g_loss: 0.70138901, ae_loss: 0.05393464\n",
      "Step: [4405] total_loss: 2.12316751 d_loss: 1.39258051, g_loss: 0.67958260, ae_loss: 0.05100438\n",
      "Step: [4406] total_loss: 2.11115026 d_loss: 1.37426448, g_loss: 0.68662655, ae_loss: 0.05025915\n",
      "Step: [4407] total_loss: 2.11559486 d_loss: 1.37643945, g_loss: 0.69331074, ae_loss: 0.04584479\n",
      "Step: [4408] total_loss: 2.11786914 d_loss: 1.38327432, g_loss: 0.68379140, ae_loss: 0.05080337\n",
      "Step: [4409] total_loss: 2.10239887 d_loss: 1.34361434, g_loss: 0.70630515, ae_loss: 0.05247948\n",
      "Step: [4410] total_loss: 2.13235617 d_loss: 1.38427651, g_loss: 0.69552308, ae_loss: 0.05255662\n",
      "Step: [4411] total_loss: 2.14309835 d_loss: 1.39502227, g_loss: 0.69863796, ae_loss: 0.04943817\n",
      "Step: [4412] total_loss: 2.13469839 d_loss: 1.37565625, g_loss: 0.70138764, ae_loss: 0.05765460\n",
      "Step: [4413] total_loss: 2.12365437 d_loss: 1.36869144, g_loss: 0.70352209, ae_loss: 0.05144097\n",
      "Step: [4414] total_loss: 2.12301350 d_loss: 1.36740232, g_loss: 0.70417958, ae_loss: 0.05143150\n",
      "Step: [4415] total_loss: 2.11274719 d_loss: 1.36298478, g_loss: 0.70126510, ae_loss: 0.04849732\n",
      "Step: [4416] total_loss: 2.13661098 d_loss: 1.40060067, g_loss: 0.68972611, ae_loss: 0.04628412\n",
      "Step: [4417] total_loss: 2.15391541 d_loss: 1.38383031, g_loss: 0.72086477, ae_loss: 0.04922028\n",
      "Step: [4418] total_loss: 2.12021971 d_loss: 1.37840223, g_loss: 0.69284272, ae_loss: 0.04897470\n",
      "Step: [4419] total_loss: 2.11135292 d_loss: 1.36494398, g_loss: 0.69566202, ae_loss: 0.05074686\n",
      "Step: [4420] total_loss: 2.12739277 d_loss: 1.39920652, g_loss: 0.67738730, ae_loss: 0.05079907\n",
      "Step: [4421] total_loss: 2.12270617 d_loss: 1.39280629, g_loss: 0.68321985, ae_loss: 0.04667999\n",
      "Step: [4422] total_loss: 2.11812496 d_loss: 1.37294161, g_loss: 0.69846058, ae_loss: 0.04672276\n",
      "Step: [4423] total_loss: 2.11471987 d_loss: 1.37927806, g_loss: 0.68283963, ae_loss: 0.05260210\n",
      "Step: [4424] total_loss: 2.13122106 d_loss: 1.36804438, g_loss: 0.70841932, ae_loss: 0.05475730\n",
      "Step: [4425] total_loss: 2.12011909 d_loss: 1.37535524, g_loss: 0.69626260, ae_loss: 0.04850140\n",
      "Step: [4426] total_loss: 2.12373924 d_loss: 1.38835776, g_loss: 0.68470216, ae_loss: 0.05067941\n",
      "Step: [4427] total_loss: 2.12279177 d_loss: 1.37822294, g_loss: 0.69476873, ae_loss: 0.04980015\n",
      "Step: [4428] total_loss: 2.13385677 d_loss: 1.37928605, g_loss: 0.70711291, ae_loss: 0.04745771\n",
      "Step: [4429] total_loss: 2.13432956 d_loss: 1.37713301, g_loss: 0.70253766, ae_loss: 0.05465888\n",
      "Step: [4430] total_loss: 2.11850929 d_loss: 1.37660229, g_loss: 0.69383019, ae_loss: 0.04807673\n",
      "Step: [4431] total_loss: 2.15153313 d_loss: 1.41089964, g_loss: 0.68938601, ae_loss: 0.05124737\n",
      "Step: [4432] total_loss: 2.11654592 d_loss: 1.37484884, g_loss: 0.69237506, ae_loss: 0.04932195\n",
      "Step: [4433] total_loss: 2.12340307 d_loss: 1.38122606, g_loss: 0.69369513, ae_loss: 0.04848197\n",
      "Step: [4434] total_loss: 2.12532473 d_loss: 1.39266372, g_loss: 0.68143690, ae_loss: 0.05122423\n",
      "Step: [4435] total_loss: 2.11929083 d_loss: 1.37661552, g_loss: 0.69211286, ae_loss: 0.05056253\n",
      "Step: [4436] total_loss: 2.11376429 d_loss: 1.37086797, g_loss: 0.69370985, ae_loss: 0.04918646\n",
      "Step: [4437] total_loss: 2.14127707 d_loss: 1.38842463, g_loss: 0.70095342, ae_loss: 0.05189892\n",
      "Step: [4438] total_loss: 2.12872076 d_loss: 1.38483691, g_loss: 0.69649392, ae_loss: 0.04738986\n",
      "Step: [4439] total_loss: 2.11915040 d_loss: 1.37914813, g_loss: 0.68571329, ae_loss: 0.05428895\n",
      "Step: [4440] total_loss: 2.12389827 d_loss: 1.38098192, g_loss: 0.69136077, ae_loss: 0.05155548\n",
      "Step: [4441] total_loss: 2.12629676 d_loss: 1.39187050, g_loss: 0.68120944, ae_loss: 0.05321680\n",
      "Step: [4442] total_loss: 2.12087584 d_loss: 1.39725566, g_loss: 0.66917491, ae_loss: 0.05444517\n",
      "Step: [4443] total_loss: 2.13036776 d_loss: 1.39413023, g_loss: 0.68235922, ae_loss: 0.05387821\n",
      "Step: [4444] total_loss: 2.12383032 d_loss: 1.37266541, g_loss: 0.69978726, ae_loss: 0.05137776\n",
      "Step: [4445] total_loss: 2.12328911 d_loss: 1.37665951, g_loss: 0.69414240, ae_loss: 0.05248729\n",
      "Step: [4446] total_loss: 2.13758183 d_loss: 1.39363909, g_loss: 0.69784570, ae_loss: 0.04609692\n",
      "Step: [4447] total_loss: 2.14283991 d_loss: 1.38618875, g_loss: 0.70535398, ae_loss: 0.05129729\n",
      "Step: [4448] total_loss: 2.14748311 d_loss: 1.40887678, g_loss: 0.69150496, ae_loss: 0.04710132\n",
      "Step: [4449] total_loss: 2.12105417 d_loss: 1.37473571, g_loss: 0.69633925, ae_loss: 0.04997919\n",
      "Step: [4450] total_loss: 2.12587547 d_loss: 1.38617158, g_loss: 0.69087815, ae_loss: 0.04882565\n",
      "Step: [4451] total_loss: 2.12224460 d_loss: 1.37153745, g_loss: 0.70349783, ae_loss: 0.04720923\n",
      "Step: [4452] total_loss: 2.13033605 d_loss: 1.38683963, g_loss: 0.69218421, ae_loss: 0.05131223\n",
      "Step: [4453] total_loss: 2.12448120 d_loss: 1.37176144, g_loss: 0.69680500, ae_loss: 0.05591470\n",
      "Step: [4454] total_loss: 2.11783743 d_loss: 1.38441133, g_loss: 0.68404508, ae_loss: 0.04938108\n",
      "Step: [4455] total_loss: 2.11776209 d_loss: 1.38068199, g_loss: 0.68678504, ae_loss: 0.05029506\n",
      "Step: [4456] total_loss: 2.12725019 d_loss: 1.37786698, g_loss: 0.70081079, ae_loss: 0.04857240\n",
      "Step: [4457] total_loss: 2.14707446 d_loss: 1.40531039, g_loss: 0.69016618, ae_loss: 0.05159792\n",
      "Step: [4458] total_loss: 2.15155554 d_loss: 1.39988661, g_loss: 0.69504488, ae_loss: 0.05662391\n",
      "Step: [4459] total_loss: 2.12553525 d_loss: 1.38093948, g_loss: 0.69595629, ae_loss: 0.04863945\n",
      "Step: [4460] total_loss: 2.12968493 d_loss: 1.39002061, g_loss: 0.68791807, ae_loss: 0.05174629\n",
      "Step: [4461] total_loss: 2.11407495 d_loss: 1.35693836, g_loss: 0.71057343, ae_loss: 0.04656312\n",
      "Step: [4462] total_loss: 2.12438774 d_loss: 1.39335120, g_loss: 0.68226433, ae_loss: 0.04877208\n",
      "Step: [4463] total_loss: 2.12437987 d_loss: 1.38899231, g_loss: 0.68375921, ae_loss: 0.05162830\n",
      "Step: [4464] total_loss: 2.11908579 d_loss: 1.39810014, g_loss: 0.66790092, ae_loss: 0.05308465\n",
      "Step: [4465] total_loss: 2.12942004 d_loss: 1.38970232, g_loss: 0.68999904, ae_loss: 0.04971868\n",
      "Step: [4466] total_loss: 2.12352133 d_loss: 1.38068235, g_loss: 0.69123256, ae_loss: 0.05160637\n",
      "Step: [4467] total_loss: 2.12986946 d_loss: 1.37236285, g_loss: 0.70924300, ae_loss: 0.04826351\n",
      "Step: [4468] total_loss: 2.14233589 d_loss: 1.39160442, g_loss: 0.69716418, ae_loss: 0.05356724\n",
      "Step: [4469] total_loss: 2.13062286 d_loss: 1.37976456, g_loss: 0.70041084, ae_loss: 0.05044754\n",
      "Step: [4470] total_loss: 2.13698673 d_loss: 1.39361989, g_loss: 0.69385886, ae_loss: 0.04950790\n",
      "Step: [4471] total_loss: 2.13521123 d_loss: 1.38491797, g_loss: 0.70030344, ae_loss: 0.04998978\n",
      "Step: [4472] total_loss: 2.15732789 d_loss: 1.40080643, g_loss: 0.70268846, ae_loss: 0.05383302\n",
      "Step: [4473] total_loss: 2.13015509 d_loss: 1.39267135, g_loss: 0.68748450, ae_loss: 0.04999939\n",
      "Step: [4474] total_loss: 2.12943697 d_loss: 1.38521898, g_loss: 0.69746274, ae_loss: 0.04675533\n",
      "Step: [4475] total_loss: 2.12569785 d_loss: 1.39389110, g_loss: 0.68232691, ae_loss: 0.04947989\n",
      "Step: [4476] total_loss: 2.11368060 d_loss: 1.36494744, g_loss: 0.69808263, ae_loss: 0.05065055\n",
      "Step: [4477] total_loss: 2.12317681 d_loss: 1.39607656, g_loss: 0.67840385, ae_loss: 0.04869641\n",
      "Step: [4478] total_loss: 2.14484382 d_loss: 1.41086292, g_loss: 0.68109715, ae_loss: 0.05288377\n",
      "Step: [4479] total_loss: 2.13331890 d_loss: 1.38618326, g_loss: 0.69521677, ae_loss: 0.05191903\n",
      "Step: [4480] total_loss: 2.11629462 d_loss: 1.38186288, g_loss: 0.68327832, ae_loss: 0.05115344\n",
      "Step: [4481] total_loss: 2.12548018 d_loss: 1.37332940, g_loss: 0.70356345, ae_loss: 0.04858727\n",
      "Step: [4482] total_loss: 2.12826586 d_loss: 1.38329482, g_loss: 0.69812673, ae_loss: 0.04684429\n",
      "Step: [4483] total_loss: 2.13290334 d_loss: 1.38975883, g_loss: 0.69279456, ae_loss: 0.05034992\n",
      "Step: [4484] total_loss: 2.13436246 d_loss: 1.38671470, g_loss: 0.69923353, ae_loss: 0.04841425\n",
      "Step: [4485] total_loss: 2.12779284 d_loss: 1.38033152, g_loss: 0.69703352, ae_loss: 0.05042784\n",
      "Step: [4486] total_loss: 2.14114857 d_loss: 1.38265538, g_loss: 0.70818901, ae_loss: 0.05030407\n",
      "Step: [4487] total_loss: 2.11639667 d_loss: 1.37791514, g_loss: 0.68976825, ae_loss: 0.04871315\n",
      "Step: [4488] total_loss: 2.13410759 d_loss: 1.39735198, g_loss: 0.68458223, ae_loss: 0.05217330\n",
      "Step: [4489] total_loss: 2.14932966 d_loss: 1.40579224, g_loss: 0.68973839, ae_loss: 0.05379903\n",
      "Step: [4490] total_loss: 2.12982917 d_loss: 1.38916254, g_loss: 0.69531721, ae_loss: 0.04534944\n",
      "Step: [4491] total_loss: 2.12591720 d_loss: 1.38698888, g_loss: 0.68968248, ae_loss: 0.04924582\n",
      "Step: [4492] total_loss: 2.13244629 d_loss: 1.37773037, g_loss: 0.70234525, ae_loss: 0.05237076\n",
      "Step: [4493] total_loss: 2.13368011 d_loss: 1.38210177, g_loss: 0.70277029, ae_loss: 0.04880799\n",
      "Step: [4494] total_loss: 2.14450359 d_loss: 1.39935160, g_loss: 0.69253403, ae_loss: 0.05261789\n",
      "Step: [4495] total_loss: 2.13005662 d_loss: 1.38966823, g_loss: 0.68911028, ae_loss: 0.05127805\n",
      "Step: [4496] total_loss: 2.13625050 d_loss: 1.39904296, g_loss: 0.68827677, ae_loss: 0.04893093\n",
      "Step: [4497] total_loss: 2.13427448 d_loss: 1.38380861, g_loss: 0.69932699, ae_loss: 0.05113899\n",
      "Step: [4498] total_loss: 2.13944221 d_loss: 1.39220464, g_loss: 0.69411814, ae_loss: 0.05311942\n",
      "Step: [4499] total_loss: 2.13625622 d_loss: 1.38702977, g_loss: 0.69978106, ae_loss: 0.04944555\n",
      "Step: [4500] total_loss: 2.13596058 d_loss: 1.39184499, g_loss: 0.69546747, ae_loss: 0.04864802\n",
      "Step: [4501] total_loss: 2.14406824 d_loss: 1.39517558, g_loss: 0.69496930, ae_loss: 0.05392331\n",
      "Step: [4502] total_loss: 2.12396049 d_loss: 1.37556052, g_loss: 0.69579720, ae_loss: 0.05260276\n",
      "Step: [4503] total_loss: 2.11815619 d_loss: 1.37875175, g_loss: 0.68750894, ae_loss: 0.05189553\n",
      "Step: [4504] total_loss: 2.12693214 d_loss: 1.39528728, g_loss: 0.68193603, ae_loss: 0.04970878\n",
      "Step: [4505] total_loss: 2.11481142 d_loss: 1.38837016, g_loss: 0.67842948, ae_loss: 0.04801182\n",
      "Step: [4506] total_loss: 2.12075853 d_loss: 1.39808273, g_loss: 0.67036802, ae_loss: 0.05230776\n",
      "Step: [4507] total_loss: 2.11021805 d_loss: 1.38356113, g_loss: 0.67541575, ae_loss: 0.05124107\n",
      "Step: [4508] total_loss: 2.11946487 d_loss: 1.37482595, g_loss: 0.69058025, ae_loss: 0.05405875\n",
      "Step: [4509] total_loss: 2.13000107 d_loss: 1.37572098, g_loss: 0.70408320, ae_loss: 0.05019690\n",
      "Step: [4510] total_loss: 2.12318993 d_loss: 1.38173985, g_loss: 0.69121480, ae_loss: 0.05023523\n",
      "Step: [4511] total_loss: 2.10932922 d_loss: 1.36709464, g_loss: 0.69054836, ae_loss: 0.05168626\n",
      "Step: [4512] total_loss: 2.12289476 d_loss: 1.37579107, g_loss: 0.69757450, ae_loss: 0.04952930\n",
      "Step: [4513] total_loss: 2.12208700 d_loss: 1.38546431, g_loss: 0.68880606, ae_loss: 0.04781677\n",
      "Step: [4514] total_loss: 2.11771011 d_loss: 1.39581776, g_loss: 0.67040807, ae_loss: 0.05148426\n",
      "Step: [4515] total_loss: 2.13114667 d_loss: 1.39361358, g_loss: 0.69063640, ae_loss: 0.04689676\n",
      "Step: [4516] total_loss: 2.13172054 d_loss: 1.38600898, g_loss: 0.69290650, ae_loss: 0.05280506\n",
      "Step: [4517] total_loss: 2.11785054 d_loss: 1.37849879, g_loss: 0.69043088, ae_loss: 0.04892092\n",
      "Step: [4518] total_loss: 2.11864710 d_loss: 1.37466753, g_loss: 0.69375408, ae_loss: 0.05022549\n",
      "Step: [4519] total_loss: 2.12208557 d_loss: 1.36568356, g_loss: 0.70775640, ae_loss: 0.04864548\n",
      "Step: [4520] total_loss: 2.13487101 d_loss: 1.37845635, g_loss: 0.70323265, ae_loss: 0.05318194\n",
      "Step: [4521] total_loss: 2.12551165 d_loss: 1.38562751, g_loss: 0.69054544, ae_loss: 0.04933869\n",
      "Step: [4522] total_loss: 2.14411449 d_loss: 1.39105701, g_loss: 0.70361841, ae_loss: 0.04943893\n",
      "Step: [4523] total_loss: 2.12099957 d_loss: 1.38199282, g_loss: 0.68605781, ae_loss: 0.05294895\n",
      "Step: [4524] total_loss: 2.12129211 d_loss: 1.38547039, g_loss: 0.68372309, ae_loss: 0.05209855\n",
      "Step: [4525] total_loss: 2.11577749 d_loss: 1.38204563, g_loss: 0.68400121, ae_loss: 0.04973057\n",
      "Step: [4526] total_loss: 2.09847164 d_loss: 1.37233984, g_loss: 0.67701995, ae_loss: 0.04911184\n",
      "Step: [4527] total_loss: 2.12724161 d_loss: 1.38764668, g_loss: 0.69233906, ae_loss: 0.04725592\n",
      "Step: [4528] total_loss: 2.12184620 d_loss: 1.37525344, g_loss: 0.69577193, ae_loss: 0.05082095\n",
      "Step: [4529] total_loss: 2.11689281 d_loss: 1.38013291, g_loss: 0.68638933, ae_loss: 0.05037069\n",
      "Step: [4530] total_loss: 2.12073278 d_loss: 1.37598765, g_loss: 0.69541281, ae_loss: 0.04933236\n",
      "Step: [4531] total_loss: 2.13082552 d_loss: 1.37805820, g_loss: 0.70095819, ae_loss: 0.05180909\n",
      "Step: [4532] total_loss: 2.13401747 d_loss: 1.38474607, g_loss: 0.70120180, ae_loss: 0.04806950\n",
      "Step: [4533] total_loss: 2.14180899 d_loss: 1.37793803, g_loss: 0.71056962, ae_loss: 0.05330136\n",
      "Step: [4534] total_loss: 2.13986301 d_loss: 1.38478768, g_loss: 0.70406699, ae_loss: 0.05100834\n",
      "Step: [4535] total_loss: 2.14442158 d_loss: 1.38211572, g_loss: 0.70999455, ae_loss: 0.05231139\n",
      "Step: [4536] total_loss: 2.13895559 d_loss: 1.39211369, g_loss: 0.69768250, ae_loss: 0.04915953\n",
      "Step: [4537] total_loss: 2.12667561 d_loss: 1.39631605, g_loss: 0.67698205, ae_loss: 0.05337739\n",
      "Step: [4538] total_loss: 2.11766911 d_loss: 1.38636494, g_loss: 0.68325567, ae_loss: 0.04804835\n",
      "Step: [4539] total_loss: 2.13582087 d_loss: 1.39430451, g_loss: 0.68818736, ae_loss: 0.05332902\n",
      "Step: [4540] total_loss: 2.15042830 d_loss: 1.40793717, g_loss: 0.69263244, ae_loss: 0.04985870\n",
      "Step: [4541] total_loss: 2.12188125 d_loss: 1.37779713, g_loss: 0.69089949, ae_loss: 0.05318459\n",
      "Step: [4542] total_loss: 2.10782313 d_loss: 1.36852312, g_loss: 0.68611193, ae_loss: 0.05318808\n",
      "Step: [4543] total_loss: 2.12502313 d_loss: 1.39279270, g_loss: 0.68532258, ae_loss: 0.04690794\n",
      "Step: [4544] total_loss: 2.13401866 d_loss: 1.38204622, g_loss: 0.69923639, ae_loss: 0.05273603\n",
      "Step: [4545] total_loss: 2.14561749 d_loss: 1.40453064, g_loss: 0.69022077, ae_loss: 0.05086615\n",
      "Step: [4546] total_loss: 2.11975932 d_loss: 1.36472702, g_loss: 0.70303082, ae_loss: 0.05200154\n",
      "Step: [4547] total_loss: 2.12549615 d_loss: 1.38420856, g_loss: 0.68904120, ae_loss: 0.05224637\n",
      "Step: [4548] total_loss: 2.12974930 d_loss: 1.39254332, g_loss: 0.68396711, ae_loss: 0.05323903\n",
      "Step: [4549] total_loss: 2.11594796 d_loss: 1.37330055, g_loss: 0.68948317, ae_loss: 0.05316425\n",
      "Step: [4550] total_loss: 2.12946701 d_loss: 1.37861991, g_loss: 0.69974798, ae_loss: 0.05109907\n",
      "Step: [4551] total_loss: 2.12133646 d_loss: 1.37841415, g_loss: 0.69333059, ae_loss: 0.04959165\n",
      "Step: [4552] total_loss: 2.11958218 d_loss: 1.37560034, g_loss: 0.68995917, ae_loss: 0.05402264\n",
      "Step: [4553] total_loss: 2.14248371 d_loss: 1.39294767, g_loss: 0.69873202, ae_loss: 0.05080410\n",
      "Step: [4554] total_loss: 2.12031937 d_loss: 1.38646698, g_loss: 0.68625873, ae_loss: 0.04759371\n",
      "Step: [4555] total_loss: 2.12183142 d_loss: 1.37993050, g_loss: 0.69110769, ae_loss: 0.05079318\n",
      "Step: [4556] total_loss: 2.11198163 d_loss: 1.37249970, g_loss: 0.68961883, ae_loss: 0.04986310\n",
      "Step: [4557] total_loss: 2.12734795 d_loss: 1.38758659, g_loss: 0.69312042, ae_loss: 0.04664085\n",
      "Step: [4558] total_loss: 2.13035798 d_loss: 1.38090944, g_loss: 0.70114595, ae_loss: 0.04830249\n",
      "Step: [4559] total_loss: 2.10792994 d_loss: 1.36423850, g_loss: 0.69406497, ae_loss: 0.04962646\n",
      "Step: [4560] total_loss: 2.12683821 d_loss: 1.38424373, g_loss: 0.69099236, ae_loss: 0.05160223\n",
      "Step: [4561] total_loss: 2.14311409 d_loss: 1.39310026, g_loss: 0.69924670, ae_loss: 0.05076718\n",
      "Step: [4562] total_loss: 2.13268614 d_loss: 1.40295029, g_loss: 0.68396163, ae_loss: 0.04577429\n",
      "Step: [4563] total_loss: 2.13476539 d_loss: 1.40143311, g_loss: 0.68207741, ae_loss: 0.05125486\n",
      "Step: [4564] total_loss: 2.13358641 d_loss: 1.37139010, g_loss: 0.71038270, ae_loss: 0.05181359\n",
      "Step: [4565] total_loss: 2.13818359 d_loss: 1.39285469, g_loss: 0.69360286, ae_loss: 0.05172595\n",
      "Step: [4566] total_loss: 2.12869430 d_loss: 1.39364409, g_loss: 0.68626666, ae_loss: 0.04878357\n",
      "Step: [4567] total_loss: 2.12117910 d_loss: 1.38024235, g_loss: 0.69379342, ae_loss: 0.04714318\n",
      "Step: [4568] total_loss: 2.12281370 d_loss: 1.37471867, g_loss: 0.69738531, ae_loss: 0.05070974\n",
      "Step: [4569] total_loss: 2.12859154 d_loss: 1.38994026, g_loss: 0.68964589, ae_loss: 0.04900555\n",
      "Step: [4570] total_loss: 2.13651299 d_loss: 1.39715552, g_loss: 0.68903941, ae_loss: 0.05031799\n",
      "Step: [4571] total_loss: 2.13263702 d_loss: 1.39790964, g_loss: 0.68604934, ae_loss: 0.04867787\n",
      "Step: [4572] total_loss: 2.12834716 d_loss: 1.39089108, g_loss: 0.68898863, ae_loss: 0.04846751\n",
      "Step: [4573] total_loss: 2.11948252 d_loss: 1.38277233, g_loss: 0.68518138, ae_loss: 0.05152876\n",
      "Step: [4574] total_loss: 2.12786818 d_loss: 1.38149500, g_loss: 0.69460994, ae_loss: 0.05176317\n",
      "Step: [4575] total_loss: 2.13741255 d_loss: 1.39821196, g_loss: 0.68577647, ae_loss: 0.05342427\n",
      "Step: [4576] total_loss: 2.14349556 d_loss: 1.38895738, g_loss: 0.70247006, ae_loss: 0.05206826\n",
      "Step: [4577] total_loss: 2.11784124 d_loss: 1.37183297, g_loss: 0.69393408, ae_loss: 0.05207404\n",
      "Step: [4578] total_loss: 2.12942934 d_loss: 1.38960075, g_loss: 0.68844724, ae_loss: 0.05138130\n",
      "Step: [4579] total_loss: 2.10669947 d_loss: 1.36558974, g_loss: 0.69644368, ae_loss: 0.04466593\n",
      "Step: [4580] total_loss: 2.11682224 d_loss: 1.37439501, g_loss: 0.69210184, ae_loss: 0.05032521\n",
      "Step: [4581] total_loss: 2.13085008 d_loss: 1.37766993, g_loss: 0.70010930, ae_loss: 0.05307093\n",
      "Step: [4582] total_loss: 2.13392925 d_loss: 1.37117112, g_loss: 0.70744127, ae_loss: 0.05531685\n",
      "Step: [4583] total_loss: 2.12251282 d_loss: 1.39179182, g_loss: 0.67679393, ae_loss: 0.05392717\n",
      "Step: [4584] total_loss: 2.12506485 d_loss: 1.38603878, g_loss: 0.68615180, ae_loss: 0.05287424\n",
      "Step: [4585] total_loss: 2.13178110 d_loss: 1.40024424, g_loss: 0.68305588, ae_loss: 0.04848087\n",
      "Step: [4586] total_loss: 2.13163853 d_loss: 1.40022898, g_loss: 0.68719471, ae_loss: 0.04421498\n",
      "Step: [4587] total_loss: 2.12558270 d_loss: 1.37960362, g_loss: 0.69607663, ae_loss: 0.04990247\n",
      "Step: [4588] total_loss: 2.13957739 d_loss: 1.38310075, g_loss: 0.69959664, ae_loss: 0.05687984\n",
      "Step: [4589] total_loss: 2.13321638 d_loss: 1.37915063, g_loss: 0.69897401, ae_loss: 0.05509159\n",
      "Step: [4590] total_loss: 2.14004993 d_loss: 1.38135982, g_loss: 0.70427835, ae_loss: 0.05441182\n",
      "Step: [4591] total_loss: 2.13876653 d_loss: 1.39802337, g_loss: 0.68947399, ae_loss: 0.05126914\n",
      "Step: [4592] total_loss: 2.14014077 d_loss: 1.39124870, g_loss: 0.69817662, ae_loss: 0.05071546\n",
      "Step: [4593] total_loss: 2.13284111 d_loss: 1.39718711, g_loss: 0.68690491, ae_loss: 0.04874893\n",
      "Step: [4594] total_loss: 2.12352991 d_loss: 1.39259934, g_loss: 0.68162918, ae_loss: 0.04930124\n",
      "Step: [4595] total_loss: 2.13105512 d_loss: 1.38208866, g_loss: 0.69553363, ae_loss: 0.05343286\n",
      "Step: [4596] total_loss: 2.12642479 d_loss: 1.39522433, g_loss: 0.67685056, ae_loss: 0.05434995\n",
      "Step: [4597] total_loss: 2.12170839 d_loss: 1.37505937, g_loss: 0.69410372, ae_loss: 0.05254523\n",
      "Step: [4598] total_loss: 2.14309835 d_loss: 1.41023540, g_loss: 0.68548965, ae_loss: 0.04737336\n",
      "Step: [4599] total_loss: 2.13092160 d_loss: 1.38483083, g_loss: 0.69565374, ae_loss: 0.05043711\n",
      "Step: [4600] total_loss: 2.13981485 d_loss: 1.39138114, g_loss: 0.69719332, ae_loss: 0.05124041\n",
      "Step: [4601] total_loss: 2.14310288 d_loss: 1.38928008, g_loss: 0.69963396, ae_loss: 0.05418881\n",
      "Step: [4602] total_loss: 2.13631129 d_loss: 1.38651705, g_loss: 0.69698673, ae_loss: 0.05280755\n",
      "Step: [4603] total_loss: 2.14704847 d_loss: 1.38429570, g_loss: 0.70832431, ae_loss: 0.05442842\n",
      "Step: [4604] total_loss: 2.12999678 d_loss: 1.38753033, g_loss: 0.69354302, ae_loss: 0.04892344\n",
      "Step: [4605] total_loss: 2.14794183 d_loss: 1.40246177, g_loss: 0.69327867, ae_loss: 0.05220142\n",
      "Step: [4606] total_loss: 2.12871575 d_loss: 1.38302183, g_loss: 0.69422972, ae_loss: 0.05146423\n",
      "Step: [4607] total_loss: 2.12569618 d_loss: 1.39458418, g_loss: 0.67802286, ae_loss: 0.05308909\n",
      "Step: [4608] total_loss: 2.12789583 d_loss: 1.38512671, g_loss: 0.69591385, ae_loss: 0.04685526\n",
      "Step: [4609] total_loss: 2.13661265 d_loss: 1.38948679, g_loss: 0.69443500, ae_loss: 0.05269086\n",
      "Step: [4610] total_loss: 2.11718559 d_loss: 1.37609935, g_loss: 0.69291097, ae_loss: 0.04817537\n",
      "Step: [4611] total_loss: 2.12973642 d_loss: 1.39384067, g_loss: 0.68887568, ae_loss: 0.04702012\n",
      "Step: [4612] total_loss: 2.12275314 d_loss: 1.38517642, g_loss: 0.68583548, ae_loss: 0.05174120\n",
      "Step: [4613] total_loss: 2.13786030 d_loss: 1.39452696, g_loss: 0.69269836, ae_loss: 0.05063489\n",
      "Step: [4614] total_loss: 2.14028406 d_loss: 1.40058994, g_loss: 0.68663049, ae_loss: 0.05306352\n",
      "Step: [4615] total_loss: 2.11457229 d_loss: 1.37464428, g_loss: 0.69105315, ae_loss: 0.04887487\n",
      "Step: [4616] total_loss: 2.12141562 d_loss: 1.37543309, g_loss: 0.69578242, ae_loss: 0.05019999\n",
      "Step: [4617] total_loss: 2.12452459 d_loss: 1.38606238, g_loss: 0.69038439, ae_loss: 0.04807791\n",
      "Step: [4618] total_loss: 2.13240004 d_loss: 1.38671410, g_loss: 0.69461143, ae_loss: 0.05107452\n",
      "Step: [4619] total_loss: 2.11799049 d_loss: 1.38513923, g_loss: 0.68585944, ae_loss: 0.04699194\n",
      "Step: [4620] total_loss: 2.12796307 d_loss: 1.39361000, g_loss: 0.68692517, ae_loss: 0.04742796\n",
      "Step: [4621] total_loss: 2.10651112 d_loss: 1.37444806, g_loss: 0.68594325, ae_loss: 0.04611972\n",
      "Step: [4622] total_loss: 2.11198425 d_loss: 1.36367083, g_loss: 0.69992429, ae_loss: 0.04838903\n",
      "Step: [4623] total_loss: 2.10786867 d_loss: 1.37482727, g_loss: 0.68370271, ae_loss: 0.04933862\n",
      "Step: [4624] total_loss: 2.14217997 d_loss: 1.40017152, g_loss: 0.69277227, ae_loss: 0.04923620\n",
      "Step: [4625] total_loss: 2.12652922 d_loss: 1.39382124, g_loss: 0.68505895, ae_loss: 0.04764901\n",
      "Step: [4626] total_loss: 2.12139964 d_loss: 1.37658930, g_loss: 0.69374841, ae_loss: 0.05106192\n",
      "Step: [4627] total_loss: 2.13830376 d_loss: 1.39052260, g_loss: 0.69578016, ae_loss: 0.05200111\n",
      "Step: [4628] total_loss: 2.13359904 d_loss: 1.37806988, g_loss: 0.70903420, ae_loss: 0.04649497\n",
      "Step: [4629] total_loss: 2.12242270 d_loss: 1.37174749, g_loss: 0.70129144, ae_loss: 0.04938368\n",
      "Step: [4630] total_loss: 2.14235210 d_loss: 1.39342201, g_loss: 0.69846898, ae_loss: 0.05046116\n",
      "Step: [4631] total_loss: 2.13923812 d_loss: 1.40195811, g_loss: 0.68903458, ae_loss: 0.04824540\n",
      "Step: [4632] total_loss: 2.13459587 d_loss: 1.37754071, g_loss: 0.70665860, ae_loss: 0.05039645\n",
      "Step: [4633] total_loss: 2.11645865 d_loss: 1.36766315, g_loss: 0.69524181, ae_loss: 0.05355375\n",
      "Step: [4634] total_loss: 2.12869787 d_loss: 1.38855934, g_loss: 0.68573129, ae_loss: 0.05440723\n",
      "Step: [4635] total_loss: 2.13909030 d_loss: 1.38375926, g_loss: 0.70191461, ae_loss: 0.05341648\n",
      "Step: [4636] total_loss: 2.10675025 d_loss: 1.37710631, g_loss: 0.68534631, ae_loss: 0.04429774\n",
      "Step: [4637] total_loss: 2.10399866 d_loss: 1.37457192, g_loss: 0.68191803, ae_loss: 0.04750867\n",
      "Step: [4638] total_loss: 2.11531138 d_loss: 1.38609028, g_loss: 0.68313372, ae_loss: 0.04608742\n",
      "Step: [4639] total_loss: 2.11836886 d_loss: 1.38655663, g_loss: 0.68539202, ae_loss: 0.04642022\n",
      "Step: [4640] total_loss: 2.12188148 d_loss: 1.38166583, g_loss: 0.69003868, ae_loss: 0.05017686\n",
      "Step: [4641] total_loss: 2.13312626 d_loss: 1.38618708, g_loss: 0.69602549, ae_loss: 0.05091385\n",
      "Step: [4642] total_loss: 2.13400650 d_loss: 1.38340497, g_loss: 0.69994366, ae_loss: 0.05065772\n",
      "Step: [4643] total_loss: 2.13581276 d_loss: 1.39483333, g_loss: 0.69206023, ae_loss: 0.04891935\n",
      "Step: [4644] total_loss: 2.12353826 d_loss: 1.38904572, g_loss: 0.68525106, ae_loss: 0.04924143\n",
      "Step: [4645] total_loss: 2.12838984 d_loss: 1.38948584, g_loss: 0.68903917, ae_loss: 0.04986485\n",
      "Step: [4646] total_loss: 2.12537789 d_loss: 1.39048386, g_loss: 0.68355423, ae_loss: 0.05133992\n",
      "Step: [4647] total_loss: 2.11934662 d_loss: 1.37446880, g_loss: 0.69191921, ae_loss: 0.05295879\n",
      "Step: [4648] total_loss: 2.13124418 d_loss: 1.38867688, g_loss: 0.69778258, ae_loss: 0.04478484\n",
      "Step: [4649] total_loss: 2.12939382 d_loss: 1.38430119, g_loss: 0.69443721, ae_loss: 0.05065543\n",
      "Step: [4650] total_loss: 2.12249780 d_loss: 1.38333273, g_loss: 0.68979275, ae_loss: 0.04937232\n",
      "Step: [4651] total_loss: 2.10323668 d_loss: 1.36319995, g_loss: 0.68908024, ae_loss: 0.05095665\n",
      "Step: [4652] total_loss: 2.14350462 d_loss: 1.39562488, g_loss: 0.69930136, ae_loss: 0.04857853\n",
      "Step: [4653] total_loss: 2.12602615 d_loss: 1.36422110, g_loss: 0.70768344, ae_loss: 0.05412172\n",
      "Step: [4654] total_loss: 2.12401485 d_loss: 1.38185883, g_loss: 0.69099623, ae_loss: 0.05115981\n",
      "Step: [4655] total_loss: 2.12140226 d_loss: 1.37929487, g_loss: 0.69052851, ae_loss: 0.05157889\n",
      "Step: [4656] total_loss: 2.11630845 d_loss: 1.37537420, g_loss: 0.69088680, ae_loss: 0.05004757\n",
      "Step: [4657] total_loss: 2.12667561 d_loss: 1.39051878, g_loss: 0.69086158, ae_loss: 0.04529540\n",
      "Step: [4658] total_loss: 2.12869883 d_loss: 1.40044224, g_loss: 0.67541981, ae_loss: 0.05283661\n",
      "Step: [4659] total_loss: 2.13592529 d_loss: 1.39762855, g_loss: 0.68934953, ae_loss: 0.04894732\n",
      "Step: [4660] total_loss: 2.12909889 d_loss: 1.39081097, g_loss: 0.68430704, ae_loss: 0.05398094\n",
      "Step: [4661] total_loss: 2.13850212 d_loss: 1.39032769, g_loss: 0.69755775, ae_loss: 0.05061675\n",
      "Step: [4662] total_loss: 2.12904263 d_loss: 1.38334036, g_loss: 0.70112813, ae_loss: 0.04457428\n",
      "Step: [4663] total_loss: 2.12239909 d_loss: 1.38839626, g_loss: 0.68656486, ae_loss: 0.04743794\n",
      "Step: [4664] total_loss: 2.13189101 d_loss: 1.39075887, g_loss: 0.69073230, ae_loss: 0.05039994\n",
      "Step: [4665] total_loss: 2.12961817 d_loss: 1.38648117, g_loss: 0.68970877, ae_loss: 0.05342818\n",
      "Step: [4666] total_loss: 2.12789321 d_loss: 1.37540436, g_loss: 0.70849162, ae_loss: 0.04399725\n",
      "Step: [4667] total_loss: 2.13256216 d_loss: 1.39116311, g_loss: 0.68789846, ae_loss: 0.05350053\n",
      "Step: [4668] total_loss: 2.11211228 d_loss: 1.37527561, g_loss: 0.68563026, ae_loss: 0.05120631\n",
      "Step: [4669] total_loss: 2.12062359 d_loss: 1.38112557, g_loss: 0.69210804, ae_loss: 0.04738991\n",
      "Step: [4670] total_loss: 2.11122656 d_loss: 1.38748002, g_loss: 0.67696357, ae_loss: 0.04678310\n",
      "Step: [4671] total_loss: 2.13471127 d_loss: 1.38804853, g_loss: 0.69399959, ae_loss: 0.05266305\n",
      "Step: [4672] total_loss: 2.13968039 d_loss: 1.38815856, g_loss: 0.70064247, ae_loss: 0.05087949\n",
      "Step: [4673] total_loss: 2.12909508 d_loss: 1.37756133, g_loss: 0.69867694, ae_loss: 0.05285674\n",
      "Step: [4674] total_loss: 2.13741398 d_loss: 1.39280927, g_loss: 0.69898355, ae_loss: 0.04562108\n",
      "Step: [4675] total_loss: 2.12438893 d_loss: 1.38638496, g_loss: 0.68970698, ae_loss: 0.04829688\n",
      "Step: [4676] total_loss: 2.12161994 d_loss: 1.37869215, g_loss: 0.69001979, ae_loss: 0.05290806\n",
      "Step: [4677] total_loss: 2.13330030 d_loss: 1.38531852, g_loss: 0.69330108, ae_loss: 0.05468075\n",
      "Step: [4678] total_loss: 2.11090732 d_loss: 1.37261093, g_loss: 0.68968022, ae_loss: 0.04861618\n",
      "Step: [4679] total_loss: 2.10704494 d_loss: 1.37134719, g_loss: 0.68403769, ae_loss: 0.05166003\n",
      "Step: [4680] total_loss: 2.10793495 d_loss: 1.36363292, g_loss: 0.70033693, ae_loss: 0.04396524\n",
      "Step: [4681] total_loss: 2.10760427 d_loss: 1.37996721, g_loss: 0.67746115, ae_loss: 0.05017591\n",
      "Step: [4682] total_loss: 2.12931848 d_loss: 1.39175892, g_loss: 0.68953258, ae_loss: 0.04802694\n",
      "Step: [4683] total_loss: 2.13573766 d_loss: 1.39645183, g_loss: 0.68887365, ae_loss: 0.05041216\n",
      "Step: [4684] total_loss: 2.13547301 d_loss: 1.39026761, g_loss: 0.69121110, ae_loss: 0.05399425\n",
      "Step: [4685] total_loss: 2.14490938 d_loss: 1.38298810, g_loss: 0.70798451, ae_loss: 0.05393689\n",
      "Step: [4686] total_loss: 2.13238621 d_loss: 1.38542140, g_loss: 0.69849455, ae_loss: 0.04847025\n",
      "Step: [4687] total_loss: 2.11622190 d_loss: 1.36296535, g_loss: 0.70028901, ae_loss: 0.05296747\n",
      "Step: [4688] total_loss: 2.11503839 d_loss: 1.37721872, g_loss: 0.68742418, ae_loss: 0.05039537\n",
      "Step: [4689] total_loss: 2.13278675 d_loss: 1.37789965, g_loss: 0.70366943, ae_loss: 0.05121759\n",
      "Step: [4690] total_loss: 2.11050296 d_loss: 1.37293553, g_loss: 0.69042045, ae_loss: 0.04714699\n",
      "Step: [4691] total_loss: 2.12749386 d_loss: 1.40028870, g_loss: 0.67662573, ae_loss: 0.05057937\n",
      "Step: [4692] total_loss: 2.13481736 d_loss: 1.40344620, g_loss: 0.68315554, ae_loss: 0.04821561\n",
      "Step: [4693] total_loss: 2.11242819 d_loss: 1.36927533, g_loss: 0.69111472, ae_loss: 0.05203823\n",
      "Step: [4694] total_loss: 2.13556337 d_loss: 1.39176285, g_loss: 0.69484270, ae_loss: 0.04895769\n",
      "Step: [4695] total_loss: 2.13043165 d_loss: 1.37520897, g_loss: 0.70292479, ae_loss: 0.05229782\n",
      "Step: [4696] total_loss: 2.12511826 d_loss: 1.37249207, g_loss: 0.69936490, ae_loss: 0.05326128\n",
      "Step: [4697] total_loss: 2.14257860 d_loss: 1.38936520, g_loss: 0.69904017, ae_loss: 0.05417323\n",
      "Step: [4698] total_loss: 2.13664412 d_loss: 1.40948319, g_loss: 0.67453736, ae_loss: 0.05262359\n",
      "Step: [4699] total_loss: 2.12188745 d_loss: 1.38115931, g_loss: 0.68839896, ae_loss: 0.05232918\n",
      "Step: [4700] total_loss: 2.13351965 d_loss: 1.38055849, g_loss: 0.69766849, ae_loss: 0.05529272\n",
      "Step: [4701] total_loss: 2.11457276 d_loss: 1.37803018, g_loss: 0.69035947, ae_loss: 0.04618313\n",
      "Step: [4702] total_loss: 2.13726187 d_loss: 1.40240359, g_loss: 0.68405211, ae_loss: 0.05080603\n",
      "Step: [4703] total_loss: 2.13316703 d_loss: 1.39499474, g_loss: 0.68869799, ae_loss: 0.04947434\n",
      "Step: [4704] total_loss: 2.13387203 d_loss: 1.38174307, g_loss: 0.69621170, ae_loss: 0.05591733\n",
      "Step: [4705] total_loss: 2.14273930 d_loss: 1.38875318, g_loss: 0.70391917, ae_loss: 0.05006687\n",
      "Step: [4706] total_loss: 2.13488531 d_loss: 1.40245795, g_loss: 0.68486422, ae_loss: 0.04756307\n",
      "Step: [4707] total_loss: 2.12262774 d_loss: 1.38004005, g_loss: 0.69103575, ae_loss: 0.05155202\n",
      "Step: [4708] total_loss: 2.13607502 d_loss: 1.39653611, g_loss: 0.69171894, ae_loss: 0.04781980\n",
      "Step: [4709] total_loss: 2.14167070 d_loss: 1.40340912, g_loss: 0.68514305, ae_loss: 0.05311858\n",
      "Step: [4710] total_loss: 2.12032795 d_loss: 1.38514173, g_loss: 0.68653238, ae_loss: 0.04865376\n",
      "Step: [4711] total_loss: 2.12580228 d_loss: 1.37510395, g_loss: 0.70079863, ae_loss: 0.04989971\n",
      "Step: [4712] total_loss: 2.13373685 d_loss: 1.39030409, g_loss: 0.69515771, ae_loss: 0.04827498\n",
      "Step: [4713] total_loss: 2.11361313 d_loss: 1.37261248, g_loss: 0.69224679, ae_loss: 0.04875389\n",
      "Step: [4714] total_loss: 2.12817860 d_loss: 1.39188313, g_loss: 0.68542308, ae_loss: 0.05087229\n",
      "Step: [4715] total_loss: 2.12754679 d_loss: 1.37837958, g_loss: 0.69959188, ae_loss: 0.04957542\n",
      "Step: [4716] total_loss: 2.12253618 d_loss: 1.38224709, g_loss: 0.69214058, ae_loss: 0.04814853\n",
      "Step: [4717] total_loss: 2.10859203 d_loss: 1.36381495, g_loss: 0.69931465, ae_loss: 0.04546245\n",
      "Step: [4718] total_loss: 2.12081265 d_loss: 1.38370514, g_loss: 0.68981588, ae_loss: 0.04729165\n",
      "Step: [4719] total_loss: 2.11949253 d_loss: 1.38114846, g_loss: 0.68633485, ae_loss: 0.05200927\n",
      "Step: [4720] total_loss: 2.12551022 d_loss: 1.36280060, g_loss: 0.70753157, ae_loss: 0.05517813\n",
      "Step: [4721] total_loss: 2.11753631 d_loss: 1.37464917, g_loss: 0.69043076, ae_loss: 0.05245633\n",
      "Step: [4722] total_loss: 2.11539793 d_loss: 1.36469316, g_loss: 0.70276129, ae_loss: 0.04794357\n",
      "Step: [4723] total_loss: 2.13286686 d_loss: 1.39887643, g_loss: 0.68008095, ae_loss: 0.05390944\n",
      "Step: [4724] total_loss: 2.12141109 d_loss: 1.38738859, g_loss: 0.68467534, ae_loss: 0.04934720\n",
      "Step: [4725] total_loss: 2.12591171 d_loss: 1.39010179, g_loss: 0.68883407, ae_loss: 0.04697570\n",
      "Step: [4726] total_loss: 2.12544584 d_loss: 1.39836979, g_loss: 0.67898166, ae_loss: 0.04809437\n",
      "Step: [4727] total_loss: 2.13262439 d_loss: 1.38732040, g_loss: 0.69485474, ae_loss: 0.05044923\n",
      "Step: [4728] total_loss: 2.12117672 d_loss: 1.38050830, g_loss: 0.69245207, ae_loss: 0.04821634\n",
      "Step: [4729] total_loss: 2.13204932 d_loss: 1.39192259, g_loss: 0.69090432, ae_loss: 0.04922237\n",
      "Step: [4730] total_loss: 2.12250900 d_loss: 1.36993289, g_loss: 0.70778990, ae_loss: 0.04478618\n",
      "Step: [4731] total_loss: 2.12103844 d_loss: 1.38699424, g_loss: 0.68186909, ae_loss: 0.05217502\n",
      "Step: [4732] total_loss: 2.11626887 d_loss: 1.37140083, g_loss: 0.69628310, ae_loss: 0.04858498\n",
      "Step: [4733] total_loss: 2.11967659 d_loss: 1.38173795, g_loss: 0.68625098, ae_loss: 0.05168774\n",
      "Step: [4734] total_loss: 2.12446856 d_loss: 1.39029408, g_loss: 0.68657976, ae_loss: 0.04759464\n",
      "Step: [4735] total_loss: 2.12584496 d_loss: 1.39036608, g_loss: 0.68230605, ae_loss: 0.05317282\n",
      "Step: [4736] total_loss: 2.12355971 d_loss: 1.38914204, g_loss: 0.68210185, ae_loss: 0.05231580\n",
      "Step: [4737] total_loss: 2.13209867 d_loss: 1.38965929, g_loss: 0.69419599, ae_loss: 0.04824338\n",
      "Step: [4738] total_loss: 2.11963224 d_loss: 1.37047577, g_loss: 0.70334876, ae_loss: 0.04580759\n",
      "Step: [4739] total_loss: 2.12283039 d_loss: 1.37790036, g_loss: 0.69609928, ae_loss: 0.04883087\n",
      "Step: [4740] total_loss: 2.13747716 d_loss: 1.38266253, g_loss: 0.70557714, ae_loss: 0.04923743\n",
      "Step: [4741] total_loss: 2.12489748 d_loss: 1.38150430, g_loss: 0.69584525, ae_loss: 0.04754796\n",
      "Step: [4742] total_loss: 2.14778805 d_loss: 1.40168428, g_loss: 0.69871485, ae_loss: 0.04738906\n",
      "Step: [4743] total_loss: 2.11197782 d_loss: 1.37668109, g_loss: 0.68480700, ae_loss: 0.05048975\n",
      "Step: [4744] total_loss: 2.12328458 d_loss: 1.38331103, g_loss: 0.69345433, ae_loss: 0.04651924\n",
      "Step: [4745] total_loss: 2.12244987 d_loss: 1.36751986, g_loss: 0.70192188, ae_loss: 0.05300817\n",
      "Step: [4746] total_loss: 2.12119508 d_loss: 1.38287497, g_loss: 0.68722612, ae_loss: 0.05109394\n",
      "Step: [4747] total_loss: 2.13388348 d_loss: 1.39255309, g_loss: 0.68948126, ae_loss: 0.05184900\n",
      "Step: [4748] total_loss: 2.13265228 d_loss: 1.38957953, g_loss: 0.68820906, ae_loss: 0.05486377\n",
      "Step: [4749] total_loss: 2.12608647 d_loss: 1.38394022, g_loss: 0.69062698, ae_loss: 0.05151927\n",
      "Step: [4750] total_loss: 2.12670135 d_loss: 1.38195324, g_loss: 0.69620860, ae_loss: 0.04853944\n",
      "Step: [4751] total_loss: 2.13740873 d_loss: 1.39485240, g_loss: 0.69251406, ae_loss: 0.05004215\n",
      "Step: [4752] total_loss: 2.13499999 d_loss: 1.38041472, g_loss: 0.70260179, ae_loss: 0.05198353\n",
      "Step: [4753] total_loss: 2.12802696 d_loss: 1.39726973, g_loss: 0.67749023, ae_loss: 0.05326694\n",
      "Step: [4754] total_loss: 2.14437294 d_loss: 1.39085984, g_loss: 0.69872093, ae_loss: 0.05479234\n",
      "Step: [4755] total_loss: 2.13391924 d_loss: 1.38750470, g_loss: 0.69780779, ae_loss: 0.04860687\n",
      "Step: [4756] total_loss: 2.11367941 d_loss: 1.38456917, g_loss: 0.67714059, ae_loss: 0.05196959\n",
      "Step: [4757] total_loss: 2.12581158 d_loss: 1.38319898, g_loss: 0.69221783, ae_loss: 0.05039468\n",
      "Step: [4758] total_loss: 2.13372350 d_loss: 1.37762427, g_loss: 0.70656860, ae_loss: 0.04953067\n",
      "Step: [4759] total_loss: 2.10609078 d_loss: 1.37749445, g_loss: 0.67973435, ae_loss: 0.04886201\n",
      "Step: [4760] total_loss: 2.09695530 d_loss: 1.36228323, g_loss: 0.68291396, ae_loss: 0.05175815\n",
      "Step: [4761] total_loss: 2.10924387 d_loss: 1.38802993, g_loss: 0.67018771, ae_loss: 0.05102636\n",
      "Step: [4762] total_loss: 2.13802171 d_loss: 1.39451396, g_loss: 0.69313037, ae_loss: 0.05037740\n",
      "Step: [4763] total_loss: 2.14450645 d_loss: 1.41231883, g_loss: 0.68275619, ae_loss: 0.04943152\n",
      "Step: [4764] total_loss: 2.14117479 d_loss: 1.38758624, g_loss: 0.70640540, ae_loss: 0.04718325\n",
      "Step: [4765] total_loss: 2.11842608 d_loss: 1.37068021, g_loss: 0.70087796, ae_loss: 0.04686782\n",
      "Step: [4766] total_loss: 2.12795472 d_loss: 1.38879776, g_loss: 0.68859786, ae_loss: 0.05055920\n",
      "Step: [4767] total_loss: 2.14335299 d_loss: 1.39890218, g_loss: 0.69283605, ae_loss: 0.05161472\n",
      "Step: [4768] total_loss: 2.14677024 d_loss: 1.40696692, g_loss: 0.68804181, ae_loss: 0.05176155\n",
      "Step: [4769] total_loss: 2.10727096 d_loss: 1.37690115, g_loss: 0.68353033, ae_loss: 0.04683951\n",
      "Step: [4770] total_loss: 2.12419462 d_loss: 1.38386011, g_loss: 0.68897045, ae_loss: 0.05136411\n",
      "Step: [4771] total_loss: 2.11139131 d_loss: 1.37441325, g_loss: 0.68869895, ae_loss: 0.04827904\n",
      "Step: [4772] total_loss: 2.10440207 d_loss: 1.37888491, g_loss: 0.67750514, ae_loss: 0.04801205\n",
      "Step: [4773] total_loss: 2.11194110 d_loss: 1.38314009, g_loss: 0.68132657, ae_loss: 0.04747450\n",
      "Step: [4774] total_loss: 2.10650110 d_loss: 1.36623609, g_loss: 0.69024831, ae_loss: 0.05001682\n",
      "Step: [4775] total_loss: 2.09855127 d_loss: 1.37004519, g_loss: 0.67906868, ae_loss: 0.04943750\n",
      "Step: [4776] total_loss: 2.13326073 d_loss: 1.39870274, g_loss: 0.68404400, ae_loss: 0.05051399\n",
      "Step: [4777] total_loss: 2.12980604 d_loss: 1.39617348, g_loss: 0.68130088, ae_loss: 0.05233181\n",
      "Step: [4778] total_loss: 2.12427473 d_loss: 1.38345218, g_loss: 0.69123471, ae_loss: 0.04958769\n",
      "Step: [4779] total_loss: 2.11420822 d_loss: 1.38530660, g_loss: 0.67927706, ae_loss: 0.04962442\n",
      "Step: [4780] total_loss: 2.12395334 d_loss: 1.37330842, g_loss: 0.69700611, ae_loss: 0.05363868\n",
      "Step: [4781] total_loss: 2.12116361 d_loss: 1.37703812, g_loss: 0.69313222, ae_loss: 0.05099320\n",
      "Step: [4782] total_loss: 2.12622571 d_loss: 1.39225411, g_loss: 0.68356580, ae_loss: 0.05040584\n",
      "Step: [4783] total_loss: 2.11156440 d_loss: 1.38405597, g_loss: 0.67977601, ae_loss: 0.04773233\n",
      "Step: [4784] total_loss: 2.13921833 d_loss: 1.39971089, g_loss: 0.69072104, ae_loss: 0.04878657\n",
      "Step: [4785] total_loss: 2.12051487 d_loss: 1.37858379, g_loss: 0.69136107, ae_loss: 0.05056985\n",
      "Step: [4786] total_loss: 2.12424278 d_loss: 1.38282394, g_loss: 0.69101828, ae_loss: 0.05040065\n",
      "Step: [4787] total_loss: 2.12708044 d_loss: 1.38055778, g_loss: 0.69887078, ae_loss: 0.04765178\n",
      "Step: [4788] total_loss: 2.12910843 d_loss: 1.37717366, g_loss: 0.69906408, ae_loss: 0.05287072\n",
      "Step: [4789] total_loss: 2.12691450 d_loss: 1.37878275, g_loss: 0.69799948, ae_loss: 0.05013224\n",
      "Step: [4790] total_loss: 2.12157393 d_loss: 1.37570786, g_loss: 0.69687212, ae_loss: 0.04899382\n",
      "Step: [4791] total_loss: 2.11455679 d_loss: 1.35998201, g_loss: 0.70343888, ae_loss: 0.05113585\n",
      "Step: [4792] total_loss: 2.12649274 d_loss: 1.36985850, g_loss: 0.70553863, ae_loss: 0.05109559\n",
      "Step: [4793] total_loss: 2.14058900 d_loss: 1.38880694, g_loss: 0.70375967, ae_loss: 0.04802241\n",
      "Step: [4794] total_loss: 2.13723183 d_loss: 1.40035701, g_loss: 0.68513668, ae_loss: 0.05173820\n",
      "Step: [4795] total_loss: 2.13516140 d_loss: 1.38562047, g_loss: 0.69554007, ae_loss: 0.05400081\n",
      "Step: [4796] total_loss: 2.12848091 d_loss: 1.39617920, g_loss: 0.68239510, ae_loss: 0.04990677\n",
      "Step: [4797] total_loss: 2.12522268 d_loss: 1.38075173, g_loss: 0.69644701, ae_loss: 0.04802379\n",
      "Step: [4798] total_loss: 2.13650942 d_loss: 1.38554871, g_loss: 0.70279467, ae_loss: 0.04816617\n",
      "Step: [4799] total_loss: 2.12859178 d_loss: 1.39356387, g_loss: 0.68308705, ae_loss: 0.05194092\n",
      "Step: [4800] total_loss: 2.13044000 d_loss: 1.38934541, g_loss: 0.68316269, ae_loss: 0.05793190\n",
      "Step: [4801] total_loss: 2.12623477 d_loss: 1.38240004, g_loss: 0.69873869, ae_loss: 0.04509604\n",
      "Step: [4802] total_loss: 2.12496114 d_loss: 1.37141871, g_loss: 0.70233870, ae_loss: 0.05120369\n",
      "Step: [4803] total_loss: 2.12218237 d_loss: 1.38256407, g_loss: 0.69242704, ae_loss: 0.04719132\n",
      "Step: [4804] total_loss: 2.12288594 d_loss: 1.39242923, g_loss: 0.67698157, ae_loss: 0.05347512\n",
      "Step: [4805] total_loss: 2.12155437 d_loss: 1.37770033, g_loss: 0.69574428, ae_loss: 0.04810963\n",
      "Step: [4806] total_loss: 2.12609720 d_loss: 1.38166046, g_loss: 0.69276154, ae_loss: 0.05167513\n",
      "Step: [4807] total_loss: 2.10969305 d_loss: 1.36906457, g_loss: 0.69071329, ae_loss: 0.04991530\n",
      "Step: [4808] total_loss: 2.11784291 d_loss: 1.37245274, g_loss: 0.69729692, ae_loss: 0.04809333\n",
      "Step: [4809] total_loss: 2.10941315 d_loss: 1.36764145, g_loss: 0.69690788, ae_loss: 0.04486391\n",
      "Step: [4810] total_loss: 2.12770081 d_loss: 1.38051653, g_loss: 0.69789082, ae_loss: 0.04929344\n",
      "Step: [4811] total_loss: 2.10615587 d_loss: 1.35839581, g_loss: 0.69450194, ae_loss: 0.05325812\n",
      "Step: [4812] total_loss: 2.11307931 d_loss: 1.36598587, g_loss: 0.69658244, ae_loss: 0.05051105\n",
      "Step: [4813] total_loss: 2.12338805 d_loss: 1.38499689, g_loss: 0.68920773, ae_loss: 0.04918352\n",
      "Step: [4814] total_loss: 2.10941076 d_loss: 1.35580289, g_loss: 0.70040703, ae_loss: 0.05320067\n",
      "Step: [4815] total_loss: 2.11591482 d_loss: 1.37625623, g_loss: 0.69178420, ae_loss: 0.04787444\n",
      "Step: [4816] total_loss: 2.13271976 d_loss: 1.40290117, g_loss: 0.67954701, ae_loss: 0.05027156\n",
      "Step: [4817] total_loss: 2.11443949 d_loss: 1.37947500, g_loss: 0.68698394, ae_loss: 0.04798046\n",
      "Step: [4818] total_loss: 2.12576270 d_loss: 1.39055014, g_loss: 0.68595588, ae_loss: 0.04925672\n",
      "Step: [4819] total_loss: 2.15160322 d_loss: 1.40743327, g_loss: 0.69543624, ae_loss: 0.04873362\n",
      "Step: [4820] total_loss: 2.12688398 d_loss: 1.38002741, g_loss: 0.69552666, ae_loss: 0.05132984\n",
      "Step: [4821] total_loss: 2.15717220 d_loss: 1.41146481, g_loss: 0.69038272, ae_loss: 0.05532475\n",
      "Step: [4822] total_loss: 2.13429022 d_loss: 1.39375627, g_loss: 0.68954003, ae_loss: 0.05099388\n",
      "Step: [4823] total_loss: 2.13030457 d_loss: 1.39534760, g_loss: 0.68899196, ae_loss: 0.04596506\n",
      "Step: [4824] total_loss: 2.11280465 d_loss: 1.37250721, g_loss: 0.68763238, ae_loss: 0.05266509\n",
      "Step: [4825] total_loss: 2.10615110 d_loss: 1.37036920, g_loss: 0.68772596, ae_loss: 0.04805592\n",
      "Step: [4826] total_loss: 2.11480474 d_loss: 1.38022852, g_loss: 0.68511802, ae_loss: 0.04945824\n",
      "Step: [4827] total_loss: 2.15336823 d_loss: 1.41114616, g_loss: 0.69194168, ae_loss: 0.05028033\n",
      "Step: [4828] total_loss: 2.11407351 d_loss: 1.38071096, g_loss: 0.68165541, ae_loss: 0.05170715\n",
      "Step: [4829] total_loss: 2.13581848 d_loss: 1.38746548, g_loss: 0.69575489, ae_loss: 0.05259797\n",
      "Step: [4830] total_loss: 2.13158512 d_loss: 1.39394236, g_loss: 0.68484712, ae_loss: 0.05279573\n",
      "Step: [4831] total_loss: 2.12292314 d_loss: 1.37085259, g_loss: 0.70111901, ae_loss: 0.05095154\n",
      "Step: [4832] total_loss: 2.12725067 d_loss: 1.38408208, g_loss: 0.69465554, ae_loss: 0.04851305\n",
      "Step: [4833] total_loss: 2.12423897 d_loss: 1.38204587, g_loss: 0.69416946, ae_loss: 0.04802373\n",
      "Step: [4834] total_loss: 2.12711191 d_loss: 1.39105272, g_loss: 0.68478584, ae_loss: 0.05127332\n",
      "Step: [4835] total_loss: 2.11657643 d_loss: 1.36991692, g_loss: 0.69728148, ae_loss: 0.04937806\n",
      "Step: [4836] total_loss: 2.13971591 d_loss: 1.39677882, g_loss: 0.69398504, ae_loss: 0.04895206\n",
      "Step: [4837] total_loss: 2.14808559 d_loss: 1.39110684, g_loss: 0.70546532, ae_loss: 0.05151342\n",
      "Step: [4838] total_loss: 2.13332272 d_loss: 1.38999116, g_loss: 0.68849993, ae_loss: 0.05483152\n",
      "Step: [4839] total_loss: 2.13045168 d_loss: 1.37654662, g_loss: 0.70197713, ae_loss: 0.05192775\n",
      "Step: [4840] total_loss: 2.12433434 d_loss: 1.36545992, g_loss: 0.71033585, ae_loss: 0.04853858\n",
      "Step: [4841] total_loss: 2.12785864 d_loss: 1.38360798, g_loss: 0.69277585, ae_loss: 0.05147476\n",
      "Step: [4842] total_loss: 2.14922333 d_loss: 1.40227997, g_loss: 0.70233953, ae_loss: 0.04460396\n",
      "Step: [4843] total_loss: 2.12815332 d_loss: 1.39181709, g_loss: 0.68263996, ae_loss: 0.05369645\n",
      "Step: [4844] total_loss: 2.13827181 d_loss: 1.37297833, g_loss: 0.71188843, ae_loss: 0.05340511\n",
      "Step: [4845] total_loss: 2.12483382 d_loss: 1.38591778, g_loss: 0.68869460, ae_loss: 0.05022148\n",
      "Step: [4846] total_loss: 2.11870813 d_loss: 1.38375580, g_loss: 0.68690640, ae_loss: 0.04804605\n",
      "Step: [4847] total_loss: 2.12916851 d_loss: 1.38889813, g_loss: 0.68700969, ae_loss: 0.05326069\n",
      "Step: [4848] total_loss: 2.12350893 d_loss: 1.38081324, g_loss: 0.69464558, ae_loss: 0.04805018\n",
      "Step: [4849] total_loss: 2.11520171 d_loss: 1.36813712, g_loss: 0.69717473, ae_loss: 0.04988978\n",
      "Step: [4850] total_loss: 2.11103415 d_loss: 1.36658704, g_loss: 0.69302630, ae_loss: 0.05142075\n",
      "Step: [4851] total_loss: 2.12254810 d_loss: 1.38551545, g_loss: 0.68643105, ae_loss: 0.05060161\n",
      "Step: [4852] total_loss: 2.11835122 d_loss: 1.37656391, g_loss: 0.69169748, ae_loss: 0.05008988\n",
      "Step: [4853] total_loss: 2.11996698 d_loss: 1.38351727, g_loss: 0.68960273, ae_loss: 0.04684690\n",
      "Step: [4854] total_loss: 2.11880016 d_loss: 1.38001156, g_loss: 0.68526542, ae_loss: 0.05352332\n",
      "Step: [4855] total_loss: 2.12097025 d_loss: 1.37300444, g_loss: 0.69957376, ae_loss: 0.04839190\n",
      "Step: [4856] total_loss: 2.14379311 d_loss: 1.37836874, g_loss: 0.71573496, ae_loss: 0.04968956\n",
      "Step: [4857] total_loss: 2.14673424 d_loss: 1.39771402, g_loss: 0.69457525, ae_loss: 0.05444487\n",
      "Step: [4858] total_loss: 2.11834502 d_loss: 1.36202550, g_loss: 0.70661622, ae_loss: 0.04970323\n",
      "Step: [4859] total_loss: 2.12332106 d_loss: 1.37197268, g_loss: 0.70153481, ae_loss: 0.04981354\n",
      "Step: [4860] total_loss: 2.13500333 d_loss: 1.37780154, g_loss: 0.70484179, ae_loss: 0.05235999\n",
      "Step: [4861] total_loss: 2.13101578 d_loss: 1.39035344, g_loss: 0.68921793, ae_loss: 0.05144456\n",
      "Step: [4862] total_loss: 2.12543845 d_loss: 1.39268994, g_loss: 0.68174654, ae_loss: 0.05100206\n",
      "Step: [4863] total_loss: 2.11175585 d_loss: 1.37052536, g_loss: 0.68935859, ae_loss: 0.05187205\n",
      "Step: [4864] total_loss: 2.11051154 d_loss: 1.37076163, g_loss: 0.68973887, ae_loss: 0.05001102\n",
      "Step: [4865] total_loss: 2.12634826 d_loss: 1.38999534, g_loss: 0.68623501, ae_loss: 0.05011790\n",
      "Step: [4866] total_loss: 2.12491584 d_loss: 1.37665105, g_loss: 0.69476455, ae_loss: 0.05350014\n",
      "Step: [4867] total_loss: 2.14306593 d_loss: 1.37957335, g_loss: 0.71189845, ae_loss: 0.05159426\n",
      "Step: [4868] total_loss: 2.14129281 d_loss: 1.39036095, g_loss: 0.69849229, ae_loss: 0.05243953\n",
      "Step: [4869] total_loss: 2.13752460 d_loss: 1.38876247, g_loss: 0.69854510, ae_loss: 0.05021710\n",
      "Step: [4870] total_loss: 2.13832283 d_loss: 1.38690007, g_loss: 0.69954515, ae_loss: 0.05187775\n",
      "Step: [4871] total_loss: 2.13898969 d_loss: 1.40058541, g_loss: 0.68565971, ae_loss: 0.05274468\n",
      "Step: [4872] total_loss: 2.11738706 d_loss: 1.37306023, g_loss: 0.69597632, ae_loss: 0.04835060\n",
      "Step: [4873] total_loss: 2.12434387 d_loss: 1.38516760, g_loss: 0.68843037, ae_loss: 0.05074580\n",
      "Step: [4874] total_loss: 2.13474107 d_loss: 1.38513386, g_loss: 0.69990712, ae_loss: 0.04970006\n",
      "Step: [4875] total_loss: 2.10866165 d_loss: 1.36827874, g_loss: 0.68963051, ae_loss: 0.05075245\n",
      "Step: [4876] total_loss: 2.12321138 d_loss: 1.39817858, g_loss: 0.66888249, ae_loss: 0.05615025\n",
      "Step: [4877] total_loss: 2.12674332 d_loss: 1.36915374, g_loss: 0.70364881, ae_loss: 0.05394090\n",
      "Step: [4878] total_loss: 2.13012838 d_loss: 1.38935232, g_loss: 0.69026327, ae_loss: 0.05051271\n",
      "Step: [4879] total_loss: 2.12488937 d_loss: 1.38402855, g_loss: 0.68975306, ae_loss: 0.05110773\n",
      "Step: [4880] total_loss: 2.13200593 d_loss: 1.36913633, g_loss: 0.71043807, ae_loss: 0.05243164\n",
      "Step: [4881] total_loss: 2.11879301 d_loss: 1.37806797, g_loss: 0.68711656, ae_loss: 0.05360837\n",
      "Step: [4882] total_loss: 2.10924315 d_loss: 1.36018777, g_loss: 0.69721979, ae_loss: 0.05183566\n",
      "Step: [4883] total_loss: 2.12093163 d_loss: 1.38357258, g_loss: 0.68436897, ae_loss: 0.05298995\n",
      "Step: [4884] total_loss: 2.12098694 d_loss: 1.39858210, g_loss: 0.66903019, ae_loss: 0.05337461\n",
      "Step: [4885] total_loss: 2.12313581 d_loss: 1.38499272, g_loss: 0.68834579, ae_loss: 0.04979727\n",
      "Step: [4886] total_loss: 2.11690474 d_loss: 1.38851428, g_loss: 0.67911357, ae_loss: 0.04927679\n",
      "Step: [4887] total_loss: 2.12384224 d_loss: 1.38541579, g_loss: 0.68106258, ae_loss: 0.05736381\n",
      "Step: [4888] total_loss: 2.11169863 d_loss: 1.37981725, g_loss: 0.68041611, ae_loss: 0.05146526\n",
      "Step: [4889] total_loss: 2.11815810 d_loss: 1.38298821, g_loss: 0.68535608, ae_loss: 0.04981375\n",
      "Step: [4890] total_loss: 2.12614083 d_loss: 1.38835943, g_loss: 0.69343597, ae_loss: 0.04434544\n",
      "Step: [4891] total_loss: 2.13427544 d_loss: 1.38157010, g_loss: 0.70127368, ae_loss: 0.05143150\n",
      "Step: [4892] total_loss: 2.13295197 d_loss: 1.39532089, g_loss: 0.68805110, ae_loss: 0.04957996\n",
      "Step: [4893] total_loss: 2.13718128 d_loss: 1.39663565, g_loss: 0.69016385, ae_loss: 0.05038171\n",
      "Step: [4894] total_loss: 2.13068581 d_loss: 1.38127303, g_loss: 0.69515729, ae_loss: 0.05425532\n",
      "Step: [4895] total_loss: 2.13026619 d_loss: 1.38973927, g_loss: 0.68983430, ae_loss: 0.05069255\n",
      "Step: [4896] total_loss: 2.12076449 d_loss: 1.37170720, g_loss: 0.69408345, ae_loss: 0.05497380\n",
      "Step: [4897] total_loss: 2.13411927 d_loss: 1.39062774, g_loss: 0.69204813, ae_loss: 0.05144330\n",
      "Step: [4898] total_loss: 2.12724400 d_loss: 1.38044441, g_loss: 0.69372094, ae_loss: 0.05307851\n",
      "Step: [4899] total_loss: 2.11606932 d_loss: 1.37248099, g_loss: 0.69199955, ae_loss: 0.05158868\n",
      "Step: [4900] total_loss: 2.13913989 d_loss: 1.39549327, g_loss: 0.68992275, ae_loss: 0.05372397\n",
      "Step: [4901] total_loss: 2.12392712 d_loss: 1.37434912, g_loss: 0.69703007, ae_loss: 0.05254776\n",
      "Step: [4902] total_loss: 2.11012030 d_loss: 1.37037444, g_loss: 0.69079477, ae_loss: 0.04895121\n",
      "Step: [4903] total_loss: 2.12758732 d_loss: 1.37880039, g_loss: 0.69136167, ae_loss: 0.05742534\n",
      "Step: [4904] total_loss: 2.13350749 d_loss: 1.38347411, g_loss: 0.70487309, ae_loss: 0.04516027\n",
      "Step: [4905] total_loss: 2.12748003 d_loss: 1.39219618, g_loss: 0.68547112, ae_loss: 0.04981264\n",
      "Step: [4906] total_loss: 2.12235761 d_loss: 1.38927698, g_loss: 0.68371582, ae_loss: 0.04936479\n",
      "Step: [4907] total_loss: 2.12251186 d_loss: 1.37482262, g_loss: 0.69630027, ae_loss: 0.05138909\n",
      "Step: [4908] total_loss: 2.13133550 d_loss: 1.38030267, g_loss: 0.70024604, ae_loss: 0.05078687\n",
      "Step: [4909] total_loss: 2.12131763 d_loss: 1.38147330, g_loss: 0.68917966, ae_loss: 0.05066470\n",
      "Step: [4910] total_loss: 2.11768413 d_loss: 1.37806439, g_loss: 0.68987387, ae_loss: 0.04974574\n",
      "Step: [4911] total_loss: 2.13739181 d_loss: 1.39761043, g_loss: 0.68656731, ae_loss: 0.05321411\n",
      "Step: [4912] total_loss: 2.12036967 d_loss: 1.37785745, g_loss: 0.69278389, ae_loss: 0.04972844\n",
      "Step: [4913] total_loss: 2.13533020 d_loss: 1.38489795, g_loss: 0.69634831, ae_loss: 0.05408385\n",
      "Step: [4914] total_loss: 2.12958050 d_loss: 1.37891865, g_loss: 0.69940704, ae_loss: 0.05125472\n",
      "Step: [4915] total_loss: 2.15176868 d_loss: 1.40322828, g_loss: 0.69478422, ae_loss: 0.05375621\n",
      "Step: [4916] total_loss: 2.12350035 d_loss: 1.37567472, g_loss: 0.69857156, ae_loss: 0.04925398\n",
      "Step: [4917] total_loss: 2.12020493 d_loss: 1.38337266, g_loss: 0.68794101, ae_loss: 0.04889116\n",
      "Step: [4918] total_loss: 2.11625648 d_loss: 1.36601686, g_loss: 0.70233572, ae_loss: 0.04790384\n",
      "Step: [4919] total_loss: 2.12383175 d_loss: 1.37542319, g_loss: 0.69838250, ae_loss: 0.05002589\n",
      "Step: [4920] total_loss: 2.11091375 d_loss: 1.38287091, g_loss: 0.67595756, ae_loss: 0.05208532\n",
      "Step: [4921] total_loss: 2.14338231 d_loss: 1.39974165, g_loss: 0.69111574, ae_loss: 0.05252491\n",
      "Step: [4922] total_loss: 2.12937880 d_loss: 1.38156319, g_loss: 0.69470334, ae_loss: 0.05311222\n",
      "Step: [4923] total_loss: 2.11285877 d_loss: 1.36969924, g_loss: 0.69332278, ae_loss: 0.04983664\n",
      "Step: [4924] total_loss: 2.12675285 d_loss: 1.39593482, g_loss: 0.68189031, ae_loss: 0.04892777\n",
      "Step: [4925] total_loss: 2.12269688 d_loss: 1.37782526, g_loss: 0.69039798, ae_loss: 0.05447366\n",
      "Step: [4926] total_loss: 2.11474323 d_loss: 1.37874913, g_loss: 0.68835789, ae_loss: 0.04763614\n",
      "Step: [4927] total_loss: 2.13717341 d_loss: 1.39734936, g_loss: 0.69069940, ae_loss: 0.04912468\n",
      "Step: [4928] total_loss: 2.12148499 d_loss: 1.38701344, g_loss: 0.68422669, ae_loss: 0.05024492\n",
      "Step: [4929] total_loss: 2.13206768 d_loss: 1.37225986, g_loss: 0.70351070, ae_loss: 0.05629707\n",
      "Step: [4930] total_loss: 2.12415099 d_loss: 1.39099431, g_loss: 0.68257338, ae_loss: 0.05058339\n",
      "Step: [4931] total_loss: 2.11901593 d_loss: 1.37977290, g_loss: 0.69073355, ae_loss: 0.04850947\n",
      "Step: [4932] total_loss: 2.13141227 d_loss: 1.37858653, g_loss: 0.69970459, ae_loss: 0.05312111\n",
      "Step: [4933] total_loss: 2.11399031 d_loss: 1.37774050, g_loss: 0.68387675, ae_loss: 0.05237291\n",
      "Step: [4934] total_loss: 2.12492514 d_loss: 1.38936794, g_loss: 0.68457955, ae_loss: 0.05097766\n",
      "Step: [4935] total_loss: 2.13855791 d_loss: 1.39419031, g_loss: 0.69548929, ae_loss: 0.04887813\n",
      "Step: [4936] total_loss: 2.12741542 d_loss: 1.37586391, g_loss: 0.69851679, ae_loss: 0.05303469\n",
      "Step: [4937] total_loss: 2.13293719 d_loss: 1.39119792, g_loss: 0.68988448, ae_loss: 0.05185485\n",
      "Step: [4938] total_loss: 2.11790085 d_loss: 1.38743341, g_loss: 0.67569900, ae_loss: 0.05476832\n",
      "Step: [4939] total_loss: 2.11318159 d_loss: 1.37781572, g_loss: 0.68302894, ae_loss: 0.05233703\n",
      "Step: [4940] total_loss: 2.12470913 d_loss: 1.38241363, g_loss: 0.68581557, ae_loss: 0.05647979\n",
      "Step: [4941] total_loss: 2.15422010 d_loss: 1.41131425, g_loss: 0.69139874, ae_loss: 0.05150717\n",
      "Step: [4942] total_loss: 2.11571956 d_loss: 1.36927116, g_loss: 0.69488454, ae_loss: 0.05156388\n",
      "Step: [4943] total_loss: 2.11830282 d_loss: 1.37880540, g_loss: 0.69062865, ae_loss: 0.04886872\n",
      "Step: [4944] total_loss: 2.14393854 d_loss: 1.40474701, g_loss: 0.68550026, ae_loss: 0.05369136\n",
      "Step: [4945] total_loss: 2.15032578 d_loss: 1.41093075, g_loss: 0.68773830, ae_loss: 0.05165677\n",
      "Step: [4946] total_loss: 2.12170005 d_loss: 1.36055338, g_loss: 0.70980281, ae_loss: 0.05134379\n",
      "Step: [4947] total_loss: 2.11805081 d_loss: 1.37896323, g_loss: 0.68649411, ae_loss: 0.05259350\n",
      "Step: [4948] total_loss: 2.12033010 d_loss: 1.38666224, g_loss: 0.68475974, ae_loss: 0.04890812\n",
      "Step: [4949] total_loss: 2.10793328 d_loss: 1.37863481, g_loss: 0.67449611, ae_loss: 0.05480233\n",
      "Step: [4950] total_loss: 2.12993717 d_loss: 1.38121223, g_loss: 0.69859868, ae_loss: 0.05012630\n",
      "Step: [4951] total_loss: 2.11559534 d_loss: 1.37855971, g_loss: 0.68505847, ae_loss: 0.05197722\n",
      "Step: [4952] total_loss: 2.14075470 d_loss: 1.39824378, g_loss: 0.68629646, ae_loss: 0.05621451\n",
      "Step: [4953] total_loss: 2.11910772 d_loss: 1.37409019, g_loss: 0.69208300, ae_loss: 0.05293445\n",
      "Step: [4954] total_loss: 2.12050486 d_loss: 1.37883830, g_loss: 0.68710041, ae_loss: 0.05456603\n",
      "Step: [4955] total_loss: 2.12228251 d_loss: 1.38748121, g_loss: 0.68472093, ae_loss: 0.05008026\n",
      "Step: [4956] total_loss: 2.12301230 d_loss: 1.38998806, g_loss: 0.68075246, ae_loss: 0.05227186\n",
      "Step: [4957] total_loss: 2.12037802 d_loss: 1.38350976, g_loss: 0.68880498, ae_loss: 0.04806336\n",
      "Step: [4958] total_loss: 2.14385152 d_loss: 1.39635754, g_loss: 0.69644189, ae_loss: 0.05105215\n",
      "Step: [4959] total_loss: 2.13966370 d_loss: 1.39917302, g_loss: 0.68654048, ae_loss: 0.05395028\n",
      "Step: [4960] total_loss: 2.13214397 d_loss: 1.38461256, g_loss: 0.69767576, ae_loss: 0.04985553\n",
      "Step: [4961] total_loss: 2.12625551 d_loss: 1.38515615, g_loss: 0.69146800, ae_loss: 0.04963119\n",
      "Step: [4962] total_loss: 2.12886763 d_loss: 1.38331628, g_loss: 0.69353640, ae_loss: 0.05201477\n",
      "Step: [4963] total_loss: 2.12009120 d_loss: 1.38709497, g_loss: 0.68287086, ae_loss: 0.05012539\n",
      "Step: [4964] total_loss: 2.12841654 d_loss: 1.38310516, g_loss: 0.69461989, ae_loss: 0.05069159\n",
      "Step: [4965] total_loss: 2.13208556 d_loss: 1.38910699, g_loss: 0.69375169, ae_loss: 0.04922684\n",
      "Step: [4966] total_loss: 2.12784386 d_loss: 1.39245307, g_loss: 0.68174589, ae_loss: 0.05364507\n",
      "Step: [4967] total_loss: 2.11609960 d_loss: 1.38086915, g_loss: 0.68391728, ae_loss: 0.05131311\n",
      "Step: [4968] total_loss: 2.12832665 d_loss: 1.37727118, g_loss: 0.69899207, ae_loss: 0.05206348\n",
      "Step: [4969] total_loss: 2.12498665 d_loss: 1.38691950, g_loss: 0.68784714, ae_loss: 0.05021996\n",
      "Step: [4970] total_loss: 2.12883186 d_loss: 1.38892627, g_loss: 0.69084299, ae_loss: 0.04906275\n",
      "Step: [4971] total_loss: 2.11342645 d_loss: 1.38112760, g_loss: 0.68396771, ae_loss: 0.04833119\n",
      "Step: [4972] total_loss: 2.12190104 d_loss: 1.38901973, g_loss: 0.68286496, ae_loss: 0.05001623\n",
      "Step: [4973] total_loss: 2.11547303 d_loss: 1.38436007, g_loss: 0.68251437, ae_loss: 0.04859867\n",
      "Step: [4974] total_loss: 2.12485051 d_loss: 1.39333093, g_loss: 0.68143177, ae_loss: 0.05008782\n",
      "Step: [4975] total_loss: 2.12007380 d_loss: 1.38014686, g_loss: 0.69391620, ae_loss: 0.04601071\n",
      "Step: [4976] total_loss: 2.13334513 d_loss: 1.38678646, g_loss: 0.69274056, ae_loss: 0.05381827\n",
      "Step: [4977] total_loss: 2.11803913 d_loss: 1.37453794, g_loss: 0.69376159, ae_loss: 0.04973952\n",
      "Step: [4978] total_loss: 2.12141991 d_loss: 1.37155938, g_loss: 0.69686580, ae_loss: 0.05299482\n",
      "Step: [4979] total_loss: 2.11321831 d_loss: 1.37392557, g_loss: 0.68831021, ae_loss: 0.05098249\n",
      "Step: [4980] total_loss: 2.12623978 d_loss: 1.38639843, g_loss: 0.68376774, ae_loss: 0.05607365\n",
      "Step: [4981] total_loss: 2.12861300 d_loss: 1.39364600, g_loss: 0.68462825, ae_loss: 0.05033878\n",
      "Step: [4982] total_loss: 2.12889290 d_loss: 1.39162719, g_loss: 0.68824005, ae_loss: 0.04902572\n",
      "Step: [4983] total_loss: 2.13842106 d_loss: 1.39772987, g_loss: 0.69210768, ae_loss: 0.04858342\n",
      "Step: [4984] total_loss: 2.13267088 d_loss: 1.38306713, g_loss: 0.69367313, ae_loss: 0.05593078\n",
      "Step: [4985] total_loss: 2.13509035 d_loss: 1.39179683, g_loss: 0.69489008, ae_loss: 0.04840345\n",
      "Step: [4986] total_loss: 2.12816024 d_loss: 1.38771474, g_loss: 0.69054520, ae_loss: 0.04990026\n",
      "Step: [4987] total_loss: 2.12378359 d_loss: 1.38818765, g_loss: 0.68881255, ae_loss: 0.04678346\n",
      "Step: [4988] total_loss: 2.11138725 d_loss: 1.37270272, g_loss: 0.68937171, ae_loss: 0.04931271\n",
      "Step: [4989] total_loss: 2.12794828 d_loss: 1.39189744, g_loss: 0.68570715, ae_loss: 0.05034370\n",
      "Step: [4990] total_loss: 2.11699629 d_loss: 1.37426186, g_loss: 0.69314682, ae_loss: 0.04958754\n",
      "Step: [4991] total_loss: 2.12380600 d_loss: 1.39503813, g_loss: 0.67415756, ae_loss: 0.05461036\n",
      "Step: [4992] total_loss: 2.11745834 d_loss: 1.39551759, g_loss: 0.66978288, ae_loss: 0.05215791\n",
      "Step: [4993] total_loss: 2.12169123 d_loss: 1.38245344, g_loss: 0.68563986, ae_loss: 0.05359779\n",
      "Step: [4994] total_loss: 2.12902403 d_loss: 1.39621496, g_loss: 0.68505758, ae_loss: 0.04775149\n",
      "Step: [4995] total_loss: 2.12411737 d_loss: 1.38833070, g_loss: 0.68766296, ae_loss: 0.04812376\n",
      "Step: [4996] total_loss: 2.13848090 d_loss: 1.40108252, g_loss: 0.68230391, ae_loss: 0.05509450\n",
      "Step: [4997] total_loss: 2.12136626 d_loss: 1.37867999, g_loss: 0.68991339, ae_loss: 0.05277285\n",
      "Step: [4998] total_loss: 2.11961436 d_loss: 1.38212478, g_loss: 0.68848735, ae_loss: 0.04900212\n",
      "Step: [4999] total_loss: 2.11667490 d_loss: 1.38932347, g_loss: 0.68236196, ae_loss: 0.04498930\n",
      "Step: [5000] total_loss: 2.12567878 d_loss: 1.38047731, g_loss: 0.69156748, ae_loss: 0.05363405\n",
      "Step: [5001] total_loss: 2.11730576 d_loss: 1.38546443, g_loss: 0.68395627, ae_loss: 0.04788511\n",
      "Step: [5002] total_loss: 2.12453532 d_loss: 1.38115919, g_loss: 0.69469458, ae_loss: 0.04868156\n",
      "Step: [5003] total_loss: 2.12988496 d_loss: 1.39575481, g_loss: 0.68407714, ae_loss: 0.05005301\n",
      "Step: [5004] total_loss: 2.13346863 d_loss: 1.37807715, g_loss: 0.70257449, ae_loss: 0.05281706\n",
      "Step: [5005] total_loss: 2.13196230 d_loss: 1.38424361, g_loss: 0.69775164, ae_loss: 0.04996723\n",
      "Step: [5006] total_loss: 2.10616755 d_loss: 1.36856878, g_loss: 0.68826514, ae_loss: 0.04933359\n",
      "Step: [5007] total_loss: 2.12801695 d_loss: 1.38691473, g_loss: 0.68850923, ae_loss: 0.05259307\n",
      "Step: [5008] total_loss: 2.13140988 d_loss: 1.39211810, g_loss: 0.69194329, ae_loss: 0.04734850\n",
      "Step: [5009] total_loss: 2.12546396 d_loss: 1.37662435, g_loss: 0.69850504, ae_loss: 0.05033455\n",
      "Step: [5010] total_loss: 2.12002420 d_loss: 1.38672280, g_loss: 0.68245471, ae_loss: 0.05084685\n",
      "Step: [5011] total_loss: 2.10620809 d_loss: 1.38093615, g_loss: 0.67210549, ae_loss: 0.05316656\n",
      "Step: [5012] total_loss: 2.12384748 d_loss: 1.38576388, g_loss: 0.68791819, ae_loss: 0.05016527\n",
      "Step: [5013] total_loss: 2.11894393 d_loss: 1.38953459, g_loss: 0.67862737, ae_loss: 0.05078194\n",
      "Step: [5014] total_loss: 2.10802078 d_loss: 1.37221670, g_loss: 0.68473244, ae_loss: 0.05107161\n",
      "Step: [5015] total_loss: 2.11041021 d_loss: 1.37884879, g_loss: 0.68553531, ae_loss: 0.04602595\n",
      "Step: [5016] total_loss: 2.11369658 d_loss: 1.37840259, g_loss: 0.68639588, ae_loss: 0.04889819\n",
      "Step: [5017] total_loss: 2.12275052 d_loss: 1.38367712, g_loss: 0.68902737, ae_loss: 0.05004610\n",
      "Step: [5018] total_loss: 2.12037396 d_loss: 1.37130475, g_loss: 0.70035678, ae_loss: 0.04871253\n",
      "Step: [5019] total_loss: 2.12407637 d_loss: 1.40156281, g_loss: 0.67214835, ae_loss: 0.05036511\n",
      "Step: [5020] total_loss: 2.11251998 d_loss: 1.38294828, g_loss: 0.68083602, ae_loss: 0.04873575\n",
      "Step: [5021] total_loss: 2.13333845 d_loss: 1.38498545, g_loss: 0.69399941, ae_loss: 0.05435371\n",
      "Step: [5022] total_loss: 2.11731529 d_loss: 1.37604213, g_loss: 0.69183481, ae_loss: 0.04943846\n",
      "Step: [5023] total_loss: 2.13652706 d_loss: 1.38709378, g_loss: 0.69947261, ae_loss: 0.04996069\n",
      "Step: [5024] total_loss: 2.11798739 d_loss: 1.38811862, g_loss: 0.68181837, ae_loss: 0.04805041\n",
      "Step: [5025] total_loss: 2.12164521 d_loss: 1.38631010, g_loss: 0.68418264, ae_loss: 0.05115244\n",
      "Step: [5026] total_loss: 2.12746239 d_loss: 1.39368057, g_loss: 0.68454039, ae_loss: 0.04924152\n",
      "Step: [5027] total_loss: 2.12672853 d_loss: 1.38747716, g_loss: 0.68707192, ae_loss: 0.05217930\n",
      "Step: [5028] total_loss: 2.11441827 d_loss: 1.38427269, g_loss: 0.67940205, ae_loss: 0.05074350\n",
      "Step: [5029] total_loss: 2.10484409 d_loss: 1.37038732, g_loss: 0.69111711, ae_loss: 0.04333970\n",
      "Step: [5030] total_loss: 2.11720181 d_loss: 1.37088442, g_loss: 0.69752795, ae_loss: 0.04878945\n",
      "Step: [5031] total_loss: 2.13715219 d_loss: 1.39321113, g_loss: 0.69411552, ae_loss: 0.04982551\n",
      "Step: [5032] total_loss: 2.12231207 d_loss: 1.37450504, g_loss: 0.70053828, ae_loss: 0.04726873\n",
      "Step: [5033] total_loss: 2.12432599 d_loss: 1.36818683, g_loss: 0.70769703, ae_loss: 0.04844209\n",
      "Step: [5034] total_loss: 2.10885620 d_loss: 1.37777472, g_loss: 0.68176973, ae_loss: 0.04931181\n",
      "Step: [5035] total_loss: 2.13193393 d_loss: 1.37652659, g_loss: 0.70740271, ae_loss: 0.04800457\n",
      "Step: [5036] total_loss: 2.11666751 d_loss: 1.38216639, g_loss: 0.68583268, ae_loss: 0.04866843\n",
      "Step: [5037] total_loss: 2.11822748 d_loss: 1.37498224, g_loss: 0.69250619, ae_loss: 0.05073904\n",
      "Step: [5038] total_loss: 2.12478685 d_loss: 1.38127851, g_loss: 0.69157344, ae_loss: 0.05193493\n",
      "Step: [5039] total_loss: 2.11903811 d_loss: 1.37228322, g_loss: 0.69648844, ae_loss: 0.05026634\n",
      "Step: [5040] total_loss: 2.12551999 d_loss: 1.38742805, g_loss: 0.68705189, ae_loss: 0.05104008\n",
      "Step: [5041] total_loss: 2.12467098 d_loss: 1.37855339, g_loss: 0.69797921, ae_loss: 0.04813826\n",
      "Step: [5042] total_loss: 2.13305902 d_loss: 1.39108157, g_loss: 0.69033742, ae_loss: 0.05164001\n",
      "Step: [5043] total_loss: 2.13596225 d_loss: 1.38418961, g_loss: 0.69909537, ae_loss: 0.05267732\n",
      "Step: [5044] total_loss: 2.14011741 d_loss: 1.39308703, g_loss: 0.69736540, ae_loss: 0.04966497\n",
      "Step: [5045] total_loss: 2.14895177 d_loss: 1.38403869, g_loss: 0.71242738, ae_loss: 0.05248574\n",
      "Step: [5046] total_loss: 2.14474249 d_loss: 1.39833724, g_loss: 0.69614363, ae_loss: 0.05026171\n",
      "Step: [5047] total_loss: 2.12520647 d_loss: 1.37727165, g_loss: 0.70235074, ae_loss: 0.04558403\n",
      "Step: [5048] total_loss: 2.13946915 d_loss: 1.37497485, g_loss: 0.71186554, ae_loss: 0.05262889\n",
      "Step: [5049] total_loss: 2.14405203 d_loss: 1.37805653, g_loss: 0.71334708, ae_loss: 0.05264834\n",
      "Step: [5050] total_loss: 2.14369655 d_loss: 1.39178085, g_loss: 0.69829553, ae_loss: 0.05362011\n",
      "Step: [5051] total_loss: 2.13309956 d_loss: 1.37646747, g_loss: 0.70363009, ae_loss: 0.05300217\n",
      "Step: [5052] total_loss: 2.13828993 d_loss: 1.39223075, g_loss: 0.69483817, ae_loss: 0.05122104\n",
      "Step: [5053] total_loss: 2.11526203 d_loss: 1.37909532, g_loss: 0.68731654, ae_loss: 0.04885002\n",
      "Step: [5054] total_loss: 2.11330271 d_loss: 1.37688982, g_loss: 0.68672395, ae_loss: 0.04968891\n",
      "Step: [5055] total_loss: 2.10967112 d_loss: 1.36226451, g_loss: 0.69803816, ae_loss: 0.04936844\n",
      "Step: [5056] total_loss: 2.12509298 d_loss: 1.37589025, g_loss: 0.70094144, ae_loss: 0.04826132\n",
      "Step: [5057] total_loss: 2.12090373 d_loss: 1.37723815, g_loss: 0.69368106, ae_loss: 0.04998460\n",
      "Step: [5058] total_loss: 2.11956930 d_loss: 1.38329053, g_loss: 0.68688613, ae_loss: 0.04939253\n",
      "Step: [5059] total_loss: 2.12007451 d_loss: 1.35958481, g_loss: 0.70684439, ae_loss: 0.05364528\n",
      "Step: [5060] total_loss: 2.11272168 d_loss: 1.37330747, g_loss: 0.69395334, ae_loss: 0.04546088\n",
      "Step: [5061] total_loss: 2.13008976 d_loss: 1.40054953, g_loss: 0.67819345, ae_loss: 0.05134680\n",
      "Step: [5062] total_loss: 2.12924528 d_loss: 1.38377547, g_loss: 0.69815814, ae_loss: 0.04731162\n",
      "Step: [5063] total_loss: 2.15026474 d_loss: 1.39264798, g_loss: 0.70750523, ae_loss: 0.05011161\n",
      "Step: [5064] total_loss: 2.12852049 d_loss: 1.39066863, g_loss: 0.68801105, ae_loss: 0.04984068\n",
      "Step: [5065] total_loss: 2.13780570 d_loss: 1.38679492, g_loss: 0.70013165, ae_loss: 0.05087911\n",
      "Step: [5066] total_loss: 2.12293696 d_loss: 1.37536216, g_loss: 0.69745666, ae_loss: 0.05011813\n",
      "Step: [5067] total_loss: 2.13313746 d_loss: 1.38709164, g_loss: 0.69382620, ae_loss: 0.05221959\n",
      "Step: [5068] total_loss: 2.12763929 d_loss: 1.38041091, g_loss: 0.69531679, ae_loss: 0.05191174\n",
      "Step: [5069] total_loss: 2.12930441 d_loss: 1.38332987, g_loss: 0.69391179, ae_loss: 0.05206263\n",
      "Step: [5070] total_loss: 2.12785292 d_loss: 1.38078547, g_loss: 0.68906558, ae_loss: 0.05800187\n",
      "Step: [5071] total_loss: 2.11461735 d_loss: 1.36254239, g_loss: 0.70341814, ae_loss: 0.04865690\n",
      "Step: [5072] total_loss: 2.12293363 d_loss: 1.38807082, g_loss: 0.67842627, ae_loss: 0.05643657\n",
      "Step: [5073] total_loss: 2.10922050 d_loss: 1.36956167, g_loss: 0.69063687, ae_loss: 0.04902212\n",
      "Step: [5074] total_loss: 2.12339306 d_loss: 1.38133299, g_loss: 0.68851757, ae_loss: 0.05354248\n",
      "Step: [5075] total_loss: 2.11935425 d_loss: 1.38012767, g_loss: 0.68843424, ae_loss: 0.05079233\n",
      "Step: [5076] total_loss: 2.12132978 d_loss: 1.37439513, g_loss: 0.69674766, ae_loss: 0.05018693\n",
      "Step: [5077] total_loss: 2.12565494 d_loss: 1.38564157, g_loss: 0.69211906, ae_loss: 0.04789425\n",
      "Step: [5078] total_loss: 2.12285686 d_loss: 1.36463308, g_loss: 0.70353758, ae_loss: 0.05468621\n",
      "Step: [5079] total_loss: 2.13599706 d_loss: 1.40085161, g_loss: 0.68380862, ae_loss: 0.05133677\n",
      "Step: [5080] total_loss: 2.13845110 d_loss: 1.38878977, g_loss: 0.69959724, ae_loss: 0.05006420\n",
      "Step: [5081] total_loss: 2.12648726 d_loss: 1.38649917, g_loss: 0.69030511, ae_loss: 0.04968314\n",
      "Step: [5082] total_loss: 2.11475515 d_loss: 1.37871003, g_loss: 0.68690813, ae_loss: 0.04913717\n",
      "Step: [5083] total_loss: 2.12296057 d_loss: 1.37663341, g_loss: 0.69406390, ae_loss: 0.05226327\n",
      "Step: [5084] total_loss: 2.12630510 d_loss: 1.39217925, g_loss: 0.68423116, ae_loss: 0.04989462\n",
      "Step: [5085] total_loss: 2.13669825 d_loss: 1.39019227, g_loss: 0.69843745, ae_loss: 0.04806838\n",
      "Step: [5086] total_loss: 2.13816452 d_loss: 1.40720546, g_loss: 0.68074912, ae_loss: 0.05021000\n",
      "Step: [5087] total_loss: 2.13492608 d_loss: 1.37217236, g_loss: 0.71034640, ae_loss: 0.05240721\n",
      "Step: [5088] total_loss: 2.12669516 d_loss: 1.37096453, g_loss: 0.70480996, ae_loss: 0.05092063\n",
      "Step: [5089] total_loss: 2.14475250 d_loss: 1.40141034, g_loss: 0.69248748, ae_loss: 0.05085476\n",
      "Step: [5090] total_loss: 2.11840057 d_loss: 1.37900567, g_loss: 0.68636620, ae_loss: 0.05302852\n",
      "Step: [5091] total_loss: 2.12878847 d_loss: 1.38797903, g_loss: 0.68760371, ae_loss: 0.05320561\n",
      "Step: [5092] total_loss: 2.10582423 d_loss: 1.36958504, g_loss: 0.68487197, ae_loss: 0.05136711\n",
      "Step: [5093] total_loss: 2.11165094 d_loss: 1.36848342, g_loss: 0.69129264, ae_loss: 0.05187501\n",
      "Step: [5094] total_loss: 2.12222147 d_loss: 1.38113403, g_loss: 0.68956327, ae_loss: 0.05152431\n",
      "Step: [5095] total_loss: 2.12671995 d_loss: 1.38651788, g_loss: 0.68970251, ae_loss: 0.05049951\n",
      "Step: [5096] total_loss: 2.13167357 d_loss: 1.38426161, g_loss: 0.69759732, ae_loss: 0.04981467\n",
      "Step: [5097] total_loss: 2.09702063 d_loss: 1.37069178, g_loss: 0.67568600, ae_loss: 0.05064270\n",
      "Step: [5098] total_loss: 2.10810757 d_loss: 1.36307359, g_loss: 0.69481474, ae_loss: 0.05021915\n",
      "Step: [5099] total_loss: 2.12417436 d_loss: 1.39171720, g_loss: 0.68207014, ae_loss: 0.05038706\n",
      "Step: [5100] total_loss: 2.11484289 d_loss: 1.38610554, g_loss: 0.67474878, ae_loss: 0.05398872\n",
      "Step: [5101] total_loss: 2.10196924 d_loss: 1.37223744, g_loss: 0.68332165, ae_loss: 0.04641025\n",
      "Step: [5102] total_loss: 2.12887955 d_loss: 1.38856030, g_loss: 0.68884933, ae_loss: 0.05146991\n",
      "Step: [5103] total_loss: 2.12099004 d_loss: 1.36708462, g_loss: 0.69996184, ae_loss: 0.05394362\n",
      "Step: [5104] total_loss: 2.13125420 d_loss: 1.39007735, g_loss: 0.69009978, ae_loss: 0.05107715\n",
      "Step: [5105] total_loss: 2.13796401 d_loss: 1.38646865, g_loss: 0.69898158, ae_loss: 0.05251379\n",
      "Step: [5106] total_loss: 2.13747644 d_loss: 1.39263773, g_loss: 0.69461966, ae_loss: 0.05021907\n",
      "Step: [5107] total_loss: 2.12077093 d_loss: 1.37573099, g_loss: 0.69408488, ae_loss: 0.05095515\n",
      "Step: [5108] total_loss: 2.11990166 d_loss: 1.38305998, g_loss: 0.68729168, ae_loss: 0.04955002\n",
      "Step: [5109] total_loss: 2.13087440 d_loss: 1.37716794, g_loss: 0.70219761, ae_loss: 0.05150883\n",
      "Step: [5110] total_loss: 2.12164450 d_loss: 1.38479376, g_loss: 0.68504441, ae_loss: 0.05180648\n",
      "Step: [5111] total_loss: 2.13313627 d_loss: 1.38979626, g_loss: 0.69451892, ae_loss: 0.04882095\n",
      "Step: [5112] total_loss: 2.11648273 d_loss: 1.37040389, g_loss: 0.69180751, ae_loss: 0.05427124\n",
      "Step: [5113] total_loss: 2.12938643 d_loss: 1.39020085, g_loss: 0.69140923, ae_loss: 0.04777626\n",
      "Step: [5114] total_loss: 2.12269616 d_loss: 1.38172317, g_loss: 0.68994826, ae_loss: 0.05102485\n",
      "Step: [5115] total_loss: 2.13216209 d_loss: 1.40176177, g_loss: 0.68148619, ae_loss: 0.04891413\n",
      "Step: [5116] total_loss: 2.10599470 d_loss: 1.36512029, g_loss: 0.69056296, ae_loss: 0.05031140\n",
      "Step: [5117] total_loss: 2.12680578 d_loss: 1.39010262, g_loss: 0.68719745, ae_loss: 0.04950585\n",
      "Step: [5118] total_loss: 2.12168646 d_loss: 1.37433851, g_loss: 0.70183122, ae_loss: 0.04551686\n",
      "Step: [5119] total_loss: 2.11465502 d_loss: 1.36880684, g_loss: 0.69593763, ae_loss: 0.04991050\n",
      "Step: [5120] total_loss: 2.12523723 d_loss: 1.37326336, g_loss: 0.70679450, ae_loss: 0.04517935\n",
      "Step: [5121] total_loss: 2.12849736 d_loss: 1.37293386, g_loss: 0.70528340, ae_loss: 0.05028012\n",
      "Step: [5122] total_loss: 2.13499951 d_loss: 1.39035964, g_loss: 0.69549149, ae_loss: 0.04914827\n",
      "Step: [5123] total_loss: 2.13233137 d_loss: 1.37458622, g_loss: 0.70663130, ae_loss: 0.05111395\n",
      "Step: [5124] total_loss: 2.12651753 d_loss: 1.36350977, g_loss: 0.71332204, ae_loss: 0.04968573\n",
      "Step: [5125] total_loss: 2.14140058 d_loss: 1.40006590, g_loss: 0.68678677, ae_loss: 0.05454789\n",
      "Step: [5126] total_loss: 2.12303925 d_loss: 1.37066126, g_loss: 0.70215452, ae_loss: 0.05022339\n",
      "Step: [5127] total_loss: 2.12150979 d_loss: 1.37677312, g_loss: 0.69265115, ae_loss: 0.05208554\n",
      "Step: [5128] total_loss: 2.10847950 d_loss: 1.36883366, g_loss: 0.69192266, ae_loss: 0.04772320\n",
      "Step: [5129] total_loss: 2.13543034 d_loss: 1.38937163, g_loss: 0.69310880, ae_loss: 0.05294984\n",
      "Step: [5130] total_loss: 2.11495709 d_loss: 1.38156199, g_loss: 0.68151414, ae_loss: 0.05188096\n",
      "Step: [5131] total_loss: 2.12969255 d_loss: 1.37744164, g_loss: 0.69949359, ae_loss: 0.05275729\n",
      "Step: [5132] total_loss: 2.13656330 d_loss: 1.38590968, g_loss: 0.69822967, ae_loss: 0.05242406\n",
      "Step: [5133] total_loss: 2.10820794 d_loss: 1.37434030, g_loss: 0.68338442, ae_loss: 0.05048326\n",
      "Step: [5134] total_loss: 2.10995317 d_loss: 1.36920547, g_loss: 0.69135880, ae_loss: 0.04938892\n",
      "Step: [5135] total_loss: 2.12759662 d_loss: 1.38966978, g_loss: 0.68710011, ae_loss: 0.05082667\n",
      "Step: [5136] total_loss: 2.13523984 d_loss: 1.38966775, g_loss: 0.69408411, ae_loss: 0.05148791\n",
      "Step: [5137] total_loss: 2.13337326 d_loss: 1.39182162, g_loss: 0.69007814, ae_loss: 0.05147365\n",
      "Step: [5138] total_loss: 2.12755179 d_loss: 1.38197553, g_loss: 0.69127619, ae_loss: 0.05430005\n",
      "Step: [5139] total_loss: 2.15673161 d_loss: 1.41853917, g_loss: 0.68698442, ae_loss: 0.05120796\n",
      "Step: [5140] total_loss: 2.12511635 d_loss: 1.38150251, g_loss: 0.69254357, ae_loss: 0.05107026\n",
      "Step: [5141] total_loss: 2.14066935 d_loss: 1.39209712, g_loss: 0.69492453, ae_loss: 0.05364770\n",
      "Step: [5142] total_loss: 2.13708925 d_loss: 1.40168929, g_loss: 0.68548524, ae_loss: 0.04991469\n",
      "Step: [5143] total_loss: 2.15140939 d_loss: 1.37704468, g_loss: 0.72317851, ae_loss: 0.05118619\n",
      "Step: [5144] total_loss: 2.12809038 d_loss: 1.39360476, g_loss: 0.68123764, ae_loss: 0.05324801\n",
      "Step: [5145] total_loss: 2.14432716 d_loss: 1.40750289, g_loss: 0.68272668, ae_loss: 0.05409767\n",
      "Step: [5146] total_loss: 2.12339711 d_loss: 1.38269043, g_loss: 0.69148564, ae_loss: 0.04922101\n",
      "Step: [5147] total_loss: 2.12378025 d_loss: 1.38533401, g_loss: 0.68854439, ae_loss: 0.04990173\n",
      "Step: [5148] total_loss: 2.11951733 d_loss: 1.38906956, g_loss: 0.68302292, ae_loss: 0.04742479\n",
      "Step: [5149] total_loss: 2.10968971 d_loss: 1.37614036, g_loss: 0.68430734, ae_loss: 0.04924218\n",
      "Step: [5150] total_loss: 2.11368084 d_loss: 1.36619091, g_loss: 0.70024848, ae_loss: 0.04724136\n",
      "Step: [5151] total_loss: 2.10702968 d_loss: 1.38775730, g_loss: 0.67313868, ae_loss: 0.04613371\n",
      "Step: [5152] total_loss: 2.11485863 d_loss: 1.38401997, g_loss: 0.67849803, ae_loss: 0.05234050\n",
      "Step: [5153] total_loss: 2.10803604 d_loss: 1.37068319, g_loss: 0.68574321, ae_loss: 0.05160961\n",
      "Step: [5154] total_loss: 2.11489534 d_loss: 1.37304974, g_loss: 0.68774742, ae_loss: 0.05409821\n",
      "Step: [5155] total_loss: 2.14461541 d_loss: 1.39642549, g_loss: 0.69683880, ae_loss: 0.05135111\n",
      "Step: [5156] total_loss: 2.14614725 d_loss: 1.39009404, g_loss: 0.70985782, ae_loss: 0.04619542\n",
      "Step: [5157] total_loss: 2.11882639 d_loss: 1.37983739, g_loss: 0.68760169, ae_loss: 0.05138728\n",
      "Step: [5158] total_loss: 2.13595986 d_loss: 1.37912762, g_loss: 0.70411021, ae_loss: 0.05272212\n",
      "Step: [5159] total_loss: 2.12756443 d_loss: 1.38015866, g_loss: 0.69548202, ae_loss: 0.05192378\n",
      "Step: [5160] total_loss: 2.11250329 d_loss: 1.37876010, g_loss: 0.68614513, ae_loss: 0.04759803\n",
      "Step: [5161] total_loss: 2.14180088 d_loss: 1.38971567, g_loss: 0.70273328, ae_loss: 0.04935203\n",
      "Step: [5162] total_loss: 2.12764120 d_loss: 1.38199019, g_loss: 0.70061374, ae_loss: 0.04503723\n",
      "Step: [5163] total_loss: 2.10968924 d_loss: 1.36323237, g_loss: 0.69857550, ae_loss: 0.04788148\n",
      "Step: [5164] total_loss: 2.12807322 d_loss: 1.39942372, g_loss: 0.67948914, ae_loss: 0.04916024\n",
      "Step: [5165] total_loss: 2.10211229 d_loss: 1.37045026, g_loss: 0.68291670, ae_loss: 0.04874528\n",
      "Step: [5166] total_loss: 2.10465193 d_loss: 1.37104940, g_loss: 0.68706465, ae_loss: 0.04653798\n",
      "Step: [5167] total_loss: 2.11823726 d_loss: 1.38158298, g_loss: 0.68808156, ae_loss: 0.04857273\n",
      "Step: [5168] total_loss: 2.15296650 d_loss: 1.40428853, g_loss: 0.69910026, ae_loss: 0.04957780\n",
      "Step: [5169] total_loss: 2.12470388 d_loss: 1.38879919, g_loss: 0.68645042, ae_loss: 0.04945418\n",
      "Step: [5170] total_loss: 2.13252544 d_loss: 1.38850832, g_loss: 0.69609797, ae_loss: 0.04791928\n",
      "Step: [5171] total_loss: 2.13465810 d_loss: 1.38644314, g_loss: 0.69449955, ae_loss: 0.05371537\n",
      "Step: [5172] total_loss: 2.13709879 d_loss: 1.39410269, g_loss: 0.69069827, ae_loss: 0.05229766\n",
      "Step: [5173] total_loss: 2.12778425 d_loss: 1.39644301, g_loss: 0.67926699, ae_loss: 0.05207427\n",
      "Step: [5174] total_loss: 2.12214279 d_loss: 1.37835503, g_loss: 0.69465935, ae_loss: 0.04912842\n",
      "Step: [5175] total_loss: 2.13017511 d_loss: 1.39592862, g_loss: 0.67978913, ae_loss: 0.05445737\n",
      "Step: [5176] total_loss: 2.12018061 d_loss: 1.38325381, g_loss: 0.68627959, ae_loss: 0.05064728\n",
      "Step: [5177] total_loss: 2.12148809 d_loss: 1.38204336, g_loss: 0.68617141, ae_loss: 0.05327347\n",
      "Step: [5178] total_loss: 2.11212254 d_loss: 1.37732160, g_loss: 0.68196785, ae_loss: 0.05283324\n",
      "Step: [5179] total_loss: 2.14144397 d_loss: 1.38535082, g_loss: 0.70336729, ae_loss: 0.05272590\n",
      "Step: [5180] total_loss: 2.13113403 d_loss: 1.38859296, g_loss: 0.69494033, ae_loss: 0.04760081\n",
      "Step: [5181] total_loss: 2.11028004 d_loss: 1.36879373, g_loss: 0.68963379, ae_loss: 0.05185264\n",
      "Step: [5182] total_loss: 2.11050844 d_loss: 1.38339305, g_loss: 0.67835414, ae_loss: 0.04876124\n",
      "Step: [5183] total_loss: 2.12797523 d_loss: 1.39190149, g_loss: 0.68398410, ae_loss: 0.05208958\n",
      "Step: [5184] total_loss: 2.12325191 d_loss: 1.39722538, g_loss: 0.67724073, ae_loss: 0.04878598\n",
      "Step: [5185] total_loss: 2.12997818 d_loss: 1.40056992, g_loss: 0.68243361, ae_loss: 0.04697463\n",
      "Step: [5186] total_loss: 2.14210939 d_loss: 1.41092300, g_loss: 0.67988819, ae_loss: 0.05129811\n",
      "Step: [5187] total_loss: 2.11634350 d_loss: 1.37139869, g_loss: 0.69597650, ae_loss: 0.04896837\n",
      "Step: [5188] total_loss: 2.11290026 d_loss: 1.38159990, g_loss: 0.68198323, ae_loss: 0.04931709\n",
      "Step: [5189] total_loss: 2.12471390 d_loss: 1.39783537, g_loss: 0.67366934, ae_loss: 0.05320912\n",
      "Step: [5190] total_loss: 2.13899946 d_loss: 1.39142251, g_loss: 0.69578993, ae_loss: 0.05178699\n",
      "Step: [5191] total_loss: 2.12762928 d_loss: 1.38621473, g_loss: 0.69484246, ae_loss: 0.04657206\n",
      "Step: [5192] total_loss: 2.11630964 d_loss: 1.36623859, g_loss: 0.69747567, ae_loss: 0.05259548\n",
      "Step: [5193] total_loss: 2.13445234 d_loss: 1.39076805, g_loss: 0.69048804, ae_loss: 0.05319619\n",
      "Step: [5194] total_loss: 2.12071490 d_loss: 1.38674963, g_loss: 0.68573141, ae_loss: 0.04823389\n",
      "Step: [5195] total_loss: 2.12850857 d_loss: 1.38115537, g_loss: 0.69789732, ae_loss: 0.04945597\n",
      "Step: [5196] total_loss: 2.12822485 d_loss: 1.38932002, g_loss: 0.68873227, ae_loss: 0.05017259\n",
      "Step: [5197] total_loss: 2.12123728 d_loss: 1.38105130, g_loss: 0.69189000, ae_loss: 0.04829599\n",
      "Step: [5198] total_loss: 2.12298274 d_loss: 1.38365793, g_loss: 0.68507969, ae_loss: 0.05424506\n",
      "Step: [5199] total_loss: 2.13646317 d_loss: 1.38696384, g_loss: 0.70098066, ae_loss: 0.04851869\n",
      "Step: [5200] total_loss: 2.13386106 d_loss: 1.37023950, g_loss: 0.71197814, ae_loss: 0.05164351\n",
      "Step: [5201] total_loss: 2.13597965 d_loss: 1.39283478, g_loss: 0.69469011, ae_loss: 0.04845484\n",
      "Step: [5202] total_loss: 2.13834953 d_loss: 1.40279150, g_loss: 0.68528116, ae_loss: 0.05027675\n",
      "Step: [5203] total_loss: 2.13843465 d_loss: 1.39247525, g_loss: 0.69712150, ae_loss: 0.04883789\n",
      "Step: [5204] total_loss: 2.11876202 d_loss: 1.36806083, g_loss: 0.70215416, ae_loss: 0.04854706\n",
      "Step: [5205] total_loss: 2.14787531 d_loss: 1.39134908, g_loss: 0.70753241, ae_loss: 0.04899380\n",
      "Step: [5206] total_loss: 2.13116503 d_loss: 1.39507437, g_loss: 0.68410635, ae_loss: 0.05198421\n",
      "Step: [5207] total_loss: 2.13353086 d_loss: 1.38905692, g_loss: 0.69478077, ae_loss: 0.04969315\n",
      "Step: [5208] total_loss: 2.12531042 d_loss: 1.39684677, g_loss: 0.67651355, ae_loss: 0.05195004\n",
      "Step: [5209] total_loss: 2.12664700 d_loss: 1.38675380, g_loss: 0.68698657, ae_loss: 0.05290662\n",
      "Step: [5210] total_loss: 2.10995579 d_loss: 1.38081205, g_loss: 0.67836136, ae_loss: 0.05078247\n",
      "Step: [5211] total_loss: 2.11469889 d_loss: 1.38141644, g_loss: 0.68525088, ae_loss: 0.04803152\n",
      "Step: [5212] total_loss: 2.14085698 d_loss: 1.38107264, g_loss: 0.70613426, ae_loss: 0.05365004\n",
      "Step: [5213] total_loss: 2.12124443 d_loss: 1.38662529, g_loss: 0.68700147, ae_loss: 0.04761777\n",
      "Step: [5214] total_loss: 2.13432956 d_loss: 1.39889276, g_loss: 0.68159103, ae_loss: 0.05384573\n",
      "Step: [5215] total_loss: 2.12250257 d_loss: 1.38147938, g_loss: 0.69313926, ae_loss: 0.04788394\n",
      "Step: [5216] total_loss: 2.12464333 d_loss: 1.38456941, g_loss: 0.69274259, ae_loss: 0.04733151\n",
      "Step: [5217] total_loss: 2.13457608 d_loss: 1.38474953, g_loss: 0.70162380, ae_loss: 0.04820274\n",
      "Step: [5218] total_loss: 2.13841748 d_loss: 1.38894820, g_loss: 0.69938463, ae_loss: 0.05008464\n",
      "Step: [5219] total_loss: 2.12954330 d_loss: 1.37363505, g_loss: 0.70198375, ae_loss: 0.05392456\n",
      "Step: [5220] total_loss: 2.13110328 d_loss: 1.39313769, g_loss: 0.68967056, ae_loss: 0.04829503\n",
      "Step: [5221] total_loss: 2.13819790 d_loss: 1.39421380, g_loss: 0.69598472, ae_loss: 0.04799934\n",
      "Step: [5222] total_loss: 2.12697387 d_loss: 1.38771474, g_loss: 0.68581575, ae_loss: 0.05344331\n",
      "Step: [5223] total_loss: 2.11164689 d_loss: 1.37919617, g_loss: 0.68261003, ae_loss: 0.04984067\n",
      "Step: [5224] total_loss: 2.13188601 d_loss: 1.40593994, g_loss: 0.67729276, ae_loss: 0.04865325\n",
      "Step: [5225] total_loss: 2.12473536 d_loss: 1.38902187, g_loss: 0.68669903, ae_loss: 0.04901447\n",
      "Step: [5226] total_loss: 2.11824965 d_loss: 1.36806822, g_loss: 0.70113015, ae_loss: 0.04905130\n",
      "Step: [5227] total_loss: 2.12080240 d_loss: 1.37751186, g_loss: 0.69404638, ae_loss: 0.04924431\n",
      "Step: [5228] total_loss: 2.12201810 d_loss: 1.38495672, g_loss: 0.68793148, ae_loss: 0.04912997\n",
      "Step: [5229] total_loss: 2.12821245 d_loss: 1.39736676, g_loss: 0.67837214, ae_loss: 0.05247350\n",
      "Step: [5230] total_loss: 2.11938858 d_loss: 1.37557578, g_loss: 0.69320679, ae_loss: 0.05060600\n",
      "Step: [5231] total_loss: 2.12750340 d_loss: 1.38333654, g_loss: 0.69353282, ae_loss: 0.05063409\n",
      "Step: [5232] total_loss: 2.12215376 d_loss: 1.37542021, g_loss: 0.69825208, ae_loss: 0.04848142\n",
      "Step: [5233] total_loss: 2.11604929 d_loss: 1.37984860, g_loss: 0.68716621, ae_loss: 0.04903454\n",
      "Step: [5234] total_loss: 2.12456489 d_loss: 1.39197445, g_loss: 0.68609786, ae_loss: 0.04649255\n",
      "Step: [5235] total_loss: 2.13574553 d_loss: 1.39262795, g_loss: 0.69276369, ae_loss: 0.05035379\n",
      "Step: [5236] total_loss: 2.11321139 d_loss: 1.37184453, g_loss: 0.68972808, ae_loss: 0.05163878\n",
      "Step: [5237] total_loss: 2.12358356 d_loss: 1.37816155, g_loss: 0.69238490, ae_loss: 0.05303708\n",
      "Step: [5238] total_loss: 2.12477446 d_loss: 1.37966013, g_loss: 0.69829988, ae_loss: 0.04681445\n",
      "Step: [5239] total_loss: 2.13122272 d_loss: 1.38727760, g_loss: 0.69211203, ae_loss: 0.05183314\n",
      "Step: [5240] total_loss: 2.12318802 d_loss: 1.38025308, g_loss: 0.69319022, ae_loss: 0.04974459\n",
      "Step: [5241] total_loss: 2.11192155 d_loss: 1.36931491, g_loss: 0.69496882, ae_loss: 0.04763787\n",
      "Step: [5242] total_loss: 2.12657404 d_loss: 1.38048029, g_loss: 0.69380653, ae_loss: 0.05228707\n",
      "Step: [5243] total_loss: 2.12721848 d_loss: 1.37467313, g_loss: 0.70238674, ae_loss: 0.05015862\n",
      "Step: [5244] total_loss: 2.12490392 d_loss: 1.39575362, g_loss: 0.68211389, ae_loss: 0.04703640\n",
      "Step: [5245] total_loss: 2.12309361 d_loss: 1.39450777, g_loss: 0.67715037, ae_loss: 0.05143561\n",
      "Step: [5246] total_loss: 2.10835862 d_loss: 1.37424493, g_loss: 0.68488461, ae_loss: 0.04922914\n",
      "Step: [5247] total_loss: 2.11392069 d_loss: 1.38106406, g_loss: 0.68036902, ae_loss: 0.05248766\n",
      "Step: [5248] total_loss: 2.12469125 d_loss: 1.37852335, g_loss: 0.69839221, ae_loss: 0.04777571\n",
      "Step: [5249] total_loss: 2.12133884 d_loss: 1.37518001, g_loss: 0.69926274, ae_loss: 0.04689604\n",
      "Step: [5250] total_loss: 2.14015961 d_loss: 1.39907348, g_loss: 0.68899953, ae_loss: 0.05208674\n",
      "Step: [5251] total_loss: 2.12823892 d_loss: 1.38015211, g_loss: 0.69356370, ae_loss: 0.05452307\n",
      "Step: [5252] total_loss: 2.11819959 d_loss: 1.38137102, g_loss: 0.68641341, ae_loss: 0.05041511\n",
      "Step: [5253] total_loss: 2.12311935 d_loss: 1.39017653, g_loss: 0.68564582, ae_loss: 0.04729702\n",
      "Step: [5254] total_loss: 2.14398479 d_loss: 1.39421320, g_loss: 0.69746679, ae_loss: 0.05230473\n",
      "Step: [5255] total_loss: 2.09973335 d_loss: 1.37317443, g_loss: 0.67700595, ae_loss: 0.04955303\n",
      "Step: [5256] total_loss: 2.11257076 d_loss: 1.37492228, g_loss: 0.68898356, ae_loss: 0.04866498\n",
      "Step: [5257] total_loss: 2.12474108 d_loss: 1.39117014, g_loss: 0.68519098, ae_loss: 0.04838001\n",
      "Step: [5258] total_loss: 2.13114595 d_loss: 1.38899231, g_loss: 0.69257444, ae_loss: 0.04957932\n",
      "Step: [5259] total_loss: 2.15471077 d_loss: 1.39808989, g_loss: 0.70871276, ae_loss: 0.04790809\n",
      "Step: [5260] total_loss: 2.11161327 d_loss: 1.36564636, g_loss: 0.69476867, ae_loss: 0.05119841\n",
      "Step: [5261] total_loss: 2.12885857 d_loss: 1.38210022, g_loss: 0.69557130, ae_loss: 0.05118712\n",
      "Step: [5262] total_loss: 2.13777637 d_loss: 1.37967157, g_loss: 0.70652819, ae_loss: 0.05157654\n",
      "Step: [5263] total_loss: 2.11310673 d_loss: 1.38060975, g_loss: 0.68198884, ae_loss: 0.05050805\n",
      "Step: [5264] total_loss: 2.12444448 d_loss: 1.38135874, g_loss: 0.69226885, ae_loss: 0.05081706\n",
      "Step: [5265] total_loss: 2.13207340 d_loss: 1.38410330, g_loss: 0.70128489, ae_loss: 0.04668507\n",
      "Step: [5266] total_loss: 2.14593148 d_loss: 1.38654375, g_loss: 0.71022350, ae_loss: 0.04916422\n",
      "Step: [5267] total_loss: 2.12840509 d_loss: 1.37352538, g_loss: 0.70533818, ae_loss: 0.04954148\n",
      "Step: [5268] total_loss: 2.13055992 d_loss: 1.39189601, g_loss: 0.69052607, ae_loss: 0.04813796\n",
      "Step: [5269] total_loss: 2.13737488 d_loss: 1.39398730, g_loss: 0.69364965, ae_loss: 0.04973783\n",
      "Step: [5270] total_loss: 2.11170864 d_loss: 1.36035395, g_loss: 0.70153236, ae_loss: 0.04982251\n",
      "Step: [5271] total_loss: 2.11108112 d_loss: 1.38302219, g_loss: 0.67736304, ae_loss: 0.05069593\n",
      "Step: [5272] total_loss: 2.12190056 d_loss: 1.38435888, g_loss: 0.68376720, ae_loss: 0.05377435\n",
      "Step: [5273] total_loss: 2.12539148 d_loss: 1.39049625, g_loss: 0.68431878, ae_loss: 0.05057643\n",
      "Step: [5274] total_loss: 2.14377379 d_loss: 1.38628912, g_loss: 0.70710433, ae_loss: 0.05038037\n",
      "Step: [5275] total_loss: 2.13417220 d_loss: 1.37852073, g_loss: 0.70661044, ae_loss: 0.04904100\n",
      "Step: [5276] total_loss: 2.12849712 d_loss: 1.38496256, g_loss: 0.69475818, ae_loss: 0.04877634\n",
      "Step: [5277] total_loss: 2.13812876 d_loss: 1.38252497, g_loss: 0.70475578, ae_loss: 0.05084790\n",
      "Step: [5278] total_loss: 2.12474537 d_loss: 1.37444699, g_loss: 0.69667304, ae_loss: 0.05362542\n",
      "Step: [5279] total_loss: 2.13692880 d_loss: 1.39283121, g_loss: 0.69592321, ae_loss: 0.04817435\n",
      "Step: [5280] total_loss: 2.12826705 d_loss: 1.38159227, g_loss: 0.69263464, ae_loss: 0.05404011\n",
      "Step: [5281] total_loss: 2.13076639 d_loss: 1.40076518, g_loss: 0.67717600, ae_loss: 0.05282510\n",
      "Step: [5282] total_loss: 2.10188198 d_loss: 1.37436604, g_loss: 0.67747891, ae_loss: 0.05003700\n",
      "Step: [5283] total_loss: 2.12582922 d_loss: 1.38549423, g_loss: 0.68801236, ae_loss: 0.05232266\n",
      "Step: [5284] total_loss: 2.11157417 d_loss: 1.36382365, g_loss: 0.69828486, ae_loss: 0.04946561\n",
      "Step: [5285] total_loss: 2.12241554 d_loss: 1.39570105, g_loss: 0.67784071, ae_loss: 0.04887395\n",
      "Step: [5286] total_loss: 2.12597179 d_loss: 1.39633620, g_loss: 0.68369508, ae_loss: 0.04594047\n",
      "Step: [5287] total_loss: 2.13898277 d_loss: 1.38999844, g_loss: 0.69726902, ae_loss: 0.05171531\n",
      "Step: [5288] total_loss: 2.13265777 d_loss: 1.39302754, g_loss: 0.68776989, ae_loss: 0.05186036\n",
      "Step: [5289] total_loss: 2.12859249 d_loss: 1.38373506, g_loss: 0.69190103, ae_loss: 0.05295644\n",
      "Step: [5290] total_loss: 2.12222052 d_loss: 1.37475634, g_loss: 0.69453424, ae_loss: 0.05292984\n",
      "Step: [5291] total_loss: 2.11525726 d_loss: 1.37692046, g_loss: 0.68762946, ae_loss: 0.05070726\n",
      "Step: [5292] total_loss: 2.11852145 d_loss: 1.38153195, g_loss: 0.68685079, ae_loss: 0.05013868\n",
      "Step: [5293] total_loss: 2.11446929 d_loss: 1.38165689, g_loss: 0.68310350, ae_loss: 0.04970881\n",
      "Step: [5294] total_loss: 2.13166380 d_loss: 1.38029718, g_loss: 0.69904846, ae_loss: 0.05231811\n",
      "Step: [5295] total_loss: 2.13519979 d_loss: 1.38033402, g_loss: 0.70359790, ae_loss: 0.05126791\n",
      "Step: [5296] total_loss: 2.14130974 d_loss: 1.40380859, g_loss: 0.69121563, ae_loss: 0.04628542\n",
      "Step: [5297] total_loss: 2.12697077 d_loss: 1.38457155, g_loss: 0.69114101, ae_loss: 0.05125831\n",
      "Step: [5298] total_loss: 2.12389398 d_loss: 1.38660395, g_loss: 0.68595445, ae_loss: 0.05133558\n",
      "Step: [5299] total_loss: 2.13660192 d_loss: 1.39421988, g_loss: 0.69296789, ae_loss: 0.04941411\n",
      "Step: [5300] total_loss: 2.12570572 d_loss: 1.38398051, g_loss: 0.69270909, ae_loss: 0.04901606\n",
      "Step: [5301] total_loss: 2.13708830 d_loss: 1.38195848, g_loss: 0.70802224, ae_loss: 0.04710770\n",
      "Step: [5302] total_loss: 2.13382483 d_loss: 1.38963175, g_loss: 0.69485211, ae_loss: 0.04934088\n",
      "Step: [5303] total_loss: 2.14544821 d_loss: 1.40077686, g_loss: 0.69234920, ae_loss: 0.05232203\n",
      "Step: [5304] total_loss: 2.14435554 d_loss: 1.39401233, g_loss: 0.69575876, ae_loss: 0.05458444\n",
      "Step: [5305] total_loss: 2.13119864 d_loss: 1.38329947, g_loss: 0.69781315, ae_loss: 0.05008599\n",
      "Step: [5306] total_loss: 2.14805841 d_loss: 1.38752294, g_loss: 0.70640159, ae_loss: 0.05413393\n",
      "Step: [5307] total_loss: 2.13476562 d_loss: 1.39476550, g_loss: 0.69168782, ae_loss: 0.04831222\n",
      "Step: [5308] total_loss: 2.11862421 d_loss: 1.36419356, g_loss: 0.70630622, ae_loss: 0.04812429\n",
      "Step: [5309] total_loss: 2.13700676 d_loss: 1.39139485, g_loss: 0.69249690, ae_loss: 0.05311507\n",
      "Step: [5310] total_loss: 2.11663389 d_loss: 1.38102591, g_loss: 0.68751216, ae_loss: 0.04809569\n",
      "Step: [5311] total_loss: 2.13797712 d_loss: 1.39926279, g_loss: 0.68659103, ae_loss: 0.05212337\n",
      "Step: [5312] total_loss: 2.11129260 d_loss: 1.37006903, g_loss: 0.69377995, ae_loss: 0.04744361\n",
      "Step: [5313] total_loss: 2.11722851 d_loss: 1.37988257, g_loss: 0.69353777, ae_loss: 0.04380818\n",
      "Step: [5314] total_loss: 2.11663818 d_loss: 1.37626445, g_loss: 0.69126713, ae_loss: 0.04910655\n",
      "Step: [5315] total_loss: 2.14500904 d_loss: 1.39304197, g_loss: 0.70333934, ae_loss: 0.04862779\n",
      "Step: [5316] total_loss: 2.13268995 d_loss: 1.39257360, g_loss: 0.69226491, ae_loss: 0.04785158\n",
      "Step: [5317] total_loss: 2.12267065 d_loss: 1.36601806, g_loss: 0.70879388, ae_loss: 0.04785854\n",
      "Step: [5318] total_loss: 2.12233996 d_loss: 1.37786460, g_loss: 0.69509149, ae_loss: 0.04938391\n",
      "Step: [5319] total_loss: 2.12884998 d_loss: 1.39479303, g_loss: 0.68389904, ae_loss: 0.05015799\n",
      "Step: [5320] total_loss: 2.13557696 d_loss: 1.38085413, g_loss: 0.70255828, ae_loss: 0.05216451\n",
      "Step: [5321] total_loss: 2.14206171 d_loss: 1.38765240, g_loss: 0.70245481, ae_loss: 0.05195457\n",
      "Step: [5322] total_loss: 2.14731646 d_loss: 1.39420688, g_loss: 0.70169985, ae_loss: 0.05140967\n",
      "Step: [5323] total_loss: 2.13937879 d_loss: 1.38924754, g_loss: 0.69950545, ae_loss: 0.05062585\n",
      "Step: [5324] total_loss: 2.11202621 d_loss: 1.37195039, g_loss: 0.68926907, ae_loss: 0.05080687\n",
      "Step: [5325] total_loss: 2.13034868 d_loss: 1.39414358, g_loss: 0.68478554, ae_loss: 0.05141951\n",
      "Step: [5326] total_loss: 2.11271095 d_loss: 1.37433457, g_loss: 0.69282675, ae_loss: 0.04554955\n",
      "Step: [5327] total_loss: 2.12564564 d_loss: 1.38819683, g_loss: 0.68714726, ae_loss: 0.05030138\n",
      "Step: [5328] total_loss: 2.11501575 d_loss: 1.38206971, g_loss: 0.68268937, ae_loss: 0.05025662\n",
      "Step: [5329] total_loss: 2.11811495 d_loss: 1.38861489, g_loss: 0.68228245, ae_loss: 0.04721753\n",
      "Step: [5330] total_loss: 2.11731529 d_loss: 1.38407171, g_loss: 0.68642169, ae_loss: 0.04682201\n",
      "Step: [5331] total_loss: 2.14727831 d_loss: 1.40336037, g_loss: 0.69431376, ae_loss: 0.04960430\n",
      "Step: [5332] total_loss: 2.12307739 d_loss: 1.39502358, g_loss: 0.68016958, ae_loss: 0.04788421\n",
      "Step: [5333] total_loss: 2.12125206 d_loss: 1.37820661, g_loss: 0.69025481, ae_loss: 0.05279056\n",
      "Step: [5334] total_loss: 2.12337971 d_loss: 1.37989402, g_loss: 0.69555092, ae_loss: 0.04793463\n",
      "Step: [5335] total_loss: 2.11832428 d_loss: 1.39382613, g_loss: 0.67700267, ae_loss: 0.04749535\n",
      "Step: [5336] total_loss: 2.12609720 d_loss: 1.39220285, g_loss: 0.68549067, ae_loss: 0.04840358\n",
      "Step: [5337] total_loss: 2.13195229 d_loss: 1.38570881, g_loss: 0.69119591, ae_loss: 0.05504769\n",
      "Step: [5338] total_loss: 2.12660861 d_loss: 1.38153172, g_loss: 0.69757789, ae_loss: 0.04749900\n",
      "Step: [5339] total_loss: 2.13368320 d_loss: 1.38201690, g_loss: 0.70158464, ae_loss: 0.05008157\n",
      "Step: [5340] total_loss: 2.11535120 d_loss: 1.37821531, g_loss: 0.68626153, ae_loss: 0.05087428\n",
      "Step: [5341] total_loss: 2.13342571 d_loss: 1.39037609, g_loss: 0.69076669, ae_loss: 0.05228291\n",
      "Step: [5342] total_loss: 2.12187862 d_loss: 1.38126516, g_loss: 0.68953073, ae_loss: 0.05108261\n",
      "Step: [5343] total_loss: 2.12430358 d_loss: 1.37914157, g_loss: 0.69231308, ae_loss: 0.05284893\n",
      "Step: [5344] total_loss: 2.13095832 d_loss: 1.39292383, g_loss: 0.68725240, ae_loss: 0.05078209\n",
      "Step: [5345] total_loss: 2.13567448 d_loss: 1.39489794, g_loss: 0.69062310, ae_loss: 0.05015337\n",
      "Step: [5346] total_loss: 2.12480211 d_loss: 1.37650633, g_loss: 0.69957685, ae_loss: 0.04871877\n",
      "Step: [5347] total_loss: 2.11273384 d_loss: 1.36903191, g_loss: 0.69431567, ae_loss: 0.04938617\n",
      "Step: [5348] total_loss: 2.12547874 d_loss: 1.38546085, g_loss: 0.69061363, ae_loss: 0.04940429\n",
      "Step: [5349] total_loss: 2.13077831 d_loss: 1.37992084, g_loss: 0.69928461, ae_loss: 0.05157295\n",
      "Step: [5350] total_loss: 2.13316965 d_loss: 1.38126373, g_loss: 0.69790351, ae_loss: 0.05400254\n",
      "Step: [5351] total_loss: 2.11683893 d_loss: 1.37850189, g_loss: 0.68798983, ae_loss: 0.05034716\n",
      "Step: [5352] total_loss: 2.13800597 d_loss: 1.38800621, g_loss: 0.69741231, ae_loss: 0.05258736\n",
      "Step: [5353] total_loss: 2.12382793 d_loss: 1.37863755, g_loss: 0.69129527, ae_loss: 0.05389509\n",
      "Step: [5354] total_loss: 2.12009430 d_loss: 1.37824643, g_loss: 0.69272685, ae_loss: 0.04912115\n",
      "Step: [5355] total_loss: 2.13703537 d_loss: 1.39757299, g_loss: 0.69159842, ae_loss: 0.04786401\n",
      "Step: [5356] total_loss: 2.12162590 d_loss: 1.38423407, g_loss: 0.68871164, ae_loss: 0.04868032\n",
      "Step: [5357] total_loss: 2.11374736 d_loss: 1.36093712, g_loss: 0.70279771, ae_loss: 0.05001253\n",
      "Step: [5358] total_loss: 2.12941837 d_loss: 1.38715351, g_loss: 0.68750656, ae_loss: 0.05475825\n",
      "Step: [5359] total_loss: 2.13327169 d_loss: 1.38148451, g_loss: 0.70127916, ae_loss: 0.05050797\n",
      "Step: [5360] total_loss: 2.12832308 d_loss: 1.38965988, g_loss: 0.68867195, ae_loss: 0.04999110\n",
      "Step: [5361] total_loss: 2.12691140 d_loss: 1.38297081, g_loss: 0.69134200, ae_loss: 0.05259859\n",
      "Step: [5362] total_loss: 2.13070059 d_loss: 1.39511621, g_loss: 0.68392313, ae_loss: 0.05166123\n",
      "Step: [5363] total_loss: 2.12865639 d_loss: 1.39117801, g_loss: 0.68703818, ae_loss: 0.05044018\n",
      "Step: [5364] total_loss: 2.13011479 d_loss: 1.39583135, g_loss: 0.68460608, ae_loss: 0.04967739\n",
      "Step: [5365] total_loss: 2.12977648 d_loss: 1.38655972, g_loss: 0.69199955, ae_loss: 0.05121733\n",
      "Step: [5366] total_loss: 2.13906288 d_loss: 1.38353550, g_loss: 0.70559692, ae_loss: 0.04993052\n",
      "Step: [5367] total_loss: 2.14310288 d_loss: 1.39674258, g_loss: 0.69486958, ae_loss: 0.05149072\n",
      "Step: [5368] total_loss: 2.11611700 d_loss: 1.37771034, g_loss: 0.69105351, ae_loss: 0.04735322\n",
      "Step: [5369] total_loss: 2.11782146 d_loss: 1.37440372, g_loss: 0.69347453, ae_loss: 0.04994324\n",
      "Step: [5370] total_loss: 2.14540482 d_loss: 1.39262938, g_loss: 0.69994533, ae_loss: 0.05283008\n",
      "Step: [5371] total_loss: 2.13169909 d_loss: 1.37655663, g_loss: 0.70476925, ae_loss: 0.05037331\n",
      "Step: [5372] total_loss: 2.11428833 d_loss: 1.37172389, g_loss: 0.69438481, ae_loss: 0.04817979\n",
      "Step: [5373] total_loss: 2.12294388 d_loss: 1.38216543, g_loss: 0.69163001, ae_loss: 0.04914850\n",
      "Step: [5374] total_loss: 2.11534595 d_loss: 1.37505865, g_loss: 0.68908256, ae_loss: 0.05120471\n",
      "Step: [5375] total_loss: 2.12255645 d_loss: 1.37658501, g_loss: 0.69820142, ae_loss: 0.04777003\n",
      "Step: [5376] total_loss: 2.11835551 d_loss: 1.39358115, g_loss: 0.67466146, ae_loss: 0.05011294\n",
      "Step: [5377] total_loss: 2.11252165 d_loss: 1.37080538, g_loss: 0.69286227, ae_loss: 0.04885397\n",
      "Step: [5378] total_loss: 2.10926723 d_loss: 1.36247218, g_loss: 0.69940436, ae_loss: 0.04739064\n",
      "Step: [5379] total_loss: 2.12125468 d_loss: 1.39252019, g_loss: 0.68137389, ae_loss: 0.04736052\n",
      "Step: [5380] total_loss: 2.12308431 d_loss: 1.39984894, g_loss: 0.67708403, ae_loss: 0.04615131\n",
      "Step: [5381] total_loss: 2.13675785 d_loss: 1.39016271, g_loss: 0.69741958, ae_loss: 0.04917552\n",
      "Step: [5382] total_loss: 2.13339901 d_loss: 1.40005112, g_loss: 0.68376535, ae_loss: 0.04958265\n",
      "Step: [5383] total_loss: 2.15417933 d_loss: 1.42076993, g_loss: 0.68128407, ae_loss: 0.05212536\n",
      "Step: [5384] total_loss: 2.13087797 d_loss: 1.39830279, g_loss: 0.68219525, ae_loss: 0.05037995\n",
      "Step: [5385] total_loss: 2.12414932 d_loss: 1.38219345, g_loss: 0.69174397, ae_loss: 0.05021198\n",
      "Step: [5386] total_loss: 2.11560726 d_loss: 1.37845087, g_loss: 0.68788898, ae_loss: 0.04926743\n",
      "Step: [5387] total_loss: 2.12416768 d_loss: 1.37484479, g_loss: 0.69900978, ae_loss: 0.05031312\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={keep_prob : 0.9})\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.astype(np.float32)\n",
    "    #batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    \n",
    "    '''get the noise data'''\n",
    "    if prior_type =='mixGaussian':\n",
    "        z_id_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = gaussian_mixture(batch_size, z_dim, label_indices=z_id_rand)\n",
    "    elif prior_type == 'swiss_roll':\n",
    "        z_id_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = swiss_roll(batch_size, z_dim, label_indices=z_id_rand)\n",
    "    elif prior_type == 'normal':\n",
    "        batch_z, z_id_rand = gaussian(batch_size, z_dim, use_label_info=True)    \n",
    "    else:\n",
    "        raise Exception(f\"[!] There is no option for {prior_type}\" )\n",
    "\n",
    "    z_id_one_hot_vector = np.zeros((batch_size, y_dim))\n",
    "    z_id_one_hot_vector[np.arange(batch_size), z_id_rand] = 1\n",
    "    \n",
    "    feed_dict_input = {\n",
    "        x_hat:batch_images,\n",
    "        x:batch_images,\n",
    "        x_id:batch_labels,\n",
    "        z_sample:batch_z,\n",
    "        z_id:z_id_one_hot_vector,\n",
    "        keep_prob: 0.9\n",
    "    }\n",
    "    '''update AE network''' \n",
    "    _, loss_likehood = sess.run([ae_optim, neg_marginal_likelihood], feed_dict=feed_dict_input)\n",
    "    '''update discriminator network'''\n",
    "    _, d_loss = sess.run([d_optim, D_loss], feed_dict=feed_dict_input)\n",
    "    '''update generator network, run 2 times'''\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "     \n",
    "    total_loss = loss_likehood + d_loss + g_loss\n",
    "    # display training status\n",
    "    print(\"Step: [%d] total_loss: %.8f d_loss: %.8f, g_loss: %.8f, ae_loss: %.8f\" % (step_ind, total_loss, d_loss, g_loss, loss_likehood) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "        samples = sess.run(fake_images, feed_dict={x_hat:test_batch_images, keep_prob:1})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :].reshape(image_dims[:2])\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
