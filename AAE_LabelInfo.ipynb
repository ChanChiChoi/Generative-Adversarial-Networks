{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the network structure borrow from https://github.com/hwalsuklee/tensorflow-mnist-AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from math import sin,cos,sqrt\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net,wscope,bscope,output_depth):\n",
    "    w_init = tf.contrib.layers.xavier_initializer()\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], initializer=w_init)\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=b_init)\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "    \n",
    "    \n",
    "def display(samples, batch_size, img_h, img_w, image_dims, step_ind, plot='reproduce'):\n",
    "    \n",
    "    if plot == 'reproduce':\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*img_h, col*img_w] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*img_w:(i+1)*img_w, j*img_h:(j+1)*img_h] = samples[i*col+j, :].reshape(image_dims[:2])\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)   \n",
    "        \n",
    "    elif plot == 'mainfold learning':\n",
    "        row = col = int(np.sqrt(samples.shape[0]))\n",
    "        img = np.zeros( [row*img_h, col*img_w] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*img_w:(i+1)*img_w, j*img_h:(j+1)*img_h] = samples[i*col+j, :].reshape(image_dims[:2])\n",
    "        scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Distribution Functions\n",
    "here define four prior distribution, which will impose onto the hidden code vector of AAE.   \n",
    "Most codes from https://github.com/musyoku/adversarial-autoencoder/blob/master/aae/sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def uniform(batch_size, n_dim, n_labels=10, minv=-1, maxv=1, label_indices=None):\n",
    "    '''if label_indices is None, then uniform will call numpy.random.uniform()'''\n",
    "    if label_indices is not None:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(label, n_labels):\n",
    "            num = int(np.ceil(np.sqrt(n_labels)))\n",
    "            size = (maxv-minv)*1.0/num\n",
    "            x, y = np.random.uniform(-size/2, size/2, (2,))\n",
    "            i = label / num\n",
    "            j = label % num\n",
    "            x += j*size+minv+0.5*size\n",
    "            y += i*size+minv+0.5*size\n",
    "            return np.array([x, y]).reshape((2,))\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "    else:\n",
    "        z = np.random.uniform(minv, maxv, (batch_size, n_dim)).astype(np.float32)\n",
    "    return z\n",
    "\n",
    "def gaussian(batch_size, n_dim, mean=0, var=1, n_labels=10, use_label_info=False):\n",
    "    if use_label_info:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(n_labels):\n",
    "            x, y = np.random.normal(mean, var, (2,))\n",
    "            angle = np.angle((x-mean) + 1j*(y-mean), deg=True)\n",
    "            dist = np.sqrt((x-mean)**2+(y-mean)**2)\n",
    "\n",
    "            # label 0\n",
    "            if dist <1.0:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = ((int)((n_labels-1)*angle))//360\n",
    "\n",
    "                if label<0:\n",
    "                    label+=n_labels-1\n",
    "\n",
    "                label += 1\n",
    "\n",
    "            return np.array([x, y]).reshape((2,)), label\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        z_id = np.empty((batch_size), dtype=np.int32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    a_sample, a_label = sample(n_labels)\n",
    "                    z[batch, zi*2:zi*2+2] = a_sample\n",
    "                    z_id[batch] = a_label\n",
    "        return z, z_id\n",
    "    else:\n",
    "        z = np.random.normal(mean, var, (batch_size, n_dim)).astype(np.float32)\n",
    "        return z\n",
    "\n",
    "def gaussian_mixture(batch_size, n_dim=2, n_labels=10, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z\n",
    "\n",
    "def swiss_roll(batch_size, n_dim=2, n_labels=10, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(label, n_labels):\n",
    "        uni = np.random.uniform(0.0, 1.0) / float(n_labels) + float(label) / float(n_labels)\n",
    "        r = sqrt(uni) * 3.0\n",
    "        rad = np.pi * 4.0 * sqrt(uni)\n",
    "        x = r * cos(rad)\n",
    "        y = r * sin(rad)\n",
    "        return np.array([x, y]).reshape((2,))\n",
    "\n",
    "    z = np.zeros((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(np.random.randint(0, n_labels), n_labels)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the discriminator part of the AAE is one MLP network, two hidden fully connected layers, which input is [batch_size, z] matrix  \n",
    "you can find 'discriminator' structure like AE_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout''' \n",
    "        net = linear(z, 'd_wlinear0', 'd_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'd_wlinear1', 'd_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'd_wlinear2', 'd_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_encoder(x, n_hidden, n_output, keep_prob, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"AE_encoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout'''\n",
    "        net = linear(x, 'aeE_wlinear0', 'aeE_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeE_wlinear1', 'aeE_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        z = out_logit = linear(net, 'aeE_wlinear2', 'aeE_blinear2', n_output)\n",
    "\n",
    "    return z\n",
    "\n",
    "def AE_decoder(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"AE_decoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropput'''\n",
    "        net = linear(z, 'aeD_wlinear0', 'aeD_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeD_wlinear1', 'aeD_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'aeD_wlinear2', 'aeD_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        x_pred = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, img_dim, n_hidden):\n",
    "\n",
    "    y = AE_decoder(z, n_hidden, img_dim, 1.0, reuse=True)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "results_path = './result'\n",
    "n_hidden = 1000  # number of hidden units in MLP\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.5\n",
    "n_epochs = 20\n",
    "prior_type = 'mixGaussian' # one of ['mixGaussian', 'swiss_roll', 'normal']\n",
    "batch_size = 64\n",
    "min_tot_loss = 1e99\n",
    "\n",
    "\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "image_dims = [img_w, img_h, 1]\n",
    "img_dim = reduce(lambda x,y:x*y, image_dims)\n",
    "z_dim = 2\n",
    "y_dim = 10\n",
    "\n",
    "# for mainfold learning\n",
    "n_img_x = 20 \n",
    "n_img_y = 20 \n",
    "resize_factor = 1.0\n",
    "z_range = 4\n",
    "\n",
    "plot = 'mainfold learning' # 'reproduce' for reproduce performance; 'mainfold learning' for manifold learning result\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# In denoising-autoencoder, x_hat == x + noise; otherwise x_hat == x\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, img_dim], name='input_img')\n",
    "x_target = tf.placeholder(tf.float32, shape=[None, img_dim], name='target_img')\n",
    "x_label = tf.placeholder(tf.float32, shape=[None, y_dim], name='input_img_label')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# input for PMLR\n",
    "z_hidden = tf.placeholder(tf.float32, shape=[None, z_dim], name='latent_variable')\n",
    "\n",
    "\n",
    "# samples drawn from prior distribution\n",
    "z_sample = tf.placeholder(tf.float32, shape=[None, z_dim], name='prior_sample')\n",
    "z_label = tf.placeholder(tf.float32, shape=[None, y_dim], name='prior_sample_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "'''Reconstruction Loss''' \n",
    "\n",
    "# encoding\n",
    "z_pred = AE_encoder(x_input, n_hidden, z_dim, keep_prob)\n",
    "# decoding\n",
    "x_pred = AE_decoder(z_pred, n_hidden, img_dim, keep_prob)\n",
    "# loss\n",
    "marginal_likelihood = -tf.reduce_mean(tf.reduce_mean(tf.squared_difference(x_input, x_pred)))\n",
    "\n",
    "'''GAN Loss'''\n",
    "''' ** label information incorporating into AE's z_hidden   '''\n",
    "z_real = tf.concat([z_sample, z_label],1)\n",
    "z_fake = tf.concat([z_pred, x_label],1)\n",
    "\n",
    "D_real, D_real_logits = discriminator(z_real, n_hidden, 1, keep_prob)\n",
    "D_fake, D_fake_logits = discriminator(z_fake, n_hidden, 1, keep_prob, reuse=True)\n",
    "\n",
    "# discriminator loss\n",
    "D_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n",
    "D_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n",
    "D_loss = D_loss_real+D_loss_fake\n",
    "\n",
    "# generator loss\n",
    "G_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))\n",
    "\n",
    "marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "D_loss = tf.reduce_mean(D_loss)\n",
    "G_loss = tf.reduce_mean(G_loss)\n",
    "\n",
    "neg_marginal_likelihood = -1*marginal_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing.  \n",
    "the z_pmlr var defintion borrowed from  https://github.com/fastforwardlabs/vae-tf/blob/master/plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G and a group for ae\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "g_vars = [var for var in t_vars if 'AE_encoder' in var.name]\n",
    "ae_vars = [var for var in t_vars if 'AE_encoder' in var.name or 'AE_decoder' in var.name]\n",
    "\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate/5, beta1=beta1).minimize(D_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(G_loss, var_list=g_vars)\n",
    "ae_optim = tf.train.AdamOptimizer(learning_rate).minimize(neg_marginal_likelihood, var_list=ae_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "\n",
    "# for test\n",
    "test_image_batch = mnist.test.next_batch(batch_size)\n",
    "test_batch_images,test_batch_labels = test_image_batch\n",
    "test_batch_images = test_batch_images.astype(np.float32)\n",
    "images_reconstruction = x_pred\n",
    "\n",
    "fake_images = generator(z_hidden, img_dim, n_hidden)\n",
    "\n",
    "# sample the prior mainflod learning of 10 class digit for z_hidden \n",
    "z_pmlr = np.rollaxis(np.mgrid[z_range:-z_range:n_img_y * 1j, z_range:-z_range:n_img_x * 1j], 0, 3) \n",
    "# np.rollaxis. here replace 0 axis wiht 3, for example: [channel, height,width] -> [height, width, channel]\n",
    "z_pmlr = z_pmlr.reshape([-1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] total_loss: 2.27858686 d_loss: 1.40255046, g_loss: 0.64523226, ae_loss: 0.23080412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1] total_loss: 2.53858209 d_loss: 1.71428442, g_loss: 0.60495067, ae_loss: 0.21934699\n",
      "Step: [2] total_loss: 3.18473005 d_loss: 1.74460757, g_loss: 1.29010224, ae_loss: 0.15002027\n",
      "Step: [3] total_loss: 2.28269935 d_loss: 1.15749216, g_loss: 0.98871946, ae_loss: 0.13648768\n",
      "Step: [4] total_loss: 2.24416399 d_loss: 1.29406357, g_loss: 0.81240976, ae_loss: 0.13769062\n",
      "Step: [5] total_loss: 2.33765125 d_loss: 1.40313613, g_loss: 0.80370128, ae_loss: 0.13081397\n",
      "Step: [6] total_loss: 2.33879566 d_loss: 1.41111600, g_loss: 0.80022156, ae_loss: 0.12745799\n",
      "Step: [7] total_loss: 2.29586673 d_loss: 1.38092971, g_loss: 0.78103244, ae_loss: 0.13390456\n",
      "Step: [8] total_loss: 2.29709148 d_loss: 1.38172817, g_loss: 0.76809430, ae_loss: 0.14726895\n",
      "Step: [9] total_loss: 2.30652976 d_loss: 1.44203389, g_loss: 0.72099227, ae_loss: 0.14350358\n",
      "Step: [10] total_loss: 2.34961319 d_loss: 1.54429674, g_loss: 0.67828763, ae_loss: 0.12702896\n",
      "Step: [11] total_loss: 2.39901567 d_loss: 1.48721147, g_loss: 0.82684857, ae_loss: 0.08495574\n",
      "Step: [12] total_loss: 2.34323120 d_loss: 1.41605973, g_loss: 0.84077734, ae_loss: 0.08639416\n",
      "Step: [13] total_loss: 2.41280031 d_loss: 1.41716564, g_loss: 0.85626709, ae_loss: 0.13936758\n",
      "Step: [14] total_loss: 2.31667948 d_loss: 1.33911657, g_loss: 0.87475252, ae_loss: 0.10281043\n",
      "Step: [15] total_loss: 2.32850218 d_loss: 1.41784525, g_loss: 0.80786204, ae_loss: 0.10279486\n",
      "Step: [16] total_loss: 2.31893802 d_loss: 1.41092920, g_loss: 0.79386902, ae_loss: 0.11413981\n",
      "Step: [17] total_loss: 2.29380941 d_loss: 1.44417679, g_loss: 0.76171732, ae_loss: 0.08791524\n",
      "Step: [18] total_loss: 2.29124045 d_loss: 1.42274296, g_loss: 0.77204204, ae_loss: 0.09645542\n",
      "Step: [19] total_loss: 2.27173781 d_loss: 1.43040133, g_loss: 0.75661528, ae_loss: 0.08472118\n",
      "Step: [20] total_loss: 2.26426172 d_loss: 1.42969584, g_loss: 0.75675911, ae_loss: 0.07780667\n",
      "Step: [21] total_loss: 2.24860501 d_loss: 1.42359424, g_loss: 0.74714893, ae_loss: 0.07786184\n",
      "Step: [22] total_loss: 2.23527670 d_loss: 1.42694271, g_loss: 0.73511869, ae_loss: 0.07321541\n",
      "Step: [23] total_loss: 2.23130846 d_loss: 1.41687214, g_loss: 0.74328196, ae_loss: 0.07115444\n",
      "Step: [24] total_loss: 2.20680857 d_loss: 1.38699174, g_loss: 0.74545670, ae_loss: 0.07436030\n",
      "Step: [25] total_loss: 2.19698095 d_loss: 1.39711392, g_loss: 0.72959733, ae_loss: 0.07026981\n",
      "Step: [26] total_loss: 2.18167019 d_loss: 1.40078115, g_loss: 0.71022749, ae_loss: 0.07066171\n",
      "Step: [27] total_loss: 2.19622564 d_loss: 1.38298464, g_loss: 0.73990297, ae_loss: 0.07333791\n",
      "Step: [28] total_loss: 2.17283773 d_loss: 1.38064218, g_loss: 0.71845841, ae_loss: 0.07373720\n",
      "Step: [29] total_loss: 2.15658545 d_loss: 1.38984132, g_loss: 0.70013589, ae_loss: 0.06660824\n",
      "Step: [30] total_loss: 2.16510868 d_loss: 1.38935077, g_loss: 0.70784277, ae_loss: 0.06791524\n",
      "Step: [31] total_loss: 2.14455652 d_loss: 1.37926197, g_loss: 0.69156778, ae_loss: 0.07372663\n",
      "Step: [32] total_loss: 2.14088845 d_loss: 1.37531137, g_loss: 0.69345671, ae_loss: 0.07212035\n",
      "Step: [33] total_loss: 2.14548969 d_loss: 1.39272237, g_loss: 0.67753083, ae_loss: 0.07523655\n",
      "Step: [34] total_loss: 2.12651253 d_loss: 1.36416388, g_loss: 0.69417834, ae_loss: 0.06817026\n",
      "Step: [35] total_loss: 2.14362431 d_loss: 1.39183772, g_loss: 0.68407583, ae_loss: 0.06771085\n",
      "Step: [36] total_loss: 2.12469196 d_loss: 1.36552882, g_loss: 0.69484246, ae_loss: 0.06432063\n",
      "Step: [37] total_loss: 2.12135577 d_loss: 1.35709453, g_loss: 0.69028258, ae_loss: 0.07397869\n",
      "Step: [38] total_loss: 2.14313269 d_loss: 1.37550902, g_loss: 0.69550550, ae_loss: 0.07211830\n",
      "Step: [39] total_loss: 2.12553787 d_loss: 1.36586666, g_loss: 0.68432605, ae_loss: 0.07534533\n",
      "Step: [40] total_loss: 2.10265136 d_loss: 1.35344303, g_loss: 0.68104589, ae_loss: 0.06816247\n",
      "Step: [41] total_loss: 2.13623857 d_loss: 1.37621081, g_loss: 0.68598449, ae_loss: 0.07404327\n",
      "Step: [42] total_loss: 2.12200451 d_loss: 1.35919261, g_loss: 0.68798041, ae_loss: 0.07483154\n",
      "Step: [43] total_loss: 2.11839342 d_loss: 1.35999942, g_loss: 0.69236249, ae_loss: 0.06603155\n",
      "Step: [44] total_loss: 2.10132456 d_loss: 1.34386325, g_loss: 0.69137871, ae_loss: 0.06608272\n",
      "Step: [45] total_loss: 2.11291361 d_loss: 1.34500492, g_loss: 0.69754493, ae_loss: 0.07036377\n",
      "Step: [46] total_loss: 2.10318422 d_loss: 1.35302436, g_loss: 0.68018210, ae_loss: 0.06997789\n",
      "Step: [47] total_loss: 2.12408543 d_loss: 1.37268519, g_loss: 0.67866468, ae_loss: 0.07273559\n",
      "Step: [48] total_loss: 2.11942101 d_loss: 1.35331893, g_loss: 0.69312453, ae_loss: 0.07297762\n",
      "Step: [49] total_loss: 2.11655068 d_loss: 1.36215448, g_loss: 0.68630564, ae_loss: 0.06809054\n",
      "Step: [50] total_loss: 2.11775470 d_loss: 1.35744607, g_loss: 0.68942314, ae_loss: 0.07088544\n",
      "Step: [51] total_loss: 2.12163115 d_loss: 1.36514640, g_loss: 0.68687475, ae_loss: 0.06960993\n",
      "Step: [52] total_loss: 2.11447763 d_loss: 1.37201142, g_loss: 0.67216754, ae_loss: 0.07029866\n",
      "Step: [53] total_loss: 2.11181974 d_loss: 1.34872782, g_loss: 0.69180560, ae_loss: 0.07128645\n",
      "Step: [54] total_loss: 2.10460234 d_loss: 1.35857165, g_loss: 0.68285799, ae_loss: 0.06317269\n",
      "Step: [55] total_loss: 2.10546184 d_loss: 1.36519492, g_loss: 0.67073935, ae_loss: 0.06952753\n",
      "Step: [56] total_loss: 2.11300898 d_loss: 1.36368573, g_loss: 0.68052602, ae_loss: 0.06879717\n",
      "Step: [57] total_loss: 2.10939145 d_loss: 1.35440004, g_loss: 0.69132352, ae_loss: 0.06366790\n",
      "Step: [58] total_loss: 2.10675240 d_loss: 1.34913826, g_loss: 0.69044197, ae_loss: 0.06717227\n",
      "Step: [59] total_loss: 2.11353683 d_loss: 1.35340774, g_loss: 0.69397402, ae_loss: 0.06615507\n",
      "Step: [60] total_loss: 2.11882234 d_loss: 1.35902941, g_loss: 0.69073325, ae_loss: 0.06905974\n",
      "Step: [61] total_loss: 2.10822344 d_loss: 1.36705613, g_loss: 0.67078543, ae_loss: 0.07038176\n",
      "Step: [62] total_loss: 2.12168193 d_loss: 1.37601292, g_loss: 0.67681891, ae_loss: 0.06885002\n",
      "Step: [63] total_loss: 2.11819077 d_loss: 1.36425638, g_loss: 0.68678498, ae_loss: 0.06714944\n",
      "Step: [64] total_loss: 2.13901210 d_loss: 1.36297977, g_loss: 0.70252573, ae_loss: 0.07350664\n",
      "Step: [65] total_loss: 2.12793541 d_loss: 1.36564982, g_loss: 0.69821286, ae_loss: 0.06407284\n",
      "Step: [66] total_loss: 2.11869383 d_loss: 1.36523414, g_loss: 0.68527359, ae_loss: 0.06818610\n",
      "Step: [67] total_loss: 2.11917925 d_loss: 1.36876178, g_loss: 0.68220329, ae_loss: 0.06821425\n",
      "Step: [68] total_loss: 2.11640406 d_loss: 1.35401702, g_loss: 0.69414544, ae_loss: 0.06824165\n",
      "Step: [69] total_loss: 2.12847257 d_loss: 1.37343478, g_loss: 0.68905401, ae_loss: 0.06598374\n",
      "Step: [70] total_loss: 2.10303402 d_loss: 1.35342181, g_loss: 0.68112588, ae_loss: 0.06848617\n",
      "Step: [71] total_loss: 2.11137366 d_loss: 1.35451078, g_loss: 0.68840545, ae_loss: 0.06845747\n",
      "Step: [72] total_loss: 2.11882567 d_loss: 1.36332238, g_loss: 0.68645847, ae_loss: 0.06904478\n",
      "Step: [73] total_loss: 2.10486150 d_loss: 1.36202645, g_loss: 0.68122995, ae_loss: 0.06160509\n",
      "Step: [74] total_loss: 2.12254238 d_loss: 1.36296904, g_loss: 0.69092488, ae_loss: 0.06864848\n",
      "Step: [75] total_loss: 2.10557938 d_loss: 1.35802650, g_loss: 0.67949641, ae_loss: 0.06805653\n",
      "Step: [76] total_loss: 2.10947132 d_loss: 1.36488247, g_loss: 0.67666221, ae_loss: 0.06792662\n",
      "Step: [77] total_loss: 2.11156368 d_loss: 1.35378301, g_loss: 0.69192666, ae_loss: 0.06585395\n",
      "Step: [78] total_loss: 2.10878801 d_loss: 1.36239040, g_loss: 0.67729616, ae_loss: 0.06910145\n",
      "Step: [79] total_loss: 2.11394334 d_loss: 1.36018085, g_loss: 0.69221222, ae_loss: 0.06155023\n",
      "Step: [80] total_loss: 2.11450362 d_loss: 1.36785221, g_loss: 0.67785603, ae_loss: 0.06879532\n",
      "Step: [81] total_loss: 2.12124276 d_loss: 1.36935163, g_loss: 0.68422198, ae_loss: 0.06766916\n",
      "Step: [82] total_loss: 2.11355376 d_loss: 1.35428739, g_loss: 0.69039345, ae_loss: 0.06887288\n",
      "Step: [83] total_loss: 2.10639334 d_loss: 1.35265613, g_loss: 0.68959546, ae_loss: 0.06414162\n",
      "Step: [84] total_loss: 2.12108064 d_loss: 1.37165904, g_loss: 0.68379050, ae_loss: 0.06563114\n",
      "Step: [85] total_loss: 2.12661409 d_loss: 1.37268615, g_loss: 0.68269461, ae_loss: 0.07123344\n",
      "Step: [86] total_loss: 2.10624337 d_loss: 1.35769820, g_loss: 0.67927670, ae_loss: 0.06926847\n",
      "Step: [87] total_loss: 2.11349940 d_loss: 1.35525811, g_loss: 0.68510145, ae_loss: 0.07313982\n",
      "Step: [88] total_loss: 2.11349726 d_loss: 1.35615921, g_loss: 0.68816686, ae_loss: 0.06917103\n",
      "Step: [89] total_loss: 2.10627007 d_loss: 1.35478294, g_loss: 0.68675399, ae_loss: 0.06473312\n",
      "Step: [90] total_loss: 2.12722301 d_loss: 1.37061715, g_loss: 0.68418181, ae_loss: 0.07242405\n",
      "Step: [91] total_loss: 2.11577940 d_loss: 1.35804701, g_loss: 0.69227862, ae_loss: 0.06545387\n",
      "Step: [92] total_loss: 2.12536216 d_loss: 1.36700845, g_loss: 0.69060874, ae_loss: 0.06774491\n",
      "Step: [93] total_loss: 2.12163019 d_loss: 1.36858010, g_loss: 0.68881822, ae_loss: 0.06423174\n",
      "Step: [94] total_loss: 2.12448883 d_loss: 1.36210656, g_loss: 0.68949515, ae_loss: 0.07288711\n",
      "Step: [95] total_loss: 2.10551381 d_loss: 1.35177267, g_loss: 0.68334788, ae_loss: 0.07039328\n",
      "Step: [96] total_loss: 2.11724281 d_loss: 1.35214186, g_loss: 0.69187772, ae_loss: 0.07322320\n",
      "Step: [97] total_loss: 2.12389183 d_loss: 1.36454892, g_loss: 0.69454390, ae_loss: 0.06479906\n",
      "Step: [98] total_loss: 2.11937666 d_loss: 1.37069547, g_loss: 0.68120092, ae_loss: 0.06748015\n",
      "Step: [99] total_loss: 2.13152313 d_loss: 1.38319349, g_loss: 0.68109202, ae_loss: 0.06723763\n",
      "Step: [100] total_loss: 2.12061596 d_loss: 1.38074684, g_loss: 0.67921352, ae_loss: 0.06065574\n",
      "Step: [101] total_loss: 2.12526560 d_loss: 1.37497067, g_loss: 0.67868018, ae_loss: 0.07161473\n",
      "Step: [102] total_loss: 2.13137269 d_loss: 1.37558603, g_loss: 0.69042552, ae_loss: 0.06536114\n",
      "Step: [103] total_loss: 2.11936569 d_loss: 1.35674119, g_loss: 0.69565022, ae_loss: 0.06697439\n",
      "Step: [104] total_loss: 2.11782551 d_loss: 1.35780799, g_loss: 0.69334674, ae_loss: 0.06667068\n",
      "Step: [105] total_loss: 2.12357545 d_loss: 1.36053479, g_loss: 0.69846606, ae_loss: 0.06457461\n",
      "Step: [106] total_loss: 2.11828923 d_loss: 1.36114311, g_loss: 0.69111872, ae_loss: 0.06602744\n",
      "Step: [107] total_loss: 2.12770844 d_loss: 1.37182915, g_loss: 0.68603337, ae_loss: 0.06984607\n",
      "Step: [108] total_loss: 2.12661362 d_loss: 1.36977208, g_loss: 0.69317061, ae_loss: 0.06367091\n",
      "Step: [109] total_loss: 2.11447096 d_loss: 1.34286606, g_loss: 0.69923890, ae_loss: 0.07236602\n",
      "Step: [110] total_loss: 2.10945773 d_loss: 1.35650396, g_loss: 0.68891245, ae_loss: 0.06404136\n",
      "Step: [111] total_loss: 2.10969973 d_loss: 1.35299385, g_loss: 0.69155622, ae_loss: 0.06514969\n",
      "Step: [112] total_loss: 2.09520268 d_loss: 1.33633494, g_loss: 0.69599926, ae_loss: 0.06286848\n",
      "Step: [113] total_loss: 2.09114647 d_loss: 1.34668171, g_loss: 0.67395341, ae_loss: 0.07051123\n",
      "Step: [114] total_loss: 2.10712409 d_loss: 1.36426878, g_loss: 0.67575765, ae_loss: 0.06709763\n",
      "Step: [115] total_loss: 2.11661935 d_loss: 1.37195265, g_loss: 0.68268311, ae_loss: 0.06198361\n",
      "Step: [116] total_loss: 2.12730885 d_loss: 1.36186528, g_loss: 0.69640577, ae_loss: 0.06903795\n",
      "Step: [117] total_loss: 2.11890030 d_loss: 1.36099935, g_loss: 0.69255435, ae_loss: 0.06534656\n",
      "Step: [118] total_loss: 2.11392474 d_loss: 1.35403621, g_loss: 0.69273192, ae_loss: 0.06715669\n",
      "Step: [119] total_loss: 2.12436461 d_loss: 1.38592911, g_loss: 0.67148530, ae_loss: 0.06695018\n",
      "Step: [120] total_loss: 2.10782790 d_loss: 1.36339736, g_loss: 0.68218625, ae_loss: 0.06224432\n",
      "Step: [121] total_loss: 2.12535906 d_loss: 1.38771474, g_loss: 0.67273426, ae_loss: 0.06490999\n",
      "Step: [122] total_loss: 2.11451674 d_loss: 1.34700489, g_loss: 0.70104051, ae_loss: 0.06647136\n",
      "Step: [123] total_loss: 2.14936590 d_loss: 1.38915443, g_loss: 0.69198400, ae_loss: 0.06822740\n",
      "Step: [124] total_loss: 2.12509155 d_loss: 1.36035061, g_loss: 0.69826806, ae_loss: 0.06647278\n",
      "Step: [125] total_loss: 2.12530375 d_loss: 1.36918294, g_loss: 0.68803442, ae_loss: 0.06808627\n",
      "Step: [126] total_loss: 2.11742353 d_loss: 1.36473942, g_loss: 0.68593460, ae_loss: 0.06674942\n",
      "Step: [127] total_loss: 2.11024499 d_loss: 1.36194015, g_loss: 0.68447220, ae_loss: 0.06383266\n",
      "Step: [128] total_loss: 2.12400937 d_loss: 1.36568832, g_loss: 0.69021153, ae_loss: 0.06810955\n",
      "Step: [129] total_loss: 2.12448215 d_loss: 1.36624122, g_loss: 0.69346583, ae_loss: 0.06477522\n",
      "Step: [130] total_loss: 2.12496805 d_loss: 1.37682092, g_loss: 0.68346316, ae_loss: 0.06468401\n",
      "Step: [131] total_loss: 2.11472797 d_loss: 1.35854363, g_loss: 0.69303787, ae_loss: 0.06314636\n",
      "Step: [132] total_loss: 2.14373302 d_loss: 1.39130831, g_loss: 0.68678629, ae_loss: 0.06563836\n",
      "Step: [133] total_loss: 2.13886619 d_loss: 1.36738753, g_loss: 0.70361215, ae_loss: 0.06786653\n",
      "Step: [134] total_loss: 2.11968565 d_loss: 1.36615849, g_loss: 0.68806922, ae_loss: 0.06545782\n",
      "Step: [135] total_loss: 2.11313248 d_loss: 1.34926391, g_loss: 0.69696331, ae_loss: 0.06690536\n",
      "Step: [136] total_loss: 2.11576080 d_loss: 1.36019349, g_loss: 0.69212568, ae_loss: 0.06344178\n",
      "Step: [137] total_loss: 2.11852026 d_loss: 1.37629831, g_loss: 0.67672437, ae_loss: 0.06549767\n",
      "Step: [138] total_loss: 2.12699056 d_loss: 1.37971246, g_loss: 0.68210816, ae_loss: 0.06516995\n",
      "Step: [139] total_loss: 2.12399554 d_loss: 1.38416505, g_loss: 0.67739671, ae_loss: 0.06243387\n",
      "Step: [140] total_loss: 2.09168077 d_loss: 1.34417224, g_loss: 0.68427628, ae_loss: 0.06323227\n",
      "Step: [141] total_loss: 2.12999201 d_loss: 1.38593078, g_loss: 0.68100625, ae_loss: 0.06305491\n",
      "Step: [142] total_loss: 2.11887503 d_loss: 1.37026715, g_loss: 0.68422955, ae_loss: 0.06437825\n",
      "Step: [143] total_loss: 2.10861349 d_loss: 1.37164223, g_loss: 0.67486876, ae_loss: 0.06210251\n",
      "Step: [144] total_loss: 2.12160969 d_loss: 1.36290741, g_loss: 0.69619811, ae_loss: 0.06250401\n",
      "Step: [145] total_loss: 2.11859393 d_loss: 1.35640347, g_loss: 0.69793189, ae_loss: 0.06425857\n",
      "Step: [146] total_loss: 2.10990000 d_loss: 1.36309898, g_loss: 0.68186080, ae_loss: 0.06494012\n",
      "Step: [147] total_loss: 2.11280203 d_loss: 1.36492360, g_loss: 0.68338549, ae_loss: 0.06449276\n",
      "Step: [148] total_loss: 2.11393905 d_loss: 1.37422335, g_loss: 0.67938781, ae_loss: 0.06032788\n",
      "Step: [149] total_loss: 2.10810208 d_loss: 1.37011147, g_loss: 0.67426598, ae_loss: 0.06372462\n",
      "Step: [150] total_loss: 2.13532257 d_loss: 1.38074803, g_loss: 0.69408268, ae_loss: 0.06049181\n",
      "Step: [151] total_loss: 2.11336994 d_loss: 1.35712695, g_loss: 0.69151986, ae_loss: 0.06472330\n",
      "Step: [152] total_loss: 2.13533568 d_loss: 1.36664915, g_loss: 0.70754677, ae_loss: 0.06113978\n",
      "Step: [153] total_loss: 2.13364506 d_loss: 1.37268019, g_loss: 0.69386637, ae_loss: 0.06709854\n",
      "Step: [154] total_loss: 2.11547828 d_loss: 1.36301684, g_loss: 0.68887228, ae_loss: 0.06358914\n",
      "Step: [155] total_loss: 2.13241434 d_loss: 1.38453484, g_loss: 0.68321717, ae_loss: 0.06466220\n",
      "Step: [156] total_loss: 2.10913277 d_loss: 1.36449194, g_loss: 0.68497980, ae_loss: 0.05966118\n",
      "Step: [157] total_loss: 2.11286855 d_loss: 1.35814023, g_loss: 0.68742681, ae_loss: 0.06730157\n",
      "Step: [158] total_loss: 2.10448432 d_loss: 1.35421216, g_loss: 0.68863869, ae_loss: 0.06163348\n",
      "Step: [159] total_loss: 2.13356137 d_loss: 1.38997638, g_loss: 0.67655027, ae_loss: 0.06703468\n",
      "Step: [160] total_loss: 2.13420677 d_loss: 1.37465048, g_loss: 0.69401860, ae_loss: 0.06553783\n",
      "Step: [161] total_loss: 2.12747097 d_loss: 1.36839843, g_loss: 0.69874096, ae_loss: 0.06033145\n",
      "Step: [162] total_loss: 2.12011671 d_loss: 1.37421346, g_loss: 0.68397510, ae_loss: 0.06192808\n",
      "Step: [163] total_loss: 2.12019634 d_loss: 1.36768723, g_loss: 0.68865263, ae_loss: 0.06385630\n",
      "Step: [164] total_loss: 2.11595321 d_loss: 1.35680270, g_loss: 0.69781309, ae_loss: 0.06133743\n",
      "Step: [165] total_loss: 2.11382794 d_loss: 1.37098825, g_loss: 0.68182379, ae_loss: 0.06101590\n",
      "Step: [166] total_loss: 2.12115717 d_loss: 1.37075019, g_loss: 0.68503761, ae_loss: 0.06536943\n",
      "Step: [167] total_loss: 2.12072134 d_loss: 1.36098146, g_loss: 0.70012081, ae_loss: 0.05961908\n",
      "Step: [168] total_loss: 2.12022090 d_loss: 1.37517190, g_loss: 0.68254876, ae_loss: 0.06250023\n",
      "Step: [169] total_loss: 2.13030434 d_loss: 1.37898779, g_loss: 0.68762100, ae_loss: 0.06369539\n",
      "Step: [170] total_loss: 2.12679243 d_loss: 1.36927605, g_loss: 0.69355434, ae_loss: 0.06396198\n",
      "Step: [171] total_loss: 2.10259628 d_loss: 1.36142361, g_loss: 0.67851043, ae_loss: 0.06266219\n",
      "Step: [172] total_loss: 2.11131358 d_loss: 1.36738527, g_loss: 0.67953038, ae_loss: 0.06439792\n",
      "Step: [173] total_loss: 2.12760925 d_loss: 1.37538755, g_loss: 0.68884504, ae_loss: 0.06337674\n",
      "Step: [174] total_loss: 2.11224842 d_loss: 1.35962355, g_loss: 0.69001746, ae_loss: 0.06260738\n",
      "Step: [175] total_loss: 2.12736535 d_loss: 1.37408471, g_loss: 0.69031602, ae_loss: 0.06296456\n",
      "Step: [176] total_loss: 2.13277483 d_loss: 1.38008785, g_loss: 0.69162977, ae_loss: 0.06105728\n",
      "Step: [177] total_loss: 2.13794422 d_loss: 1.38004398, g_loss: 0.69559646, ae_loss: 0.06230361\n",
      "Step: [178] total_loss: 2.12268949 d_loss: 1.37458873, g_loss: 0.68371749, ae_loss: 0.06438322\n",
      "Step: [179] total_loss: 2.12768364 d_loss: 1.38019395, g_loss: 0.68217587, ae_loss: 0.06531388\n",
      "Step: [180] total_loss: 2.12193632 d_loss: 1.36154723, g_loss: 0.69725502, ae_loss: 0.06313393\n",
      "Step: [181] total_loss: 2.12323236 d_loss: 1.36883640, g_loss: 0.69205427, ae_loss: 0.06234184\n",
      "Step: [182] total_loss: 2.11828232 d_loss: 1.36177421, g_loss: 0.69179261, ae_loss: 0.06471560\n",
      "Step: [183] total_loss: 2.11837864 d_loss: 1.37023783, g_loss: 0.68650591, ae_loss: 0.06163475\n",
      "Step: [184] total_loss: 2.10805273 d_loss: 1.36886859, g_loss: 0.67701495, ae_loss: 0.06216909\n",
      "Step: [185] total_loss: 2.12833071 d_loss: 1.35803354, g_loss: 0.70867193, ae_loss: 0.06162540\n",
      "Step: [186] total_loss: 2.12374640 d_loss: 1.37143493, g_loss: 0.69178456, ae_loss: 0.06052691\n",
      "Step: [187] total_loss: 2.12247181 d_loss: 1.37477636, g_loss: 0.68685120, ae_loss: 0.06084421\n",
      "Step: [188] total_loss: 2.13078427 d_loss: 1.38019514, g_loss: 0.68627632, ae_loss: 0.06431281\n",
      "Step: [189] total_loss: 2.10277414 d_loss: 1.34905243, g_loss: 0.69024581, ae_loss: 0.06347602\n",
      "Step: [190] total_loss: 2.10742903 d_loss: 1.34304321, g_loss: 0.70078743, ae_loss: 0.06359828\n",
      "Step: [191] total_loss: 2.13650322 d_loss: 1.36984718, g_loss: 0.70317990, ae_loss: 0.06347619\n",
      "Step: [192] total_loss: 2.13059473 d_loss: 1.38443041, g_loss: 0.68274939, ae_loss: 0.06341500\n",
      "Step: [193] total_loss: 2.12239146 d_loss: 1.37867558, g_loss: 0.67926109, ae_loss: 0.06445480\n",
      "Step: [194] total_loss: 2.11015415 d_loss: 1.35969663, g_loss: 0.68928069, ae_loss: 0.06117680\n",
      "Step: [195] total_loss: 2.12180376 d_loss: 1.38812757, g_loss: 0.67346323, ae_loss: 0.06021302\n",
      "Step: [196] total_loss: 2.10245371 d_loss: 1.35094976, g_loss: 0.68652880, ae_loss: 0.06497500\n",
      "Step: [197] total_loss: 2.11664867 d_loss: 1.37178707, g_loss: 0.68459451, ae_loss: 0.06026703\n",
      "Step: [198] total_loss: 2.11934376 d_loss: 1.36533988, g_loss: 0.69233030, ae_loss: 0.06167362\n",
      "Step: [199] total_loss: 2.10994792 d_loss: 1.37113392, g_loss: 0.68244445, ae_loss: 0.05636958\n",
      "Step: [200] total_loss: 2.12357187 d_loss: 1.37861347, g_loss: 0.67986017, ae_loss: 0.06509817\n",
      "Step: [201] total_loss: 2.11980009 d_loss: 1.36548913, g_loss: 0.69126964, ae_loss: 0.06304131\n",
      "Step: [202] total_loss: 2.11527681 d_loss: 1.36730480, g_loss: 0.68162322, ae_loss: 0.06634884\n",
      "Step: [203] total_loss: 2.12150359 d_loss: 1.37960482, g_loss: 0.68457168, ae_loss: 0.05732710\n",
      "Step: [204] total_loss: 2.12444639 d_loss: 1.37980986, g_loss: 0.67973906, ae_loss: 0.06489754\n",
      "Step: [205] total_loss: 2.10621977 d_loss: 1.37156689, g_loss: 0.67440116, ae_loss: 0.06025174\n",
      "Step: [206] total_loss: 2.10384870 d_loss: 1.36653185, g_loss: 0.67842346, ae_loss: 0.05889347\n",
      "Step: [207] total_loss: 2.12369037 d_loss: 1.37113440, g_loss: 0.68658286, ae_loss: 0.06597312\n",
      "Step: [208] total_loss: 2.12369609 d_loss: 1.36968005, g_loss: 0.68940854, ae_loss: 0.06460752\n",
      "Step: [209] total_loss: 2.11403894 d_loss: 1.35983706, g_loss: 0.68943459, ae_loss: 0.06476724\n",
      "Step: [210] total_loss: 2.11085320 d_loss: 1.36924088, g_loss: 0.67841303, ae_loss: 0.06319939\n",
      "Step: [211] total_loss: 2.11753654 d_loss: 1.36185789, g_loss: 0.69311339, ae_loss: 0.06256525\n",
      "Step: [212] total_loss: 2.12373352 d_loss: 1.35528326, g_loss: 0.70368242, ae_loss: 0.06476767\n",
      "Step: [213] total_loss: 2.13473630 d_loss: 1.38741601, g_loss: 0.68698114, ae_loss: 0.06033904\n",
      "Step: [214] total_loss: 2.13173389 d_loss: 1.35830617, g_loss: 0.70971906, ae_loss: 0.06370856\n",
      "Step: [215] total_loss: 2.13143063 d_loss: 1.36534095, g_loss: 0.70359606, ae_loss: 0.06249364\n",
      "Step: [216] total_loss: 2.12505007 d_loss: 1.36606288, g_loss: 0.69867432, ae_loss: 0.06031282\n",
      "Step: [217] total_loss: 2.11262035 d_loss: 1.35864806, g_loss: 0.69020188, ae_loss: 0.06377029\n",
      "Step: [218] total_loss: 2.11894083 d_loss: 1.35481250, g_loss: 0.69774926, ae_loss: 0.06637911\n",
      "Step: [219] total_loss: 2.12891889 d_loss: 1.37721658, g_loss: 0.68494332, ae_loss: 0.06675903\n",
      "Step: [220] total_loss: 2.10323095 d_loss: 1.36973393, g_loss: 0.67218518, ae_loss: 0.06131172\n",
      "Step: [221] total_loss: 2.11429214 d_loss: 1.39716244, g_loss: 0.66010612, ae_loss: 0.05702360\n",
      "Step: [222] total_loss: 2.13075304 d_loss: 1.38263345, g_loss: 0.68199730, ae_loss: 0.06612241\n",
      "Step: [223] total_loss: 2.12732720 d_loss: 1.38531208, g_loss: 0.68294948, ae_loss: 0.05906571\n",
      "Step: [224] total_loss: 2.13209629 d_loss: 1.38514006, g_loss: 0.68306923, ae_loss: 0.06388697\n",
      "Step: [225] total_loss: 2.13020897 d_loss: 1.38374197, g_loss: 0.68508607, ae_loss: 0.06138084\n",
      "Step: [226] total_loss: 2.12502289 d_loss: 1.38754153, g_loss: 0.67675304, ae_loss: 0.06072827\n",
      "Step: [227] total_loss: 2.13318110 d_loss: 1.38745260, g_loss: 0.68248093, ae_loss: 0.06324761\n",
      "Step: [228] total_loss: 2.09343338 d_loss: 1.33395338, g_loss: 0.69478381, ae_loss: 0.06469610\n",
      "Step: [229] total_loss: 2.10465407 d_loss: 1.37076128, g_loss: 0.67306757, ae_loss: 0.06082519\n",
      "Step: [230] total_loss: 2.12275529 d_loss: 1.38010502, g_loss: 0.68136752, ae_loss: 0.06128273\n",
      "Step: [231] total_loss: 2.11598492 d_loss: 1.38582706, g_loss: 0.66695118, ae_loss: 0.06320665\n",
      "Step: [232] total_loss: 2.11104870 d_loss: 1.35901427, g_loss: 0.69123745, ae_loss: 0.06079707\n",
      "Step: [233] total_loss: 2.10525799 d_loss: 1.37001479, g_loss: 0.67107153, ae_loss: 0.06417166\n",
      "Step: [234] total_loss: 2.12819934 d_loss: 1.39618587, g_loss: 0.66766685, ae_loss: 0.06434672\n",
      "Step: [235] total_loss: 2.11631751 d_loss: 1.37603021, g_loss: 0.68358243, ae_loss: 0.05670489\n",
      "Step: [236] total_loss: 2.12407446 d_loss: 1.36766696, g_loss: 0.69147956, ae_loss: 0.06492776\n",
      "Step: [237] total_loss: 2.13645673 d_loss: 1.38343334, g_loss: 0.68560803, ae_loss: 0.06741537\n",
      "Step: [238] total_loss: 2.12818241 d_loss: 1.36303759, g_loss: 0.70734125, ae_loss: 0.05780361\n",
      "Step: [239] total_loss: 2.13485098 d_loss: 1.38705564, g_loss: 0.68332219, ae_loss: 0.06447323\n",
      "Step: [240] total_loss: 2.10574031 d_loss: 1.35969996, g_loss: 0.68540078, ae_loss: 0.06063953\n",
      "Step: [241] total_loss: 2.12419820 d_loss: 1.38580108, g_loss: 0.67347032, ae_loss: 0.06492690\n",
      "Step: [242] total_loss: 2.12298989 d_loss: 1.37749338, g_loss: 0.68515342, ae_loss: 0.06034298\n",
      "Step: [243] total_loss: 2.11559248 d_loss: 1.36674404, g_loss: 0.68653846, ae_loss: 0.06230989\n",
      "Step: [244] total_loss: 2.12453055 d_loss: 1.38862300, g_loss: 0.67609805, ae_loss: 0.05980942\n",
      "Step: [245] total_loss: 2.13862538 d_loss: 1.37909591, g_loss: 0.69758058, ae_loss: 0.06194894\n",
      "Step: [246] total_loss: 2.13398218 d_loss: 1.38859963, g_loss: 0.68803930, ae_loss: 0.05734314\n",
      "Step: [247] total_loss: 2.12681222 d_loss: 1.38826728, g_loss: 0.67882311, ae_loss: 0.05972178\n",
      "Step: [248] total_loss: 2.12306547 d_loss: 1.38217330, g_loss: 0.67956960, ae_loss: 0.06132251\n",
      "Step: [249] total_loss: 2.11932230 d_loss: 1.38099074, g_loss: 0.67843473, ae_loss: 0.05989696\n",
      "Step: [250] total_loss: 2.10874534 d_loss: 1.37383318, g_loss: 0.67650646, ae_loss: 0.05840573\n",
      "Step: [251] total_loss: 2.10048389 d_loss: 1.36366832, g_loss: 0.67347550, ae_loss: 0.06334016\n",
      "Step: [252] total_loss: 2.09522200 d_loss: 1.36480749, g_loss: 0.66997784, ae_loss: 0.06043668\n",
      "Step: [253] total_loss: 2.12357759 d_loss: 1.38631284, g_loss: 0.67731643, ae_loss: 0.05994834\n",
      "Step: [254] total_loss: 2.12283373 d_loss: 1.36543894, g_loss: 0.69700068, ae_loss: 0.06039412\n",
      "Step: [255] total_loss: 2.10556555 d_loss: 1.36158156, g_loss: 0.67991281, ae_loss: 0.06407133\n",
      "Step: [256] total_loss: 2.12341285 d_loss: 1.37469232, g_loss: 0.68741494, ae_loss: 0.06130569\n",
      "Step: [257] total_loss: 2.12873697 d_loss: 1.35944247, g_loss: 0.70652914, ae_loss: 0.06276535\n",
      "Step: [258] total_loss: 2.11458254 d_loss: 1.34770608, g_loss: 0.70658064, ae_loss: 0.06029574\n",
      "Step: [259] total_loss: 2.11633778 d_loss: 1.36911416, g_loss: 0.68462265, ae_loss: 0.06260107\n",
      "Step: [260] total_loss: 2.10952044 d_loss: 1.34749293, g_loss: 0.70266342, ae_loss: 0.05936409\n",
      "Step: [261] total_loss: 2.09217548 d_loss: 1.35067964, g_loss: 0.67823386, ae_loss: 0.06326206\n",
      "Step: [262] total_loss: 2.10708237 d_loss: 1.34169292, g_loss: 0.70355475, ae_loss: 0.06183462\n",
      "Step: [263] total_loss: 2.12404323 d_loss: 1.38137889, g_loss: 0.68044168, ae_loss: 0.06222267\n",
      "Step: [264] total_loss: 2.12079573 d_loss: 1.35703683, g_loss: 0.70184422, ae_loss: 0.06191486\n",
      "Step: [265] total_loss: 2.09615779 d_loss: 1.36813247, g_loss: 0.66726702, ae_loss: 0.06075832\n",
      "Step: [266] total_loss: 2.12308884 d_loss: 1.37156045, g_loss: 0.69410938, ae_loss: 0.05741900\n",
      "Step: [267] total_loss: 2.09704351 d_loss: 1.35644484, g_loss: 0.68324816, ae_loss: 0.05735049\n",
      "Step: [268] total_loss: 2.11072183 d_loss: 1.37934542, g_loss: 0.67628574, ae_loss: 0.05509071\n",
      "Step: [269] total_loss: 2.11075902 d_loss: 1.36968338, g_loss: 0.68257153, ae_loss: 0.05850415\n",
      "Step: [270] total_loss: 2.12318802 d_loss: 1.36459732, g_loss: 0.69692302, ae_loss: 0.06166752\n",
      "Step: [271] total_loss: 2.12258625 d_loss: 1.38302982, g_loss: 0.67763484, ae_loss: 0.06192147\n",
      "Step: [272] total_loss: 2.10594225 d_loss: 1.35639882, g_loss: 0.68920672, ae_loss: 0.06033663\n",
      "Step: [273] total_loss: 2.11396646 d_loss: 1.36962223, g_loss: 0.68803215, ae_loss: 0.05631201\n",
      "Step: [274] total_loss: 2.12648416 d_loss: 1.38135433, g_loss: 0.68476045, ae_loss: 0.06036935\n",
      "Step: [275] total_loss: 2.10239196 d_loss: 1.36047935, g_loss: 0.67957133, ae_loss: 0.06234115\n",
      "Step: [276] total_loss: 2.11105680 d_loss: 1.37650740, g_loss: 0.66905886, ae_loss: 0.06549066\n",
      "Step: [277] total_loss: 2.12042689 d_loss: 1.38435698, g_loss: 0.67109340, ae_loss: 0.06497659\n",
      "Step: [278] total_loss: 2.11484718 d_loss: 1.37858021, g_loss: 0.67235750, ae_loss: 0.06390946\n",
      "Step: [279] total_loss: 2.11312246 d_loss: 1.36191416, g_loss: 0.68809795, ae_loss: 0.06311020\n",
      "Step: [280] total_loss: 2.13246250 d_loss: 1.38496757, g_loss: 0.67890340, ae_loss: 0.06859159\n",
      "Step: [281] total_loss: 2.11740661 d_loss: 1.36645293, g_loss: 0.68788773, ae_loss: 0.06306594\n",
      "Step: [282] total_loss: 2.14153862 d_loss: 1.39533699, g_loss: 0.68578303, ae_loss: 0.06041854\n",
      "Step: [283] total_loss: 2.13684607 d_loss: 1.37942505, g_loss: 0.69882619, ae_loss: 0.05859482\n",
      "Step: [284] total_loss: 2.11838651 d_loss: 1.36684847, g_loss: 0.69031751, ae_loss: 0.06122047\n",
      "Step: [285] total_loss: 2.12705445 d_loss: 1.37163568, g_loss: 0.69457465, ae_loss: 0.06084413\n",
      "Step: [286] total_loss: 2.12510824 d_loss: 1.38385510, g_loss: 0.68300092, ae_loss: 0.05825205\n",
      "Step: [287] total_loss: 2.11699152 d_loss: 1.37486780, g_loss: 0.68045878, ae_loss: 0.06166500\n",
      "Step: [288] total_loss: 2.12792468 d_loss: 1.36860096, g_loss: 0.69713897, ae_loss: 0.06218480\n",
      "Step: [289] total_loss: 2.11898041 d_loss: 1.37568307, g_loss: 0.68436193, ae_loss: 0.05893523\n",
      "Step: [290] total_loss: 2.13673067 d_loss: 1.38589287, g_loss: 0.68734908, ae_loss: 0.06348865\n",
      "Step: [291] total_loss: 2.11894417 d_loss: 1.36324692, g_loss: 0.69681489, ae_loss: 0.05888229\n",
      "Step: [292] total_loss: 2.12751770 d_loss: 1.35995126, g_loss: 0.70496142, ae_loss: 0.06260487\n",
      "Step: [293] total_loss: 2.12664604 d_loss: 1.35219002, g_loss: 0.71693534, ae_loss: 0.05752058\n",
      "Step: [294] total_loss: 2.12541533 d_loss: 1.37387359, g_loss: 0.69284809, ae_loss: 0.05869374\n",
      "Step: [295] total_loss: 2.14349937 d_loss: 1.38282180, g_loss: 0.69804418, ae_loss: 0.06263349\n",
      "Step: [296] total_loss: 2.12275314 d_loss: 1.39115393, g_loss: 0.67244029, ae_loss: 0.05915882\n",
      "Step: [297] total_loss: 2.10730553 d_loss: 1.35716808, g_loss: 0.68629891, ae_loss: 0.06383852\n",
      "Step: [298] total_loss: 2.10057640 d_loss: 1.37250733, g_loss: 0.66399336, ae_loss: 0.06407576\n",
      "Step: [299] total_loss: 2.11405420 d_loss: 1.39358187, g_loss: 0.66107994, ae_loss: 0.05939234\n",
      "Step: [300] total_loss: 2.10305452 d_loss: 1.38362002, g_loss: 0.66153145, ae_loss: 0.05790288\n",
      "Step: [301] total_loss: 2.12569237 d_loss: 1.37260079, g_loss: 0.69591284, ae_loss: 0.05717866\n",
      "Step: [302] total_loss: 2.13906097 d_loss: 1.38548946, g_loss: 0.69270128, ae_loss: 0.06087027\n",
      "Step: [303] total_loss: 2.12039638 d_loss: 1.38761425, g_loss: 0.67239499, ae_loss: 0.06038712\n",
      "Step: [304] total_loss: 2.14458752 d_loss: 1.37411666, g_loss: 0.70799625, ae_loss: 0.06247476\n",
      "Step: [305] total_loss: 2.12755346 d_loss: 1.37617517, g_loss: 0.69365305, ae_loss: 0.05772515\n",
      "Step: [306] total_loss: 2.13987947 d_loss: 1.37511671, g_loss: 0.70393789, ae_loss: 0.06082489\n",
      "Step: [307] total_loss: 2.11589789 d_loss: 1.38322425, g_loss: 0.67059529, ae_loss: 0.06207836\n",
      "Step: [308] total_loss: 2.10514784 d_loss: 1.32943106, g_loss: 0.71512449, ae_loss: 0.06059238\n",
      "Step: [309] total_loss: 2.14184952 d_loss: 1.39068413, g_loss: 0.69057345, ae_loss: 0.06059192\n",
      "Step: [310] total_loss: 2.10555768 d_loss: 1.36107421, g_loss: 0.67961931, ae_loss: 0.06486411\n",
      "Step: [311] total_loss: 2.11401629 d_loss: 1.36097455, g_loss: 0.69379550, ae_loss: 0.05924630\n",
      "Step: [312] total_loss: 2.11909294 d_loss: 1.37145162, g_loss: 0.68424928, ae_loss: 0.06339195\n",
      "Step: [313] total_loss: 2.11147332 d_loss: 1.36668921, g_loss: 0.68121237, ae_loss: 0.06357168\n",
      "Step: [314] total_loss: 2.13066459 d_loss: 1.39933181, g_loss: 0.67343616, ae_loss: 0.05789667\n",
      "Step: [315] total_loss: 2.15828037 d_loss: 1.38255239, g_loss: 0.71587378, ae_loss: 0.05985424\n",
      "Step: [316] total_loss: 2.15174389 d_loss: 1.38851547, g_loss: 0.70548701, ae_loss: 0.05774139\n",
      "Step: [317] total_loss: 2.15133500 d_loss: 1.40120745, g_loss: 0.69289863, ae_loss: 0.05722897\n",
      "Step: [318] total_loss: 2.12916803 d_loss: 1.38212097, g_loss: 0.68706113, ae_loss: 0.05998605\n",
      "Step: [319] total_loss: 2.14139485 d_loss: 1.38009930, g_loss: 0.69843477, ae_loss: 0.06286076\n",
      "Step: [320] total_loss: 2.13512015 d_loss: 1.39833105, g_loss: 0.67521858, ae_loss: 0.06157053\n",
      "Step: [321] total_loss: 2.12043238 d_loss: 1.37887287, g_loss: 0.68470484, ae_loss: 0.05685472\n",
      "Step: [322] total_loss: 2.12586951 d_loss: 1.38584256, g_loss: 0.67971098, ae_loss: 0.06031592\n",
      "Step: [323] total_loss: 2.12753749 d_loss: 1.36594343, g_loss: 0.69980800, ae_loss: 0.06178603\n",
      "Step: [324] total_loss: 2.12333226 d_loss: 1.38848400, g_loss: 0.67665642, ae_loss: 0.05819182\n",
      "Step: [325] total_loss: 2.12094355 d_loss: 1.37814879, g_loss: 0.68458819, ae_loss: 0.05820649\n",
      "Step: [326] total_loss: 2.13063240 d_loss: 1.39082754, g_loss: 0.67537093, ae_loss: 0.06443381\n",
      "Step: [327] total_loss: 2.11451960 d_loss: 1.37939131, g_loss: 0.67606413, ae_loss: 0.05906428\n",
      "Step: [328] total_loss: 2.12182045 d_loss: 1.38704443, g_loss: 0.67495686, ae_loss: 0.05981909\n",
      "Step: [329] total_loss: 2.12993574 d_loss: 1.37811184, g_loss: 0.68720937, ae_loss: 0.06461436\n",
      "Step: [330] total_loss: 2.12514210 d_loss: 1.37456632, g_loss: 0.69019431, ae_loss: 0.06038137\n",
      "Step: [331] total_loss: 2.12682343 d_loss: 1.37705088, g_loss: 0.69104433, ae_loss: 0.05872811\n",
      "Step: [332] total_loss: 2.12929463 d_loss: 1.35209537, g_loss: 0.71538055, ae_loss: 0.06181867\n",
      "Step: [333] total_loss: 2.12561464 d_loss: 1.37862015, g_loss: 0.68421602, ae_loss: 0.06277844\n",
      "Step: [334] total_loss: 2.12335634 d_loss: 1.36653256, g_loss: 0.69964337, ae_loss: 0.05718028\n",
      "Step: [335] total_loss: 2.12985516 d_loss: 1.37686574, g_loss: 0.69291806, ae_loss: 0.06007136\n",
      "Step: [336] total_loss: 2.10254574 d_loss: 1.37043214, g_loss: 0.67674524, ae_loss: 0.05536840\n",
      "Step: [337] total_loss: 2.11821699 d_loss: 1.37131202, g_loss: 0.69216710, ae_loss: 0.05473797\n",
      "Step: [338] total_loss: 2.09432459 d_loss: 1.36482477, g_loss: 0.67301166, ae_loss: 0.05648817\n",
      "Step: [339] total_loss: 2.09873581 d_loss: 1.34910893, g_loss: 0.69117355, ae_loss: 0.05845326\n",
      "Step: [340] total_loss: 2.09697461 d_loss: 1.35788298, g_loss: 0.67817032, ae_loss: 0.06092130\n",
      "Step: [341] total_loss: 2.10765910 d_loss: 1.37184834, g_loss: 0.67772913, ae_loss: 0.05808160\n",
      "Step: [342] total_loss: 2.14084911 d_loss: 1.38485861, g_loss: 0.69694340, ae_loss: 0.05904704\n",
      "Step: [343] total_loss: 2.13587332 d_loss: 1.37719345, g_loss: 0.69429445, ae_loss: 0.06438553\n",
      "Step: [344] total_loss: 2.12873864 d_loss: 1.36421037, g_loss: 0.70965981, ae_loss: 0.05486841\n",
      "Step: [345] total_loss: 2.11967468 d_loss: 1.35961604, g_loss: 0.70217049, ae_loss: 0.05788822\n",
      "Step: [346] total_loss: 2.13428617 d_loss: 1.38249934, g_loss: 0.69271153, ae_loss: 0.05907522\n",
      "Step: [347] total_loss: 2.11841965 d_loss: 1.35860848, g_loss: 0.69808090, ae_loss: 0.06173017\n",
      "Step: [348] total_loss: 2.12207079 d_loss: 1.35823870, g_loss: 0.70060033, ae_loss: 0.06323182\n",
      "Step: [349] total_loss: 2.11595631 d_loss: 1.37823796, g_loss: 0.68132210, ae_loss: 0.05639623\n",
      "Step: [350] total_loss: 2.09856486 d_loss: 1.34936690, g_loss: 0.68842244, ae_loss: 0.06077553\n",
      "Step: [351] total_loss: 2.12476182 d_loss: 1.38027918, g_loss: 0.68382651, ae_loss: 0.06065611\n",
      "Step: [352] total_loss: 2.12388158 d_loss: 1.37341022, g_loss: 0.69348413, ae_loss: 0.05698721\n",
      "Step: [353] total_loss: 2.11968255 d_loss: 1.40640712, g_loss: 0.65629625, ae_loss: 0.05697918\n",
      "Step: [354] total_loss: 2.15177870 d_loss: 1.40254760, g_loss: 0.69147730, ae_loss: 0.05775375\n",
      "Step: [355] total_loss: 2.12745047 d_loss: 1.36551547, g_loss: 0.70570076, ae_loss: 0.05623417\n",
      "Step: [356] total_loss: 2.12813807 d_loss: 1.37232578, g_loss: 0.69722748, ae_loss: 0.05858479\n",
      "Step: [357] total_loss: 2.13406181 d_loss: 1.38490117, g_loss: 0.69056988, ae_loss: 0.05859092\n",
      "Step: [358] total_loss: 2.13127375 d_loss: 1.37556088, g_loss: 0.69756806, ae_loss: 0.05814490\n",
      "Step: [359] total_loss: 2.14008641 d_loss: 1.39270365, g_loss: 0.68194914, ae_loss: 0.06543359\n",
      "Step: [360] total_loss: 2.11421585 d_loss: 1.37453008, g_loss: 0.68065798, ae_loss: 0.05902787\n",
      "Step: [361] total_loss: 2.11736059 d_loss: 1.37473249, g_loss: 0.68144196, ae_loss: 0.06118616\n",
      "Step: [362] total_loss: 2.13995409 d_loss: 1.39864302, g_loss: 0.68074381, ae_loss: 0.06056718\n",
      "Step: [363] total_loss: 2.10721278 d_loss: 1.36086273, g_loss: 0.68745279, ae_loss: 0.05889728\n",
      "Step: [364] total_loss: 2.11028671 d_loss: 1.36369658, g_loss: 0.68350267, ae_loss: 0.06308745\n",
      "Step: [365] total_loss: 2.12162256 d_loss: 1.36839199, g_loss: 0.69150913, ae_loss: 0.06172132\n",
      "Step: [366] total_loss: 2.11870146 d_loss: 1.37185454, g_loss: 0.68672031, ae_loss: 0.06012669\n",
      "Step: [367] total_loss: 2.10988092 d_loss: 1.37851036, g_loss: 0.67382908, ae_loss: 0.05754165\n",
      "Step: [368] total_loss: 2.12095428 d_loss: 1.38285685, g_loss: 0.67912948, ae_loss: 0.05896799\n",
      "Step: [369] total_loss: 2.13695765 d_loss: 1.38676929, g_loss: 0.68986261, ae_loss: 0.06032583\n",
      "Step: [370] total_loss: 2.13000679 d_loss: 1.38649023, g_loss: 0.68593806, ae_loss: 0.05757844\n",
      "Step: [371] total_loss: 2.11937881 d_loss: 1.36859536, g_loss: 0.69273686, ae_loss: 0.05804663\n",
      "Step: [372] total_loss: 2.11562490 d_loss: 1.38488889, g_loss: 0.67253441, ae_loss: 0.05820170\n",
      "Step: [373] total_loss: 2.11454105 d_loss: 1.35904682, g_loss: 0.69404447, ae_loss: 0.06144970\n",
      "Step: [374] total_loss: 2.11551380 d_loss: 1.38773382, g_loss: 0.67109466, ae_loss: 0.05668520\n",
      "Step: [375] total_loss: 2.11976361 d_loss: 1.38483644, g_loss: 0.67668319, ae_loss: 0.05824396\n",
      "Step: [376] total_loss: 2.14099574 d_loss: 1.35605597, g_loss: 0.72227073, ae_loss: 0.06266899\n",
      "Step: [377] total_loss: 2.12827778 d_loss: 1.37677670, g_loss: 0.69047844, ae_loss: 0.06102257\n",
      "Step: [378] total_loss: 2.12388158 d_loss: 1.38082647, g_loss: 0.68807960, ae_loss: 0.05497545\n",
      "Step: [379] total_loss: 2.11933494 d_loss: 1.37741244, g_loss: 0.67806351, ae_loss: 0.06385896\n",
      "Step: [380] total_loss: 2.11135006 d_loss: 1.37808871, g_loss: 0.67400193, ae_loss: 0.05925940\n",
      "Step: [381] total_loss: 2.12046599 d_loss: 1.37454391, g_loss: 0.68937743, ae_loss: 0.05654464\n",
      "Step: [382] total_loss: 2.12821674 d_loss: 1.37823367, g_loss: 0.69249207, ae_loss: 0.05749103\n",
      "Step: [383] total_loss: 2.12499380 d_loss: 1.37318373, g_loss: 0.69251412, ae_loss: 0.05929585\n",
      "Step: [384] total_loss: 2.12069654 d_loss: 1.37345290, g_loss: 0.68881959, ae_loss: 0.05842410\n",
      "Step: [385] total_loss: 2.12953353 d_loss: 1.37799144, g_loss: 0.69011992, ae_loss: 0.06142227\n",
      "Step: [386] total_loss: 2.11772537 d_loss: 1.37063968, g_loss: 0.68625510, ae_loss: 0.06083054\n",
      "Step: [387] total_loss: 2.10395956 d_loss: 1.36536431, g_loss: 0.67778265, ae_loss: 0.06081272\n",
      "Step: [388] total_loss: 2.09601212 d_loss: 1.35021782, g_loss: 0.68722987, ae_loss: 0.05856436\n",
      "Step: [389] total_loss: 2.11303425 d_loss: 1.37482953, g_loss: 0.68357909, ae_loss: 0.05462575\n",
      "Step: [390] total_loss: 2.13140655 d_loss: 1.37217414, g_loss: 0.70054728, ae_loss: 0.05868508\n",
      "Step: [391] total_loss: 2.11431646 d_loss: 1.37516224, g_loss: 0.68179482, ae_loss: 0.05735949\n",
      "Step: [392] total_loss: 2.11594820 d_loss: 1.35887313, g_loss: 0.69807851, ae_loss: 0.05899655\n",
      "Step: [393] total_loss: 2.14392257 d_loss: 1.37047982, g_loss: 0.71614820, ae_loss: 0.05729461\n",
      "Step: [394] total_loss: 2.09012032 d_loss: 1.37035394, g_loss: 0.65831035, ae_loss: 0.06145610\n",
      "Step: [395] total_loss: 2.11587667 d_loss: 1.39301276, g_loss: 0.66071993, ae_loss: 0.06214393\n",
      "Step: [396] total_loss: 2.11040688 d_loss: 1.39079595, g_loss: 0.65789479, ae_loss: 0.06171614\n",
      "Step: [397] total_loss: 2.11351013 d_loss: 1.37526226, g_loss: 0.68012965, ae_loss: 0.05811805\n",
      "Step: [398] total_loss: 2.13381267 d_loss: 1.40227008, g_loss: 0.67579317, ae_loss: 0.05574936\n",
      "Step: [399] total_loss: 2.10573435 d_loss: 1.38015532, g_loss: 0.66863680, ae_loss: 0.05694218\n",
      "Step: [400] total_loss: 2.11758232 d_loss: 1.36840069, g_loss: 0.68844515, ae_loss: 0.06073642\n",
      "Step: [401] total_loss: 2.11840463 d_loss: 1.37423253, g_loss: 0.68711996, ae_loss: 0.05705217\n",
      "Step: [402] total_loss: 2.12781405 d_loss: 1.38005698, g_loss: 0.68950880, ae_loss: 0.05824831\n",
      "Step: [403] total_loss: 2.11260939 d_loss: 1.36736858, g_loss: 0.68317974, ae_loss: 0.06206104\n",
      "Step: [404] total_loss: 2.12697077 d_loss: 1.38885939, g_loss: 0.67780280, ae_loss: 0.06030862\n",
      "Step: [405] total_loss: 2.12493682 d_loss: 1.37036991, g_loss: 0.69918418, ae_loss: 0.05538267\n",
      "Step: [406] total_loss: 2.11538363 d_loss: 1.37796903, g_loss: 0.67142332, ae_loss: 0.06599146\n",
      "Step: [407] total_loss: 2.11654782 d_loss: 1.37246096, g_loss: 0.68479294, ae_loss: 0.05929382\n",
      "Step: [408] total_loss: 2.13245153 d_loss: 1.36535382, g_loss: 0.70551217, ae_loss: 0.06158552\n",
      "Step: [409] total_loss: 2.12498689 d_loss: 1.37860966, g_loss: 0.68761033, ae_loss: 0.05876686\n",
      "Step: [410] total_loss: 2.12230849 d_loss: 1.38514376, g_loss: 0.67710781, ae_loss: 0.06005698\n",
      "Step: [411] total_loss: 2.11014605 d_loss: 1.37050414, g_loss: 0.68036693, ae_loss: 0.05927490\n",
      "Step: [412] total_loss: 2.12723351 d_loss: 1.37623501, g_loss: 0.69246018, ae_loss: 0.05853845\n",
      "Step: [413] total_loss: 2.11075640 d_loss: 1.36997700, g_loss: 0.68381357, ae_loss: 0.05696569\n",
      "Step: [414] total_loss: 2.12186861 d_loss: 1.37827086, g_loss: 0.68465853, ae_loss: 0.05893925\n",
      "Step: [415] total_loss: 2.10742378 d_loss: 1.38278103, g_loss: 0.66881227, ae_loss: 0.05583065\n",
      "Step: [416] total_loss: 2.11999726 d_loss: 1.37981939, g_loss: 0.68215835, ae_loss: 0.05801948\n",
      "Step: [417] total_loss: 2.10282779 d_loss: 1.36342263, g_loss: 0.68004405, ae_loss: 0.05936112\n",
      "Step: [418] total_loss: 2.12232542 d_loss: 1.36761212, g_loss: 0.69241285, ae_loss: 0.06230050\n",
      "Step: [419] total_loss: 2.12984610 d_loss: 1.38645697, g_loss: 0.68590069, ae_loss: 0.05748848\n",
      "Step: [420] total_loss: 2.10292387 d_loss: 1.35689747, g_loss: 0.68944824, ae_loss: 0.05657810\n",
      "Step: [421] total_loss: 2.13524079 d_loss: 1.38659167, g_loss: 0.68929356, ae_loss: 0.05935564\n",
      "Step: [422] total_loss: 2.13283777 d_loss: 1.39104342, g_loss: 0.68294322, ae_loss: 0.05885112\n",
      "Step: [423] total_loss: 2.13647699 d_loss: 1.39364445, g_loss: 0.68905139, ae_loss: 0.05378123\n",
      "Step: [424] total_loss: 2.12265110 d_loss: 1.37368369, g_loss: 0.69288099, ae_loss: 0.05608632\n",
      "Step: [425] total_loss: 2.12286854 d_loss: 1.38674450, g_loss: 0.67501032, ae_loss: 0.06111365\n",
      "Step: [426] total_loss: 2.12326169 d_loss: 1.39915991, g_loss: 0.66721541, ae_loss: 0.05688637\n",
      "Step: [427] total_loss: 2.11389923 d_loss: 1.36174858, g_loss: 0.69366014, ae_loss: 0.05849065\n",
      "Step: [428] total_loss: 2.12564278 d_loss: 1.38175011, g_loss: 0.68438435, ae_loss: 0.05950829\n",
      "Step: [429] total_loss: 2.13773370 d_loss: 1.37955034, g_loss: 0.69944662, ae_loss: 0.05873682\n",
      "Step: [430] total_loss: 2.13297868 d_loss: 1.39125907, g_loss: 0.68010885, ae_loss: 0.06161073\n",
      "Step: [431] total_loss: 2.11169362 d_loss: 1.36141419, g_loss: 0.69341075, ae_loss: 0.05686870\n",
      "Step: [432] total_loss: 2.14231896 d_loss: 1.39182723, g_loss: 0.68882805, ae_loss: 0.06166366\n",
      "Step: [433] total_loss: 2.11280465 d_loss: 1.36650884, g_loss: 0.68493438, ae_loss: 0.06136139\n",
      "Step: [434] total_loss: 2.12450838 d_loss: 1.38475037, g_loss: 0.68620443, ae_loss: 0.05355357\n",
      "Step: [435] total_loss: 2.11302233 d_loss: 1.37444079, g_loss: 0.68000770, ae_loss: 0.05857390\n",
      "Step: [436] total_loss: 2.12091899 d_loss: 1.37875414, g_loss: 0.68405306, ae_loss: 0.05811181\n",
      "Step: [437] total_loss: 2.12810826 d_loss: 1.37526238, g_loss: 0.69332588, ae_loss: 0.05951997\n",
      "Step: [438] total_loss: 2.11566401 d_loss: 1.38199699, g_loss: 0.67708421, ae_loss: 0.05658265\n",
      "Step: [439] total_loss: 2.13127947 d_loss: 1.37834311, g_loss: 0.69449914, ae_loss: 0.05843721\n",
      "Step: [440] total_loss: 2.11834097 d_loss: 1.37439144, g_loss: 0.68796611, ae_loss: 0.05598347\n",
      "Step: [441] total_loss: 2.13394594 d_loss: 1.37660921, g_loss: 0.69618809, ae_loss: 0.06114868\n",
      "Step: [442] total_loss: 2.11751318 d_loss: 1.38153815, g_loss: 0.68094969, ae_loss: 0.05502537\n",
      "Step: [443] total_loss: 2.11985922 d_loss: 1.38135386, g_loss: 0.67909503, ae_loss: 0.05941046\n",
      "Step: [444] total_loss: 2.12100959 d_loss: 1.39291906, g_loss: 0.66671455, ae_loss: 0.06137600\n",
      "Step: [445] total_loss: 2.08735490 d_loss: 1.35613513, g_loss: 0.67429638, ae_loss: 0.05692338\n",
      "Step: [446] total_loss: 2.12474251 d_loss: 1.38643813, g_loss: 0.67961836, ae_loss: 0.05868602\n",
      "Step: [447] total_loss: 2.11467028 d_loss: 1.37063468, g_loss: 0.68075979, ae_loss: 0.06327582\n",
      "Step: [448] total_loss: 2.14820957 d_loss: 1.40337205, g_loss: 0.68615031, ae_loss: 0.05868721\n",
      "Step: [449] total_loss: 2.13058043 d_loss: 1.36750245, g_loss: 0.70248365, ae_loss: 0.06059433\n",
      "Step: [450] total_loss: 2.11216283 d_loss: 1.37881255, g_loss: 0.67217302, ae_loss: 0.06117725\n",
      "Step: [451] total_loss: 2.11884117 d_loss: 1.38956165, g_loss: 0.67368615, ae_loss: 0.05559338\n",
      "Step: [452] total_loss: 2.11347985 d_loss: 1.36114645, g_loss: 0.69309825, ae_loss: 0.05923522\n",
      "Step: [453] total_loss: 2.11964703 d_loss: 1.37554359, g_loss: 0.68825197, ae_loss: 0.05585155\n",
      "Step: [454] total_loss: 2.10645604 d_loss: 1.37128019, g_loss: 0.67883873, ae_loss: 0.05633710\n",
      "Step: [455] total_loss: 2.13990355 d_loss: 1.40667892, g_loss: 0.67655796, ae_loss: 0.05666665\n",
      "Step: [456] total_loss: 2.12623596 d_loss: 1.37165713, g_loss: 0.69240391, ae_loss: 0.06217508\n",
      "Step: [457] total_loss: 2.10785341 d_loss: 1.37143540, g_loss: 0.67934668, ae_loss: 0.05707146\n",
      "Step: [458] total_loss: 2.09859180 d_loss: 1.36531341, g_loss: 0.67869246, ae_loss: 0.05458601\n",
      "Step: [459] total_loss: 2.12240314 d_loss: 1.38397503, g_loss: 0.68044758, ae_loss: 0.05798038\n",
      "Step: [460] total_loss: 2.13363576 d_loss: 1.37598109, g_loss: 0.70004827, ae_loss: 0.05760650\n",
      "Step: [461] total_loss: 2.13030219 d_loss: 1.38566756, g_loss: 0.68476427, ae_loss: 0.05987032\n",
      "Step: [462] total_loss: 2.13469934 d_loss: 1.36895108, g_loss: 0.70985693, ae_loss: 0.05589134\n",
      "Step: [463] total_loss: 2.12164021 d_loss: 1.35752320, g_loss: 0.70417368, ae_loss: 0.05994346\n",
      "Step: [464] total_loss: 2.10302114 d_loss: 1.36222076, g_loss: 0.68365800, ae_loss: 0.05714231\n",
      "Step: [465] total_loss: 2.11401010 d_loss: 1.38172817, g_loss: 0.67690229, ae_loss: 0.05537960\n",
      "Step: [466] total_loss: 2.10733032 d_loss: 1.36991692, g_loss: 0.68106943, ae_loss: 0.05634398\n",
      "Step: [467] total_loss: 2.12365007 d_loss: 1.38533628, g_loss: 0.67818063, ae_loss: 0.06013325\n",
      "Step: [468] total_loss: 2.11319399 d_loss: 1.34670305, g_loss: 0.71106327, ae_loss: 0.05542782\n",
      "Step: [469] total_loss: 2.13035631 d_loss: 1.37274957, g_loss: 0.69413042, ae_loss: 0.06347619\n",
      "Step: [470] total_loss: 2.12253118 d_loss: 1.38278985, g_loss: 0.67769730, ae_loss: 0.06204398\n",
      "Step: [471] total_loss: 2.12501836 d_loss: 1.38983393, g_loss: 0.68191195, ae_loss: 0.05327247\n",
      "Step: [472] total_loss: 2.11021757 d_loss: 1.37096786, g_loss: 0.67997766, ae_loss: 0.05927210\n",
      "Step: [473] total_loss: 2.12196112 d_loss: 1.37720394, g_loss: 0.68673563, ae_loss: 0.05802170\n",
      "Step: [474] total_loss: 2.09912395 d_loss: 1.36836553, g_loss: 0.67653060, ae_loss: 0.05422775\n",
      "Step: [475] total_loss: 2.11248755 d_loss: 1.37035251, g_loss: 0.68587953, ae_loss: 0.05625543\n",
      "Step: [476] total_loss: 2.12424088 d_loss: 1.39191711, g_loss: 0.67744112, ae_loss: 0.05488256\n",
      "Step: [477] total_loss: 2.12630939 d_loss: 1.38580918, g_loss: 0.68485284, ae_loss: 0.05564734\n",
      "Step: [478] total_loss: 2.13935280 d_loss: 1.39038289, g_loss: 0.69554722, ae_loss: 0.05342280\n",
      "Step: [479] total_loss: 2.14647841 d_loss: 1.38838458, g_loss: 0.69769728, ae_loss: 0.06039658\n",
      "Step: [480] total_loss: 2.13449192 d_loss: 1.36593843, g_loss: 0.70694226, ae_loss: 0.06161130\n",
      "Step: [481] total_loss: 2.14005160 d_loss: 1.38142347, g_loss: 0.70315647, ae_loss: 0.05547170\n",
      "Step: [482] total_loss: 2.12596369 d_loss: 1.39406729, g_loss: 0.67159629, ae_loss: 0.06030015\n",
      "Step: [483] total_loss: 2.13318944 d_loss: 1.37300777, g_loss: 0.70435750, ae_loss: 0.05582413\n",
      "Step: [484] total_loss: 2.13027310 d_loss: 1.39480782, g_loss: 0.68046480, ae_loss: 0.05500053\n",
      "Step: [485] total_loss: 2.12810159 d_loss: 1.37656248, g_loss: 0.69187552, ae_loss: 0.05966354\n",
      "Step: [486] total_loss: 2.14688492 d_loss: 1.40471458, g_loss: 0.67921335, ae_loss: 0.06295690\n",
      "Step: [487] total_loss: 2.13991785 d_loss: 1.39994287, g_loss: 0.68584448, ae_loss: 0.05413057\n",
      "Step: [488] total_loss: 2.14639711 d_loss: 1.38606036, g_loss: 0.70115948, ae_loss: 0.05917724\n",
      "Step: [489] total_loss: 2.12439013 d_loss: 1.37806082, g_loss: 0.68685055, ae_loss: 0.05947861\n",
      "Step: [490] total_loss: 2.14003277 d_loss: 1.39427137, g_loss: 0.68994915, ae_loss: 0.05581216\n",
      "Step: [491] total_loss: 2.15759134 d_loss: 1.39586592, g_loss: 0.70304519, ae_loss: 0.05868015\n",
      "Step: [492] total_loss: 2.14459038 d_loss: 1.40989327, g_loss: 0.67877591, ae_loss: 0.05592111\n",
      "Step: [493] total_loss: 2.12553120 d_loss: 1.36466169, g_loss: 0.70193601, ae_loss: 0.05893359\n",
      "Step: [494] total_loss: 2.13134623 d_loss: 1.38267088, g_loss: 0.69384980, ae_loss: 0.05482538\n",
      "Step: [495] total_loss: 2.12370467 d_loss: 1.37204480, g_loss: 0.69392997, ae_loss: 0.05772985\n",
      "Step: [496] total_loss: 2.12424183 d_loss: 1.38205075, g_loss: 0.68300438, ae_loss: 0.05918679\n",
      "Step: [497] total_loss: 2.12737370 d_loss: 1.38177896, g_loss: 0.68563008, ae_loss: 0.05996453\n",
      "Step: [498] total_loss: 2.11665154 d_loss: 1.36997271, g_loss: 0.68495643, ae_loss: 0.06172249\n",
      "Step: [499] total_loss: 2.12455559 d_loss: 1.38426805, g_loss: 0.68473947, ae_loss: 0.05554798\n",
      "Step: [500] total_loss: 2.14551592 d_loss: 1.41723955, g_loss: 0.67516339, ae_loss: 0.05311314\n",
      "Step: [501] total_loss: 2.12348437 d_loss: 1.38145614, g_loss: 0.68477046, ae_loss: 0.05725773\n",
      "Step: [502] total_loss: 2.12176514 d_loss: 1.37575018, g_loss: 0.68865985, ae_loss: 0.05735506\n",
      "Step: [503] total_loss: 2.11919165 d_loss: 1.37397337, g_loss: 0.68974918, ae_loss: 0.05546913\n",
      "Step: [504] total_loss: 2.13824344 d_loss: 1.38693333, g_loss: 0.69528925, ae_loss: 0.05602083\n",
      "Step: [505] total_loss: 2.14249349 d_loss: 1.38908219, g_loss: 0.69518882, ae_loss: 0.05822251\n",
      "Step: [506] total_loss: 2.11630869 d_loss: 1.36793399, g_loss: 0.69301248, ae_loss: 0.05536212\n",
      "Step: [507] total_loss: 2.12246943 d_loss: 1.38183904, g_loss: 0.68681729, ae_loss: 0.05381300\n",
      "Step: [508] total_loss: 2.12908864 d_loss: 1.38174689, g_loss: 0.69088531, ae_loss: 0.05645645\n",
      "Step: [509] total_loss: 2.13472557 d_loss: 1.37591326, g_loss: 0.69844455, ae_loss: 0.06036777\n",
      "Step: [510] total_loss: 2.11616850 d_loss: 1.38429296, g_loss: 0.67820430, ae_loss: 0.05367130\n",
      "Step: [511] total_loss: 2.13428593 d_loss: 1.37018156, g_loss: 0.70400333, ae_loss: 0.06010103\n",
      "Step: [512] total_loss: 2.14101982 d_loss: 1.40456820, g_loss: 0.67673868, ae_loss: 0.05971302\n",
      "Step: [513] total_loss: 2.11825943 d_loss: 1.38481140, g_loss: 0.67877424, ae_loss: 0.05467386\n",
      "Step: [514] total_loss: 2.13130856 d_loss: 1.39705849, g_loss: 0.68090379, ae_loss: 0.05334613\n",
      "Step: [515] total_loss: 2.11935306 d_loss: 1.37454426, g_loss: 0.68765277, ae_loss: 0.05715604\n",
      "Step: [516] total_loss: 2.11989403 d_loss: 1.37152135, g_loss: 0.69117177, ae_loss: 0.05720098\n",
      "Step: [517] total_loss: 2.11990547 d_loss: 1.38678002, g_loss: 0.67532599, ae_loss: 0.05779945\n",
      "Step: [518] total_loss: 2.13302183 d_loss: 1.38448465, g_loss: 0.69011736, ae_loss: 0.05841971\n",
      "Step: [519] total_loss: 2.11381030 d_loss: 1.36294603, g_loss: 0.69422537, ae_loss: 0.05663881\n",
      "Step: [520] total_loss: 2.14187241 d_loss: 1.39492536, g_loss: 0.69119740, ae_loss: 0.05574979\n",
      "Step: [521] total_loss: 2.13056231 d_loss: 1.36223507, g_loss: 0.70940655, ae_loss: 0.05892073\n",
      "Step: [522] total_loss: 2.12899733 d_loss: 1.38669789, g_loss: 0.68630373, ae_loss: 0.05599564\n",
      "Step: [523] total_loss: 2.11628914 d_loss: 1.37289989, g_loss: 0.68397403, ae_loss: 0.05941526\n",
      "Step: [524] total_loss: 2.12242889 d_loss: 1.38419569, g_loss: 0.68600726, ae_loss: 0.05222585\n",
      "Step: [525] total_loss: 2.11232758 d_loss: 1.37522912, g_loss: 0.68143517, ae_loss: 0.05566327\n",
      "Step: [526] total_loss: 2.11617947 d_loss: 1.37099600, g_loss: 0.68903875, ae_loss: 0.05614470\n",
      "Step: [527] total_loss: 2.12607288 d_loss: 1.37971044, g_loss: 0.68715012, ae_loss: 0.05921240\n",
      "Step: [528] total_loss: 2.12932873 d_loss: 1.39251328, g_loss: 0.67545235, ae_loss: 0.06136293\n",
      "Step: [529] total_loss: 2.11271667 d_loss: 1.37835860, g_loss: 0.67653215, ae_loss: 0.05782580\n",
      "Step: [530] total_loss: 2.10068750 d_loss: 1.36717844, g_loss: 0.67941636, ae_loss: 0.05409276\n",
      "Step: [531] total_loss: 2.12430954 d_loss: 1.37974775, g_loss: 0.68822801, ae_loss: 0.05633383\n",
      "Step: [532] total_loss: 2.10753918 d_loss: 1.37259936, g_loss: 0.68269134, ae_loss: 0.05224847\n",
      "Step: [533] total_loss: 2.13779330 d_loss: 1.39125001, g_loss: 0.68504262, ae_loss: 0.06150068\n",
      "Step: [534] total_loss: 2.10298491 d_loss: 1.36362922, g_loss: 0.68298721, ae_loss: 0.05636841\n",
      "Step: [535] total_loss: 2.11794257 d_loss: 1.37739348, g_loss: 0.68356735, ae_loss: 0.05698170\n",
      "Step: [536] total_loss: 2.13313174 d_loss: 1.39534426, g_loss: 0.68277794, ae_loss: 0.05500949\n",
      "Step: [537] total_loss: 2.13483930 d_loss: 1.38611829, g_loss: 0.69113612, ae_loss: 0.05758492\n",
      "Step: [538] total_loss: 2.12369490 d_loss: 1.37107086, g_loss: 0.69414991, ae_loss: 0.05847406\n",
      "Step: [539] total_loss: 2.13793516 d_loss: 1.37999487, g_loss: 0.69926631, ae_loss: 0.05867395\n",
      "Step: [540] total_loss: 2.12964678 d_loss: 1.38026500, g_loss: 0.69418716, ae_loss: 0.05519479\n",
      "Step: [541] total_loss: 2.11957812 d_loss: 1.35746861, g_loss: 0.70212001, ae_loss: 0.05998952\n",
      "Step: [542] total_loss: 2.12753606 d_loss: 1.37167239, g_loss: 0.70099944, ae_loss: 0.05486428\n",
      "Step: [543] total_loss: 2.12997055 d_loss: 1.37540305, g_loss: 0.69439387, ae_loss: 0.06017347\n",
      "Step: [544] total_loss: 2.11604309 d_loss: 1.37432814, g_loss: 0.68674135, ae_loss: 0.05497374\n",
      "Step: [545] total_loss: 2.11419153 d_loss: 1.37454176, g_loss: 0.68476397, ae_loss: 0.05488571\n",
      "Step: [546] total_loss: 2.09860611 d_loss: 1.35854244, g_loss: 0.68453860, ae_loss: 0.05552510\n",
      "Step: [547] total_loss: 2.10443664 d_loss: 1.36511064, g_loss: 0.68457675, ae_loss: 0.05474921\n",
      "Step: [548] total_loss: 2.11478519 d_loss: 1.37871957, g_loss: 0.68466383, ae_loss: 0.05140190\n",
      "Step: [549] total_loss: 2.12278557 d_loss: 1.38130474, g_loss: 0.68220401, ae_loss: 0.05927685\n",
      "Step: [550] total_loss: 2.13106060 d_loss: 1.39607465, g_loss: 0.67756474, ae_loss: 0.05742136\n",
      "Step: [551] total_loss: 2.12599134 d_loss: 1.37134564, g_loss: 0.70023090, ae_loss: 0.05441492\n",
      "Step: [552] total_loss: 2.15352368 d_loss: 1.38906407, g_loss: 0.69993448, ae_loss: 0.06452518\n",
      "Step: [553] total_loss: 2.12591410 d_loss: 1.38978338, g_loss: 0.68084860, ae_loss: 0.05528212\n",
      "Step: [554] total_loss: 2.11988997 d_loss: 1.37884450, g_loss: 0.68178213, ae_loss: 0.05926337\n",
      "Step: [555] total_loss: 2.10027647 d_loss: 1.36738348, g_loss: 0.67454576, ae_loss: 0.05834717\n",
      "Step: [556] total_loss: 2.13048553 d_loss: 1.37430644, g_loss: 0.69847977, ae_loss: 0.05769925\n",
      "Step: [557] total_loss: 2.13023400 d_loss: 1.37587965, g_loss: 0.69485450, ae_loss: 0.05949988\n",
      "Step: [558] total_loss: 2.08546257 d_loss: 1.34745324, g_loss: 0.68117929, ae_loss: 0.05683002\n",
      "Step: [559] total_loss: 2.12617397 d_loss: 1.39550567, g_loss: 0.67482126, ae_loss: 0.05584699\n",
      "Step: [560] total_loss: 2.12000632 d_loss: 1.38207936, g_loss: 0.68178129, ae_loss: 0.05614565\n",
      "Step: [561] total_loss: 2.12099791 d_loss: 1.37412000, g_loss: 0.68966997, ae_loss: 0.05720809\n",
      "Step: [562] total_loss: 2.15097141 d_loss: 1.39318109, g_loss: 0.70093024, ae_loss: 0.05686020\n",
      "Step: [563] total_loss: 2.12054372 d_loss: 1.35812640, g_loss: 0.70530742, ae_loss: 0.05710986\n",
      "Step: [564] total_loss: 2.12999916 d_loss: 1.38168979, g_loss: 0.69419897, ae_loss: 0.05411054\n",
      "Step: [565] total_loss: 2.12669945 d_loss: 1.37684989, g_loss: 0.69173616, ae_loss: 0.05811340\n",
      "Step: [566] total_loss: 2.14780331 d_loss: 1.39131689, g_loss: 0.69683820, ae_loss: 0.05964829\n",
      "Step: [567] total_loss: 2.13110971 d_loss: 1.37339771, g_loss: 0.70094275, ae_loss: 0.05676909\n",
      "Step: [568] total_loss: 2.13639665 d_loss: 1.39100361, g_loss: 0.68777686, ae_loss: 0.05761625\n",
      "Step: [569] total_loss: 2.10581374 d_loss: 1.36075521, g_loss: 0.68905258, ae_loss: 0.05600601\n",
      "Step: [570] total_loss: 2.13450956 d_loss: 1.38828945, g_loss: 0.69309789, ae_loss: 0.05312224\n",
      "Step: [571] total_loss: 2.12113857 d_loss: 1.37630892, g_loss: 0.68903404, ae_loss: 0.05579551\n",
      "Step: [572] total_loss: 2.13021135 d_loss: 1.38498211, g_loss: 0.69187617, ae_loss: 0.05335310\n",
      "Step: [573] total_loss: 2.11257362 d_loss: 1.36261606, g_loss: 0.69764733, ae_loss: 0.05231032\n",
      "Step: [574] total_loss: 2.12341166 d_loss: 1.36769128, g_loss: 0.70161474, ae_loss: 0.05410557\n",
      "Step: [575] total_loss: 2.11826468 d_loss: 1.38309813, g_loss: 0.68172371, ae_loss: 0.05344293\n",
      "Step: [576] total_loss: 2.11905289 d_loss: 1.38194060, g_loss: 0.68491280, ae_loss: 0.05219962\n",
      "Step: [577] total_loss: 2.11126757 d_loss: 1.37803376, g_loss: 0.67783785, ae_loss: 0.05539612\n",
      "Step: [578] total_loss: 2.11090398 d_loss: 1.38008308, g_loss: 0.67236710, ae_loss: 0.05845380\n",
      "Step: [579] total_loss: 2.11852622 d_loss: 1.37378454, g_loss: 0.68765622, ae_loss: 0.05708553\n",
      "Step: [580] total_loss: 2.12567043 d_loss: 1.35272491, g_loss: 0.71444106, ae_loss: 0.05850442\n",
      "Step: [581] total_loss: 2.11843491 d_loss: 1.37499928, g_loss: 0.68596959, ae_loss: 0.05746592\n",
      "Step: [582] total_loss: 2.10030508 d_loss: 1.35682809, g_loss: 0.68889034, ae_loss: 0.05458665\n",
      "Step: [583] total_loss: 2.14541602 d_loss: 1.41228962, g_loss: 0.67680663, ae_loss: 0.05631981\n",
      "Step: [584] total_loss: 2.12731147 d_loss: 1.37905812, g_loss: 0.68842793, ae_loss: 0.05982540\n",
      "Step: [585] total_loss: 2.12551355 d_loss: 1.39011943, g_loss: 0.68288910, ae_loss: 0.05250485\n",
      "Step: [586] total_loss: 2.12269735 d_loss: 1.35597444, g_loss: 0.70975506, ae_loss: 0.05696794\n",
      "Step: [587] total_loss: 2.12191486 d_loss: 1.38781214, g_loss: 0.68104529, ae_loss: 0.05305756\n",
      "Step: [588] total_loss: 2.09686518 d_loss: 1.35657299, g_loss: 0.68550122, ae_loss: 0.05479110\n",
      "Step: [589] total_loss: 2.13022709 d_loss: 1.38405573, g_loss: 0.68808413, ae_loss: 0.05808711\n",
      "Step: [590] total_loss: 2.11366820 d_loss: 1.36997008, g_loss: 0.68685883, ae_loss: 0.05683918\n",
      "Step: [591] total_loss: 2.11688805 d_loss: 1.38351130, g_loss: 0.67766607, ae_loss: 0.05571061\n",
      "Step: [592] total_loss: 2.13102531 d_loss: 1.38446355, g_loss: 0.68927872, ae_loss: 0.05728316\n",
      "Step: [593] total_loss: 2.13216758 d_loss: 1.38745260, g_loss: 0.68810433, ae_loss: 0.05661055\n",
      "Step: [594] total_loss: 2.12192059 d_loss: 1.36561799, g_loss: 0.70195460, ae_loss: 0.05434816\n",
      "Step: [595] total_loss: 2.14095712 d_loss: 1.37479830, g_loss: 0.71312505, ae_loss: 0.05303386\n",
      "Step: [596] total_loss: 2.13422871 d_loss: 1.37261271, g_loss: 0.70492435, ae_loss: 0.05669160\n",
      "Step: [597] total_loss: 2.11500025 d_loss: 1.36621761, g_loss: 0.69616073, ae_loss: 0.05262182\n",
      "Step: [598] total_loss: 2.14249945 d_loss: 1.40390658, g_loss: 0.67938119, ae_loss: 0.05921160\n",
      "Step: [599] total_loss: 2.12146997 d_loss: 1.37746310, g_loss: 0.69191432, ae_loss: 0.05209268\n",
      "Step: [600] total_loss: 2.11828947 d_loss: 1.38398910, g_loss: 0.68051016, ae_loss: 0.05379033\n",
      "Step: [601] total_loss: 2.12459779 d_loss: 1.37745810, g_loss: 0.69196928, ae_loss: 0.05517047\n",
      "Step: [602] total_loss: 2.16103387 d_loss: 1.40965915, g_loss: 0.69398963, ae_loss: 0.05738504\n",
      "Step: [603] total_loss: 2.12513900 d_loss: 1.37905502, g_loss: 0.69026208, ae_loss: 0.05582193\n",
      "Step: [604] total_loss: 2.12105608 d_loss: 1.38194728, g_loss: 0.68406188, ae_loss: 0.05504691\n",
      "Step: [605] total_loss: 2.12480497 d_loss: 1.38683772, g_loss: 0.68140042, ae_loss: 0.05656676\n",
      "Step: [606] total_loss: 2.14412594 d_loss: 1.39483619, g_loss: 0.69386971, ae_loss: 0.05542006\n",
      "Step: [607] total_loss: 2.11710954 d_loss: 1.34846807, g_loss: 0.71344084, ae_loss: 0.05520060\n",
      "Step: [608] total_loss: 2.13625431 d_loss: 1.38218307, g_loss: 0.70005667, ae_loss: 0.05401469\n",
      "Step: [609] total_loss: 2.12627769 d_loss: 1.38008785, g_loss: 0.69164437, ae_loss: 0.05454555\n",
      "Step: [610] total_loss: 2.11224890 d_loss: 1.36668348, g_loss: 0.69476295, ae_loss: 0.05080245\n",
      "Step: [611] total_loss: 2.14575648 d_loss: 1.39236879, g_loss: 0.69751751, ae_loss: 0.05587012\n",
      "Step: [612] total_loss: 2.12924027 d_loss: 1.37664497, g_loss: 0.69704270, ae_loss: 0.05555256\n",
      "Step: [613] total_loss: 2.12167406 d_loss: 1.40037489, g_loss: 0.66572392, ae_loss: 0.05557508\n",
      "Step: [614] total_loss: 2.11189318 d_loss: 1.36964965, g_loss: 0.68622845, ae_loss: 0.05601506\n",
      "Step: [615] total_loss: 2.11385059 d_loss: 1.38469124, g_loss: 0.67328817, ae_loss: 0.05587123\n",
      "Step: [616] total_loss: 2.10072279 d_loss: 1.35415447, g_loss: 0.69211209, ae_loss: 0.05445613\n",
      "Step: [617] total_loss: 2.13025141 d_loss: 1.39721954, g_loss: 0.67841047, ae_loss: 0.05462132\n",
      "Step: [618] total_loss: 2.10662365 d_loss: 1.37895358, g_loss: 0.67488551, ae_loss: 0.05278473\n",
      "Step: [619] total_loss: 2.13780975 d_loss: 1.39029241, g_loss: 0.68838644, ae_loss: 0.05913079\n",
      "Step: [620] total_loss: 2.13808107 d_loss: 1.39422894, g_loss: 0.68937910, ae_loss: 0.05447303\n",
      "Step: [621] total_loss: 2.13198471 d_loss: 1.35350966, g_loss: 0.72541463, ae_loss: 0.05306025\n",
      "Step: [622] total_loss: 2.12043333 d_loss: 1.38099504, g_loss: 0.68347108, ae_loss: 0.05596704\n",
      "Step: [623] total_loss: 2.10421491 d_loss: 1.36754036, g_loss: 0.68076688, ae_loss: 0.05590759\n",
      "Step: [624] total_loss: 2.12724113 d_loss: 1.39127636, g_loss: 0.68070459, ae_loss: 0.05526007\n",
      "Step: [625] total_loss: 2.12937903 d_loss: 1.39642406, g_loss: 0.67917132, ae_loss: 0.05378370\n",
      "Step: [626] total_loss: 2.13818932 d_loss: 1.39104235, g_loss: 0.69154394, ae_loss: 0.05560321\n",
      "Step: [627] total_loss: 2.14067101 d_loss: 1.37857151, g_loss: 0.70640033, ae_loss: 0.05569907\n",
      "Step: [628] total_loss: 2.14314079 d_loss: 1.40023494, g_loss: 0.68726492, ae_loss: 0.05564099\n",
      "Step: [629] total_loss: 2.11424351 d_loss: 1.36065245, g_loss: 0.69830334, ae_loss: 0.05528779\n",
      "Step: [630] total_loss: 2.13156557 d_loss: 1.36698043, g_loss: 0.71015954, ae_loss: 0.05442548\n",
      "Step: [631] total_loss: 2.12994337 d_loss: 1.38544142, g_loss: 0.69006908, ae_loss: 0.05443301\n",
      "Step: [632] total_loss: 2.12187004 d_loss: 1.37081242, g_loss: 0.70011324, ae_loss: 0.05094428\n",
      "Step: [633] total_loss: 2.10975790 d_loss: 1.37574863, g_loss: 0.67841327, ae_loss: 0.05559601\n",
      "Step: [634] total_loss: 2.13903499 d_loss: 1.39942896, g_loss: 0.68608946, ae_loss: 0.05351649\n",
      "Step: [635] total_loss: 2.11988449 d_loss: 1.38219273, g_loss: 0.68087214, ae_loss: 0.05681952\n",
      "Step: [636] total_loss: 2.10260725 d_loss: 1.36360097, g_loss: 0.68597531, ae_loss: 0.05303093\n",
      "Step: [637] total_loss: 2.12583303 d_loss: 1.37551737, g_loss: 0.69763917, ae_loss: 0.05267653\n",
      "Step: [638] total_loss: 2.12527800 d_loss: 1.38981509, g_loss: 0.68043911, ae_loss: 0.05502375\n",
      "Step: [639] total_loss: 2.12518930 d_loss: 1.36847758, g_loss: 0.70115709, ae_loss: 0.05555454\n",
      "Step: [640] total_loss: 2.14966631 d_loss: 1.37240577, g_loss: 0.72041023, ae_loss: 0.05685020\n",
      "Step: [641] total_loss: 2.12358713 d_loss: 1.36465454, g_loss: 0.70060962, ae_loss: 0.05832285\n",
      "Step: [642] total_loss: 2.14325309 d_loss: 1.40410721, g_loss: 0.68418175, ae_loss: 0.05496402\n",
      "Step: [643] total_loss: 2.12975669 d_loss: 1.38578761, g_loss: 0.69047052, ae_loss: 0.05349848\n",
      "Step: [644] total_loss: 2.13560700 d_loss: 1.38234472, g_loss: 0.69694364, ae_loss: 0.05631868\n",
      "Step: [645] total_loss: 2.12968063 d_loss: 1.39178514, g_loss: 0.68129706, ae_loss: 0.05659851\n",
      "Step: [646] total_loss: 2.15773392 d_loss: 1.38619804, g_loss: 0.71337605, ae_loss: 0.05815976\n",
      "Step: [647] total_loss: 2.10504508 d_loss: 1.35845923, g_loss: 0.69550991, ae_loss: 0.05107599\n",
      "Step: [648] total_loss: 2.12046766 d_loss: 1.36055088, g_loss: 0.70371974, ae_loss: 0.05619707\n",
      "Step: [649] total_loss: 2.14926481 d_loss: 1.39890349, g_loss: 0.69449085, ae_loss: 0.05587040\n",
      "Step: [650] total_loss: 2.13385820 d_loss: 1.38279283, g_loss: 0.69534147, ae_loss: 0.05572378\n",
      "Step: [651] total_loss: 2.12082386 d_loss: 1.36806989, g_loss: 0.70039451, ae_loss: 0.05235931\n",
      "Step: [652] total_loss: 2.12211275 d_loss: 1.36899829, g_loss: 0.69629228, ae_loss: 0.05682229\n",
      "Step: [653] total_loss: 2.13445568 d_loss: 1.39159727, g_loss: 0.68636882, ae_loss: 0.05648964\n",
      "Step: [654] total_loss: 2.11976480 d_loss: 1.37843323, g_loss: 0.68821061, ae_loss: 0.05312096\n",
      "Step: [655] total_loss: 2.11362386 d_loss: 1.37204885, g_loss: 0.68634892, ae_loss: 0.05522607\n",
      "Step: [656] total_loss: 2.09080529 d_loss: 1.35346150, g_loss: 0.68545467, ae_loss: 0.05188905\n",
      "Step: [657] total_loss: 2.13067651 d_loss: 1.36961305, g_loss: 0.70447892, ae_loss: 0.05658453\n",
      "Step: [658] total_loss: 2.13235092 d_loss: 1.37957680, g_loss: 0.69561338, ae_loss: 0.05716064\n",
      "Step: [659] total_loss: 2.12719536 d_loss: 1.37848210, g_loss: 0.69421965, ae_loss: 0.05449365\n",
      "Step: [660] total_loss: 2.12764311 d_loss: 1.36960232, g_loss: 0.70412004, ae_loss: 0.05392057\n",
      "Step: [661] total_loss: 2.13928199 d_loss: 1.38392949, g_loss: 0.69642884, ae_loss: 0.05892367\n",
      "Step: [662] total_loss: 2.15260720 d_loss: 1.38151658, g_loss: 0.71500564, ae_loss: 0.05608499\n",
      "Step: [663] total_loss: 2.11399770 d_loss: 1.35948062, g_loss: 0.70227849, ae_loss: 0.05223863\n",
      "Step: [664] total_loss: 2.11761880 d_loss: 1.37885559, g_loss: 0.68316710, ae_loss: 0.05559611\n",
      "Step: [665] total_loss: 2.12105489 d_loss: 1.34903753, g_loss: 0.71788305, ae_loss: 0.05413434\n",
      "Step: [666] total_loss: 2.11617541 d_loss: 1.36308861, g_loss: 0.69685382, ae_loss: 0.05623306\n",
      "Step: [667] total_loss: 2.13320923 d_loss: 1.38072670, g_loss: 0.69221395, ae_loss: 0.06026846\n",
      "Step: [668] total_loss: 2.11323857 d_loss: 1.37509871, g_loss: 0.68086463, ae_loss: 0.05727512\n",
      "Step: [669] total_loss: 2.11066532 d_loss: 1.33977127, g_loss: 0.71447110, ae_loss: 0.05642292\n",
      "Step: [670] total_loss: 2.11816621 d_loss: 1.34058309, g_loss: 0.72461826, ae_loss: 0.05296484\n",
      "Step: [671] total_loss: 2.11456394 d_loss: 1.37079859, g_loss: 0.69158506, ae_loss: 0.05218018\n",
      "Step: [672] total_loss: 2.09906173 d_loss: 1.35521555, g_loss: 0.69310039, ae_loss: 0.05074590\n",
      "Step: [673] total_loss: 2.14519238 d_loss: 1.37802052, g_loss: 0.71342123, ae_loss: 0.05375062\n",
      "Step: [674] total_loss: 2.11804295 d_loss: 1.35268617, g_loss: 0.71043342, ae_loss: 0.05492347\n",
      "Step: [675] total_loss: 2.10659027 d_loss: 1.38337970, g_loss: 0.66660500, ae_loss: 0.05660575\n",
      "Step: [676] total_loss: 2.11770988 d_loss: 1.39219511, g_loss: 0.66972119, ae_loss: 0.05579364\n",
      "Step: [677] total_loss: 2.12477636 d_loss: 1.37457442, g_loss: 0.70079601, ae_loss: 0.04940609\n",
      "Step: [678] total_loss: 2.12670302 d_loss: 1.36921024, g_loss: 0.70444924, ae_loss: 0.05304348\n",
      "Step: [679] total_loss: 2.12235093 d_loss: 1.36874032, g_loss: 0.70648754, ae_loss: 0.04712310\n",
      "Step: [680] total_loss: 2.14112496 d_loss: 1.37975812, g_loss: 0.70830232, ae_loss: 0.05306463\n",
      "Step: [681] total_loss: 2.14570665 d_loss: 1.38177550, g_loss: 0.71259111, ae_loss: 0.05134007\n",
      "Step: [682] total_loss: 2.14208174 d_loss: 1.37917256, g_loss: 0.71114421, ae_loss: 0.05176495\n",
      "Step: [683] total_loss: 2.13366175 d_loss: 1.36066985, g_loss: 0.71916127, ae_loss: 0.05383046\n",
      "Step: [684] total_loss: 2.13019276 d_loss: 1.39292169, g_loss: 0.68353999, ae_loss: 0.05373098\n",
      "Step: [685] total_loss: 2.12817168 d_loss: 1.38910639, g_loss: 0.68271124, ae_loss: 0.05635400\n",
      "Step: [686] total_loss: 2.13863254 d_loss: 1.38203633, g_loss: 0.70212120, ae_loss: 0.05447507\n",
      "Step: [687] total_loss: 2.10838842 d_loss: 1.35115528, g_loss: 0.70804751, ae_loss: 0.04918551\n",
      "Step: [688] total_loss: 2.12098408 d_loss: 1.38867581, g_loss: 0.67803872, ae_loss: 0.05426949\n",
      "Step: [689] total_loss: 2.11676764 d_loss: 1.39329481, g_loss: 0.67028350, ae_loss: 0.05318942\n",
      "Step: [690] total_loss: 2.09593821 d_loss: 1.36243820, g_loss: 0.68094295, ae_loss: 0.05255706\n",
      "Step: [691] total_loss: 2.10970712 d_loss: 1.35988498, g_loss: 0.69334728, ae_loss: 0.05647489\n",
      "Step: [692] total_loss: 2.12184191 d_loss: 1.35935688, g_loss: 0.70573282, ae_loss: 0.05675213\n",
      "Step: [693] total_loss: 2.14084196 d_loss: 1.39755201, g_loss: 0.69177538, ae_loss: 0.05151457\n",
      "Step: [694] total_loss: 2.15758657 d_loss: 1.39347506, g_loss: 0.70952028, ae_loss: 0.05459130\n",
      "Step: [695] total_loss: 2.13377142 d_loss: 1.38085341, g_loss: 0.69727826, ae_loss: 0.05563984\n",
      "Step: [696] total_loss: 2.14846992 d_loss: 1.39236760, g_loss: 0.70078349, ae_loss: 0.05531891\n",
      "Step: [697] total_loss: 2.13645148 d_loss: 1.37721515, g_loss: 0.70089877, ae_loss: 0.05833754\n",
      "Step: [698] total_loss: 2.14131594 d_loss: 1.39820337, g_loss: 0.68728757, ae_loss: 0.05582485\n",
      "Step: [699] total_loss: 2.13540411 d_loss: 1.38976359, g_loss: 0.68744457, ae_loss: 0.05819602\n",
      "Step: [700] total_loss: 2.14402986 d_loss: 1.38651466, g_loss: 0.69787073, ae_loss: 0.05964446\n",
      "Step: [701] total_loss: 2.13504720 d_loss: 1.38065982, g_loss: 0.69908702, ae_loss: 0.05530033\n",
      "Step: [702] total_loss: 2.11842608 d_loss: 1.36956859, g_loss: 0.69710076, ae_loss: 0.05175675\n",
      "Step: [703] total_loss: 2.11819029 d_loss: 1.37599516, g_loss: 0.68997711, ae_loss: 0.05221811\n",
      "Step: [704] total_loss: 2.12956572 d_loss: 1.40925527, g_loss: 0.66652411, ae_loss: 0.05378623\n",
      "Step: [705] total_loss: 2.12087822 d_loss: 1.39658916, g_loss: 0.67094374, ae_loss: 0.05334518\n",
      "Step: [706] total_loss: 2.13299227 d_loss: 1.39504671, g_loss: 0.68484175, ae_loss: 0.05310392\n",
      "Step: [707] total_loss: 2.13020945 d_loss: 1.38134181, g_loss: 0.69230229, ae_loss: 0.05656546\n",
      "Step: [708] total_loss: 2.11894202 d_loss: 1.37528217, g_loss: 0.69199318, ae_loss: 0.05166674\n",
      "Step: [709] total_loss: 2.13171959 d_loss: 1.36716807, g_loss: 0.70833242, ae_loss: 0.05621925\n",
      "Step: [710] total_loss: 2.13227463 d_loss: 1.38339138, g_loss: 0.69567370, ae_loss: 0.05320960\n",
      "Step: [711] total_loss: 2.13197851 d_loss: 1.40361929, g_loss: 0.67118001, ae_loss: 0.05717925\n",
      "Step: [712] total_loss: 2.11409903 d_loss: 1.37531972, g_loss: 0.68107784, ae_loss: 0.05770142\n",
      "Step: [713] total_loss: 2.12415981 d_loss: 1.38476300, g_loss: 0.68552482, ae_loss: 0.05387200\n",
      "Step: [714] total_loss: 2.13780069 d_loss: 1.38760078, g_loss: 0.69376510, ae_loss: 0.05643479\n",
      "Step: [715] total_loss: 2.14519000 d_loss: 1.37270880, g_loss: 0.71647066, ae_loss: 0.05601066\n",
      "Step: [716] total_loss: 2.11430097 d_loss: 1.37808967, g_loss: 0.68735313, ae_loss: 0.04885815\n",
      "Step: [717] total_loss: 2.12910485 d_loss: 1.38090420, g_loss: 0.69575799, ae_loss: 0.05244269\n",
      "Step: [718] total_loss: 2.11073208 d_loss: 1.36907148, g_loss: 0.69082069, ae_loss: 0.05084000\n",
      "Step: [719] total_loss: 2.09008312 d_loss: 1.36800432, g_loss: 0.66996157, ae_loss: 0.05211727\n",
      "Step: [720] total_loss: 2.12301683 d_loss: 1.35536838, g_loss: 0.71275485, ae_loss: 0.05489357\n",
      "Step: [721] total_loss: 2.10578823 d_loss: 1.37809479, g_loss: 0.67453134, ae_loss: 0.05316206\n",
      "Step: [722] total_loss: 2.13125372 d_loss: 1.37901974, g_loss: 0.69471633, ae_loss: 0.05751762\n",
      "Step: [723] total_loss: 2.11669588 d_loss: 1.37577379, g_loss: 0.68765712, ae_loss: 0.05326514\n",
      "Step: [724] total_loss: 2.13164425 d_loss: 1.37888241, g_loss: 0.70057499, ae_loss: 0.05218695\n",
      "Step: [725] total_loss: 2.10750389 d_loss: 1.37063372, g_loss: 0.67793167, ae_loss: 0.05893855\n",
      "Step: [726] total_loss: 2.13412404 d_loss: 1.36143470, g_loss: 0.71617138, ae_loss: 0.05651794\n",
      "Step: [727] total_loss: 2.11706710 d_loss: 1.36633348, g_loss: 0.69661039, ae_loss: 0.05412319\n",
      "Step: [728] total_loss: 2.13104534 d_loss: 1.38833332, g_loss: 0.68879402, ae_loss: 0.05391802\n",
      "Step: [729] total_loss: 2.12598419 d_loss: 1.39760566, g_loss: 0.67282283, ae_loss: 0.05555561\n",
      "Step: [730] total_loss: 2.12692857 d_loss: 1.37858975, g_loss: 0.69491124, ae_loss: 0.05342761\n",
      "Step: [731] total_loss: 2.12464595 d_loss: 1.39702916, g_loss: 0.67626506, ae_loss: 0.05135173\n",
      "Step: [732] total_loss: 2.12099600 d_loss: 1.36809301, g_loss: 0.69763935, ae_loss: 0.05526381\n",
      "Step: [733] total_loss: 2.12378049 d_loss: 1.37768602, g_loss: 0.69188344, ae_loss: 0.05421106\n",
      "Step: [734] total_loss: 2.12213874 d_loss: 1.36750436, g_loss: 0.69879985, ae_loss: 0.05583449\n",
      "Step: [735] total_loss: 2.14232588 d_loss: 1.37257540, g_loss: 0.71696979, ae_loss: 0.05278062\n",
      "Step: [736] total_loss: 2.16045690 d_loss: 1.39602995, g_loss: 0.71127170, ae_loss: 0.05315524\n",
      "Step: [737] total_loss: 2.14410400 d_loss: 1.39669991, g_loss: 0.69135547, ae_loss: 0.05604850\n",
      "Step: [738] total_loss: 2.11635661 d_loss: 1.37915707, g_loss: 0.68278617, ae_loss: 0.05441339\n",
      "Step: [739] total_loss: 2.12695289 d_loss: 1.38559759, g_loss: 0.68597162, ae_loss: 0.05538373\n",
      "Step: [740] total_loss: 2.10589600 d_loss: 1.36027169, g_loss: 0.69109780, ae_loss: 0.05452653\n",
      "Step: [741] total_loss: 2.14040089 d_loss: 1.41752195, g_loss: 0.67039013, ae_loss: 0.05248878\n",
      "Step: [742] total_loss: 2.12928200 d_loss: 1.37368202, g_loss: 0.70304012, ae_loss: 0.05256000\n",
      "Step: [743] total_loss: 2.13963461 d_loss: 1.39342213, g_loss: 0.69640219, ae_loss: 0.04981023\n",
      "Step: [744] total_loss: 2.13252115 d_loss: 1.38345218, g_loss: 0.69889832, ae_loss: 0.05017071\n",
      "Step: [745] total_loss: 2.14494252 d_loss: 1.40371978, g_loss: 0.68717241, ae_loss: 0.05405027\n",
      "Step: [746] total_loss: 2.12272859 d_loss: 1.39471555, g_loss: 0.67843324, ae_loss: 0.04957981\n",
      "Step: [747] total_loss: 2.14072847 d_loss: 1.36990905, g_loss: 0.71359885, ae_loss: 0.05722055\n",
      "Step: [748] total_loss: 2.11601734 d_loss: 1.35985017, g_loss: 0.70179999, ae_loss: 0.05436702\n",
      "Step: [749] total_loss: 2.11082506 d_loss: 1.38720965, g_loss: 0.67118829, ae_loss: 0.05242712\n",
      "Step: [750] total_loss: 2.11181736 d_loss: 1.36188889, g_loss: 0.69660497, ae_loss: 0.05332366\n",
      "Step: [751] total_loss: 2.10759115 d_loss: 1.35756242, g_loss: 0.68951440, ae_loss: 0.06051429\n",
      "Step: [752] total_loss: 2.12173367 d_loss: 1.37409568, g_loss: 0.69206023, ae_loss: 0.05557789\n",
      "Step: [753] total_loss: 2.13643241 d_loss: 1.38375556, g_loss: 0.69543743, ae_loss: 0.05723945\n",
      "Step: [754] total_loss: 2.11055374 d_loss: 1.37507868, g_loss: 0.68364131, ae_loss: 0.05183381\n",
      "Step: [755] total_loss: 2.11710119 d_loss: 1.36632335, g_loss: 0.69498396, ae_loss: 0.05579384\n",
      "Step: [756] total_loss: 2.14241123 d_loss: 1.39221036, g_loss: 0.69772255, ae_loss: 0.05247827\n",
      "Step: [757] total_loss: 2.14556456 d_loss: 1.38153446, g_loss: 0.71137244, ae_loss: 0.05265757\n",
      "Step: [758] total_loss: 2.13449955 d_loss: 1.38228631, g_loss: 0.69969225, ae_loss: 0.05252109\n",
      "Step: [759] total_loss: 2.12968493 d_loss: 1.38675833, g_loss: 0.68467879, ae_loss: 0.05824787\n",
      "Step: [760] total_loss: 2.13113689 d_loss: 1.37810600, g_loss: 0.70212615, ae_loss: 0.05090484\n",
      "Step: [761] total_loss: 2.09575582 d_loss: 1.35337520, g_loss: 0.68416947, ae_loss: 0.05821104\n",
      "Step: [762] total_loss: 2.12198353 d_loss: 1.39546514, g_loss: 0.67475492, ae_loss: 0.05176338\n",
      "Step: [763] total_loss: 2.14731789 d_loss: 1.38868332, g_loss: 0.70691311, ae_loss: 0.05172157\n",
      "Step: [764] total_loss: 2.12534714 d_loss: 1.35836589, g_loss: 0.71490002, ae_loss: 0.05208139\n",
      "Step: [765] total_loss: 2.13328075 d_loss: 1.37193513, g_loss: 0.70483792, ae_loss: 0.05650784\n",
      "Step: [766] total_loss: 2.12333083 d_loss: 1.37165809, g_loss: 0.69730449, ae_loss: 0.05436822\n",
      "Step: [767] total_loss: 2.13976526 d_loss: 1.37265623, g_loss: 0.71362913, ae_loss: 0.05348009\n",
      "Step: [768] total_loss: 2.14631653 d_loss: 1.39640129, g_loss: 0.69465297, ae_loss: 0.05526219\n",
      "Step: [769] total_loss: 2.14097309 d_loss: 1.38473809, g_loss: 0.70347762, ae_loss: 0.05275747\n",
      "Step: [770] total_loss: 2.14479256 d_loss: 1.39752221, g_loss: 0.69543946, ae_loss: 0.05183096\n",
      "Step: [771] total_loss: 2.12792516 d_loss: 1.38341141, g_loss: 0.69452065, ae_loss: 0.04999319\n",
      "Step: [772] total_loss: 2.14456511 d_loss: 1.40221095, g_loss: 0.69150376, ae_loss: 0.05085025\n",
      "Step: [773] total_loss: 2.14703965 d_loss: 1.40248394, g_loss: 0.69214445, ae_loss: 0.05241131\n",
      "Step: [774] total_loss: 2.13343692 d_loss: 1.38911617, g_loss: 0.69135207, ae_loss: 0.05296866\n",
      "Step: [775] total_loss: 2.13834286 d_loss: 1.38908660, g_loss: 0.69643903, ae_loss: 0.05281724\n",
      "Step: [776] total_loss: 2.13524175 d_loss: 1.39218402, g_loss: 0.68940485, ae_loss: 0.05365293\n",
      "Step: [777] total_loss: 2.13745403 d_loss: 1.37557971, g_loss: 0.70684004, ae_loss: 0.05503444\n",
      "Step: [778] total_loss: 2.10946488 d_loss: 1.37197471, g_loss: 0.68765163, ae_loss: 0.04983853\n",
      "Step: [779] total_loss: 2.11357641 d_loss: 1.38891602, g_loss: 0.67124784, ae_loss: 0.05341244\n",
      "Step: [780] total_loss: 2.10639381 d_loss: 1.38327074, g_loss: 0.66976541, ae_loss: 0.05335755\n",
      "Step: [781] total_loss: 2.12470770 d_loss: 1.39763319, g_loss: 0.67297614, ae_loss: 0.05409820\n",
      "Step: [782] total_loss: 2.11966658 d_loss: 1.37956786, g_loss: 0.68053859, ae_loss: 0.05956003\n",
      "Step: [783] total_loss: 2.12816334 d_loss: 1.37023246, g_loss: 0.70312262, ae_loss: 0.05480828\n",
      "Step: [784] total_loss: 2.14288068 d_loss: 1.38052642, g_loss: 0.70971638, ae_loss: 0.05263795\n",
      "Step: [785] total_loss: 2.14691639 d_loss: 1.38550043, g_loss: 0.70352030, ae_loss: 0.05789573\n",
      "Step: [786] total_loss: 2.13152575 d_loss: 1.37514830, g_loss: 0.70148963, ae_loss: 0.05488774\n",
      "Step: [787] total_loss: 2.12588453 d_loss: 1.37529778, g_loss: 0.69731605, ae_loss: 0.05327079\n",
      "Step: [788] total_loss: 2.11873531 d_loss: 1.35611057, g_loss: 0.71005797, ae_loss: 0.05256675\n",
      "Step: [789] total_loss: 2.11747122 d_loss: 1.35649014, g_loss: 0.70997870, ae_loss: 0.05100239\n",
      "Step: [790] total_loss: 2.13143802 d_loss: 1.38962889, g_loss: 0.69008833, ae_loss: 0.05172077\n",
      "Step: [791] total_loss: 2.10956955 d_loss: 1.37368751, g_loss: 0.68111420, ae_loss: 0.05476775\n",
      "Step: [792] total_loss: 2.10231638 d_loss: 1.34028995, g_loss: 0.70923394, ae_loss: 0.05279260\n",
      "Step: [793] total_loss: 2.12931204 d_loss: 1.38700604, g_loss: 0.68990314, ae_loss: 0.05240271\n",
      "Step: [794] total_loss: 2.11408091 d_loss: 1.37332880, g_loss: 0.68938255, ae_loss: 0.05136961\n",
      "Step: [795] total_loss: 2.13320446 d_loss: 1.38939261, g_loss: 0.69273674, ae_loss: 0.05107507\n",
      "Step: [796] total_loss: 2.10507917 d_loss: 1.36438012, g_loss: 0.68614036, ae_loss: 0.05455869\n",
      "Step: [797] total_loss: 2.12582207 d_loss: 1.38989294, g_loss: 0.67630327, ae_loss: 0.05962589\n",
      "Step: [798] total_loss: 2.12477374 d_loss: 1.37600589, g_loss: 0.69607419, ae_loss: 0.05269364\n",
      "Step: [799] total_loss: 2.13012338 d_loss: 1.38029361, g_loss: 0.69974947, ae_loss: 0.05008031\n",
      "Step: [800] total_loss: 2.11605549 d_loss: 1.37783933, g_loss: 0.68236130, ae_loss: 0.05585494\n",
      "Step: [801] total_loss: 2.13611770 d_loss: 1.39457750, g_loss: 0.68536210, ae_loss: 0.05617807\n",
      "Step: [802] total_loss: 2.14031386 d_loss: 1.37552142, g_loss: 0.70890367, ae_loss: 0.05588877\n",
      "Step: [803] total_loss: 2.15010595 d_loss: 1.39202654, g_loss: 0.69843805, ae_loss: 0.05964137\n",
      "Step: [804] total_loss: 2.13223529 d_loss: 1.36991441, g_loss: 0.71049190, ae_loss: 0.05182894\n",
      "Step: [805] total_loss: 2.12911010 d_loss: 1.37946033, g_loss: 0.69875133, ae_loss: 0.05089843\n",
      "Step: [806] total_loss: 2.12476110 d_loss: 1.37143540, g_loss: 0.70039958, ae_loss: 0.05292606\n",
      "Step: [807] total_loss: 2.11841941 d_loss: 1.38396144, g_loss: 0.68494976, ae_loss: 0.04950821\n",
      "Step: [808] total_loss: 2.10159159 d_loss: 1.38039184, g_loss: 0.66997057, ae_loss: 0.05122922\n",
      "Step: [809] total_loss: 2.11600542 d_loss: 1.35885036, g_loss: 0.70290017, ae_loss: 0.05425479\n",
      "Step: [810] total_loss: 2.10450602 d_loss: 1.38702583, g_loss: 0.66301274, ae_loss: 0.05446755\n",
      "Step: [811] total_loss: 2.13144541 d_loss: 1.40151525, g_loss: 0.67563033, ae_loss: 0.05429978\n",
      "Step: [812] total_loss: 2.11889434 d_loss: 1.38035631, g_loss: 0.68606448, ae_loss: 0.05247356\n",
      "Step: [813] total_loss: 2.12945223 d_loss: 1.36876905, g_loss: 0.70477116, ae_loss: 0.05591210\n",
      "Step: [814] total_loss: 2.11358070 d_loss: 1.36458313, g_loss: 0.69436252, ae_loss: 0.05463492\n",
      "Step: [815] total_loss: 2.13678026 d_loss: 1.38498676, g_loss: 0.69473797, ae_loss: 0.05705544\n",
      "Step: [816] total_loss: 2.14322162 d_loss: 1.39256203, g_loss: 0.69412720, ae_loss: 0.05653241\n",
      "Step: [817] total_loss: 2.13612795 d_loss: 1.36479509, g_loss: 0.71305525, ae_loss: 0.05827777\n",
      "Step: [818] total_loss: 2.12813807 d_loss: 1.39501190, g_loss: 0.68134332, ae_loss: 0.05178275\n",
      "Step: [819] total_loss: 2.12905121 d_loss: 1.37546325, g_loss: 0.70177180, ae_loss: 0.05181613\n",
      "Step: [820] total_loss: 2.11437750 d_loss: 1.36195254, g_loss: 0.69829458, ae_loss: 0.05413030\n",
      "Step: [821] total_loss: 2.12175679 d_loss: 1.36073482, g_loss: 0.70922601, ae_loss: 0.05179597\n",
      "Step: [822] total_loss: 2.11558771 d_loss: 1.38085866, g_loss: 0.68073541, ae_loss: 0.05399354\n",
      "Step: [823] total_loss: 2.16438866 d_loss: 1.43486583, g_loss: 0.67226923, ae_loss: 0.05725361\n",
      "Step: [824] total_loss: 2.11609364 d_loss: 1.37432694, g_loss: 0.69183254, ae_loss: 0.04993404\n",
      "Step: [825] total_loss: 2.11008596 d_loss: 1.37572026, g_loss: 0.68017423, ae_loss: 0.05419135\n",
      "Step: [826] total_loss: 2.12501788 d_loss: 1.37413490, g_loss: 0.69381016, ae_loss: 0.05707292\n",
      "Step: [827] total_loss: 2.10983706 d_loss: 1.37463307, g_loss: 0.68324268, ae_loss: 0.05196146\n",
      "Step: [828] total_loss: 2.12604809 d_loss: 1.38551128, g_loss: 0.68278575, ae_loss: 0.05775113\n",
      "Step: [829] total_loss: 2.12121248 d_loss: 1.38429606, g_loss: 0.67966771, ae_loss: 0.05724862\n",
      "Step: [830] total_loss: 2.13916659 d_loss: 1.37705779, g_loss: 0.70607519, ae_loss: 0.05603367\n",
      "Step: [831] total_loss: 2.12520599 d_loss: 1.38787436, g_loss: 0.68289435, ae_loss: 0.05443711\n",
      "Step: [832] total_loss: 2.14745617 d_loss: 1.38960075, g_loss: 0.70685828, ae_loss: 0.05099718\n",
      "Step: [833] total_loss: 2.15444684 d_loss: 1.41363192, g_loss: 0.68943816, ae_loss: 0.05137685\n",
      "Step: [834] total_loss: 2.12256336 d_loss: 1.38624012, g_loss: 0.68195117, ae_loss: 0.05437220\n",
      "Step: [835] total_loss: 2.14697695 d_loss: 1.39041853, g_loss: 0.69918203, ae_loss: 0.05737645\n",
      "Step: [836] total_loss: 2.14878750 d_loss: 1.39902377, g_loss: 0.69438124, ae_loss: 0.05538233\n",
      "Step: [837] total_loss: 2.13976717 d_loss: 1.36480260, g_loss: 0.72084355, ae_loss: 0.05412086\n",
      "Step: [838] total_loss: 2.12307692 d_loss: 1.38397419, g_loss: 0.68376601, ae_loss: 0.05533668\n",
      "Step: [839] total_loss: 2.11968946 d_loss: 1.36572194, g_loss: 0.70156741, ae_loss: 0.05240026\n",
      "Step: [840] total_loss: 2.12646818 d_loss: 1.38310814, g_loss: 0.69287896, ae_loss: 0.05048105\n",
      "Step: [841] total_loss: 2.14692712 d_loss: 1.39139271, g_loss: 0.70046490, ae_loss: 0.05506956\n",
      "Step: [842] total_loss: 2.11447930 d_loss: 1.36608243, g_loss: 0.69500065, ae_loss: 0.05339619\n",
      "Step: [843] total_loss: 2.11524487 d_loss: 1.37567258, g_loss: 0.68639183, ae_loss: 0.05318028\n",
      "Step: [844] total_loss: 2.12026691 d_loss: 1.39132822, g_loss: 0.67499238, ae_loss: 0.05394632\n",
      "Step: [845] total_loss: 2.12459850 d_loss: 1.36750710, g_loss: 0.70257699, ae_loss: 0.05451455\n",
      "Step: [846] total_loss: 2.12962842 d_loss: 1.37711143, g_loss: 0.70026982, ae_loss: 0.05224717\n",
      "Step: [847] total_loss: 2.11139965 d_loss: 1.36347914, g_loss: 0.69373292, ae_loss: 0.05418749\n",
      "Step: [848] total_loss: 2.12227583 d_loss: 1.38184202, g_loss: 0.68305695, ae_loss: 0.05737703\n",
      "Step: [849] total_loss: 2.12731624 d_loss: 1.38443756, g_loss: 0.68626243, ae_loss: 0.05661633\n",
      "Step: [850] total_loss: 2.12336802 d_loss: 1.38609672, g_loss: 0.68559098, ae_loss: 0.05168032\n",
      "Step: [851] total_loss: 2.10766745 d_loss: 1.38088942, g_loss: 0.66823316, ae_loss: 0.05854492\n",
      "Step: [852] total_loss: 2.11969471 d_loss: 1.38828528, g_loss: 0.67969847, ae_loss: 0.05171093\n",
      "Step: [853] total_loss: 2.12676811 d_loss: 1.37783122, g_loss: 0.69817626, ae_loss: 0.05076046\n",
      "Step: [854] total_loss: 2.14851570 d_loss: 1.37944126, g_loss: 0.71494031, ae_loss: 0.05413426\n",
      "Step: [855] total_loss: 2.11951327 d_loss: 1.36790490, g_loss: 0.69408691, ae_loss: 0.05752142\n",
      "Step: [856] total_loss: 2.12117481 d_loss: 1.36792719, g_loss: 0.69925827, ae_loss: 0.05398944\n",
      "Step: [857] total_loss: 2.13445330 d_loss: 1.38924432, g_loss: 0.69004935, ae_loss: 0.05515972\n",
      "Step: [858] total_loss: 2.11536670 d_loss: 1.36668396, g_loss: 0.69675112, ae_loss: 0.05193166\n",
      "Step: [859] total_loss: 2.11990261 d_loss: 1.38161552, g_loss: 0.68391109, ae_loss: 0.05437604\n",
      "Step: [860] total_loss: 2.11267543 d_loss: 1.37124658, g_loss: 0.68898886, ae_loss: 0.05243993\n",
      "Step: [861] total_loss: 2.12747979 d_loss: 1.37261081, g_loss: 0.70047450, ae_loss: 0.05439444\n",
      "Step: [862] total_loss: 2.14114475 d_loss: 1.40037847, g_loss: 0.68796027, ae_loss: 0.05280615\n",
      "Step: [863] total_loss: 2.13084197 d_loss: 1.38519204, g_loss: 0.69477344, ae_loss: 0.05087651\n",
      "Step: [864] total_loss: 2.15165329 d_loss: 1.39722872, g_loss: 0.69880307, ae_loss: 0.05562164\n",
      "Step: [865] total_loss: 2.10353041 d_loss: 1.36391914, g_loss: 0.69079781, ae_loss: 0.04881349\n",
      "Step: [866] total_loss: 2.11946535 d_loss: 1.39132524, g_loss: 0.67709291, ae_loss: 0.05104718\n",
      "Step: [867] total_loss: 2.10729504 d_loss: 1.34931481, g_loss: 0.70181417, ae_loss: 0.05616589\n",
      "Step: [868] total_loss: 2.14197922 d_loss: 1.40619564, g_loss: 0.68407750, ae_loss: 0.05170623\n",
      "Step: [869] total_loss: 2.13672304 d_loss: 1.34925187, g_loss: 0.73264337, ae_loss: 0.05482764\n",
      "Step: [870] total_loss: 2.13640833 d_loss: 1.37908483, g_loss: 0.70458043, ae_loss: 0.05274291\n",
      "Step: [871] total_loss: 2.14488530 d_loss: 1.38836169, g_loss: 0.70299083, ae_loss: 0.05353286\n",
      "Step: [872] total_loss: 2.13092256 d_loss: 1.38227105, g_loss: 0.69435912, ae_loss: 0.05429249\n",
      "Step: [873] total_loss: 2.13042212 d_loss: 1.38767195, g_loss: 0.68982804, ae_loss: 0.05292197\n",
      "Step: [874] total_loss: 2.15127397 d_loss: 1.41299415, g_loss: 0.68521279, ae_loss: 0.05306692\n",
      "Step: [875] total_loss: 2.13867521 d_loss: 1.38262284, g_loss: 0.70724064, ae_loss: 0.04881173\n",
      "Step: [876] total_loss: 2.13012791 d_loss: 1.40254128, g_loss: 0.67335027, ae_loss: 0.05423636\n",
      "Step: [877] total_loss: 2.10320711 d_loss: 1.37931097, g_loss: 0.67063749, ae_loss: 0.05325881\n",
      "Step: [878] total_loss: 2.12585688 d_loss: 1.38624191, g_loss: 0.68587536, ae_loss: 0.05373966\n",
      "Step: [879] total_loss: 2.13127899 d_loss: 1.39478588, g_loss: 0.68134379, ae_loss: 0.05514927\n",
      "Step: [880] total_loss: 2.12513947 d_loss: 1.38471210, g_loss: 0.69045818, ae_loss: 0.04996921\n",
      "Step: [881] total_loss: 2.13702583 d_loss: 1.38695276, g_loss: 0.69850171, ae_loss: 0.05157119\n",
      "Step: [882] total_loss: 2.14465237 d_loss: 1.35963500, g_loss: 0.72897661, ae_loss: 0.05604084\n",
      "Step: [883] total_loss: 2.12096262 d_loss: 1.35793722, g_loss: 0.70941699, ae_loss: 0.05360845\n",
      "Step: [884] total_loss: 2.13990593 d_loss: 1.38556755, g_loss: 0.69769502, ae_loss: 0.05664352\n",
      "Step: [885] total_loss: 2.12897468 d_loss: 1.37967396, g_loss: 0.69449258, ae_loss: 0.05480817\n",
      "Step: [886] total_loss: 2.12849569 d_loss: 1.38031602, g_loss: 0.69993526, ae_loss: 0.04824437\n",
      "Step: [887] total_loss: 2.11316919 d_loss: 1.38832319, g_loss: 0.66659933, ae_loss: 0.05824676\n",
      "Step: [888] total_loss: 2.14215851 d_loss: 1.38613617, g_loss: 0.70699251, ae_loss: 0.04902991\n",
      "Step: [889] total_loss: 2.14221001 d_loss: 1.38625026, g_loss: 0.70391983, ae_loss: 0.05203990\n",
      "Step: [890] total_loss: 2.14640307 d_loss: 1.36203265, g_loss: 0.72680330, ae_loss: 0.05756716\n",
      "Step: [891] total_loss: 2.11593008 d_loss: 1.38250530, g_loss: 0.68146485, ae_loss: 0.05196002\n",
      "Step: [892] total_loss: 2.12158251 d_loss: 1.37803316, g_loss: 0.68805683, ae_loss: 0.05549249\n",
      "Step: [893] total_loss: 2.12958479 d_loss: 1.38900590, g_loss: 0.68825483, ae_loss: 0.05232415\n",
      "Step: [894] total_loss: 2.11279416 d_loss: 1.37828732, g_loss: 0.68348569, ae_loss: 0.05102115\n",
      "Step: [895] total_loss: 2.13115168 d_loss: 1.38448632, g_loss: 0.69273877, ae_loss: 0.05392642\n",
      "Step: [896] total_loss: 2.14572763 d_loss: 1.38742757, g_loss: 0.70949203, ae_loss: 0.04880804\n",
      "Step: [897] total_loss: 2.11348343 d_loss: 1.37281871, g_loss: 0.68814111, ae_loss: 0.05252379\n",
      "Step: [898] total_loss: 2.12063384 d_loss: 1.38100135, g_loss: 0.68876177, ae_loss: 0.05087067\n",
      "Step: [899] total_loss: 2.11195278 d_loss: 1.36988866, g_loss: 0.68948722, ae_loss: 0.05257674\n",
      "Step: [900] total_loss: 2.11921358 d_loss: 1.37083173, g_loss: 0.69234681, ae_loss: 0.05603500\n",
      "Step: [901] total_loss: 2.13073659 d_loss: 1.38226604, g_loss: 0.69362295, ae_loss: 0.05484759\n",
      "Step: [902] total_loss: 2.11672974 d_loss: 1.37440336, g_loss: 0.68678319, ae_loss: 0.05554302\n",
      "Step: [903] total_loss: 2.13775349 d_loss: 1.39114618, g_loss: 0.69810700, ae_loss: 0.04850020\n",
      "Step: [904] total_loss: 2.14753389 d_loss: 1.39598799, g_loss: 0.69743562, ae_loss: 0.05411015\n",
      "Step: [905] total_loss: 2.12421036 d_loss: 1.37855601, g_loss: 0.69316506, ae_loss: 0.05248923\n",
      "Step: [906] total_loss: 2.15958405 d_loss: 1.39716232, g_loss: 0.70854962, ae_loss: 0.05387214\n",
      "Step: [907] total_loss: 2.13571453 d_loss: 1.38775992, g_loss: 0.69045639, ae_loss: 0.05749807\n",
      "Step: [908] total_loss: 2.13393068 d_loss: 1.39034557, g_loss: 0.68771827, ae_loss: 0.05586702\n",
      "Step: [909] total_loss: 2.10660577 d_loss: 1.37705779, g_loss: 0.67595088, ae_loss: 0.05359710\n",
      "Step: [910] total_loss: 2.11446428 d_loss: 1.39567006, g_loss: 0.66358602, ae_loss: 0.05520824\n",
      "Step: [911] total_loss: 2.13418341 d_loss: 1.40747929, g_loss: 0.67488503, ae_loss: 0.05181908\n",
      "Step: [912] total_loss: 2.15086031 d_loss: 1.40664053, g_loss: 0.68939900, ae_loss: 0.05482082\n",
      "Step: [913] total_loss: 2.13473487 d_loss: 1.38699150, g_loss: 0.68420815, ae_loss: 0.06353518\n",
      "Step: [914] total_loss: 2.12951016 d_loss: 1.38216782, g_loss: 0.69542575, ae_loss: 0.05191665\n",
      "Step: [915] total_loss: 2.14156437 d_loss: 1.37928772, g_loss: 0.70977777, ae_loss: 0.05249896\n",
      "Step: [916] total_loss: 2.11861110 d_loss: 1.38526928, g_loss: 0.68155879, ae_loss: 0.05178298\n",
      "Step: [917] total_loss: 2.14483547 d_loss: 1.38279545, g_loss: 0.70643950, ae_loss: 0.05560043\n",
      "Step: [918] total_loss: 2.14090204 d_loss: 1.39699328, g_loss: 0.69131869, ae_loss: 0.05259015\n",
      "Step: [919] total_loss: 2.12913799 d_loss: 1.37205935, g_loss: 0.70691061, ae_loss: 0.05016800\n",
      "Step: [920] total_loss: 2.12496758 d_loss: 1.37251806, g_loss: 0.69701493, ae_loss: 0.05543446\n",
      "Step: [921] total_loss: 2.14027119 d_loss: 1.37713838, g_loss: 0.70977211, ae_loss: 0.05336063\n",
      "Step: [922] total_loss: 2.11575699 d_loss: 1.37464058, g_loss: 0.68765330, ae_loss: 0.05346316\n",
      "Step: [923] total_loss: 2.11587310 d_loss: 1.37738121, g_loss: 0.68227595, ae_loss: 0.05621584\n",
      "Step: [924] total_loss: 2.12027454 d_loss: 1.38762283, g_loss: 0.68443441, ae_loss: 0.04821737\n",
      "Step: [925] total_loss: 2.13495326 d_loss: 1.37629664, g_loss: 0.70216632, ae_loss: 0.05649031\n",
      "Step: [926] total_loss: 2.12447309 d_loss: 1.35399508, g_loss: 0.71676195, ae_loss: 0.05371615\n",
      "Step: [927] total_loss: 2.12554145 d_loss: 1.38967574, g_loss: 0.68341804, ae_loss: 0.05244768\n",
      "Step: [928] total_loss: 2.13225126 d_loss: 1.37742054, g_loss: 0.70253885, ae_loss: 0.05229178\n",
      "Step: [929] total_loss: 2.12770629 d_loss: 1.37674201, g_loss: 0.69693983, ae_loss: 0.05402441\n",
      "Step: [930] total_loss: 2.12485838 d_loss: 1.37476587, g_loss: 0.69522488, ae_loss: 0.05486768\n",
      "Step: [931] total_loss: 2.12454796 d_loss: 1.37221384, g_loss: 0.69968528, ae_loss: 0.05264889\n",
      "Step: [932] total_loss: 2.13940668 d_loss: 1.38941491, g_loss: 0.69428259, ae_loss: 0.05570930\n",
      "Step: [933] total_loss: 2.15135050 d_loss: 1.39320874, g_loss: 0.70151603, ae_loss: 0.05662569\n",
      "Step: [934] total_loss: 2.10838342 d_loss: 1.36930323, g_loss: 0.68849158, ae_loss: 0.05058864\n",
      "Step: [935] total_loss: 2.12207818 d_loss: 1.38456964, g_loss: 0.68622696, ae_loss: 0.05128160\n",
      "Step: [936] total_loss: 2.13501263 d_loss: 1.37997651, g_loss: 0.70261049, ae_loss: 0.05242550\n",
      "Step: [937] total_loss: 2.13202548 d_loss: 1.38986063, g_loss: 0.69042379, ae_loss: 0.05174104\n",
      "Step: [938] total_loss: 2.11654902 d_loss: 1.37190390, g_loss: 0.68930447, ae_loss: 0.05534049\n",
      "Step: [939] total_loss: 2.12169528 d_loss: 1.35609198, g_loss: 0.71078598, ae_loss: 0.05481729\n",
      "Step: [940] total_loss: 2.10583258 d_loss: 1.35980511, g_loss: 0.68957663, ae_loss: 0.05645097\n",
      "Step: [941] total_loss: 2.10562372 d_loss: 1.37286830, g_loss: 0.67729282, ae_loss: 0.05546276\n",
      "Step: [942] total_loss: 2.13602829 d_loss: 1.37920487, g_loss: 0.70336628, ae_loss: 0.05345698\n",
      "Step: [943] total_loss: 2.14028263 d_loss: 1.40005171, g_loss: 0.68694073, ae_loss: 0.05329024\n",
      "Step: [944] total_loss: 2.12068820 d_loss: 1.37147927, g_loss: 0.69221354, ae_loss: 0.05699536\n",
      "Step: [945] total_loss: 2.12673473 d_loss: 1.39867866, g_loss: 0.67238319, ae_loss: 0.05567297\n",
      "Step: [946] total_loss: 2.13295674 d_loss: 1.39068615, g_loss: 0.68128443, ae_loss: 0.06098614\n",
      "Step: [947] total_loss: 2.13153148 d_loss: 1.38593721, g_loss: 0.69187564, ae_loss: 0.05371856\n",
      "Step: [948] total_loss: 2.12573814 d_loss: 1.37893057, g_loss: 0.69822258, ae_loss: 0.04858511\n",
      "Step: [949] total_loss: 2.13017607 d_loss: 1.37982059, g_loss: 0.69830120, ae_loss: 0.05205442\n",
      "Step: [950] total_loss: 2.12597799 d_loss: 1.38434935, g_loss: 0.68856597, ae_loss: 0.05306263\n",
      "Step: [951] total_loss: 2.12633967 d_loss: 1.37026870, g_loss: 0.69727278, ae_loss: 0.05879816\n",
      "Step: [952] total_loss: 2.15033674 d_loss: 1.38377357, g_loss: 0.71377885, ae_loss: 0.05278443\n",
      "Step: [953] total_loss: 2.12869763 d_loss: 1.36537731, g_loss: 0.71479368, ae_loss: 0.04852667\n",
      "Step: [954] total_loss: 2.13880110 d_loss: 1.38476455, g_loss: 0.69602406, ae_loss: 0.05801258\n",
      "Step: [955] total_loss: 2.14862180 d_loss: 1.38233876, g_loss: 0.71323144, ae_loss: 0.05305160\n",
      "Step: [956] total_loss: 2.13899374 d_loss: 1.38409805, g_loss: 0.70297796, ae_loss: 0.05191771\n",
      "Step: [957] total_loss: 2.14531446 d_loss: 1.38066769, g_loss: 0.71331334, ae_loss: 0.05133340\n",
      "Step: [958] total_loss: 2.15136051 d_loss: 1.39843917, g_loss: 0.69810700, ae_loss: 0.05481416\n",
      "Step: [959] total_loss: 2.12272000 d_loss: 1.38944364, g_loss: 0.68660575, ae_loss: 0.04667056\n",
      "Step: [960] total_loss: 2.12496805 d_loss: 1.38209176, g_loss: 0.68511391, ae_loss: 0.05776250\n",
      "Step: [961] total_loss: 2.11918974 d_loss: 1.39683008, g_loss: 0.66985238, ae_loss: 0.05250711\n",
      "Step: [962] total_loss: 2.13039589 d_loss: 1.37661195, g_loss: 0.69895780, ae_loss: 0.05482604\n",
      "Step: [963] total_loss: 2.12574673 d_loss: 1.37491488, g_loss: 0.70229733, ae_loss: 0.04853445\n",
      "Step: [964] total_loss: 2.13150811 d_loss: 1.38257790, g_loss: 0.69252795, ae_loss: 0.05640222\n",
      "Step: [965] total_loss: 2.12317920 d_loss: 1.37494516, g_loss: 0.69915098, ae_loss: 0.04908304\n",
      "Step: [966] total_loss: 2.13392448 d_loss: 1.39839244, g_loss: 0.68242157, ae_loss: 0.05311065\n",
      "Step: [967] total_loss: 2.12316585 d_loss: 1.38044548, g_loss: 0.69108456, ae_loss: 0.05163584\n",
      "Step: [968] total_loss: 2.13793898 d_loss: 1.38948858, g_loss: 0.69286513, ae_loss: 0.05558519\n",
      "Step: [969] total_loss: 2.13879108 d_loss: 1.38485765, g_loss: 0.69704783, ae_loss: 0.05688562\n",
      "Step: [970] total_loss: 2.12594771 d_loss: 1.39347529, g_loss: 0.68437362, ae_loss: 0.04809882\n",
      "Step: [971] total_loss: 2.11166453 d_loss: 1.38450074, g_loss: 0.67360252, ae_loss: 0.05356127\n",
      "Step: [972] total_loss: 2.11612439 d_loss: 1.36728239, g_loss: 0.69327939, ae_loss: 0.05556264\n",
      "Step: [973] total_loss: 2.11796379 d_loss: 1.40296817, g_loss: 0.65907323, ae_loss: 0.05592253\n",
      "Step: [974] total_loss: 2.10938740 d_loss: 1.37203956, g_loss: 0.68594968, ae_loss: 0.05139816\n",
      "Step: [975] total_loss: 2.13911271 d_loss: 1.38942301, g_loss: 0.69877368, ae_loss: 0.05091612\n",
      "Step: [976] total_loss: 2.15774918 d_loss: 1.37473869, g_loss: 0.72518885, ae_loss: 0.05782162\n",
      "Step: [977] total_loss: 2.14301586 d_loss: 1.38964915, g_loss: 0.70352912, ae_loss: 0.04983768\n",
      "Step: [978] total_loss: 2.13342547 d_loss: 1.39576030, g_loss: 0.68525106, ae_loss: 0.05241402\n",
      "Step: [979] total_loss: 2.12667942 d_loss: 1.38963354, g_loss: 0.68210804, ae_loss: 0.05493777\n",
      "Step: [980] total_loss: 2.10684967 d_loss: 1.37472904, g_loss: 0.68021512, ae_loss: 0.05190552\n",
      "Step: [981] total_loss: 2.12055659 d_loss: 1.40008235, g_loss: 0.66671962, ae_loss: 0.05375474\n",
      "Step: [982] total_loss: 2.09944344 d_loss: 1.37233365, g_loss: 0.67926431, ae_loss: 0.04784542\n",
      "Step: [983] total_loss: 2.10607100 d_loss: 1.37574363, g_loss: 0.67727613, ae_loss: 0.05305133\n",
      "Step: [984] total_loss: 2.13914442 d_loss: 1.38900232, g_loss: 0.69322890, ae_loss: 0.05691320\n",
      "Step: [985] total_loss: 2.12762547 d_loss: 1.37850177, g_loss: 0.69504654, ae_loss: 0.05407698\n",
      "Step: [986] total_loss: 2.12810802 d_loss: 1.38393104, g_loss: 0.69293851, ae_loss: 0.05123843\n",
      "Step: [987] total_loss: 2.11872387 d_loss: 1.36683047, g_loss: 0.69822329, ae_loss: 0.05367022\n",
      "Step: [988] total_loss: 2.13259172 d_loss: 1.37940156, g_loss: 0.69464713, ae_loss: 0.05854303\n",
      "Step: [989] total_loss: 2.11048174 d_loss: 1.37445879, g_loss: 0.68023837, ae_loss: 0.05578468\n",
      "Step: [990] total_loss: 2.12496710 d_loss: 1.38889265, g_loss: 0.68728781, ae_loss: 0.04878673\n",
      "Step: [991] total_loss: 2.12587619 d_loss: 1.38039327, g_loss: 0.69136488, ae_loss: 0.05411803\n",
      "Step: [992] total_loss: 2.13636994 d_loss: 1.38844919, g_loss: 0.69407594, ae_loss: 0.05384478\n",
      "Step: [993] total_loss: 2.13820124 d_loss: 1.40066028, g_loss: 0.68394446, ae_loss: 0.05359634\n",
      "Step: [994] total_loss: 2.13277650 d_loss: 1.38995194, g_loss: 0.69065726, ae_loss: 0.05216734\n",
      "Step: [995] total_loss: 2.12870741 d_loss: 1.37384951, g_loss: 0.70089161, ae_loss: 0.05396625\n",
      "Step: [996] total_loss: 2.14774823 d_loss: 1.39133334, g_loss: 0.70502615, ae_loss: 0.05138877\n",
      "Step: [997] total_loss: 2.14048290 d_loss: 1.40410519, g_loss: 0.68305373, ae_loss: 0.05332381\n",
      "Step: [998] total_loss: 2.14114904 d_loss: 1.37952876, g_loss: 0.71015096, ae_loss: 0.05146940\n",
      "Step: [999] total_loss: 2.13500929 d_loss: 1.36316800, g_loss: 0.71812689, ae_loss: 0.05371429\n",
      "Step: [1000] total_loss: 2.12687635 d_loss: 1.37500417, g_loss: 0.70254546, ae_loss: 0.04932666\n",
      "Step: [1001] total_loss: 2.12502456 d_loss: 1.38284564, g_loss: 0.68953967, ae_loss: 0.05263928\n",
      "Step: [1002] total_loss: 2.11330175 d_loss: 1.37833178, g_loss: 0.68482256, ae_loss: 0.05014754\n",
      "Step: [1003] total_loss: 2.12743926 d_loss: 1.40153575, g_loss: 0.67491925, ae_loss: 0.05098427\n",
      "Step: [1004] total_loss: 2.11927915 d_loss: 1.37997746, g_loss: 0.68629634, ae_loss: 0.05300537\n",
      "Step: [1005] total_loss: 2.11852288 d_loss: 1.37444043, g_loss: 0.69152719, ae_loss: 0.05255521\n",
      "Step: [1006] total_loss: 2.12621641 d_loss: 1.39392710, g_loss: 0.67908716, ae_loss: 0.05320204\n",
      "Step: [1007] total_loss: 2.13340425 d_loss: 1.39086807, g_loss: 0.68830645, ae_loss: 0.05422956\n",
      "Step: [1008] total_loss: 2.12355018 d_loss: 1.38183725, g_loss: 0.69100964, ae_loss: 0.05070333\n",
      "Step: [1009] total_loss: 2.11886525 d_loss: 1.38007760, g_loss: 0.68394965, ae_loss: 0.05483796\n",
      "Step: [1010] total_loss: 2.12516212 d_loss: 1.38383305, g_loss: 0.68678236, ae_loss: 0.05454681\n",
      "Step: [1011] total_loss: 2.12235999 d_loss: 1.39861608, g_loss: 0.66988850, ae_loss: 0.05385537\n",
      "Step: [1012] total_loss: 2.12007332 d_loss: 1.38522100, g_loss: 0.68440753, ae_loss: 0.05044472\n",
      "Step: [1013] total_loss: 2.13012791 d_loss: 1.38895607, g_loss: 0.68429029, ae_loss: 0.05688148\n",
      "Step: [1014] total_loss: 2.13985610 d_loss: 1.38417947, g_loss: 0.70272708, ae_loss: 0.05294959\n",
      "Step: [1015] total_loss: 2.12313509 d_loss: 1.36773205, g_loss: 0.70394218, ae_loss: 0.05146094\n",
      "Step: [1016] total_loss: 2.13860345 d_loss: 1.39550781, g_loss: 0.68825465, ae_loss: 0.05484096\n",
      "Step: [1017] total_loss: 2.13274336 d_loss: 1.37389147, g_loss: 0.70411611, ae_loss: 0.05473571\n",
      "Step: [1018] total_loss: 2.13872480 d_loss: 1.39124489, g_loss: 0.69488382, ae_loss: 0.05259603\n",
      "Step: [1019] total_loss: 2.10501885 d_loss: 1.34496593, g_loss: 0.70756698, ae_loss: 0.05248593\n",
      "Step: [1020] total_loss: 2.10830641 d_loss: 1.39081240, g_loss: 0.66462922, ae_loss: 0.05286479\n",
      "Step: [1021] total_loss: 2.13910198 d_loss: 1.38272357, g_loss: 0.69845736, ae_loss: 0.05792120\n",
      "Step: [1022] total_loss: 2.15216637 d_loss: 1.38049078, g_loss: 0.71560180, ae_loss: 0.05607373\n",
      "Step: [1023] total_loss: 2.16366673 d_loss: 1.38866591, g_loss: 0.71860158, ae_loss: 0.05639917\n",
      "Step: [1024] total_loss: 2.14320898 d_loss: 1.36763752, g_loss: 0.72478813, ae_loss: 0.05078338\n",
      "Step: [1025] total_loss: 2.12802219 d_loss: 1.36189425, g_loss: 0.71457416, ae_loss: 0.05155389\n",
      "Step: [1026] total_loss: 2.15605044 d_loss: 1.40974784, g_loss: 0.69350570, ae_loss: 0.05279680\n",
      "Step: [1027] total_loss: 2.11331177 d_loss: 1.37670374, g_loss: 0.68303901, ae_loss: 0.05356903\n",
      "Step: [1028] total_loss: 2.11770773 d_loss: 1.38395357, g_loss: 0.68093359, ae_loss: 0.05282041\n",
      "Step: [1029] total_loss: 2.13744116 d_loss: 1.37178373, g_loss: 0.71066773, ae_loss: 0.05498955\n",
      "Step: [1030] total_loss: 2.11928749 d_loss: 1.36104941, g_loss: 0.70552504, ae_loss: 0.05271304\n",
      "Step: [1031] total_loss: 2.13799191 d_loss: 1.39491630, g_loss: 0.69428909, ae_loss: 0.04878637\n",
      "Step: [1032] total_loss: 2.11333156 d_loss: 1.37320125, g_loss: 0.68551445, ae_loss: 0.05461583\n",
      "Step: [1033] total_loss: 2.14440155 d_loss: 1.38690794, g_loss: 0.70141172, ae_loss: 0.05608200\n",
      "Step: [1034] total_loss: 2.13444805 d_loss: 1.37097096, g_loss: 0.71054620, ae_loss: 0.05293081\n",
      "Step: [1035] total_loss: 2.13266516 d_loss: 1.37250924, g_loss: 0.70813334, ae_loss: 0.05202259\n",
      "Step: [1036] total_loss: 2.11783552 d_loss: 1.35609126, g_loss: 0.70474124, ae_loss: 0.05700317\n",
      "Step: [1037] total_loss: 2.09059072 d_loss: 1.37066019, g_loss: 0.66784012, ae_loss: 0.05209038\n",
      "Step: [1038] total_loss: 2.11941981 d_loss: 1.37436819, g_loss: 0.68510550, ae_loss: 0.05994622\n",
      "Step: [1039] total_loss: 2.11416459 d_loss: 1.37400556, g_loss: 0.68293822, ae_loss: 0.05722087\n",
      "Step: [1040] total_loss: 2.09424281 d_loss: 1.34540284, g_loss: 0.68992060, ae_loss: 0.05891932\n",
      "Step: [1041] total_loss: 2.13875198 d_loss: 1.39655089, g_loss: 0.68681353, ae_loss: 0.05538752\n",
      "Step: [1042] total_loss: 2.14583778 d_loss: 1.38868737, g_loss: 0.70626044, ae_loss: 0.05088989\n",
      "Step: [1043] total_loss: 2.12659359 d_loss: 1.36874354, g_loss: 0.70268202, ae_loss: 0.05516795\n",
      "Step: [1044] total_loss: 2.17303610 d_loss: 1.41111338, g_loss: 0.70478511, ae_loss: 0.05713754\n",
      "Step: [1045] total_loss: 2.14018488 d_loss: 1.38109541, g_loss: 0.70696306, ae_loss: 0.05212641\n",
      "Step: [1046] total_loss: 2.13770986 d_loss: 1.38566971, g_loss: 0.69708979, ae_loss: 0.05495035\n",
      "Step: [1047] total_loss: 2.17300653 d_loss: 1.41727817, g_loss: 0.69726926, ae_loss: 0.05845908\n",
      "Step: [1048] total_loss: 2.12705040 d_loss: 1.37458086, g_loss: 0.69721889, ae_loss: 0.05525066\n",
      "Step: [1049] total_loss: 2.14872265 d_loss: 1.39155364, g_loss: 0.70308834, ae_loss: 0.05408064\n",
      "Step: [1050] total_loss: 2.13010073 d_loss: 1.36863875, g_loss: 0.70585293, ae_loss: 0.05560905\n",
      "Step: [1051] total_loss: 2.12276936 d_loss: 1.39455986, g_loss: 0.67469984, ae_loss: 0.05350957\n",
      "Step: [1052] total_loss: 2.08480549 d_loss: 1.36533737, g_loss: 0.66703641, ae_loss: 0.05243167\n",
      "Step: [1053] total_loss: 2.11215401 d_loss: 1.39091730, g_loss: 0.67062402, ae_loss: 0.05061255\n",
      "Step: [1054] total_loss: 2.10459495 d_loss: 1.36554730, g_loss: 0.68765312, ae_loss: 0.05139446\n",
      "Step: [1055] total_loss: 2.12639427 d_loss: 1.37855458, g_loss: 0.69314671, ae_loss: 0.05469291\n",
      "Step: [1056] total_loss: 2.14811540 d_loss: 1.39351213, g_loss: 0.69916219, ae_loss: 0.05544103\n",
      "Step: [1057] total_loss: 2.15001249 d_loss: 1.38931727, g_loss: 0.70682061, ae_loss: 0.05387476\n",
      "Step: [1058] total_loss: 2.14627838 d_loss: 1.38036132, g_loss: 0.71121085, ae_loss: 0.05470615\n",
      "Step: [1059] total_loss: 2.13625741 d_loss: 1.38921928, g_loss: 0.69548732, ae_loss: 0.05155074\n",
      "Step: [1060] total_loss: 2.14368415 d_loss: 1.40526581, g_loss: 0.68627149, ae_loss: 0.05214696\n",
      "Step: [1061] total_loss: 2.16526127 d_loss: 1.40761864, g_loss: 0.70430946, ae_loss: 0.05333302\n",
      "Step: [1062] total_loss: 2.13754725 d_loss: 1.37271667, g_loss: 0.70924693, ae_loss: 0.05558365\n",
      "Step: [1063] total_loss: 2.12001491 d_loss: 1.36320424, g_loss: 0.70177007, ae_loss: 0.05504063\n",
      "Step: [1064] total_loss: 2.12025785 d_loss: 1.37550855, g_loss: 0.69390422, ae_loss: 0.05084506\n",
      "Step: [1065] total_loss: 2.09701228 d_loss: 1.35680890, g_loss: 0.68315560, ae_loss: 0.05704784\n",
      "Step: [1066] total_loss: 2.13066721 d_loss: 1.38787293, g_loss: 0.68946445, ae_loss: 0.05332993\n",
      "Step: [1067] total_loss: 2.14227724 d_loss: 1.39112484, g_loss: 0.69637656, ae_loss: 0.05477593\n",
      "Step: [1068] total_loss: 2.15161014 d_loss: 1.38666725, g_loss: 0.70736116, ae_loss: 0.05758163\n",
      "Step: [1069] total_loss: 2.14397407 d_loss: 1.37251174, g_loss: 0.71823162, ae_loss: 0.05323073\n",
      "Step: [1070] total_loss: 2.14682794 d_loss: 1.40498757, g_loss: 0.68861550, ae_loss: 0.05322493\n",
      "Step: [1071] total_loss: 2.14389372 d_loss: 1.39554191, g_loss: 0.69379985, ae_loss: 0.05455209\n",
      "Step: [1072] total_loss: 2.11825204 d_loss: 1.37046027, g_loss: 0.69090420, ae_loss: 0.05688762\n",
      "Step: [1073] total_loss: 2.11946797 d_loss: 1.37265992, g_loss: 0.69569451, ae_loss: 0.05111366\n",
      "Step: [1074] total_loss: 2.10092258 d_loss: 1.36961699, g_loss: 0.68052757, ae_loss: 0.05077787\n",
      "Step: [1075] total_loss: 2.11874676 d_loss: 1.40033937, g_loss: 0.66623741, ae_loss: 0.05217001\n",
      "Step: [1076] total_loss: 2.12680483 d_loss: 1.37744451, g_loss: 0.69437063, ae_loss: 0.05498959\n",
      "Step: [1077] total_loss: 2.14720869 d_loss: 1.39667916, g_loss: 0.69615340, ae_loss: 0.05437611\n",
      "Step: [1078] total_loss: 2.12707591 d_loss: 1.38762999, g_loss: 0.68717122, ae_loss: 0.05227467\n",
      "Step: [1079] total_loss: 2.12795115 d_loss: 1.38703251, g_loss: 0.68642247, ae_loss: 0.05449618\n",
      "Step: [1080] total_loss: 2.11941290 d_loss: 1.38230979, g_loss: 0.68564928, ae_loss: 0.05145390\n",
      "Step: [1081] total_loss: 2.10929632 d_loss: 1.36287332, g_loss: 0.69608891, ae_loss: 0.05033414\n",
      "Step: [1082] total_loss: 2.12986851 d_loss: 1.36982226, g_loss: 0.70620298, ae_loss: 0.05384337\n",
      "Step: [1083] total_loss: 2.10438514 d_loss: 1.37001896, g_loss: 0.68025661, ae_loss: 0.05410960\n",
      "Step: [1084] total_loss: 2.12487936 d_loss: 1.39942455, g_loss: 0.67313063, ae_loss: 0.05232427\n",
      "Step: [1085] total_loss: 2.15075874 d_loss: 1.39843535, g_loss: 0.69737107, ae_loss: 0.05495222\n",
      "Step: [1086] total_loss: 2.13198924 d_loss: 1.37429786, g_loss: 0.70358658, ae_loss: 0.05410486\n",
      "Step: [1087] total_loss: 2.14755392 d_loss: 1.39651179, g_loss: 0.69571149, ae_loss: 0.05533055\n",
      "Step: [1088] total_loss: 2.11594844 d_loss: 1.36920667, g_loss: 0.69912100, ae_loss: 0.04762074\n",
      "Step: [1089] total_loss: 2.12369823 d_loss: 1.37840986, g_loss: 0.69152927, ae_loss: 0.05375911\n",
      "Step: [1090] total_loss: 2.11881685 d_loss: 1.38732708, g_loss: 0.67918813, ae_loss: 0.05230152\n",
      "Step: [1091] total_loss: 2.11990213 d_loss: 1.37682533, g_loss: 0.68477780, ae_loss: 0.05829907\n",
      "Step: [1092] total_loss: 2.11590815 d_loss: 1.37812877, g_loss: 0.68261337, ae_loss: 0.05516612\n",
      "Step: [1093] total_loss: 2.12256098 d_loss: 1.36862862, g_loss: 0.69563580, ae_loss: 0.05829656\n",
      "Step: [1094] total_loss: 2.09419537 d_loss: 1.34859109, g_loss: 0.69499880, ae_loss: 0.05060553\n",
      "Step: [1095] total_loss: 2.12681723 d_loss: 1.36916733, g_loss: 0.70022964, ae_loss: 0.05742029\n",
      "Step: [1096] total_loss: 2.14713168 d_loss: 1.38599110, g_loss: 0.70549142, ae_loss: 0.05564913\n",
      "Step: [1097] total_loss: 2.12997341 d_loss: 1.37896895, g_loss: 0.69699013, ae_loss: 0.05401434\n",
      "Step: [1098] total_loss: 2.14946365 d_loss: 1.39211941, g_loss: 0.70396703, ae_loss: 0.05337733\n",
      "Step: [1099] total_loss: 2.11854839 d_loss: 1.36225700, g_loss: 0.70309997, ae_loss: 0.05319147\n",
      "Step: [1100] total_loss: 2.14874315 d_loss: 1.39146805, g_loss: 0.70515597, ae_loss: 0.05211930\n",
      "Step: [1101] total_loss: 2.13327479 d_loss: 1.38426054, g_loss: 0.69531435, ae_loss: 0.05369985\n",
      "Step: [1102] total_loss: 2.13423252 d_loss: 1.37529647, g_loss: 0.70314425, ae_loss: 0.05579185\n",
      "Step: [1103] total_loss: 2.12668490 d_loss: 1.37855148, g_loss: 0.69656348, ae_loss: 0.05156995\n",
      "Step: [1104] total_loss: 2.12613153 d_loss: 1.37452030, g_loss: 0.69634008, ae_loss: 0.05527109\n",
      "Step: [1105] total_loss: 2.12589502 d_loss: 1.39731050, g_loss: 0.67494553, ae_loss: 0.05363888\n",
      "Step: [1106] total_loss: 2.10589027 d_loss: 1.37694371, g_loss: 0.67731166, ae_loss: 0.05163507\n",
      "Step: [1107] total_loss: 2.12870884 d_loss: 1.38412786, g_loss: 0.68824220, ae_loss: 0.05633896\n",
      "Step: [1108] total_loss: 2.13799429 d_loss: 1.37948358, g_loss: 0.70480084, ae_loss: 0.05370972\n",
      "Step: [1109] total_loss: 2.14204168 d_loss: 1.39029431, g_loss: 0.70034575, ae_loss: 0.05140171\n",
      "Step: [1110] total_loss: 2.14543247 d_loss: 1.39751148, g_loss: 0.69531512, ae_loss: 0.05260587\n",
      "Step: [1111] total_loss: 2.12568426 d_loss: 1.37626684, g_loss: 0.69207120, ae_loss: 0.05734610\n",
      "Step: [1112] total_loss: 2.13555241 d_loss: 1.38143384, g_loss: 0.70100617, ae_loss: 0.05311251\n",
      "Step: [1113] total_loss: 2.14340067 d_loss: 1.39896393, g_loss: 0.68723106, ae_loss: 0.05720553\n",
      "Step: [1114] total_loss: 2.14023590 d_loss: 1.39371729, g_loss: 0.68915892, ae_loss: 0.05735977\n",
      "Step: [1115] total_loss: 2.11439848 d_loss: 1.37185168, g_loss: 0.68911177, ae_loss: 0.05343507\n",
      "Step: [1116] total_loss: 2.13162088 d_loss: 1.35930336, g_loss: 0.71798086, ae_loss: 0.05433675\n",
      "Step: [1117] total_loss: 2.11154175 d_loss: 1.37390471, g_loss: 0.68421489, ae_loss: 0.05342226\n",
      "Step: [1118] total_loss: 2.11427307 d_loss: 1.37669277, g_loss: 0.68686581, ae_loss: 0.05071455\n",
      "Step: [1119] total_loss: 2.14793634 d_loss: 1.40035725, g_loss: 0.69231415, ae_loss: 0.05526487\n",
      "Step: [1120] total_loss: 2.15189838 d_loss: 1.38709402, g_loss: 0.71025056, ae_loss: 0.05455372\n",
      "Step: [1121] total_loss: 2.13987780 d_loss: 1.38765359, g_loss: 0.69726378, ae_loss: 0.05496038\n",
      "Step: [1122] total_loss: 2.16804266 d_loss: 1.39513481, g_loss: 0.71703362, ae_loss: 0.05587423\n",
      "Step: [1123] total_loss: 2.14694834 d_loss: 1.38873863, g_loss: 0.70530385, ae_loss: 0.05290594\n",
      "Step: [1124] total_loss: 2.12814188 d_loss: 1.37307072, g_loss: 0.70420563, ae_loss: 0.05086550\n",
      "Step: [1125] total_loss: 2.14504147 d_loss: 1.39564633, g_loss: 0.69456279, ae_loss: 0.05483249\n",
      "Step: [1126] total_loss: 2.12376213 d_loss: 1.38639903, g_loss: 0.68859231, ae_loss: 0.04877094\n",
      "Step: [1127] total_loss: 2.13132048 d_loss: 1.38478279, g_loss: 0.69395769, ae_loss: 0.05258018\n",
      "Step: [1128] total_loss: 2.13741827 d_loss: 1.39538670, g_loss: 0.69208950, ae_loss: 0.04994208\n",
      "Step: [1129] total_loss: 2.11815310 d_loss: 1.38855314, g_loss: 0.67529458, ae_loss: 0.05430544\n",
      "Step: [1130] total_loss: 2.09336472 d_loss: 1.37108493, g_loss: 0.67085314, ae_loss: 0.05142671\n",
      "Step: [1131] total_loss: 2.11953092 d_loss: 1.38830233, g_loss: 0.68102384, ae_loss: 0.05020469\n",
      "Step: [1132] total_loss: 2.11658049 d_loss: 1.37705445, g_loss: 0.69081557, ae_loss: 0.04871047\n",
      "Step: [1133] total_loss: 2.14240003 d_loss: 1.39436734, g_loss: 0.69559604, ae_loss: 0.05243670\n",
      "Step: [1134] total_loss: 2.12403607 d_loss: 1.35434413, g_loss: 0.71587300, ae_loss: 0.05381891\n",
      "Step: [1135] total_loss: 2.12613511 d_loss: 1.37986219, g_loss: 0.68735528, ae_loss: 0.05891768\n",
      "Step: [1136] total_loss: 2.13209319 d_loss: 1.37340796, g_loss: 0.70174551, ae_loss: 0.05693968\n",
      "Step: [1137] total_loss: 2.13818336 d_loss: 1.37680268, g_loss: 0.70312166, ae_loss: 0.05825896\n",
      "Step: [1138] total_loss: 2.12196565 d_loss: 1.37324882, g_loss: 0.69737792, ae_loss: 0.05133888\n",
      "Step: [1139] total_loss: 2.11112046 d_loss: 1.37618029, g_loss: 0.67951900, ae_loss: 0.05542108\n",
      "Step: [1140] total_loss: 2.11163116 d_loss: 1.37409806, g_loss: 0.68298393, ae_loss: 0.05454908\n",
      "Step: [1141] total_loss: 2.12216043 d_loss: 1.37796330, g_loss: 0.69004571, ae_loss: 0.05415142\n",
      "Step: [1142] total_loss: 2.12739325 d_loss: 1.39953136, g_loss: 0.67700624, ae_loss: 0.05085575\n",
      "Step: [1143] total_loss: 2.13750267 d_loss: 1.36975169, g_loss: 0.70995015, ae_loss: 0.05780074\n",
      "Step: [1144] total_loss: 2.12615228 d_loss: 1.37552118, g_loss: 0.70047927, ae_loss: 0.05015184\n",
      "Step: [1145] total_loss: 2.13231611 d_loss: 1.38807821, g_loss: 0.69116914, ae_loss: 0.05306891\n",
      "Step: [1146] total_loss: 2.14149427 d_loss: 1.39664876, g_loss: 0.69085228, ae_loss: 0.05399314\n",
      "Step: [1147] total_loss: 2.14110303 d_loss: 1.37857866, g_loss: 0.70783252, ae_loss: 0.05469180\n",
      "Step: [1148] total_loss: 2.13873148 d_loss: 1.36737728, g_loss: 0.71958232, ae_loss: 0.05177191\n",
      "Step: [1149] total_loss: 2.12873983 d_loss: 1.37175679, g_loss: 0.70113528, ae_loss: 0.05584771\n",
      "Step: [1150] total_loss: 2.12283802 d_loss: 1.37303591, g_loss: 0.69573045, ae_loss: 0.05407159\n",
      "Step: [1151] total_loss: 2.13892746 d_loss: 1.39825809, g_loss: 0.68729103, ae_loss: 0.05337821\n",
      "Step: [1152] total_loss: 2.11991930 d_loss: 1.35706210, g_loss: 0.70620012, ae_loss: 0.05665723\n",
      "Step: [1153] total_loss: 2.13439107 d_loss: 1.36700463, g_loss: 0.71621138, ae_loss: 0.05117517\n",
      "Step: [1154] total_loss: 2.13140631 d_loss: 1.40376472, g_loss: 0.67514849, ae_loss: 0.05249309\n",
      "Step: [1155] total_loss: 2.12473583 d_loss: 1.38982010, g_loss: 0.68261623, ae_loss: 0.05229961\n",
      "Step: [1156] total_loss: 2.09864068 d_loss: 1.35213161, g_loss: 0.69381768, ae_loss: 0.05269143\n",
      "Step: [1157] total_loss: 2.11280584 d_loss: 1.38301921, g_loss: 0.68314612, ae_loss: 0.04664054\n",
      "Step: [1158] total_loss: 2.13185930 d_loss: 1.37289941, g_loss: 0.70433700, ae_loss: 0.05462280\n",
      "Step: [1159] total_loss: 2.11218047 d_loss: 1.38312137, g_loss: 0.67663890, ae_loss: 0.05242024\n",
      "Step: [1160] total_loss: 2.12813950 d_loss: 1.38665915, g_loss: 0.68641770, ae_loss: 0.05506260\n",
      "Step: [1161] total_loss: 2.13612652 d_loss: 1.38728476, g_loss: 0.69498277, ae_loss: 0.05385903\n",
      "Step: [1162] total_loss: 2.16132498 d_loss: 1.40421760, g_loss: 0.70298576, ae_loss: 0.05412145\n",
      "Step: [1163] total_loss: 2.11543489 d_loss: 1.36835945, g_loss: 0.69752508, ae_loss: 0.04955038\n",
      "Step: [1164] total_loss: 2.13214612 d_loss: 1.38775635, g_loss: 0.69090593, ae_loss: 0.05348384\n",
      "Step: [1165] total_loss: 2.11260557 d_loss: 1.35619378, g_loss: 0.70137358, ae_loss: 0.05503820\n",
      "Step: [1166] total_loss: 2.11080956 d_loss: 1.37352526, g_loss: 0.68255275, ae_loss: 0.05473151\n",
      "Step: [1167] total_loss: 2.11248493 d_loss: 1.37294745, g_loss: 0.68863511, ae_loss: 0.05090235\n",
      "Step: [1168] total_loss: 2.12232351 d_loss: 1.38681638, g_loss: 0.68274140, ae_loss: 0.05276565\n",
      "Step: [1169] total_loss: 2.15647173 d_loss: 1.39348900, g_loss: 0.71209383, ae_loss: 0.05088875\n",
      "Step: [1170] total_loss: 2.15156770 d_loss: 1.39296198, g_loss: 0.70693612, ae_loss: 0.05166959\n",
      "Step: [1171] total_loss: 2.16126060 d_loss: 1.40595472, g_loss: 0.70140254, ae_loss: 0.05390340\n",
      "Step: [1172] total_loss: 2.16934586 d_loss: 1.40559149, g_loss: 0.70823145, ae_loss: 0.05552305\n",
      "Step: [1173] total_loss: 2.14342594 d_loss: 1.37123775, g_loss: 0.71790367, ae_loss: 0.05428457\n",
      "Step: [1174] total_loss: 2.14846897 d_loss: 1.39652228, g_loss: 0.69879967, ae_loss: 0.05314707\n",
      "Step: [1175] total_loss: 2.12229943 d_loss: 1.37965035, g_loss: 0.68642676, ae_loss: 0.05622228\n",
      "Step: [1176] total_loss: 2.11048102 d_loss: 1.38881600, g_loss: 0.66718960, ae_loss: 0.05447542\n",
      "Step: [1177] total_loss: 2.11154270 d_loss: 1.37136757, g_loss: 0.69409120, ae_loss: 0.04608383\n",
      "Step: [1178] total_loss: 2.10966969 d_loss: 1.34351838, g_loss: 0.71299303, ae_loss: 0.05315825\n",
      "Step: [1179] total_loss: 2.13242292 d_loss: 1.38808155, g_loss: 0.69342220, ae_loss: 0.05091920\n",
      "Step: [1180] total_loss: 2.14228439 d_loss: 1.38188386, g_loss: 0.70585060, ae_loss: 0.05454996\n",
      "Step: [1181] total_loss: 2.13375592 d_loss: 1.36816382, g_loss: 0.71335036, ae_loss: 0.05224171\n",
      "Step: [1182] total_loss: 2.13643408 d_loss: 1.39239669, g_loss: 0.69011831, ae_loss: 0.05391895\n",
      "Step: [1183] total_loss: 2.14510012 d_loss: 1.39336562, g_loss: 0.69438672, ae_loss: 0.05734789\n",
      "Step: [1184] total_loss: 2.11035633 d_loss: 1.34716129, g_loss: 0.70851648, ae_loss: 0.05467860\n",
      "Step: [1185] total_loss: 2.14878893 d_loss: 1.38050723, g_loss: 0.71762860, ae_loss: 0.05065296\n",
      "Step: [1186] total_loss: 2.11520672 d_loss: 1.37023497, g_loss: 0.69206595, ae_loss: 0.05290589\n",
      "Step: [1187] total_loss: 2.14466953 d_loss: 1.38404632, g_loss: 0.70783174, ae_loss: 0.05279144\n",
      "Step: [1188] total_loss: 2.13849640 d_loss: 1.39081645, g_loss: 0.69463491, ae_loss: 0.05304489\n",
      "Step: [1189] total_loss: 2.10739684 d_loss: 1.33727419, g_loss: 0.72015047, ae_loss: 0.04997214\n",
      "Step: [1190] total_loss: 2.13787651 d_loss: 1.37608552, g_loss: 0.69887662, ae_loss: 0.06291451\n",
      "Step: [1191] total_loss: 2.12925553 d_loss: 1.36890602, g_loss: 0.70871949, ae_loss: 0.05163002\n",
      "Step: [1192] total_loss: 2.14907575 d_loss: 1.41311061, g_loss: 0.68369937, ae_loss: 0.05226573\n",
      "Step: [1193] total_loss: 2.12478137 d_loss: 1.38309240, g_loss: 0.68730682, ae_loss: 0.05438211\n",
      "Step: [1194] total_loss: 2.13436031 d_loss: 1.37949133, g_loss: 0.70559150, ae_loss: 0.04927757\n",
      "Step: [1195] total_loss: 2.09166670 d_loss: 1.33384442, g_loss: 0.70500982, ae_loss: 0.05281251\n",
      "Step: [1196] total_loss: 2.12327147 d_loss: 1.38489532, g_loss: 0.68454361, ae_loss: 0.05383252\n",
      "Step: [1197] total_loss: 2.11694932 d_loss: 1.38651562, g_loss: 0.67704916, ae_loss: 0.05338454\n",
      "Step: [1198] total_loss: 2.16632342 d_loss: 1.40949869, g_loss: 0.70575523, ae_loss: 0.05106945\n",
      "Step: [1199] total_loss: 2.13329506 d_loss: 1.36427569, g_loss: 0.72070348, ae_loss: 0.04831572\n",
      "Step: [1200] total_loss: 2.15219545 d_loss: 1.40444088, g_loss: 0.69559276, ae_loss: 0.05216189\n",
      "Step: [1201] total_loss: 2.11844873 d_loss: 1.36542606, g_loss: 0.70065844, ae_loss: 0.05236432\n",
      "Step: [1202] total_loss: 2.14881396 d_loss: 1.40093112, g_loss: 0.69375300, ae_loss: 0.05412989\n",
      "Step: [1203] total_loss: 2.13682747 d_loss: 1.38373542, g_loss: 0.69744158, ae_loss: 0.05565050\n",
      "Step: [1204] total_loss: 2.12445736 d_loss: 1.39102769, g_loss: 0.68283284, ae_loss: 0.05059676\n",
      "Step: [1205] total_loss: 2.12002683 d_loss: 1.37896705, g_loss: 0.69406217, ae_loss: 0.04699750\n",
      "Step: [1206] total_loss: 2.10717797 d_loss: 1.37444007, g_loss: 0.68263638, ae_loss: 0.05010150\n",
      "Step: [1207] total_loss: 2.15092874 d_loss: 1.40135741, g_loss: 0.69534910, ae_loss: 0.05422224\n",
      "Step: [1208] total_loss: 2.13413334 d_loss: 1.38354301, g_loss: 0.69678879, ae_loss: 0.05380157\n",
      "Step: [1209] total_loss: 2.13632822 d_loss: 1.36545908, g_loss: 0.72368890, ae_loss: 0.04718034\n",
      "Step: [1210] total_loss: 2.13900399 d_loss: 1.36072206, g_loss: 0.72478586, ae_loss: 0.05349601\n",
      "Step: [1211] total_loss: 2.13759565 d_loss: 1.39688146, g_loss: 0.68749982, ae_loss: 0.05321442\n",
      "Step: [1212] total_loss: 2.12703967 d_loss: 1.39939857, g_loss: 0.67264932, ae_loss: 0.05499181\n",
      "Step: [1213] total_loss: 2.10891533 d_loss: 1.37546146, g_loss: 0.67935538, ae_loss: 0.05409854\n",
      "Step: [1214] total_loss: 2.14644289 d_loss: 1.39771438, g_loss: 0.69531071, ae_loss: 0.05341795\n",
      "Step: [1215] total_loss: 2.12122464 d_loss: 1.38005936, g_loss: 0.69156027, ae_loss: 0.04960504\n",
      "Step: [1216] total_loss: 2.18589687 d_loss: 1.39320219, g_loss: 0.74065840, ae_loss: 0.05203627\n",
      "Step: [1217] total_loss: 2.13180804 d_loss: 1.36208630, g_loss: 0.71956253, ae_loss: 0.05015919\n",
      "Step: [1218] total_loss: 2.15906525 d_loss: 1.38848448, g_loss: 0.71516263, ae_loss: 0.05541811\n",
      "Step: [1219] total_loss: 2.13065529 d_loss: 1.37588823, g_loss: 0.69939733, ae_loss: 0.05536966\n",
      "Step: [1220] total_loss: 2.09262371 d_loss: 1.36459923, g_loss: 0.67789447, ae_loss: 0.05012985\n",
      "Step: [1221] total_loss: 2.10289216 d_loss: 1.36911988, g_loss: 0.68160921, ae_loss: 0.05216311\n",
      "Step: [1222] total_loss: 2.11226892 d_loss: 1.39639926, g_loss: 0.66932702, ae_loss: 0.04654252\n",
      "Step: [1223] total_loss: 2.11047745 d_loss: 1.38866830, g_loss: 0.66872907, ae_loss: 0.05308018\n",
      "Step: [1224] total_loss: 2.11795354 d_loss: 1.38481069, g_loss: 0.67944503, ae_loss: 0.05369778\n",
      "Step: [1225] total_loss: 2.12972832 d_loss: 1.37207890, g_loss: 0.70379496, ae_loss: 0.05385461\n",
      "Step: [1226] total_loss: 2.12676620 d_loss: 1.38165021, g_loss: 0.69530773, ae_loss: 0.04980814\n",
      "Step: [1227] total_loss: 2.15499640 d_loss: 1.38499594, g_loss: 0.71800220, ae_loss: 0.05199815\n",
      "Step: [1228] total_loss: 2.14013028 d_loss: 1.37841189, g_loss: 0.71071005, ae_loss: 0.05100840\n",
      "Step: [1229] total_loss: 2.15134263 d_loss: 1.39554167, g_loss: 0.70121664, ae_loss: 0.05458443\n",
      "Step: [1230] total_loss: 2.13869834 d_loss: 1.37203074, g_loss: 0.71268231, ae_loss: 0.05398541\n",
      "Step: [1231] total_loss: 2.13811398 d_loss: 1.39913201, g_loss: 0.68904215, ae_loss: 0.04993989\n",
      "Step: [1232] total_loss: 2.12147498 d_loss: 1.37597585, g_loss: 0.69089240, ae_loss: 0.05460668\n",
      "Step: [1233] total_loss: 2.09892964 d_loss: 1.35262609, g_loss: 0.69367099, ae_loss: 0.05263262\n",
      "Step: [1234] total_loss: 2.11627817 d_loss: 1.38797939, g_loss: 0.67649937, ae_loss: 0.05179936\n",
      "Step: [1235] total_loss: 2.11643672 d_loss: 1.37524784, g_loss: 0.68454498, ae_loss: 0.05664401\n",
      "Step: [1236] total_loss: 2.13506126 d_loss: 1.38444602, g_loss: 0.70085996, ae_loss: 0.04975519\n",
      "Step: [1237] total_loss: 2.09570098 d_loss: 1.35378289, g_loss: 0.68873352, ae_loss: 0.05318465\n",
      "Step: [1238] total_loss: 2.10311174 d_loss: 1.34449506, g_loss: 0.70695829, ae_loss: 0.05165841\n",
      "Step: [1239] total_loss: 2.12310076 d_loss: 1.38558078, g_loss: 0.68125737, ae_loss: 0.05626250\n",
      "Step: [1240] total_loss: 2.13410854 d_loss: 1.39258432, g_loss: 0.68635434, ae_loss: 0.05516991\n",
      "Step: [1241] total_loss: 2.11956835 d_loss: 1.36998963, g_loss: 0.69510889, ae_loss: 0.05446974\n",
      "Step: [1242] total_loss: 2.10806656 d_loss: 1.37509930, g_loss: 0.67981207, ae_loss: 0.05315501\n",
      "Step: [1243] total_loss: 2.12475801 d_loss: 1.38086891, g_loss: 0.68999004, ae_loss: 0.05389910\n",
      "Step: [1244] total_loss: 2.13410640 d_loss: 1.40035129, g_loss: 0.68185109, ae_loss: 0.05190413\n",
      "Step: [1245] total_loss: 2.14514041 d_loss: 1.39710546, g_loss: 0.69473433, ae_loss: 0.05330062\n",
      "Step: [1246] total_loss: 2.15157890 d_loss: 1.40680385, g_loss: 0.68991244, ae_loss: 0.05486260\n",
      "Step: [1247] total_loss: 2.14263749 d_loss: 1.39272571, g_loss: 0.69769484, ae_loss: 0.05221698\n",
      "Step: [1248] total_loss: 2.13124776 d_loss: 1.37882257, g_loss: 0.70161140, ae_loss: 0.05081374\n",
      "Step: [1249] total_loss: 2.13518524 d_loss: 1.38696766, g_loss: 0.69523823, ae_loss: 0.05297918\n",
      "Step: [1250] total_loss: 2.14351249 d_loss: 1.38178790, g_loss: 0.70506263, ae_loss: 0.05666202\n",
      "Step: [1251] total_loss: 2.15104246 d_loss: 1.39305091, g_loss: 0.70677984, ae_loss: 0.05121155\n",
      "Step: [1252] total_loss: 2.11747551 d_loss: 1.38582850, g_loss: 0.67649436, ae_loss: 0.05515281\n",
      "Step: [1253] total_loss: 2.13328838 d_loss: 1.39331245, g_loss: 0.68764561, ae_loss: 0.05233026\n",
      "Step: [1254] total_loss: 2.10714269 d_loss: 1.37710381, g_loss: 0.67457396, ae_loss: 0.05546497\n",
      "Step: [1255] total_loss: 2.14247799 d_loss: 1.37461853, g_loss: 0.71441650, ae_loss: 0.05344294\n",
      "Step: [1256] total_loss: 2.11846447 d_loss: 1.35851181, g_loss: 0.70506310, ae_loss: 0.05488962\n",
      "Step: [1257] total_loss: 2.11621404 d_loss: 1.37242317, g_loss: 0.69082999, ae_loss: 0.05296084\n",
      "Step: [1258] total_loss: 2.12805629 d_loss: 1.37442541, g_loss: 0.70475268, ae_loss: 0.04887824\n",
      "Step: [1259] total_loss: 2.13917065 d_loss: 1.37936056, g_loss: 0.70872033, ae_loss: 0.05108975\n",
      "Step: [1260] total_loss: 2.12207985 d_loss: 1.39766741, g_loss: 0.67323291, ae_loss: 0.05117970\n",
      "Step: [1261] total_loss: 2.13073921 d_loss: 1.40892565, g_loss: 0.66859800, ae_loss: 0.05321550\n",
      "Step: [1262] total_loss: 2.11394405 d_loss: 1.36384618, g_loss: 0.69706106, ae_loss: 0.05303673\n",
      "Step: [1263] total_loss: 2.14419055 d_loss: 1.40719914, g_loss: 0.68495423, ae_loss: 0.05203710\n",
      "Step: [1264] total_loss: 2.16896677 d_loss: 1.41471815, g_loss: 0.69875318, ae_loss: 0.05549552\n",
      "Step: [1265] total_loss: 2.12148476 d_loss: 1.35567200, g_loss: 0.71566188, ae_loss: 0.05015096\n",
      "Step: [1266] total_loss: 2.13941097 d_loss: 1.39239502, g_loss: 0.69349647, ae_loss: 0.05351941\n",
      "Step: [1267] total_loss: 2.12917542 d_loss: 1.36383724, g_loss: 0.71159869, ae_loss: 0.05373952\n",
      "Step: [1268] total_loss: 2.12493849 d_loss: 1.39520872, g_loss: 0.67521060, ae_loss: 0.05451931\n",
      "Step: [1269] total_loss: 2.12242651 d_loss: 1.37845147, g_loss: 0.68672800, ae_loss: 0.05724711\n",
      "Step: [1270] total_loss: 2.12538290 d_loss: 1.37620234, g_loss: 0.69030321, ae_loss: 0.05887722\n",
      "Step: [1271] total_loss: 2.13270688 d_loss: 1.37490582, g_loss: 0.70179015, ae_loss: 0.05601088\n",
      "Step: [1272] total_loss: 2.13339186 d_loss: 1.37421989, g_loss: 0.70548153, ae_loss: 0.05369046\n",
      "Step: [1273] total_loss: 2.11339855 d_loss: 1.36633301, g_loss: 0.69380474, ae_loss: 0.05326096\n",
      "Step: [1274] total_loss: 2.13557005 d_loss: 1.39971721, g_loss: 0.68505847, ae_loss: 0.05079438\n",
      "Step: [1275] total_loss: 2.14269733 d_loss: 1.38682842, g_loss: 0.70144117, ae_loss: 0.05442762\n",
      "Step: [1276] total_loss: 2.13788557 d_loss: 1.38568330, g_loss: 0.70239651, ae_loss: 0.04980592\n",
      "Step: [1277] total_loss: 2.14467430 d_loss: 1.38597012, g_loss: 0.70728606, ae_loss: 0.05141818\n",
      "Step: [1278] total_loss: 2.11811876 d_loss: 1.37786901, g_loss: 0.68710488, ae_loss: 0.05314497\n",
      "Step: [1279] total_loss: 2.11271429 d_loss: 1.37996840, g_loss: 0.67635083, ae_loss: 0.05639489\n",
      "Step: [1280] total_loss: 2.10594940 d_loss: 1.37179542, g_loss: 0.68202972, ae_loss: 0.05212421\n",
      "Step: [1281] total_loss: 2.11674905 d_loss: 1.37139463, g_loss: 0.69071615, ae_loss: 0.05463832\n",
      "Step: [1282] total_loss: 2.14047551 d_loss: 1.38688076, g_loss: 0.70069265, ae_loss: 0.05290205\n",
      "Step: [1283] total_loss: 2.15095329 d_loss: 1.39762306, g_loss: 0.69706112, ae_loss: 0.05626909\n",
      "Step: [1284] total_loss: 2.10833240 d_loss: 1.36557436, g_loss: 0.69376737, ae_loss: 0.04899058\n",
      "Step: [1285] total_loss: 2.13715911 d_loss: 1.39244580, g_loss: 0.68833971, ae_loss: 0.05637362\n",
      "Step: [1286] total_loss: 2.13836479 d_loss: 1.41260242, g_loss: 0.67462188, ae_loss: 0.05114045\n",
      "Step: [1287] total_loss: 2.14418840 d_loss: 1.38634872, g_loss: 0.70427966, ae_loss: 0.05356019\n",
      "Step: [1288] total_loss: 2.12015700 d_loss: 1.36986470, g_loss: 0.69679779, ae_loss: 0.05349439\n",
      "Step: [1289] total_loss: 2.13894033 d_loss: 1.39521813, g_loss: 0.69473696, ae_loss: 0.04898524\n",
      "Step: [1290] total_loss: 2.14546347 d_loss: 1.39488792, g_loss: 0.69946408, ae_loss: 0.05111140\n",
      "Step: [1291] total_loss: 2.12340713 d_loss: 1.38604403, g_loss: 0.68281990, ae_loss: 0.05454328\n",
      "Step: [1292] total_loss: 2.12144923 d_loss: 1.37847674, g_loss: 0.68917233, ae_loss: 0.05380009\n",
      "Step: [1293] total_loss: 2.11664271 d_loss: 1.37159789, g_loss: 0.69235325, ae_loss: 0.05269157\n",
      "Step: [1294] total_loss: 2.12888861 d_loss: 1.39465666, g_loss: 0.68267655, ae_loss: 0.05155543\n",
      "Step: [1295] total_loss: 2.11870623 d_loss: 1.37892604, g_loss: 0.69104314, ae_loss: 0.04873721\n",
      "Step: [1296] total_loss: 2.13317680 d_loss: 1.39407170, g_loss: 0.68351293, ae_loss: 0.05559226\n",
      "Step: [1297] total_loss: 2.13859367 d_loss: 1.38736677, g_loss: 0.69767141, ae_loss: 0.05355557\n",
      "Step: [1298] total_loss: 2.13855982 d_loss: 1.36993122, g_loss: 0.71352541, ae_loss: 0.05510304\n",
      "Step: [1299] total_loss: 2.11326838 d_loss: 1.34518445, g_loss: 0.71657312, ae_loss: 0.05151072\n",
      "Step: [1300] total_loss: 2.12656116 d_loss: 1.38249052, g_loss: 0.68535078, ae_loss: 0.05871996\n",
      "Step: [1301] total_loss: 2.13855553 d_loss: 1.38160312, g_loss: 0.69971538, ae_loss: 0.05723689\n",
      "Step: [1302] total_loss: 2.13150597 d_loss: 1.38462412, g_loss: 0.69467640, ae_loss: 0.05220554\n",
      "Step: [1303] total_loss: 2.12868142 d_loss: 1.38721991, g_loss: 0.68738151, ae_loss: 0.05408001\n",
      "Step: [1304] total_loss: 2.15631008 d_loss: 1.39093590, g_loss: 0.71389359, ae_loss: 0.05148048\n",
      "Step: [1305] total_loss: 2.14340067 d_loss: 1.38545334, g_loss: 0.70408046, ae_loss: 0.05386670\n",
      "Step: [1306] total_loss: 2.13348770 d_loss: 1.38640475, g_loss: 0.68955892, ae_loss: 0.05752408\n",
      "Step: [1307] total_loss: 2.13691711 d_loss: 1.39898968, g_loss: 0.68409860, ae_loss: 0.05382881\n",
      "Step: [1308] total_loss: 2.12381530 d_loss: 1.36952567, g_loss: 0.70136327, ae_loss: 0.05292635\n",
      "Step: [1309] total_loss: 2.12714314 d_loss: 1.36964858, g_loss: 0.70297587, ae_loss: 0.05451866\n",
      "Step: [1310] total_loss: 2.11659956 d_loss: 1.37642980, g_loss: 0.68879461, ae_loss: 0.05137515\n",
      "Step: [1311] total_loss: 2.14409208 d_loss: 1.39676571, g_loss: 0.69471884, ae_loss: 0.05260751\n",
      "Step: [1312] total_loss: 2.12453866 d_loss: 1.38370466, g_loss: 0.69042253, ae_loss: 0.05041148\n",
      "Step: [1313] total_loss: 2.11982393 d_loss: 1.36268973, g_loss: 0.70269036, ae_loss: 0.05444379\n",
      "Step: [1314] total_loss: 2.12430811 d_loss: 1.35495973, g_loss: 0.71883923, ae_loss: 0.05050910\n",
      "Step: [1315] total_loss: 2.12100244 d_loss: 1.36666727, g_loss: 0.70143819, ae_loss: 0.05289702\n",
      "Step: [1316] total_loss: 2.12100577 d_loss: 1.38628316, g_loss: 0.67953682, ae_loss: 0.05518584\n",
      "Step: [1317] total_loss: 2.12842607 d_loss: 1.38332009, g_loss: 0.68531907, ae_loss: 0.05978692\n",
      "Step: [1318] total_loss: 2.11857605 d_loss: 1.35235405, g_loss: 0.71312183, ae_loss: 0.05310014\n",
      "Step: [1319] total_loss: 2.14297199 d_loss: 1.39732695, g_loss: 0.69541574, ae_loss: 0.05022922\n",
      "Step: [1320] total_loss: 2.14868069 d_loss: 1.37694478, g_loss: 0.71883947, ae_loss: 0.05289645\n",
      "Step: [1321] total_loss: 2.12179136 d_loss: 1.38639259, g_loss: 0.67504638, ae_loss: 0.06035249\n",
      "Step: [1322] total_loss: 2.09897017 d_loss: 1.38691199, g_loss: 0.66049367, ae_loss: 0.05156445\n",
      "Step: [1323] total_loss: 2.11827016 d_loss: 1.39812958, g_loss: 0.67211670, ae_loss: 0.04802389\n",
      "Step: [1324] total_loss: 2.12463903 d_loss: 1.35040283, g_loss: 0.71827960, ae_loss: 0.05595676\n",
      "Step: [1325] total_loss: 2.14395475 d_loss: 1.37553620, g_loss: 0.71812588, ae_loss: 0.05029259\n",
      "Step: [1326] total_loss: 2.12072515 d_loss: 1.34793353, g_loss: 0.71587121, ae_loss: 0.05692055\n",
      "Step: [1327] total_loss: 2.13059735 d_loss: 1.39320779, g_loss: 0.67987061, ae_loss: 0.05751900\n",
      "Step: [1328] total_loss: 2.15004873 d_loss: 1.40293074, g_loss: 0.69068199, ae_loss: 0.05643592\n",
      "Step: [1329] total_loss: 2.14576197 d_loss: 1.37857246, g_loss: 0.71352923, ae_loss: 0.05366015\n",
      "Step: [1330] total_loss: 2.12966394 d_loss: 1.38999248, g_loss: 0.68476605, ae_loss: 0.05490548\n",
      "Step: [1331] total_loss: 2.11777306 d_loss: 1.38561618, g_loss: 0.68151999, ae_loss: 0.05063682\n",
      "Step: [1332] total_loss: 2.14013505 d_loss: 1.38129497, g_loss: 0.70439786, ae_loss: 0.05444234\n",
      "Step: [1333] total_loss: 2.13546491 d_loss: 1.38780618, g_loss: 0.68995762, ae_loss: 0.05770117\n",
      "Step: [1334] total_loss: 2.14404249 d_loss: 1.39360714, g_loss: 0.69677532, ae_loss: 0.05365995\n",
      "Step: [1335] total_loss: 2.14214396 d_loss: 1.40419531, g_loss: 0.68565089, ae_loss: 0.05229769\n",
      "Step: [1336] total_loss: 2.12955618 d_loss: 1.37758040, g_loss: 0.69799644, ae_loss: 0.05397944\n",
      "Step: [1337] total_loss: 2.13902140 d_loss: 1.38168144, g_loss: 0.70668876, ae_loss: 0.05065119\n",
      "Step: [1338] total_loss: 2.11539936 d_loss: 1.35406375, g_loss: 0.70902342, ae_loss: 0.05231226\n",
      "Step: [1339] total_loss: 2.14678717 d_loss: 1.39890075, g_loss: 0.69562960, ae_loss: 0.05225682\n",
      "Step: [1340] total_loss: 2.12273645 d_loss: 1.36425567, g_loss: 0.70212060, ae_loss: 0.05636023\n",
      "Step: [1341] total_loss: 2.12662077 d_loss: 1.38051295, g_loss: 0.69843352, ae_loss: 0.04767448\n",
      "Step: [1342] total_loss: 2.12158656 d_loss: 1.35347509, g_loss: 0.71571958, ae_loss: 0.05239194\n",
      "Step: [1343] total_loss: 2.08944607 d_loss: 1.35623825, g_loss: 0.68230939, ae_loss: 0.05089837\n",
      "Step: [1344] total_loss: 2.13266969 d_loss: 1.38265920, g_loss: 0.69825000, ae_loss: 0.05176046\n",
      "Step: [1345] total_loss: 2.14156151 d_loss: 1.37713480, g_loss: 0.71005523, ae_loss: 0.05437155\n",
      "Step: [1346] total_loss: 2.13504410 d_loss: 1.40714037, g_loss: 0.67625087, ae_loss: 0.05165285\n",
      "Step: [1347] total_loss: 2.12322855 d_loss: 1.38074565, g_loss: 0.68914068, ae_loss: 0.05334207\n",
      "Step: [1348] total_loss: 2.13571548 d_loss: 1.39551568, g_loss: 0.68531156, ae_loss: 0.05488836\n",
      "Step: [1349] total_loss: 2.12984395 d_loss: 1.38023007, g_loss: 0.69785398, ae_loss: 0.05175989\n",
      "Step: [1350] total_loss: 2.11808920 d_loss: 1.38010812, g_loss: 0.68690443, ae_loss: 0.05107673\n",
      "Step: [1351] total_loss: 2.11417103 d_loss: 1.38978386, g_loss: 0.67409778, ae_loss: 0.05028928\n",
      "Step: [1352] total_loss: 2.13780046 d_loss: 1.38969481, g_loss: 0.69578934, ae_loss: 0.05231632\n",
      "Step: [1353] total_loss: 2.12748528 d_loss: 1.39801240, g_loss: 0.67996609, ae_loss: 0.04950695\n",
      "Step: [1354] total_loss: 2.14127588 d_loss: 1.39127612, g_loss: 0.69919801, ae_loss: 0.05080165\n",
      "Step: [1355] total_loss: 2.15264130 d_loss: 1.39006889, g_loss: 0.71037817, ae_loss: 0.05219431\n",
      "Step: [1356] total_loss: 2.15226698 d_loss: 1.38174200, g_loss: 0.71974885, ae_loss: 0.05077624\n",
      "Step: [1357] total_loss: 2.14443421 d_loss: 1.38508105, g_loss: 0.70737690, ae_loss: 0.05197636\n",
      "Step: [1358] total_loss: 2.12771773 d_loss: 1.37537587, g_loss: 0.69835246, ae_loss: 0.05398944\n",
      "Step: [1359] total_loss: 2.10938454 d_loss: 1.37942243, g_loss: 0.67871344, ae_loss: 0.05124876\n",
      "Step: [1360] total_loss: 2.12428331 d_loss: 1.38507617, g_loss: 0.68475282, ae_loss: 0.05445441\n",
      "Step: [1361] total_loss: 2.13186622 d_loss: 1.39613235, g_loss: 0.68052638, ae_loss: 0.05520754\n",
      "Step: [1362] total_loss: 2.12912798 d_loss: 1.34036803, g_loss: 0.73375547, ae_loss: 0.05500462\n",
      "Step: [1363] total_loss: 2.13037491 d_loss: 1.36625075, g_loss: 0.70996726, ae_loss: 0.05415699\n",
      "Step: [1364] total_loss: 2.13211823 d_loss: 1.37552190, g_loss: 0.70522821, ae_loss: 0.05136808\n",
      "Step: [1365] total_loss: 2.12697220 d_loss: 1.40257192, g_loss: 0.67306566, ae_loss: 0.05133472\n",
      "Step: [1366] total_loss: 2.12299013 d_loss: 1.38881469, g_loss: 0.67935979, ae_loss: 0.05481558\n",
      "Step: [1367] total_loss: 2.12503147 d_loss: 1.38875675, g_loss: 0.68551874, ae_loss: 0.05075590\n",
      "Step: [1368] total_loss: 2.13197327 d_loss: 1.38161957, g_loss: 0.70043814, ae_loss: 0.04991569\n",
      "Step: [1369] total_loss: 2.12194204 d_loss: 1.38556969, g_loss: 0.68206197, ae_loss: 0.05431048\n",
      "Step: [1370] total_loss: 2.12832856 d_loss: 1.36780429, g_loss: 0.71060991, ae_loss: 0.04991435\n",
      "Step: [1371] total_loss: 2.11985946 d_loss: 1.36239529, g_loss: 0.70711648, ae_loss: 0.05034768\n",
      "Step: [1372] total_loss: 2.13804436 d_loss: 1.38986135, g_loss: 0.69118953, ae_loss: 0.05699339\n",
      "Step: [1373] total_loss: 2.15071392 d_loss: 1.39309037, g_loss: 0.70440340, ae_loss: 0.05322019\n",
      "Step: [1374] total_loss: 2.14829445 d_loss: 1.38864374, g_loss: 0.70872974, ae_loss: 0.05092104\n",
      "Step: [1375] total_loss: 2.17250061 d_loss: 1.41080499, g_loss: 0.71031606, ae_loss: 0.05137939\n",
      "Step: [1376] total_loss: 2.13934612 d_loss: 1.39848042, g_loss: 0.68787497, ae_loss: 0.05299077\n",
      "Step: [1377] total_loss: 2.12619305 d_loss: 1.36304080, g_loss: 0.71207523, ae_loss: 0.05107706\n",
      "Step: [1378] total_loss: 2.13937664 d_loss: 1.37929666, g_loss: 0.70554054, ae_loss: 0.05453932\n",
      "Step: [1379] total_loss: 2.13066912 d_loss: 1.38254964, g_loss: 0.69480145, ae_loss: 0.05331801\n",
      "Step: [1380] total_loss: 2.13646126 d_loss: 1.40557289, g_loss: 0.67752522, ae_loss: 0.05336310\n",
      "Step: [1381] total_loss: 2.12214375 d_loss: 1.37739170, g_loss: 0.69189811, ae_loss: 0.05285409\n",
      "Step: [1382] total_loss: 2.13341904 d_loss: 1.38269413, g_loss: 0.69892925, ae_loss: 0.05179557\n",
      "Step: [1383] total_loss: 2.12483931 d_loss: 1.37258410, g_loss: 0.69948530, ae_loss: 0.05276985\n",
      "Step: [1384] total_loss: 2.12812042 d_loss: 1.38387108, g_loss: 0.68889654, ae_loss: 0.05535268\n",
      "Step: [1385] total_loss: 2.10887241 d_loss: 1.37636876, g_loss: 0.68284941, ae_loss: 0.04965431\n",
      "Step: [1386] total_loss: 2.15643573 d_loss: 1.39677119, g_loss: 0.70516014, ae_loss: 0.05450443\n",
      "Step: [1387] total_loss: 2.11363935 d_loss: 1.38200808, g_loss: 0.67630678, ae_loss: 0.05532440\n",
      "Step: [1388] total_loss: 2.14141822 d_loss: 1.37425566, g_loss: 0.71708268, ae_loss: 0.05007997\n",
      "Step: [1389] total_loss: 2.12006950 d_loss: 1.38506281, g_loss: 0.68188918, ae_loss: 0.05311753\n",
      "Step: [1390] total_loss: 2.11767673 d_loss: 1.38680339, g_loss: 0.67774200, ae_loss: 0.05313137\n",
      "Step: [1391] total_loss: 2.13344765 d_loss: 1.39488292, g_loss: 0.68488348, ae_loss: 0.05368139\n",
      "Step: [1392] total_loss: 2.11758852 d_loss: 1.37027717, g_loss: 0.69137645, ae_loss: 0.05593476\n",
      "Step: [1393] total_loss: 2.11142635 d_loss: 1.37313485, g_loss: 0.68668830, ae_loss: 0.05160308\n",
      "Step: [1394] total_loss: 2.12113762 d_loss: 1.36047626, g_loss: 0.70670366, ae_loss: 0.05395766\n",
      "Step: [1395] total_loss: 2.12836146 d_loss: 1.37627339, g_loss: 0.69397670, ae_loss: 0.05811149\n",
      "Step: [1396] total_loss: 2.12189126 d_loss: 1.35823202, g_loss: 0.71066332, ae_loss: 0.05299587\n",
      "Step: [1397] total_loss: 2.14711976 d_loss: 1.39673996, g_loss: 0.69506133, ae_loss: 0.05531853\n",
      "Step: [1398] total_loss: 2.14068604 d_loss: 1.39884365, g_loss: 0.68964243, ae_loss: 0.05219997\n",
      "Step: [1399] total_loss: 2.12730742 d_loss: 1.39289677, g_loss: 0.68461394, ae_loss: 0.04979682\n",
      "Step: [1400] total_loss: 2.09647727 d_loss: 1.35443091, g_loss: 0.68935037, ae_loss: 0.05269599\n",
      "Step: [1401] total_loss: 2.13103008 d_loss: 1.39645875, g_loss: 0.68204975, ae_loss: 0.05252156\n",
      "Step: [1402] total_loss: 2.11466479 d_loss: 1.37537491, g_loss: 0.68313831, ae_loss: 0.05615158\n",
      "Step: [1403] total_loss: 2.12510467 d_loss: 1.38454914, g_loss: 0.69116783, ae_loss: 0.04938775\n",
      "Step: [1404] total_loss: 2.11311197 d_loss: 1.36563623, g_loss: 0.69262052, ae_loss: 0.05485535\n",
      "Step: [1405] total_loss: 2.12028790 d_loss: 1.38553667, g_loss: 0.68131083, ae_loss: 0.05344031\n",
      "Step: [1406] total_loss: 2.11861277 d_loss: 1.37820625, g_loss: 0.68893158, ae_loss: 0.05147509\n",
      "Step: [1407] total_loss: 2.12485313 d_loss: 1.37537134, g_loss: 0.69770455, ae_loss: 0.05177731\n",
      "Step: [1408] total_loss: 2.13841343 d_loss: 1.38240194, g_loss: 0.70408094, ae_loss: 0.05193038\n",
      "Step: [1409] total_loss: 2.12734103 d_loss: 1.37867248, g_loss: 0.69671822, ae_loss: 0.05195039\n",
      "Step: [1410] total_loss: 2.12891531 d_loss: 1.37768555, g_loss: 0.69979441, ae_loss: 0.05143525\n",
      "Step: [1411] total_loss: 2.11214447 d_loss: 1.38222432, g_loss: 0.67647725, ae_loss: 0.05344284\n",
      "Step: [1412] total_loss: 2.12728739 d_loss: 1.38214171, g_loss: 0.69326007, ae_loss: 0.05188547\n",
      "Step: [1413] total_loss: 2.16705608 d_loss: 1.42154121, g_loss: 0.69206583, ae_loss: 0.05344893\n",
      "Step: [1414] total_loss: 2.13839531 d_loss: 1.38074660, g_loss: 0.70576024, ae_loss: 0.05188861\n",
      "Step: [1415] total_loss: 2.11051846 d_loss: 1.37214684, g_loss: 0.68706012, ae_loss: 0.05131140\n",
      "Step: [1416] total_loss: 2.13430357 d_loss: 1.40192378, g_loss: 0.67912632, ae_loss: 0.05325339\n",
      "Step: [1417] total_loss: 2.12272978 d_loss: 1.36685538, g_loss: 0.70129907, ae_loss: 0.05457524\n",
      "Step: [1418] total_loss: 2.14034510 d_loss: 1.36784339, g_loss: 0.72439158, ae_loss: 0.04811006\n",
      "Step: [1419] total_loss: 2.12044716 d_loss: 1.35305977, g_loss: 0.71347755, ae_loss: 0.05390977\n",
      "Step: [1420] total_loss: 2.12423182 d_loss: 1.35430205, g_loss: 0.71659511, ae_loss: 0.05333455\n",
      "Step: [1421] total_loss: 2.11809683 d_loss: 1.36016917, g_loss: 0.70463598, ae_loss: 0.05329167\n",
      "Step: [1422] total_loss: 2.10435557 d_loss: 1.38528037, g_loss: 0.66952252, ae_loss: 0.04955272\n",
      "Step: [1423] total_loss: 2.12510037 d_loss: 1.39380562, g_loss: 0.67869949, ae_loss: 0.05259521\n",
      "Step: [1424] total_loss: 2.14524126 d_loss: 1.40962481, g_loss: 0.67981601, ae_loss: 0.05580053\n",
      "Step: [1425] total_loss: 2.13736677 d_loss: 1.39936996, g_loss: 0.68900794, ae_loss: 0.04898891\n",
      "Step: [1426] total_loss: 2.14134836 d_loss: 1.39549708, g_loss: 0.69125015, ae_loss: 0.05460105\n",
      "Step: [1427] total_loss: 2.13885641 d_loss: 1.38464499, g_loss: 0.70524657, ae_loss: 0.04896480\n",
      "Step: [1428] total_loss: 2.13967204 d_loss: 1.37765121, g_loss: 0.71125704, ae_loss: 0.05076375\n",
      "Step: [1429] total_loss: 2.14330101 d_loss: 1.38291454, g_loss: 0.70736003, ae_loss: 0.05302653\n",
      "Step: [1430] total_loss: 2.12106323 d_loss: 1.37699616, g_loss: 0.69212902, ae_loss: 0.05193797\n",
      "Step: [1431] total_loss: 2.12721252 d_loss: 1.40738153, g_loss: 0.66896284, ae_loss: 0.05086832\n",
      "Step: [1432] total_loss: 2.11293745 d_loss: 1.38537788, g_loss: 0.67661405, ae_loss: 0.05094568\n",
      "Step: [1433] total_loss: 2.09941363 d_loss: 1.37931049, g_loss: 0.66752684, ae_loss: 0.05257636\n",
      "Step: [1434] total_loss: 2.12030983 d_loss: 1.35253167, g_loss: 0.71502393, ae_loss: 0.05275429\n",
      "Step: [1435] total_loss: 2.13991213 d_loss: 1.39166021, g_loss: 0.69799018, ae_loss: 0.05026176\n",
      "Step: [1436] total_loss: 2.13781834 d_loss: 1.39230847, g_loss: 0.68743598, ae_loss: 0.05807381\n",
      "Step: [1437] total_loss: 2.10850239 d_loss: 1.37571526, g_loss: 0.68191993, ae_loss: 0.05086713\n",
      "Step: [1438] total_loss: 2.13251662 d_loss: 1.39267015, g_loss: 0.68910688, ae_loss: 0.05073952\n",
      "Step: [1439] total_loss: 2.13643646 d_loss: 1.38995099, g_loss: 0.68948442, ae_loss: 0.05700097\n",
      "Step: [1440] total_loss: 2.11688781 d_loss: 1.37614059, g_loss: 0.68583947, ae_loss: 0.05490777\n",
      "Step: [1441] total_loss: 2.10933542 d_loss: 1.37511384, g_loss: 0.68543327, ae_loss: 0.04878831\n",
      "Step: [1442] total_loss: 2.12176156 d_loss: 1.38407993, g_loss: 0.67917329, ae_loss: 0.05850833\n",
      "Step: [1443] total_loss: 2.13637590 d_loss: 1.37706494, g_loss: 0.70320523, ae_loss: 0.05610564\n",
      "Step: [1444] total_loss: 2.14285183 d_loss: 1.39235187, g_loss: 0.69671607, ae_loss: 0.05378374\n",
      "Step: [1445] total_loss: 2.10875821 d_loss: 1.36183953, g_loss: 0.69620639, ae_loss: 0.05071232\n",
      "Step: [1446] total_loss: 2.14043760 d_loss: 1.38600707, g_loss: 0.70504069, ae_loss: 0.04938970\n",
      "Step: [1447] total_loss: 2.11186767 d_loss: 1.38094926, g_loss: 0.67954969, ae_loss: 0.05136869\n",
      "Step: [1448] total_loss: 2.11740756 d_loss: 1.37553811, g_loss: 0.68862778, ae_loss: 0.05324178\n",
      "Step: [1449] total_loss: 2.12193465 d_loss: 1.38612485, g_loss: 0.68290716, ae_loss: 0.05290262\n",
      "Step: [1450] total_loss: 2.13743520 d_loss: 1.39684272, g_loss: 0.68869227, ae_loss: 0.05190032\n",
      "Step: [1451] total_loss: 2.12811565 d_loss: 1.36535096, g_loss: 0.70798612, ae_loss: 0.05477841\n",
      "Step: [1452] total_loss: 2.13554478 d_loss: 1.38980722, g_loss: 0.69068623, ae_loss: 0.05505148\n",
      "Step: [1453] total_loss: 2.11132884 d_loss: 1.35641432, g_loss: 0.69756413, ae_loss: 0.05735043\n",
      "Step: [1454] total_loss: 2.14405918 d_loss: 1.41489005, g_loss: 0.67795944, ae_loss: 0.05120984\n",
      "Step: [1455] total_loss: 2.15299678 d_loss: 1.38596153, g_loss: 0.70968157, ae_loss: 0.05735367\n",
      "Step: [1456] total_loss: 2.10842228 d_loss: 1.35794950, g_loss: 0.69766510, ae_loss: 0.05280771\n",
      "Step: [1457] total_loss: 2.09440589 d_loss: 1.35723066, g_loss: 0.68378478, ae_loss: 0.05339034\n",
      "Step: [1458] total_loss: 2.10146737 d_loss: 1.37451792, g_loss: 0.67308867, ae_loss: 0.05386080\n",
      "Step: [1459] total_loss: 2.10413027 d_loss: 1.35029888, g_loss: 0.70240414, ae_loss: 0.05142725\n",
      "Step: [1460] total_loss: 2.12071133 d_loss: 1.35758913, g_loss: 0.71194112, ae_loss: 0.05118117\n",
      "Step: [1461] total_loss: 2.14548397 d_loss: 1.38814223, g_loss: 0.69936144, ae_loss: 0.05798046\n",
      "Step: [1462] total_loss: 2.16049409 d_loss: 1.41269636, g_loss: 0.69730318, ae_loss: 0.05049454\n",
      "Step: [1463] total_loss: 2.15581441 d_loss: 1.40404606, g_loss: 0.70162368, ae_loss: 0.05014464\n",
      "Step: [1464] total_loss: 2.13441181 d_loss: 1.36580229, g_loss: 0.71588778, ae_loss: 0.05272166\n",
      "Step: [1465] total_loss: 2.13555384 d_loss: 1.37944317, g_loss: 0.69834101, ae_loss: 0.05776948\n",
      "Step: [1466] total_loss: 2.17378139 d_loss: 1.40510464, g_loss: 0.71777803, ae_loss: 0.05089881\n",
      "Step: [1467] total_loss: 2.14997172 d_loss: 1.38249493, g_loss: 0.70913661, ae_loss: 0.05834021\n",
      "Step: [1468] total_loss: 2.15133524 d_loss: 1.40553498, g_loss: 0.69022238, ae_loss: 0.05557795\n",
      "Step: [1469] total_loss: 2.11484194 d_loss: 1.38177538, g_loss: 0.68068397, ae_loss: 0.05238248\n",
      "Step: [1470] total_loss: 2.13336110 d_loss: 1.37689245, g_loss: 0.70190984, ae_loss: 0.05455884\n",
      "Step: [1471] total_loss: 2.11752367 d_loss: 1.38235402, g_loss: 0.68215537, ae_loss: 0.05301431\n",
      "Step: [1472] total_loss: 2.12307358 d_loss: 1.38619924, g_loss: 0.68549454, ae_loss: 0.05137972\n",
      "Step: [1473] total_loss: 2.12657952 d_loss: 1.38563347, g_loss: 0.69009840, ae_loss: 0.05084771\n",
      "Step: [1474] total_loss: 2.11643744 d_loss: 1.35587907, g_loss: 0.70553577, ae_loss: 0.05502251\n",
      "Step: [1475] total_loss: 2.14069891 d_loss: 1.37554407, g_loss: 0.70826292, ae_loss: 0.05689194\n",
      "Step: [1476] total_loss: 2.16169024 d_loss: 1.40294325, g_loss: 0.70593470, ae_loss: 0.05281230\n",
      "Step: [1477] total_loss: 2.14930582 d_loss: 1.39996493, g_loss: 0.69449049, ae_loss: 0.05485031\n",
      "Step: [1478] total_loss: 2.14003515 d_loss: 1.37796164, g_loss: 0.70653808, ae_loss: 0.05553547\n",
      "Step: [1479] total_loss: 2.13166857 d_loss: 1.36238956, g_loss: 0.71574914, ae_loss: 0.05353000\n",
      "Step: [1480] total_loss: 2.11984634 d_loss: 1.37744439, g_loss: 0.69272667, ae_loss: 0.04967536\n",
      "Step: [1481] total_loss: 2.14112473 d_loss: 1.38351083, g_loss: 0.70386529, ae_loss: 0.05374844\n",
      "Step: [1482] total_loss: 2.13447714 d_loss: 1.36309767, g_loss: 0.71431828, ae_loss: 0.05706133\n",
      "Step: [1483] total_loss: 2.16110659 d_loss: 1.39605772, g_loss: 0.70747578, ae_loss: 0.05757293\n",
      "Step: [1484] total_loss: 2.11993337 d_loss: 1.39052987, g_loss: 0.68237787, ae_loss: 0.04702562\n",
      "Step: [1485] total_loss: 2.11948037 d_loss: 1.37627554, g_loss: 0.68919361, ae_loss: 0.05401122\n",
      "Step: [1486] total_loss: 2.12467813 d_loss: 1.37783289, g_loss: 0.69540012, ae_loss: 0.05144518\n",
      "Step: [1487] total_loss: 2.14184141 d_loss: 1.38445520, g_loss: 0.70443690, ae_loss: 0.05294949\n",
      "Step: [1488] total_loss: 2.15521598 d_loss: 1.40639210, g_loss: 0.69440013, ae_loss: 0.05442373\n",
      "Step: [1489] total_loss: 2.13346362 d_loss: 1.35550904, g_loss: 0.72611910, ae_loss: 0.05183541\n",
      "Step: [1490] total_loss: 2.13028765 d_loss: 1.38465393, g_loss: 0.69556665, ae_loss: 0.05006702\n",
      "Step: [1491] total_loss: 2.12663603 d_loss: 1.37233293, g_loss: 0.70627385, ae_loss: 0.04802921\n",
      "Step: [1492] total_loss: 2.13478088 d_loss: 1.39662576, g_loss: 0.68454814, ae_loss: 0.05360715\n",
      "Step: [1493] total_loss: 2.12311435 d_loss: 1.38041377, g_loss: 0.69490653, ae_loss: 0.04779412\n",
      "Step: [1494] total_loss: 2.10826874 d_loss: 1.36737561, g_loss: 0.68890321, ae_loss: 0.05198985\n",
      "Step: [1495] total_loss: 2.14152908 d_loss: 1.38285601, g_loss: 0.71035647, ae_loss: 0.04831664\n",
      "Step: [1496] total_loss: 2.13087320 d_loss: 1.37010336, g_loss: 0.70660871, ae_loss: 0.05416118\n",
      "Step: [1497] total_loss: 2.13066006 d_loss: 1.38997161, g_loss: 0.68788910, ae_loss: 0.05279936\n",
      "Step: [1498] total_loss: 2.14043665 d_loss: 1.39404011, g_loss: 0.69393498, ae_loss: 0.05246149\n",
      "Step: [1499] total_loss: 2.13883185 d_loss: 1.37265062, g_loss: 0.71774375, ae_loss: 0.04843742\n",
      "Step: [1500] total_loss: 2.12365580 d_loss: 1.39213836, g_loss: 0.67405617, ae_loss: 0.05746135\n",
      "Step: [1501] total_loss: 2.10694170 d_loss: 1.37512982, g_loss: 0.67802030, ae_loss: 0.05379164\n",
      "Step: [1502] total_loss: 2.10571480 d_loss: 1.37750268, g_loss: 0.67646551, ae_loss: 0.05174661\n",
      "Step: [1503] total_loss: 2.11362553 d_loss: 1.38611150, g_loss: 0.67525864, ae_loss: 0.05225556\n",
      "Step: [1504] total_loss: 2.11683035 d_loss: 1.36475885, g_loss: 0.70169020, ae_loss: 0.05038133\n",
      "Step: [1505] total_loss: 2.12052584 d_loss: 1.38188672, g_loss: 0.68139625, ae_loss: 0.05724290\n",
      "Step: [1506] total_loss: 2.13964033 d_loss: 1.40905476, g_loss: 0.67777503, ae_loss: 0.05281043\n",
      "Step: [1507] total_loss: 2.12239742 d_loss: 1.37745309, g_loss: 0.69125170, ae_loss: 0.05369272\n",
      "Step: [1508] total_loss: 2.13794613 d_loss: 1.39133739, g_loss: 0.69465309, ae_loss: 0.05195555\n",
      "Step: [1509] total_loss: 2.15401554 d_loss: 1.39544439, g_loss: 0.70778918, ae_loss: 0.05078199\n",
      "Step: [1510] total_loss: 2.13403416 d_loss: 1.37440431, g_loss: 0.70792651, ae_loss: 0.05170326\n",
      "Step: [1511] total_loss: 2.12054300 d_loss: 1.37198997, g_loss: 0.69414723, ae_loss: 0.05440591\n",
      "Step: [1512] total_loss: 2.13389039 d_loss: 1.37184727, g_loss: 0.70817351, ae_loss: 0.05386962\n",
      "Step: [1513] total_loss: 2.14792156 d_loss: 1.38171124, g_loss: 0.71558404, ae_loss: 0.05062611\n",
      "Step: [1514] total_loss: 2.11504197 d_loss: 1.37506616, g_loss: 0.68746883, ae_loss: 0.05250694\n",
      "Step: [1515] total_loss: 2.13799667 d_loss: 1.38455439, g_loss: 0.69605350, ae_loss: 0.05738892\n",
      "Step: [1516] total_loss: 2.14303565 d_loss: 1.37170112, g_loss: 0.72101623, ae_loss: 0.05031823\n",
      "Step: [1517] total_loss: 2.13933086 d_loss: 1.39464378, g_loss: 0.69359446, ae_loss: 0.05109267\n",
      "Step: [1518] total_loss: 2.12985587 d_loss: 1.36704278, g_loss: 0.70716733, ae_loss: 0.05564583\n",
      "Step: [1519] total_loss: 2.14147925 d_loss: 1.38578320, g_loss: 0.69967538, ae_loss: 0.05602060\n",
      "Step: [1520] total_loss: 2.12361574 d_loss: 1.37202418, g_loss: 0.69700575, ae_loss: 0.05458571\n",
      "Step: [1521] total_loss: 2.10586977 d_loss: 1.37880254, g_loss: 0.67409766, ae_loss: 0.05296944\n",
      "Step: [1522] total_loss: 2.11502767 d_loss: 1.37809038, g_loss: 0.68423080, ae_loss: 0.05270648\n",
      "Step: [1523] total_loss: 2.12094212 d_loss: 1.36525774, g_loss: 0.70362306, ae_loss: 0.05206120\n",
      "Step: [1524] total_loss: 2.09383917 d_loss: 1.36422837, g_loss: 0.67634213, ae_loss: 0.05326863\n",
      "Step: [1525] total_loss: 2.13104606 d_loss: 1.40047860, g_loss: 0.67654324, ae_loss: 0.05402416\n",
      "Step: [1526] total_loss: 2.11270523 d_loss: 1.37511206, g_loss: 0.68085480, ae_loss: 0.05673825\n",
      "Step: [1527] total_loss: 2.15363407 d_loss: 1.39978623, g_loss: 0.69861746, ae_loss: 0.05523052\n",
      "Step: [1528] total_loss: 2.13944697 d_loss: 1.38167763, g_loss: 0.70285368, ae_loss: 0.05491566\n",
      "Step: [1529] total_loss: 2.14946151 d_loss: 1.38816118, g_loss: 0.70869893, ae_loss: 0.05260132\n",
      "Step: [1530] total_loss: 2.14151740 d_loss: 1.39196253, g_loss: 0.69784194, ae_loss: 0.05171283\n",
      "Step: [1531] total_loss: 2.14144659 d_loss: 1.39995503, g_loss: 0.68661678, ae_loss: 0.05487479\n",
      "Step: [1532] total_loss: 2.13475442 d_loss: 1.38386273, g_loss: 0.69742674, ae_loss: 0.05346495\n",
      "Step: [1533] total_loss: 2.12254214 d_loss: 1.35975099, g_loss: 0.71174544, ae_loss: 0.05104575\n",
      "Step: [1534] total_loss: 2.12055087 d_loss: 1.37749183, g_loss: 0.69117904, ae_loss: 0.05188002\n",
      "Step: [1535] total_loss: 2.11760545 d_loss: 1.36796165, g_loss: 0.69539022, ae_loss: 0.05425363\n",
      "Step: [1536] total_loss: 2.13610888 d_loss: 1.39064550, g_loss: 0.69253469, ae_loss: 0.05292865\n",
      "Step: [1537] total_loss: 2.12923765 d_loss: 1.40064037, g_loss: 0.67716479, ae_loss: 0.05143248\n",
      "Step: [1538] total_loss: 2.14591551 d_loss: 1.38106680, g_loss: 0.71358681, ae_loss: 0.05126205\n",
      "Step: [1539] total_loss: 2.13083744 d_loss: 1.39029098, g_loss: 0.69006312, ae_loss: 0.05048344\n",
      "Step: [1540] total_loss: 2.09156513 d_loss: 1.33498597, g_loss: 0.70494819, ae_loss: 0.05163091\n",
      "Step: [1541] total_loss: 2.13282394 d_loss: 1.38565946, g_loss: 0.69659609, ae_loss: 0.05056848\n",
      "Step: [1542] total_loss: 2.12483740 d_loss: 1.39253807, g_loss: 0.67745686, ae_loss: 0.05484237\n",
      "Step: [1543] total_loss: 2.11354971 d_loss: 1.37231112, g_loss: 0.68921393, ae_loss: 0.05202474\n",
      "Step: [1544] total_loss: 2.11744046 d_loss: 1.38220680, g_loss: 0.68624902, ae_loss: 0.04898469\n",
      "Step: [1545] total_loss: 2.11850071 d_loss: 1.38490772, g_loss: 0.68353033, ae_loss: 0.05006255\n",
      "Step: [1546] total_loss: 2.13402462 d_loss: 1.38720846, g_loss: 0.69017261, ae_loss: 0.05664356\n",
      "Step: [1547] total_loss: 2.13728762 d_loss: 1.39668775, g_loss: 0.68516284, ae_loss: 0.05543708\n",
      "Step: [1548] total_loss: 2.14377594 d_loss: 1.38221526, g_loss: 0.70722830, ae_loss: 0.05433236\n",
      "Step: [1549] total_loss: 2.12290812 d_loss: 1.36072493, g_loss: 0.70755804, ae_loss: 0.05462512\n",
      "Step: [1550] total_loss: 2.12012696 d_loss: 1.38153076, g_loss: 0.68485701, ae_loss: 0.05373920\n",
      "Step: [1551] total_loss: 2.16072202 d_loss: 1.39812469, g_loss: 0.70690930, ae_loss: 0.05568800\n",
      "Step: [1552] total_loss: 2.13409233 d_loss: 1.38137758, g_loss: 0.69786012, ae_loss: 0.05485476\n",
      "Step: [1553] total_loss: 2.17606902 d_loss: 1.39528489, g_loss: 0.73020625, ae_loss: 0.05057791\n",
      "Step: [1554] total_loss: 2.12873077 d_loss: 1.38207054, g_loss: 0.69174504, ae_loss: 0.05491516\n",
      "Step: [1555] total_loss: 2.11681223 d_loss: 1.36098838, g_loss: 0.70259333, ae_loss: 0.05323045\n",
      "Step: [1556] total_loss: 2.11113954 d_loss: 1.37090635, g_loss: 0.68377113, ae_loss: 0.05646209\n",
      "Step: [1557] total_loss: 2.15183282 d_loss: 1.41356754, g_loss: 0.68157238, ae_loss: 0.05669297\n",
      "Step: [1558] total_loss: 2.12256002 d_loss: 1.37439871, g_loss: 0.69294137, ae_loss: 0.05521996\n",
      "Step: [1559] total_loss: 2.13356876 d_loss: 1.39455736, g_loss: 0.68608820, ae_loss: 0.05292320\n",
      "Step: [1560] total_loss: 2.11546254 d_loss: 1.37422216, g_loss: 0.68502104, ae_loss: 0.05621929\n",
      "Step: [1561] total_loss: 2.12836242 d_loss: 1.38632298, g_loss: 0.68272495, ae_loss: 0.05931449\n",
      "Step: [1562] total_loss: 2.12906933 d_loss: 1.40143752, g_loss: 0.67484033, ae_loss: 0.05279165\n",
      "Step: [1563] total_loss: 2.13778019 d_loss: 1.39034784, g_loss: 0.69327486, ae_loss: 0.05415758\n",
      "Step: [1564] total_loss: 2.14551115 d_loss: 1.37836790, g_loss: 0.71342838, ae_loss: 0.05371486\n",
      "Step: [1565] total_loss: 2.13673639 d_loss: 1.36787677, g_loss: 0.71522754, ae_loss: 0.05363213\n",
      "Step: [1566] total_loss: 2.13238430 d_loss: 1.38881755, g_loss: 0.68924117, ae_loss: 0.05432576\n",
      "Step: [1567] total_loss: 2.12159848 d_loss: 1.36825633, g_loss: 0.69911617, ae_loss: 0.05422586\n",
      "Step: [1568] total_loss: 2.14557242 d_loss: 1.38683105, g_loss: 0.70357960, ae_loss: 0.05516183\n",
      "Step: [1569] total_loss: 2.12273407 d_loss: 1.37951720, g_loss: 0.68947411, ae_loss: 0.05374260\n",
      "Step: [1570] total_loss: 2.12974286 d_loss: 1.40453827, g_loss: 0.67698175, ae_loss: 0.04822274\n",
      "Step: [1571] total_loss: 2.13457799 d_loss: 1.39114320, g_loss: 0.68734312, ae_loss: 0.05609164\n",
      "Step: [1572] total_loss: 2.10747290 d_loss: 1.36680651, g_loss: 0.68615061, ae_loss: 0.05451580\n",
      "Step: [1573] total_loss: 2.13195705 d_loss: 1.40245306, g_loss: 0.67634314, ae_loss: 0.05316086\n",
      "Step: [1574] total_loss: 2.11967683 d_loss: 1.39127147, g_loss: 0.67692584, ae_loss: 0.05147956\n",
      "Step: [1575] total_loss: 2.11336946 d_loss: 1.36599994, g_loss: 0.69150960, ae_loss: 0.05585990\n",
      "Step: [1576] total_loss: 2.15862393 d_loss: 1.39503193, g_loss: 0.70454103, ae_loss: 0.05905096\n",
      "Step: [1577] total_loss: 2.14170599 d_loss: 1.38380480, g_loss: 0.70306063, ae_loss: 0.05484051\n",
      "Step: [1578] total_loss: 2.15409756 d_loss: 1.39663172, g_loss: 0.70235413, ae_loss: 0.05511162\n",
      "Step: [1579] total_loss: 2.13218355 d_loss: 1.38134456, g_loss: 0.69148237, ae_loss: 0.05935670\n",
      "Step: [1580] total_loss: 2.12283945 d_loss: 1.38331366, g_loss: 0.68652403, ae_loss: 0.05300187\n",
      "Step: [1581] total_loss: 2.12546110 d_loss: 1.37974882, g_loss: 0.69116127, ae_loss: 0.05455112\n",
      "Step: [1582] total_loss: 2.12396407 d_loss: 1.40357566, g_loss: 0.66441476, ae_loss: 0.05597369\n",
      "Step: [1583] total_loss: 2.11643171 d_loss: 1.37695897, g_loss: 0.68770450, ae_loss: 0.05176814\n",
      "Step: [1584] total_loss: 2.10899591 d_loss: 1.37231576, g_loss: 0.68422645, ae_loss: 0.05245370\n",
      "Step: [1585] total_loss: 2.11277509 d_loss: 1.36030400, g_loss: 0.70184088, ae_loss: 0.05063027\n",
      "Step: [1586] total_loss: 2.12657785 d_loss: 1.38886547, g_loss: 0.68687725, ae_loss: 0.05083520\n",
      "Step: [1587] total_loss: 2.14116931 d_loss: 1.37495852, g_loss: 0.71127343, ae_loss: 0.05493730\n",
      "Step: [1588] total_loss: 2.11239123 d_loss: 1.34926414, g_loss: 0.70881748, ae_loss: 0.05430957\n",
      "Step: [1589] total_loss: 2.13044691 d_loss: 1.37329197, g_loss: 0.70591462, ae_loss: 0.05124033\n",
      "Step: [1590] total_loss: 2.12751913 d_loss: 1.40035415, g_loss: 0.67719990, ae_loss: 0.04996502\n",
      "Step: [1591] total_loss: 2.12595654 d_loss: 1.36643898, g_loss: 0.70721644, ae_loss: 0.05230115\n",
      "Step: [1592] total_loss: 2.12132120 d_loss: 1.37500644, g_loss: 0.69101155, ae_loss: 0.05530328\n",
      "Step: [1593] total_loss: 2.10345030 d_loss: 1.38172996, g_loss: 0.67061532, ae_loss: 0.05110508\n",
      "Step: [1594] total_loss: 2.11005974 d_loss: 1.37156749, g_loss: 0.68511605, ae_loss: 0.05337607\n",
      "Step: [1595] total_loss: 2.13464355 d_loss: 1.39534652, g_loss: 0.68974841, ae_loss: 0.04954864\n",
      "Step: [1596] total_loss: 2.15354204 d_loss: 1.39047766, g_loss: 0.71068692, ae_loss: 0.05237748\n",
      "Step: [1597] total_loss: 2.12911677 d_loss: 1.38050079, g_loss: 0.69399762, ae_loss: 0.05461839\n",
      "Step: [1598] total_loss: 2.12649465 d_loss: 1.37488937, g_loss: 0.69935369, ae_loss: 0.05225161\n",
      "Step: [1599] total_loss: 2.13707376 d_loss: 1.38616776, g_loss: 0.70168054, ae_loss: 0.04922540\n",
      "Step: [1600] total_loss: 2.14641023 d_loss: 1.39092636, g_loss: 0.70271975, ae_loss: 0.05276406\n",
      "Step: [1601] total_loss: 2.13028121 d_loss: 1.37261486, g_loss: 0.70791537, ae_loss: 0.04975105\n",
      "Step: [1602] total_loss: 2.13886237 d_loss: 1.37227285, g_loss: 0.71016556, ae_loss: 0.05642403\n",
      "Step: [1603] total_loss: 2.11770678 d_loss: 1.37737250, g_loss: 0.68918538, ae_loss: 0.05114900\n",
      "Step: [1604] total_loss: 2.09887981 d_loss: 1.36511064, g_loss: 0.68720996, ae_loss: 0.04655924\n",
      "Step: [1605] total_loss: 2.12768507 d_loss: 1.39356780, g_loss: 0.68343472, ae_loss: 0.05068237\n",
      "Step: [1606] total_loss: 2.11830950 d_loss: 1.38488507, g_loss: 0.67861032, ae_loss: 0.05481410\n",
      "Step: [1607] total_loss: 2.12876368 d_loss: 1.38953328, g_loss: 0.68864346, ae_loss: 0.05058698\n",
      "Step: [1608] total_loss: 2.15056300 d_loss: 1.39819670, g_loss: 0.69981843, ae_loss: 0.05254782\n",
      "Step: [1609] total_loss: 2.13528872 d_loss: 1.37712336, g_loss: 0.70123249, ae_loss: 0.05693299\n",
      "Step: [1610] total_loss: 2.14879060 d_loss: 1.38828456, g_loss: 0.70708805, ae_loss: 0.05341808\n",
      "Step: [1611] total_loss: 2.13879871 d_loss: 1.37697661, g_loss: 0.70955956, ae_loss: 0.05226268\n",
      "Step: [1612] total_loss: 2.13712144 d_loss: 1.36123598, g_loss: 0.71543908, ae_loss: 0.06044638\n",
      "Step: [1613] total_loss: 2.13548183 d_loss: 1.39627457, g_loss: 0.68809676, ae_loss: 0.05111046\n",
      "Step: [1614] total_loss: 2.14291430 d_loss: 1.38775682, g_loss: 0.69960940, ae_loss: 0.05554795\n",
      "Step: [1615] total_loss: 2.08972216 d_loss: 1.35850036, g_loss: 0.67536521, ae_loss: 0.05585663\n",
      "Step: [1616] total_loss: 2.11352682 d_loss: 1.37134576, g_loss: 0.69424105, ae_loss: 0.04793993\n",
      "Step: [1617] total_loss: 2.09502029 d_loss: 1.37981486, g_loss: 0.66472197, ae_loss: 0.05048333\n",
      "Step: [1618] total_loss: 2.11534452 d_loss: 1.38270140, g_loss: 0.67753899, ae_loss: 0.05510414\n",
      "Step: [1619] total_loss: 2.12892246 d_loss: 1.39665568, g_loss: 0.67625183, ae_loss: 0.05601485\n",
      "Step: [1620] total_loss: 2.12157369 d_loss: 1.38213944, g_loss: 0.69032180, ae_loss: 0.04911248\n",
      "Step: [1621] total_loss: 2.14866686 d_loss: 1.38246691, g_loss: 0.71115661, ae_loss: 0.05504336\n",
      "Step: [1622] total_loss: 2.15282488 d_loss: 1.38906145, g_loss: 0.71286082, ae_loss: 0.05090246\n",
      "Step: [1623] total_loss: 2.12711596 d_loss: 1.35853100, g_loss: 0.71692038, ae_loss: 0.05166455\n",
      "Step: [1624] total_loss: 2.12510443 d_loss: 1.37949598, g_loss: 0.69696295, ae_loss: 0.04864567\n",
      "Step: [1625] total_loss: 2.10796976 d_loss: 1.36947179, g_loss: 0.68833101, ae_loss: 0.05016693\n",
      "Step: [1626] total_loss: 2.09828019 d_loss: 1.36673355, g_loss: 0.67647052, ae_loss: 0.05507612\n",
      "Step: [1627] total_loss: 2.14543295 d_loss: 1.39736640, g_loss: 0.69310176, ae_loss: 0.05496466\n",
      "Step: [1628] total_loss: 2.13883042 d_loss: 1.39829624, g_loss: 0.68535864, ae_loss: 0.05517554\n",
      "Step: [1629] total_loss: 2.14264035 d_loss: 1.36894417, g_loss: 0.72589284, ae_loss: 0.04780336\n",
      "Step: [1630] total_loss: 2.12611485 d_loss: 1.36157048, g_loss: 0.71080077, ae_loss: 0.05374376\n",
      "Step: [1631] total_loss: 2.14661884 d_loss: 1.37892270, g_loss: 0.71396798, ae_loss: 0.05372823\n",
      "Step: [1632] total_loss: 2.15330338 d_loss: 1.39967251, g_loss: 0.69897431, ae_loss: 0.05465649\n",
      "Step: [1633] total_loss: 2.13248634 d_loss: 1.39157271, g_loss: 0.68633330, ae_loss: 0.05458031\n",
      "Step: [1634] total_loss: 2.11282182 d_loss: 1.38596547, g_loss: 0.67348713, ae_loss: 0.05336925\n",
      "Step: [1635] total_loss: 2.13104010 d_loss: 1.38812566, g_loss: 0.69499522, ae_loss: 0.04791921\n",
      "Step: [1636] total_loss: 2.13804102 d_loss: 1.40463829, g_loss: 0.68154281, ae_loss: 0.05186000\n",
      "Step: [1637] total_loss: 2.11613655 d_loss: 1.36539364, g_loss: 0.69910491, ae_loss: 0.05163796\n",
      "Step: [1638] total_loss: 2.13960600 d_loss: 1.38770533, g_loss: 0.69836366, ae_loss: 0.05353684\n",
      "Step: [1639] total_loss: 2.15060425 d_loss: 1.39762878, g_loss: 0.69833612, ae_loss: 0.05463932\n",
      "Step: [1640] total_loss: 2.12893319 d_loss: 1.37230301, g_loss: 0.70279121, ae_loss: 0.05383902\n",
      "Step: [1641] total_loss: 2.14895821 d_loss: 1.38526273, g_loss: 0.71152472, ae_loss: 0.05217074\n",
      "Step: [1642] total_loss: 2.10710287 d_loss: 1.37228012, g_loss: 0.68445516, ae_loss: 0.05036751\n",
      "Step: [1643] total_loss: 2.09936047 d_loss: 1.34352612, g_loss: 0.70446730, ae_loss: 0.05136712\n",
      "Step: [1644] total_loss: 2.09883928 d_loss: 1.35884190, g_loss: 0.68308973, ae_loss: 0.05690773\n",
      "Step: [1645] total_loss: 2.11770535 d_loss: 1.39211512, g_loss: 0.67676306, ae_loss: 0.04882714\n",
      "Step: [1646] total_loss: 2.13762021 d_loss: 1.37288356, g_loss: 0.71359330, ae_loss: 0.05114325\n",
      "Step: [1647] total_loss: 2.12230349 d_loss: 1.36415768, g_loss: 0.70251584, ae_loss: 0.05563007\n",
      "Step: [1648] total_loss: 2.12565255 d_loss: 1.37430632, g_loss: 0.70009613, ae_loss: 0.05125007\n",
      "Step: [1649] total_loss: 2.12938929 d_loss: 1.37937856, g_loss: 0.69861090, ae_loss: 0.05139987\n",
      "Step: [1650] total_loss: 2.10854602 d_loss: 1.37643194, g_loss: 0.67820555, ae_loss: 0.05390853\n",
      "Step: [1651] total_loss: 2.11689615 d_loss: 1.37328756, g_loss: 0.68859255, ae_loss: 0.05501588\n",
      "Step: [1652] total_loss: 2.15400362 d_loss: 1.38287401, g_loss: 0.71844637, ae_loss: 0.05268314\n",
      "Step: [1653] total_loss: 2.13365221 d_loss: 1.36533737, g_loss: 0.71204406, ae_loss: 0.05627075\n",
      "Step: [1654] total_loss: 2.11354876 d_loss: 1.36915016, g_loss: 0.69322604, ae_loss: 0.05117247\n",
      "Step: [1655] total_loss: 2.09766865 d_loss: 1.35762739, g_loss: 0.68588352, ae_loss: 0.05415762\n",
      "Step: [1656] total_loss: 2.13468099 d_loss: 1.34713650, g_loss: 0.73316741, ae_loss: 0.05437714\n",
      "Step: [1657] total_loss: 2.12433863 d_loss: 1.40843916, g_loss: 0.66518319, ae_loss: 0.05071636\n",
      "Step: [1658] total_loss: 2.08894539 d_loss: 1.35471416, g_loss: 0.67783296, ae_loss: 0.05639828\n",
      "Step: [1659] total_loss: 2.12182260 d_loss: 1.36154830, g_loss: 0.70587057, ae_loss: 0.05440372\n",
      "Step: [1660] total_loss: 2.11328268 d_loss: 1.37359071, g_loss: 0.68184245, ae_loss: 0.05784939\n",
      "Step: [1661] total_loss: 2.13715792 d_loss: 1.38459587, g_loss: 0.69647670, ae_loss: 0.05608546\n",
      "Step: [1662] total_loss: 2.11892891 d_loss: 1.36973834, g_loss: 0.69586587, ae_loss: 0.05332455\n",
      "Step: [1663] total_loss: 2.12725735 d_loss: 1.37459803, g_loss: 0.69995904, ae_loss: 0.05270042\n",
      "Step: [1664] total_loss: 2.11196065 d_loss: 1.35246587, g_loss: 0.70617747, ae_loss: 0.05331735\n",
      "Step: [1665] total_loss: 2.11881804 d_loss: 1.37016904, g_loss: 0.69714355, ae_loss: 0.05150548\n",
      "Step: [1666] total_loss: 2.13138175 d_loss: 1.40932679, g_loss: 0.66953963, ae_loss: 0.05251523\n",
      "Step: [1667] total_loss: 2.10760307 d_loss: 1.37719607, g_loss: 0.67998421, ae_loss: 0.05042273\n",
      "Step: [1668] total_loss: 2.12973070 d_loss: 1.37927401, g_loss: 0.69442666, ae_loss: 0.05603000\n",
      "Step: [1669] total_loss: 2.13503361 d_loss: 1.38614559, g_loss: 0.69847059, ae_loss: 0.05041744\n",
      "Step: [1670] total_loss: 2.13718557 d_loss: 1.38663077, g_loss: 0.69711852, ae_loss: 0.05343642\n",
      "Step: [1671] total_loss: 2.15404058 d_loss: 1.39672804, g_loss: 0.70579690, ae_loss: 0.05151561\n",
      "Step: [1672] total_loss: 2.16818237 d_loss: 1.39533126, g_loss: 0.72268701, ae_loss: 0.05016394\n",
      "Step: [1673] total_loss: 2.16593575 d_loss: 1.39005959, g_loss: 0.72222048, ae_loss: 0.05365575\n",
      "Step: [1674] total_loss: 2.16244125 d_loss: 1.41988468, g_loss: 0.68892258, ae_loss: 0.05363391\n",
      "Step: [1675] total_loss: 2.13860846 d_loss: 1.38451087, g_loss: 0.69964564, ae_loss: 0.05445180\n",
      "Step: [1676] total_loss: 2.11940908 d_loss: 1.36657333, g_loss: 0.69994140, ae_loss: 0.05289438\n",
      "Step: [1677] total_loss: 2.12376285 d_loss: 1.37498593, g_loss: 0.69722241, ae_loss: 0.05155452\n",
      "Step: [1678] total_loss: 2.12092257 d_loss: 1.36961770, g_loss: 0.69580710, ae_loss: 0.05549765\n",
      "Step: [1679] total_loss: 2.07921839 d_loss: 1.34769142, g_loss: 0.68023884, ae_loss: 0.05128811\n",
      "Step: [1680] total_loss: 2.11103725 d_loss: 1.38242924, g_loss: 0.67908573, ae_loss: 0.04952212\n",
      "Step: [1681] total_loss: 2.12378216 d_loss: 1.39477050, g_loss: 0.67391050, ae_loss: 0.05510112\n",
      "Step: [1682] total_loss: 2.12461472 d_loss: 1.38895869, g_loss: 0.67833221, ae_loss: 0.05732390\n",
      "Step: [1683] total_loss: 2.11338377 d_loss: 1.37535799, g_loss: 0.68283367, ae_loss: 0.05519214\n",
      "Step: [1684] total_loss: 2.13699508 d_loss: 1.38972402, g_loss: 0.69352466, ae_loss: 0.05374645\n",
      "Step: [1685] total_loss: 2.13459206 d_loss: 1.36179948, g_loss: 0.72176397, ae_loss: 0.05102848\n",
      "Step: [1686] total_loss: 2.12857437 d_loss: 1.37299275, g_loss: 0.70241189, ae_loss: 0.05316967\n",
      "Step: [1687] total_loss: 2.12178206 d_loss: 1.36814046, g_loss: 0.70142424, ae_loss: 0.05221741\n",
      "Step: [1688] total_loss: 2.12883472 d_loss: 1.38225603, g_loss: 0.69440758, ae_loss: 0.05217109\n",
      "Step: [1689] total_loss: 2.10091114 d_loss: 1.38603866, g_loss: 0.66360289, ae_loss: 0.05126955\n",
      "Step: [1690] total_loss: 2.11061740 d_loss: 1.38971972, g_loss: 0.66628230, ae_loss: 0.05461532\n",
      "Step: [1691] total_loss: 2.12223005 d_loss: 1.35548699, g_loss: 0.71535385, ae_loss: 0.05138921\n",
      "Step: [1692] total_loss: 2.14085102 d_loss: 1.41018558, g_loss: 0.67390394, ae_loss: 0.05676161\n",
      "Step: [1693] total_loss: 2.14610004 d_loss: 1.38468432, g_loss: 0.70599532, ae_loss: 0.05542046\n",
      "Step: [1694] total_loss: 2.13242412 d_loss: 1.37925684, g_loss: 0.70475590, ae_loss: 0.04841138\n",
      "Step: [1695] total_loss: 2.15401268 d_loss: 1.38528728, g_loss: 0.71902716, ae_loss: 0.04969826\n",
      "Step: [1696] total_loss: 2.14283228 d_loss: 1.38710964, g_loss: 0.70334959, ae_loss: 0.05237311\n",
      "Step: [1697] total_loss: 2.14041471 d_loss: 1.38101530, g_loss: 0.70762408, ae_loss: 0.05177542\n",
      "Step: [1698] total_loss: 2.13754892 d_loss: 1.39468622, g_loss: 0.69210607, ae_loss: 0.05075675\n",
      "Step: [1699] total_loss: 2.11689687 d_loss: 1.36740398, g_loss: 0.70020157, ae_loss: 0.04929123\n",
      "Step: [1700] total_loss: 2.13867378 d_loss: 1.39309669, g_loss: 0.69151205, ae_loss: 0.05406507\n",
      "Step: [1701] total_loss: 2.11624050 d_loss: 1.39694154, g_loss: 0.66967201, ae_loss: 0.04962700\n",
      "Step: [1702] total_loss: 2.10698318 d_loss: 1.36339450, g_loss: 0.69075203, ae_loss: 0.05283665\n",
      "Step: [1703] total_loss: 2.11897707 d_loss: 1.39760637, g_loss: 0.66428053, ae_loss: 0.05709022\n",
      "Step: [1704] total_loss: 2.13435984 d_loss: 1.38698506, g_loss: 0.69473821, ae_loss: 0.05263648\n",
      "Step: [1705] total_loss: 2.13967037 d_loss: 1.36765194, g_loss: 0.72063720, ae_loss: 0.05138123\n",
      "Step: [1706] total_loss: 2.12905121 d_loss: 1.39249659, g_loss: 0.68688500, ae_loss: 0.04966946\n",
      "Step: [1707] total_loss: 2.13241005 d_loss: 1.36352360, g_loss: 0.71802819, ae_loss: 0.05085814\n",
      "Step: [1708] total_loss: 2.16924191 d_loss: 1.40170312, g_loss: 0.71352929, ae_loss: 0.05400947\n",
      "Step: [1709] total_loss: 2.14477706 d_loss: 1.39192021, g_loss: 0.69776464, ae_loss: 0.05509217\n",
      "Step: [1710] total_loss: 2.13021851 d_loss: 1.37494469, g_loss: 0.70280826, ae_loss: 0.05246544\n",
      "Step: [1711] total_loss: 2.12364602 d_loss: 1.37447715, g_loss: 0.69322699, ae_loss: 0.05594177\n",
      "Step: [1712] total_loss: 2.14583206 d_loss: 1.38947797, g_loss: 0.70439970, ae_loss: 0.05195438\n",
      "Step: [1713] total_loss: 2.12940383 d_loss: 1.38941002, g_loss: 0.68694246, ae_loss: 0.05305140\n",
      "Step: [1714] total_loss: 2.12069511 d_loss: 1.39458466, g_loss: 0.67366391, ae_loss: 0.05244666\n",
      "Step: [1715] total_loss: 2.11151552 d_loss: 1.36827946, g_loss: 0.69278437, ae_loss: 0.05045168\n",
      "Step: [1716] total_loss: 2.13091326 d_loss: 1.38478518, g_loss: 0.69481838, ae_loss: 0.05130957\n",
      "Step: [1717] total_loss: 2.13639832 d_loss: 1.37542629, g_loss: 0.70464796, ae_loss: 0.05632418\n",
      "Step: [1718] total_loss: 2.10714340 d_loss: 1.36972642, g_loss: 0.68403637, ae_loss: 0.05338055\n",
      "Step: [1719] total_loss: 2.11166501 d_loss: 1.37933624, g_loss: 0.67713904, ae_loss: 0.05518972\n",
      "Step: [1720] total_loss: 2.10996914 d_loss: 1.36045909, g_loss: 0.69724071, ae_loss: 0.05226947\n",
      "Step: [1721] total_loss: 2.09805012 d_loss: 1.37082458, g_loss: 0.67371511, ae_loss: 0.05351055\n",
      "Step: [1722] total_loss: 2.12535906 d_loss: 1.39001656, g_loss: 0.68394989, ae_loss: 0.05139268\n",
      "Step: [1723] total_loss: 2.13106155 d_loss: 1.37644649, g_loss: 0.70255458, ae_loss: 0.05206054\n",
      "Step: [1724] total_loss: 2.14327002 d_loss: 1.39766324, g_loss: 0.69055688, ae_loss: 0.05504991\n",
      "Step: [1725] total_loss: 2.13211250 d_loss: 1.38005328, g_loss: 0.70338416, ae_loss: 0.04867513\n",
      "Step: [1726] total_loss: 2.11352921 d_loss: 1.38103318, g_loss: 0.68226910, ae_loss: 0.05022690\n",
      "Step: [1727] total_loss: 2.11387110 d_loss: 1.35766196, g_loss: 0.70507336, ae_loss: 0.05113561\n",
      "Step: [1728] total_loss: 2.15183210 d_loss: 1.40555775, g_loss: 0.68845356, ae_loss: 0.05782066\n",
      "Step: [1729] total_loss: 2.14014220 d_loss: 1.38597488, g_loss: 0.70010513, ae_loss: 0.05406225\n",
      "Step: [1730] total_loss: 2.12208366 d_loss: 1.37622201, g_loss: 0.69044352, ae_loss: 0.05541820\n",
      "Step: [1731] total_loss: 2.14954424 d_loss: 1.37515569, g_loss: 0.72054803, ae_loss: 0.05384043\n",
      "Step: [1732] total_loss: 2.13114882 d_loss: 1.38105702, g_loss: 0.69912684, ae_loss: 0.05096484\n",
      "Step: [1733] total_loss: 2.14226365 d_loss: 1.37975919, g_loss: 0.70925832, ae_loss: 0.05324612\n",
      "Step: [1734] total_loss: 2.17132092 d_loss: 1.39160550, g_loss: 0.72865820, ae_loss: 0.05105724\n",
      "Step: [1735] total_loss: 2.11587667 d_loss: 1.37204361, g_loss: 0.69223058, ae_loss: 0.05160243\n",
      "Step: [1736] total_loss: 2.13479710 d_loss: 1.38089132, g_loss: 0.70376432, ae_loss: 0.05014139\n",
      "Step: [1737] total_loss: 2.12866497 d_loss: 1.39617956, g_loss: 0.68193018, ae_loss: 0.05055510\n",
      "Step: [1738] total_loss: 2.13547516 d_loss: 1.35513926, g_loss: 0.72019637, ae_loss: 0.06013969\n",
      "Step: [1739] total_loss: 2.12529421 d_loss: 1.36979842, g_loss: 0.69987118, ae_loss: 0.05562478\n",
      "Step: [1740] total_loss: 2.13085651 d_loss: 1.37648153, g_loss: 0.69961965, ae_loss: 0.05475526\n",
      "Step: [1741] total_loss: 2.14869976 d_loss: 1.39813519, g_loss: 0.69607830, ae_loss: 0.05448640\n",
      "Step: [1742] total_loss: 2.13690066 d_loss: 1.40216851, g_loss: 0.68160212, ae_loss: 0.05313008\n",
      "Step: [1743] total_loss: 2.12211680 d_loss: 1.38446820, g_loss: 0.68968630, ae_loss: 0.04796226\n",
      "Step: [1744] total_loss: 2.13631153 d_loss: 1.41655445, g_loss: 0.66604662, ae_loss: 0.05371037\n",
      "Step: [1745] total_loss: 2.14033318 d_loss: 1.40375781, g_loss: 0.68541992, ae_loss: 0.05115555\n",
      "Step: [1746] total_loss: 2.09433126 d_loss: 1.37662554, g_loss: 0.66867703, ae_loss: 0.04902879\n",
      "Step: [1747] total_loss: 2.13017058 d_loss: 1.39885950, g_loss: 0.68148971, ae_loss: 0.04982132\n",
      "Step: [1748] total_loss: 2.13628936 d_loss: 1.39295053, g_loss: 0.68827307, ae_loss: 0.05506581\n",
      "Step: [1749] total_loss: 2.13971782 d_loss: 1.39975214, g_loss: 0.68648875, ae_loss: 0.05347697\n",
      "Step: [1750] total_loss: 2.12306476 d_loss: 1.37636542, g_loss: 0.69342226, ae_loss: 0.05327712\n",
      "Step: [1751] total_loss: 2.14333320 d_loss: 1.39999461, g_loss: 0.69396693, ae_loss: 0.04937159\n",
      "Step: [1752] total_loss: 2.12466574 d_loss: 1.37858665, g_loss: 0.69379503, ae_loss: 0.05228400\n",
      "Step: [1753] total_loss: 2.13995457 d_loss: 1.38575530, g_loss: 0.70070457, ae_loss: 0.05349458\n",
      "Step: [1754] total_loss: 2.12256289 d_loss: 1.36009884, g_loss: 0.70678318, ae_loss: 0.05568099\n",
      "Step: [1755] total_loss: 2.12370443 d_loss: 1.37094688, g_loss: 0.70358318, ae_loss: 0.04917445\n",
      "Step: [1756] total_loss: 2.11209726 d_loss: 1.37679958, g_loss: 0.68303287, ae_loss: 0.05226476\n",
      "Step: [1757] total_loss: 2.12050509 d_loss: 1.39177763, g_loss: 0.68025881, ae_loss: 0.04846865\n",
      "Step: [1758] total_loss: 2.12077641 d_loss: 1.37542784, g_loss: 0.69302821, ae_loss: 0.05232039\n",
      "Step: [1759] total_loss: 2.14066839 d_loss: 1.39443791, g_loss: 0.69732320, ae_loss: 0.04890739\n",
      "Step: [1760] total_loss: 2.12061691 d_loss: 1.36447418, g_loss: 0.69988644, ae_loss: 0.05625636\n",
      "Step: [1761] total_loss: 2.13171697 d_loss: 1.38739419, g_loss: 0.69011551, ae_loss: 0.05420724\n",
      "Step: [1762] total_loss: 2.14964342 d_loss: 1.41209984, g_loss: 0.68424141, ae_loss: 0.05330232\n",
      "Step: [1763] total_loss: 2.13632178 d_loss: 1.35147214, g_loss: 0.73368710, ae_loss: 0.05116262\n",
      "Step: [1764] total_loss: 2.12558222 d_loss: 1.37497759, g_loss: 0.69466549, ae_loss: 0.05593916\n",
      "Step: [1765] total_loss: 2.11731768 d_loss: 1.37289262, g_loss: 0.69087553, ae_loss: 0.05354950\n",
      "Step: [1766] total_loss: 2.11170244 d_loss: 1.36448514, g_loss: 0.69652891, ae_loss: 0.05068830\n",
      "Step: [1767] total_loss: 2.11897588 d_loss: 1.38006115, g_loss: 0.68605840, ae_loss: 0.05285638\n",
      "Step: [1768] total_loss: 2.11449337 d_loss: 1.37066197, g_loss: 0.69270992, ae_loss: 0.05112152\n",
      "Step: [1769] total_loss: 2.12363863 d_loss: 1.39530897, g_loss: 0.67577583, ae_loss: 0.05255373\n",
      "Step: [1770] total_loss: 2.13196898 d_loss: 1.37660241, g_loss: 0.70012963, ae_loss: 0.05523685\n",
      "Step: [1771] total_loss: 2.11285543 d_loss: 1.37236357, g_loss: 0.68981475, ae_loss: 0.05067719\n",
      "Step: [1772] total_loss: 2.12693548 d_loss: 1.37957549, g_loss: 0.69510454, ae_loss: 0.05225555\n",
      "Step: [1773] total_loss: 2.09568334 d_loss: 1.36056256, g_loss: 0.68786025, ae_loss: 0.04726050\n",
      "Step: [1774] total_loss: 2.13771057 d_loss: 1.39430070, g_loss: 0.69261718, ae_loss: 0.05079254\n",
      "Step: [1775] total_loss: 2.12856746 d_loss: 1.36994338, g_loss: 0.70445746, ae_loss: 0.05416654\n",
      "Step: [1776] total_loss: 2.14385891 d_loss: 1.41107225, g_loss: 0.67830145, ae_loss: 0.05448508\n",
      "Step: [1777] total_loss: 2.13265657 d_loss: 1.38409090, g_loss: 0.69337165, ae_loss: 0.05519393\n",
      "Step: [1778] total_loss: 2.13708448 d_loss: 1.38501132, g_loss: 0.69938552, ae_loss: 0.05268747\n",
      "Step: [1779] total_loss: 2.11818433 d_loss: 1.37615180, g_loss: 0.68997729, ae_loss: 0.05205527\n",
      "Step: [1780] total_loss: 2.12129498 d_loss: 1.36377954, g_loss: 0.70532161, ae_loss: 0.05219387\n",
      "Step: [1781] total_loss: 2.14991117 d_loss: 1.42872238, g_loss: 0.66507977, ae_loss: 0.05610891\n",
      "Step: [1782] total_loss: 2.13042712 d_loss: 1.37949395, g_loss: 0.69804662, ae_loss: 0.05288650\n",
      "Step: [1783] total_loss: 2.12565184 d_loss: 1.37948632, g_loss: 0.69458628, ae_loss: 0.05157907\n",
      "Step: [1784] total_loss: 2.13885617 d_loss: 1.39806485, g_loss: 0.69220054, ae_loss: 0.04859074\n",
      "Step: [1785] total_loss: 2.12790990 d_loss: 1.38742173, g_loss: 0.68794632, ae_loss: 0.05254182\n",
      "Step: [1786] total_loss: 2.12316561 d_loss: 1.37091398, g_loss: 0.69958901, ae_loss: 0.05266257\n",
      "Step: [1787] total_loss: 2.14054251 d_loss: 1.38572788, g_loss: 0.70288211, ae_loss: 0.05193252\n",
      "Step: [1788] total_loss: 2.13380218 d_loss: 1.38331187, g_loss: 0.70328879, ae_loss: 0.04720149\n",
      "Step: [1789] total_loss: 2.14323425 d_loss: 1.38503408, g_loss: 0.70720732, ae_loss: 0.05099294\n",
      "Step: [1790] total_loss: 2.12450600 d_loss: 1.38884950, g_loss: 0.68046367, ae_loss: 0.05519272\n",
      "Step: [1791] total_loss: 2.11945987 d_loss: 1.40420461, g_loss: 0.66315365, ae_loss: 0.05210160\n",
      "Step: [1792] total_loss: 2.10277271 d_loss: 1.37096167, g_loss: 0.68250901, ae_loss: 0.04930199\n",
      "Step: [1793] total_loss: 2.13573956 d_loss: 1.39591789, g_loss: 0.68479466, ae_loss: 0.05502702\n",
      "Step: [1794] total_loss: 2.13482714 d_loss: 1.39074302, g_loss: 0.69004965, ae_loss: 0.05403458\n",
      "Step: [1795] total_loss: 2.12542486 d_loss: 1.36713767, g_loss: 0.70706141, ae_loss: 0.05122572\n",
      "Step: [1796] total_loss: 2.13488436 d_loss: 1.37827110, g_loss: 0.70444238, ae_loss: 0.05217081\n",
      "Step: [1797] total_loss: 2.14897370 d_loss: 1.39210451, g_loss: 0.70253342, ae_loss: 0.05433570\n",
      "Step: [1798] total_loss: 2.13878155 d_loss: 1.39598131, g_loss: 0.68969584, ae_loss: 0.05310431\n",
      "Step: [1799] total_loss: 2.13407922 d_loss: 1.38964438, g_loss: 0.69358057, ae_loss: 0.05085419\n",
      "Step: [1800] total_loss: 2.14959145 d_loss: 1.39429879, g_loss: 0.69886261, ae_loss: 0.05643013\n",
      "Step: [1801] total_loss: 2.12790298 d_loss: 1.38041914, g_loss: 0.69474620, ae_loss: 0.05273768\n",
      "Step: [1802] total_loss: 2.14192915 d_loss: 1.39765322, g_loss: 0.69478238, ae_loss: 0.04949349\n",
      "Step: [1803] total_loss: 2.14687014 d_loss: 1.39811826, g_loss: 0.69187331, ae_loss: 0.05687874\n",
      "Step: [1804] total_loss: 2.12882662 d_loss: 1.38357818, g_loss: 0.69529366, ae_loss: 0.04995485\n",
      "Step: [1805] total_loss: 2.11621761 d_loss: 1.36250830, g_loss: 0.70233387, ae_loss: 0.05137540\n",
      "Step: [1806] total_loss: 2.14423084 d_loss: 1.36446691, g_loss: 0.72475785, ae_loss: 0.05500608\n",
      "Step: [1807] total_loss: 2.13967180 d_loss: 1.37917042, g_loss: 0.70727170, ae_loss: 0.05322968\n",
      "Step: [1808] total_loss: 2.10077739 d_loss: 1.36946189, g_loss: 0.68019122, ae_loss: 0.05112419\n",
      "Step: [1809] total_loss: 2.12791014 d_loss: 1.38003135, g_loss: 0.69450903, ae_loss: 0.05336968\n",
      "Step: [1810] total_loss: 2.10507584 d_loss: 1.35817075, g_loss: 0.69698536, ae_loss: 0.04991984\n",
      "Step: [1811] total_loss: 2.11620808 d_loss: 1.38865185, g_loss: 0.67675078, ae_loss: 0.05080533\n",
      "Step: [1812] total_loss: 2.13693190 d_loss: 1.38320196, g_loss: 0.69806254, ae_loss: 0.05566741\n",
      "Step: [1813] total_loss: 2.14164114 d_loss: 1.38415360, g_loss: 0.70247298, ae_loss: 0.05501461\n",
      "Step: [1814] total_loss: 2.13471556 d_loss: 1.37847853, g_loss: 0.70396698, ae_loss: 0.05227008\n",
      "Step: [1815] total_loss: 2.13739491 d_loss: 1.38196945, g_loss: 0.70035189, ae_loss: 0.05507356\n",
      "Step: [1816] total_loss: 2.14393520 d_loss: 1.40565896, g_loss: 0.68663216, ae_loss: 0.05164407\n",
      "Step: [1817] total_loss: 2.12663126 d_loss: 1.37631166, g_loss: 0.69525969, ae_loss: 0.05506007\n",
      "Step: [1818] total_loss: 2.14735985 d_loss: 1.39590561, g_loss: 0.69917387, ae_loss: 0.05228028\n",
      "Step: [1819] total_loss: 2.12583637 d_loss: 1.37388778, g_loss: 0.69817084, ae_loss: 0.05377772\n",
      "Step: [1820] total_loss: 2.12172651 d_loss: 1.39830327, g_loss: 0.67194259, ae_loss: 0.05148068\n",
      "Step: [1821] total_loss: 2.12173223 d_loss: 1.38757992, g_loss: 0.68000662, ae_loss: 0.05414584\n",
      "Step: [1822] total_loss: 2.12273097 d_loss: 1.38267398, g_loss: 0.68457133, ae_loss: 0.05548564\n",
      "Step: [1823] total_loss: 2.13812661 d_loss: 1.39482069, g_loss: 0.69396287, ae_loss: 0.04934314\n",
      "Step: [1824] total_loss: 2.12892747 d_loss: 1.36432481, g_loss: 0.71324670, ae_loss: 0.05135597\n",
      "Step: [1825] total_loss: 2.13356447 d_loss: 1.38210738, g_loss: 0.70092458, ae_loss: 0.05053263\n",
      "Step: [1826] total_loss: 2.11956120 d_loss: 1.38566542, g_loss: 0.68423843, ae_loss: 0.04965749\n",
      "Step: [1827] total_loss: 2.13739324 d_loss: 1.38413143, g_loss: 0.70470023, ae_loss: 0.04856158\n",
      "Step: [1828] total_loss: 2.13514304 d_loss: 1.38241446, g_loss: 0.69938910, ae_loss: 0.05333950\n",
      "Step: [1829] total_loss: 2.13441682 d_loss: 1.38034868, g_loss: 0.70264715, ae_loss: 0.05142098\n",
      "Step: [1830] total_loss: 2.11461878 d_loss: 1.36058903, g_loss: 0.70434809, ae_loss: 0.04968176\n",
      "Step: [1831] total_loss: 2.11510110 d_loss: 1.38794756, g_loss: 0.67384613, ae_loss: 0.05330738\n",
      "Step: [1832] total_loss: 2.11499238 d_loss: 1.38086689, g_loss: 0.68212223, ae_loss: 0.05200326\n",
      "Step: [1833] total_loss: 2.13328695 d_loss: 1.40191460, g_loss: 0.68110710, ae_loss: 0.05026516\n",
      "Step: [1834] total_loss: 2.11835861 d_loss: 1.37061369, g_loss: 0.69505507, ae_loss: 0.05268976\n",
      "Step: [1835] total_loss: 2.14341831 d_loss: 1.37920249, g_loss: 0.70932388, ae_loss: 0.05489203\n",
      "Step: [1836] total_loss: 2.16367912 d_loss: 1.39585650, g_loss: 0.71558315, ae_loss: 0.05223947\n",
      "Step: [1837] total_loss: 2.14824057 d_loss: 1.39886940, g_loss: 0.69661319, ae_loss: 0.05275795\n",
      "Step: [1838] total_loss: 2.11738348 d_loss: 1.36675644, g_loss: 0.69763815, ae_loss: 0.05298894\n",
      "Step: [1839] total_loss: 2.14701724 d_loss: 1.39662814, g_loss: 0.70169556, ae_loss: 0.04869355\n",
      "Step: [1840] total_loss: 2.12810993 d_loss: 1.37334180, g_loss: 0.70312113, ae_loss: 0.05164702\n",
      "Step: [1841] total_loss: 2.13312149 d_loss: 1.36856031, g_loss: 0.71292782, ae_loss: 0.05163319\n",
      "Step: [1842] total_loss: 2.12350845 d_loss: 1.38738132, g_loss: 0.68460131, ae_loss: 0.05152596\n",
      "Step: [1843] total_loss: 2.09738016 d_loss: 1.37180662, g_loss: 0.67235780, ae_loss: 0.05321590\n",
      "Step: [1844] total_loss: 2.12723732 d_loss: 1.40770996, g_loss: 0.66885936, ae_loss: 0.05066800\n",
      "Step: [1845] total_loss: 2.11689186 d_loss: 1.36465383, g_loss: 0.70110261, ae_loss: 0.05113556\n",
      "Step: [1846] total_loss: 2.11096263 d_loss: 1.37870383, g_loss: 0.68054157, ae_loss: 0.05171728\n",
      "Step: [1847] total_loss: 2.11214161 d_loss: 1.37898898, g_loss: 0.68311226, ae_loss: 0.05004046\n",
      "Step: [1848] total_loss: 2.14527774 d_loss: 1.38857365, g_loss: 0.70569414, ae_loss: 0.05101002\n",
      "Step: [1849] total_loss: 2.14553142 d_loss: 1.37567699, g_loss: 0.71789438, ae_loss: 0.05195998\n",
      "Step: [1850] total_loss: 2.12406254 d_loss: 1.35407782, g_loss: 0.71658212, ae_loss: 0.05340258\n",
      "Step: [1851] total_loss: 2.12856746 d_loss: 1.39911962, g_loss: 0.67637509, ae_loss: 0.05307278\n",
      "Step: [1852] total_loss: 2.12547302 d_loss: 1.39134526, g_loss: 0.68303615, ae_loss: 0.05109164\n",
      "Step: [1853] total_loss: 2.10085917 d_loss: 1.38648391, g_loss: 0.66333210, ae_loss: 0.05104311\n",
      "Step: [1854] total_loss: 2.11302757 d_loss: 1.37553716, g_loss: 0.68222785, ae_loss: 0.05526270\n",
      "Step: [1855] total_loss: 2.13324499 d_loss: 1.39276206, g_loss: 0.69062412, ae_loss: 0.04985891\n",
      "Step: [1856] total_loss: 2.13158417 d_loss: 1.37579095, g_loss: 0.70547485, ae_loss: 0.05031828\n",
      "Step: [1857] total_loss: 2.09934711 d_loss: 1.34743166, g_loss: 0.69861984, ae_loss: 0.05329569\n",
      "Step: [1858] total_loss: 2.10053968 d_loss: 1.36304998, g_loss: 0.68159676, ae_loss: 0.05589300\n",
      "Step: [1859] total_loss: 2.13790774 d_loss: 1.39559305, g_loss: 0.69198984, ae_loss: 0.05032479\n",
      "Step: [1860] total_loss: 2.11661220 d_loss: 1.35378277, g_loss: 0.71218818, ae_loss: 0.05064115\n",
      "Step: [1861] total_loss: 2.11544943 d_loss: 1.37783122, g_loss: 0.68413621, ae_loss: 0.05348206\n",
      "Step: [1862] total_loss: 2.15356874 d_loss: 1.38997841, g_loss: 0.70886230, ae_loss: 0.05472810\n",
      "Step: [1863] total_loss: 2.13171291 d_loss: 1.40002120, g_loss: 0.67440116, ae_loss: 0.05729057\n",
      "Step: [1864] total_loss: 2.15419006 d_loss: 1.39036250, g_loss: 0.71156645, ae_loss: 0.05226112\n",
      "Step: [1865] total_loss: 2.12440467 d_loss: 1.37483454, g_loss: 0.69957662, ae_loss: 0.04999350\n",
      "Step: [1866] total_loss: 2.13770580 d_loss: 1.39161849, g_loss: 0.69489688, ae_loss: 0.05119053\n",
      "Step: [1867] total_loss: 2.11942911 d_loss: 1.38020563, g_loss: 0.68509328, ae_loss: 0.05413030\n",
      "Step: [1868] total_loss: 2.13308167 d_loss: 1.37541890, g_loss: 0.70516229, ae_loss: 0.05250051\n",
      "Step: [1869] total_loss: 2.11239743 d_loss: 1.37414086, g_loss: 0.68758643, ae_loss: 0.05067010\n",
      "Step: [1870] total_loss: 2.11759067 d_loss: 1.36089003, g_loss: 0.70730293, ae_loss: 0.04939772\n",
      "Step: [1871] total_loss: 2.09992838 d_loss: 1.35697794, g_loss: 0.69152772, ae_loss: 0.05142258\n",
      "Step: [1872] total_loss: 2.12217140 d_loss: 1.38268995, g_loss: 0.68312192, ae_loss: 0.05635951\n",
      "Step: [1873] total_loss: 2.11214018 d_loss: 1.37727666, g_loss: 0.68286753, ae_loss: 0.05199586\n",
      "Step: [1874] total_loss: 2.10702848 d_loss: 1.37775660, g_loss: 0.67730975, ae_loss: 0.05196211\n",
      "Step: [1875] total_loss: 2.09874415 d_loss: 1.37390554, g_loss: 0.67340183, ae_loss: 0.05143680\n",
      "Step: [1876] total_loss: 2.14444232 d_loss: 1.41235471, g_loss: 0.67841774, ae_loss: 0.05366983\n",
      "Step: [1877] total_loss: 2.13514566 d_loss: 1.38197374, g_loss: 0.70120859, ae_loss: 0.05196324\n",
      "Step: [1878] total_loss: 2.12295771 d_loss: 1.35994482, g_loss: 0.70651287, ae_loss: 0.05649999\n",
      "Step: [1879] total_loss: 2.12671065 d_loss: 1.36956406, g_loss: 0.70483679, ae_loss: 0.05230976\n",
      "Step: [1880] total_loss: 2.14068604 d_loss: 1.38806522, g_loss: 0.69708949, ae_loss: 0.05553136\n",
      "Step: [1881] total_loss: 2.12882566 d_loss: 1.36661935, g_loss: 0.70782280, ae_loss: 0.05438359\n",
      "Step: [1882] total_loss: 2.11576605 d_loss: 1.36747897, g_loss: 0.69736093, ae_loss: 0.05092622\n",
      "Step: [1883] total_loss: 2.11407089 d_loss: 1.37791252, g_loss: 0.68345791, ae_loss: 0.05270047\n",
      "Step: [1884] total_loss: 2.13351130 d_loss: 1.37737012, g_loss: 0.70329845, ae_loss: 0.05284274\n",
      "Step: [1885] total_loss: 2.12617135 d_loss: 1.39389014, g_loss: 0.68148977, ae_loss: 0.05079133\n",
      "Step: [1886] total_loss: 2.12336850 d_loss: 1.37894988, g_loss: 0.68883377, ae_loss: 0.05558490\n",
      "Step: [1887] total_loss: 2.11601043 d_loss: 1.37522519, g_loss: 0.68653214, ae_loss: 0.05425312\n",
      "Step: [1888] total_loss: 2.14518380 d_loss: 1.39188910, g_loss: 0.70002252, ae_loss: 0.05327209\n",
      "Step: [1889] total_loss: 2.12896013 d_loss: 1.38671672, g_loss: 0.69311136, ae_loss: 0.04913193\n",
      "Step: [1890] total_loss: 2.14429235 d_loss: 1.40391612, g_loss: 0.68540120, ae_loss: 0.05497486\n",
      "Step: [1891] total_loss: 2.14892054 d_loss: 1.39777410, g_loss: 0.69454777, ae_loss: 0.05659866\n",
      "Step: [1892] total_loss: 2.14593029 d_loss: 1.40749121, g_loss: 0.68520653, ae_loss: 0.05323238\n",
      "Step: [1893] total_loss: 2.12309480 d_loss: 1.38686407, g_loss: 0.68340302, ae_loss: 0.05282773\n",
      "Step: [1894] total_loss: 2.15222478 d_loss: 1.38505161, g_loss: 0.71220189, ae_loss: 0.05497131\n",
      "Step: [1895] total_loss: 2.14822054 d_loss: 1.39012718, g_loss: 0.70437843, ae_loss: 0.05371490\n",
      "Step: [1896] total_loss: 2.12193274 d_loss: 1.36523271, g_loss: 0.70285356, ae_loss: 0.05384653\n",
      "Step: [1897] total_loss: 2.12194419 d_loss: 1.38351846, g_loss: 0.68700165, ae_loss: 0.05142401\n",
      "Step: [1898] total_loss: 2.12749529 d_loss: 1.39443445, g_loss: 0.68137646, ae_loss: 0.05168433\n",
      "Step: [1899] total_loss: 2.09406972 d_loss: 1.36751330, g_loss: 0.67763275, ae_loss: 0.04892359\n",
      "Step: [1900] total_loss: 2.13229036 d_loss: 1.38454914, g_loss: 0.69695377, ae_loss: 0.05078742\n",
      "Step: [1901] total_loss: 2.11916327 d_loss: 1.37930107, g_loss: 0.68720156, ae_loss: 0.05266056\n",
      "Step: [1902] total_loss: 2.13034916 d_loss: 1.38594317, g_loss: 0.69369233, ae_loss: 0.05071371\n",
      "Step: [1903] total_loss: 2.13513923 d_loss: 1.39975452, g_loss: 0.68669105, ae_loss: 0.04869367\n",
      "Step: [1904] total_loss: 2.12270498 d_loss: 1.37244725, g_loss: 0.70439750, ae_loss: 0.04586016\n",
      "Step: [1905] total_loss: 2.13122654 d_loss: 1.37600660, g_loss: 0.70391339, ae_loss: 0.05130658\n",
      "Step: [1906] total_loss: 2.15902686 d_loss: 1.39332557, g_loss: 0.71171772, ae_loss: 0.05398358\n",
      "Step: [1907] total_loss: 2.12314224 d_loss: 1.37351537, g_loss: 0.69962919, ae_loss: 0.04999761\n",
      "Step: [1908] total_loss: 2.14720392 d_loss: 1.37465692, g_loss: 0.72091949, ae_loss: 0.05162738\n",
      "Step: [1909] total_loss: 2.11448812 d_loss: 1.36985838, g_loss: 0.69257522, ae_loss: 0.05205439\n",
      "Step: [1910] total_loss: 2.13216114 d_loss: 1.39480090, g_loss: 0.68918687, ae_loss: 0.04817330\n",
      "Step: [1911] total_loss: 2.13287497 d_loss: 1.37584543, g_loss: 0.70398235, ae_loss: 0.05304717\n",
      "Step: [1912] total_loss: 2.11622667 d_loss: 1.39640236, g_loss: 0.66691494, ae_loss: 0.05290948\n",
      "Step: [1913] total_loss: 2.11825418 d_loss: 1.39568830, g_loss: 0.67154038, ae_loss: 0.05102566\n",
      "Step: [1914] total_loss: 2.13882160 d_loss: 1.37646329, g_loss: 0.71337163, ae_loss: 0.04898682\n",
      "Step: [1915] total_loss: 2.11728621 d_loss: 1.38208032, g_loss: 0.68324924, ae_loss: 0.05195674\n",
      "Step: [1916] total_loss: 2.13283920 d_loss: 1.40086544, g_loss: 0.67786014, ae_loss: 0.05411350\n",
      "Step: [1917] total_loss: 2.10432053 d_loss: 1.35098279, g_loss: 0.70390862, ae_loss: 0.04942918\n",
      "Step: [1918] total_loss: 2.13758993 d_loss: 1.39276981, g_loss: 0.69576955, ae_loss: 0.04905070\n",
      "Step: [1919] total_loss: 2.12600660 d_loss: 1.35290384, g_loss: 0.72232395, ae_loss: 0.05077869\n",
      "Step: [1920] total_loss: 2.12435317 d_loss: 1.37712121, g_loss: 0.69828051, ae_loss: 0.04895156\n",
      "Step: [1921] total_loss: 2.13701487 d_loss: 1.38293064, g_loss: 0.70081222, ae_loss: 0.05327212\n",
      "Step: [1922] total_loss: 2.14208770 d_loss: 1.39403510, g_loss: 0.69314957, ae_loss: 0.05490300\n",
      "Step: [1923] total_loss: 2.17739558 d_loss: 1.39484954, g_loss: 0.72709656, ae_loss: 0.05544952\n",
      "Step: [1924] total_loss: 2.15819073 d_loss: 1.41276908, g_loss: 0.69400942, ae_loss: 0.05141225\n",
      "Step: [1925] total_loss: 2.12677097 d_loss: 1.38084102, g_loss: 0.69559395, ae_loss: 0.05033617\n",
      "Step: [1926] total_loss: 2.13238764 d_loss: 1.35900664, g_loss: 0.72077930, ae_loss: 0.05260163\n",
      "Step: [1927] total_loss: 2.12604904 d_loss: 1.39189816, g_loss: 0.68353099, ae_loss: 0.05061990\n",
      "Step: [1928] total_loss: 2.14112234 d_loss: 1.39190555, g_loss: 0.69934684, ae_loss: 0.04986987\n",
      "Step: [1929] total_loss: 2.11778021 d_loss: 1.38708079, g_loss: 0.67716992, ae_loss: 0.05352947\n",
      "Step: [1930] total_loss: 2.11269069 d_loss: 1.39066374, g_loss: 0.66573173, ae_loss: 0.05629518\n",
      "Step: [1931] total_loss: 2.12893796 d_loss: 1.38317585, g_loss: 0.69267023, ae_loss: 0.05309183\n",
      "Step: [1932] total_loss: 2.11677217 d_loss: 1.36734974, g_loss: 0.69675267, ae_loss: 0.05266963\n",
      "Step: [1933] total_loss: 2.11633444 d_loss: 1.36977625, g_loss: 0.69417864, ae_loss: 0.05237949\n",
      "Step: [1934] total_loss: 2.13327885 d_loss: 1.37984872, g_loss: 0.69947076, ae_loss: 0.05395953\n",
      "Step: [1935] total_loss: 2.13081074 d_loss: 1.38446534, g_loss: 0.69113916, ae_loss: 0.05520629\n",
      "Step: [1936] total_loss: 2.13165784 d_loss: 1.40095520, g_loss: 0.67555946, ae_loss: 0.05514323\n",
      "Step: [1937] total_loss: 2.10494328 d_loss: 1.34071898, g_loss: 0.71336615, ae_loss: 0.05085821\n",
      "Step: [1938] total_loss: 2.12444115 d_loss: 1.37951684, g_loss: 0.69588292, ae_loss: 0.04904121\n",
      "Step: [1939] total_loss: 2.12181687 d_loss: 1.39298129, g_loss: 0.67894650, ae_loss: 0.04988906\n",
      "Step: [1940] total_loss: 2.11914921 d_loss: 1.38136578, g_loss: 0.68851936, ae_loss: 0.04926397\n",
      "Step: [1941] total_loss: 2.13112783 d_loss: 1.39482045, g_loss: 0.68324912, ae_loss: 0.05305835\n",
      "Step: [1942] total_loss: 2.13887429 d_loss: 1.38977885, g_loss: 0.69635063, ae_loss: 0.05274472\n",
      "Step: [1943] total_loss: 2.16335177 d_loss: 1.41360021, g_loss: 0.69444585, ae_loss: 0.05530567\n",
      "Step: [1944] total_loss: 2.13651943 d_loss: 1.36334419, g_loss: 0.71560085, ae_loss: 0.05757430\n",
      "Step: [1945] total_loss: 2.12751937 d_loss: 1.36674833, g_loss: 0.70308769, ae_loss: 0.05768333\n",
      "Step: [1946] total_loss: 2.12516260 d_loss: 1.37804091, g_loss: 0.69671309, ae_loss: 0.05040875\n",
      "Step: [1947] total_loss: 2.12949324 d_loss: 1.39368939, g_loss: 0.68419129, ae_loss: 0.05161259\n",
      "Step: [1948] total_loss: 2.11811662 d_loss: 1.39068866, g_loss: 0.67760456, ae_loss: 0.04982335\n",
      "Step: [1949] total_loss: 2.12328386 d_loss: 1.38809848, g_loss: 0.67807913, ae_loss: 0.05710639\n",
      "Step: [1950] total_loss: 2.10279727 d_loss: 1.36977863, g_loss: 0.67986822, ae_loss: 0.05315045\n",
      "Step: [1951] total_loss: 2.11761332 d_loss: 1.36839914, g_loss: 0.69457757, ae_loss: 0.05463650\n",
      "Step: [1952] total_loss: 2.14167476 d_loss: 1.38732517, g_loss: 0.69953722, ae_loss: 0.05481242\n",
      "Step: [1953] total_loss: 2.12964487 d_loss: 1.38726032, g_loss: 0.68332744, ae_loss: 0.05905718\n",
      "Step: [1954] total_loss: 2.13323975 d_loss: 1.37404943, g_loss: 0.70196503, ae_loss: 0.05722529\n",
      "Step: [1955] total_loss: 2.14455032 d_loss: 1.39236379, g_loss: 0.69622076, ae_loss: 0.05596564\n",
      "Step: [1956] total_loss: 2.13404512 d_loss: 1.38175750, g_loss: 0.69856012, ae_loss: 0.05372755\n",
      "Step: [1957] total_loss: 2.11061192 d_loss: 1.36607158, g_loss: 0.69170254, ae_loss: 0.05283777\n",
      "Step: [1958] total_loss: 2.13697505 d_loss: 1.38710046, g_loss: 0.69446981, ae_loss: 0.05540480\n",
      "Step: [1959] total_loss: 2.13310957 d_loss: 1.37707806, g_loss: 0.69863379, ae_loss: 0.05739770\n",
      "Step: [1960] total_loss: 2.11362815 d_loss: 1.36327541, g_loss: 0.70114022, ae_loss: 0.04921252\n",
      "Step: [1961] total_loss: 2.14450240 d_loss: 1.39119244, g_loss: 0.70068544, ae_loss: 0.05262462\n",
      "Step: [1962] total_loss: 2.13454747 d_loss: 1.38127446, g_loss: 0.70093256, ae_loss: 0.05234037\n",
      "Step: [1963] total_loss: 2.14215803 d_loss: 1.39303517, g_loss: 0.69720387, ae_loss: 0.05191904\n",
      "Step: [1964] total_loss: 2.11954379 d_loss: 1.38470173, g_loss: 0.68748951, ae_loss: 0.04735254\n",
      "Step: [1965] total_loss: 2.13980961 d_loss: 1.36659086, g_loss: 0.71413189, ae_loss: 0.05908682\n",
      "Step: [1966] total_loss: 2.13883996 d_loss: 1.38543057, g_loss: 0.70012438, ae_loss: 0.05328501\n",
      "Step: [1967] total_loss: 2.12952209 d_loss: 1.36008286, g_loss: 0.71401799, ae_loss: 0.05542119\n",
      "Step: [1968] total_loss: 2.12920046 d_loss: 1.36060941, g_loss: 0.71908915, ae_loss: 0.04950186\n",
      "Step: [1969] total_loss: 2.14145756 d_loss: 1.39430022, g_loss: 0.69581324, ae_loss: 0.05134420\n",
      "Step: [1970] total_loss: 2.12858415 d_loss: 1.38221836, g_loss: 0.69509649, ae_loss: 0.05126930\n",
      "Step: [1971] total_loss: 2.12432671 d_loss: 1.38184381, g_loss: 0.69153798, ae_loss: 0.05094483\n",
      "Step: [1972] total_loss: 2.13304496 d_loss: 1.38352716, g_loss: 0.69751471, ae_loss: 0.05200306\n",
      "Step: [1973] total_loss: 2.13680172 d_loss: 1.37286115, g_loss: 0.70736635, ae_loss: 0.05657423\n",
      "Step: [1974] total_loss: 2.13499546 d_loss: 1.40276349, g_loss: 0.67955279, ae_loss: 0.05267916\n",
      "Step: [1975] total_loss: 2.10708570 d_loss: 1.36673927, g_loss: 0.68922055, ae_loss: 0.05112599\n",
      "Step: [1976] total_loss: 2.12851810 d_loss: 1.40056300, g_loss: 0.67775124, ae_loss: 0.05020386\n",
      "Step: [1977] total_loss: 2.13654995 d_loss: 1.39293897, g_loss: 0.68923676, ae_loss: 0.05437412\n",
      "Step: [1978] total_loss: 2.14683247 d_loss: 1.41948104, g_loss: 0.67539322, ae_loss: 0.05195810\n",
      "Step: [1979] total_loss: 2.13703680 d_loss: 1.39104104, g_loss: 0.69307280, ae_loss: 0.05292286\n",
      "Step: [1980] total_loss: 2.13196898 d_loss: 1.36514544, g_loss: 0.71256733, ae_loss: 0.05425629\n",
      "Step: [1981] total_loss: 2.13726902 d_loss: 1.37181413, g_loss: 0.71000898, ae_loss: 0.05544575\n",
      "Step: [1982] total_loss: 2.11881351 d_loss: 1.36672831, g_loss: 0.69625258, ae_loss: 0.05583274\n",
      "Step: [1983] total_loss: 2.12990618 d_loss: 1.38407159, g_loss: 0.69183075, ae_loss: 0.05400370\n",
      "Step: [1984] total_loss: 2.12772083 d_loss: 1.38744187, g_loss: 0.68543953, ae_loss: 0.05483944\n",
      "Step: [1985] total_loss: 2.12436247 d_loss: 1.40139651, g_loss: 0.67344868, ae_loss: 0.04951710\n",
      "Step: [1986] total_loss: 2.12353420 d_loss: 1.38802528, g_loss: 0.67858356, ae_loss: 0.05692529\n",
      "Step: [1987] total_loss: 2.11201239 d_loss: 1.36848402, g_loss: 0.69153249, ae_loss: 0.05199571\n",
      "Step: [1988] total_loss: 2.11429262 d_loss: 1.37645483, g_loss: 0.68535239, ae_loss: 0.05248550\n",
      "Step: [1989] total_loss: 2.11803818 d_loss: 1.39740968, g_loss: 0.66907251, ae_loss: 0.05155606\n",
      "Step: [1990] total_loss: 2.14445972 d_loss: 1.38941240, g_loss: 0.70026410, ae_loss: 0.05478305\n",
      "Step: [1991] total_loss: 2.12517738 d_loss: 1.37423527, g_loss: 0.70400918, ae_loss: 0.04693285\n",
      "Step: [1992] total_loss: 2.12941551 d_loss: 1.37314725, g_loss: 0.70756233, ae_loss: 0.04870608\n",
      "Step: [1993] total_loss: 2.13426924 d_loss: 1.39397895, g_loss: 0.68536609, ae_loss: 0.05492407\n",
      "Step: [1994] total_loss: 2.13891482 d_loss: 1.39452577, g_loss: 0.68973517, ae_loss: 0.05465392\n",
      "Step: [1995] total_loss: 2.13932800 d_loss: 1.37343383, g_loss: 0.71121860, ae_loss: 0.05467561\n",
      "Step: [1996] total_loss: 2.11942625 d_loss: 1.36201680, g_loss: 0.71026361, ae_loss: 0.04714570\n",
      "Step: [1997] total_loss: 2.15027094 d_loss: 1.39870906, g_loss: 0.69784957, ae_loss: 0.05371221\n",
      "Step: [1998] total_loss: 2.13878536 d_loss: 1.37040591, g_loss: 0.71514034, ae_loss: 0.05323901\n",
      "Step: [1999] total_loss: 2.10666919 d_loss: 1.36581564, g_loss: 0.68631828, ae_loss: 0.05453528\n",
      "Step: [2000] total_loss: 2.11837626 d_loss: 1.39255786, g_loss: 0.67736155, ae_loss: 0.04845688\n",
      "Step: [2001] total_loss: 2.10715055 d_loss: 1.37009001, g_loss: 0.68486297, ae_loss: 0.05219743\n",
      "Step: [2002] total_loss: 2.13216400 d_loss: 1.38273060, g_loss: 0.69451559, ae_loss: 0.05491793\n",
      "Step: [2003] total_loss: 2.14080000 d_loss: 1.37329185, g_loss: 0.71571791, ae_loss: 0.05179035\n",
      "Step: [2004] total_loss: 2.12272859 d_loss: 1.37923491, g_loss: 0.68823051, ae_loss: 0.05526321\n",
      "Step: [2005] total_loss: 2.12858415 d_loss: 1.37698603, g_loss: 0.69964862, ae_loss: 0.05194950\n",
      "Step: [2006] total_loss: 2.11161232 d_loss: 1.37683284, g_loss: 0.68342865, ae_loss: 0.05135098\n",
      "Step: [2007] total_loss: 2.13464499 d_loss: 1.39964318, g_loss: 0.68447435, ae_loss: 0.05052751\n",
      "Step: [2008] total_loss: 2.11874270 d_loss: 1.37697554, g_loss: 0.68629581, ae_loss: 0.05547133\n",
      "Step: [2009] total_loss: 2.13239002 d_loss: 1.39624214, g_loss: 0.68551183, ae_loss: 0.05063596\n",
      "Step: [2010] total_loss: 2.12246060 d_loss: 1.38456964, g_loss: 0.68745977, ae_loss: 0.05043125\n",
      "Step: [2011] total_loss: 2.13079977 d_loss: 1.37433648, g_loss: 0.70434344, ae_loss: 0.05212003\n",
      "Step: [2012] total_loss: 2.13551474 d_loss: 1.39869738, g_loss: 0.68578351, ae_loss: 0.05103390\n",
      "Step: [2013] total_loss: 2.14438558 d_loss: 1.39703250, g_loss: 0.69415951, ae_loss: 0.05319363\n",
      "Step: [2014] total_loss: 2.11669397 d_loss: 1.37525034, g_loss: 0.68968713, ae_loss: 0.05175663\n",
      "Step: [2015] total_loss: 2.14278269 d_loss: 1.38523650, g_loss: 0.70483696, ae_loss: 0.05270928\n",
      "Step: [2016] total_loss: 2.11920500 d_loss: 1.38792408, g_loss: 0.68382847, ae_loss: 0.04745261\n",
      "Step: [2017] total_loss: 2.12407446 d_loss: 1.38163829, g_loss: 0.69341505, ae_loss: 0.04902099\n",
      "Step: [2018] total_loss: 2.13126421 d_loss: 1.39065480, g_loss: 0.69148171, ae_loss: 0.04912780\n",
      "Step: [2019] total_loss: 2.13843155 d_loss: 1.38649821, g_loss: 0.69806314, ae_loss: 0.05387006\n",
      "Step: [2020] total_loss: 2.15010571 d_loss: 1.38460755, g_loss: 0.71433568, ae_loss: 0.05116250\n",
      "Step: [2021] total_loss: 2.13788605 d_loss: 1.36148405, g_loss: 0.72656322, ae_loss: 0.04983881\n",
      "Step: [2022] total_loss: 2.13789153 d_loss: 1.38658237, g_loss: 0.69605929, ae_loss: 0.05524987\n",
      "Step: [2023] total_loss: 2.16211820 d_loss: 1.41976285, g_loss: 0.69129622, ae_loss: 0.05105917\n",
      "Step: [2024] total_loss: 2.13949156 d_loss: 1.39506233, g_loss: 0.69397753, ae_loss: 0.05045164\n",
      "Step: [2025] total_loss: 2.10483432 d_loss: 1.37565327, g_loss: 0.67740154, ae_loss: 0.05177955\n",
      "Step: [2026] total_loss: 2.10909986 d_loss: 1.37922454, g_loss: 0.67547727, ae_loss: 0.05439799\n",
      "Step: [2027] total_loss: 2.11351252 d_loss: 1.38358307, g_loss: 0.68089461, ae_loss: 0.04903483\n",
      "Step: [2028] total_loss: 2.12127900 d_loss: 1.38313019, g_loss: 0.68646318, ae_loss: 0.05168558\n",
      "Step: [2029] total_loss: 2.13445139 d_loss: 1.38725901, g_loss: 0.69900084, ae_loss: 0.04819154\n",
      "Step: [2030] total_loss: 2.13724566 d_loss: 1.39472771, g_loss: 0.69140875, ae_loss: 0.05110914\n",
      "Step: [2031] total_loss: 2.10413361 d_loss: 1.35200238, g_loss: 0.70312226, ae_loss: 0.04900903\n",
      "Step: [2032] total_loss: 2.10920525 d_loss: 1.34159565, g_loss: 0.71507931, ae_loss: 0.05253011\n",
      "Step: [2033] total_loss: 2.13585901 d_loss: 1.39568508, g_loss: 0.68492323, ae_loss: 0.05525070\n",
      "Step: [2034] total_loss: 2.13087416 d_loss: 1.37790501, g_loss: 0.70113367, ae_loss: 0.05183555\n",
      "Step: [2035] total_loss: 2.13030386 d_loss: 1.38240266, g_loss: 0.69507086, ae_loss: 0.05283045\n",
      "Step: [2036] total_loss: 2.13949966 d_loss: 1.37773156, g_loss: 0.71027601, ae_loss: 0.05149209\n",
      "Step: [2037] total_loss: 2.12704372 d_loss: 1.37746000, g_loss: 0.69641817, ae_loss: 0.05316565\n",
      "Step: [2038] total_loss: 2.16080427 d_loss: 1.42589736, g_loss: 0.68220723, ae_loss: 0.05269954\n",
      "Step: [2039] total_loss: 2.12361550 d_loss: 1.37726080, g_loss: 0.69419605, ae_loss: 0.05215859\n",
      "Step: [2040] total_loss: 2.11612988 d_loss: 1.38387883, g_loss: 0.68398345, ae_loss: 0.04826758\n",
      "Step: [2041] total_loss: 2.11252069 d_loss: 1.38106823, g_loss: 0.68317318, ae_loss: 0.04827924\n",
      "Step: [2042] total_loss: 2.13310194 d_loss: 1.39256454, g_loss: 0.69362348, ae_loss: 0.04691386\n",
      "Step: [2043] total_loss: 2.12009001 d_loss: 1.37797999, g_loss: 0.69271803, ae_loss: 0.04939192\n",
      "Step: [2044] total_loss: 2.10453868 d_loss: 1.37040913, g_loss: 0.68521398, ae_loss: 0.04891553\n",
      "Step: [2045] total_loss: 2.11914968 d_loss: 1.38401759, g_loss: 0.68307304, ae_loss: 0.05205891\n",
      "Step: [2046] total_loss: 2.12027454 d_loss: 1.38212752, g_loss: 0.68859684, ae_loss: 0.04955009\n",
      "Step: [2047] total_loss: 2.15065813 d_loss: 1.39086115, g_loss: 0.70480919, ae_loss: 0.05498766\n",
      "Step: [2048] total_loss: 2.12891269 d_loss: 1.38331604, g_loss: 0.69202745, ae_loss: 0.05356925\n",
      "Step: [2049] total_loss: 2.13755083 d_loss: 1.37067914, g_loss: 0.71385944, ae_loss: 0.05301208\n",
      "Step: [2050] total_loss: 2.13745856 d_loss: 1.37726784, g_loss: 0.70764482, ae_loss: 0.05254586\n",
      "Step: [2051] total_loss: 2.11487818 d_loss: 1.38196051, g_loss: 0.68403649, ae_loss: 0.04888104\n",
      "Step: [2052] total_loss: 2.13070440 d_loss: 1.38939905, g_loss: 0.68711352, ae_loss: 0.05419199\n",
      "Step: [2053] total_loss: 2.11737680 d_loss: 1.36256206, g_loss: 0.70086360, ae_loss: 0.05395101\n",
      "Step: [2054] total_loss: 2.12113953 d_loss: 1.37858272, g_loss: 0.68855023, ae_loss: 0.05400644\n",
      "Step: [2055] total_loss: 2.09388638 d_loss: 1.37117696, g_loss: 0.67087173, ae_loss: 0.05183766\n",
      "Step: [2056] total_loss: 2.11369920 d_loss: 1.37696588, g_loss: 0.68274516, ae_loss: 0.05398814\n",
      "Step: [2057] total_loss: 2.16985655 d_loss: 1.41731167, g_loss: 0.69858098, ae_loss: 0.05396391\n",
      "Step: [2058] total_loss: 2.13620925 d_loss: 1.37389481, g_loss: 0.71197104, ae_loss: 0.05034345\n",
      "Step: [2059] total_loss: 2.12102270 d_loss: 1.35765958, g_loss: 0.70755672, ae_loss: 0.05580624\n",
      "Step: [2060] total_loss: 2.11411476 d_loss: 1.37006462, g_loss: 0.69210160, ae_loss: 0.05194864\n",
      "Step: [2061] total_loss: 2.13503242 d_loss: 1.38515329, g_loss: 0.69839627, ae_loss: 0.05148282\n",
      "Step: [2062] total_loss: 2.14502525 d_loss: 1.39423096, g_loss: 0.69954455, ae_loss: 0.05124970\n",
      "Step: [2063] total_loss: 2.14989138 d_loss: 1.37339735, g_loss: 0.72162509, ae_loss: 0.05486899\n",
      "Step: [2064] total_loss: 2.11885333 d_loss: 1.37164319, g_loss: 0.69840288, ae_loss: 0.04880722\n",
      "Step: [2065] total_loss: 2.12177730 d_loss: 1.39283919, g_loss: 0.67433667, ae_loss: 0.05460149\n",
      "Step: [2066] total_loss: 2.11955452 d_loss: 1.37869716, g_loss: 0.68639171, ae_loss: 0.05446581\n",
      "Step: [2067] total_loss: 2.12466526 d_loss: 1.39284217, g_loss: 0.68034983, ae_loss: 0.05147329\n",
      "Step: [2068] total_loss: 2.17147231 d_loss: 1.40728498, g_loss: 0.71370953, ae_loss: 0.05047780\n",
      "Step: [2069] total_loss: 2.11442947 d_loss: 1.37304282, g_loss: 0.69247824, ae_loss: 0.04890851\n",
      "Step: [2070] total_loss: 2.15733933 d_loss: 1.39355969, g_loss: 0.71500653, ae_loss: 0.04877317\n",
      "Step: [2071] total_loss: 2.15075350 d_loss: 1.40507436, g_loss: 0.69305897, ae_loss: 0.05262015\n",
      "Step: [2072] total_loss: 2.13082981 d_loss: 1.38118935, g_loss: 0.69486487, ae_loss: 0.05477575\n",
      "Step: [2073] total_loss: 2.14698839 d_loss: 1.38482356, g_loss: 0.70903879, ae_loss: 0.05312596\n",
      "Step: [2074] total_loss: 2.11128473 d_loss: 1.35675192, g_loss: 0.70490360, ae_loss: 0.04962935\n",
      "Step: [2075] total_loss: 2.14296103 d_loss: 1.40855122, g_loss: 0.68439221, ae_loss: 0.05001762\n",
      "Step: [2076] total_loss: 2.12388420 d_loss: 1.34866667, g_loss: 0.71936822, ae_loss: 0.05584920\n",
      "Step: [2077] total_loss: 2.10456848 d_loss: 1.36960769, g_loss: 0.68170166, ae_loss: 0.05325902\n",
      "Step: [2078] total_loss: 2.12056160 d_loss: 1.40184689, g_loss: 0.66389894, ae_loss: 0.05481567\n",
      "Step: [2079] total_loss: 2.11209464 d_loss: 1.38067126, g_loss: 0.67621946, ae_loss: 0.05520390\n",
      "Step: [2080] total_loss: 2.12652063 d_loss: 1.40411961, g_loss: 0.67073596, ae_loss: 0.05166511\n",
      "Step: [2081] total_loss: 2.12834167 d_loss: 1.40025437, g_loss: 0.67770910, ae_loss: 0.05037836\n",
      "Step: [2082] total_loss: 2.14683247 d_loss: 1.37934101, g_loss: 0.71306348, ae_loss: 0.05442805\n",
      "Step: [2083] total_loss: 2.14036679 d_loss: 1.39460754, g_loss: 0.69120443, ae_loss: 0.05455478\n",
      "Step: [2084] total_loss: 2.15127182 d_loss: 1.39426041, g_loss: 0.70448911, ae_loss: 0.05252223\n",
      "Step: [2085] total_loss: 2.12772608 d_loss: 1.38628316, g_loss: 0.68889904, ae_loss: 0.05254370\n",
      "Step: [2086] total_loss: 2.11882257 d_loss: 1.37233472, g_loss: 0.69361854, ae_loss: 0.05286934\n",
      "Step: [2087] total_loss: 2.11032915 d_loss: 1.36224961, g_loss: 0.69775808, ae_loss: 0.05032155\n",
      "Step: [2088] total_loss: 2.12941241 d_loss: 1.40182257, g_loss: 0.67855835, ae_loss: 0.04903145\n",
      "Step: [2089] total_loss: 2.10614538 d_loss: 1.36774647, g_loss: 0.68621051, ae_loss: 0.05218837\n",
      "Step: [2090] total_loss: 2.11379719 d_loss: 1.38378453, g_loss: 0.68044960, ae_loss: 0.04956291\n",
      "Step: [2091] total_loss: 2.13500023 d_loss: 1.38964701, g_loss: 0.69754404, ae_loss: 0.04780922\n",
      "Step: [2092] total_loss: 2.14329290 d_loss: 1.38909686, g_loss: 0.69976497, ae_loss: 0.05443109\n",
      "Step: [2093] total_loss: 2.12967849 d_loss: 1.38257456, g_loss: 0.69902456, ae_loss: 0.04807934\n",
      "Step: [2094] total_loss: 2.12762523 d_loss: 1.36841989, g_loss: 0.70643836, ae_loss: 0.05276704\n",
      "Step: [2095] total_loss: 2.12564874 d_loss: 1.39075232, g_loss: 0.68254977, ae_loss: 0.05234657\n",
      "Step: [2096] total_loss: 2.12853050 d_loss: 1.36710227, g_loss: 0.70705873, ae_loss: 0.05436955\n",
      "Step: [2097] total_loss: 2.13800859 d_loss: 1.39450598, g_loss: 0.68953276, ae_loss: 0.05397002\n",
      "Step: [2098] total_loss: 2.13470697 d_loss: 1.37565100, g_loss: 0.70776075, ae_loss: 0.05129518\n",
      "Step: [2099] total_loss: 2.13094234 d_loss: 1.38454759, g_loss: 0.69546616, ae_loss: 0.05092864\n",
      "Step: [2100] total_loss: 2.10837221 d_loss: 1.36843777, g_loss: 0.69033027, ae_loss: 0.04960410\n",
      "Step: [2101] total_loss: 2.12784576 d_loss: 1.39156353, g_loss: 0.68303132, ae_loss: 0.05325082\n",
      "Step: [2102] total_loss: 2.11238980 d_loss: 1.37885356, g_loss: 0.68173504, ae_loss: 0.05180117\n",
      "Step: [2103] total_loss: 2.11406016 d_loss: 1.37640047, g_loss: 0.68725890, ae_loss: 0.05040080\n",
      "Step: [2104] total_loss: 2.12772036 d_loss: 1.39340472, g_loss: 0.68034446, ae_loss: 0.05397122\n",
      "Step: [2105] total_loss: 2.14052153 d_loss: 1.39828491, g_loss: 0.68584359, ae_loss: 0.05639286\n",
      "Step: [2106] total_loss: 2.13460779 d_loss: 1.38261366, g_loss: 0.69765592, ae_loss: 0.05433805\n",
      "Step: [2107] total_loss: 2.11548805 d_loss: 1.37445819, g_loss: 0.69147754, ae_loss: 0.04955243\n",
      "Step: [2108] total_loss: 2.11608815 d_loss: 1.39113379, g_loss: 0.67421031, ae_loss: 0.05074404\n",
      "Step: [2109] total_loss: 2.12913942 d_loss: 1.39086306, g_loss: 0.68435168, ae_loss: 0.05392480\n",
      "Step: [2110] total_loss: 2.11614656 d_loss: 1.37447751, g_loss: 0.68710184, ae_loss: 0.05456719\n",
      "Step: [2111] total_loss: 2.11718392 d_loss: 1.38197947, g_loss: 0.68358946, ae_loss: 0.05161497\n",
      "Step: [2112] total_loss: 2.12583065 d_loss: 1.40057015, g_loss: 0.67545277, ae_loss: 0.04980765\n",
      "Step: [2113] total_loss: 2.11149597 d_loss: 1.37573624, g_loss: 0.68218648, ae_loss: 0.05357334\n",
      "Step: [2114] total_loss: 2.11506510 d_loss: 1.36327863, g_loss: 0.69976664, ae_loss: 0.05201986\n",
      "Step: [2115] total_loss: 2.12718225 d_loss: 1.37827146, g_loss: 0.69776553, ae_loss: 0.05114536\n",
      "Step: [2116] total_loss: 2.14163017 d_loss: 1.39501202, g_loss: 0.69692755, ae_loss: 0.04969057\n",
      "Step: [2117] total_loss: 2.12692046 d_loss: 1.38078082, g_loss: 0.69221592, ae_loss: 0.05392379\n",
      "Step: [2118] total_loss: 2.11997461 d_loss: 1.37942576, g_loss: 0.68936551, ae_loss: 0.05118334\n",
      "Step: [2119] total_loss: 2.11736870 d_loss: 1.38492393, g_loss: 0.68640047, ae_loss: 0.04604432\n",
      "Step: [2120] total_loss: 2.11632729 d_loss: 1.38544106, g_loss: 0.68091619, ae_loss: 0.04996996\n",
      "Step: [2121] total_loss: 2.14307809 d_loss: 1.39146614, g_loss: 0.69383609, ae_loss: 0.05777589\n",
      "Step: [2122] total_loss: 2.15263462 d_loss: 1.39521742, g_loss: 0.70800829, ae_loss: 0.04940882\n",
      "Step: [2123] total_loss: 2.11805224 d_loss: 1.37343764, g_loss: 0.69467849, ae_loss: 0.04993611\n",
      "Step: [2124] total_loss: 2.13556981 d_loss: 1.37399614, g_loss: 0.70560026, ae_loss: 0.05597335\n",
      "Step: [2125] total_loss: 2.12038183 d_loss: 1.37083018, g_loss: 0.69280851, ae_loss: 0.05674298\n",
      "Step: [2126] total_loss: 2.14491272 d_loss: 1.40113211, g_loss: 0.69141746, ae_loss: 0.05236311\n",
      "Step: [2127] total_loss: 2.13783073 d_loss: 1.37794709, g_loss: 0.71021050, ae_loss: 0.04967315\n",
      "Step: [2128] total_loss: 2.12307882 d_loss: 1.37684333, g_loss: 0.69521648, ae_loss: 0.05101899\n",
      "Step: [2129] total_loss: 2.15319514 d_loss: 1.39296579, g_loss: 0.70306271, ae_loss: 0.05716669\n",
      "Step: [2130] total_loss: 2.15597010 d_loss: 1.39226508, g_loss: 0.71435207, ae_loss: 0.04935301\n",
      "Step: [2131] total_loss: 2.12899113 d_loss: 1.36288583, g_loss: 0.71134996, ae_loss: 0.05475535\n",
      "Step: [2132] total_loss: 2.12801504 d_loss: 1.39282548, g_loss: 0.68157071, ae_loss: 0.05361896\n",
      "Step: [2133] total_loss: 2.12589145 d_loss: 1.38102448, g_loss: 0.69374883, ae_loss: 0.05111809\n",
      "Step: [2134] total_loss: 2.12500930 d_loss: 1.39335120, g_loss: 0.67839313, ae_loss: 0.05326493\n",
      "Step: [2135] total_loss: 2.08830547 d_loss: 1.36763263, g_loss: 0.66834962, ae_loss: 0.05232318\n",
      "Step: [2136] total_loss: 2.11978459 d_loss: 1.37488675, g_loss: 0.69348657, ae_loss: 0.05141127\n",
      "Step: [2137] total_loss: 2.10246181 d_loss: 1.38263750, g_loss: 0.67017949, ae_loss: 0.04964485\n",
      "Step: [2138] total_loss: 2.12585759 d_loss: 1.39807272, g_loss: 0.67554665, ae_loss: 0.05223820\n",
      "Step: [2139] total_loss: 2.13259768 d_loss: 1.38443685, g_loss: 0.69944084, ae_loss: 0.04872003\n",
      "Step: [2140] total_loss: 2.14249325 d_loss: 1.37378716, g_loss: 0.71754968, ae_loss: 0.05115652\n",
      "Step: [2141] total_loss: 2.13767719 d_loss: 1.38897109, g_loss: 0.69859874, ae_loss: 0.05010738\n",
      "Step: [2142] total_loss: 2.12036252 d_loss: 1.37546587, g_loss: 0.69300371, ae_loss: 0.05189285\n",
      "Step: [2143] total_loss: 2.15627456 d_loss: 1.38396025, g_loss: 0.72434008, ae_loss: 0.04797425\n",
      "Step: [2144] total_loss: 2.11522698 d_loss: 1.37420011, g_loss: 0.69094580, ae_loss: 0.05008114\n",
      "Step: [2145] total_loss: 2.12689900 d_loss: 1.37151289, g_loss: 0.70555425, ae_loss: 0.04983186\n",
      "Step: [2146] total_loss: 2.11044931 d_loss: 1.37059474, g_loss: 0.68531418, ae_loss: 0.05454042\n",
      "Step: [2147] total_loss: 2.12976646 d_loss: 1.39571750, g_loss: 0.67788935, ae_loss: 0.05615949\n",
      "Step: [2148] total_loss: 2.09298396 d_loss: 1.37285912, g_loss: 0.66495091, ae_loss: 0.05517394\n",
      "Step: [2149] total_loss: 2.10542345 d_loss: 1.37660480, g_loss: 0.67642772, ae_loss: 0.05239084\n",
      "Step: [2150] total_loss: 2.11265612 d_loss: 1.37346447, g_loss: 0.69099379, ae_loss: 0.04819789\n",
      "Step: [2151] total_loss: 2.14054012 d_loss: 1.38799632, g_loss: 0.69978368, ae_loss: 0.05276028\n",
      "Step: [2152] total_loss: 2.12099171 d_loss: 1.35366178, g_loss: 0.71126592, ae_loss: 0.05606406\n",
      "Step: [2153] total_loss: 2.11384654 d_loss: 1.37373877, g_loss: 0.68919557, ae_loss: 0.05091217\n",
      "Step: [2154] total_loss: 2.11121392 d_loss: 1.37672496, g_loss: 0.68617868, ae_loss: 0.04831034\n",
      "Step: [2155] total_loss: 2.10936093 d_loss: 1.38173997, g_loss: 0.67496485, ae_loss: 0.05265608\n",
      "Step: [2156] total_loss: 2.12827206 d_loss: 1.39363980, g_loss: 0.68162215, ae_loss: 0.05301013\n",
      "Step: [2157] total_loss: 2.11136913 d_loss: 1.37408400, g_loss: 0.68123370, ae_loss: 0.05605136\n",
      "Step: [2158] total_loss: 2.11994672 d_loss: 1.36970758, g_loss: 0.70216191, ae_loss: 0.04807726\n",
      "Step: [2159] total_loss: 2.13808775 d_loss: 1.37486851, g_loss: 0.71196461, ae_loss: 0.05125479\n",
      "Step: [2160] total_loss: 2.13701963 d_loss: 1.36796856, g_loss: 0.72168350, ae_loss: 0.04736775\n",
      "Step: [2161] total_loss: 2.12774014 d_loss: 1.35648918, g_loss: 0.71926796, ae_loss: 0.05198301\n",
      "Step: [2162] total_loss: 2.13587284 d_loss: 1.39819086, g_loss: 0.68188900, ae_loss: 0.05579289\n",
      "Step: [2163] total_loss: 2.12223434 d_loss: 1.37834275, g_loss: 0.69016635, ae_loss: 0.05372509\n",
      "Step: [2164] total_loss: 2.11492395 d_loss: 1.35993648, g_loss: 0.70204890, ae_loss: 0.05293849\n",
      "Step: [2165] total_loss: 2.12203455 d_loss: 1.39133489, g_loss: 0.67834890, ae_loss: 0.05235083\n",
      "Step: [2166] total_loss: 2.16094470 d_loss: 1.41992426, g_loss: 0.68949771, ae_loss: 0.05152277\n",
      "Step: [2167] total_loss: 2.13761377 d_loss: 1.37057889, g_loss: 0.71984571, ae_loss: 0.04718923\n",
      "Step: [2168] total_loss: 2.13215733 d_loss: 1.36393929, g_loss: 0.71468651, ae_loss: 0.05353156\n",
      "Step: [2169] total_loss: 2.14492130 d_loss: 1.37371492, g_loss: 0.71802199, ae_loss: 0.05318433\n",
      "Step: [2170] total_loss: 2.13837671 d_loss: 1.40010500, g_loss: 0.68774056, ae_loss: 0.05053118\n",
      "Step: [2171] total_loss: 2.14962888 d_loss: 1.38989544, g_loss: 0.70355099, ae_loss: 0.05618242\n",
      "Step: [2172] total_loss: 2.13109517 d_loss: 1.38657939, g_loss: 0.69057631, ae_loss: 0.05393944\n",
      "Step: [2173] total_loss: 2.13840747 d_loss: 1.38812876, g_loss: 0.69668019, ae_loss: 0.05359850\n",
      "Step: [2174] total_loss: 2.14073658 d_loss: 1.39309013, g_loss: 0.69451785, ae_loss: 0.05312873\n",
      "Step: [2175] total_loss: 2.12284756 d_loss: 1.38344574, g_loss: 0.68712270, ae_loss: 0.05227896\n",
      "Step: [2176] total_loss: 2.12780857 d_loss: 1.39196026, g_loss: 0.68339241, ae_loss: 0.05245600\n",
      "Step: [2177] total_loss: 2.10939622 d_loss: 1.36922956, g_loss: 0.68707615, ae_loss: 0.05309055\n",
      "Step: [2178] total_loss: 2.12737179 d_loss: 1.38888502, g_loss: 0.68224728, ae_loss: 0.05623954\n",
      "Step: [2179] total_loss: 2.16594028 d_loss: 1.40275097, g_loss: 0.71290684, ae_loss: 0.05028264\n",
      "Step: [2180] total_loss: 2.14684200 d_loss: 1.38846588, g_loss: 0.70593244, ae_loss: 0.05244379\n",
      "Step: [2181] total_loss: 2.13607264 d_loss: 1.38506591, g_loss: 0.69899893, ae_loss: 0.05200779\n",
      "Step: [2182] total_loss: 2.13050294 d_loss: 1.36884737, g_loss: 0.70788622, ae_loss: 0.05376931\n",
      "Step: [2183] total_loss: 2.12723255 d_loss: 1.38944459, g_loss: 0.68346858, ae_loss: 0.05431933\n",
      "Step: [2184] total_loss: 2.13370037 d_loss: 1.38680267, g_loss: 0.69364154, ae_loss: 0.05325619\n",
      "Step: [2185] total_loss: 2.10532475 d_loss: 1.37700045, g_loss: 0.67383987, ae_loss: 0.05448445\n",
      "Step: [2186] total_loss: 2.12630081 d_loss: 1.37984490, g_loss: 0.68903291, ae_loss: 0.05742313\n",
      "Step: [2187] total_loss: 2.10714722 d_loss: 1.36143756, g_loss: 0.69092441, ae_loss: 0.05478511\n",
      "Step: [2188] total_loss: 2.13287258 d_loss: 1.39003968, g_loss: 0.68679124, ae_loss: 0.05604170\n",
      "Step: [2189] total_loss: 2.09789968 d_loss: 1.36720562, g_loss: 0.67974699, ae_loss: 0.05094709\n",
      "Step: [2190] total_loss: 2.10721707 d_loss: 1.36939180, g_loss: 0.68624473, ae_loss: 0.05158054\n",
      "Step: [2191] total_loss: 2.12966919 d_loss: 1.37768912, g_loss: 0.69424033, ae_loss: 0.05773966\n",
      "Step: [2192] total_loss: 2.13516331 d_loss: 1.37707078, g_loss: 0.70668674, ae_loss: 0.05140586\n",
      "Step: [2193] total_loss: 2.13518476 d_loss: 1.38021040, g_loss: 0.70072484, ae_loss: 0.05424949\n",
      "Step: [2194] total_loss: 2.15489912 d_loss: 1.38554502, g_loss: 0.71907717, ae_loss: 0.05027705\n",
      "Step: [2195] total_loss: 2.13402510 d_loss: 1.38826931, g_loss: 0.69221860, ae_loss: 0.05353723\n",
      "Step: [2196] total_loss: 2.12325287 d_loss: 1.38685787, g_loss: 0.68741822, ae_loss: 0.04897668\n",
      "Step: [2197] total_loss: 2.13455558 d_loss: 1.38074684, g_loss: 0.70418888, ae_loss: 0.04961994\n",
      "Step: [2198] total_loss: 2.12071180 d_loss: 1.37142372, g_loss: 0.69765574, ae_loss: 0.05163231\n",
      "Step: [2199] total_loss: 2.12343168 d_loss: 1.38063359, g_loss: 0.68926686, ae_loss: 0.05353129\n",
      "Step: [2200] total_loss: 2.13160563 d_loss: 1.39705420, g_loss: 0.68233830, ae_loss: 0.05221316\n",
      "Step: [2201] total_loss: 2.13659096 d_loss: 1.39956295, g_loss: 0.68908072, ae_loss: 0.04794724\n",
      "Step: [2202] total_loss: 2.13023067 d_loss: 1.38686562, g_loss: 0.69445968, ae_loss: 0.04890542\n",
      "Step: [2203] total_loss: 2.12945795 d_loss: 1.38009739, g_loss: 0.69791079, ae_loss: 0.05144969\n",
      "Step: [2204] total_loss: 2.14242578 d_loss: 1.39369822, g_loss: 0.69426751, ae_loss: 0.05446006\n",
      "Step: [2205] total_loss: 2.13344193 d_loss: 1.38043380, g_loss: 0.70017159, ae_loss: 0.05283637\n",
      "Step: [2206] total_loss: 2.14552259 d_loss: 1.39785540, g_loss: 0.69520390, ae_loss: 0.05246342\n",
      "Step: [2207] total_loss: 2.12713242 d_loss: 1.36979198, g_loss: 0.70744979, ae_loss: 0.04989063\n",
      "Step: [2208] total_loss: 2.11916876 d_loss: 1.36612988, g_loss: 0.69876826, ae_loss: 0.05427065\n",
      "Step: [2209] total_loss: 2.11488819 d_loss: 1.38511610, g_loss: 0.67786968, ae_loss: 0.05190249\n",
      "Step: [2210] total_loss: 2.11407709 d_loss: 1.38108015, g_loss: 0.68311632, ae_loss: 0.04988046\n",
      "Step: [2211] total_loss: 2.10242081 d_loss: 1.36948442, g_loss: 0.68496281, ae_loss: 0.04797347\n",
      "Step: [2212] total_loss: 2.11737561 d_loss: 1.38095093, g_loss: 0.68291008, ae_loss: 0.05351464\n",
      "Step: [2213] total_loss: 2.12628341 d_loss: 1.38546562, g_loss: 0.68436098, ae_loss: 0.05645685\n",
      "Step: [2214] total_loss: 2.12542892 d_loss: 1.37104034, g_loss: 0.69633949, ae_loss: 0.05804905\n",
      "Step: [2215] total_loss: 2.12305784 d_loss: 1.38577306, g_loss: 0.68554640, ae_loss: 0.05173833\n",
      "Step: [2216] total_loss: 2.11578679 d_loss: 1.37696028, g_loss: 0.69025588, ae_loss: 0.04857068\n",
      "Step: [2217] total_loss: 2.12699008 d_loss: 1.39608502, g_loss: 0.67917818, ae_loss: 0.05172677\n",
      "Step: [2218] total_loss: 2.13858390 d_loss: 1.39261413, g_loss: 0.69372714, ae_loss: 0.05224261\n",
      "Step: [2219] total_loss: 2.14439774 d_loss: 1.41304302, g_loss: 0.67874765, ae_loss: 0.05260716\n",
      "Step: [2220] total_loss: 2.11534691 d_loss: 1.35685635, g_loss: 0.70617104, ae_loss: 0.05231939\n",
      "Step: [2221] total_loss: 2.13867354 d_loss: 1.38298941, g_loss: 0.70496440, ae_loss: 0.05071976\n",
      "Step: [2222] total_loss: 2.14779305 d_loss: 1.38679731, g_loss: 0.70039809, ae_loss: 0.06059761\n",
      "Step: [2223] total_loss: 2.12215042 d_loss: 1.37977004, g_loss: 0.69348228, ae_loss: 0.04889799\n",
      "Step: [2224] total_loss: 2.12774563 d_loss: 1.37136626, g_loss: 0.70181012, ae_loss: 0.05456928\n",
      "Step: [2225] total_loss: 2.11345029 d_loss: 1.36938286, g_loss: 0.69638032, ae_loss: 0.04768701\n",
      "Step: [2226] total_loss: 2.12956715 d_loss: 1.36476302, g_loss: 0.71174252, ae_loss: 0.05306145\n",
      "Step: [2227] total_loss: 2.11497855 d_loss: 1.37561953, g_loss: 0.68892294, ae_loss: 0.05043606\n",
      "Step: [2228] total_loss: 2.11772060 d_loss: 1.37961483, g_loss: 0.68326485, ae_loss: 0.05484081\n",
      "Step: [2229] total_loss: 2.12339973 d_loss: 1.37954223, g_loss: 0.69178873, ae_loss: 0.05206874\n",
      "Step: [2230] total_loss: 2.13842511 d_loss: 1.36995232, g_loss: 0.71750414, ae_loss: 0.05096865\n",
      "Step: [2231] total_loss: 2.14774442 d_loss: 1.37697673, g_loss: 0.71991014, ae_loss: 0.05085760\n",
      "Step: [2232] total_loss: 2.13378787 d_loss: 1.39988375, g_loss: 0.68174362, ae_loss: 0.05216051\n",
      "Step: [2233] total_loss: 2.14115858 d_loss: 1.38736582, g_loss: 0.70165050, ae_loss: 0.05214213\n",
      "Step: [2234] total_loss: 2.10265613 d_loss: 1.36439061, g_loss: 0.68425715, ae_loss: 0.05400836\n",
      "Step: [2235] total_loss: 2.11204410 d_loss: 1.38421345, g_loss: 0.67412210, ae_loss: 0.05370853\n",
      "Step: [2236] total_loss: 2.12780046 d_loss: 1.37461948, g_loss: 0.69889128, ae_loss: 0.05428970\n",
      "Step: [2237] total_loss: 2.10908008 d_loss: 1.38549328, g_loss: 0.66804832, ae_loss: 0.05553838\n",
      "Step: [2238] total_loss: 2.10621834 d_loss: 1.37448049, g_loss: 0.67802465, ae_loss: 0.05371309\n",
      "Step: [2239] total_loss: 2.13325644 d_loss: 1.37867904, g_loss: 0.70009285, ae_loss: 0.05448465\n",
      "Step: [2240] total_loss: 2.13253355 d_loss: 1.38957298, g_loss: 0.69094086, ae_loss: 0.05201974\n",
      "Step: [2241] total_loss: 2.15141702 d_loss: 1.37820244, g_loss: 0.71779579, ae_loss: 0.05541888\n",
      "Step: [2242] total_loss: 2.14520931 d_loss: 1.36694717, g_loss: 0.72503245, ae_loss: 0.05322966\n",
      "Step: [2243] total_loss: 2.14478970 d_loss: 1.37998271, g_loss: 0.70703030, ae_loss: 0.05777657\n",
      "Step: [2244] total_loss: 2.14138341 d_loss: 1.40049791, g_loss: 0.68668377, ae_loss: 0.05420172\n",
      "Step: [2245] total_loss: 2.13981342 d_loss: 1.38607538, g_loss: 0.69979227, ae_loss: 0.05394561\n",
      "Step: [2246] total_loss: 2.11265612 d_loss: 1.37996185, g_loss: 0.68092370, ae_loss: 0.05177067\n",
      "Step: [2247] total_loss: 2.11618471 d_loss: 1.35884440, g_loss: 0.70260918, ae_loss: 0.05473131\n",
      "Step: [2248] total_loss: 2.11477566 d_loss: 1.38528800, g_loss: 0.67467713, ae_loss: 0.05481040\n",
      "Step: [2249] total_loss: 2.12119985 d_loss: 1.37318361, g_loss: 0.69510102, ae_loss: 0.05291525\n",
      "Step: [2250] total_loss: 2.09565973 d_loss: 1.36464417, g_loss: 0.68356478, ae_loss: 0.04745088\n",
      "Step: [2251] total_loss: 2.10718966 d_loss: 1.36529589, g_loss: 0.68716562, ae_loss: 0.05472824\n",
      "Step: [2252] total_loss: 2.12792444 d_loss: 1.37380803, g_loss: 0.70075834, ae_loss: 0.05335812\n",
      "Step: [2253] total_loss: 2.09207296 d_loss: 1.35429490, g_loss: 0.68494070, ae_loss: 0.05283748\n",
      "Step: [2254] total_loss: 2.12043643 d_loss: 1.37123013, g_loss: 0.69578803, ae_loss: 0.05341826\n",
      "Step: [2255] total_loss: 2.14190531 d_loss: 1.38546145, g_loss: 0.70449257, ae_loss: 0.05195132\n",
      "Step: [2256] total_loss: 2.14622235 d_loss: 1.39491761, g_loss: 0.69977123, ae_loss: 0.05153355\n",
      "Step: [2257] total_loss: 2.15131450 d_loss: 1.40887332, g_loss: 0.69156063, ae_loss: 0.05088055\n",
      "Step: [2258] total_loss: 2.14622068 d_loss: 1.37327933, g_loss: 0.71654630, ae_loss: 0.05639490\n",
      "Step: [2259] total_loss: 2.14886570 d_loss: 1.38830817, g_loss: 0.70549262, ae_loss: 0.05506490\n",
      "Step: [2260] total_loss: 2.13221073 d_loss: 1.37858486, g_loss: 0.70347416, ae_loss: 0.05015158\n",
      "Step: [2261] total_loss: 2.13535452 d_loss: 1.37796605, g_loss: 0.70088851, ae_loss: 0.05650014\n",
      "Step: [2262] total_loss: 2.11984825 d_loss: 1.37999320, g_loss: 0.68539113, ae_loss: 0.05446389\n",
      "Step: [2263] total_loss: 2.09558034 d_loss: 1.35882545, g_loss: 0.68463022, ae_loss: 0.05212476\n",
      "Step: [2264] total_loss: 2.11371565 d_loss: 1.39033103, g_loss: 0.67379522, ae_loss: 0.04958954\n",
      "Step: [2265] total_loss: 2.10170126 d_loss: 1.37383938, g_loss: 0.67495567, ae_loss: 0.05290613\n",
      "Step: [2266] total_loss: 2.10726261 d_loss: 1.35968554, g_loss: 0.69323730, ae_loss: 0.05433960\n",
      "Step: [2267] total_loss: 2.14036274 d_loss: 1.40906870, g_loss: 0.67886031, ae_loss: 0.05243371\n",
      "Step: [2268] total_loss: 2.10720062 d_loss: 1.36427283, g_loss: 0.69131720, ae_loss: 0.05161053\n",
      "Step: [2269] total_loss: 2.12994337 d_loss: 1.37448156, g_loss: 0.70133972, ae_loss: 0.05412217\n",
      "Step: [2270] total_loss: 2.14869690 d_loss: 1.41525543, g_loss: 0.68115079, ae_loss: 0.05229082\n",
      "Step: [2271] total_loss: 2.11574602 d_loss: 1.37002039, g_loss: 0.69320822, ae_loss: 0.05251743\n",
      "Step: [2272] total_loss: 2.13673329 d_loss: 1.38728404, g_loss: 0.69838196, ae_loss: 0.05106736\n",
      "Step: [2273] total_loss: 2.13682246 d_loss: 1.36164165, g_loss: 0.72266126, ae_loss: 0.05251951\n",
      "Step: [2274] total_loss: 2.13720798 d_loss: 1.38572776, g_loss: 0.69798720, ae_loss: 0.05349301\n",
      "Step: [2275] total_loss: 2.11889362 d_loss: 1.37320018, g_loss: 0.69333851, ae_loss: 0.05235506\n",
      "Step: [2276] total_loss: 2.14714956 d_loss: 1.38500834, g_loss: 0.70385182, ae_loss: 0.05828933\n",
      "Step: [2277] total_loss: 2.11900377 d_loss: 1.37358010, g_loss: 0.69789350, ae_loss: 0.04753010\n",
      "Step: [2278] total_loss: 2.09234285 d_loss: 1.35008454, g_loss: 0.69209069, ae_loss: 0.05016774\n",
      "Step: [2279] total_loss: 2.12560225 d_loss: 1.39185929, g_loss: 0.68236518, ae_loss: 0.05137780\n",
      "Step: [2280] total_loss: 2.09214020 d_loss: 1.36009026, g_loss: 0.67723691, ae_loss: 0.05481314\n",
      "Step: [2281] total_loss: 2.12433958 d_loss: 1.38834429, g_loss: 0.68748617, ae_loss: 0.04850908\n",
      "Step: [2282] total_loss: 2.13106418 d_loss: 1.40780330, g_loss: 0.67313844, ae_loss: 0.05012236\n",
      "Step: [2283] total_loss: 2.10796976 d_loss: 1.37328672, g_loss: 0.68382353, ae_loss: 0.05085952\n",
      "Step: [2284] total_loss: 2.11961746 d_loss: 1.37028670, g_loss: 0.70023322, ae_loss: 0.04909737\n",
      "Step: [2285] total_loss: 2.14070821 d_loss: 1.39972365, g_loss: 0.69070482, ae_loss: 0.05027971\n",
      "Step: [2286] total_loss: 2.14520931 d_loss: 1.37781322, g_loss: 0.71699274, ae_loss: 0.05040326\n",
      "Step: [2287] total_loss: 2.12951279 d_loss: 1.38645411, g_loss: 0.69044721, ae_loss: 0.05261133\n",
      "Step: [2288] total_loss: 2.15129137 d_loss: 1.40658998, g_loss: 0.69095421, ae_loss: 0.05374708\n",
      "Step: [2289] total_loss: 2.13176465 d_loss: 1.39088428, g_loss: 0.68524677, ae_loss: 0.05563359\n",
      "Step: [2290] total_loss: 2.12037373 d_loss: 1.37544894, g_loss: 0.69193745, ae_loss: 0.05298727\n",
      "Step: [2291] total_loss: 2.14848733 d_loss: 1.38865864, g_loss: 0.70595014, ae_loss: 0.05387858\n",
      "Step: [2292] total_loss: 2.11680603 d_loss: 1.39548719, g_loss: 0.66992533, ae_loss: 0.05139362\n",
      "Step: [2293] total_loss: 2.13142872 d_loss: 1.37220788, g_loss: 0.70250177, ae_loss: 0.05671889\n",
      "Step: [2294] total_loss: 2.14636993 d_loss: 1.40255356, g_loss: 0.68900049, ae_loss: 0.05481593\n",
      "Step: [2295] total_loss: 2.13806701 d_loss: 1.38615453, g_loss: 0.69755286, ae_loss: 0.05435964\n",
      "Step: [2296] total_loss: 2.12591910 d_loss: 1.38866949, g_loss: 0.68608862, ae_loss: 0.05116091\n",
      "Step: [2297] total_loss: 2.14241028 d_loss: 1.38243437, g_loss: 0.71238148, ae_loss: 0.04759461\n",
      "Step: [2298] total_loss: 2.12189555 d_loss: 1.38058424, g_loss: 0.69200450, ae_loss: 0.04930672\n",
      "Step: [2299] total_loss: 2.14037561 d_loss: 1.39572501, g_loss: 0.69226098, ae_loss: 0.05238964\n",
      "Step: [2300] total_loss: 2.13786459 d_loss: 1.38520813, g_loss: 0.70101273, ae_loss: 0.05164368\n",
      "Step: [2301] total_loss: 2.11790276 d_loss: 1.36357665, g_loss: 0.69974160, ae_loss: 0.05458444\n",
      "Step: [2302] total_loss: 2.12608886 d_loss: 1.37446237, g_loss: 0.69983047, ae_loss: 0.05179598\n",
      "Step: [2303] total_loss: 2.13444114 d_loss: 1.40235806, g_loss: 0.67812997, ae_loss: 0.05395303\n",
      "Step: [2304] total_loss: 2.13301849 d_loss: 1.37855661, g_loss: 0.70233375, ae_loss: 0.05212822\n",
      "Step: [2305] total_loss: 2.11875010 d_loss: 1.37293315, g_loss: 0.69345617, ae_loss: 0.05236078\n",
      "Step: [2306] total_loss: 2.11428833 d_loss: 1.37395191, g_loss: 0.68763590, ae_loss: 0.05270060\n",
      "Step: [2307] total_loss: 2.10185409 d_loss: 1.35048532, g_loss: 0.69524193, ae_loss: 0.05612680\n",
      "Step: [2308] total_loss: 2.11732697 d_loss: 1.36537325, g_loss: 0.69804788, ae_loss: 0.05390580\n",
      "Step: [2309] total_loss: 2.10542250 d_loss: 1.35677373, g_loss: 0.69517422, ae_loss: 0.05347463\n",
      "Step: [2310] total_loss: 2.10547376 d_loss: 1.37115574, g_loss: 0.68251139, ae_loss: 0.05180662\n",
      "Step: [2311] total_loss: 2.13213778 d_loss: 1.39514554, g_loss: 0.68342042, ae_loss: 0.05357194\n",
      "Step: [2312] total_loss: 2.12704778 d_loss: 1.35494006, g_loss: 0.72148061, ae_loss: 0.05062715\n",
      "Step: [2313] total_loss: 2.10008502 d_loss: 1.36460423, g_loss: 0.67875963, ae_loss: 0.05672117\n",
      "Step: [2314] total_loss: 2.11246300 d_loss: 1.37259316, g_loss: 0.68462729, ae_loss: 0.05524264\n",
      "Step: [2315] total_loss: 2.13012528 d_loss: 1.40075684, g_loss: 0.67710781, ae_loss: 0.05226067\n",
      "Step: [2316] total_loss: 2.12757134 d_loss: 1.36190665, g_loss: 0.70979434, ae_loss: 0.05587044\n",
      "Step: [2317] total_loss: 2.12924337 d_loss: 1.37724197, g_loss: 0.69816351, ae_loss: 0.05383795\n",
      "Step: [2318] total_loss: 2.12263656 d_loss: 1.38382936, g_loss: 0.68697244, ae_loss: 0.05183483\n",
      "Step: [2319] total_loss: 2.11632967 d_loss: 1.37492418, g_loss: 0.68554699, ae_loss: 0.05585845\n",
      "Step: [2320] total_loss: 2.10759950 d_loss: 1.35832870, g_loss: 0.69981313, ae_loss: 0.04945763\n",
      "Step: [2321] total_loss: 2.11634660 d_loss: 1.38590097, g_loss: 0.68171275, ae_loss: 0.04873285\n",
      "Step: [2322] total_loss: 2.13790274 d_loss: 1.39081919, g_loss: 0.69826508, ae_loss: 0.04881846\n",
      "Step: [2323] total_loss: 2.14139700 d_loss: 1.38916588, g_loss: 0.70154893, ae_loss: 0.05068219\n",
      "Step: [2324] total_loss: 2.13358736 d_loss: 1.38746679, g_loss: 0.69323075, ae_loss: 0.05288969\n",
      "Step: [2325] total_loss: 2.13371038 d_loss: 1.39289927, g_loss: 0.69030398, ae_loss: 0.05050711\n",
      "Step: [2326] total_loss: 2.12278199 d_loss: 1.38625884, g_loss: 0.68294120, ae_loss: 0.05358195\n",
      "Step: [2327] total_loss: 2.09542990 d_loss: 1.35902727, g_loss: 0.68455243, ae_loss: 0.05185003\n",
      "Step: [2328] total_loss: 2.12453794 d_loss: 1.39556456, g_loss: 0.67299426, ae_loss: 0.05597898\n",
      "Step: [2329] total_loss: 2.13887978 d_loss: 1.38621736, g_loss: 0.69652641, ae_loss: 0.05613618\n",
      "Step: [2330] total_loss: 2.13389921 d_loss: 1.40838742, g_loss: 0.66969121, ae_loss: 0.05582055\n",
      "Step: [2331] total_loss: 2.12575626 d_loss: 1.37080383, g_loss: 0.70400834, ae_loss: 0.05094416\n",
      "Step: [2332] total_loss: 2.14131236 d_loss: 1.39764178, g_loss: 0.68854582, ae_loss: 0.05512471\n",
      "Step: [2333] total_loss: 2.10579062 d_loss: 1.36909688, g_loss: 0.68461621, ae_loss: 0.05207752\n",
      "Step: [2334] total_loss: 2.13696289 d_loss: 1.40240204, g_loss: 0.68263197, ae_loss: 0.05192871\n",
      "Step: [2335] total_loss: 2.13788319 d_loss: 1.39479518, g_loss: 0.68422508, ae_loss: 0.05886305\n",
      "Step: [2336] total_loss: 2.11354256 d_loss: 1.38058805, g_loss: 0.68037665, ae_loss: 0.05257787\n",
      "Step: [2337] total_loss: 2.13199759 d_loss: 1.36141086, g_loss: 0.71805286, ae_loss: 0.05253369\n",
      "Step: [2338] total_loss: 2.13198376 d_loss: 1.36675799, g_loss: 0.71680474, ae_loss: 0.04842114\n",
      "Step: [2339] total_loss: 2.13814116 d_loss: 1.35689771, g_loss: 0.72268438, ae_loss: 0.05855897\n",
      "Step: [2340] total_loss: 2.14011288 d_loss: 1.39373815, g_loss: 0.69451070, ae_loss: 0.05186389\n",
      "Step: [2341] total_loss: 2.14028287 d_loss: 1.36884356, g_loss: 0.71411878, ae_loss: 0.05732058\n",
      "Step: [2342] total_loss: 2.13780165 d_loss: 1.36858964, g_loss: 0.71659154, ae_loss: 0.05262048\n",
      "Step: [2343] total_loss: 2.11141586 d_loss: 1.34965026, g_loss: 0.70957088, ae_loss: 0.05219463\n",
      "Step: [2344] total_loss: 2.13669086 d_loss: 1.39703560, g_loss: 0.68361461, ae_loss: 0.05604062\n",
      "Step: [2345] total_loss: 2.11532640 d_loss: 1.36913717, g_loss: 0.69011641, ae_loss: 0.05607283\n",
      "Step: [2346] total_loss: 2.13430452 d_loss: 1.38168073, g_loss: 0.69804341, ae_loss: 0.05458040\n",
      "Step: [2347] total_loss: 2.11096692 d_loss: 1.36863899, g_loss: 0.68788004, ae_loss: 0.05444793\n",
      "Step: [2348] total_loss: 2.11590862 d_loss: 1.37698138, g_loss: 0.68493879, ae_loss: 0.05398851\n",
      "Step: [2349] total_loss: 2.11643910 d_loss: 1.38019335, g_loss: 0.68122137, ae_loss: 0.05502436\n",
      "Step: [2350] total_loss: 2.11074734 d_loss: 1.37404394, g_loss: 0.68794870, ae_loss: 0.04875482\n",
      "Step: [2351] total_loss: 2.13951564 d_loss: 1.39512944, g_loss: 0.68824255, ae_loss: 0.05614366\n",
      "Step: [2352] total_loss: 2.11152935 d_loss: 1.36801434, g_loss: 0.69426680, ae_loss: 0.04924829\n",
      "Step: [2353] total_loss: 2.13664913 d_loss: 1.37677550, g_loss: 0.70864999, ae_loss: 0.05122377\n",
      "Step: [2354] total_loss: 2.14029360 d_loss: 1.36861336, g_loss: 0.71759361, ae_loss: 0.05408657\n",
      "Step: [2355] total_loss: 2.13683987 d_loss: 1.38773239, g_loss: 0.69776368, ae_loss: 0.05134375\n",
      "Step: [2356] total_loss: 2.12698007 d_loss: 1.38052762, g_loss: 0.69301951, ae_loss: 0.05343297\n",
      "Step: [2357] total_loss: 2.12812853 d_loss: 1.37463784, g_loss: 0.70240283, ae_loss: 0.05108802\n",
      "Step: [2358] total_loss: 2.11575222 d_loss: 1.37520957, g_loss: 0.68922639, ae_loss: 0.05131642\n",
      "Step: [2359] total_loss: 2.13704538 d_loss: 1.39232945, g_loss: 0.69164014, ae_loss: 0.05307571\n",
      "Step: [2360] total_loss: 2.12132096 d_loss: 1.38543797, g_loss: 0.68308723, ae_loss: 0.05279573\n",
      "Step: [2361] total_loss: 2.12935162 d_loss: 1.38782060, g_loss: 0.68766809, ae_loss: 0.05386278\n",
      "Step: [2362] total_loss: 2.12182856 d_loss: 1.38570833, g_loss: 0.68339264, ae_loss: 0.05272758\n",
      "Step: [2363] total_loss: 2.14805079 d_loss: 1.40274835, g_loss: 0.69249701, ae_loss: 0.05280533\n",
      "Step: [2364] total_loss: 2.13437128 d_loss: 1.38967538, g_loss: 0.69242430, ae_loss: 0.05227170\n",
      "Step: [2365] total_loss: 2.13043571 d_loss: 1.38920307, g_loss: 0.69175982, ae_loss: 0.04947284\n",
      "Step: [2366] total_loss: 2.12410069 d_loss: 1.35571229, g_loss: 0.71869689, ae_loss: 0.04969154\n",
      "Step: [2367] total_loss: 2.11721230 d_loss: 1.36475945, g_loss: 0.70070016, ae_loss: 0.05175278\n",
      "Step: [2368] total_loss: 2.12921119 d_loss: 1.37809873, g_loss: 0.69866025, ae_loss: 0.05245223\n",
      "Step: [2369] total_loss: 2.12352681 d_loss: 1.39184499, g_loss: 0.68314910, ae_loss: 0.04853268\n",
      "Step: [2370] total_loss: 2.14599705 d_loss: 1.40256727, g_loss: 0.69239271, ae_loss: 0.05103690\n",
      "Step: [2371] total_loss: 2.11290359 d_loss: 1.36439085, g_loss: 0.69550323, ae_loss: 0.05300967\n",
      "Step: [2372] total_loss: 2.12855148 d_loss: 1.38613224, g_loss: 0.69237697, ae_loss: 0.05004244\n",
      "Step: [2373] total_loss: 2.13677454 d_loss: 1.38742375, g_loss: 0.69798684, ae_loss: 0.05136395\n",
      "Step: [2374] total_loss: 2.12519360 d_loss: 1.37639546, g_loss: 0.69620657, ae_loss: 0.05259164\n",
      "Step: [2375] total_loss: 2.10833836 d_loss: 1.36512244, g_loss: 0.69371700, ae_loss: 0.04949886\n",
      "Step: [2376] total_loss: 2.13867903 d_loss: 1.39069319, g_loss: 0.69424582, ae_loss: 0.05374004\n",
      "Step: [2377] total_loss: 2.10995054 d_loss: 1.38113964, g_loss: 0.67754412, ae_loss: 0.05126679\n",
      "Step: [2378] total_loss: 2.10152769 d_loss: 1.36531854, g_loss: 0.68379486, ae_loss: 0.05241432\n",
      "Step: [2379] total_loss: 2.11494589 d_loss: 1.38040042, g_loss: 0.68151194, ae_loss: 0.05303362\n",
      "Step: [2380] total_loss: 2.13004708 d_loss: 1.39384055, g_loss: 0.68896884, ae_loss: 0.04723758\n",
      "Step: [2381] total_loss: 2.11380506 d_loss: 1.37863553, g_loss: 0.68347543, ae_loss: 0.05169417\n",
      "Step: [2382] total_loss: 2.12352943 d_loss: 1.37622750, g_loss: 0.69346362, ae_loss: 0.05383822\n",
      "Step: [2383] total_loss: 2.14832187 d_loss: 1.38481987, g_loss: 0.71462965, ae_loss: 0.04887230\n",
      "Step: [2384] total_loss: 2.13374996 d_loss: 1.39892650, g_loss: 0.67889971, ae_loss: 0.05592386\n",
      "Step: [2385] total_loss: 2.12364459 d_loss: 1.36117733, g_loss: 0.71118516, ae_loss: 0.05128200\n",
      "Step: [2386] total_loss: 2.12217879 d_loss: 1.37993813, g_loss: 0.68531656, ae_loss: 0.05692407\n",
      "Step: [2387] total_loss: 2.11847663 d_loss: 1.37444806, g_loss: 0.69028854, ae_loss: 0.05374005\n",
      "Step: [2388] total_loss: 2.13420725 d_loss: 1.41445291, g_loss: 0.66838217, ae_loss: 0.05137209\n",
      "Step: [2389] total_loss: 2.12808943 d_loss: 1.37899160, g_loss: 0.69240266, ae_loss: 0.05669524\n",
      "Step: [2390] total_loss: 2.13411880 d_loss: 1.37387049, g_loss: 0.70838094, ae_loss: 0.05186732\n",
      "Step: [2391] total_loss: 2.13024282 d_loss: 1.39140844, g_loss: 0.68790269, ae_loss: 0.05093176\n",
      "Step: [2392] total_loss: 2.11654902 d_loss: 1.34517705, g_loss: 0.71527475, ae_loss: 0.05609732\n",
      "Step: [2393] total_loss: 2.13465858 d_loss: 1.40554798, g_loss: 0.67674118, ae_loss: 0.05236937\n",
      "Step: [2394] total_loss: 2.11118793 d_loss: 1.33407855, g_loss: 0.72187769, ae_loss: 0.05523182\n",
      "Step: [2395] total_loss: 2.16826963 d_loss: 1.36918974, g_loss: 0.74315524, ae_loss: 0.05592461\n",
      "Step: [2396] total_loss: 2.12413073 d_loss: 1.37584114, g_loss: 0.69339776, ae_loss: 0.05489166\n",
      "Step: [2397] total_loss: 2.15655994 d_loss: 1.37363994, g_loss: 0.72904301, ae_loss: 0.05387693\n",
      "Step: [2398] total_loss: 2.10645962 d_loss: 1.35940528, g_loss: 0.69407701, ae_loss: 0.05297745\n",
      "Step: [2399] total_loss: 2.10638237 d_loss: 1.38248146, g_loss: 0.67111468, ae_loss: 0.05278639\n",
      "Step: [2400] total_loss: 2.12813997 d_loss: 1.37777865, g_loss: 0.69806540, ae_loss: 0.05229591\n",
      "Step: [2401] total_loss: 2.11287045 d_loss: 1.37793839, g_loss: 0.68095684, ae_loss: 0.05397523\n",
      "Step: [2402] total_loss: 2.13253307 d_loss: 1.38466978, g_loss: 0.69746101, ae_loss: 0.05040226\n",
      "Step: [2403] total_loss: 2.11789322 d_loss: 1.39025521, g_loss: 0.67371505, ae_loss: 0.05392294\n",
      "Step: [2404] total_loss: 2.13780308 d_loss: 1.40718079, g_loss: 0.67893326, ae_loss: 0.05168891\n",
      "Step: [2405] total_loss: 2.13933969 d_loss: 1.38546479, g_loss: 0.70267898, ae_loss: 0.05119601\n",
      "Step: [2406] total_loss: 2.12304783 d_loss: 1.37559175, g_loss: 0.69777423, ae_loss: 0.04968179\n",
      "Step: [2407] total_loss: 2.12983322 d_loss: 1.38357985, g_loss: 0.69744658, ae_loss: 0.04880683\n",
      "Step: [2408] total_loss: 2.14128971 d_loss: 1.38844728, g_loss: 0.69688523, ae_loss: 0.05595705\n",
      "Step: [2409] total_loss: 2.14005184 d_loss: 1.38164866, g_loss: 0.70830965, ae_loss: 0.05009348\n",
      "Step: [2410] total_loss: 2.12924123 d_loss: 1.36934912, g_loss: 0.70508736, ae_loss: 0.05480477\n",
      "Step: [2411] total_loss: 2.13149548 d_loss: 1.37113571, g_loss: 0.70736897, ae_loss: 0.05299085\n",
      "Step: [2412] total_loss: 2.10777497 d_loss: 1.36875677, g_loss: 0.68563718, ae_loss: 0.05338097\n",
      "Step: [2413] total_loss: 2.09863758 d_loss: 1.36982501, g_loss: 0.67536259, ae_loss: 0.05344987\n",
      "Step: [2414] total_loss: 2.12823534 d_loss: 1.40278363, g_loss: 0.67698240, ae_loss: 0.04846938\n",
      "Step: [2415] total_loss: 2.15774107 d_loss: 1.41074848, g_loss: 0.69744706, ae_loss: 0.04954551\n",
      "Step: [2416] total_loss: 2.14319658 d_loss: 1.39629424, g_loss: 0.69462532, ae_loss: 0.05227708\n",
      "Step: [2417] total_loss: 2.13523793 d_loss: 1.39915133, g_loss: 0.68401164, ae_loss: 0.05207502\n",
      "Step: [2418] total_loss: 2.11357236 d_loss: 1.36848724, g_loss: 0.69308788, ae_loss: 0.05199732\n",
      "Step: [2419] total_loss: 2.12055540 d_loss: 1.38335764, g_loss: 0.68548822, ae_loss: 0.05170957\n",
      "Step: [2420] total_loss: 2.10926867 d_loss: 1.37954021, g_loss: 0.67808580, ae_loss: 0.05164248\n",
      "Step: [2421] total_loss: 2.12479877 d_loss: 1.38088167, g_loss: 0.69140100, ae_loss: 0.05251625\n",
      "Step: [2422] total_loss: 2.11147666 d_loss: 1.37021017, g_loss: 0.68933260, ae_loss: 0.05193389\n",
      "Step: [2423] total_loss: 2.13907886 d_loss: 1.38603902, g_loss: 0.70353889, ae_loss: 0.04950090\n",
      "Step: [2424] total_loss: 2.14084244 d_loss: 1.39034462, g_loss: 0.69857615, ae_loss: 0.05192161\n",
      "Step: [2425] total_loss: 2.12762260 d_loss: 1.37967157, g_loss: 0.69346976, ae_loss: 0.05448128\n",
      "Step: [2426] total_loss: 2.11771059 d_loss: 1.38210547, g_loss: 0.68185866, ae_loss: 0.05374629\n",
      "Step: [2427] total_loss: 2.12299824 d_loss: 1.39011431, g_loss: 0.68236375, ae_loss: 0.05052034\n",
      "Step: [2428] total_loss: 2.13085365 d_loss: 1.40571809, g_loss: 0.67488003, ae_loss: 0.05025550\n",
      "Step: [2429] total_loss: 2.09807849 d_loss: 1.36424565, g_loss: 0.67883188, ae_loss: 0.05500089\n",
      "Step: [2430] total_loss: 2.11846495 d_loss: 1.38705671, g_loss: 0.67770535, ae_loss: 0.05370298\n",
      "Step: [2431] total_loss: 2.12190819 d_loss: 1.39562869, g_loss: 0.67443287, ae_loss: 0.05184644\n",
      "Step: [2432] total_loss: 2.10321021 d_loss: 1.36855054, g_loss: 0.68281537, ae_loss: 0.05184435\n",
      "Step: [2433] total_loss: 2.12538695 d_loss: 1.37029147, g_loss: 0.70050997, ae_loss: 0.05458546\n",
      "Step: [2434] total_loss: 2.12076116 d_loss: 1.38883245, g_loss: 0.68223017, ae_loss: 0.04969862\n",
      "Step: [2435] total_loss: 2.12825656 d_loss: 1.39706588, g_loss: 0.67762923, ae_loss: 0.05356140\n",
      "Step: [2436] total_loss: 2.12382698 d_loss: 1.38561511, g_loss: 0.68530905, ae_loss: 0.05290265\n",
      "Step: [2437] total_loss: 2.13046718 d_loss: 1.37301672, g_loss: 0.71092731, ae_loss: 0.04652322\n",
      "Step: [2438] total_loss: 2.13376641 d_loss: 1.39036238, g_loss: 0.68975580, ae_loss: 0.05364822\n",
      "Step: [2439] total_loss: 2.13064170 d_loss: 1.38626981, g_loss: 0.69136393, ae_loss: 0.05300793\n",
      "Step: [2440] total_loss: 2.10757017 d_loss: 1.36588240, g_loss: 0.68755019, ae_loss: 0.05413756\n",
      "Step: [2441] total_loss: 2.11891222 d_loss: 1.37949204, g_loss: 0.68939829, ae_loss: 0.05002195\n",
      "Step: [2442] total_loss: 2.11555219 d_loss: 1.37522364, g_loss: 0.68694061, ae_loss: 0.05338795\n",
      "Step: [2443] total_loss: 2.11859846 d_loss: 1.38405681, g_loss: 0.68398136, ae_loss: 0.05056029\n",
      "Step: [2444] total_loss: 2.12071896 d_loss: 1.35450912, g_loss: 0.71362859, ae_loss: 0.05258123\n",
      "Step: [2445] total_loss: 2.13672423 d_loss: 1.38908851, g_loss: 0.69433248, ae_loss: 0.05330329\n",
      "Step: [2446] total_loss: 2.12285328 d_loss: 1.37158537, g_loss: 0.69996542, ae_loss: 0.05130240\n",
      "Step: [2447] total_loss: 2.12124681 d_loss: 1.37480044, g_loss: 0.69260955, ae_loss: 0.05383684\n",
      "Step: [2448] total_loss: 2.11105704 d_loss: 1.37157512, g_loss: 0.68608868, ae_loss: 0.05339325\n",
      "Step: [2449] total_loss: 2.10827971 d_loss: 1.36824369, g_loss: 0.68575490, ae_loss: 0.05428098\n",
      "Step: [2450] total_loss: 2.12677288 d_loss: 1.40101874, g_loss: 0.67552334, ae_loss: 0.05023082\n",
      "Step: [2451] total_loss: 2.12217212 d_loss: 1.38477492, g_loss: 0.68597502, ae_loss: 0.05142223\n",
      "Step: [2452] total_loss: 2.12476969 d_loss: 1.37059927, g_loss: 0.70656645, ae_loss: 0.04760382\n",
      "Step: [2453] total_loss: 2.13459587 d_loss: 1.38496208, g_loss: 0.70067930, ae_loss: 0.04895433\n",
      "Step: [2454] total_loss: 2.13680530 d_loss: 1.38102245, g_loss: 0.70448083, ae_loss: 0.05130205\n",
      "Step: [2455] total_loss: 2.14749002 d_loss: 1.39277065, g_loss: 0.70307606, ae_loss: 0.05164338\n",
      "Step: [2456] total_loss: 2.14039755 d_loss: 1.36234295, g_loss: 0.72844303, ae_loss: 0.04961170\n",
      "Step: [2457] total_loss: 2.13614798 d_loss: 1.37669992, g_loss: 0.70882297, ae_loss: 0.05062494\n",
      "Step: [2458] total_loss: 2.10918951 d_loss: 1.35936356, g_loss: 0.69367707, ae_loss: 0.05614899\n",
      "Step: [2459] total_loss: 2.14036727 d_loss: 1.40105891, g_loss: 0.68337196, ae_loss: 0.05593640\n",
      "Step: [2460] total_loss: 2.12638044 d_loss: 1.38090789, g_loss: 0.69306487, ae_loss: 0.05240773\n",
      "Step: [2461] total_loss: 2.14305258 d_loss: 1.38645005, g_loss: 0.70561022, ae_loss: 0.05099237\n",
      "Step: [2462] total_loss: 2.11764336 d_loss: 1.39195919, g_loss: 0.67490608, ae_loss: 0.05077799\n",
      "Step: [2463] total_loss: 2.13025308 d_loss: 1.39615321, g_loss: 0.68117458, ae_loss: 0.05292525\n",
      "Step: [2464] total_loss: 2.12150359 d_loss: 1.36782610, g_loss: 0.70408493, ae_loss: 0.04959260\n",
      "Step: [2465] total_loss: 2.14670467 d_loss: 1.41074753, g_loss: 0.68406153, ae_loss: 0.05189553\n",
      "Step: [2466] total_loss: 2.11414528 d_loss: 1.36472607, g_loss: 0.70054799, ae_loss: 0.04887123\n",
      "Step: [2467] total_loss: 2.11234474 d_loss: 1.35123289, g_loss: 0.70786422, ae_loss: 0.05324754\n",
      "Step: [2468] total_loss: 2.11608624 d_loss: 1.37557244, g_loss: 0.68912828, ae_loss: 0.05138556\n",
      "Step: [2469] total_loss: 2.11025810 d_loss: 1.36219907, g_loss: 0.69500554, ae_loss: 0.05305346\n",
      "Step: [2470] total_loss: 2.10504818 d_loss: 1.37337911, g_loss: 0.68190444, ae_loss: 0.04976481\n",
      "Step: [2471] total_loss: 2.13649511 d_loss: 1.40866792, g_loss: 0.67792249, ae_loss: 0.04990470\n",
      "Step: [2472] total_loss: 2.10999584 d_loss: 1.37129164, g_loss: 0.69301790, ae_loss: 0.04568633\n",
      "Step: [2473] total_loss: 2.13956523 d_loss: 1.37846458, g_loss: 0.71044374, ae_loss: 0.05065693\n",
      "Step: [2474] total_loss: 2.11665177 d_loss: 1.36064386, g_loss: 0.70304114, ae_loss: 0.05296689\n",
      "Step: [2475] total_loss: 2.11309195 d_loss: 1.34417129, g_loss: 0.71368980, ae_loss: 0.05523089\n",
      "Step: [2476] total_loss: 2.12980294 d_loss: 1.39464188, g_loss: 0.68392617, ae_loss: 0.05123488\n",
      "Step: [2477] total_loss: 2.11494231 d_loss: 1.37435579, g_loss: 0.69419926, ae_loss: 0.04638730\n",
      "Step: [2478] total_loss: 2.08732128 d_loss: 1.35158539, g_loss: 0.68905228, ae_loss: 0.04668355\n",
      "Step: [2479] total_loss: 2.12214446 d_loss: 1.37977099, g_loss: 0.69034696, ae_loss: 0.05202654\n",
      "Step: [2480] total_loss: 2.12734699 d_loss: 1.38612866, g_loss: 0.69248366, ae_loss: 0.04873477\n",
      "Step: [2481] total_loss: 2.10381913 d_loss: 1.37162888, g_loss: 0.67501944, ae_loss: 0.05717080\n",
      "Step: [2482] total_loss: 2.12997580 d_loss: 1.37992620, g_loss: 0.69277668, ae_loss: 0.05727296\n",
      "Step: [2483] total_loss: 2.14364147 d_loss: 1.36331177, g_loss: 0.72963762, ae_loss: 0.05069225\n",
      "Step: [2484] total_loss: 2.12497306 d_loss: 1.36832476, g_loss: 0.70306534, ae_loss: 0.05358303\n",
      "Step: [2485] total_loss: 2.10136080 d_loss: 1.36349297, g_loss: 0.68925881, ae_loss: 0.04860889\n",
      "Step: [2486] total_loss: 2.10767484 d_loss: 1.37138772, g_loss: 0.68846387, ae_loss: 0.04782316\n",
      "Step: [2487] total_loss: 2.11310983 d_loss: 1.38513136, g_loss: 0.67564768, ae_loss: 0.05233086\n",
      "Step: [2488] total_loss: 2.12769175 d_loss: 1.39346290, g_loss: 0.68186235, ae_loss: 0.05236640\n",
      "Step: [2489] total_loss: 2.11374664 d_loss: 1.38074279, g_loss: 0.68191230, ae_loss: 0.05109154\n",
      "Step: [2490] total_loss: 2.13052845 d_loss: 1.37323189, g_loss: 0.70696253, ae_loss: 0.05033402\n",
      "Step: [2491] total_loss: 2.13786793 d_loss: 1.40440273, g_loss: 0.68071330, ae_loss: 0.05275182\n",
      "Step: [2492] total_loss: 2.11542892 d_loss: 1.37080133, g_loss: 0.69495314, ae_loss: 0.04967445\n",
      "Step: [2493] total_loss: 2.14477205 d_loss: 1.40172875, g_loss: 0.69102275, ae_loss: 0.05202041\n",
      "Step: [2494] total_loss: 2.13614988 d_loss: 1.36526239, g_loss: 0.71414375, ae_loss: 0.05674378\n",
      "Step: [2495] total_loss: 2.13941979 d_loss: 1.39320230, g_loss: 0.69544435, ae_loss: 0.05077320\n",
      "Step: [2496] total_loss: 2.12284398 d_loss: 1.35904253, g_loss: 0.71195376, ae_loss: 0.05184767\n",
      "Step: [2497] total_loss: 2.11127901 d_loss: 1.35039330, g_loss: 0.70636213, ae_loss: 0.05452358\n",
      "Step: [2498] total_loss: 2.10992384 d_loss: 1.36481500, g_loss: 0.69257081, ae_loss: 0.05253799\n",
      "Step: [2499] total_loss: 2.11681366 d_loss: 1.38083792, g_loss: 0.68764150, ae_loss: 0.04833427\n",
      "Step: [2500] total_loss: 2.14096355 d_loss: 1.36938798, g_loss: 0.71799064, ae_loss: 0.05358504\n",
      "Step: [2501] total_loss: 2.13116670 d_loss: 1.37529647, g_loss: 0.70338601, ae_loss: 0.05248425\n",
      "Step: [2502] total_loss: 2.12687755 d_loss: 1.39287782, g_loss: 0.67922294, ae_loss: 0.05477683\n",
      "Step: [2503] total_loss: 2.16827059 d_loss: 1.39738250, g_loss: 0.71863055, ae_loss: 0.05225768\n",
      "Step: [2504] total_loss: 2.11490679 d_loss: 1.35952914, g_loss: 0.70148629, ae_loss: 0.05389130\n",
      "Step: [2505] total_loss: 2.10360432 d_loss: 1.38442326, g_loss: 0.66627681, ae_loss: 0.05290413\n",
      "Step: [2506] total_loss: 2.12103176 d_loss: 1.39832187, g_loss: 0.67048109, ae_loss: 0.05222873\n",
      "Step: [2507] total_loss: 2.13796735 d_loss: 1.40588915, g_loss: 0.67967933, ae_loss: 0.05239897\n",
      "Step: [2508] total_loss: 2.13436222 d_loss: 1.39005184, g_loss: 0.69289875, ae_loss: 0.05141146\n",
      "Step: [2509] total_loss: 2.13047218 d_loss: 1.37995386, g_loss: 0.69794476, ae_loss: 0.05257368\n",
      "Step: [2510] total_loss: 2.12673974 d_loss: 1.38062227, g_loss: 0.69521183, ae_loss: 0.05090572\n",
      "Step: [2511] total_loss: 2.13268328 d_loss: 1.36960053, g_loss: 0.71405083, ae_loss: 0.04903185\n",
      "Step: [2512] total_loss: 2.14576101 d_loss: 1.36565351, g_loss: 0.73177755, ae_loss: 0.04833008\n",
      "Step: [2513] total_loss: 2.13056159 d_loss: 1.37408876, g_loss: 0.71120864, ae_loss: 0.04526429\n",
      "Step: [2514] total_loss: 2.11831021 d_loss: 1.36236632, g_loss: 0.70209569, ae_loss: 0.05384811\n",
      "Step: [2515] total_loss: 2.14086723 d_loss: 1.39579332, g_loss: 0.69395310, ae_loss: 0.05112071\n",
      "Step: [2516] total_loss: 2.11983728 d_loss: 1.35693920, g_loss: 0.70998275, ae_loss: 0.05291522\n",
      "Step: [2517] total_loss: 2.12070489 d_loss: 1.40208256, g_loss: 0.67028701, ae_loss: 0.04833536\n",
      "Step: [2518] total_loss: 2.10661936 d_loss: 1.37820101, g_loss: 0.67336786, ae_loss: 0.05505036\n",
      "Step: [2519] total_loss: 2.11946511 d_loss: 1.37434995, g_loss: 0.69410586, ae_loss: 0.05100927\n",
      "Step: [2520] total_loss: 2.10801315 d_loss: 1.37913680, g_loss: 0.67805821, ae_loss: 0.05081813\n",
      "Step: [2521] total_loss: 2.11407161 d_loss: 1.38734663, g_loss: 0.67802119, ae_loss: 0.04870380\n",
      "Step: [2522] total_loss: 2.10841227 d_loss: 1.36903203, g_loss: 0.68670571, ae_loss: 0.05267460\n",
      "Step: [2523] total_loss: 2.10799313 d_loss: 1.36556768, g_loss: 0.69079208, ae_loss: 0.05163337\n",
      "Step: [2524] total_loss: 2.11359262 d_loss: 1.36922491, g_loss: 0.69497621, ae_loss: 0.04939169\n",
      "Step: [2525] total_loss: 2.11567020 d_loss: 1.36568511, g_loss: 0.69854271, ae_loss: 0.05144245\n",
      "Step: [2526] total_loss: 2.12626362 d_loss: 1.36891341, g_loss: 0.70892376, ae_loss: 0.04842655\n",
      "Step: [2527] total_loss: 2.13940859 d_loss: 1.36949015, g_loss: 0.72009683, ae_loss: 0.04982152\n",
      "Step: [2528] total_loss: 2.13982296 d_loss: 1.38649559, g_loss: 0.69894171, ae_loss: 0.05438572\n",
      "Step: [2529] total_loss: 2.12142348 d_loss: 1.38434780, g_loss: 0.68461400, ae_loss: 0.05246180\n",
      "Step: [2530] total_loss: 2.13257694 d_loss: 1.40825748, g_loss: 0.67502457, ae_loss: 0.04929484\n",
      "Step: [2531] total_loss: 2.12046099 d_loss: 1.38523829, g_loss: 0.68641734, ae_loss: 0.04880521\n",
      "Step: [2532] total_loss: 2.12278461 d_loss: 1.37576413, g_loss: 0.69893181, ae_loss: 0.04808864\n",
      "Step: [2533] total_loss: 2.10646391 d_loss: 1.37349308, g_loss: 0.68163514, ae_loss: 0.05133562\n",
      "Step: [2534] total_loss: 2.11542201 d_loss: 1.37392032, g_loss: 0.68694592, ae_loss: 0.05455582\n",
      "Step: [2535] total_loss: 2.14580584 d_loss: 1.38787782, g_loss: 0.70272291, ae_loss: 0.05520493\n",
      "Step: [2536] total_loss: 2.11586452 d_loss: 1.38219762, g_loss: 0.68704450, ae_loss: 0.04662237\n",
      "Step: [2537] total_loss: 2.13299274 d_loss: 1.38701653, g_loss: 0.69178736, ae_loss: 0.05418900\n",
      "Step: [2538] total_loss: 2.12813759 d_loss: 1.38740849, g_loss: 0.68733585, ae_loss: 0.05339327\n",
      "Step: [2539] total_loss: 2.13701987 d_loss: 1.38531756, g_loss: 0.70125347, ae_loss: 0.05044883\n",
      "Step: [2540] total_loss: 2.12957072 d_loss: 1.38000965, g_loss: 0.69826102, ae_loss: 0.05130001\n",
      "Step: [2541] total_loss: 2.12465429 d_loss: 1.39136267, g_loss: 0.68087643, ae_loss: 0.05241517\n",
      "Step: [2542] total_loss: 2.10702491 d_loss: 1.37556505, g_loss: 0.67865860, ae_loss: 0.05280123\n",
      "Step: [2543] total_loss: 2.13403869 d_loss: 1.39283717, g_loss: 0.69232166, ae_loss: 0.04887991\n",
      "Step: [2544] total_loss: 2.09552598 d_loss: 1.34455681, g_loss: 0.69806397, ae_loss: 0.05290518\n",
      "Step: [2545] total_loss: 2.13765550 d_loss: 1.39764285, g_loss: 0.68757194, ae_loss: 0.05244075\n",
      "Step: [2546] total_loss: 2.12132049 d_loss: 1.37425208, g_loss: 0.69792289, ae_loss: 0.04914562\n",
      "Step: [2547] total_loss: 2.12863040 d_loss: 1.37691712, g_loss: 0.70625478, ae_loss: 0.04545848\n",
      "Step: [2548] total_loss: 2.12639546 d_loss: 1.37257481, g_loss: 0.70270830, ae_loss: 0.05111241\n",
      "Step: [2549] total_loss: 2.14030218 d_loss: 1.38691556, g_loss: 0.70097750, ae_loss: 0.05240906\n",
      "Step: [2550] total_loss: 2.14867353 d_loss: 1.38719678, g_loss: 0.71220267, ae_loss: 0.04927412\n",
      "Step: [2551] total_loss: 2.11788464 d_loss: 1.37858570, g_loss: 0.68378854, ae_loss: 0.05551032\n",
      "Step: [2552] total_loss: 2.11870956 d_loss: 1.37420106, g_loss: 0.69526780, ae_loss: 0.04924070\n",
      "Step: [2553] total_loss: 2.12292099 d_loss: 1.35556817, g_loss: 0.71546638, ae_loss: 0.05188652\n",
      "Step: [2554] total_loss: 2.12519646 d_loss: 1.37986422, g_loss: 0.69146788, ae_loss: 0.05386429\n",
      "Step: [2555] total_loss: 2.10746121 d_loss: 1.37817764, g_loss: 0.67472905, ae_loss: 0.05455450\n",
      "Step: [2556] total_loss: 2.10307598 d_loss: 1.38012779, g_loss: 0.67314386, ae_loss: 0.04980422\n",
      "Step: [2557] total_loss: 2.12358713 d_loss: 1.38840556, g_loss: 0.68267357, ae_loss: 0.05250805\n",
      "Step: [2558] total_loss: 2.09408450 d_loss: 1.36642873, g_loss: 0.67994022, ae_loss: 0.04771555\n",
      "Step: [2559] total_loss: 2.12785912 d_loss: 1.37201500, g_loss: 0.70399892, ae_loss: 0.05184528\n",
      "Step: [2560] total_loss: 2.09603071 d_loss: 1.33538771, g_loss: 0.71027702, ae_loss: 0.05036587\n",
      "Step: [2561] total_loss: 2.13047624 d_loss: 1.38173151, g_loss: 0.69686443, ae_loss: 0.05188037\n",
      "Step: [2562] total_loss: 2.18049622 d_loss: 1.41910017, g_loss: 0.71114576, ae_loss: 0.05025040\n",
      "Step: [2563] total_loss: 2.12376213 d_loss: 1.36777186, g_loss: 0.70262867, ae_loss: 0.05336166\n",
      "Step: [2564] total_loss: 2.11696672 d_loss: 1.38279295, g_loss: 0.68107128, ae_loss: 0.05310233\n",
      "Step: [2565] total_loss: 2.11158514 d_loss: 1.37704110, g_loss: 0.68301117, ae_loss: 0.05153273\n",
      "Step: [2566] total_loss: 2.12883258 d_loss: 1.35948300, g_loss: 0.71628970, ae_loss: 0.05305980\n",
      "Step: [2567] total_loss: 2.12149763 d_loss: 1.37363744, g_loss: 0.69199806, ae_loss: 0.05586209\n",
      "Step: [2568] total_loss: 2.13974524 d_loss: 1.38884163, g_loss: 0.69536090, ae_loss: 0.05554285\n",
      "Step: [2569] total_loss: 2.12065053 d_loss: 1.37598729, g_loss: 0.69344062, ae_loss: 0.05122253\n",
      "Step: [2570] total_loss: 2.16635084 d_loss: 1.41695690, g_loss: 0.69854128, ae_loss: 0.05085261\n",
      "Step: [2571] total_loss: 2.13784361 d_loss: 1.38975477, g_loss: 0.69640052, ae_loss: 0.05168847\n",
      "Step: [2572] total_loss: 2.14408112 d_loss: 1.34939456, g_loss: 0.74669933, ae_loss: 0.04798732\n",
      "Step: [2573] total_loss: 2.12983489 d_loss: 1.40184307, g_loss: 0.67165118, ae_loss: 0.05634053\n",
      "Step: [2574] total_loss: 2.13048291 d_loss: 1.40780497, g_loss: 0.66612691, ae_loss: 0.05655104\n",
      "Step: [2575] total_loss: 2.16940498 d_loss: 1.40569818, g_loss: 0.70941973, ae_loss: 0.05428709\n",
      "Step: [2576] total_loss: 2.10847497 d_loss: 1.36636388, g_loss: 0.68909538, ae_loss: 0.05301575\n",
      "Step: [2577] total_loss: 2.10315371 d_loss: 1.35717797, g_loss: 0.69285458, ae_loss: 0.05312107\n",
      "Step: [2578] total_loss: 2.08899736 d_loss: 1.35901821, g_loss: 0.68406421, ae_loss: 0.04591488\n",
      "Step: [2579] total_loss: 2.13997793 d_loss: 1.34865558, g_loss: 0.73842263, ae_loss: 0.05289969\n",
      "Step: [2580] total_loss: 2.12843657 d_loss: 1.38666582, g_loss: 0.69199663, ae_loss: 0.04977409\n",
      "Step: [2581] total_loss: 2.15586257 d_loss: 1.40062928, g_loss: 0.70182455, ae_loss: 0.05340879\n",
      "Step: [2582] total_loss: 2.13329601 d_loss: 1.39242709, g_loss: 0.69043452, ae_loss: 0.05043432\n",
      "Step: [2583] total_loss: 2.12275362 d_loss: 1.38104939, g_loss: 0.69051981, ae_loss: 0.05118433\n",
      "Step: [2584] total_loss: 2.14214444 d_loss: 1.40372586, g_loss: 0.68604267, ae_loss: 0.05237594\n",
      "Step: [2585] total_loss: 2.14562321 d_loss: 1.40895224, g_loss: 0.68827820, ae_loss: 0.04839266\n",
      "Step: [2586] total_loss: 2.14541435 d_loss: 1.39541984, g_loss: 0.69626474, ae_loss: 0.05372976\n",
      "Step: [2587] total_loss: 2.13203287 d_loss: 1.40539348, g_loss: 0.67198741, ae_loss: 0.05465204\n",
      "Step: [2588] total_loss: 2.10661077 d_loss: 1.37789249, g_loss: 0.68068016, ae_loss: 0.04803823\n",
      "Step: [2589] total_loss: 2.13305044 d_loss: 1.37504578, g_loss: 0.70621657, ae_loss: 0.05178817\n",
      "Step: [2590] total_loss: 2.12131071 d_loss: 1.36490941, g_loss: 0.70325637, ae_loss: 0.05314503\n",
      "Step: [2591] total_loss: 2.10366702 d_loss: 1.37321770, g_loss: 0.67948335, ae_loss: 0.05096589\n",
      "Step: [2592] total_loss: 2.12513852 d_loss: 1.39301908, g_loss: 0.68032950, ae_loss: 0.05179000\n",
      "Step: [2593] total_loss: 2.12156582 d_loss: 1.38690138, g_loss: 0.68602216, ae_loss: 0.04864230\n",
      "Step: [2594] total_loss: 2.13989449 d_loss: 1.40559196, g_loss: 0.68411708, ae_loss: 0.05018561\n",
      "Step: [2595] total_loss: 2.14856029 d_loss: 1.38760698, g_loss: 0.71290076, ae_loss: 0.04805255\n",
      "Step: [2596] total_loss: 2.12297916 d_loss: 1.37981629, g_loss: 0.69092607, ae_loss: 0.05223686\n",
      "Step: [2597] total_loss: 2.13182044 d_loss: 1.38843369, g_loss: 0.69463122, ae_loss: 0.04875551\n",
      "Step: [2598] total_loss: 2.10470772 d_loss: 1.37119448, g_loss: 0.68557358, ae_loss: 0.04793949\n",
      "Step: [2599] total_loss: 2.16701651 d_loss: 1.42510509, g_loss: 0.69102955, ae_loss: 0.05088193\n",
      "Step: [2600] total_loss: 2.12922978 d_loss: 1.39492249, g_loss: 0.68579799, ae_loss: 0.04850924\n",
      "Step: [2601] total_loss: 2.14541149 d_loss: 1.39583158, g_loss: 0.69595379, ae_loss: 0.05362603\n",
      "Step: [2602] total_loss: 2.14661551 d_loss: 1.37742317, g_loss: 0.72026241, ae_loss: 0.04892991\n",
      "Step: [2603] total_loss: 2.11667061 d_loss: 1.36935377, g_loss: 0.69658637, ae_loss: 0.05073060\n",
      "Step: [2604] total_loss: 2.14540195 d_loss: 1.40242863, g_loss: 0.68832016, ae_loss: 0.05465328\n",
      "Step: [2605] total_loss: 2.13158798 d_loss: 1.38738716, g_loss: 0.69153547, ae_loss: 0.05266521\n",
      "Step: [2606] total_loss: 2.13569498 d_loss: 1.35543299, g_loss: 0.72710407, ae_loss: 0.05315803\n",
      "Step: [2607] total_loss: 2.12647200 d_loss: 1.37285376, g_loss: 0.69857973, ae_loss: 0.05503856\n",
      "Step: [2608] total_loss: 2.09922767 d_loss: 1.37227261, g_loss: 0.67658734, ae_loss: 0.05036769\n",
      "Step: [2609] total_loss: 2.13883853 d_loss: 1.40386379, g_loss: 0.68371308, ae_loss: 0.05126170\n",
      "Step: [2610] total_loss: 2.13440943 d_loss: 1.36980426, g_loss: 0.71373421, ae_loss: 0.05087107\n",
      "Step: [2611] total_loss: 2.13697314 d_loss: 1.39672816, g_loss: 0.69210613, ae_loss: 0.04813882\n",
      "Step: [2612] total_loss: 2.11425328 d_loss: 1.37368608, g_loss: 0.68810743, ae_loss: 0.05245973\n",
      "Step: [2613] total_loss: 2.11830616 d_loss: 1.38450134, g_loss: 0.67773515, ae_loss: 0.05606970\n",
      "Step: [2614] total_loss: 2.11119390 d_loss: 1.36858201, g_loss: 0.68982393, ae_loss: 0.05278793\n",
      "Step: [2615] total_loss: 2.13364315 d_loss: 1.37616503, g_loss: 0.70412540, ae_loss: 0.05335283\n",
      "Step: [2616] total_loss: 2.11290169 d_loss: 1.38655949, g_loss: 0.67514610, ae_loss: 0.05119611\n",
      "Step: [2617] total_loss: 2.13536358 d_loss: 1.37853050, g_loss: 0.70601207, ae_loss: 0.05082107\n",
      "Step: [2618] total_loss: 2.12652111 d_loss: 1.38329434, g_loss: 0.69208735, ae_loss: 0.05113930\n",
      "Step: [2619] total_loss: 2.12979531 d_loss: 1.39072609, g_loss: 0.68516016, ae_loss: 0.05390906\n",
      "Step: [2620] total_loss: 2.14247465 d_loss: 1.39838910, g_loss: 0.69313633, ae_loss: 0.05094911\n",
      "Step: [2621] total_loss: 2.14591146 d_loss: 1.40240359, g_loss: 0.68727231, ae_loss: 0.05623554\n",
      "Step: [2622] total_loss: 2.13555527 d_loss: 1.39644313, g_loss: 0.68647289, ae_loss: 0.05263934\n",
      "Step: [2623] total_loss: 2.12101483 d_loss: 1.39518237, g_loss: 0.67865396, ae_loss: 0.04717846\n",
      "Step: [2624] total_loss: 2.11690378 d_loss: 1.37502861, g_loss: 0.68882751, ae_loss: 0.05304760\n",
      "Step: [2625] total_loss: 2.10870647 d_loss: 1.36092544, g_loss: 0.69303268, ae_loss: 0.05474847\n",
      "Step: [2626] total_loss: 2.11318350 d_loss: 1.38853514, g_loss: 0.67386883, ae_loss: 0.05077956\n",
      "Step: [2627] total_loss: 2.14092493 d_loss: 1.38312674, g_loss: 0.70538074, ae_loss: 0.05241746\n",
      "Step: [2628] total_loss: 2.14268827 d_loss: 1.40068686, g_loss: 0.69402766, ae_loss: 0.04797390\n",
      "Step: [2629] total_loss: 2.14987373 d_loss: 1.37296987, g_loss: 0.72294033, ae_loss: 0.05396371\n",
      "Step: [2630] total_loss: 2.13875723 d_loss: 1.38602722, g_loss: 0.70025015, ae_loss: 0.05247986\n",
      "Step: [2631] total_loss: 2.13633728 d_loss: 1.37985492, g_loss: 0.70372176, ae_loss: 0.05276067\n",
      "Step: [2632] total_loss: 2.11845875 d_loss: 1.36909521, g_loss: 0.69495118, ae_loss: 0.05441242\n",
      "Step: [2633] total_loss: 2.11505651 d_loss: 1.34481120, g_loss: 0.71738267, ae_loss: 0.05286252\n",
      "Step: [2634] total_loss: 2.16047716 d_loss: 1.39102149, g_loss: 0.71291798, ae_loss: 0.05653772\n",
      "Step: [2635] total_loss: 2.16075587 d_loss: 1.40779626, g_loss: 0.69670874, ae_loss: 0.05625084\n",
      "Step: [2636] total_loss: 2.15121722 d_loss: 1.39896905, g_loss: 0.69560641, ae_loss: 0.05664179\n",
      "Step: [2637] total_loss: 2.13575315 d_loss: 1.37917209, g_loss: 0.70138752, ae_loss: 0.05519344\n",
      "Step: [2638] total_loss: 2.14369321 d_loss: 1.38744903, g_loss: 0.70424783, ae_loss: 0.05199636\n",
      "Step: [2639] total_loss: 2.12772679 d_loss: 1.37474751, g_loss: 0.69945586, ae_loss: 0.05352345\n",
      "Step: [2640] total_loss: 2.13960075 d_loss: 1.37655962, g_loss: 0.70906997, ae_loss: 0.05397108\n",
      "Step: [2641] total_loss: 2.11874533 d_loss: 1.37051547, g_loss: 0.70013821, ae_loss: 0.04809151\n",
      "Step: [2642] total_loss: 2.10277176 d_loss: 1.36042380, g_loss: 0.69221550, ae_loss: 0.05013245\n",
      "Step: [2643] total_loss: 2.10698843 d_loss: 1.35892355, g_loss: 0.69698739, ae_loss: 0.05107734\n",
      "Step: [2644] total_loss: 2.14283752 d_loss: 1.38061810, g_loss: 0.70575410, ae_loss: 0.05646537\n",
      "Step: [2645] total_loss: 2.15403128 d_loss: 1.40951109, g_loss: 0.69377786, ae_loss: 0.05074236\n",
      "Step: [2646] total_loss: 2.12460709 d_loss: 1.36913741, g_loss: 0.70591986, ae_loss: 0.04954997\n",
      "Step: [2647] total_loss: 2.12885714 d_loss: 1.37354648, g_loss: 0.70592439, ae_loss: 0.04938644\n",
      "Step: [2648] total_loss: 2.14853859 d_loss: 1.38206494, g_loss: 0.71231306, ae_loss: 0.05416062\n",
      "Step: [2649] total_loss: 2.12890220 d_loss: 1.37819433, g_loss: 0.69512975, ae_loss: 0.05557806\n",
      "Step: [2650] total_loss: 2.13279366 d_loss: 1.40361142, g_loss: 0.67638713, ae_loss: 0.05279514\n",
      "Step: [2651] total_loss: 2.13570738 d_loss: 1.38529634, g_loss: 0.69442904, ae_loss: 0.05598181\n",
      "Step: [2652] total_loss: 2.11034131 d_loss: 1.37913024, g_loss: 0.68304569, ae_loss: 0.04816540\n",
      "Step: [2653] total_loss: 2.12969685 d_loss: 1.39009058, g_loss: 0.68372405, ae_loss: 0.05588207\n",
      "Step: [2654] total_loss: 2.13944244 d_loss: 1.38610101, g_loss: 0.69705594, ae_loss: 0.05628560\n",
      "Step: [2655] total_loss: 2.12499142 d_loss: 1.37316918, g_loss: 0.69689864, ae_loss: 0.05492361\n",
      "Step: [2656] total_loss: 2.13154364 d_loss: 1.38073540, g_loss: 0.69889319, ae_loss: 0.05191511\n",
      "Step: [2657] total_loss: 2.12241983 d_loss: 1.35833931, g_loss: 0.70872933, ae_loss: 0.05535130\n",
      "Step: [2658] total_loss: 2.11633611 d_loss: 1.38358641, g_loss: 0.68034643, ae_loss: 0.05240323\n",
      "Step: [2659] total_loss: 2.11264896 d_loss: 1.37351072, g_loss: 0.68958849, ae_loss: 0.04954970\n",
      "Step: [2660] total_loss: 2.13078332 d_loss: 1.38062942, g_loss: 0.69406390, ae_loss: 0.05608999\n",
      "Step: [2661] total_loss: 2.14424849 d_loss: 1.40885806, g_loss: 0.68506694, ae_loss: 0.05032358\n",
      "Step: [2662] total_loss: 2.10828662 d_loss: 1.37363386, g_loss: 0.68314630, ae_loss: 0.05150653\n",
      "Step: [2663] total_loss: 2.13492322 d_loss: 1.39406776, g_loss: 0.68997741, ae_loss: 0.05087811\n",
      "Step: [2664] total_loss: 2.12978125 d_loss: 1.38488245, g_loss: 0.69431543, ae_loss: 0.05058323\n",
      "Step: [2665] total_loss: 2.11872005 d_loss: 1.37627459, g_loss: 0.68902761, ae_loss: 0.05341782\n",
      "Step: [2666] total_loss: 2.12636232 d_loss: 1.37663364, g_loss: 0.69616210, ae_loss: 0.05356669\n",
      "Step: [2667] total_loss: 2.11895418 d_loss: 1.37870288, g_loss: 0.68717587, ae_loss: 0.05307529\n",
      "Step: [2668] total_loss: 2.12651658 d_loss: 1.36279178, g_loss: 0.70845950, ae_loss: 0.05526530\n",
      "Step: [2669] total_loss: 2.12486887 d_loss: 1.38692212, g_loss: 0.68484956, ae_loss: 0.05309719\n",
      "Step: [2670] total_loss: 2.11769032 d_loss: 1.37040997, g_loss: 0.69820279, ae_loss: 0.04907757\n",
      "Step: [2671] total_loss: 2.15157580 d_loss: 1.38638508, g_loss: 0.70945764, ae_loss: 0.05573314\n",
      "Step: [2672] total_loss: 2.13803363 d_loss: 1.39226174, g_loss: 0.69223970, ae_loss: 0.05353209\n",
      "Step: [2673] total_loss: 2.13870716 d_loss: 1.38083720, g_loss: 0.70811248, ae_loss: 0.04975735\n",
      "Step: [2674] total_loss: 2.12629724 d_loss: 1.38155198, g_loss: 0.69115406, ae_loss: 0.05359115\n",
      "Step: [2675] total_loss: 2.12201309 d_loss: 1.38102913, g_loss: 0.68820977, ae_loss: 0.05277435\n",
      "Step: [2676] total_loss: 2.14290142 d_loss: 1.39532423, g_loss: 0.69192511, ae_loss: 0.05565219\n",
      "Step: [2677] total_loss: 2.14547491 d_loss: 1.39125359, g_loss: 0.69797111, ae_loss: 0.05625013\n",
      "Step: [2678] total_loss: 2.12887144 d_loss: 1.38751173, g_loss: 0.68920648, ae_loss: 0.05215335\n",
      "Step: [2679] total_loss: 2.13100958 d_loss: 1.39185858, g_loss: 0.68832660, ae_loss: 0.05082452\n",
      "Step: [2680] total_loss: 2.12688780 d_loss: 1.39156365, g_loss: 0.68500948, ae_loss: 0.05031480\n",
      "Step: [2681] total_loss: 2.13232565 d_loss: 1.38480532, g_loss: 0.69728744, ae_loss: 0.05023304\n",
      "Step: [2682] total_loss: 2.13521624 d_loss: 1.40185857, g_loss: 0.68177080, ae_loss: 0.05158682\n",
      "Step: [2683] total_loss: 2.12125325 d_loss: 1.38080406, g_loss: 0.68868732, ae_loss: 0.05176189\n",
      "Step: [2684] total_loss: 2.14344978 d_loss: 1.39455390, g_loss: 0.70164740, ae_loss: 0.04724841\n",
      "Step: [2685] total_loss: 2.14565063 d_loss: 1.38460517, g_loss: 0.70693982, ae_loss: 0.05410568\n",
      "Step: [2686] total_loss: 2.13416600 d_loss: 1.37263811, g_loss: 0.71221668, ae_loss: 0.04931114\n",
      "Step: [2687] total_loss: 2.14611363 d_loss: 1.38058162, g_loss: 0.71451831, ae_loss: 0.05101366\n",
      "Step: [2688] total_loss: 2.12909508 d_loss: 1.37586367, g_loss: 0.70089221, ae_loss: 0.05233917\n",
      "Step: [2689] total_loss: 2.12857533 d_loss: 1.36761427, g_loss: 0.70808876, ae_loss: 0.05287246\n",
      "Step: [2690] total_loss: 2.12186837 d_loss: 1.37240911, g_loss: 0.69667280, ae_loss: 0.05278651\n",
      "Step: [2691] total_loss: 2.12098336 d_loss: 1.39174128, g_loss: 0.67858714, ae_loss: 0.05065492\n",
      "Step: [2692] total_loss: 2.13028240 d_loss: 1.39822221, g_loss: 0.68242091, ae_loss: 0.04963918\n",
      "Step: [2693] total_loss: 2.12438393 d_loss: 1.37252831, g_loss: 0.69989955, ae_loss: 0.05195597\n",
      "Step: [2694] total_loss: 2.11806774 d_loss: 1.37827039, g_loss: 0.68859202, ae_loss: 0.05120534\n",
      "Step: [2695] total_loss: 2.12820673 d_loss: 1.38334727, g_loss: 0.69330293, ae_loss: 0.05155665\n",
      "Step: [2696] total_loss: 2.14002228 d_loss: 1.39527905, g_loss: 0.69077379, ae_loss: 0.05396951\n",
      "Step: [2697] total_loss: 2.12320876 d_loss: 1.39053011, g_loss: 0.68126428, ae_loss: 0.05141434\n",
      "Step: [2698] total_loss: 2.13358736 d_loss: 1.37912059, g_loss: 0.69890481, ae_loss: 0.05556202\n",
      "Step: [2699] total_loss: 2.13676810 d_loss: 1.37439859, g_loss: 0.71167880, ae_loss: 0.05069062\n",
      "Step: [2700] total_loss: 2.11659908 d_loss: 1.37902045, g_loss: 0.68614578, ae_loss: 0.05143282\n",
      "Step: [2701] total_loss: 2.12293530 d_loss: 1.39013433, g_loss: 0.67827493, ae_loss: 0.05452599\n",
      "Step: [2702] total_loss: 2.11255717 d_loss: 1.37789154, g_loss: 0.68443710, ae_loss: 0.05022863\n",
      "Step: [2703] total_loss: 2.10909247 d_loss: 1.38079643, g_loss: 0.68188149, ae_loss: 0.04641453\n",
      "Step: [2704] total_loss: 2.10991859 d_loss: 1.39067268, g_loss: 0.66774642, ae_loss: 0.05149942\n",
      "Step: [2705] total_loss: 2.11271477 d_loss: 1.38693726, g_loss: 0.67505419, ae_loss: 0.05072325\n",
      "Step: [2706] total_loss: 2.13432765 d_loss: 1.37052250, g_loss: 0.70867586, ae_loss: 0.05512932\n",
      "Step: [2707] total_loss: 2.10485268 d_loss: 1.36469936, g_loss: 0.68801683, ae_loss: 0.05213650\n",
      "Step: [2708] total_loss: 2.16304207 d_loss: 1.40564823, g_loss: 0.70534641, ae_loss: 0.05204743\n",
      "Step: [2709] total_loss: 2.12680101 d_loss: 1.36304486, g_loss: 0.71287519, ae_loss: 0.05088096\n",
      "Step: [2710] total_loss: 2.13553882 d_loss: 1.39423180, g_loss: 0.69130290, ae_loss: 0.05000407\n",
      "Step: [2711] total_loss: 2.14265203 d_loss: 1.36640787, g_loss: 0.72291112, ae_loss: 0.05333295\n",
      "Step: [2712] total_loss: 2.14160395 d_loss: 1.41103625, g_loss: 0.67927110, ae_loss: 0.05129658\n",
      "Step: [2713] total_loss: 2.09634185 d_loss: 1.36276639, g_loss: 0.68321413, ae_loss: 0.05036122\n",
      "Step: [2714] total_loss: 2.10224247 d_loss: 1.37658834, g_loss: 0.67569244, ae_loss: 0.04996180\n",
      "Step: [2715] total_loss: 2.13037062 d_loss: 1.37419224, g_loss: 0.70062292, ae_loss: 0.05555557\n",
      "Step: [2716] total_loss: 2.14596701 d_loss: 1.39826941, g_loss: 0.69724303, ae_loss: 0.05045451\n",
      "Step: [2717] total_loss: 2.12531447 d_loss: 1.36695707, g_loss: 0.70855141, ae_loss: 0.04980602\n",
      "Step: [2718] total_loss: 2.13834929 d_loss: 1.39492190, g_loss: 0.69150764, ae_loss: 0.05191972\n",
      "Step: [2719] total_loss: 2.14005017 d_loss: 1.38240218, g_loss: 0.70655048, ae_loss: 0.05109748\n",
      "Step: [2720] total_loss: 2.14511442 d_loss: 1.38971722, g_loss: 0.70462972, ae_loss: 0.05076753\n",
      "Step: [2721] total_loss: 2.12702751 d_loss: 1.38766861, g_loss: 0.68841910, ae_loss: 0.05093978\n",
      "Step: [2722] total_loss: 2.13980865 d_loss: 1.38449514, g_loss: 0.70161128, ae_loss: 0.05370222\n",
      "Step: [2723] total_loss: 2.13727403 d_loss: 1.40789926, g_loss: 0.67999804, ae_loss: 0.04937678\n",
      "Step: [2724] total_loss: 2.12587690 d_loss: 1.37844443, g_loss: 0.69702798, ae_loss: 0.05040444\n",
      "Step: [2725] total_loss: 2.12645197 d_loss: 1.37260008, g_loss: 0.70529276, ae_loss: 0.04855925\n",
      "Step: [2726] total_loss: 2.12772083 d_loss: 1.37717938, g_loss: 0.69360399, ae_loss: 0.05693740\n",
      "Step: [2727] total_loss: 2.12688661 d_loss: 1.37929869, g_loss: 0.69538736, ae_loss: 0.05220059\n",
      "Step: [2728] total_loss: 2.11348677 d_loss: 1.34653080, g_loss: 0.71657568, ae_loss: 0.05038034\n",
      "Step: [2729] total_loss: 2.11453867 d_loss: 1.37490535, g_loss: 0.68961620, ae_loss: 0.05001727\n",
      "Step: [2730] total_loss: 2.10976267 d_loss: 1.37681615, g_loss: 0.68140966, ae_loss: 0.05153698\n",
      "Step: [2731] total_loss: 2.13425660 d_loss: 1.41163087, g_loss: 0.66683120, ae_loss: 0.05579459\n",
      "Step: [2732] total_loss: 2.11880064 d_loss: 1.38852835, g_loss: 0.67639458, ae_loss: 0.05387766\n",
      "Step: [2733] total_loss: 2.14286256 d_loss: 1.38923788, g_loss: 0.70240027, ae_loss: 0.05122438\n",
      "Step: [2734] total_loss: 2.12257671 d_loss: 1.38946545, g_loss: 0.68071562, ae_loss: 0.05239566\n",
      "Step: [2735] total_loss: 2.11443996 d_loss: 1.36285138, g_loss: 0.69897437, ae_loss: 0.05261434\n",
      "Step: [2736] total_loss: 2.10868478 d_loss: 1.36642051, g_loss: 0.69523048, ae_loss: 0.04703385\n",
      "Step: [2737] total_loss: 2.13755679 d_loss: 1.39735675, g_loss: 0.68868595, ae_loss: 0.05151412\n",
      "Step: [2738] total_loss: 2.09711289 d_loss: 1.36364782, g_loss: 0.68151963, ae_loss: 0.05194544\n",
      "Step: [2739] total_loss: 2.13041925 d_loss: 1.37881112, g_loss: 0.70090592, ae_loss: 0.05070218\n",
      "Step: [2740] total_loss: 2.13020325 d_loss: 1.37586856, g_loss: 0.70531774, ae_loss: 0.04901698\n",
      "Step: [2741] total_loss: 2.11408257 d_loss: 1.35064065, g_loss: 0.71419770, ae_loss: 0.04924421\n",
      "Step: [2742] total_loss: 2.13673711 d_loss: 1.38724160, g_loss: 0.70073092, ae_loss: 0.04876463\n",
      "Step: [2743] total_loss: 2.12633801 d_loss: 1.37372494, g_loss: 0.69793445, ae_loss: 0.05467868\n",
      "Step: [2744] total_loss: 2.12691569 d_loss: 1.38286781, g_loss: 0.69632685, ae_loss: 0.04772102\n",
      "Step: [2745] total_loss: 2.14803720 d_loss: 1.38305032, g_loss: 0.71627200, ae_loss: 0.04871482\n",
      "Step: [2746] total_loss: 2.11709619 d_loss: 1.36571956, g_loss: 0.70062232, ae_loss: 0.05075431\n",
      "Step: [2747] total_loss: 2.13351965 d_loss: 1.40430057, g_loss: 0.67837596, ae_loss: 0.05084315\n",
      "Step: [2748] total_loss: 2.13059664 d_loss: 1.37350094, g_loss: 0.70427752, ae_loss: 0.05281806\n",
      "Step: [2749] total_loss: 2.14290047 d_loss: 1.36597908, g_loss: 0.72356755, ae_loss: 0.05335393\n",
      "Step: [2750] total_loss: 2.13197279 d_loss: 1.36914730, g_loss: 0.71201587, ae_loss: 0.05080952\n",
      "Step: [2751] total_loss: 2.13632154 d_loss: 1.39108872, g_loss: 0.69145596, ae_loss: 0.05377693\n",
      "Step: [2752] total_loss: 2.17171693 d_loss: 1.44231200, g_loss: 0.67863357, ae_loss: 0.05077133\n",
      "Step: [2753] total_loss: 2.15307117 d_loss: 1.38694763, g_loss: 0.71351206, ae_loss: 0.05261149\n",
      "Step: [2754] total_loss: 2.13892317 d_loss: 1.39029932, g_loss: 0.69778287, ae_loss: 0.05084085\n",
      "Step: [2755] total_loss: 2.14839649 d_loss: 1.39750957, g_loss: 0.70087600, ae_loss: 0.05001077\n",
      "Step: [2756] total_loss: 2.11765695 d_loss: 1.35797834, g_loss: 0.70777774, ae_loss: 0.05190087\n",
      "Step: [2757] total_loss: 2.12757468 d_loss: 1.38008130, g_loss: 0.69009006, ae_loss: 0.05740338\n",
      "Step: [2758] total_loss: 2.10274935 d_loss: 1.36885083, g_loss: 0.68185747, ae_loss: 0.05204121\n",
      "Step: [2759] total_loss: 2.13509893 d_loss: 1.37829614, g_loss: 0.70269048, ae_loss: 0.05411240\n",
      "Step: [2760] total_loss: 2.11216879 d_loss: 1.36902833, g_loss: 0.69123548, ae_loss: 0.05190504\n",
      "Step: [2761] total_loss: 2.12551332 d_loss: 1.39114213, g_loss: 0.68680453, ae_loss: 0.04756670\n",
      "Step: [2762] total_loss: 2.13984537 d_loss: 1.38144827, g_loss: 0.70553517, ae_loss: 0.05286200\n",
      "Step: [2763] total_loss: 2.14368415 d_loss: 1.38827729, g_loss: 0.70159572, ae_loss: 0.05381101\n",
      "Step: [2764] total_loss: 2.13164186 d_loss: 1.37356544, g_loss: 0.70656109, ae_loss: 0.05151543\n",
      "Step: [2765] total_loss: 2.11221647 d_loss: 1.36410403, g_loss: 0.69410241, ae_loss: 0.05400988\n",
      "Step: [2766] total_loss: 2.13039470 d_loss: 1.39219737, g_loss: 0.68268228, ae_loss: 0.05551503\n",
      "Step: [2767] total_loss: 2.13361192 d_loss: 1.38178682, g_loss: 0.69583440, ae_loss: 0.05599074\n",
      "Step: [2768] total_loss: 2.10539627 d_loss: 1.37042475, g_loss: 0.68581414, ae_loss: 0.04915743\n",
      "Step: [2769] total_loss: 2.13488245 d_loss: 1.38019860, g_loss: 0.69953156, ae_loss: 0.05515233\n",
      "Step: [2770] total_loss: 2.13154292 d_loss: 1.39547694, g_loss: 0.68182540, ae_loss: 0.05424059\n",
      "Step: [2771] total_loss: 2.12080860 d_loss: 1.37036157, g_loss: 0.70090568, ae_loss: 0.04954152\n",
      "Step: [2772] total_loss: 2.11187696 d_loss: 1.36172771, g_loss: 0.69656527, ae_loss: 0.05358409\n",
      "Step: [2773] total_loss: 2.09354210 d_loss: 1.36009932, g_loss: 0.67870998, ae_loss: 0.05473284\n",
      "Step: [2774] total_loss: 2.09256315 d_loss: 1.34971857, g_loss: 0.69597876, ae_loss: 0.04686600\n",
      "Step: [2775] total_loss: 2.11757851 d_loss: 1.37943935, g_loss: 0.68301964, ae_loss: 0.05511948\n",
      "Step: [2776] total_loss: 2.13172793 d_loss: 1.40030956, g_loss: 0.68379110, ae_loss: 0.04762718\n",
      "Step: [2777] total_loss: 2.10655928 d_loss: 1.38037395, g_loss: 0.67415178, ae_loss: 0.05203363\n",
      "Step: [2778] total_loss: 2.11863184 d_loss: 1.37290335, g_loss: 0.69377899, ae_loss: 0.05194939\n",
      "Step: [2779] total_loss: 2.12409949 d_loss: 1.38569117, g_loss: 0.68750477, ae_loss: 0.05090356\n",
      "Step: [2780] total_loss: 2.13978672 d_loss: 1.40862417, g_loss: 0.67949235, ae_loss: 0.05167033\n",
      "Step: [2781] total_loss: 2.11556768 d_loss: 1.38674688, g_loss: 0.68026221, ae_loss: 0.04855868\n",
      "Step: [2782] total_loss: 2.11902857 d_loss: 1.37956405, g_loss: 0.68693936, ae_loss: 0.05252516\n",
      "Step: [2783] total_loss: 2.09891367 d_loss: 1.35903502, g_loss: 0.69254673, ae_loss: 0.04733175\n",
      "Step: [2784] total_loss: 2.14346957 d_loss: 1.39749217, g_loss: 0.68663204, ae_loss: 0.05934539\n",
      "Step: [2785] total_loss: 2.15036917 d_loss: 1.38565087, g_loss: 0.71103913, ae_loss: 0.05367906\n",
      "Step: [2786] total_loss: 2.13521743 d_loss: 1.40024567, g_loss: 0.68599170, ae_loss: 0.04897998\n",
      "Step: [2787] total_loss: 2.11157894 d_loss: 1.38256764, g_loss: 0.67515326, ae_loss: 0.05385809\n",
      "Step: [2788] total_loss: 2.11126637 d_loss: 1.37111735, g_loss: 0.68618524, ae_loss: 0.05396375\n",
      "Step: [2789] total_loss: 2.11402154 d_loss: 1.37631822, g_loss: 0.68243104, ae_loss: 0.05527235\n",
      "Step: [2790] total_loss: 2.11251616 d_loss: 1.37861323, g_loss: 0.68294698, ae_loss: 0.05095595\n",
      "Step: [2791] total_loss: 2.11825585 d_loss: 1.37574744, g_loss: 0.69443458, ae_loss: 0.04807394\n",
      "Step: [2792] total_loss: 2.15851521 d_loss: 1.41541994, g_loss: 0.68539262, ae_loss: 0.05770266\n",
      "Step: [2793] total_loss: 2.12051392 d_loss: 1.37089157, g_loss: 0.69837177, ae_loss: 0.05125065\n",
      "Step: [2794] total_loss: 2.15989757 d_loss: 1.40253210, g_loss: 0.70227116, ae_loss: 0.05509422\n",
      "Step: [2795] total_loss: 2.13646698 d_loss: 1.37404037, g_loss: 0.71543324, ae_loss: 0.04699344\n",
      "Step: [2796] total_loss: 2.15468645 d_loss: 1.40218854, g_loss: 0.69927824, ae_loss: 0.05321966\n",
      "Step: [2797] total_loss: 2.11995935 d_loss: 1.38261175, g_loss: 0.68409646, ae_loss: 0.05325107\n",
      "Step: [2798] total_loss: 2.14565039 d_loss: 1.37479687, g_loss: 0.71969008, ae_loss: 0.05116329\n",
      "Step: [2799] total_loss: 2.15756106 d_loss: 1.36866748, g_loss: 0.73707533, ae_loss: 0.05181831\n",
      "Step: [2800] total_loss: 2.11975527 d_loss: 1.37242615, g_loss: 0.69966161, ae_loss: 0.04766767\n",
      "Step: [2801] total_loss: 2.12262344 d_loss: 1.39105153, g_loss: 0.68275446, ae_loss: 0.04881739\n",
      "Step: [2802] total_loss: 2.12698460 d_loss: 1.38750982, g_loss: 0.68538153, ae_loss: 0.05409342\n",
      "Step: [2803] total_loss: 2.11965942 d_loss: 1.38888049, g_loss: 0.67715549, ae_loss: 0.05362331\n",
      "Step: [2804] total_loss: 2.10546541 d_loss: 1.36343145, g_loss: 0.69594002, ae_loss: 0.04609410\n",
      "Step: [2805] total_loss: 2.13795829 d_loss: 1.39760494, g_loss: 0.68773448, ae_loss: 0.05261881\n",
      "Step: [2806] total_loss: 2.10187626 d_loss: 1.34249485, g_loss: 0.70622432, ae_loss: 0.05315710\n",
      "Step: [2807] total_loss: 2.12789607 d_loss: 1.39854491, g_loss: 0.67670566, ae_loss: 0.05264556\n",
      "Step: [2808] total_loss: 2.11504459 d_loss: 1.37750018, g_loss: 0.68659878, ae_loss: 0.05094551\n",
      "Step: [2809] total_loss: 2.10593390 d_loss: 1.35857630, g_loss: 0.69521213, ae_loss: 0.05214549\n",
      "Step: [2810] total_loss: 2.13101578 d_loss: 1.39355087, g_loss: 0.68627512, ae_loss: 0.05118977\n",
      "Step: [2811] total_loss: 2.12388039 d_loss: 1.38122511, g_loss: 0.69330442, ae_loss: 0.04935092\n",
      "Step: [2812] total_loss: 2.12352204 d_loss: 1.38135386, g_loss: 0.68955767, ae_loss: 0.05261049\n",
      "Step: [2813] total_loss: 2.10896516 d_loss: 1.37061548, g_loss: 0.68784672, ae_loss: 0.05050296\n",
      "Step: [2814] total_loss: 2.13753462 d_loss: 1.38275039, g_loss: 0.70447350, ae_loss: 0.05031088\n",
      "Step: [2815] total_loss: 2.13220882 d_loss: 1.36746883, g_loss: 0.71465480, ae_loss: 0.05008529\n",
      "Step: [2816] total_loss: 2.13911867 d_loss: 1.38045549, g_loss: 0.70773208, ae_loss: 0.05093118\n",
      "Step: [2817] total_loss: 2.12996626 d_loss: 1.38598704, g_loss: 0.69237697, ae_loss: 0.05160224\n",
      "Step: [2818] total_loss: 2.12185478 d_loss: 1.38042533, g_loss: 0.68590170, ae_loss: 0.05552778\n",
      "Step: [2819] total_loss: 2.13447976 d_loss: 1.36263776, g_loss: 0.72234589, ae_loss: 0.04949620\n",
      "Step: [2820] total_loss: 2.11366796 d_loss: 1.37441611, g_loss: 0.69095510, ae_loss: 0.04829670\n",
      "Step: [2821] total_loss: 2.11322331 d_loss: 1.36159253, g_loss: 0.70169878, ae_loss: 0.04993195\n",
      "Step: [2822] total_loss: 2.12557125 d_loss: 1.38073349, g_loss: 0.69222033, ae_loss: 0.05261748\n",
      "Step: [2823] total_loss: 2.10948277 d_loss: 1.37096775, g_loss: 0.68617582, ae_loss: 0.05233906\n",
      "Step: [2824] total_loss: 2.12852550 d_loss: 1.38325500, g_loss: 0.69367570, ae_loss: 0.05159483\n",
      "Step: [2825] total_loss: 2.14179635 d_loss: 1.39944744, g_loss: 0.69125724, ae_loss: 0.05109170\n",
      "Step: [2826] total_loss: 2.12942743 d_loss: 1.36530781, g_loss: 0.71227086, ae_loss: 0.05184885\n",
      "Step: [2827] total_loss: 2.10753345 d_loss: 1.36295557, g_loss: 0.69388437, ae_loss: 0.05069353\n",
      "Step: [2828] total_loss: 2.12004137 d_loss: 1.35000741, g_loss: 0.71745867, ae_loss: 0.05257538\n",
      "Step: [2829] total_loss: 2.13227606 d_loss: 1.40028930, g_loss: 0.67720532, ae_loss: 0.05478146\n",
      "Step: [2830] total_loss: 2.13753581 d_loss: 1.38022423, g_loss: 0.70572990, ae_loss: 0.05158165\n",
      "Step: [2831] total_loss: 2.11251593 d_loss: 1.38854110, g_loss: 0.67533016, ae_loss: 0.04864461\n",
      "Step: [2832] total_loss: 2.11403942 d_loss: 1.36549449, g_loss: 0.69309253, ae_loss: 0.05545230\n",
      "Step: [2833] total_loss: 2.11098814 d_loss: 1.37862754, g_loss: 0.68003392, ae_loss: 0.05232660\n",
      "Step: [2834] total_loss: 2.13478541 d_loss: 1.38948524, g_loss: 0.69278133, ae_loss: 0.05251889\n",
      "Step: [2835] total_loss: 2.11961484 d_loss: 1.38328481, g_loss: 0.68403471, ae_loss: 0.05229530\n",
      "Step: [2836] total_loss: 2.11977863 d_loss: 1.39716733, g_loss: 0.66908395, ae_loss: 0.05352742\n",
      "Step: [2837] total_loss: 2.14335871 d_loss: 1.39928198, g_loss: 0.69399118, ae_loss: 0.05008564\n",
      "Step: [2838] total_loss: 2.10669661 d_loss: 1.36577797, g_loss: 0.68634319, ae_loss: 0.05457551\n",
      "Step: [2839] total_loss: 2.11334181 d_loss: 1.36766255, g_loss: 0.69720894, ae_loss: 0.04847034\n",
      "Step: [2840] total_loss: 2.14125061 d_loss: 1.36928749, g_loss: 0.72121823, ae_loss: 0.05074490\n",
      "Step: [2841] total_loss: 2.12371492 d_loss: 1.35913968, g_loss: 0.71196008, ae_loss: 0.05261499\n",
      "Step: [2842] total_loss: 2.14842701 d_loss: 1.39108229, g_loss: 0.70347703, ae_loss: 0.05386762\n",
      "Step: [2843] total_loss: 2.16233397 d_loss: 1.39850283, g_loss: 0.71495163, ae_loss: 0.04887955\n",
      "Step: [2844] total_loss: 2.12566113 d_loss: 1.38218999, g_loss: 0.69235188, ae_loss: 0.05111916\n",
      "Step: [2845] total_loss: 2.12329316 d_loss: 1.39876640, g_loss: 0.67013592, ae_loss: 0.05439091\n",
      "Step: [2846] total_loss: 2.13033438 d_loss: 1.37577081, g_loss: 0.70063627, ae_loss: 0.05392731\n",
      "Step: [2847] total_loss: 2.13765192 d_loss: 1.39124048, g_loss: 0.69659007, ae_loss: 0.04982154\n",
      "Step: [2848] total_loss: 2.14192533 d_loss: 1.39176917, g_loss: 0.70140660, ae_loss: 0.04874942\n",
      "Step: [2849] total_loss: 2.11499238 d_loss: 1.38181281, g_loss: 0.68109155, ae_loss: 0.05208802\n",
      "Step: [2850] total_loss: 2.14226341 d_loss: 1.41106987, g_loss: 0.67808318, ae_loss: 0.05311051\n",
      "Step: [2851] total_loss: 2.12044716 d_loss: 1.38839340, g_loss: 0.68290091, ae_loss: 0.04915279\n",
      "Step: [2852] total_loss: 2.10492325 d_loss: 1.36626184, g_loss: 0.68771470, ae_loss: 0.05094668\n",
      "Step: [2853] total_loss: 2.12225008 d_loss: 1.37824023, g_loss: 0.68981433, ae_loss: 0.05419555\n",
      "Step: [2854] total_loss: 2.13226509 d_loss: 1.39621854, g_loss: 0.68623972, ae_loss: 0.04980667\n",
      "Step: [2855] total_loss: 2.10980630 d_loss: 1.37565899, g_loss: 0.68855226, ae_loss: 0.04559501\n",
      "Step: [2856] total_loss: 2.12707281 d_loss: 1.38400304, g_loss: 0.69394445, ae_loss: 0.04912514\n",
      "Step: [2857] total_loss: 2.11907864 d_loss: 1.38231504, g_loss: 0.69042683, ae_loss: 0.04633671\n",
      "Step: [2858] total_loss: 2.11588097 d_loss: 1.36929941, g_loss: 0.69407642, ae_loss: 0.05250500\n",
      "Step: [2859] total_loss: 2.10029173 d_loss: 1.37334466, g_loss: 0.67627472, ae_loss: 0.05067237\n",
      "Step: [2860] total_loss: 2.11918259 d_loss: 1.38213277, g_loss: 0.68708473, ae_loss: 0.04996503\n",
      "Step: [2861] total_loss: 2.12049341 d_loss: 1.38258338, g_loss: 0.68542039, ae_loss: 0.05248964\n",
      "Step: [2862] total_loss: 2.12488580 d_loss: 1.40873158, g_loss: 0.66361612, ae_loss: 0.05253814\n",
      "Step: [2863] total_loss: 2.13089657 d_loss: 1.39546561, g_loss: 0.68325263, ae_loss: 0.05217837\n",
      "Step: [2864] total_loss: 2.12117004 d_loss: 1.37942719, g_loss: 0.68964803, ae_loss: 0.05209478\n",
      "Step: [2865] total_loss: 2.11424780 d_loss: 1.37606907, g_loss: 0.69010556, ae_loss: 0.04807315\n",
      "Step: [2866] total_loss: 2.11959386 d_loss: 1.37887394, g_loss: 0.69343519, ae_loss: 0.04728477\n",
      "Step: [2867] total_loss: 2.11564255 d_loss: 1.35266149, g_loss: 0.71217608, ae_loss: 0.05080501\n",
      "Step: [2868] total_loss: 2.15226150 d_loss: 1.39583480, g_loss: 0.70452112, ae_loss: 0.05190561\n",
      "Step: [2869] total_loss: 2.12238979 d_loss: 1.37206483, g_loss: 0.69802058, ae_loss: 0.05230448\n",
      "Step: [2870] total_loss: 2.13302279 d_loss: 1.37795603, g_loss: 0.70522827, ae_loss: 0.04983853\n",
      "Step: [2871] total_loss: 2.13395452 d_loss: 1.39616585, g_loss: 0.68868577, ae_loss: 0.04910306\n",
      "Step: [2872] total_loss: 2.14282870 d_loss: 1.40788400, g_loss: 0.68416482, ae_loss: 0.05077979\n",
      "Step: [2873] total_loss: 2.12759018 d_loss: 1.36764598, g_loss: 0.70803851, ae_loss: 0.05190564\n",
      "Step: [2874] total_loss: 2.14248490 d_loss: 1.39287472, g_loss: 0.69749451, ae_loss: 0.05211570\n",
      "Step: [2875] total_loss: 2.14083052 d_loss: 1.39791989, g_loss: 0.69242108, ae_loss: 0.05048952\n",
      "Step: [2876] total_loss: 2.11037588 d_loss: 1.37288046, g_loss: 0.68393147, ae_loss: 0.05356396\n",
      "Step: [2877] total_loss: 2.13052940 d_loss: 1.38075280, g_loss: 0.69747090, ae_loss: 0.05230579\n",
      "Step: [2878] total_loss: 2.12321997 d_loss: 1.38694704, g_loss: 0.68058914, ae_loss: 0.05568371\n",
      "Step: [2879] total_loss: 2.11945677 d_loss: 1.38683558, g_loss: 0.68198907, ae_loss: 0.05063215\n",
      "Step: [2880] total_loss: 2.12604380 d_loss: 1.37947464, g_loss: 0.69431090, ae_loss: 0.05225810\n",
      "Step: [2881] total_loss: 2.13091803 d_loss: 1.38267469, g_loss: 0.69371808, ae_loss: 0.05452534\n",
      "Step: [2882] total_loss: 2.13088322 d_loss: 1.39488149, g_loss: 0.68585324, ae_loss: 0.05014854\n",
      "Step: [2883] total_loss: 2.12691069 d_loss: 1.39188802, g_loss: 0.68192989, ae_loss: 0.05309286\n",
      "Step: [2884] total_loss: 2.11023664 d_loss: 1.38038981, g_loss: 0.67884135, ae_loss: 0.05100552\n",
      "Step: [2885] total_loss: 2.11985707 d_loss: 1.36613691, g_loss: 0.70531917, ae_loss: 0.04840094\n",
      "Step: [2886] total_loss: 2.11610174 d_loss: 1.37110424, g_loss: 0.69476485, ae_loss: 0.05023278\n",
      "Step: [2887] total_loss: 2.11254811 d_loss: 1.36901486, g_loss: 0.69013637, ae_loss: 0.05339687\n",
      "Step: [2888] total_loss: 2.13182926 d_loss: 1.39055336, g_loss: 0.68876505, ae_loss: 0.05251078\n",
      "Step: [2889] total_loss: 2.11655998 d_loss: 1.37977600, g_loss: 0.68463039, ae_loss: 0.05215366\n",
      "Step: [2890] total_loss: 2.11753964 d_loss: 1.37471688, g_loss: 0.69213784, ae_loss: 0.05068489\n",
      "Step: [2891] total_loss: 2.11029601 d_loss: 1.36501586, g_loss: 0.69579309, ae_loss: 0.04948708\n",
      "Step: [2892] total_loss: 2.13051581 d_loss: 1.36877608, g_loss: 0.70669007, ae_loss: 0.05504961\n",
      "Step: [2893] total_loss: 2.13755751 d_loss: 1.38368046, g_loss: 0.69733441, ae_loss: 0.05654275\n",
      "Step: [2894] total_loss: 2.13063717 d_loss: 1.37581050, g_loss: 0.70187235, ae_loss: 0.05295437\n",
      "Step: [2895] total_loss: 2.09147978 d_loss: 1.36617041, g_loss: 0.67379320, ae_loss: 0.05151613\n",
      "Step: [2896] total_loss: 2.12997341 d_loss: 1.39992499, g_loss: 0.67974651, ae_loss: 0.05030186\n",
      "Step: [2897] total_loss: 2.14052629 d_loss: 1.38060331, g_loss: 0.70336282, ae_loss: 0.05656015\n",
      "Step: [2898] total_loss: 2.11868262 d_loss: 1.36912000, g_loss: 0.70122665, ae_loss: 0.04833593\n",
      "Step: [2899] total_loss: 2.11852169 d_loss: 1.36030459, g_loss: 0.70795989, ae_loss: 0.05025703\n",
      "Step: [2900] total_loss: 2.12850928 d_loss: 1.37213063, g_loss: 0.70757252, ae_loss: 0.04880625\n",
      "Step: [2901] total_loss: 2.13622761 d_loss: 1.39880633, g_loss: 0.68551981, ae_loss: 0.05190137\n",
      "Step: [2902] total_loss: 2.12879610 d_loss: 1.35904098, g_loss: 0.71954203, ae_loss: 0.05021298\n",
      "Step: [2903] total_loss: 2.12307525 d_loss: 1.38069320, g_loss: 0.69305885, ae_loss: 0.04932320\n",
      "Step: [2904] total_loss: 2.13204551 d_loss: 1.39334393, g_loss: 0.68637437, ae_loss: 0.05232716\n",
      "Step: [2905] total_loss: 2.15546942 d_loss: 1.39441514, g_loss: 0.70908070, ae_loss: 0.05197358\n",
      "Step: [2906] total_loss: 2.13425636 d_loss: 1.38205314, g_loss: 0.69979084, ae_loss: 0.05241248\n",
      "Step: [2907] total_loss: 2.12443352 d_loss: 1.38890541, g_loss: 0.68314159, ae_loss: 0.05238655\n",
      "Step: [2908] total_loss: 2.10387659 d_loss: 1.36746132, g_loss: 0.68510401, ae_loss: 0.05131135\n",
      "Step: [2909] total_loss: 2.11816549 d_loss: 1.39712453, g_loss: 0.66752839, ae_loss: 0.05351256\n",
      "Step: [2910] total_loss: 2.12120032 d_loss: 1.38697290, g_loss: 0.68123507, ae_loss: 0.05299231\n",
      "Step: [2911] total_loss: 2.13317442 d_loss: 1.39542127, g_loss: 0.68967986, ae_loss: 0.04807331\n",
      "Step: [2912] total_loss: 2.12424636 d_loss: 1.38284564, g_loss: 0.68849099, ae_loss: 0.05290977\n",
      "Step: [2913] total_loss: 2.14139462 d_loss: 1.40291500, g_loss: 0.68590438, ae_loss: 0.05257530\n",
      "Step: [2914] total_loss: 2.13793945 d_loss: 1.40297222, g_loss: 0.68561465, ae_loss: 0.04935270\n",
      "Step: [2915] total_loss: 2.12121725 d_loss: 1.36938965, g_loss: 0.69558543, ae_loss: 0.05624210\n",
      "Step: [2916] total_loss: 2.10658121 d_loss: 1.37981546, g_loss: 0.67941773, ae_loss: 0.04734793\n",
      "Step: [2917] total_loss: 2.10417342 d_loss: 1.37646365, g_loss: 0.67855209, ae_loss: 0.04915764\n",
      "Step: [2918] total_loss: 2.12808490 d_loss: 1.38306141, g_loss: 0.69444174, ae_loss: 0.05058177\n",
      "Step: [2919] total_loss: 2.12684226 d_loss: 1.38880908, g_loss: 0.68952227, ae_loss: 0.04851090\n",
      "Step: [2920] total_loss: 2.15125823 d_loss: 1.38282478, g_loss: 0.72135115, ae_loss: 0.04708226\n",
      "Step: [2921] total_loss: 2.12757778 d_loss: 1.38418210, g_loss: 0.68945229, ae_loss: 0.05394355\n",
      "Step: [2922] total_loss: 2.12134266 d_loss: 1.39104509, g_loss: 0.67554712, ae_loss: 0.05475060\n",
      "Step: [2923] total_loss: 2.12021923 d_loss: 1.37541413, g_loss: 0.69321895, ae_loss: 0.05158608\n",
      "Step: [2924] total_loss: 2.11219263 d_loss: 1.37199020, g_loss: 0.68785936, ae_loss: 0.05234296\n",
      "Step: [2925] total_loss: 2.14356804 d_loss: 1.37621856, g_loss: 0.71358854, ae_loss: 0.05376099\n",
      "Step: [2926] total_loss: 2.10329223 d_loss: 1.37049627, g_loss: 0.68188184, ae_loss: 0.05091406\n",
      "Step: [2927] total_loss: 2.09952593 d_loss: 1.36475492, g_loss: 0.68528122, ae_loss: 0.04948981\n",
      "Step: [2928] total_loss: 2.11997700 d_loss: 1.37786651, g_loss: 0.68791872, ae_loss: 0.05419185\n",
      "Step: [2929] total_loss: 2.13876557 d_loss: 1.39986324, g_loss: 0.68981695, ae_loss: 0.04908540\n",
      "Step: [2930] total_loss: 2.11067390 d_loss: 1.37166893, g_loss: 0.68967175, ae_loss: 0.04933310\n",
      "Step: [2931] total_loss: 2.11357164 d_loss: 1.38456416, g_loss: 0.67925030, ae_loss: 0.04975722\n",
      "Step: [2932] total_loss: 2.14959025 d_loss: 1.39086175, g_loss: 0.70954275, ae_loss: 0.04918577\n",
      "Step: [2933] total_loss: 2.11242008 d_loss: 1.36737370, g_loss: 0.69222474, ae_loss: 0.05282175\n",
      "Step: [2934] total_loss: 2.12919331 d_loss: 1.36998248, g_loss: 0.70843703, ae_loss: 0.05077389\n",
      "Step: [2935] total_loss: 2.10437107 d_loss: 1.35624528, g_loss: 0.69726670, ae_loss: 0.05085899\n",
      "Step: [2936] total_loss: 2.11286116 d_loss: 1.37340307, g_loss: 0.68668127, ae_loss: 0.05277699\n",
      "Step: [2937] total_loss: 2.12074995 d_loss: 1.38434339, g_loss: 0.68357909, ae_loss: 0.05282736\n",
      "Step: [2938] total_loss: 2.13578081 d_loss: 1.39276254, g_loss: 0.69513613, ae_loss: 0.04788220\n",
      "Step: [2939] total_loss: 2.16148591 d_loss: 1.42114735, g_loss: 0.69010472, ae_loss: 0.05023386\n",
      "Step: [2940] total_loss: 2.12083316 d_loss: 1.37782145, g_loss: 0.69072396, ae_loss: 0.05228774\n",
      "Step: [2941] total_loss: 2.11787319 d_loss: 1.39723790, g_loss: 0.66831106, ae_loss: 0.05232423\n",
      "Step: [2942] total_loss: 2.13348460 d_loss: 1.38465357, g_loss: 0.69565892, ae_loss: 0.05317215\n",
      "Step: [2943] total_loss: 2.12395477 d_loss: 1.37771440, g_loss: 0.69429928, ae_loss: 0.05194113\n",
      "Step: [2944] total_loss: 2.11302900 d_loss: 1.36300027, g_loss: 0.69795549, ae_loss: 0.05207335\n",
      "Step: [2945] total_loss: 2.12999153 d_loss: 1.38600469, g_loss: 0.69195336, ae_loss: 0.05203347\n",
      "Step: [2946] total_loss: 2.11559534 d_loss: 1.37353492, g_loss: 0.68827105, ae_loss: 0.05378937\n",
      "Step: [2947] total_loss: 2.12858558 d_loss: 1.39043307, g_loss: 0.68353850, ae_loss: 0.05461396\n",
      "Step: [2948] total_loss: 2.12120771 d_loss: 1.35944676, g_loss: 0.71203005, ae_loss: 0.04973076\n",
      "Step: [2949] total_loss: 2.12408590 d_loss: 1.36465669, g_loss: 0.70878202, ae_loss: 0.05064711\n",
      "Step: [2950] total_loss: 2.11832142 d_loss: 1.38461328, g_loss: 0.68317068, ae_loss: 0.05053758\n",
      "Step: [2951] total_loss: 2.13394928 d_loss: 1.37338090, g_loss: 0.70686591, ae_loss: 0.05370262\n",
      "Step: [2952] total_loss: 2.12849903 d_loss: 1.37965906, g_loss: 0.69860959, ae_loss: 0.05023032\n",
      "Step: [2953] total_loss: 2.13280249 d_loss: 1.39006042, g_loss: 0.68895566, ae_loss: 0.05378644\n",
      "Step: [2954] total_loss: 2.14971185 d_loss: 1.38990557, g_loss: 0.70985860, ae_loss: 0.04994756\n",
      "Step: [2955] total_loss: 2.13895178 d_loss: 1.38873768, g_loss: 0.69798875, ae_loss: 0.05222525\n",
      "Step: [2956] total_loss: 2.12932444 d_loss: 1.39037299, g_loss: 0.68349779, ae_loss: 0.05545382\n",
      "Step: [2957] total_loss: 2.13688827 d_loss: 1.38482881, g_loss: 0.69913626, ae_loss: 0.05292319\n",
      "Step: [2958] total_loss: 2.12263465 d_loss: 1.35831225, g_loss: 0.70947683, ae_loss: 0.05484558\n",
      "Step: [2959] total_loss: 2.11151218 d_loss: 1.37964535, g_loss: 0.67959523, ae_loss: 0.05227147\n",
      "Step: [2960] total_loss: 2.12184143 d_loss: 1.39000678, g_loss: 0.68058771, ae_loss: 0.05124696\n",
      "Step: [2961] total_loss: 2.12933445 d_loss: 1.37292886, g_loss: 0.70586240, ae_loss: 0.05054324\n",
      "Step: [2962] total_loss: 2.12157798 d_loss: 1.37295270, g_loss: 0.69515127, ae_loss: 0.05347403\n",
      "Step: [2963] total_loss: 2.13618374 d_loss: 1.38977516, g_loss: 0.69669282, ae_loss: 0.04971561\n",
      "Step: [2964] total_loss: 2.13208365 d_loss: 1.38794482, g_loss: 0.69136906, ae_loss: 0.05276978\n",
      "Step: [2965] total_loss: 2.13646078 d_loss: 1.37341022, g_loss: 0.71422029, ae_loss: 0.04883010\n",
      "Step: [2966] total_loss: 2.17395592 d_loss: 1.39321637, g_loss: 0.72865951, ae_loss: 0.05207990\n",
      "Step: [2967] total_loss: 2.12332892 d_loss: 1.37626600, g_loss: 0.69474542, ae_loss: 0.05231746\n",
      "Step: [2968] total_loss: 2.14535213 d_loss: 1.39997077, g_loss: 0.69502437, ae_loss: 0.05035696\n",
      "Step: [2969] total_loss: 2.10493207 d_loss: 1.37184978, g_loss: 0.68237233, ae_loss: 0.05071000\n",
      "Step: [2970] total_loss: 2.15436125 d_loss: 1.40888476, g_loss: 0.69394159, ae_loss: 0.05153500\n",
      "Step: [2971] total_loss: 2.12179303 d_loss: 1.37778533, g_loss: 0.68934894, ae_loss: 0.05465876\n",
      "Step: [2972] total_loss: 2.12348866 d_loss: 1.38479769, g_loss: 0.68639982, ae_loss: 0.05229116\n",
      "Step: [2973] total_loss: 2.10794401 d_loss: 1.34864759, g_loss: 0.70810729, ae_loss: 0.05118919\n",
      "Step: [2974] total_loss: 2.13228464 d_loss: 1.39057684, g_loss: 0.68967164, ae_loss: 0.05203609\n",
      "Step: [2975] total_loss: 2.14048052 d_loss: 1.39613438, g_loss: 0.69404101, ae_loss: 0.05030520\n",
      "Step: [2976] total_loss: 2.14276648 d_loss: 1.38195193, g_loss: 0.70786452, ae_loss: 0.05295005\n",
      "Step: [2977] total_loss: 2.11934400 d_loss: 1.39398015, g_loss: 0.67871666, ae_loss: 0.04664722\n",
      "Step: [2978] total_loss: 2.10641623 d_loss: 1.37238145, g_loss: 0.68570375, ae_loss: 0.04833090\n",
      "Step: [2979] total_loss: 2.13188720 d_loss: 1.39865136, g_loss: 0.67926562, ae_loss: 0.05397018\n",
      "Step: [2980] total_loss: 2.12290597 d_loss: 1.39255905, g_loss: 0.67526579, ae_loss: 0.05508113\n",
      "Step: [2981] total_loss: 2.13241363 d_loss: 1.38485456, g_loss: 0.69300216, ae_loss: 0.05455701\n",
      "Step: [2982] total_loss: 2.10621095 d_loss: 1.35819483, g_loss: 0.69874054, ae_loss: 0.04927547\n",
      "Step: [2983] total_loss: 2.12269139 d_loss: 1.37709427, g_loss: 0.69472629, ae_loss: 0.05087083\n",
      "Step: [2984] total_loss: 2.12254596 d_loss: 1.38086057, g_loss: 0.69018281, ae_loss: 0.05150253\n",
      "Step: [2985] total_loss: 2.14394498 d_loss: 1.40581918, g_loss: 0.68412650, ae_loss: 0.05399932\n",
      "Step: [2986] total_loss: 2.10892677 d_loss: 1.36359906, g_loss: 0.68987834, ae_loss: 0.05544926\n",
      "Step: [2987] total_loss: 2.12290692 d_loss: 1.39059591, g_loss: 0.68047196, ae_loss: 0.05183903\n",
      "Step: [2988] total_loss: 2.15120840 d_loss: 1.36737180, g_loss: 0.72848970, ae_loss: 0.05534687\n",
      "Step: [2989] total_loss: 2.10930085 d_loss: 1.38354933, g_loss: 0.67342359, ae_loss: 0.05232795\n",
      "Step: [2990] total_loss: 2.11991143 d_loss: 1.38507152, g_loss: 0.68275702, ae_loss: 0.05208291\n",
      "Step: [2991] total_loss: 2.12476778 d_loss: 1.36820364, g_loss: 0.70366192, ae_loss: 0.05290206\n",
      "Step: [2992] total_loss: 2.12071085 d_loss: 1.36045766, g_loss: 0.70707864, ae_loss: 0.05317464\n",
      "Step: [2993] total_loss: 2.14212441 d_loss: 1.37832618, g_loss: 0.71197963, ae_loss: 0.05181858\n",
      "Step: [2994] total_loss: 2.12054849 d_loss: 1.37484550, g_loss: 0.69461387, ae_loss: 0.05108902\n",
      "Step: [2995] total_loss: 2.12049484 d_loss: 1.36580563, g_loss: 0.70658231, ae_loss: 0.04810696\n",
      "Step: [2996] total_loss: 2.14899683 d_loss: 1.38752615, g_loss: 0.70899129, ae_loss: 0.05247942\n",
      "Step: [2997] total_loss: 2.14301729 d_loss: 1.38587952, g_loss: 0.70613927, ae_loss: 0.05099860\n",
      "Step: [2998] total_loss: 2.12338829 d_loss: 1.38016427, g_loss: 0.69229257, ae_loss: 0.05093141\n",
      "Step: [2999] total_loss: 2.14076042 d_loss: 1.39202595, g_loss: 0.69675601, ae_loss: 0.05197861\n",
      "Step: [3000] total_loss: 2.15496850 d_loss: 1.39163601, g_loss: 0.71307963, ae_loss: 0.05025278\n",
      "Step: [3001] total_loss: 2.13937426 d_loss: 1.39437056, g_loss: 0.69059062, ae_loss: 0.05441317\n",
      "Step: [3002] total_loss: 2.11870456 d_loss: 1.36250615, g_loss: 0.70402396, ae_loss: 0.05217448\n",
      "Step: [3003] total_loss: 2.14113951 d_loss: 1.39465499, g_loss: 0.69070184, ae_loss: 0.05578281\n",
      "Step: [3004] total_loss: 2.12499404 d_loss: 1.37987471, g_loss: 0.69555950, ae_loss: 0.04955987\n",
      "Step: [3005] total_loss: 2.12108302 d_loss: 1.39266157, g_loss: 0.67601675, ae_loss: 0.05240477\n",
      "Step: [3006] total_loss: 2.11728549 d_loss: 1.38626933, g_loss: 0.68198353, ae_loss: 0.04903254\n",
      "Step: [3007] total_loss: 2.13562584 d_loss: 1.39518988, g_loss: 0.68619901, ae_loss: 0.05423687\n",
      "Step: [3008] total_loss: 2.13651419 d_loss: 1.36598051, g_loss: 0.72114491, ae_loss: 0.04938893\n",
      "Step: [3009] total_loss: 2.14098859 d_loss: 1.39956784, g_loss: 0.68981957, ae_loss: 0.05160117\n",
      "Step: [3010] total_loss: 2.11954069 d_loss: 1.38428295, g_loss: 0.68252671, ae_loss: 0.05273100\n",
      "Step: [3011] total_loss: 2.13204217 d_loss: 1.37081027, g_loss: 0.70490348, ae_loss: 0.05632840\n",
      "Step: [3012] total_loss: 2.14212227 d_loss: 1.39047039, g_loss: 0.69417346, ae_loss: 0.05747841\n",
      "Step: [3013] total_loss: 2.12254214 d_loss: 1.38007379, g_loss: 0.69081759, ae_loss: 0.05165071\n",
      "Step: [3014] total_loss: 2.11950135 d_loss: 1.37714648, g_loss: 0.69707978, ae_loss: 0.04527513\n",
      "Step: [3015] total_loss: 2.13186216 d_loss: 1.39244521, g_loss: 0.68842447, ae_loss: 0.05099263\n",
      "Step: [3016] total_loss: 2.13154435 d_loss: 1.38188791, g_loss: 0.69953835, ae_loss: 0.05011808\n",
      "Step: [3017] total_loss: 2.12662959 d_loss: 1.39573336, g_loss: 0.68100768, ae_loss: 0.04988844\n",
      "Step: [3018] total_loss: 2.11443067 d_loss: 1.38156760, g_loss: 0.68031156, ae_loss: 0.05255155\n",
      "Step: [3019] total_loss: 2.11636639 d_loss: 1.38795519, g_loss: 0.67731547, ae_loss: 0.05109581\n",
      "Step: [3020] total_loss: 2.10768199 d_loss: 1.38347006, g_loss: 0.67108238, ae_loss: 0.05312961\n",
      "Step: [3021] total_loss: 2.12260938 d_loss: 1.39273453, g_loss: 0.68001473, ae_loss: 0.04986012\n",
      "Step: [3022] total_loss: 2.14103222 d_loss: 1.39894915, g_loss: 0.69568098, ae_loss: 0.04640222\n",
      "Step: [3023] total_loss: 2.11864805 d_loss: 1.36490357, g_loss: 0.70354128, ae_loss: 0.05020333\n",
      "Step: [3024] total_loss: 2.10798407 d_loss: 1.36635041, g_loss: 0.68932402, ae_loss: 0.05230966\n",
      "Step: [3025] total_loss: 2.14835119 d_loss: 1.40436172, g_loss: 0.69290137, ae_loss: 0.05108814\n",
      "Step: [3026] total_loss: 2.13255477 d_loss: 1.38199294, g_loss: 0.69468123, ae_loss: 0.05588056\n",
      "Step: [3027] total_loss: 2.13032317 d_loss: 1.37708640, g_loss: 0.69598889, ae_loss: 0.05724790\n",
      "Step: [3028] total_loss: 2.11252427 d_loss: 1.37770212, g_loss: 0.68647259, ae_loss: 0.04834951\n",
      "Step: [3029] total_loss: 2.11808014 d_loss: 1.38714790, g_loss: 0.68061107, ae_loss: 0.05032106\n",
      "Step: [3030] total_loss: 2.12735415 d_loss: 1.39468622, g_loss: 0.68183851, ae_loss: 0.05082955\n",
      "Step: [3031] total_loss: 2.12072873 d_loss: 1.38647532, g_loss: 0.68095630, ae_loss: 0.05329711\n",
      "Step: [3032] total_loss: 2.13701391 d_loss: 1.37639499, g_loss: 0.71224868, ae_loss: 0.04837034\n",
      "Step: [3033] total_loss: 2.12394214 d_loss: 1.39148057, g_loss: 0.68099028, ae_loss: 0.05147127\n",
      "Step: [3034] total_loss: 2.14004755 d_loss: 1.37277436, g_loss: 0.70727408, ae_loss: 0.05999922\n",
      "Step: [3035] total_loss: 2.13571787 d_loss: 1.38511825, g_loss: 0.69763911, ae_loss: 0.05296041\n",
      "Step: [3036] total_loss: 2.11180878 d_loss: 1.36252141, g_loss: 0.69606876, ae_loss: 0.05321852\n",
      "Step: [3037] total_loss: 2.13521838 d_loss: 1.38004112, g_loss: 0.70405936, ae_loss: 0.05111784\n",
      "Step: [3038] total_loss: 2.13667774 d_loss: 1.38823068, g_loss: 0.69426012, ae_loss: 0.05418697\n",
      "Step: [3039] total_loss: 2.10955405 d_loss: 1.35967255, g_loss: 0.70291078, ae_loss: 0.04697067\n",
      "Step: [3040] total_loss: 2.12785244 d_loss: 1.39633155, g_loss: 0.68103230, ae_loss: 0.05048857\n",
      "Step: [3041] total_loss: 2.14232755 d_loss: 1.39308834, g_loss: 0.69354665, ae_loss: 0.05569258\n",
      "Step: [3042] total_loss: 2.12016177 d_loss: 1.37001681, g_loss: 0.69713473, ae_loss: 0.05301023\n",
      "Step: [3043] total_loss: 2.11765575 d_loss: 1.36116743, g_loss: 0.70709759, ae_loss: 0.04939072\n",
      "Step: [3044] total_loss: 2.14276433 d_loss: 1.40570271, g_loss: 0.68695223, ae_loss: 0.05010943\n",
      "Step: [3045] total_loss: 2.11898041 d_loss: 1.37711143, g_loss: 0.69065273, ae_loss: 0.05121627\n",
      "Step: [3046] total_loss: 2.11989212 d_loss: 1.38312793, g_loss: 0.68606615, ae_loss: 0.05069809\n",
      "Step: [3047] total_loss: 2.13157845 d_loss: 1.38308668, g_loss: 0.69497091, ae_loss: 0.05352087\n",
      "Step: [3048] total_loss: 2.11260414 d_loss: 1.35304904, g_loss: 0.70735717, ae_loss: 0.05219794\n",
      "Step: [3049] total_loss: 2.12786961 d_loss: 1.38741469, g_loss: 0.69265550, ae_loss: 0.04779946\n",
      "Step: [3050] total_loss: 2.11526775 d_loss: 1.37917662, g_loss: 0.68314660, ae_loss: 0.05294437\n",
      "Step: [3051] total_loss: 2.11361170 d_loss: 1.36088812, g_loss: 0.70084012, ae_loss: 0.05188362\n",
      "Step: [3052] total_loss: 2.11404705 d_loss: 1.36196423, g_loss: 0.69997758, ae_loss: 0.05210528\n",
      "Step: [3053] total_loss: 2.13648701 d_loss: 1.38529646, g_loss: 0.69983226, ae_loss: 0.05135834\n",
      "Step: [3054] total_loss: 2.12504864 d_loss: 1.37908828, g_loss: 0.69242322, ae_loss: 0.05353728\n",
      "Step: [3055] total_loss: 2.12633848 d_loss: 1.38738191, g_loss: 0.68897390, ae_loss: 0.04998250\n",
      "Step: [3056] total_loss: 2.15841508 d_loss: 1.39888740, g_loss: 0.70817500, ae_loss: 0.05135279\n",
      "Step: [3057] total_loss: 2.12378025 d_loss: 1.37005198, g_loss: 0.69763321, ae_loss: 0.05609514\n",
      "Step: [3058] total_loss: 2.14717388 d_loss: 1.41579521, g_loss: 0.68252528, ae_loss: 0.04885329\n",
      "Step: [3059] total_loss: 2.11487198 d_loss: 1.37562537, g_loss: 0.68916631, ae_loss: 0.05008028\n",
      "Step: [3060] total_loss: 2.12273812 d_loss: 1.38405502, g_loss: 0.68634313, ae_loss: 0.05234004\n",
      "Step: [3061] total_loss: 2.12924194 d_loss: 1.38689351, g_loss: 0.68672091, ae_loss: 0.05562753\n",
      "Step: [3062] total_loss: 2.14014125 d_loss: 1.38811278, g_loss: 0.70287865, ae_loss: 0.04914986\n",
      "Step: [3063] total_loss: 2.12436008 d_loss: 1.37214947, g_loss: 0.69784701, ae_loss: 0.05436344\n",
      "Step: [3064] total_loss: 2.12896776 d_loss: 1.37795866, g_loss: 0.69462264, ae_loss: 0.05638652\n",
      "Step: [3065] total_loss: 2.11854148 d_loss: 1.37614512, g_loss: 0.69147772, ae_loss: 0.05091867\n",
      "Step: [3066] total_loss: 2.09792256 d_loss: 1.36020875, g_loss: 0.68541986, ae_loss: 0.05229386\n",
      "Step: [3067] total_loss: 2.13649440 d_loss: 1.37749887, g_loss: 0.70755792, ae_loss: 0.05143766\n",
      "Step: [3068] total_loss: 2.13149881 d_loss: 1.37796211, g_loss: 0.70508742, ae_loss: 0.04844927\n",
      "Step: [3069] total_loss: 2.13376951 d_loss: 1.38296080, g_loss: 0.69996357, ae_loss: 0.05084504\n",
      "Step: [3070] total_loss: 2.14628839 d_loss: 1.40512526, g_loss: 0.69088256, ae_loss: 0.05028075\n",
      "Step: [3071] total_loss: 2.13362408 d_loss: 1.37219322, g_loss: 0.70648491, ae_loss: 0.05494593\n",
      "Step: [3072] total_loss: 2.12994385 d_loss: 1.37518907, g_loss: 0.70385045, ae_loss: 0.05090432\n",
      "Step: [3073] total_loss: 2.10503030 d_loss: 1.36005998, g_loss: 0.69379085, ae_loss: 0.05117943\n",
      "Step: [3074] total_loss: 2.14520121 d_loss: 1.35686326, g_loss: 0.73453438, ae_loss: 0.05380343\n",
      "Step: [3075] total_loss: 2.13388014 d_loss: 1.36834025, g_loss: 0.71136618, ae_loss: 0.05417361\n",
      "Step: [3076] total_loss: 2.13795710 d_loss: 1.39899957, g_loss: 0.68562120, ae_loss: 0.05333638\n",
      "Step: [3077] total_loss: 2.12568116 d_loss: 1.38802505, g_loss: 0.68487144, ae_loss: 0.05278462\n",
      "Step: [3078] total_loss: 2.09868813 d_loss: 1.36247206, g_loss: 0.68623888, ae_loss: 0.04997734\n",
      "Step: [3079] total_loss: 2.13778639 d_loss: 1.40680265, g_loss: 0.67987919, ae_loss: 0.05110450\n",
      "Step: [3080] total_loss: 2.14248371 d_loss: 1.39377081, g_loss: 0.69692385, ae_loss: 0.05178908\n",
      "Step: [3081] total_loss: 2.14666176 d_loss: 1.39479494, g_loss: 0.70331264, ae_loss: 0.04855432\n",
      "Step: [3082] total_loss: 2.13492966 d_loss: 1.37218022, g_loss: 0.71165025, ae_loss: 0.05109916\n",
      "Step: [3083] total_loss: 2.14921141 d_loss: 1.39538777, g_loss: 0.69915175, ae_loss: 0.05467183\n",
      "Step: [3084] total_loss: 2.13691521 d_loss: 1.39680755, g_loss: 0.68824267, ae_loss: 0.05186495\n",
      "Step: [3085] total_loss: 2.11361337 d_loss: 1.35468543, g_loss: 0.71208632, ae_loss: 0.04684165\n",
      "Step: [3086] total_loss: 2.15064096 d_loss: 1.38869452, g_loss: 0.70835489, ae_loss: 0.05359147\n",
      "Step: [3087] total_loss: 2.12629604 d_loss: 1.36027026, g_loss: 0.71434796, ae_loss: 0.05167787\n",
      "Step: [3088] total_loss: 2.12476325 d_loss: 1.38690603, g_loss: 0.68787599, ae_loss: 0.04998128\n",
      "Step: [3089] total_loss: 2.14044499 d_loss: 1.40323865, g_loss: 0.68495083, ae_loss: 0.05225552\n",
      "Step: [3090] total_loss: 2.13685989 d_loss: 1.39327693, g_loss: 0.69246948, ae_loss: 0.05111367\n",
      "Step: [3091] total_loss: 2.12582111 d_loss: 1.38067162, g_loss: 0.69076085, ae_loss: 0.05438879\n",
      "Step: [3092] total_loss: 2.13569832 d_loss: 1.39535463, g_loss: 0.68788064, ae_loss: 0.05246309\n",
      "Step: [3093] total_loss: 2.10645390 d_loss: 1.38470912, g_loss: 0.67318332, ae_loss: 0.04856154\n",
      "Step: [3094] total_loss: 2.15393019 d_loss: 1.41319418, g_loss: 0.69182169, ae_loss: 0.04891426\n",
      "Step: [3095] total_loss: 2.15038919 d_loss: 1.40194750, g_loss: 0.69286770, ae_loss: 0.05557391\n",
      "Step: [3096] total_loss: 2.13098049 d_loss: 1.36767447, g_loss: 0.71036601, ae_loss: 0.05293987\n",
      "Step: [3097] total_loss: 2.11322498 d_loss: 1.36585438, g_loss: 0.69323605, ae_loss: 0.05413452\n",
      "Step: [3098] total_loss: 2.11502123 d_loss: 1.38055706, g_loss: 0.68428206, ae_loss: 0.05018198\n",
      "Step: [3099] total_loss: 2.13183832 d_loss: 1.37390327, g_loss: 0.70780480, ae_loss: 0.05013013\n",
      "Step: [3100] total_loss: 2.13828969 d_loss: 1.39093387, g_loss: 0.69504881, ae_loss: 0.05230701\n",
      "Step: [3101] total_loss: 2.16049051 d_loss: 1.39569938, g_loss: 0.71056843, ae_loss: 0.05422275\n",
      "Step: [3102] total_loss: 2.12438273 d_loss: 1.36753166, g_loss: 0.70162427, ae_loss: 0.05522677\n",
      "Step: [3103] total_loss: 2.15326810 d_loss: 1.38368571, g_loss: 0.71336842, ae_loss: 0.05621403\n",
      "Step: [3104] total_loss: 2.10955358 d_loss: 1.37544727, g_loss: 0.68034601, ae_loss: 0.05376026\n",
      "Step: [3105] total_loss: 2.12818384 d_loss: 1.37539065, g_loss: 0.69988412, ae_loss: 0.05290899\n",
      "Step: [3106] total_loss: 2.11596489 d_loss: 1.38200259, g_loss: 0.68214309, ae_loss: 0.05181935\n",
      "Step: [3107] total_loss: 2.11753774 d_loss: 1.39358509, g_loss: 0.67547208, ae_loss: 0.04848054\n",
      "Step: [3108] total_loss: 2.12898755 d_loss: 1.38009715, g_loss: 0.69696558, ae_loss: 0.05192483\n",
      "Step: [3109] total_loss: 2.13581443 d_loss: 1.37149620, g_loss: 0.71587086, ae_loss: 0.04844739\n",
      "Step: [3110] total_loss: 2.12484884 d_loss: 1.36892557, g_loss: 0.70091790, ae_loss: 0.05500530\n",
      "Step: [3111] total_loss: 2.15700102 d_loss: 1.41696548, g_loss: 0.68735701, ae_loss: 0.05267849\n",
      "Step: [3112] total_loss: 2.12493706 d_loss: 1.37853420, g_loss: 0.69957668, ae_loss: 0.04682625\n",
      "Step: [3113] total_loss: 2.12534761 d_loss: 1.38237929, g_loss: 0.68950576, ae_loss: 0.05346250\n",
      "Step: [3114] total_loss: 2.12712359 d_loss: 1.38178635, g_loss: 0.69618082, ae_loss: 0.04915638\n",
      "Step: [3115] total_loss: 2.11419201 d_loss: 1.36102211, g_loss: 0.70172620, ae_loss: 0.05144373\n",
      "Step: [3116] total_loss: 2.10652065 d_loss: 1.37808657, g_loss: 0.67754853, ae_loss: 0.05088561\n",
      "Step: [3117] total_loss: 2.10988522 d_loss: 1.37600577, g_loss: 0.68548453, ae_loss: 0.04839479\n",
      "Step: [3118] total_loss: 2.12425280 d_loss: 1.33507276, g_loss: 0.73387802, ae_loss: 0.05530200\n",
      "Step: [3119] total_loss: 2.12809515 d_loss: 1.39372432, g_loss: 0.68353868, ae_loss: 0.05083222\n",
      "Step: [3120] total_loss: 2.11187005 d_loss: 1.37288702, g_loss: 0.68295240, ae_loss: 0.05603058\n",
      "Step: [3121] total_loss: 2.13523293 d_loss: 1.39102530, g_loss: 0.69155967, ae_loss: 0.05264806\n",
      "Step: [3122] total_loss: 2.12320423 d_loss: 1.40015960, g_loss: 0.67594814, ae_loss: 0.04709664\n",
      "Step: [3123] total_loss: 2.13275051 d_loss: 1.37434936, g_loss: 0.70632333, ae_loss: 0.05207777\n",
      "Step: [3124] total_loss: 2.11719918 d_loss: 1.37014961, g_loss: 0.69581819, ae_loss: 0.05123135\n",
      "Step: [3125] total_loss: 2.13906336 d_loss: 1.38431489, g_loss: 0.70411175, ae_loss: 0.05063671\n",
      "Step: [3126] total_loss: 2.15315485 d_loss: 1.38426614, g_loss: 0.71636927, ae_loss: 0.05251959\n",
      "Step: [3127] total_loss: 2.13107705 d_loss: 1.39400041, g_loss: 0.68881285, ae_loss: 0.04826384\n",
      "Step: [3128] total_loss: 2.11979055 d_loss: 1.37730122, g_loss: 0.69384098, ae_loss: 0.04864839\n",
      "Step: [3129] total_loss: 2.13120914 d_loss: 1.39913392, g_loss: 0.68172807, ae_loss: 0.05034723\n",
      "Step: [3130] total_loss: 2.10812068 d_loss: 1.36432862, g_loss: 0.68567443, ae_loss: 0.05811758\n",
      "Step: [3131] total_loss: 2.12603164 d_loss: 1.38401401, g_loss: 0.68645978, ae_loss: 0.05555779\n",
      "Step: [3132] total_loss: 2.12311125 d_loss: 1.36160541, g_loss: 0.71077609, ae_loss: 0.05072991\n",
      "Step: [3133] total_loss: 2.12550235 d_loss: 1.39152718, g_loss: 0.67685866, ae_loss: 0.05711657\n",
      "Step: [3134] total_loss: 2.12066841 d_loss: 1.37210023, g_loss: 0.70013273, ae_loss: 0.04843545\n",
      "Step: [3135] total_loss: 2.12992477 d_loss: 1.39903736, g_loss: 0.68025219, ae_loss: 0.05063505\n",
      "Step: [3136] total_loss: 2.12406754 d_loss: 1.38060534, g_loss: 0.69103205, ae_loss: 0.05243016\n",
      "Step: [3137] total_loss: 2.12185431 d_loss: 1.35955787, g_loss: 0.71230054, ae_loss: 0.04999592\n",
      "Step: [3138] total_loss: 2.12368774 d_loss: 1.37305963, g_loss: 0.69455242, ae_loss: 0.05607554\n",
      "Step: [3139] total_loss: 2.11189413 d_loss: 1.36377883, g_loss: 0.69457901, ae_loss: 0.05353614\n",
      "Step: [3140] total_loss: 2.12187672 d_loss: 1.39476240, g_loss: 0.67504299, ae_loss: 0.05207124\n",
      "Step: [3141] total_loss: 2.12211132 d_loss: 1.37991345, g_loss: 0.69381040, ae_loss: 0.04838745\n",
      "Step: [3142] total_loss: 2.12649536 d_loss: 1.38917446, g_loss: 0.68493432, ae_loss: 0.05238649\n",
      "Step: [3143] total_loss: 2.10587168 d_loss: 1.36538649, g_loss: 0.69004130, ae_loss: 0.05044384\n",
      "Step: [3144] total_loss: 2.12909985 d_loss: 1.39233661, g_loss: 0.68295050, ae_loss: 0.05381265\n",
      "Step: [3145] total_loss: 2.14379025 d_loss: 1.40215635, g_loss: 0.69067085, ae_loss: 0.05096306\n",
      "Step: [3146] total_loss: 2.13278437 d_loss: 1.38463891, g_loss: 0.69496274, ae_loss: 0.05318273\n",
      "Step: [3147] total_loss: 2.13707685 d_loss: 1.38862121, g_loss: 0.69978225, ae_loss: 0.04867340\n",
      "Step: [3148] total_loss: 2.13691497 d_loss: 1.38842010, g_loss: 0.69429338, ae_loss: 0.05420153\n",
      "Step: [3149] total_loss: 2.10062242 d_loss: 1.36024952, g_loss: 0.68890816, ae_loss: 0.05146467\n",
      "Step: [3150] total_loss: 2.12201834 d_loss: 1.37216902, g_loss: 0.69819653, ae_loss: 0.05165262\n",
      "Step: [3151] total_loss: 2.12626410 d_loss: 1.38524246, g_loss: 0.68795085, ae_loss: 0.05307062\n",
      "Step: [3152] total_loss: 2.13130569 d_loss: 1.39177048, g_loss: 0.68558657, ae_loss: 0.05394848\n",
      "Step: [3153] total_loss: 2.12747431 d_loss: 1.38663292, g_loss: 0.69032896, ae_loss: 0.05051233\n",
      "Step: [3154] total_loss: 2.12254310 d_loss: 1.38055599, g_loss: 0.69314170, ae_loss: 0.04884541\n",
      "Step: [3155] total_loss: 2.12593436 d_loss: 1.38792527, g_loss: 0.68905932, ae_loss: 0.04894978\n",
      "Step: [3156] total_loss: 2.14086103 d_loss: 1.37053943, g_loss: 0.71717668, ae_loss: 0.05314501\n",
      "Step: [3157] total_loss: 2.11906672 d_loss: 1.37260985, g_loss: 0.69568217, ae_loss: 0.05077481\n",
      "Step: [3158] total_loss: 2.13654995 d_loss: 1.40093803, g_loss: 0.68458569, ae_loss: 0.05102619\n",
      "Step: [3159] total_loss: 2.10920191 d_loss: 1.36883545, g_loss: 0.68906039, ae_loss: 0.05130596\n",
      "Step: [3160] total_loss: 2.12563968 d_loss: 1.37336373, g_loss: 0.69822699, ae_loss: 0.05404884\n",
      "Step: [3161] total_loss: 2.14539766 d_loss: 1.37382340, g_loss: 0.72078621, ae_loss: 0.05078793\n",
      "Step: [3162] total_loss: 2.16713333 d_loss: 1.38966990, g_loss: 0.72327113, ae_loss: 0.05419222\n",
      "Step: [3163] total_loss: 2.14628100 d_loss: 1.37582111, g_loss: 0.72079206, ae_loss: 0.04966787\n",
      "Step: [3164] total_loss: 2.15904856 d_loss: 1.38965249, g_loss: 0.71356767, ae_loss: 0.05582846\n",
      "Step: [3165] total_loss: 2.11676598 d_loss: 1.36849928, g_loss: 0.69699144, ae_loss: 0.05127539\n",
      "Step: [3166] total_loss: 2.11602855 d_loss: 1.38507509, g_loss: 0.68189776, ae_loss: 0.04905567\n",
      "Step: [3167] total_loss: 2.13355637 d_loss: 1.38920820, g_loss: 0.69273734, ae_loss: 0.05161066\n",
      "Step: [3168] total_loss: 2.12027168 d_loss: 1.36851096, g_loss: 0.69932461, ae_loss: 0.05243619\n",
      "Step: [3169] total_loss: 2.14754796 d_loss: 1.39894748, g_loss: 0.69604492, ae_loss: 0.05255561\n",
      "Step: [3170] total_loss: 2.13257313 d_loss: 1.36590278, g_loss: 0.71444631, ae_loss: 0.05222392\n",
      "Step: [3171] total_loss: 2.12409234 d_loss: 1.38472271, g_loss: 0.68818355, ae_loss: 0.05118612\n",
      "Step: [3172] total_loss: 2.12762022 d_loss: 1.40679646, g_loss: 0.66266656, ae_loss: 0.05815733\n",
      "Step: [3173] total_loss: 2.11238956 d_loss: 1.37426221, g_loss: 0.68664348, ae_loss: 0.05148378\n",
      "Step: [3174] total_loss: 2.13701963 d_loss: 1.39411819, g_loss: 0.69229096, ae_loss: 0.05061058\n",
      "Step: [3175] total_loss: 2.12675309 d_loss: 1.38728499, g_loss: 0.68558288, ae_loss: 0.05388523\n",
      "Step: [3176] total_loss: 2.13352585 d_loss: 1.39370489, g_loss: 0.68463421, ae_loss: 0.05518686\n",
      "Step: [3177] total_loss: 2.12870646 d_loss: 1.38615704, g_loss: 0.68777442, ae_loss: 0.05477486\n",
      "Step: [3178] total_loss: 2.13322115 d_loss: 1.37008595, g_loss: 0.71357262, ae_loss: 0.04956259\n",
      "Step: [3179] total_loss: 2.12669921 d_loss: 1.38300276, g_loss: 0.68707949, ae_loss: 0.05661693\n",
      "Step: [3180] total_loss: 2.11328697 d_loss: 1.35622025, g_loss: 0.70906341, ae_loss: 0.04800333\n",
      "Step: [3181] total_loss: 2.12942553 d_loss: 1.39166999, g_loss: 0.69046557, ae_loss: 0.04728993\n",
      "Step: [3182] total_loss: 2.13260269 d_loss: 1.37854695, g_loss: 0.70731652, ae_loss: 0.04673910\n",
      "Step: [3183] total_loss: 2.14235401 d_loss: 1.37975109, g_loss: 0.71149892, ae_loss: 0.05110394\n",
      "Step: [3184] total_loss: 2.11400008 d_loss: 1.35072744, g_loss: 0.71309125, ae_loss: 0.05018144\n",
      "Step: [3185] total_loss: 2.12448907 d_loss: 1.37516522, g_loss: 0.69646722, ae_loss: 0.05285670\n",
      "Step: [3186] total_loss: 2.11739206 d_loss: 1.37179232, g_loss: 0.69530046, ae_loss: 0.05029912\n",
      "Step: [3187] total_loss: 2.11457753 d_loss: 1.37466908, g_loss: 0.68759853, ae_loss: 0.05230999\n",
      "Step: [3188] total_loss: 2.11793780 d_loss: 1.36513531, g_loss: 0.71056837, ae_loss: 0.04223406\n",
      "Step: [3189] total_loss: 2.12691116 d_loss: 1.38091874, g_loss: 0.69297171, ae_loss: 0.05302064\n",
      "Step: [3190] total_loss: 2.10554552 d_loss: 1.36915803, g_loss: 0.68431252, ae_loss: 0.05207491\n",
      "Step: [3191] total_loss: 2.12431049 d_loss: 1.39585209, g_loss: 0.67674053, ae_loss: 0.05171797\n",
      "Step: [3192] total_loss: 2.12737560 d_loss: 1.37315273, g_loss: 0.70269775, ae_loss: 0.05152506\n",
      "Step: [3193] total_loss: 2.12211418 d_loss: 1.36591470, g_loss: 0.70319927, ae_loss: 0.05300006\n",
      "Step: [3194] total_loss: 2.13491464 d_loss: 1.39356089, g_loss: 0.68982041, ae_loss: 0.05153333\n",
      "Step: [3195] total_loss: 2.12755060 d_loss: 1.39116657, g_loss: 0.68575788, ae_loss: 0.05062630\n",
      "Step: [3196] total_loss: 2.12795734 d_loss: 1.36534429, g_loss: 0.71338391, ae_loss: 0.04922906\n",
      "Step: [3197] total_loss: 2.13484955 d_loss: 1.37041628, g_loss: 0.71479809, ae_loss: 0.04963532\n",
      "Step: [3198] total_loss: 2.12309933 d_loss: 1.37202370, g_loss: 0.69840610, ae_loss: 0.05266969\n",
      "Step: [3199] total_loss: 2.12219572 d_loss: 1.37432349, g_loss: 0.69853437, ae_loss: 0.04933786\n",
      "Step: [3200] total_loss: 2.12200689 d_loss: 1.38319468, g_loss: 0.68981659, ae_loss: 0.04899548\n",
      "Step: [3201] total_loss: 2.09867525 d_loss: 1.37035680, g_loss: 0.67622453, ae_loss: 0.05209383\n",
      "Step: [3202] total_loss: 2.13596487 d_loss: 1.38574779, g_loss: 0.69935441, ae_loss: 0.05086251\n",
      "Step: [3203] total_loss: 2.13502145 d_loss: 1.38525736, g_loss: 0.69821942, ae_loss: 0.05154471\n",
      "Step: [3204] total_loss: 2.14930344 d_loss: 1.38430357, g_loss: 0.71092761, ae_loss: 0.05407230\n",
      "Step: [3205] total_loss: 2.10529566 d_loss: 1.34581792, g_loss: 0.71071875, ae_loss: 0.04875882\n",
      "Step: [3206] total_loss: 2.10937595 d_loss: 1.35139692, g_loss: 0.70617473, ae_loss: 0.05180431\n",
      "Step: [3207] total_loss: 2.14251900 d_loss: 1.40198267, g_loss: 0.69047701, ae_loss: 0.05005918\n",
      "Step: [3208] total_loss: 2.13230824 d_loss: 1.39122307, g_loss: 0.68822062, ae_loss: 0.05286452\n",
      "Step: [3209] total_loss: 2.13334036 d_loss: 1.38194394, g_loss: 0.69993138, ae_loss: 0.05146510\n",
      "Step: [3210] total_loss: 2.11565304 d_loss: 1.37375140, g_loss: 0.68732202, ae_loss: 0.05457944\n",
      "Step: [3211] total_loss: 2.10771179 d_loss: 1.37821496, g_loss: 0.67958987, ae_loss: 0.04990700\n",
      "Step: [3212] total_loss: 2.12406945 d_loss: 1.37452245, g_loss: 0.69697124, ae_loss: 0.05257578\n",
      "Step: [3213] total_loss: 2.14301467 d_loss: 1.37378502, g_loss: 0.71863788, ae_loss: 0.05059183\n",
      "Step: [3214] total_loss: 2.11834788 d_loss: 1.37408042, g_loss: 0.69811499, ae_loss: 0.04615245\n",
      "Step: [3215] total_loss: 2.14284682 d_loss: 1.38322186, g_loss: 0.70828533, ae_loss: 0.05133958\n",
      "Step: [3216] total_loss: 2.13884783 d_loss: 1.37763333, g_loss: 0.70694965, ae_loss: 0.05426477\n",
      "Step: [3217] total_loss: 2.14251018 d_loss: 1.39276338, g_loss: 0.69741923, ae_loss: 0.05232757\n",
      "Step: [3218] total_loss: 2.11510038 d_loss: 1.35234654, g_loss: 0.70934153, ae_loss: 0.05341248\n",
      "Step: [3219] total_loss: 2.11417580 d_loss: 1.37226510, g_loss: 0.68967450, ae_loss: 0.05223615\n",
      "Step: [3220] total_loss: 2.14638686 d_loss: 1.40326691, g_loss: 0.68874836, ae_loss: 0.05437161\n",
      "Step: [3221] total_loss: 2.12131095 d_loss: 1.38135612, g_loss: 0.68766177, ae_loss: 0.05229311\n",
      "Step: [3222] total_loss: 2.14601707 d_loss: 1.38426423, g_loss: 0.70954168, ae_loss: 0.05221110\n",
      "Step: [3223] total_loss: 2.11341095 d_loss: 1.36196697, g_loss: 0.69982159, ae_loss: 0.05162230\n",
      "Step: [3224] total_loss: 2.10664701 d_loss: 1.37747204, g_loss: 0.67707878, ae_loss: 0.05209627\n",
      "Step: [3225] total_loss: 2.14877796 d_loss: 1.39760172, g_loss: 0.69545949, ae_loss: 0.05571661\n",
      "Step: [3226] total_loss: 2.11726475 d_loss: 1.38411987, g_loss: 0.68169415, ae_loss: 0.05145071\n",
      "Step: [3227] total_loss: 2.12150836 d_loss: 1.37101042, g_loss: 0.70111144, ae_loss: 0.04938649\n",
      "Step: [3228] total_loss: 2.12858248 d_loss: 1.37807775, g_loss: 0.69688201, ae_loss: 0.05362274\n",
      "Step: [3229] total_loss: 2.13834286 d_loss: 1.39164925, g_loss: 0.69341540, ae_loss: 0.05327833\n",
      "Step: [3230] total_loss: 2.11897159 d_loss: 1.37309647, g_loss: 0.69225371, ae_loss: 0.05362144\n",
      "Step: [3231] total_loss: 2.12258387 d_loss: 1.38807440, g_loss: 0.67841554, ae_loss: 0.05609376\n",
      "Step: [3232] total_loss: 2.10710573 d_loss: 1.37544119, g_loss: 0.68193597, ae_loss: 0.04972863\n",
      "Step: [3233] total_loss: 2.11491799 d_loss: 1.36547589, g_loss: 0.70034075, ae_loss: 0.04910138\n",
      "Step: [3234] total_loss: 2.11912441 d_loss: 1.37711585, g_loss: 0.68953192, ae_loss: 0.05247653\n",
      "Step: [3235] total_loss: 2.13933158 d_loss: 1.39982080, g_loss: 0.68719667, ae_loss: 0.05231421\n",
      "Step: [3236] total_loss: 2.14108920 d_loss: 1.38207901, g_loss: 0.70662385, ae_loss: 0.05238624\n",
      "Step: [3237] total_loss: 2.14465523 d_loss: 1.37416220, g_loss: 0.71551973, ae_loss: 0.05497322\n",
      "Step: [3238] total_loss: 2.12664342 d_loss: 1.37110424, g_loss: 0.70487666, ae_loss: 0.05066249\n",
      "Step: [3239] total_loss: 2.12334204 d_loss: 1.40007377, g_loss: 0.67340446, ae_loss: 0.04986380\n",
      "Step: [3240] total_loss: 2.13013029 d_loss: 1.38600159, g_loss: 0.69173008, ae_loss: 0.05239855\n",
      "Step: [3241] total_loss: 2.14475870 d_loss: 1.39489996, g_loss: 0.69741058, ae_loss: 0.05244799\n",
      "Step: [3242] total_loss: 2.13202333 d_loss: 1.39047575, g_loss: 0.68890989, ae_loss: 0.05263767\n",
      "Step: [3243] total_loss: 2.12952185 d_loss: 1.39037967, g_loss: 0.69171584, ae_loss: 0.04742623\n",
      "Step: [3244] total_loss: 2.15111136 d_loss: 1.37534213, g_loss: 0.72328001, ae_loss: 0.05248912\n",
      "Step: [3245] total_loss: 2.11958289 d_loss: 1.38200843, g_loss: 0.68886763, ae_loss: 0.04870685\n",
      "Step: [3246] total_loss: 2.13079548 d_loss: 1.37147951, g_loss: 0.70568836, ae_loss: 0.05362746\n",
      "Step: [3247] total_loss: 2.11421442 d_loss: 1.38749421, g_loss: 0.67460716, ae_loss: 0.05211307\n",
      "Step: [3248] total_loss: 2.12630939 d_loss: 1.37526560, g_loss: 0.70067155, ae_loss: 0.05037209\n",
      "Step: [3249] total_loss: 2.11170578 d_loss: 1.36890328, g_loss: 0.69476843, ae_loss: 0.04803402\n",
      "Step: [3250] total_loss: 2.12640715 d_loss: 1.38984632, g_loss: 0.68670797, ae_loss: 0.04985273\n",
      "Step: [3251] total_loss: 2.11021852 d_loss: 1.37466908, g_loss: 0.68448818, ae_loss: 0.05106120\n",
      "Step: [3252] total_loss: 2.12076664 d_loss: 1.38791203, g_loss: 0.68206024, ae_loss: 0.05079451\n",
      "Step: [3253] total_loss: 2.13722396 d_loss: 1.39159536, g_loss: 0.69460166, ae_loss: 0.05102696\n",
      "Step: [3254] total_loss: 2.14389706 d_loss: 1.37248278, g_loss: 0.71901661, ae_loss: 0.05239776\n",
      "Step: [3255] total_loss: 2.14071107 d_loss: 1.36788595, g_loss: 0.71899575, ae_loss: 0.05382948\n",
      "Step: [3256] total_loss: 2.13099790 d_loss: 1.36095405, g_loss: 0.71913773, ae_loss: 0.05090617\n",
      "Step: [3257] total_loss: 2.16504979 d_loss: 1.39535260, g_loss: 0.71781933, ae_loss: 0.05187784\n",
      "Step: [3258] total_loss: 2.14563560 d_loss: 1.41438699, g_loss: 0.67871797, ae_loss: 0.05253058\n",
      "Step: [3259] total_loss: 2.14445043 d_loss: 1.37799561, g_loss: 0.71758080, ae_loss: 0.04887399\n",
      "Step: [3260] total_loss: 2.15855932 d_loss: 1.38629150, g_loss: 0.72305191, ae_loss: 0.04921595\n",
      "Step: [3261] total_loss: 2.13098431 d_loss: 1.39557314, g_loss: 0.68711221, ae_loss: 0.04829902\n",
      "Step: [3262] total_loss: 2.11860871 d_loss: 1.38676095, g_loss: 0.68079191, ae_loss: 0.05105594\n",
      "Step: [3263] total_loss: 2.12660742 d_loss: 1.39023197, g_loss: 0.68456781, ae_loss: 0.05180774\n",
      "Step: [3264] total_loss: 2.12160873 d_loss: 1.38198853, g_loss: 0.69065064, ae_loss: 0.04896959\n",
      "Step: [3265] total_loss: 2.12775493 d_loss: 1.38861203, g_loss: 0.68465245, ae_loss: 0.05449046\n",
      "Step: [3266] total_loss: 2.14535952 d_loss: 1.39174438, g_loss: 0.69912350, ae_loss: 0.05449165\n",
      "Step: [3267] total_loss: 2.13668442 d_loss: 1.35889125, g_loss: 0.72636437, ae_loss: 0.05142884\n",
      "Step: [3268] total_loss: 2.13568854 d_loss: 1.39023793, g_loss: 0.69190085, ae_loss: 0.05354978\n",
      "Step: [3269] total_loss: 2.12034154 d_loss: 1.36356473, g_loss: 0.70661938, ae_loss: 0.05015744\n",
      "Step: [3270] total_loss: 2.10908580 d_loss: 1.38345838, g_loss: 0.67429441, ae_loss: 0.05133292\n",
      "Step: [3271] total_loss: 2.10281706 d_loss: 1.38046861, g_loss: 0.66694748, ae_loss: 0.05540085\n",
      "Step: [3272] total_loss: 2.09882307 d_loss: 1.37567413, g_loss: 0.67120636, ae_loss: 0.05194277\n",
      "Step: [3273] total_loss: 2.12250614 d_loss: 1.38720393, g_loss: 0.68585193, ae_loss: 0.04945026\n",
      "Step: [3274] total_loss: 2.10914421 d_loss: 1.37542534, g_loss: 0.68676931, ae_loss: 0.04694965\n",
      "Step: [3275] total_loss: 2.13516760 d_loss: 1.38185966, g_loss: 0.70575285, ae_loss: 0.04755500\n",
      "Step: [3276] total_loss: 2.14947224 d_loss: 1.40201533, g_loss: 0.69767487, ae_loss: 0.04978213\n",
      "Step: [3277] total_loss: 2.13625669 d_loss: 1.39065456, g_loss: 0.69424880, ae_loss: 0.05135349\n",
      "Step: [3278] total_loss: 2.13713503 d_loss: 1.38108635, g_loss: 0.70320451, ae_loss: 0.05284413\n",
      "Step: [3279] total_loss: 2.14770579 d_loss: 1.39720523, g_loss: 0.70124727, ae_loss: 0.04925328\n",
      "Step: [3280] total_loss: 2.10440350 d_loss: 1.35194802, g_loss: 0.69956893, ae_loss: 0.05288643\n",
      "Step: [3281] total_loss: 2.14040518 d_loss: 1.39832187, g_loss: 0.69477224, ae_loss: 0.04731106\n",
      "Step: [3282] total_loss: 2.12705255 d_loss: 1.37365866, g_loss: 0.70236355, ae_loss: 0.05103032\n",
      "Step: [3283] total_loss: 2.11763430 d_loss: 1.33723009, g_loss: 0.72426498, ae_loss: 0.05613929\n",
      "Step: [3284] total_loss: 2.14747500 d_loss: 1.41173089, g_loss: 0.68557703, ae_loss: 0.05016711\n",
      "Step: [3285] total_loss: 2.14494491 d_loss: 1.38220906, g_loss: 0.70710915, ae_loss: 0.05562658\n",
      "Step: [3286] total_loss: 2.11484528 d_loss: 1.36383176, g_loss: 0.70345891, ae_loss: 0.04755479\n",
      "Step: [3287] total_loss: 2.11483502 d_loss: 1.36839414, g_loss: 0.69423479, ae_loss: 0.05220610\n",
      "Step: [3288] total_loss: 2.12848043 d_loss: 1.38753581, g_loss: 0.69067132, ae_loss: 0.05027321\n",
      "Step: [3289] total_loss: 2.11934710 d_loss: 1.38913286, g_loss: 0.67916840, ae_loss: 0.05104593\n",
      "Step: [3290] total_loss: 2.12348080 d_loss: 1.38475859, g_loss: 0.68823147, ae_loss: 0.05049086\n",
      "Step: [3291] total_loss: 2.15939093 d_loss: 1.42829013, g_loss: 0.67882031, ae_loss: 0.05228045\n",
      "Step: [3292] total_loss: 2.11630249 d_loss: 1.36587584, g_loss: 0.69525456, ae_loss: 0.05517203\n",
      "Step: [3293] total_loss: 2.15970802 d_loss: 1.39957583, g_loss: 0.71018636, ae_loss: 0.04994593\n",
      "Step: [3294] total_loss: 2.12357211 d_loss: 1.39340627, g_loss: 0.67811674, ae_loss: 0.05204912\n",
      "Step: [3295] total_loss: 2.11495066 d_loss: 1.38247108, g_loss: 0.68548590, ae_loss: 0.04699359\n",
      "Step: [3296] total_loss: 2.11870408 d_loss: 1.37716961, g_loss: 0.68777061, ae_loss: 0.05376383\n",
      "Step: [3297] total_loss: 2.13969994 d_loss: 1.37810969, g_loss: 0.70775682, ae_loss: 0.05383331\n",
      "Step: [3298] total_loss: 2.13125300 d_loss: 1.38849497, g_loss: 0.68670672, ae_loss: 0.05605136\n",
      "Step: [3299] total_loss: 2.13033509 d_loss: 1.38635457, g_loss: 0.69267005, ae_loss: 0.05131059\n",
      "Step: [3300] total_loss: 2.12819195 d_loss: 1.38305449, g_loss: 0.69441283, ae_loss: 0.05072465\n",
      "Step: [3301] total_loss: 2.13672590 d_loss: 1.39719856, g_loss: 0.68724430, ae_loss: 0.05228295\n",
      "Step: [3302] total_loss: 2.13536716 d_loss: 1.38492286, g_loss: 0.70447528, ae_loss: 0.04596902\n",
      "Step: [3303] total_loss: 2.12070918 d_loss: 1.38941848, g_loss: 0.67960000, ae_loss: 0.05169066\n",
      "Step: [3304] total_loss: 2.12316418 d_loss: 1.37814915, g_loss: 0.69582474, ae_loss: 0.04919033\n",
      "Step: [3305] total_loss: 2.11182165 d_loss: 1.38931954, g_loss: 0.67178857, ae_loss: 0.05071353\n",
      "Step: [3306] total_loss: 2.12537766 d_loss: 1.37358308, g_loss: 0.70244145, ae_loss: 0.04935305\n",
      "Step: [3307] total_loss: 2.12011313 d_loss: 1.38971126, g_loss: 0.67805880, ae_loss: 0.05234307\n",
      "Step: [3308] total_loss: 2.12473154 d_loss: 1.38705277, g_loss: 0.69100022, ae_loss: 0.04667857\n",
      "Step: [3309] total_loss: 2.13666415 d_loss: 1.38037539, g_loss: 0.70158356, ae_loss: 0.05470525\n",
      "Step: [3310] total_loss: 2.12914681 d_loss: 1.38050091, g_loss: 0.69295412, ae_loss: 0.05569181\n",
      "Step: [3311] total_loss: 2.10996628 d_loss: 1.38245773, g_loss: 0.67507958, ae_loss: 0.05242883\n",
      "Step: [3312] total_loss: 2.12181854 d_loss: 1.39696050, g_loss: 0.67549276, ae_loss: 0.04936542\n",
      "Step: [3313] total_loss: 2.10609722 d_loss: 1.36927342, g_loss: 0.68538296, ae_loss: 0.05144086\n",
      "Step: [3314] total_loss: 2.12434983 d_loss: 1.38695991, g_loss: 0.68681055, ae_loss: 0.05057932\n",
      "Step: [3315] total_loss: 2.12180400 d_loss: 1.39065623, g_loss: 0.67990798, ae_loss: 0.05123989\n",
      "Step: [3316] total_loss: 2.12743497 d_loss: 1.34568727, g_loss: 0.73119450, ae_loss: 0.05055317\n",
      "Step: [3317] total_loss: 2.12945557 d_loss: 1.36808848, g_loss: 0.71081674, ae_loss: 0.05055022\n",
      "Step: [3318] total_loss: 2.12230134 d_loss: 1.35768020, g_loss: 0.71291614, ae_loss: 0.05170501\n",
      "Step: [3319] total_loss: 2.14124632 d_loss: 1.37574244, g_loss: 0.70813388, ae_loss: 0.05737006\n",
      "Step: [3320] total_loss: 2.10913062 d_loss: 1.35683799, g_loss: 0.69596159, ae_loss: 0.05633106\n",
      "Step: [3321] total_loss: 2.12829494 d_loss: 1.37591624, g_loss: 0.70063078, ae_loss: 0.05174783\n",
      "Step: [3322] total_loss: 2.12514210 d_loss: 1.37309265, g_loss: 0.70082742, ae_loss: 0.05122191\n",
      "Step: [3323] total_loss: 2.10278487 d_loss: 1.34272707, g_loss: 0.70640546, ae_loss: 0.05365232\n",
      "Step: [3324] total_loss: 2.10911202 d_loss: 1.37424600, g_loss: 0.68437690, ae_loss: 0.05048908\n",
      "Step: [3325] total_loss: 2.10758090 d_loss: 1.38366628, g_loss: 0.67505407, ae_loss: 0.04886050\n",
      "Step: [3326] total_loss: 2.09542751 d_loss: 1.35288537, g_loss: 0.69201791, ae_loss: 0.05052430\n",
      "Step: [3327] total_loss: 2.12939930 d_loss: 1.39047039, g_loss: 0.68819422, ae_loss: 0.05073458\n",
      "Step: [3328] total_loss: 2.12935734 d_loss: 1.37627220, g_loss: 0.69961107, ae_loss: 0.05347401\n",
      "Step: [3329] total_loss: 2.13724327 d_loss: 1.38121736, g_loss: 0.70552427, ae_loss: 0.05050173\n",
      "Step: [3330] total_loss: 2.10428905 d_loss: 1.36642039, g_loss: 0.68826413, ae_loss: 0.04960456\n",
      "Step: [3331] total_loss: 2.14373684 d_loss: 1.39787388, g_loss: 0.69437194, ae_loss: 0.05149096\n",
      "Step: [3332] total_loss: 2.11762810 d_loss: 1.37899685, g_loss: 0.68668580, ae_loss: 0.05194532\n",
      "Step: [3333] total_loss: 2.14230299 d_loss: 1.41209149, g_loss: 0.68094575, ae_loss: 0.04926559\n",
      "Step: [3334] total_loss: 2.15026808 d_loss: 1.39090657, g_loss: 0.70435941, ae_loss: 0.05500193\n",
      "Step: [3335] total_loss: 2.13131309 d_loss: 1.38658798, g_loss: 0.69439763, ae_loss: 0.05032754\n",
      "Step: [3336] total_loss: 2.14771748 d_loss: 1.40418315, g_loss: 0.69055080, ae_loss: 0.05298352\n",
      "Step: [3337] total_loss: 2.11213326 d_loss: 1.38069987, g_loss: 0.68108797, ae_loss: 0.05034545\n",
      "Step: [3338] total_loss: 2.14978361 d_loss: 1.36511469, g_loss: 0.73056769, ae_loss: 0.05410131\n",
      "Step: [3339] total_loss: 2.13008404 d_loss: 1.38222241, g_loss: 0.69743276, ae_loss: 0.05042877\n",
      "Step: [3340] total_loss: 2.12000561 d_loss: 1.36138082, g_loss: 0.70400167, ae_loss: 0.05462326\n",
      "Step: [3341] total_loss: 2.12472367 d_loss: 1.37704837, g_loss: 0.69475198, ae_loss: 0.05292331\n",
      "Step: [3342] total_loss: 2.15018368 d_loss: 1.38397384, g_loss: 0.71557772, ae_loss: 0.05063196\n",
      "Step: [3343] total_loss: 2.10338688 d_loss: 1.34683466, g_loss: 0.70692992, ae_loss: 0.04962220\n",
      "Step: [3344] total_loss: 2.11248684 d_loss: 1.36985397, g_loss: 0.68936527, ae_loss: 0.05326751\n",
      "Step: [3345] total_loss: 2.12751031 d_loss: 1.37788773, g_loss: 0.69926322, ae_loss: 0.05035936\n",
      "Step: [3346] total_loss: 2.10905266 d_loss: 1.38360238, g_loss: 0.67280567, ae_loss: 0.05264477\n",
      "Step: [3347] total_loss: 2.12081361 d_loss: 1.36052847, g_loss: 0.70320010, ae_loss: 0.05708505\n",
      "Step: [3348] total_loss: 2.14459252 d_loss: 1.36545360, g_loss: 0.72576559, ae_loss: 0.05337331\n",
      "Step: [3349] total_loss: 2.12152600 d_loss: 1.36724758, g_loss: 0.69818741, ae_loss: 0.05609090\n",
      "Step: [3350] total_loss: 2.13863778 d_loss: 1.38691771, g_loss: 0.69750798, ae_loss: 0.05421214\n",
      "Step: [3351] total_loss: 2.13668537 d_loss: 1.38116908, g_loss: 0.70086586, ae_loss: 0.05465050\n",
      "Step: [3352] total_loss: 2.11734128 d_loss: 1.38444257, g_loss: 0.68554151, ae_loss: 0.04735718\n",
      "Step: [3353] total_loss: 2.13145256 d_loss: 1.38259935, g_loss: 0.69430798, ae_loss: 0.05454528\n",
      "Step: [3354] total_loss: 2.12875509 d_loss: 1.39456010, g_loss: 0.68644387, ae_loss: 0.04775110\n",
      "Step: [3355] total_loss: 2.13641715 d_loss: 1.39509964, g_loss: 0.69024563, ae_loss: 0.05107193\n",
      "Step: [3356] total_loss: 2.10971618 d_loss: 1.37078178, g_loss: 0.68699574, ae_loss: 0.05193864\n",
      "Step: [3357] total_loss: 2.12719297 d_loss: 1.37548316, g_loss: 0.70179033, ae_loss: 0.04991962\n",
      "Step: [3358] total_loss: 2.12223291 d_loss: 1.34658813, g_loss: 0.72199440, ae_loss: 0.05365049\n",
      "Step: [3359] total_loss: 2.09499598 d_loss: 1.36301339, g_loss: 0.68329883, ae_loss: 0.04868392\n",
      "Step: [3360] total_loss: 2.12287188 d_loss: 1.39462852, g_loss: 0.67906225, ae_loss: 0.04918115\n",
      "Step: [3361] total_loss: 2.12322044 d_loss: 1.38049126, g_loss: 0.69204986, ae_loss: 0.05067919\n",
      "Step: [3362] total_loss: 2.10978079 d_loss: 1.37405086, g_loss: 0.68858618, ae_loss: 0.04714372\n",
      "Step: [3363] total_loss: 2.13941240 d_loss: 1.37252152, g_loss: 0.70576835, ae_loss: 0.06112259\n",
      "Step: [3364] total_loss: 2.13970327 d_loss: 1.41634774, g_loss: 0.66789472, ae_loss: 0.05546079\n",
      "Step: [3365] total_loss: 2.15145373 d_loss: 1.39335871, g_loss: 0.70633781, ae_loss: 0.05175717\n",
      "Step: [3366] total_loss: 2.11921215 d_loss: 1.36086333, g_loss: 0.70392036, ae_loss: 0.05442831\n",
      "Step: [3367] total_loss: 2.11655593 d_loss: 1.36484230, g_loss: 0.69892508, ae_loss: 0.05278855\n",
      "Step: [3368] total_loss: 2.14429951 d_loss: 1.39980018, g_loss: 0.69233048, ae_loss: 0.05216894\n",
      "Step: [3369] total_loss: 2.10844851 d_loss: 1.37792015, g_loss: 0.68131363, ae_loss: 0.04921468\n",
      "Step: [3370] total_loss: 2.15247130 d_loss: 1.39541245, g_loss: 0.70435023, ae_loss: 0.05270867\n",
      "Step: [3371] total_loss: 2.14752936 d_loss: 1.39905286, g_loss: 0.69854224, ae_loss: 0.04993421\n",
      "Step: [3372] total_loss: 2.12836742 d_loss: 1.37720394, g_loss: 0.69997674, ae_loss: 0.05118665\n",
      "Step: [3373] total_loss: 2.11038709 d_loss: 1.36656427, g_loss: 0.69613248, ae_loss: 0.04769034\n",
      "Step: [3374] total_loss: 2.18506694 d_loss: 1.44335961, g_loss: 0.68155247, ae_loss: 0.06015489\n",
      "Step: [3375] total_loss: 2.13530684 d_loss: 1.37731981, g_loss: 0.70496958, ae_loss: 0.05301741\n",
      "Step: [3376] total_loss: 2.11235785 d_loss: 1.37619519, g_loss: 0.68450731, ae_loss: 0.05165531\n",
      "Step: [3377] total_loss: 2.11601782 d_loss: 1.38004351, g_loss: 0.68249667, ae_loss: 0.05347761\n",
      "Step: [3378] total_loss: 2.10969830 d_loss: 1.37994313, g_loss: 0.67789692, ae_loss: 0.05185834\n",
      "Step: [3379] total_loss: 2.13137674 d_loss: 1.40029669, g_loss: 0.67961466, ae_loss: 0.05146533\n",
      "Step: [3380] total_loss: 2.11455536 d_loss: 1.37975192, g_loss: 0.68323529, ae_loss: 0.05156830\n",
      "Step: [3381] total_loss: 2.11362767 d_loss: 1.37757635, g_loss: 0.68217349, ae_loss: 0.05387783\n",
      "Step: [3382] total_loss: 2.12431717 d_loss: 1.38790369, g_loss: 0.68484747, ae_loss: 0.05156599\n",
      "Step: [3383] total_loss: 2.12227297 d_loss: 1.38789415, g_loss: 0.68442696, ae_loss: 0.04995176\n",
      "Step: [3384] total_loss: 2.12612414 d_loss: 1.37120843, g_loss: 0.70221728, ae_loss: 0.05269853\n",
      "Step: [3385] total_loss: 2.13442397 d_loss: 1.35766590, g_loss: 0.71985167, ae_loss: 0.05690640\n",
      "Step: [3386] total_loss: 2.12687254 d_loss: 1.37295890, g_loss: 0.70423329, ae_loss: 0.04968037\n",
      "Step: [3387] total_loss: 2.13889527 d_loss: 1.40025306, g_loss: 0.68804854, ae_loss: 0.05059373\n",
      "Step: [3388] total_loss: 2.15018463 d_loss: 1.37747920, g_loss: 0.72160643, ae_loss: 0.05109902\n",
      "Step: [3389] total_loss: 2.13409710 d_loss: 1.37646008, g_loss: 0.70605481, ae_loss: 0.05158216\n",
      "Step: [3390] total_loss: 2.12064934 d_loss: 1.37782645, g_loss: 0.69070601, ae_loss: 0.05211693\n",
      "Step: [3391] total_loss: 2.14015007 d_loss: 1.39501929, g_loss: 0.69414806, ae_loss: 0.05098271\n",
      "Step: [3392] total_loss: 2.11091900 d_loss: 1.38098168, g_loss: 0.67509329, ae_loss: 0.05484391\n",
      "Step: [3393] total_loss: 2.12972927 d_loss: 1.36654890, g_loss: 0.70920527, ae_loss: 0.05397517\n",
      "Step: [3394] total_loss: 2.13091135 d_loss: 1.38742113, g_loss: 0.69203687, ae_loss: 0.05145337\n",
      "Step: [3395] total_loss: 2.09952354 d_loss: 1.37116838, g_loss: 0.67421275, ae_loss: 0.05414240\n",
      "Step: [3396] total_loss: 2.13409615 d_loss: 1.40483880, g_loss: 0.67768025, ae_loss: 0.05157715\n",
      "Step: [3397] total_loss: 2.13425589 d_loss: 1.39524913, g_loss: 0.68752825, ae_loss: 0.05147846\n",
      "Step: [3398] total_loss: 2.11414361 d_loss: 1.37093663, g_loss: 0.69326746, ae_loss: 0.04993952\n",
      "Step: [3399] total_loss: 2.11247969 d_loss: 1.37576342, g_loss: 0.68585986, ae_loss: 0.05085630\n",
      "Step: [3400] total_loss: 2.10472178 d_loss: 1.36295414, g_loss: 0.68851179, ae_loss: 0.05325591\n",
      "Step: [3401] total_loss: 2.12068415 d_loss: 1.38041210, g_loss: 0.68732798, ae_loss: 0.05294412\n",
      "Step: [3402] total_loss: 2.13197565 d_loss: 1.40355062, g_loss: 0.67510962, ae_loss: 0.05331553\n",
      "Step: [3403] total_loss: 2.12369275 d_loss: 1.37841129, g_loss: 0.69131792, ae_loss: 0.05396350\n",
      "Step: [3404] total_loss: 2.13225031 d_loss: 1.39026666, g_loss: 0.69357240, ae_loss: 0.04841131\n",
      "Step: [3405] total_loss: 2.13107157 d_loss: 1.38827181, g_loss: 0.69358826, ae_loss: 0.04921147\n",
      "Step: [3406] total_loss: 2.13840961 d_loss: 1.37930989, g_loss: 0.70663530, ae_loss: 0.05246440\n",
      "Step: [3407] total_loss: 2.13436079 d_loss: 1.38210988, g_loss: 0.69441092, ae_loss: 0.05784002\n",
      "Step: [3408] total_loss: 2.13862634 d_loss: 1.37581229, g_loss: 0.70739698, ae_loss: 0.05541711\n",
      "Step: [3409] total_loss: 2.13992023 d_loss: 1.37889194, g_loss: 0.70934641, ae_loss: 0.05168175\n",
      "Step: [3410] total_loss: 2.12880683 d_loss: 1.36466789, g_loss: 0.71316713, ae_loss: 0.05097170\n",
      "Step: [3411] total_loss: 2.11993909 d_loss: 1.37281322, g_loss: 0.69889247, ae_loss: 0.04823341\n",
      "Step: [3412] total_loss: 2.13243055 d_loss: 1.39002025, g_loss: 0.69119942, ae_loss: 0.05121101\n",
      "Step: [3413] total_loss: 2.13503909 d_loss: 1.38441777, g_loss: 0.69875771, ae_loss: 0.05186364\n",
      "Step: [3414] total_loss: 2.13930440 d_loss: 1.39312339, g_loss: 0.69674546, ae_loss: 0.04943554\n",
      "Step: [3415] total_loss: 2.10824108 d_loss: 1.36180186, g_loss: 0.69577885, ae_loss: 0.05066034\n",
      "Step: [3416] total_loss: 2.12598038 d_loss: 1.39582837, g_loss: 0.67940438, ae_loss: 0.05074762\n",
      "Step: [3417] total_loss: 2.11445522 d_loss: 1.36877978, g_loss: 0.69497114, ae_loss: 0.05070442\n",
      "Step: [3418] total_loss: 2.11995745 d_loss: 1.38154864, g_loss: 0.68526614, ae_loss: 0.05314250\n",
      "Step: [3419] total_loss: 2.12320685 d_loss: 1.38618135, g_loss: 0.68483508, ae_loss: 0.05219046\n",
      "Step: [3420] total_loss: 2.12870455 d_loss: 1.37435508, g_loss: 0.70118129, ae_loss: 0.05316833\n",
      "Step: [3421] total_loss: 2.12165070 d_loss: 1.39133894, g_loss: 0.67826515, ae_loss: 0.05204663\n",
      "Step: [3422] total_loss: 2.13988590 d_loss: 1.38641024, g_loss: 0.70266330, ae_loss: 0.05081220\n",
      "Step: [3423] total_loss: 2.11694384 d_loss: 1.37698734, g_loss: 0.69287407, ae_loss: 0.04708247\n",
      "Step: [3424] total_loss: 2.13812804 d_loss: 1.38644910, g_loss: 0.70485407, ae_loss: 0.04682499\n",
      "Step: [3425] total_loss: 2.10673714 d_loss: 1.36516571, g_loss: 0.69195235, ae_loss: 0.04961919\n",
      "Step: [3426] total_loss: 2.12452364 d_loss: 1.39400482, g_loss: 0.68272173, ae_loss: 0.04779699\n",
      "Step: [3427] total_loss: 2.11495471 d_loss: 1.36930132, g_loss: 0.69207615, ae_loss: 0.05357721\n",
      "Step: [3428] total_loss: 2.11230707 d_loss: 1.36632919, g_loss: 0.69946975, ae_loss: 0.04650802\n",
      "Step: [3429] total_loss: 2.11723113 d_loss: 1.37782037, g_loss: 0.68989670, ae_loss: 0.04951409\n",
      "Step: [3430] total_loss: 2.11169529 d_loss: 1.36476159, g_loss: 0.69444311, ae_loss: 0.05249063\n",
      "Step: [3431] total_loss: 2.13422251 d_loss: 1.40000212, g_loss: 0.68521279, ae_loss: 0.04900771\n",
      "Step: [3432] total_loss: 2.13143325 d_loss: 1.37941790, g_loss: 0.69849378, ae_loss: 0.05352163\n",
      "Step: [3433] total_loss: 2.12976837 d_loss: 1.39924550, g_loss: 0.67944884, ae_loss: 0.05107403\n",
      "Step: [3434] total_loss: 2.14857149 d_loss: 1.37513161, g_loss: 0.72199523, ae_loss: 0.05144478\n",
      "Step: [3435] total_loss: 2.11315632 d_loss: 1.36365294, g_loss: 0.70107228, ae_loss: 0.04843099\n",
      "Step: [3436] total_loss: 2.14625096 d_loss: 1.38496494, g_loss: 0.70752102, ae_loss: 0.05376503\n",
      "Step: [3437] total_loss: 2.12921000 d_loss: 1.37395906, g_loss: 0.70329320, ae_loss: 0.05195769\n",
      "Step: [3438] total_loss: 2.14668512 d_loss: 1.40003633, g_loss: 0.69969022, ae_loss: 0.04695854\n",
      "Step: [3439] total_loss: 2.12132835 d_loss: 1.38670397, g_loss: 0.68585217, ae_loss: 0.04877214\n",
      "Step: [3440] total_loss: 2.10861516 d_loss: 1.35805929, g_loss: 0.70263171, ae_loss: 0.04792419\n",
      "Step: [3441] total_loss: 2.11019135 d_loss: 1.35263371, g_loss: 0.70763195, ae_loss: 0.04992584\n",
      "Step: [3442] total_loss: 2.11105657 d_loss: 1.37727392, g_loss: 0.68451333, ae_loss: 0.04926935\n",
      "Step: [3443] total_loss: 2.13668156 d_loss: 1.38416982, g_loss: 0.70500362, ae_loss: 0.04750819\n",
      "Step: [3444] total_loss: 2.11824369 d_loss: 1.36093831, g_loss: 0.70487773, ae_loss: 0.05242761\n",
      "Step: [3445] total_loss: 2.13344240 d_loss: 1.37522650, g_loss: 0.70497310, ae_loss: 0.05324271\n",
      "Step: [3446] total_loss: 2.15003490 d_loss: 1.42177892, g_loss: 0.67392349, ae_loss: 0.05433243\n",
      "Step: [3447] total_loss: 2.13476753 d_loss: 1.39927280, g_loss: 0.68762362, ae_loss: 0.04787096\n",
      "Step: [3448] total_loss: 2.12874842 d_loss: 1.39043796, g_loss: 0.68512148, ae_loss: 0.05318896\n",
      "Step: [3449] total_loss: 2.12591600 d_loss: 1.37625432, g_loss: 0.69507587, ae_loss: 0.05458590\n",
      "Step: [3450] total_loss: 2.09251094 d_loss: 1.33716965, g_loss: 0.70468754, ae_loss: 0.05065379\n",
      "Step: [3451] total_loss: 2.13530397 d_loss: 1.38308990, g_loss: 0.70062220, ae_loss: 0.05159185\n",
      "Step: [3452] total_loss: 2.13719869 d_loss: 1.36824155, g_loss: 0.71753979, ae_loss: 0.05141739\n",
      "Step: [3453] total_loss: 2.11570978 d_loss: 1.37610745, g_loss: 0.68428224, ae_loss: 0.05532017\n",
      "Step: [3454] total_loss: 2.14795804 d_loss: 1.40388596, g_loss: 0.69258577, ae_loss: 0.05148635\n",
      "Step: [3455] total_loss: 2.10766101 d_loss: 1.35559201, g_loss: 0.70437109, ae_loss: 0.04769789\n",
      "Step: [3456] total_loss: 2.11616850 d_loss: 1.37011933, g_loss: 0.69366848, ae_loss: 0.05238080\n",
      "Step: [3457] total_loss: 2.14550734 d_loss: 1.39908242, g_loss: 0.69792855, ae_loss: 0.04849652\n",
      "Step: [3458] total_loss: 2.11717510 d_loss: 1.38602996, g_loss: 0.68169904, ae_loss: 0.04944594\n",
      "Step: [3459] total_loss: 2.12054801 d_loss: 1.38829923, g_loss: 0.68290555, ae_loss: 0.04934324\n",
      "Step: [3460] total_loss: 2.12317204 d_loss: 1.36861181, g_loss: 0.70113057, ae_loss: 0.05342961\n",
      "Step: [3461] total_loss: 2.10953140 d_loss: 1.38818777, g_loss: 0.67219901, ae_loss: 0.04914480\n",
      "Step: [3462] total_loss: 2.09742689 d_loss: 1.36544311, g_loss: 0.68094075, ae_loss: 0.05104321\n",
      "Step: [3463] total_loss: 2.11852622 d_loss: 1.37740993, g_loss: 0.68766814, ae_loss: 0.05344803\n",
      "Step: [3464] total_loss: 2.11306977 d_loss: 1.37049639, g_loss: 0.69174516, ae_loss: 0.05082825\n",
      "Step: [3465] total_loss: 2.11006999 d_loss: 1.36543155, g_loss: 0.69367546, ae_loss: 0.05096310\n",
      "Step: [3466] total_loss: 2.14354658 d_loss: 1.39927101, g_loss: 0.69194579, ae_loss: 0.05232968\n",
      "Step: [3467] total_loss: 2.13294506 d_loss: 1.40119314, g_loss: 0.67949969, ae_loss: 0.05225228\n",
      "Step: [3468] total_loss: 2.14284229 d_loss: 1.39174521, g_loss: 0.70060384, ae_loss: 0.05049339\n",
      "Step: [3469] total_loss: 2.09526277 d_loss: 1.35015929, g_loss: 0.69742912, ae_loss: 0.04767428\n",
      "Step: [3470] total_loss: 2.13511324 d_loss: 1.38558459, g_loss: 0.70056880, ae_loss: 0.04895976\n",
      "Step: [3471] total_loss: 2.12383628 d_loss: 1.38220084, g_loss: 0.69002646, ae_loss: 0.05160907\n",
      "Step: [3472] total_loss: 2.13562632 d_loss: 1.39354241, g_loss: 0.69094151, ae_loss: 0.05114244\n",
      "Step: [3473] total_loss: 2.12221098 d_loss: 1.36572146, g_loss: 0.70516688, ae_loss: 0.05132267\n",
      "Step: [3474] total_loss: 2.13076639 d_loss: 1.38220525, g_loss: 0.69899797, ae_loss: 0.04956307\n",
      "Step: [3475] total_loss: 2.13882899 d_loss: 1.40594053, g_loss: 0.68090284, ae_loss: 0.05198558\n",
      "Step: [3476] total_loss: 2.11468840 d_loss: 1.36954260, g_loss: 0.69506580, ae_loss: 0.05008005\n",
      "Step: [3477] total_loss: 2.14237452 d_loss: 1.39203501, g_loss: 0.69804823, ae_loss: 0.05229116\n",
      "Step: [3478] total_loss: 2.10296011 d_loss: 1.36629593, g_loss: 0.68747950, ae_loss: 0.04918472\n",
      "Step: [3479] total_loss: 2.09977794 d_loss: 1.35667729, g_loss: 0.69056892, ae_loss: 0.05253176\n",
      "Step: [3480] total_loss: 2.13052034 d_loss: 1.39795327, g_loss: 0.68239546, ae_loss: 0.05017167\n",
      "Step: [3481] total_loss: 2.14252090 d_loss: 1.38725948, g_loss: 0.70242363, ae_loss: 0.05283787\n",
      "Step: [3482] total_loss: 2.12349629 d_loss: 1.40067577, g_loss: 0.66970927, ae_loss: 0.05311119\n",
      "Step: [3483] total_loss: 2.12990236 d_loss: 1.37696505, g_loss: 0.70229733, ae_loss: 0.05063994\n",
      "Step: [3484] total_loss: 2.12839913 d_loss: 1.38290501, g_loss: 0.69525403, ae_loss: 0.05024008\n",
      "Step: [3485] total_loss: 2.14971256 d_loss: 1.39343417, g_loss: 0.70417166, ae_loss: 0.05210669\n",
      "Step: [3486] total_loss: 2.14948869 d_loss: 1.40368867, g_loss: 0.69294864, ae_loss: 0.05285144\n",
      "Step: [3487] total_loss: 2.15988016 d_loss: 1.40393841, g_loss: 0.70236158, ae_loss: 0.05358012\n",
      "Step: [3488] total_loss: 2.13656569 d_loss: 1.38797355, g_loss: 0.69588327, ae_loss: 0.05270893\n",
      "Step: [3489] total_loss: 2.11609244 d_loss: 1.36902761, g_loss: 0.69581383, ae_loss: 0.05125101\n",
      "Step: [3490] total_loss: 2.12947154 d_loss: 1.39105451, g_loss: 0.68480879, ae_loss: 0.05360831\n",
      "Step: [3491] total_loss: 2.12638116 d_loss: 1.36764395, g_loss: 0.70906800, ae_loss: 0.04966924\n",
      "Step: [3492] total_loss: 2.13390851 d_loss: 1.39076269, g_loss: 0.69219631, ae_loss: 0.05094954\n",
      "Step: [3493] total_loss: 2.13371873 d_loss: 1.39869690, g_loss: 0.68186384, ae_loss: 0.05315807\n",
      "Step: [3494] total_loss: 2.11342931 d_loss: 1.37080216, g_loss: 0.68962264, ae_loss: 0.05300450\n",
      "Step: [3495] total_loss: 2.11891127 d_loss: 1.37428451, g_loss: 0.69408262, ae_loss: 0.05054404\n",
      "Step: [3496] total_loss: 2.10859489 d_loss: 1.35933208, g_loss: 0.70057523, ae_loss: 0.04868755\n",
      "Step: [3497] total_loss: 2.14269280 d_loss: 1.39696264, g_loss: 0.69493383, ae_loss: 0.05079628\n",
      "Step: [3498] total_loss: 2.11261010 d_loss: 1.37598252, g_loss: 0.68745726, ae_loss: 0.04917035\n",
      "Step: [3499] total_loss: 2.14303589 d_loss: 1.40698218, g_loss: 0.68526638, ae_loss: 0.05078725\n",
      "Step: [3500] total_loss: 2.11610031 d_loss: 1.37222791, g_loss: 0.68857193, ae_loss: 0.05530050\n",
      "Step: [3501] total_loss: 2.12439299 d_loss: 1.38918924, g_loss: 0.68337899, ae_loss: 0.05182478\n",
      "Step: [3502] total_loss: 2.15209913 d_loss: 1.39439940, g_loss: 0.70638311, ae_loss: 0.05131670\n",
      "Step: [3503] total_loss: 2.13534522 d_loss: 1.39890623, g_loss: 0.68701297, ae_loss: 0.04942591\n",
      "Step: [3504] total_loss: 2.12443614 d_loss: 1.36831832, g_loss: 0.71001852, ae_loss: 0.04609925\n",
      "Step: [3505] total_loss: 2.13709760 d_loss: 1.37653470, g_loss: 0.70945585, ae_loss: 0.05110703\n",
      "Step: [3506] total_loss: 2.11456466 d_loss: 1.37920403, g_loss: 0.68710655, ae_loss: 0.04825400\n",
      "Step: [3507] total_loss: 2.15100360 d_loss: 1.38353729, g_loss: 0.71349961, ae_loss: 0.05396672\n",
      "Step: [3508] total_loss: 2.13210726 d_loss: 1.39592433, g_loss: 0.68680388, ae_loss: 0.04937910\n",
      "Step: [3509] total_loss: 2.12735844 d_loss: 1.39092886, g_loss: 0.68501627, ae_loss: 0.05141332\n",
      "Step: [3510] total_loss: 2.13705826 d_loss: 1.38756371, g_loss: 0.70066798, ae_loss: 0.04882646\n",
      "Step: [3511] total_loss: 2.10390329 d_loss: 1.34985638, g_loss: 0.70394170, ae_loss: 0.05010508\n",
      "Step: [3512] total_loss: 2.12120652 d_loss: 1.39302731, g_loss: 0.67447793, ae_loss: 0.05370131\n",
      "Step: [3513] total_loss: 2.11009693 d_loss: 1.38357532, g_loss: 0.67639780, ae_loss: 0.05012374\n",
      "Step: [3514] total_loss: 2.14387631 d_loss: 1.37592423, g_loss: 0.71549213, ae_loss: 0.05245997\n",
      "Step: [3515] total_loss: 2.11839533 d_loss: 1.37928534, g_loss: 0.68737006, ae_loss: 0.05174007\n",
      "Step: [3516] total_loss: 2.12490630 d_loss: 1.38620746, g_loss: 0.68855762, ae_loss: 0.05014125\n",
      "Step: [3517] total_loss: 2.13941908 d_loss: 1.40394402, g_loss: 0.68737417, ae_loss: 0.04810092\n",
      "Step: [3518] total_loss: 2.09712434 d_loss: 1.35071719, g_loss: 0.69418550, ae_loss: 0.05222168\n",
      "Step: [3519] total_loss: 2.12200999 d_loss: 1.36837566, g_loss: 0.70374799, ae_loss: 0.04988640\n",
      "Step: [3520] total_loss: 2.12021685 d_loss: 1.39417684, g_loss: 0.67753911, ae_loss: 0.04850090\n",
      "Step: [3521] total_loss: 2.11294603 d_loss: 1.36902475, g_loss: 0.69063783, ae_loss: 0.05328349\n",
      "Step: [3522] total_loss: 2.13253593 d_loss: 1.37623167, g_loss: 0.69959915, ae_loss: 0.05670497\n",
      "Step: [3523] total_loss: 2.11948156 d_loss: 1.38212192, g_loss: 0.68520188, ae_loss: 0.05215789\n",
      "Step: [3524] total_loss: 2.12980509 d_loss: 1.38610387, g_loss: 0.68819571, ae_loss: 0.05550558\n",
      "Step: [3525] total_loss: 2.12577367 d_loss: 1.38668990, g_loss: 0.68906492, ae_loss: 0.05001893\n",
      "Step: [3526] total_loss: 2.15138626 d_loss: 1.40574026, g_loss: 0.69363374, ae_loss: 0.05201216\n",
      "Step: [3527] total_loss: 2.12464333 d_loss: 1.39209425, g_loss: 0.68346119, ae_loss: 0.04908795\n",
      "Step: [3528] total_loss: 2.13804913 d_loss: 1.38752270, g_loss: 0.69432062, ae_loss: 0.05620582\n",
      "Step: [3529] total_loss: 2.13049746 d_loss: 1.38031828, g_loss: 0.69756484, ae_loss: 0.05261432\n",
      "Step: [3530] total_loss: 2.11687517 d_loss: 1.36446095, g_loss: 0.69768429, ae_loss: 0.05472995\n",
      "Step: [3531] total_loss: 2.10232592 d_loss: 1.35485351, g_loss: 0.69286716, ae_loss: 0.05460533\n",
      "Step: [3532] total_loss: 2.11865854 d_loss: 1.39983368, g_loss: 0.66413260, ae_loss: 0.05469225\n",
      "Step: [3533] total_loss: 2.13936877 d_loss: 1.39630044, g_loss: 0.68890619, ae_loss: 0.05416212\n",
      "Step: [3534] total_loss: 2.10555077 d_loss: 1.36627114, g_loss: 0.68961477, ae_loss: 0.04966483\n",
      "Step: [3535] total_loss: 2.14122152 d_loss: 1.39720774, g_loss: 0.69610351, ae_loss: 0.04791017\n",
      "Step: [3536] total_loss: 2.14083624 d_loss: 1.40068829, g_loss: 0.68868685, ae_loss: 0.05146093\n",
      "Step: [3537] total_loss: 2.11425853 d_loss: 1.36699462, g_loss: 0.69480705, ae_loss: 0.05245681\n",
      "Step: [3538] total_loss: 2.09407926 d_loss: 1.37387753, g_loss: 0.67070860, ae_loss: 0.04949308\n",
      "Step: [3539] total_loss: 2.10043597 d_loss: 1.36635017, g_loss: 0.68101543, ae_loss: 0.05307046\n",
      "Step: [3540] total_loss: 2.10760784 d_loss: 1.36656225, g_loss: 0.69292831, ae_loss: 0.04811733\n",
      "Step: [3541] total_loss: 2.10360146 d_loss: 1.36343443, g_loss: 0.69186258, ae_loss: 0.04830427\n",
      "Step: [3542] total_loss: 2.11815500 d_loss: 1.39811158, g_loss: 0.67119890, ae_loss: 0.04884451\n",
      "Step: [3543] total_loss: 2.13840866 d_loss: 1.37090492, g_loss: 0.71595573, ae_loss: 0.05154785\n",
      "Step: [3544] total_loss: 2.14385080 d_loss: 1.39163363, g_loss: 0.70256209, ae_loss: 0.04965517\n",
      "Step: [3545] total_loss: 2.12531233 d_loss: 1.36052942, g_loss: 0.70979679, ae_loss: 0.05498599\n",
      "Step: [3546] total_loss: 2.14818239 d_loss: 1.41447043, g_loss: 0.68383801, ae_loss: 0.04987405\n",
      "Step: [3547] total_loss: 2.12792873 d_loss: 1.37092876, g_loss: 0.70445967, ae_loss: 0.05254016\n",
      "Step: [3548] total_loss: 2.13116264 d_loss: 1.37221086, g_loss: 0.70649397, ae_loss: 0.05245781\n",
      "Step: [3549] total_loss: 2.13759613 d_loss: 1.37615263, g_loss: 0.70867586, ae_loss: 0.05276749\n",
      "Step: [3550] total_loss: 2.12147427 d_loss: 1.36998248, g_loss: 0.69683051, ae_loss: 0.05466131\n",
      "Step: [3551] total_loss: 2.13757610 d_loss: 1.40031958, g_loss: 0.68099701, ae_loss: 0.05625957\n",
      "Step: [3552] total_loss: 2.13294125 d_loss: 1.40073550, g_loss: 0.68082368, ae_loss: 0.05138221\n",
      "Step: [3553] total_loss: 2.11716652 d_loss: 1.38190317, g_loss: 0.68339449, ae_loss: 0.05186889\n",
      "Step: [3554] total_loss: 2.15139914 d_loss: 1.40850759, g_loss: 0.68991244, ae_loss: 0.05297915\n",
      "Step: [3555] total_loss: 2.13744688 d_loss: 1.41363347, g_loss: 0.67501855, ae_loss: 0.04879490\n",
      "Step: [3556] total_loss: 2.13388348 d_loss: 1.38037252, g_loss: 0.70132351, ae_loss: 0.05218735\n",
      "Step: [3557] total_loss: 2.12768149 d_loss: 1.37444675, g_loss: 0.70281446, ae_loss: 0.05042030\n",
      "Step: [3558] total_loss: 2.11452246 d_loss: 1.37698460, g_loss: 0.68796146, ae_loss: 0.04957638\n",
      "Step: [3559] total_loss: 2.11432600 d_loss: 1.37468708, g_loss: 0.68912435, ae_loss: 0.05051447\n",
      "Step: [3560] total_loss: 2.13718271 d_loss: 1.39016032, g_loss: 0.69732630, ae_loss: 0.04969592\n",
      "Step: [3561] total_loss: 2.11662769 d_loss: 1.38209963, g_loss: 0.68446505, ae_loss: 0.05006306\n",
      "Step: [3562] total_loss: 2.10952950 d_loss: 1.36708260, g_loss: 0.68909454, ae_loss: 0.05335243\n",
      "Step: [3563] total_loss: 2.13888001 d_loss: 1.38698399, g_loss: 0.70171863, ae_loss: 0.05017731\n",
      "Step: [3564] total_loss: 2.12503672 d_loss: 1.38593364, g_loss: 0.68855160, ae_loss: 0.05055143\n",
      "Step: [3565] total_loss: 2.11660290 d_loss: 1.38216472, g_loss: 0.68463588, ae_loss: 0.04980218\n",
      "Step: [3566] total_loss: 2.11814022 d_loss: 1.38633585, g_loss: 0.67831844, ae_loss: 0.05348591\n",
      "Step: [3567] total_loss: 2.11061001 d_loss: 1.36748719, g_loss: 0.69293809, ae_loss: 0.05018482\n",
      "Step: [3568] total_loss: 2.11890411 d_loss: 1.36895001, g_loss: 0.69965649, ae_loss: 0.05029751\n",
      "Step: [3569] total_loss: 2.12052536 d_loss: 1.37286425, g_loss: 0.69523811, ae_loss: 0.05242316\n",
      "Step: [3570] total_loss: 2.14682484 d_loss: 1.40249503, g_loss: 0.69577700, ae_loss: 0.04855293\n",
      "Step: [3571] total_loss: 2.13920546 d_loss: 1.41443121, g_loss: 0.67395610, ae_loss: 0.05081814\n",
      "Step: [3572] total_loss: 2.13547564 d_loss: 1.37292325, g_loss: 0.70743585, ae_loss: 0.05511661\n",
      "Step: [3573] total_loss: 2.14029264 d_loss: 1.39397454, g_loss: 0.69040567, ae_loss: 0.05591244\n",
      "Step: [3574] total_loss: 2.14693785 d_loss: 1.38593006, g_loss: 0.70917714, ae_loss: 0.05183072\n",
      "Step: [3575] total_loss: 2.13619256 d_loss: 1.40377760, g_loss: 0.68371910, ae_loss: 0.04869585\n",
      "Step: [3576] total_loss: 2.13250589 d_loss: 1.36963546, g_loss: 0.71053171, ae_loss: 0.05233889\n",
      "Step: [3577] total_loss: 2.12498999 d_loss: 1.37742245, g_loss: 0.69400418, ae_loss: 0.05356321\n",
      "Step: [3578] total_loss: 2.11087132 d_loss: 1.37787139, g_loss: 0.68488193, ae_loss: 0.04811789\n",
      "Step: [3579] total_loss: 2.10964727 d_loss: 1.38468623, g_loss: 0.67508638, ae_loss: 0.04987462\n",
      "Step: [3580] total_loss: 2.12263823 d_loss: 1.39613879, g_loss: 0.67658412, ae_loss: 0.04991528\n",
      "Step: [3581] total_loss: 2.10911942 d_loss: 1.37425649, g_loss: 0.68315434, ae_loss: 0.05170850\n",
      "Step: [3582] total_loss: 2.12319207 d_loss: 1.37761819, g_loss: 0.69647956, ae_loss: 0.04909435\n",
      "Step: [3583] total_loss: 2.10220051 d_loss: 1.34777522, g_loss: 0.70115614, ae_loss: 0.05326902\n",
      "Step: [3584] total_loss: 2.11360812 d_loss: 1.36378372, g_loss: 0.69751436, ae_loss: 0.05231003\n",
      "Step: [3585] total_loss: 2.13198185 d_loss: 1.38265932, g_loss: 0.69734496, ae_loss: 0.05197768\n",
      "Step: [3586] total_loss: 2.11669779 d_loss: 1.39181244, g_loss: 0.67648947, ae_loss: 0.04839582\n",
      "Step: [3587] total_loss: 2.11839437 d_loss: 1.38506544, g_loss: 0.68578625, ae_loss: 0.04754253\n",
      "Step: [3588] total_loss: 2.12856150 d_loss: 1.37204826, g_loss: 0.70356631, ae_loss: 0.05294699\n",
      "Step: [3589] total_loss: 2.14138484 d_loss: 1.39941990, g_loss: 0.68711209, ae_loss: 0.05485287\n",
      "Step: [3590] total_loss: 2.10243750 d_loss: 1.37663054, g_loss: 0.67665362, ae_loss: 0.04915350\n",
      "Step: [3591] total_loss: 2.11425972 d_loss: 1.37801242, g_loss: 0.68864131, ae_loss: 0.04760617\n",
      "Step: [3592] total_loss: 2.13852882 d_loss: 1.40077353, g_loss: 0.68377125, ae_loss: 0.05398408\n",
      "Step: [3593] total_loss: 2.12896585 d_loss: 1.36325908, g_loss: 0.71354353, ae_loss: 0.05216327\n",
      "Step: [3594] total_loss: 2.14004374 d_loss: 1.37588143, g_loss: 0.71080816, ae_loss: 0.05335406\n",
      "Step: [3595] total_loss: 2.11128283 d_loss: 1.37928820, g_loss: 0.68292850, ae_loss: 0.04906616\n",
      "Step: [3596] total_loss: 2.14923048 d_loss: 1.39288902, g_loss: 0.70660347, ae_loss: 0.04973797\n",
      "Step: [3597] total_loss: 2.11249352 d_loss: 1.37208033, g_loss: 0.68831694, ae_loss: 0.05209620\n",
      "Step: [3598] total_loss: 2.11716485 d_loss: 1.37906694, g_loss: 0.68897402, ae_loss: 0.04912390\n",
      "Step: [3599] total_loss: 2.13420677 d_loss: 1.37748194, g_loss: 0.70738059, ae_loss: 0.04934435\n",
      "Step: [3600] total_loss: 2.13978100 d_loss: 1.38091683, g_loss: 0.70518792, ae_loss: 0.05367624\n",
      "Step: [3601] total_loss: 2.13649726 d_loss: 1.38294888, g_loss: 0.70060384, ae_loss: 0.05294452\n",
      "Step: [3602] total_loss: 2.13965607 d_loss: 1.38021493, g_loss: 0.71021855, ae_loss: 0.04922258\n",
      "Step: [3603] total_loss: 2.13827157 d_loss: 1.37640023, g_loss: 0.70776236, ae_loss: 0.05410896\n",
      "Step: [3604] total_loss: 2.13826847 d_loss: 1.40009022, g_loss: 0.68294537, ae_loss: 0.05523279\n",
      "Step: [3605] total_loss: 2.12729311 d_loss: 1.36973810, g_loss: 0.70884919, ae_loss: 0.04870581\n",
      "Step: [3606] total_loss: 2.13072491 d_loss: 1.38279557, g_loss: 0.69838709, ae_loss: 0.04954220\n",
      "Step: [3607] total_loss: 2.12513781 d_loss: 1.37347853, g_loss: 0.69993818, ae_loss: 0.05172095\n",
      "Step: [3608] total_loss: 2.12802958 d_loss: 1.39326417, g_loss: 0.68517703, ae_loss: 0.04958838\n",
      "Step: [3609] total_loss: 2.13049388 d_loss: 1.37098539, g_loss: 0.71089619, ae_loss: 0.04861218\n",
      "Step: [3610] total_loss: 2.12514567 d_loss: 1.37805152, g_loss: 0.69410139, ae_loss: 0.05299277\n",
      "Step: [3611] total_loss: 2.09585762 d_loss: 1.34805644, g_loss: 0.69543493, ae_loss: 0.05236620\n",
      "Step: [3612] total_loss: 2.13151240 d_loss: 1.37766290, g_loss: 0.69920206, ae_loss: 0.05464746\n",
      "Step: [3613] total_loss: 2.11332178 d_loss: 1.37960553, g_loss: 0.68158197, ae_loss: 0.05213419\n",
      "Step: [3614] total_loss: 2.12749338 d_loss: 1.39623499, g_loss: 0.67779481, ae_loss: 0.05346340\n",
      "Step: [3615] total_loss: 2.12728739 d_loss: 1.38875425, g_loss: 0.69113743, ae_loss: 0.04739556\n",
      "Step: [3616] total_loss: 2.12019062 d_loss: 1.38050795, g_loss: 0.68930197, ae_loss: 0.05038081\n",
      "Step: [3617] total_loss: 2.13367844 d_loss: 1.38330722, g_loss: 0.69881606, ae_loss: 0.05155519\n",
      "Step: [3618] total_loss: 2.12425756 d_loss: 1.39783669, g_loss: 0.67435849, ae_loss: 0.05206251\n",
      "Step: [3619] total_loss: 2.16069174 d_loss: 1.39994192, g_loss: 0.70669186, ae_loss: 0.05405782\n",
      "Step: [3620] total_loss: 2.16517091 d_loss: 1.41402268, g_loss: 0.70214993, ae_loss: 0.04899834\n",
      "Step: [3621] total_loss: 2.14207697 d_loss: 1.40196717, g_loss: 0.68623769, ae_loss: 0.05387222\n",
      "Step: [3622] total_loss: 2.12971854 d_loss: 1.39411199, g_loss: 0.68007559, ae_loss: 0.05553089\n",
      "Step: [3623] total_loss: 2.11825991 d_loss: 1.38180041, g_loss: 0.68769258, ae_loss: 0.04876698\n",
      "Step: [3624] total_loss: 2.10667038 d_loss: 1.36702764, g_loss: 0.68472648, ae_loss: 0.05491635\n",
      "Step: [3625] total_loss: 2.11536455 d_loss: 1.37318563, g_loss: 0.69256699, ae_loss: 0.04961204\n",
      "Step: [3626] total_loss: 2.13900733 d_loss: 1.38841820, g_loss: 0.69996619, ae_loss: 0.05062289\n",
      "Step: [3627] total_loss: 2.12513781 d_loss: 1.39580345, g_loss: 0.67834187, ae_loss: 0.05099235\n",
      "Step: [3628] total_loss: 2.09686613 d_loss: 1.36102319, g_loss: 0.68493539, ae_loss: 0.05090767\n",
      "Step: [3629] total_loss: 2.12786269 d_loss: 1.37383592, g_loss: 0.70043439, ae_loss: 0.05359250\n",
      "Step: [3630] total_loss: 2.14896917 d_loss: 1.39450002, g_loss: 0.70661545, ae_loss: 0.04785366\n",
      "Step: [3631] total_loss: 2.14047861 d_loss: 1.38614595, g_loss: 0.70178139, ae_loss: 0.05255119\n",
      "Step: [3632] total_loss: 2.12767124 d_loss: 1.38254309, g_loss: 0.69525182, ae_loss: 0.04987635\n",
      "Step: [3633] total_loss: 2.12719250 d_loss: 1.37858677, g_loss: 0.69940114, ae_loss: 0.04920456\n",
      "Step: [3634] total_loss: 2.13895655 d_loss: 1.37613583, g_loss: 0.71190059, ae_loss: 0.05092023\n",
      "Step: [3635] total_loss: 2.11063099 d_loss: 1.37649536, g_loss: 0.68338346, ae_loss: 0.05075211\n",
      "Step: [3636] total_loss: 2.12124133 d_loss: 1.39434755, g_loss: 0.67837906, ae_loss: 0.04851468\n",
      "Step: [3637] total_loss: 2.09831882 d_loss: 1.36825299, g_loss: 0.68071347, ae_loss: 0.04935236\n",
      "Step: [3638] total_loss: 2.12370539 d_loss: 1.38139999, g_loss: 0.68905747, ae_loss: 0.05324796\n",
      "Step: [3639] total_loss: 2.11434078 d_loss: 1.38178170, g_loss: 0.67941332, ae_loss: 0.05314571\n",
      "Step: [3640] total_loss: 2.10704899 d_loss: 1.35322690, g_loss: 0.70230401, ae_loss: 0.05151804\n",
      "Step: [3641] total_loss: 2.10272479 d_loss: 1.37554479, g_loss: 0.67962331, ae_loss: 0.04755678\n",
      "Step: [3642] total_loss: 2.12881255 d_loss: 1.37815464, g_loss: 0.69963437, ae_loss: 0.05102346\n",
      "Step: [3643] total_loss: 2.11407471 d_loss: 1.38460803, g_loss: 0.68054616, ae_loss: 0.04892058\n",
      "Step: [3644] total_loss: 2.12229085 d_loss: 1.36948955, g_loss: 0.70219970, ae_loss: 0.05060155\n",
      "Step: [3645] total_loss: 2.14439821 d_loss: 1.36682177, g_loss: 0.72690797, ae_loss: 0.05066864\n",
      "Step: [3646] total_loss: 2.10904217 d_loss: 1.36981297, g_loss: 0.68915772, ae_loss: 0.05007156\n",
      "Step: [3647] total_loss: 2.15272164 d_loss: 1.40721107, g_loss: 0.69223464, ae_loss: 0.05327592\n",
      "Step: [3648] total_loss: 2.14881039 d_loss: 1.41699016, g_loss: 0.67826664, ae_loss: 0.05355363\n",
      "Step: [3649] total_loss: 2.11898279 d_loss: 1.37996018, g_loss: 0.68647623, ae_loss: 0.05254625\n",
      "Step: [3650] total_loss: 2.12022877 d_loss: 1.37700367, g_loss: 0.69287378, ae_loss: 0.05035143\n",
      "Step: [3651] total_loss: 2.12836790 d_loss: 1.36489308, g_loss: 0.70871460, ae_loss: 0.05476018\n",
      "Step: [3652] total_loss: 2.13417435 d_loss: 1.40019011, g_loss: 0.68351388, ae_loss: 0.05047018\n",
      "Step: [3653] total_loss: 2.12483835 d_loss: 1.38438404, g_loss: 0.69255471, ae_loss: 0.04789963\n",
      "Step: [3654] total_loss: 2.13373899 d_loss: 1.36759830, g_loss: 0.71687567, ae_loss: 0.04926492\n",
      "Step: [3655] total_loss: 2.11518240 d_loss: 1.38057137, g_loss: 0.68359268, ae_loss: 0.05101851\n",
      "Step: [3656] total_loss: 2.12207985 d_loss: 1.38811624, g_loss: 0.68133163, ae_loss: 0.05263182\n",
      "Step: [3657] total_loss: 2.13626480 d_loss: 1.37568212, g_loss: 0.70900238, ae_loss: 0.05158016\n",
      "Step: [3658] total_loss: 2.13221931 d_loss: 1.36557376, g_loss: 0.71763384, ae_loss: 0.04901180\n",
      "Step: [3659] total_loss: 2.13218403 d_loss: 1.40331721, g_loss: 0.67734754, ae_loss: 0.05151925\n",
      "Step: [3660] total_loss: 2.14569879 d_loss: 1.41006756, g_loss: 0.68149292, ae_loss: 0.05413830\n",
      "Step: [3661] total_loss: 2.11504221 d_loss: 1.36523843, g_loss: 0.69694346, ae_loss: 0.05286025\n",
      "Step: [3662] total_loss: 2.11796451 d_loss: 1.37309074, g_loss: 0.69286114, ae_loss: 0.05201266\n",
      "Step: [3663] total_loss: 2.14447308 d_loss: 1.38784146, g_loss: 0.70744658, ae_loss: 0.04918504\n",
      "Step: [3664] total_loss: 2.13093734 d_loss: 1.36790395, g_loss: 0.71210057, ae_loss: 0.05093282\n",
      "Step: [3665] total_loss: 2.14336658 d_loss: 1.39859951, g_loss: 0.69105208, ae_loss: 0.05371503\n",
      "Step: [3666] total_loss: 2.13848090 d_loss: 1.38620555, g_loss: 0.70058900, ae_loss: 0.05168628\n",
      "Step: [3667] total_loss: 2.12714720 d_loss: 1.38079035, g_loss: 0.69355023, ae_loss: 0.05280671\n",
      "Step: [3668] total_loss: 2.12678599 d_loss: 1.38291025, g_loss: 0.69240957, ae_loss: 0.05146618\n",
      "Step: [3669] total_loss: 2.12218380 d_loss: 1.38282537, g_loss: 0.69145209, ae_loss: 0.04790645\n",
      "Step: [3670] total_loss: 2.12938142 d_loss: 1.38125992, g_loss: 0.69942892, ae_loss: 0.04869256\n",
      "Step: [3671] total_loss: 2.10507131 d_loss: 1.36138391, g_loss: 0.69330359, ae_loss: 0.05038383\n",
      "Step: [3672] total_loss: 2.13635063 d_loss: 1.38601398, g_loss: 0.70038134, ae_loss: 0.04995528\n",
      "Step: [3673] total_loss: 2.10795307 d_loss: 1.35608256, g_loss: 0.69903857, ae_loss: 0.05283184\n",
      "Step: [3674] total_loss: 2.10694432 d_loss: 1.36302435, g_loss: 0.69171464, ae_loss: 0.05220529\n",
      "Step: [3675] total_loss: 2.13668418 d_loss: 1.39350343, g_loss: 0.69292057, ae_loss: 0.05026013\n",
      "Step: [3676] total_loss: 2.11991072 d_loss: 1.38931942, g_loss: 0.67914248, ae_loss: 0.05144895\n",
      "Step: [3677] total_loss: 2.12734127 d_loss: 1.39088523, g_loss: 0.68426824, ae_loss: 0.05218776\n",
      "Step: [3678] total_loss: 2.12881041 d_loss: 1.38441634, g_loss: 0.69607967, ae_loss: 0.04831446\n",
      "Step: [3679] total_loss: 2.11291504 d_loss: 1.37274575, g_loss: 0.68923748, ae_loss: 0.05093170\n",
      "Step: [3680] total_loss: 2.14044428 d_loss: 1.37128425, g_loss: 0.71721339, ae_loss: 0.05194668\n",
      "Step: [3681] total_loss: 2.14596224 d_loss: 1.39456463, g_loss: 0.69751793, ae_loss: 0.05387956\n",
      "Step: [3682] total_loss: 2.12827229 d_loss: 1.36241317, g_loss: 0.71405774, ae_loss: 0.05180136\n",
      "Step: [3683] total_loss: 2.12702823 d_loss: 1.38128614, g_loss: 0.69385839, ae_loss: 0.05188373\n",
      "Step: [3684] total_loss: 2.11986732 d_loss: 1.38249230, g_loss: 0.68338740, ae_loss: 0.05398757\n",
      "Step: [3685] total_loss: 2.12472320 d_loss: 1.37158179, g_loss: 0.70088661, ae_loss: 0.05225474\n",
      "Step: [3686] total_loss: 2.13522792 d_loss: 1.39259183, g_loss: 0.69319266, ae_loss: 0.04944330\n",
      "Step: [3687] total_loss: 2.12634182 d_loss: 1.37885666, g_loss: 0.70007706, ae_loss: 0.04740796\n",
      "Step: [3688] total_loss: 2.12448835 d_loss: 1.39324498, g_loss: 0.68331933, ae_loss: 0.04792390\n",
      "Step: [3689] total_loss: 2.12685919 d_loss: 1.38781095, g_loss: 0.68361586, ae_loss: 0.05543229\n",
      "Step: [3690] total_loss: 2.12017894 d_loss: 1.37006974, g_loss: 0.70020175, ae_loss: 0.04990745\n",
      "Step: [3691] total_loss: 2.13630819 d_loss: 1.38572407, g_loss: 0.69366121, ae_loss: 0.05692295\n",
      "Step: [3692] total_loss: 2.12515640 d_loss: 1.37487245, g_loss: 0.69675267, ae_loss: 0.05353130\n",
      "Step: [3693] total_loss: 2.11099148 d_loss: 1.37514830, g_loss: 0.68090558, ae_loss: 0.05493772\n",
      "Step: [3694] total_loss: 2.11461735 d_loss: 1.36953473, g_loss: 0.69491518, ae_loss: 0.05016726\n",
      "Step: [3695] total_loss: 2.15038919 d_loss: 1.37954450, g_loss: 0.71966100, ae_loss: 0.05118370\n",
      "Step: [3696] total_loss: 2.11797190 d_loss: 1.36112666, g_loss: 0.70618689, ae_loss: 0.05065820\n",
      "Step: [3697] total_loss: 2.14204502 d_loss: 1.39226317, g_loss: 0.69567358, ae_loss: 0.05410815\n",
      "Step: [3698] total_loss: 2.14878893 d_loss: 1.40662920, g_loss: 0.69035983, ae_loss: 0.05179981\n",
      "Step: [3699] total_loss: 2.13938260 d_loss: 1.39545906, g_loss: 0.68985337, ae_loss: 0.05407021\n",
      "Step: [3700] total_loss: 2.11968613 d_loss: 1.36607933, g_loss: 0.70136690, ae_loss: 0.05224003\n",
      "Step: [3701] total_loss: 2.11238098 d_loss: 1.37118208, g_loss: 0.68878287, ae_loss: 0.05241608\n",
      "Step: [3702] total_loss: 2.13753986 d_loss: 1.40167201, g_loss: 0.68539667, ae_loss: 0.05047117\n",
      "Step: [3703] total_loss: 2.11136603 d_loss: 1.38749683, g_loss: 0.67372465, ae_loss: 0.05014452\n",
      "Step: [3704] total_loss: 2.13875031 d_loss: 1.39891577, g_loss: 0.68880355, ae_loss: 0.05103097\n",
      "Step: [3705] total_loss: 2.10585880 d_loss: 1.36015892, g_loss: 0.69601405, ae_loss: 0.04968597\n",
      "Step: [3706] total_loss: 2.10829830 d_loss: 1.36091280, g_loss: 0.69733536, ae_loss: 0.05005029\n",
      "Step: [3707] total_loss: 2.14570379 d_loss: 1.40072989, g_loss: 0.69079340, ae_loss: 0.05418051\n",
      "Step: [3708] total_loss: 2.11447024 d_loss: 1.38732326, g_loss: 0.67997366, ae_loss: 0.04717340\n",
      "Step: [3709] total_loss: 2.10319614 d_loss: 1.37345958, g_loss: 0.68351316, ae_loss: 0.04622327\n",
      "Step: [3710] total_loss: 2.12528825 d_loss: 1.39922464, g_loss: 0.67742229, ae_loss: 0.04864131\n",
      "Step: [3711] total_loss: 2.12048388 d_loss: 1.37550306, g_loss: 0.68661833, ae_loss: 0.05836254\n",
      "Step: [3712] total_loss: 2.12555528 d_loss: 1.34652495, g_loss: 0.72592026, ae_loss: 0.05311013\n",
      "Step: [3713] total_loss: 2.12965131 d_loss: 1.35109532, g_loss: 0.72547722, ae_loss: 0.05307871\n",
      "Step: [3714] total_loss: 2.12301636 d_loss: 1.38394880, g_loss: 0.69078732, ae_loss: 0.04828021\n",
      "Step: [3715] total_loss: 2.11857867 d_loss: 1.37400138, g_loss: 0.69330800, ae_loss: 0.05126934\n",
      "Step: [3716] total_loss: 2.14812803 d_loss: 1.40847385, g_loss: 0.68748236, ae_loss: 0.05217186\n",
      "Step: [3717] total_loss: 2.13823295 d_loss: 1.40020847, g_loss: 0.68272609, ae_loss: 0.05529837\n",
      "Step: [3718] total_loss: 2.11048245 d_loss: 1.36690378, g_loss: 0.69189721, ae_loss: 0.05168137\n",
      "Step: [3719] total_loss: 2.09973860 d_loss: 1.36167121, g_loss: 0.69110471, ae_loss: 0.04696273\n",
      "Step: [3720] total_loss: 2.12127709 d_loss: 1.38019776, g_loss: 0.68931311, ae_loss: 0.05176633\n",
      "Step: [3721] total_loss: 2.09759235 d_loss: 1.38487709, g_loss: 0.66314220, ae_loss: 0.04957290\n",
      "Step: [3722] total_loss: 2.13419390 d_loss: 1.39611769, g_loss: 0.68997329, ae_loss: 0.04810301\n",
      "Step: [3723] total_loss: 2.14424586 d_loss: 1.39151525, g_loss: 0.69782573, ae_loss: 0.05490496\n",
      "Step: [3724] total_loss: 2.10612297 d_loss: 1.37702692, g_loss: 0.67839456, ae_loss: 0.05070160\n",
      "Step: [3725] total_loss: 2.12228966 d_loss: 1.36262918, g_loss: 0.70901084, ae_loss: 0.05064965\n",
      "Step: [3726] total_loss: 2.13897061 d_loss: 1.38240278, g_loss: 0.70172167, ae_loss: 0.05484616\n",
      "Step: [3727] total_loss: 2.11658192 d_loss: 1.36949849, g_loss: 0.69605213, ae_loss: 0.05103131\n",
      "Step: [3728] total_loss: 2.13782024 d_loss: 1.38358629, g_loss: 0.70455807, ae_loss: 0.04967595\n",
      "Step: [3729] total_loss: 2.12317276 d_loss: 1.37257886, g_loss: 0.70173156, ae_loss: 0.04886246\n",
      "Step: [3730] total_loss: 2.11175728 d_loss: 1.39369476, g_loss: 0.66966474, ae_loss: 0.04839768\n",
      "Step: [3731] total_loss: 2.09570074 d_loss: 1.36473918, g_loss: 0.67799032, ae_loss: 0.05297131\n",
      "Step: [3732] total_loss: 2.13147759 d_loss: 1.40365744, g_loss: 0.67654514, ae_loss: 0.05127500\n",
      "Step: [3733] total_loss: 2.11980009 d_loss: 1.37515759, g_loss: 0.69457686, ae_loss: 0.05006550\n",
      "Step: [3734] total_loss: 2.18960333 d_loss: 1.38102984, g_loss: 0.75635839, ae_loss: 0.05221505\n",
      "Step: [3735] total_loss: 2.12709427 d_loss: 1.37680268, g_loss: 0.69992977, ae_loss: 0.05036179\n",
      "Step: [3736] total_loss: 2.14580369 d_loss: 1.39050090, g_loss: 0.70362759, ae_loss: 0.05167525\n",
      "Step: [3737] total_loss: 2.14161825 d_loss: 1.38949907, g_loss: 0.69864100, ae_loss: 0.05347814\n",
      "Step: [3738] total_loss: 2.13668537 d_loss: 1.38041329, g_loss: 0.70372510, ae_loss: 0.05254693\n",
      "Step: [3739] total_loss: 2.12801981 d_loss: 1.36975670, g_loss: 0.70916247, ae_loss: 0.04910073\n",
      "Step: [3740] total_loss: 2.14082360 d_loss: 1.38264143, g_loss: 0.70180166, ae_loss: 0.05638053\n",
      "Step: [3741] total_loss: 2.15186167 d_loss: 1.38178146, g_loss: 0.71749175, ae_loss: 0.05258854\n",
      "Step: [3742] total_loss: 2.13436985 d_loss: 1.38686895, g_loss: 0.69802999, ae_loss: 0.04947084\n",
      "Step: [3743] total_loss: 2.13796973 d_loss: 1.40078568, g_loss: 0.68594646, ae_loss: 0.05123760\n",
      "Step: [3744] total_loss: 2.13889194 d_loss: 1.37142920, g_loss: 0.71779191, ae_loss: 0.04967082\n",
      "Step: [3745] total_loss: 2.11835384 d_loss: 1.36775231, g_loss: 0.69907451, ae_loss: 0.05152711\n",
      "Step: [3746] total_loss: 2.13478732 d_loss: 1.40018296, g_loss: 0.68248165, ae_loss: 0.05212268\n",
      "Step: [3747] total_loss: 2.12750745 d_loss: 1.37848222, g_loss: 0.70226383, ae_loss: 0.04676134\n",
      "Step: [3748] total_loss: 2.14316988 d_loss: 1.40878570, g_loss: 0.68205512, ae_loss: 0.05232913\n",
      "Step: [3749] total_loss: 2.12659431 d_loss: 1.38565898, g_loss: 0.69225192, ae_loss: 0.04868341\n",
      "Step: [3750] total_loss: 2.12120104 d_loss: 1.37575901, g_loss: 0.69237942, ae_loss: 0.05306264\n",
      "Step: [3751] total_loss: 2.14571285 d_loss: 1.39517939, g_loss: 0.70020783, ae_loss: 0.05032581\n",
      "Step: [3752] total_loss: 2.10659981 d_loss: 1.35792112, g_loss: 0.70246661, ae_loss: 0.04621220\n",
      "Step: [3753] total_loss: 2.12428164 d_loss: 1.38132286, g_loss: 0.68862844, ae_loss: 0.05433041\n",
      "Step: [3754] total_loss: 2.13601756 d_loss: 1.38551855, g_loss: 0.70467865, ae_loss: 0.04582034\n",
      "Step: [3755] total_loss: 2.14821434 d_loss: 1.39060497, g_loss: 0.70234680, ae_loss: 0.05526252\n",
      "Step: [3756] total_loss: 2.12412691 d_loss: 1.37129760, g_loss: 0.70160973, ae_loss: 0.05121959\n",
      "Step: [3757] total_loss: 2.14248180 d_loss: 1.40431762, g_loss: 0.68445349, ae_loss: 0.05371073\n",
      "Step: [3758] total_loss: 2.12255788 d_loss: 1.37571406, g_loss: 0.69538164, ae_loss: 0.05146219\n",
      "Step: [3759] total_loss: 2.11981535 d_loss: 1.38057029, g_loss: 0.68815726, ae_loss: 0.05108778\n",
      "Step: [3760] total_loss: 2.12723684 d_loss: 1.40757513, g_loss: 0.66680217, ae_loss: 0.05285940\n",
      "Step: [3761] total_loss: 2.14262700 d_loss: 1.39430475, g_loss: 0.69917381, ae_loss: 0.04914844\n",
      "Step: [3762] total_loss: 2.11592984 d_loss: 1.38021874, g_loss: 0.68792689, ae_loss: 0.04778421\n",
      "Step: [3763] total_loss: 2.12836289 d_loss: 1.38267457, g_loss: 0.69214195, ae_loss: 0.05354625\n",
      "Step: [3764] total_loss: 2.11652422 d_loss: 1.38290536, g_loss: 0.68143964, ae_loss: 0.05217908\n",
      "Step: [3765] total_loss: 2.13949919 d_loss: 1.40301871, g_loss: 0.68729889, ae_loss: 0.04918165\n",
      "Step: [3766] total_loss: 2.13182259 d_loss: 1.38855982, g_loss: 0.69333017, ae_loss: 0.04993263\n",
      "Step: [3767] total_loss: 2.13943219 d_loss: 1.38474977, g_loss: 0.70240784, ae_loss: 0.05227460\n",
      "Step: [3768] total_loss: 2.14273357 d_loss: 1.38453078, g_loss: 0.70910192, ae_loss: 0.04910103\n",
      "Step: [3769] total_loss: 2.15116024 d_loss: 1.39347732, g_loss: 0.70738256, ae_loss: 0.05030045\n",
      "Step: [3770] total_loss: 2.12065649 d_loss: 1.37503481, g_loss: 0.69415402, ae_loss: 0.05146759\n",
      "Step: [3771] total_loss: 2.13275957 d_loss: 1.38522005, g_loss: 0.69459462, ae_loss: 0.05294493\n",
      "Step: [3772] total_loss: 2.12569618 d_loss: 1.37850952, g_loss: 0.69576341, ae_loss: 0.05142321\n",
      "Step: [3773] total_loss: 2.12867260 d_loss: 1.37915659, g_loss: 0.69839895, ae_loss: 0.05111706\n",
      "Step: [3774] total_loss: 2.11769891 d_loss: 1.37590837, g_loss: 0.69386941, ae_loss: 0.04792114\n",
      "Step: [3775] total_loss: 2.13017678 d_loss: 1.39220524, g_loss: 0.68538052, ae_loss: 0.05259102\n",
      "Step: [3776] total_loss: 2.13123846 d_loss: 1.36919117, g_loss: 0.71111810, ae_loss: 0.05092934\n",
      "Step: [3777] total_loss: 2.11929226 d_loss: 1.37506843, g_loss: 0.69346189, ae_loss: 0.05076194\n",
      "Step: [3778] total_loss: 2.11697769 d_loss: 1.38549984, g_loss: 0.67798841, ae_loss: 0.05348957\n",
      "Step: [3779] total_loss: 2.13648939 d_loss: 1.36655915, g_loss: 0.71635377, ae_loss: 0.05357657\n",
      "Step: [3780] total_loss: 2.13060737 d_loss: 1.39607286, g_loss: 0.68064761, ae_loss: 0.05388684\n",
      "Step: [3781] total_loss: 2.11559725 d_loss: 1.38026237, g_loss: 0.68282467, ae_loss: 0.05251026\n",
      "Step: [3782] total_loss: 2.12843132 d_loss: 1.37310946, g_loss: 0.70405918, ae_loss: 0.05126271\n",
      "Step: [3783] total_loss: 2.12052941 d_loss: 1.38855326, g_loss: 0.68159235, ae_loss: 0.05038376\n",
      "Step: [3784] total_loss: 2.11264324 d_loss: 1.36769295, g_loss: 0.69818449, ae_loss: 0.04676594\n",
      "Step: [3785] total_loss: 2.11501932 d_loss: 1.37009501, g_loss: 0.68968379, ae_loss: 0.05524060\n",
      "Step: [3786] total_loss: 2.15192699 d_loss: 1.40633309, g_loss: 0.69547200, ae_loss: 0.05012177\n",
      "Step: [3787] total_loss: 2.10823131 d_loss: 1.37027609, g_loss: 0.68803477, ae_loss: 0.04992038\n",
      "Step: [3788] total_loss: 2.14394808 d_loss: 1.40025139, g_loss: 0.69365227, ae_loss: 0.05004441\n",
      "Step: [3789] total_loss: 2.12261987 d_loss: 1.37615013, g_loss: 0.69735932, ae_loss: 0.04911039\n",
      "Step: [3790] total_loss: 2.12696314 d_loss: 1.37852669, g_loss: 0.70201278, ae_loss: 0.04642368\n",
      "Step: [3791] total_loss: 2.13847232 d_loss: 1.39129639, g_loss: 0.69728011, ae_loss: 0.04989572\n",
      "Step: [3792] total_loss: 2.14439106 d_loss: 1.37158024, g_loss: 0.72227144, ae_loss: 0.05053943\n",
      "Step: [3793] total_loss: 2.12634468 d_loss: 1.37764728, g_loss: 0.70018381, ae_loss: 0.04851350\n",
      "Step: [3794] total_loss: 2.12608957 d_loss: 1.37293732, g_loss: 0.70214748, ae_loss: 0.05100492\n",
      "Step: [3795] total_loss: 2.12543225 d_loss: 1.38548422, g_loss: 0.68895179, ae_loss: 0.05099621\n",
      "Step: [3796] total_loss: 2.11515713 d_loss: 1.38535833, g_loss: 0.67721808, ae_loss: 0.05258089\n",
      "Step: [3797] total_loss: 2.10668063 d_loss: 1.38383102, g_loss: 0.67214161, ae_loss: 0.05070796\n",
      "Step: [3798] total_loss: 2.11469460 d_loss: 1.37209702, g_loss: 0.69172990, ae_loss: 0.05086776\n",
      "Step: [3799] total_loss: 2.12431765 d_loss: 1.38095069, g_loss: 0.69287121, ae_loss: 0.05049581\n",
      "Step: [3800] total_loss: 2.12823296 d_loss: 1.39438486, g_loss: 0.68479764, ae_loss: 0.04905052\n",
      "Step: [3801] total_loss: 2.10438609 d_loss: 1.36704326, g_loss: 0.68614691, ae_loss: 0.05119590\n",
      "Step: [3802] total_loss: 2.12942052 d_loss: 1.38393664, g_loss: 0.69244808, ae_loss: 0.05303583\n",
      "Step: [3803] total_loss: 2.12683773 d_loss: 1.38814461, g_loss: 0.68806225, ae_loss: 0.05063079\n",
      "Step: [3804] total_loss: 2.12642479 d_loss: 1.36572099, g_loss: 0.70878315, ae_loss: 0.05192075\n",
      "Step: [3805] total_loss: 2.13685036 d_loss: 1.39551902, g_loss: 0.69193125, ae_loss: 0.04939995\n",
      "Step: [3806] total_loss: 2.12119579 d_loss: 1.37046587, g_loss: 0.69980478, ae_loss: 0.05092506\n",
      "Step: [3807] total_loss: 2.12016487 d_loss: 1.39082754, g_loss: 0.67964715, ae_loss: 0.04969026\n",
      "Step: [3808] total_loss: 2.11575246 d_loss: 1.38206744, g_loss: 0.68635857, ae_loss: 0.04732645\n",
      "Step: [3809] total_loss: 2.12951851 d_loss: 1.37564754, g_loss: 0.70081127, ae_loss: 0.05305964\n",
      "Step: [3810] total_loss: 2.12289357 d_loss: 1.38473868, g_loss: 0.69134283, ae_loss: 0.04681211\n",
      "Step: [3811] total_loss: 2.13802385 d_loss: 1.39990306, g_loss: 0.68872476, ae_loss: 0.04939604\n",
      "Step: [3812] total_loss: 2.13657403 d_loss: 1.37753987, g_loss: 0.71193826, ae_loss: 0.04709591\n",
      "Step: [3813] total_loss: 2.11297226 d_loss: 1.36727977, g_loss: 0.69818664, ae_loss: 0.04750589\n",
      "Step: [3814] total_loss: 2.11585832 d_loss: 1.37280500, g_loss: 0.69526750, ae_loss: 0.04778587\n",
      "Step: [3815] total_loss: 2.13561320 d_loss: 1.36457074, g_loss: 0.71493632, ae_loss: 0.05610609\n",
      "Step: [3816] total_loss: 2.12943459 d_loss: 1.37061858, g_loss: 0.70953906, ae_loss: 0.04927677\n",
      "Step: [3817] total_loss: 2.13512397 d_loss: 1.38874912, g_loss: 0.69524658, ae_loss: 0.05112827\n",
      "Step: [3818] total_loss: 2.13051009 d_loss: 1.39434898, g_loss: 0.68053210, ae_loss: 0.05562903\n",
      "Step: [3819] total_loss: 2.11738610 d_loss: 1.37349939, g_loss: 0.69135076, ae_loss: 0.05253600\n",
      "Step: [3820] total_loss: 2.11965656 d_loss: 1.38071465, g_loss: 0.68802679, ae_loss: 0.05091522\n",
      "Step: [3821] total_loss: 2.13602662 d_loss: 1.38963294, g_loss: 0.69366413, ae_loss: 0.05272943\n",
      "Step: [3822] total_loss: 2.12783813 d_loss: 1.38696909, g_loss: 0.69108832, ae_loss: 0.04978056\n",
      "Step: [3823] total_loss: 2.12198067 d_loss: 1.37577295, g_loss: 0.69645822, ae_loss: 0.04974961\n",
      "Step: [3824] total_loss: 2.11461949 d_loss: 1.37554955, g_loss: 0.69071257, ae_loss: 0.04835735\n",
      "Step: [3825] total_loss: 2.11403847 d_loss: 1.36467087, g_loss: 0.70004773, ae_loss: 0.04931992\n",
      "Step: [3826] total_loss: 2.13412762 d_loss: 1.38303685, g_loss: 0.69661522, ae_loss: 0.05447537\n",
      "Step: [3827] total_loss: 2.13073134 d_loss: 1.40102887, g_loss: 0.67715555, ae_loss: 0.05254680\n",
      "Step: [3828] total_loss: 2.11226106 d_loss: 1.37689662, g_loss: 0.68222320, ae_loss: 0.05314124\n",
      "Step: [3829] total_loss: 2.12273717 d_loss: 1.37592208, g_loss: 0.69842714, ae_loss: 0.04838796\n",
      "Step: [3830] total_loss: 2.12887168 d_loss: 1.36142921, g_loss: 0.71260953, ae_loss: 0.05483299\n",
      "Step: [3831] total_loss: 2.11611557 d_loss: 1.38621426, g_loss: 0.67672229, ae_loss: 0.05317906\n",
      "Step: [3832] total_loss: 2.11693811 d_loss: 1.39213991, g_loss: 0.67390168, ae_loss: 0.05089667\n",
      "Step: [3833] total_loss: 2.10941410 d_loss: 1.38815463, g_loss: 0.66853392, ae_loss: 0.05272557\n",
      "Step: [3834] total_loss: 2.10038114 d_loss: 1.35553062, g_loss: 0.69935524, ae_loss: 0.04549522\n",
      "Step: [3835] total_loss: 2.12779665 d_loss: 1.37256646, g_loss: 0.70723164, ae_loss: 0.04799849\n",
      "Step: [3836] total_loss: 2.14176273 d_loss: 1.40615427, g_loss: 0.68313247, ae_loss: 0.05247604\n",
      "Step: [3837] total_loss: 2.13857841 d_loss: 1.38282382, g_loss: 0.70430422, ae_loss: 0.05145040\n",
      "Step: [3838] total_loss: 2.09658909 d_loss: 1.36163247, g_loss: 0.68085766, ae_loss: 0.05409889\n",
      "Step: [3839] total_loss: 2.13246465 d_loss: 1.38426983, g_loss: 0.69434839, ae_loss: 0.05384646\n",
      "Step: [3840] total_loss: 2.12464666 d_loss: 1.37904429, g_loss: 0.69588822, ae_loss: 0.04971414\n",
      "Step: [3841] total_loss: 2.11822987 d_loss: 1.38288593, g_loss: 0.68139589, ae_loss: 0.05394787\n",
      "Step: [3842] total_loss: 2.10493112 d_loss: 1.37978840, g_loss: 0.67605126, ae_loss: 0.04909147\n",
      "Step: [3843] total_loss: 2.12497163 d_loss: 1.38633847, g_loss: 0.68277210, ae_loss: 0.05586103\n",
      "Step: [3844] total_loss: 2.15252328 d_loss: 1.40442145, g_loss: 0.69526666, ae_loss: 0.05283514\n",
      "Step: [3845] total_loss: 2.11489248 d_loss: 1.37646079, g_loss: 0.68712234, ae_loss: 0.05130950\n",
      "Step: [3846] total_loss: 2.13668203 d_loss: 1.37584150, g_loss: 0.70908558, ae_loss: 0.05175486\n",
      "Step: [3847] total_loss: 2.15076923 d_loss: 1.39581800, g_loss: 0.70335990, ae_loss: 0.05159134\n",
      "Step: [3848] total_loss: 2.15007973 d_loss: 1.39144397, g_loss: 0.70407826, ae_loss: 0.05455755\n",
      "Step: [3849] total_loss: 2.14911771 d_loss: 1.38719964, g_loss: 0.71066010, ae_loss: 0.05125796\n",
      "Step: [3850] total_loss: 2.12463617 d_loss: 1.37123764, g_loss: 0.69877553, ae_loss: 0.05462314\n",
      "Step: [3851] total_loss: 2.13400412 d_loss: 1.38767791, g_loss: 0.69209182, ae_loss: 0.05423423\n",
      "Step: [3852] total_loss: 2.14940429 d_loss: 1.39157486, g_loss: 0.70320344, ae_loss: 0.05462601\n",
      "Step: [3853] total_loss: 2.12814522 d_loss: 1.38430369, g_loss: 0.69505423, ae_loss: 0.04878718\n",
      "Step: [3854] total_loss: 2.14673114 d_loss: 1.36533546, g_loss: 0.72375119, ae_loss: 0.05764448\n",
      "Step: [3855] total_loss: 2.12941885 d_loss: 1.37921476, g_loss: 0.69932735, ae_loss: 0.05087676\n",
      "Step: [3856] total_loss: 2.13208556 d_loss: 1.38414407, g_loss: 0.69494498, ae_loss: 0.05299654\n",
      "Step: [3857] total_loss: 2.14447308 d_loss: 1.40165341, g_loss: 0.69252038, ae_loss: 0.05029920\n",
      "Step: [3858] total_loss: 2.14693022 d_loss: 1.39575994, g_loss: 0.70199668, ae_loss: 0.04917359\n",
      "Step: [3859] total_loss: 2.15109587 d_loss: 1.39079785, g_loss: 0.70818877, ae_loss: 0.05210936\n",
      "Step: [3860] total_loss: 2.14725542 d_loss: 1.37291563, g_loss: 0.72045267, ae_loss: 0.05388705\n",
      "Step: [3861] total_loss: 2.14848256 d_loss: 1.40820980, g_loss: 0.68802738, ae_loss: 0.05224539\n",
      "Step: [3862] total_loss: 2.16028786 d_loss: 1.41697741, g_loss: 0.69168580, ae_loss: 0.05162476\n",
      "Step: [3863] total_loss: 2.11814809 d_loss: 1.38114214, g_loss: 0.68361551, ae_loss: 0.05339051\n",
      "Step: [3864] total_loss: 2.13887811 d_loss: 1.39218962, g_loss: 0.69520092, ae_loss: 0.05148762\n",
      "Step: [3865] total_loss: 2.13838863 d_loss: 1.39073181, g_loss: 0.70060098, ae_loss: 0.04705599\n",
      "Step: [3866] total_loss: 2.12804461 d_loss: 1.38702524, g_loss: 0.68818617, ae_loss: 0.05283310\n",
      "Step: [3867] total_loss: 2.11063766 d_loss: 1.37583137, g_loss: 0.68495691, ae_loss: 0.04984938\n",
      "Step: [3868] total_loss: 2.13271189 d_loss: 1.40087938, g_loss: 0.68619645, ae_loss: 0.04563614\n",
      "Step: [3869] total_loss: 2.10255027 d_loss: 1.36284006, g_loss: 0.69005644, ae_loss: 0.04965372\n",
      "Step: [3870] total_loss: 2.12491345 d_loss: 1.37932110, g_loss: 0.69417000, ae_loss: 0.05142235\n",
      "Step: [3871] total_loss: 2.12863874 d_loss: 1.39420080, g_loss: 0.68336225, ae_loss: 0.05107575\n",
      "Step: [3872] total_loss: 2.11492968 d_loss: 1.36296999, g_loss: 0.70083964, ae_loss: 0.05111998\n",
      "Step: [3873] total_loss: 2.13501334 d_loss: 1.37487221, g_loss: 0.71054220, ae_loss: 0.04959895\n",
      "Step: [3874] total_loss: 2.10498738 d_loss: 1.37694442, g_loss: 0.67751992, ae_loss: 0.05052299\n",
      "Step: [3875] total_loss: 2.12416744 d_loss: 1.37610722, g_loss: 0.70086157, ae_loss: 0.04719869\n",
      "Step: [3876] total_loss: 2.11983275 d_loss: 1.38145733, g_loss: 0.68621135, ae_loss: 0.05216410\n",
      "Step: [3877] total_loss: 2.10695601 d_loss: 1.38349330, g_loss: 0.67233908, ae_loss: 0.05112350\n",
      "Step: [3878] total_loss: 2.12077284 d_loss: 1.38256419, g_loss: 0.68333542, ae_loss: 0.05487318\n",
      "Step: [3879] total_loss: 2.13479257 d_loss: 1.39626169, g_loss: 0.68574816, ae_loss: 0.05278270\n",
      "Step: [3880] total_loss: 2.13563490 d_loss: 1.40661764, g_loss: 0.67465556, ae_loss: 0.05436171\n",
      "Step: [3881] total_loss: 2.12979436 d_loss: 1.37166119, g_loss: 0.70361841, ae_loss: 0.05451478\n",
      "Step: [3882] total_loss: 2.11502457 d_loss: 1.36378384, g_loss: 0.69828188, ae_loss: 0.05295896\n",
      "Step: [3883] total_loss: 2.13133788 d_loss: 1.37359142, g_loss: 0.70540035, ae_loss: 0.05234613\n",
      "Step: [3884] total_loss: 2.14229989 d_loss: 1.38292491, g_loss: 0.70887119, ae_loss: 0.05050374\n",
      "Step: [3885] total_loss: 2.12687111 d_loss: 1.37655640, g_loss: 0.69870204, ae_loss: 0.05161275\n",
      "Step: [3886] total_loss: 2.13730383 d_loss: 1.39991963, g_loss: 0.68438935, ae_loss: 0.05299501\n",
      "Step: [3887] total_loss: 2.11291742 d_loss: 1.37942314, g_loss: 0.68576431, ae_loss: 0.04773005\n",
      "Step: [3888] total_loss: 2.14115262 d_loss: 1.36462092, g_loss: 0.72368896, ae_loss: 0.05284277\n",
      "Step: [3889] total_loss: 2.11320496 d_loss: 1.37268424, g_loss: 0.68907642, ae_loss: 0.05144438\n",
      "Step: [3890] total_loss: 2.11202621 d_loss: 1.37573814, g_loss: 0.68581390, ae_loss: 0.05047423\n",
      "Step: [3891] total_loss: 2.14627504 d_loss: 1.40583181, g_loss: 0.68627751, ae_loss: 0.05416561\n",
      "Step: [3892] total_loss: 2.12251759 d_loss: 1.37595463, g_loss: 0.69317031, ae_loss: 0.05339249\n",
      "Step: [3893] total_loss: 2.12571931 d_loss: 1.37565839, g_loss: 0.69827086, ae_loss: 0.05179005\n",
      "Step: [3894] total_loss: 2.10117006 d_loss: 1.36449301, g_loss: 0.68956256, ae_loss: 0.04711441\n",
      "Step: [3895] total_loss: 2.11051941 d_loss: 1.38494039, g_loss: 0.67294246, ae_loss: 0.05263648\n",
      "Step: [3896] total_loss: 2.12434864 d_loss: 1.38842618, g_loss: 0.68201625, ae_loss: 0.05390617\n",
      "Step: [3897] total_loss: 2.11698198 d_loss: 1.38235235, g_loss: 0.68951809, ae_loss: 0.04511157\n",
      "Step: [3898] total_loss: 2.13673949 d_loss: 1.38665700, g_loss: 0.69996619, ae_loss: 0.05011632\n",
      "Step: [3899] total_loss: 2.13901639 d_loss: 1.36324131, g_loss: 0.72565329, ae_loss: 0.05012177\n",
      "Step: [3900] total_loss: 2.12527204 d_loss: 1.38478398, g_loss: 0.69372123, ae_loss: 0.04676688\n",
      "Step: [3901] total_loss: 2.11910200 d_loss: 1.39349687, g_loss: 0.67676795, ae_loss: 0.04883722\n",
      "Step: [3902] total_loss: 2.13527012 d_loss: 1.38958073, g_loss: 0.69440860, ae_loss: 0.05128068\n",
      "Step: [3903] total_loss: 2.11100578 d_loss: 1.35776341, g_loss: 0.70264935, ae_loss: 0.05059299\n",
      "Step: [3904] total_loss: 2.12603164 d_loss: 1.39941430, g_loss: 0.67177355, ae_loss: 0.05484381\n",
      "Step: [3905] total_loss: 2.11644030 d_loss: 1.38750982, g_loss: 0.67806071, ae_loss: 0.05086976\n",
      "Step: [3906] total_loss: 2.10223150 d_loss: 1.37128806, g_loss: 0.67472267, ae_loss: 0.05622080\n",
      "Step: [3907] total_loss: 2.11055589 d_loss: 1.37086284, g_loss: 0.69128436, ae_loss: 0.04840868\n",
      "Step: [3908] total_loss: 2.14074874 d_loss: 1.37873125, g_loss: 0.70945418, ae_loss: 0.05256327\n",
      "Step: [3909] total_loss: 2.12750387 d_loss: 1.39827037, g_loss: 0.67678928, ae_loss: 0.05244420\n",
      "Step: [3910] total_loss: 2.13521218 d_loss: 1.37683868, g_loss: 0.71138316, ae_loss: 0.04699027\n",
      "Step: [3911] total_loss: 2.14336681 d_loss: 1.39111936, g_loss: 0.70156038, ae_loss: 0.05068709\n",
      "Step: [3912] total_loss: 2.12942791 d_loss: 1.36790633, g_loss: 0.70859402, ae_loss: 0.05292763\n",
      "Step: [3913] total_loss: 2.11093736 d_loss: 1.36114275, g_loss: 0.70224041, ae_loss: 0.04755425\n",
      "Step: [3914] total_loss: 2.14518023 d_loss: 1.40481985, g_loss: 0.68811804, ae_loss: 0.05224242\n",
      "Step: [3915] total_loss: 2.11187124 d_loss: 1.37305272, g_loss: 0.69305837, ae_loss: 0.04576001\n",
      "Step: [3916] total_loss: 2.11602163 d_loss: 1.37882221, g_loss: 0.68759215, ae_loss: 0.04960736\n",
      "Step: [3917] total_loss: 2.11863136 d_loss: 1.38420916, g_loss: 0.68139493, ae_loss: 0.05302712\n",
      "Step: [3918] total_loss: 2.13741398 d_loss: 1.37884760, g_loss: 0.70201731, ae_loss: 0.05654898\n",
      "Step: [3919] total_loss: 2.13703084 d_loss: 1.38846755, g_loss: 0.69980884, ae_loss: 0.04875446\n",
      "Step: [3920] total_loss: 2.13244867 d_loss: 1.39306295, g_loss: 0.69045323, ae_loss: 0.04893259\n",
      "Step: [3921] total_loss: 2.12849617 d_loss: 1.36412489, g_loss: 0.71215266, ae_loss: 0.05221869\n",
      "Step: [3922] total_loss: 2.11886215 d_loss: 1.38170934, g_loss: 0.68479180, ae_loss: 0.05236104\n",
      "Step: [3923] total_loss: 2.12575054 d_loss: 1.38652539, g_loss: 0.69024014, ae_loss: 0.04898490\n",
      "Step: [3924] total_loss: 2.13557625 d_loss: 1.40278959, g_loss: 0.68677557, ae_loss: 0.04601099\n",
      "Step: [3925] total_loss: 2.11694026 d_loss: 1.37169790, g_loss: 0.69336826, ae_loss: 0.05187421\n",
      "Step: [3926] total_loss: 2.09152961 d_loss: 1.37591481, g_loss: 0.66506773, ae_loss: 0.05054703\n",
      "Step: [3927] total_loss: 2.10973525 d_loss: 1.37061036, g_loss: 0.68899602, ae_loss: 0.05012876\n",
      "Step: [3928] total_loss: 2.10801172 d_loss: 1.37845731, g_loss: 0.67930937, ae_loss: 0.05024509\n",
      "Step: [3929] total_loss: 2.11705017 d_loss: 1.37964606, g_loss: 0.68634266, ae_loss: 0.05106153\n",
      "Step: [3930] total_loss: 2.12906623 d_loss: 1.35815144, g_loss: 0.72060978, ae_loss: 0.05030503\n",
      "Step: [3931] total_loss: 2.12566400 d_loss: 1.37356782, g_loss: 0.70010912, ae_loss: 0.05198703\n",
      "Step: [3932] total_loss: 2.13059235 d_loss: 1.36803746, g_loss: 0.71619356, ae_loss: 0.04636123\n",
      "Step: [3933] total_loss: 2.12322617 d_loss: 1.38018763, g_loss: 0.69374788, ae_loss: 0.04929071\n",
      "Step: [3934] total_loss: 2.12945938 d_loss: 1.38001943, g_loss: 0.69624484, ae_loss: 0.05319529\n",
      "Step: [3935] total_loss: 2.12374473 d_loss: 1.36494040, g_loss: 0.70928079, ae_loss: 0.04952354\n",
      "Step: [3936] total_loss: 2.12603521 d_loss: 1.37903690, g_loss: 0.69285882, ae_loss: 0.05413950\n",
      "Step: [3937] total_loss: 2.11607552 d_loss: 1.37406373, g_loss: 0.68907207, ae_loss: 0.05293982\n",
      "Step: [3938] total_loss: 2.12789226 d_loss: 1.38245583, g_loss: 0.69729137, ae_loss: 0.04814507\n",
      "Step: [3939] total_loss: 2.12763810 d_loss: 1.38455987, g_loss: 0.69344193, ae_loss: 0.04963623\n",
      "Step: [3940] total_loss: 2.13838768 d_loss: 1.37083960, g_loss: 0.71893704, ae_loss: 0.04861095\n",
      "Step: [3941] total_loss: 2.12375402 d_loss: 1.36625910, g_loss: 0.70700276, ae_loss: 0.05049205\n",
      "Step: [3942] total_loss: 2.14487076 d_loss: 1.37645340, g_loss: 0.71351492, ae_loss: 0.05490231\n",
      "Step: [3943] total_loss: 2.13671970 d_loss: 1.38782191, g_loss: 0.69896150, ae_loss: 0.04993634\n",
      "Step: [3944] total_loss: 2.13907146 d_loss: 1.36091316, g_loss: 0.72781575, ae_loss: 0.05034238\n",
      "Step: [3945] total_loss: 2.14512992 d_loss: 1.38284874, g_loss: 0.70379335, ae_loss: 0.05848781\n",
      "Step: [3946] total_loss: 2.12657785 d_loss: 1.36471248, g_loss: 0.71112555, ae_loss: 0.05073973\n",
      "Step: [3947] total_loss: 2.13840389 d_loss: 1.39965343, g_loss: 0.69035792, ae_loss: 0.04839241\n",
      "Step: [3948] total_loss: 2.13242006 d_loss: 1.37076759, g_loss: 0.71147478, ae_loss: 0.05017756\n",
      "Step: [3949] total_loss: 2.11353135 d_loss: 1.36325586, g_loss: 0.70039982, ae_loss: 0.04987562\n",
      "Step: [3950] total_loss: 2.12666941 d_loss: 1.36896539, g_loss: 0.70730555, ae_loss: 0.05039847\n",
      "Step: [3951] total_loss: 2.12389541 d_loss: 1.38987756, g_loss: 0.68032455, ae_loss: 0.05369327\n",
      "Step: [3952] total_loss: 2.10822153 d_loss: 1.37706614, g_loss: 0.68376637, ae_loss: 0.04738910\n",
      "Step: [3953] total_loss: 2.10854149 d_loss: 1.37027073, g_loss: 0.68546712, ae_loss: 0.05280358\n",
      "Step: [3954] total_loss: 2.11142254 d_loss: 1.38030553, g_loss: 0.68193400, ae_loss: 0.04918315\n",
      "Step: [3955] total_loss: 2.12647724 d_loss: 1.33655334, g_loss: 0.74007308, ae_loss: 0.04985077\n",
      "Step: [3956] total_loss: 2.11737514 d_loss: 1.38834119, g_loss: 0.67532521, ae_loss: 0.05370880\n",
      "Step: [3957] total_loss: 2.16250300 d_loss: 1.39665854, g_loss: 0.71549857, ae_loss: 0.05034592\n",
      "Step: [3958] total_loss: 2.18416452 d_loss: 1.41310024, g_loss: 0.72153676, ae_loss: 0.04952743\n",
      "Step: [3959] total_loss: 2.14578438 d_loss: 1.40037930, g_loss: 0.69432318, ae_loss: 0.05108184\n",
      "Step: [3960] total_loss: 2.12431264 d_loss: 1.39740419, g_loss: 0.67752695, ae_loss: 0.04938152\n",
      "Step: [3961] total_loss: 2.15238285 d_loss: 1.38584590, g_loss: 0.71587718, ae_loss: 0.05065973\n",
      "Step: [3962] total_loss: 2.11387777 d_loss: 1.35694993, g_loss: 0.70426917, ae_loss: 0.05265861\n",
      "Step: [3963] total_loss: 2.11352229 d_loss: 1.36607003, g_loss: 0.69430655, ae_loss: 0.05314563\n",
      "Step: [3964] total_loss: 2.13897038 d_loss: 1.39621782, g_loss: 0.69288146, ae_loss: 0.04987113\n",
      "Step: [3965] total_loss: 2.12028074 d_loss: 1.38600492, g_loss: 0.68365073, ae_loss: 0.05062496\n",
      "Step: [3966] total_loss: 2.09822965 d_loss: 1.35889101, g_loss: 0.69001645, ae_loss: 0.04932213\n",
      "Step: [3967] total_loss: 2.13674140 d_loss: 1.39588964, g_loss: 0.69011617, ae_loss: 0.05073555\n",
      "Step: [3968] total_loss: 2.08341455 d_loss: 1.35724378, g_loss: 0.67571187, ae_loss: 0.05045900\n",
      "Step: [3969] total_loss: 2.12602329 d_loss: 1.38095689, g_loss: 0.69057882, ae_loss: 0.05448750\n",
      "Step: [3970] total_loss: 2.14374471 d_loss: 1.41342676, g_loss: 0.68182904, ae_loss: 0.04848882\n",
      "Step: [3971] total_loss: 2.12091923 d_loss: 1.37720132, g_loss: 0.69435412, ae_loss: 0.04936388\n",
      "Step: [3972] total_loss: 2.12111330 d_loss: 1.39012790, g_loss: 0.68091911, ae_loss: 0.05006620\n",
      "Step: [3973] total_loss: 2.10603714 d_loss: 1.35024595, g_loss: 0.70404625, ae_loss: 0.05174510\n",
      "Step: [3974] total_loss: 2.12066603 d_loss: 1.37562478, g_loss: 0.69286776, ae_loss: 0.05217344\n",
      "Step: [3975] total_loss: 2.16267657 d_loss: 1.41454935, g_loss: 0.69720805, ae_loss: 0.05091917\n",
      "Step: [3976] total_loss: 2.15195990 d_loss: 1.40681124, g_loss: 0.69563603, ae_loss: 0.04951260\n",
      "Step: [3977] total_loss: 2.13254714 d_loss: 1.38958383, g_loss: 0.69387066, ae_loss: 0.04909264\n",
      "Step: [3978] total_loss: 2.14934564 d_loss: 1.41607833, g_loss: 0.68396831, ae_loss: 0.04929895\n",
      "Step: [3979] total_loss: 2.14835620 d_loss: 1.39559913, g_loss: 0.70047045, ae_loss: 0.05228657\n",
      "Step: [3980] total_loss: 2.12624788 d_loss: 1.38777518, g_loss: 0.68718833, ae_loss: 0.05128431\n",
      "Step: [3981] total_loss: 2.12736368 d_loss: 1.38284159, g_loss: 0.69384623, ae_loss: 0.05067572\n",
      "Step: [3982] total_loss: 2.13164902 d_loss: 1.38507962, g_loss: 0.68833077, ae_loss: 0.05823858\n",
      "Step: [3983] total_loss: 2.12166119 d_loss: 1.38934267, g_loss: 0.68432224, ae_loss: 0.04799619\n",
      "Step: [3984] total_loss: 2.12564802 d_loss: 1.37762678, g_loss: 0.69763529, ae_loss: 0.05038612\n",
      "Step: [3985] total_loss: 2.12440705 d_loss: 1.35418153, g_loss: 0.72332525, ae_loss: 0.04690029\n",
      "Step: [3986] total_loss: 2.11754918 d_loss: 1.38105357, g_loss: 0.68531442, ae_loss: 0.05118118\n",
      "Step: [3987] total_loss: 2.14575052 d_loss: 1.39976156, g_loss: 0.69042557, ae_loss: 0.05556333\n",
      "Step: [3988] total_loss: 2.11207390 d_loss: 1.36345935, g_loss: 0.70020235, ae_loss: 0.04841215\n",
      "Step: [3989] total_loss: 2.11907196 d_loss: 1.37513173, g_loss: 0.68922389, ae_loss: 0.05471643\n",
      "Step: [3990] total_loss: 2.13067055 d_loss: 1.37252378, g_loss: 0.70738614, ae_loss: 0.05076079\n",
      "Step: [3991] total_loss: 2.12989020 d_loss: 1.38536429, g_loss: 0.69365191, ae_loss: 0.05087396\n",
      "Step: [3992] total_loss: 2.14107561 d_loss: 1.39504433, g_loss: 0.69315064, ae_loss: 0.05288078\n",
      "Step: [3993] total_loss: 2.11589384 d_loss: 1.36359692, g_loss: 0.70354819, ae_loss: 0.04874868\n",
      "Step: [3994] total_loss: 2.13979673 d_loss: 1.39169812, g_loss: 0.69272196, ae_loss: 0.05537675\n",
      "Step: [3995] total_loss: 2.13434601 d_loss: 1.39106750, g_loss: 0.69038248, ae_loss: 0.05289605\n",
      "Step: [3996] total_loss: 2.12220240 d_loss: 1.38086689, g_loss: 0.69002664, ae_loss: 0.05130890\n",
      "Step: [3997] total_loss: 2.11647058 d_loss: 1.36918426, g_loss: 0.69456601, ae_loss: 0.05272027\n",
      "Step: [3998] total_loss: 2.12143707 d_loss: 1.38646936, g_loss: 0.68109626, ae_loss: 0.05387142\n",
      "Step: [3999] total_loss: 2.10870099 d_loss: 1.38609171, g_loss: 0.67295134, ae_loss: 0.04965794\n",
      "Step: [4000] total_loss: 2.12706947 d_loss: 1.36938429, g_loss: 0.70864773, ae_loss: 0.04903757\n",
      "Step: [4001] total_loss: 2.15041637 d_loss: 1.41460013, g_loss: 0.68523836, ae_loss: 0.05057780\n",
      "Step: [4002] total_loss: 2.12739992 d_loss: 1.36915541, g_loss: 0.70206833, ae_loss: 0.05617614\n",
      "Step: [4003] total_loss: 2.11378765 d_loss: 1.36655128, g_loss: 0.69703603, ae_loss: 0.05020043\n",
      "Step: [4004] total_loss: 2.14371157 d_loss: 1.39120030, g_loss: 0.70203316, ae_loss: 0.05047806\n",
      "Step: [4005] total_loss: 2.11526155 d_loss: 1.36866951, g_loss: 0.69046509, ae_loss: 0.05612694\n",
      "Step: [4006] total_loss: 2.12384224 d_loss: 1.37396157, g_loss: 0.70214152, ae_loss: 0.04773922\n",
      "Step: [4007] total_loss: 2.14091015 d_loss: 1.38831174, g_loss: 0.69811082, ae_loss: 0.05448753\n",
      "Step: [4008] total_loss: 2.11603832 d_loss: 1.38676941, g_loss: 0.68028510, ae_loss: 0.04898372\n",
      "Step: [4009] total_loss: 2.12691164 d_loss: 1.37066317, g_loss: 0.70462120, ae_loss: 0.05162712\n",
      "Step: [4010] total_loss: 2.11327672 d_loss: 1.37969899, g_loss: 0.68432057, ae_loss: 0.04925714\n",
      "Step: [4011] total_loss: 2.10997009 d_loss: 1.36878777, g_loss: 0.69011545, ae_loss: 0.05106686\n",
      "Step: [4012] total_loss: 2.13121247 d_loss: 1.39897919, g_loss: 0.68214703, ae_loss: 0.05008623\n",
      "Step: [4013] total_loss: 2.11487150 d_loss: 1.36010551, g_loss: 0.70030701, ae_loss: 0.05445900\n",
      "Step: [4014] total_loss: 2.10213137 d_loss: 1.37826967, g_loss: 0.67032999, ae_loss: 0.05353159\n",
      "Step: [4015] total_loss: 2.14086366 d_loss: 1.39319301, g_loss: 0.69665676, ae_loss: 0.05101387\n",
      "Step: [4016] total_loss: 2.12072277 d_loss: 1.36710668, g_loss: 0.70506871, ae_loss: 0.04854744\n",
      "Step: [4017] total_loss: 2.13381910 d_loss: 1.38823867, g_loss: 0.69689304, ae_loss: 0.04868752\n",
      "Step: [4018] total_loss: 2.15346932 d_loss: 1.38703895, g_loss: 0.71748763, ae_loss: 0.04894280\n",
      "Step: [4019] total_loss: 2.11518455 d_loss: 1.35761273, g_loss: 0.70702970, ae_loss: 0.05054216\n",
      "Step: [4020] total_loss: 2.16665602 d_loss: 1.39168549, g_loss: 0.72284383, ae_loss: 0.05212678\n",
      "Step: [4021] total_loss: 2.13067985 d_loss: 1.38938880, g_loss: 0.68484998, ae_loss: 0.05644102\n",
      "Step: [4022] total_loss: 2.11781859 d_loss: 1.36699402, g_loss: 0.70328790, ae_loss: 0.04753673\n",
      "Step: [4023] total_loss: 2.12000704 d_loss: 1.38275719, g_loss: 0.68935859, ae_loss: 0.04789125\n",
      "Step: [4024] total_loss: 2.12716889 d_loss: 1.39101398, g_loss: 0.68919241, ae_loss: 0.04696250\n",
      "Step: [4025] total_loss: 2.12158847 d_loss: 1.35093749, g_loss: 0.71987551, ae_loss: 0.05077555\n",
      "Step: [4026] total_loss: 2.12596774 d_loss: 1.38123035, g_loss: 0.69176912, ae_loss: 0.05296822\n",
      "Step: [4027] total_loss: 2.11308861 d_loss: 1.38584149, g_loss: 0.67906302, ae_loss: 0.04818407\n",
      "Step: [4028] total_loss: 2.10239697 d_loss: 1.36790276, g_loss: 0.68093252, ae_loss: 0.05356178\n",
      "Step: [4029] total_loss: 2.12388635 d_loss: 1.38871503, g_loss: 0.68737483, ae_loss: 0.04779645\n",
      "Step: [4030] total_loss: 2.14476705 d_loss: 1.38934338, g_loss: 0.70707327, ae_loss: 0.04835036\n",
      "Step: [4031] total_loss: 2.10506701 d_loss: 1.35581994, g_loss: 0.69714051, ae_loss: 0.05210656\n",
      "Step: [4032] total_loss: 2.11593771 d_loss: 1.36942267, g_loss: 0.69657600, ae_loss: 0.04993900\n",
      "Step: [4033] total_loss: 2.12353659 d_loss: 1.37707186, g_loss: 0.69835734, ae_loss: 0.04810753\n",
      "Step: [4034] total_loss: 2.14327574 d_loss: 1.40620613, g_loss: 0.68258905, ae_loss: 0.05448065\n",
      "Step: [4035] total_loss: 2.11407471 d_loss: 1.37074220, g_loss: 0.68942893, ae_loss: 0.05390361\n",
      "Step: [4036] total_loss: 2.16346550 d_loss: 1.40235829, g_loss: 0.71081448, ae_loss: 0.05029289\n",
      "Step: [4037] total_loss: 2.16053629 d_loss: 1.40606952, g_loss: 0.70074672, ae_loss: 0.05372012\n",
      "Step: [4038] total_loss: 2.11495161 d_loss: 1.36458325, g_loss: 0.69869769, ae_loss: 0.05167051\n",
      "Step: [4039] total_loss: 2.11767817 d_loss: 1.39784014, g_loss: 0.66809875, ae_loss: 0.05173924\n",
      "Step: [4040] total_loss: 2.13852382 d_loss: 1.37498844, g_loss: 0.71266735, ae_loss: 0.05086805\n",
      "Step: [4041] total_loss: 2.11757803 d_loss: 1.38290465, g_loss: 0.68243873, ae_loss: 0.05223481\n",
      "Step: [4042] total_loss: 2.13477278 d_loss: 1.39550436, g_loss: 0.68329811, ae_loss: 0.05597027\n",
      "Step: [4043] total_loss: 2.13639617 d_loss: 1.40388882, g_loss: 0.68246955, ae_loss: 0.05003778\n",
      "Step: [4044] total_loss: 2.11192298 d_loss: 1.35773206, g_loss: 0.69950122, ae_loss: 0.05468968\n",
      "Step: [4045] total_loss: 2.12627363 d_loss: 1.38118005, g_loss: 0.69686919, ae_loss: 0.04822443\n",
      "Step: [4046] total_loss: 2.12223625 d_loss: 1.38661814, g_loss: 0.68673837, ae_loss: 0.04887968\n",
      "Step: [4047] total_loss: 2.11939692 d_loss: 1.37649012, g_loss: 0.69180864, ae_loss: 0.05109809\n",
      "Step: [4048] total_loss: 2.15753031 d_loss: 1.42076302, g_loss: 0.68538731, ae_loss: 0.05138004\n",
      "Step: [4049] total_loss: 2.12318349 d_loss: 1.36761618, g_loss: 0.70515776, ae_loss: 0.05040960\n",
      "Step: [4050] total_loss: 2.13087225 d_loss: 1.40288186, g_loss: 0.67561007, ae_loss: 0.05238023\n",
      "Step: [4051] total_loss: 2.13357735 d_loss: 1.39253998, g_loss: 0.68957222, ae_loss: 0.05146522\n",
      "Step: [4052] total_loss: 2.11804581 d_loss: 1.38980913, g_loss: 0.67495918, ae_loss: 0.05327738\n",
      "Step: [4053] total_loss: 2.10938382 d_loss: 1.37711382, g_loss: 0.68366200, ae_loss: 0.04860802\n",
      "Step: [4054] total_loss: 2.12463188 d_loss: 1.38339818, g_loss: 0.68795443, ae_loss: 0.05327913\n",
      "Step: [4055] total_loss: 2.14897013 d_loss: 1.38429224, g_loss: 0.71213710, ae_loss: 0.05254060\n",
      "Step: [4056] total_loss: 2.13299990 d_loss: 1.38477111, g_loss: 0.70003426, ae_loss: 0.04819440\n",
      "Step: [4057] total_loss: 2.12798738 d_loss: 1.38592744, g_loss: 0.69038826, ae_loss: 0.05167173\n",
      "Step: [4058] total_loss: 2.15710187 d_loss: 1.40454447, g_loss: 0.70310402, ae_loss: 0.04945336\n",
      "Step: [4059] total_loss: 2.14028096 d_loss: 1.38491690, g_loss: 0.70410573, ae_loss: 0.05125832\n",
      "Step: [4060] total_loss: 2.12452412 d_loss: 1.38125122, g_loss: 0.69204879, ae_loss: 0.05122418\n",
      "Step: [4061] total_loss: 2.11477804 d_loss: 1.37296104, g_loss: 0.69376481, ae_loss: 0.04805216\n",
      "Step: [4062] total_loss: 2.11722708 d_loss: 1.39205456, g_loss: 0.67295980, ae_loss: 0.05221282\n",
      "Step: [4063] total_loss: 2.14199400 d_loss: 1.38912475, g_loss: 0.70212519, ae_loss: 0.05074410\n",
      "Step: [4064] total_loss: 2.10567236 d_loss: 1.35555053, g_loss: 0.69811493, ae_loss: 0.05200693\n",
      "Step: [4065] total_loss: 2.11952281 d_loss: 1.37053275, g_loss: 0.69992357, ae_loss: 0.04906651\n",
      "Step: [4066] total_loss: 2.12285185 d_loss: 1.37754107, g_loss: 0.69431180, ae_loss: 0.05099887\n",
      "Step: [4067] total_loss: 2.10852718 d_loss: 1.37992346, g_loss: 0.68059731, ae_loss: 0.04800625\n",
      "Step: [4068] total_loss: 2.14043164 d_loss: 1.40344429, g_loss: 0.68553311, ae_loss: 0.05145429\n",
      "Step: [4069] total_loss: 2.13652229 d_loss: 1.35253119, g_loss: 0.73329777, ae_loss: 0.05069343\n",
      "Step: [4070] total_loss: 2.11865187 d_loss: 1.37354207, g_loss: 0.69253308, ae_loss: 0.05257669\n",
      "Step: [4071] total_loss: 2.11624932 d_loss: 1.39794099, g_loss: 0.67085767, ae_loss: 0.04745063\n",
      "Step: [4072] total_loss: 2.11397672 d_loss: 1.38204980, g_loss: 0.68182170, ae_loss: 0.05010526\n",
      "Step: [4073] total_loss: 2.12271309 d_loss: 1.37014604, g_loss: 0.69637465, ae_loss: 0.05619240\n",
      "Step: [4074] total_loss: 2.09748793 d_loss: 1.35487640, g_loss: 0.69359452, ae_loss: 0.04901697\n",
      "Step: [4075] total_loss: 2.13111401 d_loss: 1.38086796, g_loss: 0.70065403, ae_loss: 0.04959185\n",
      "Step: [4076] total_loss: 2.12851095 d_loss: 1.37844896, g_loss: 0.69845355, ae_loss: 0.05160833\n",
      "Step: [4077] total_loss: 2.12603664 d_loss: 1.39206505, g_loss: 0.68326741, ae_loss: 0.05070424\n",
      "Step: [4078] total_loss: 2.14457798 d_loss: 1.39453459, g_loss: 0.69673622, ae_loss: 0.05330715\n",
      "Step: [4079] total_loss: 2.13574553 d_loss: 1.39456725, g_loss: 0.68994093, ae_loss: 0.05123737\n",
      "Step: [4080] total_loss: 2.13419390 d_loss: 1.38806081, g_loss: 0.69333231, ae_loss: 0.05280085\n",
      "Step: [4081] total_loss: 2.13602638 d_loss: 1.36779881, g_loss: 0.71872926, ae_loss: 0.04949826\n",
      "Step: [4082] total_loss: 2.15426731 d_loss: 1.36621189, g_loss: 0.73141980, ae_loss: 0.05663573\n",
      "Step: [4083] total_loss: 2.12471056 d_loss: 1.35130823, g_loss: 0.72831690, ae_loss: 0.04508540\n",
      "Step: [4084] total_loss: 2.11520910 d_loss: 1.37418294, g_loss: 0.68887103, ae_loss: 0.05215531\n",
      "Step: [4085] total_loss: 2.12681723 d_loss: 1.37023282, g_loss: 0.70718980, ae_loss: 0.04939458\n",
      "Step: [4086] total_loss: 2.13528419 d_loss: 1.37757492, g_loss: 0.70517552, ae_loss: 0.05253375\n",
      "Step: [4087] total_loss: 2.12009907 d_loss: 1.38596070, g_loss: 0.68392140, ae_loss: 0.05021691\n",
      "Step: [4088] total_loss: 2.12266064 d_loss: 1.38449800, g_loss: 0.68517989, ae_loss: 0.05298284\n",
      "Step: [4089] total_loss: 2.12177968 d_loss: 1.37822533, g_loss: 0.69367385, ae_loss: 0.04988053\n",
      "Step: [4090] total_loss: 2.11403322 d_loss: 1.37304270, g_loss: 0.69360936, ae_loss: 0.04738112\n",
      "Step: [4091] total_loss: 2.12229347 d_loss: 1.40082228, g_loss: 0.67188549, ae_loss: 0.04958558\n",
      "Step: [4092] total_loss: 2.12154531 d_loss: 1.39429080, g_loss: 0.67934430, ae_loss: 0.04791034\n",
      "Step: [4093] total_loss: 2.13162661 d_loss: 1.39478016, g_loss: 0.68649369, ae_loss: 0.05035279\n",
      "Step: [4094] total_loss: 2.14347458 d_loss: 1.38709486, g_loss: 0.70211685, ae_loss: 0.05426275\n",
      "Step: [4095] total_loss: 2.15147352 d_loss: 1.40881896, g_loss: 0.69165170, ae_loss: 0.05100300\n",
      "Step: [4096] total_loss: 2.14226127 d_loss: 1.39881134, g_loss: 0.69209421, ae_loss: 0.05135575\n",
      "Step: [4097] total_loss: 2.14579797 d_loss: 1.38984454, g_loss: 0.69773340, ae_loss: 0.05822001\n",
      "Step: [4098] total_loss: 2.14365029 d_loss: 1.37781131, g_loss: 0.71330523, ae_loss: 0.05253378\n",
      "Step: [4099] total_loss: 2.13144732 d_loss: 1.38288260, g_loss: 0.69788957, ae_loss: 0.05067519\n",
      "Step: [4100] total_loss: 2.13204050 d_loss: 1.37666214, g_loss: 0.70320165, ae_loss: 0.05217683\n",
      "Step: [4101] total_loss: 2.12894011 d_loss: 1.38091779, g_loss: 0.69462001, ae_loss: 0.05340222\n",
      "Step: [4102] total_loss: 2.12342405 d_loss: 1.38563526, g_loss: 0.68475521, ae_loss: 0.05303349\n",
      "Step: [4103] total_loss: 2.11885929 d_loss: 1.37322259, g_loss: 0.69593078, ae_loss: 0.04970594\n",
      "Step: [4104] total_loss: 2.12812901 d_loss: 1.39294982, g_loss: 0.68610060, ae_loss: 0.04907845\n",
      "Step: [4105] total_loss: 2.13253284 d_loss: 1.37694573, g_loss: 0.70153147, ae_loss: 0.05405565\n",
      "Step: [4106] total_loss: 2.12979650 d_loss: 1.39814723, g_loss: 0.67964309, ae_loss: 0.05200610\n",
      "Step: [4107] total_loss: 2.12589121 d_loss: 1.38072407, g_loss: 0.69168270, ae_loss: 0.05348460\n",
      "Step: [4108] total_loss: 2.12594461 d_loss: 1.36773849, g_loss: 0.70562625, ae_loss: 0.05257984\n",
      "Step: [4109] total_loss: 2.13369751 d_loss: 1.39337730, g_loss: 0.68601024, ae_loss: 0.05431005\n",
      "Step: [4110] total_loss: 2.10968399 d_loss: 1.35835016, g_loss: 0.70197868, ae_loss: 0.04935509\n",
      "Step: [4111] total_loss: 2.10583758 d_loss: 1.38100553, g_loss: 0.67451537, ae_loss: 0.05031664\n",
      "Step: [4112] total_loss: 2.10948682 d_loss: 1.36811352, g_loss: 0.69086987, ae_loss: 0.05050335\n",
      "Step: [4113] total_loss: 2.12074280 d_loss: 1.37501967, g_loss: 0.69354880, ae_loss: 0.05217420\n",
      "Step: [4114] total_loss: 2.14266992 d_loss: 1.39769506, g_loss: 0.68995821, ae_loss: 0.05501663\n",
      "Step: [4115] total_loss: 2.13596582 d_loss: 1.37836361, g_loss: 0.70519501, ae_loss: 0.05240723\n",
      "Step: [4116] total_loss: 2.13671827 d_loss: 1.37722993, g_loss: 0.70362377, ae_loss: 0.05586464\n",
      "Step: [4117] total_loss: 2.12575245 d_loss: 1.37636602, g_loss: 0.69512731, ae_loss: 0.05425904\n",
      "Step: [4118] total_loss: 2.13670826 d_loss: 1.38624465, g_loss: 0.69883811, ae_loss: 0.05162554\n",
      "Step: [4119] total_loss: 2.14229679 d_loss: 1.38649714, g_loss: 0.70722491, ae_loss: 0.04857469\n",
      "Step: [4120] total_loss: 2.14212179 d_loss: 1.38286448, g_loss: 0.71238732, ae_loss: 0.04686995\n",
      "Step: [4121] total_loss: 2.13945103 d_loss: 1.38476396, g_loss: 0.70455027, ae_loss: 0.05013696\n",
      "Step: [4122] total_loss: 2.13496733 d_loss: 1.36162329, g_loss: 0.72164118, ae_loss: 0.05170293\n",
      "Step: [4123] total_loss: 2.14871335 d_loss: 1.39665627, g_loss: 0.70571017, ae_loss: 0.04634689\n",
      "Step: [4124] total_loss: 2.13601398 d_loss: 1.38006639, g_loss: 0.70615697, ae_loss: 0.04979067\n",
      "Step: [4125] total_loss: 2.11378431 d_loss: 1.36227763, g_loss: 0.69727844, ae_loss: 0.05422836\n",
      "Step: [4126] total_loss: 2.11672974 d_loss: 1.38354492, g_loss: 0.68075937, ae_loss: 0.05242548\n",
      "Step: [4127] total_loss: 2.12593555 d_loss: 1.38028264, g_loss: 0.69729316, ae_loss: 0.04835978\n",
      "Step: [4128] total_loss: 2.11368370 d_loss: 1.36975837, g_loss: 0.69359910, ae_loss: 0.05032632\n",
      "Step: [4129] total_loss: 2.12785387 d_loss: 1.38598680, g_loss: 0.69217157, ae_loss: 0.04969538\n",
      "Step: [4130] total_loss: 2.12240934 d_loss: 1.38273919, g_loss: 0.68783265, ae_loss: 0.05183740\n",
      "Step: [4131] total_loss: 2.10532022 d_loss: 1.36856091, g_loss: 0.68812555, ae_loss: 0.04863386\n",
      "Step: [4132] total_loss: 2.11775780 d_loss: 1.38992763, g_loss: 0.67016721, ae_loss: 0.05766307\n",
      "Step: [4133] total_loss: 2.11912489 d_loss: 1.38219357, g_loss: 0.68426239, ae_loss: 0.05266897\n",
      "Step: [4134] total_loss: 2.11335468 d_loss: 1.37093794, g_loss: 0.68853128, ae_loss: 0.05388553\n",
      "Step: [4135] total_loss: 2.12706184 d_loss: 1.37869966, g_loss: 0.69643617, ae_loss: 0.05192591\n",
      "Step: [4136] total_loss: 2.13373041 d_loss: 1.38650954, g_loss: 0.69882423, ae_loss: 0.04839662\n",
      "Step: [4137] total_loss: 2.12629604 d_loss: 1.37617218, g_loss: 0.69997251, ae_loss: 0.05015118\n",
      "Step: [4138] total_loss: 2.12108898 d_loss: 1.39964342, g_loss: 0.66992736, ae_loss: 0.05151821\n",
      "Step: [4139] total_loss: 2.12242413 d_loss: 1.38256729, g_loss: 0.68743366, ae_loss: 0.05242322\n",
      "Step: [4140] total_loss: 2.11733770 d_loss: 1.37424707, g_loss: 0.69548827, ae_loss: 0.04760236\n",
      "Step: [4141] total_loss: 2.12225008 d_loss: 1.38628435, g_loss: 0.68388695, ae_loss: 0.05207885\n",
      "Step: [4142] total_loss: 2.12849808 d_loss: 1.38137555, g_loss: 0.69908738, ae_loss: 0.04803505\n",
      "Step: [4143] total_loss: 2.12884521 d_loss: 1.37891185, g_loss: 0.69987452, ae_loss: 0.05005873\n",
      "Step: [4144] total_loss: 2.11940193 d_loss: 1.38330722, g_loss: 0.68678856, ae_loss: 0.04930601\n",
      "Step: [4145] total_loss: 2.11740732 d_loss: 1.38618040, g_loss: 0.68058228, ae_loss: 0.05064473\n",
      "Step: [4146] total_loss: 2.13296533 d_loss: 1.37763751, g_loss: 0.70608407, ae_loss: 0.04924371\n",
      "Step: [4147] total_loss: 2.13362598 d_loss: 1.38441038, g_loss: 0.69660360, ae_loss: 0.05261203\n",
      "Step: [4148] total_loss: 2.10629916 d_loss: 1.36649776, g_loss: 0.68983018, ae_loss: 0.04997127\n",
      "Step: [4149] total_loss: 2.12308717 d_loss: 1.36741614, g_loss: 0.70562291, ae_loss: 0.05004810\n",
      "Step: [4150] total_loss: 2.11827612 d_loss: 1.37857962, g_loss: 0.68746388, ae_loss: 0.05223280\n",
      "Step: [4151] total_loss: 2.13175297 d_loss: 1.39722598, g_loss: 0.68423665, ae_loss: 0.05029025\n",
      "Step: [4152] total_loss: 2.12555552 d_loss: 1.38601255, g_loss: 0.68865585, ae_loss: 0.05088722\n",
      "Step: [4153] total_loss: 2.13609648 d_loss: 1.38916910, g_loss: 0.69574606, ae_loss: 0.05118122\n",
      "Step: [4154] total_loss: 2.11130905 d_loss: 1.37861443, g_loss: 0.68312919, ae_loss: 0.04956528\n",
      "Step: [4155] total_loss: 2.14967442 d_loss: 1.38685870, g_loss: 0.71068537, ae_loss: 0.05213030\n",
      "Step: [4156] total_loss: 2.11845350 d_loss: 1.36288524, g_loss: 0.70606416, ae_loss: 0.04950401\n",
      "Step: [4157] total_loss: 2.13470006 d_loss: 1.39882374, g_loss: 0.67932999, ae_loss: 0.05654627\n",
      "Step: [4158] total_loss: 2.13201952 d_loss: 1.39776492, g_loss: 0.68551028, ae_loss: 0.04874434\n",
      "Step: [4159] total_loss: 2.11129045 d_loss: 1.36803186, g_loss: 0.68924904, ae_loss: 0.05400961\n",
      "Step: [4160] total_loss: 2.11716461 d_loss: 1.37244523, g_loss: 0.69393873, ae_loss: 0.05078074\n",
      "Step: [4161] total_loss: 2.14186716 d_loss: 1.40005517, g_loss: 0.69237554, ae_loss: 0.04943629\n",
      "Step: [4162] total_loss: 2.11908245 d_loss: 1.37764168, g_loss: 0.68684638, ae_loss: 0.05459423\n",
      "Step: [4163] total_loss: 2.14598989 d_loss: 1.40444613, g_loss: 0.68881667, ae_loss: 0.05272725\n",
      "Step: [4164] total_loss: 2.12280560 d_loss: 1.37824202, g_loss: 0.69751942, ae_loss: 0.04704423\n",
      "Step: [4165] total_loss: 2.12960124 d_loss: 1.35750365, g_loss: 0.71622157, ae_loss: 0.05587604\n",
      "Step: [4166] total_loss: 2.10386062 d_loss: 1.36464906, g_loss: 0.68461508, ae_loss: 0.05459644\n",
      "Step: [4167] total_loss: 2.14408112 d_loss: 1.40039575, g_loss: 0.68648851, ae_loss: 0.05719668\n",
      "Step: [4168] total_loss: 2.10914373 d_loss: 1.36779094, g_loss: 0.69274074, ae_loss: 0.04861195\n",
      "Step: [4169] total_loss: 2.12065935 d_loss: 1.38579023, g_loss: 0.68512321, ae_loss: 0.04974590\n",
      "Step: [4170] total_loss: 2.10933566 d_loss: 1.37681651, g_loss: 0.68150866, ae_loss: 0.05101049\n",
      "Step: [4171] total_loss: 2.13086796 d_loss: 1.39338303, g_loss: 0.68898082, ae_loss: 0.04850417\n",
      "Step: [4172] total_loss: 2.10728836 d_loss: 1.36898983, g_loss: 0.68482208, ae_loss: 0.05347645\n",
      "Step: [4173] total_loss: 2.13905072 d_loss: 1.40795255, g_loss: 0.67486930, ae_loss: 0.05622893\n",
      "Step: [4174] total_loss: 2.09797001 d_loss: 1.35748589, g_loss: 0.68756294, ae_loss: 0.05292114\n",
      "Step: [4175] total_loss: 2.13621092 d_loss: 1.39242089, g_loss: 0.69778311, ae_loss: 0.04600684\n",
      "Step: [4176] total_loss: 2.12481070 d_loss: 1.37638986, g_loss: 0.69846022, ae_loss: 0.04996055\n",
      "Step: [4177] total_loss: 2.17072105 d_loss: 1.40499568, g_loss: 0.71401536, ae_loss: 0.05171014\n",
      "Step: [4178] total_loss: 2.14217734 d_loss: 1.39124393, g_loss: 0.69780868, ae_loss: 0.05312468\n",
      "Step: [4179] total_loss: 2.13826084 d_loss: 1.38065004, g_loss: 0.70763636, ae_loss: 0.04997436\n",
      "Step: [4180] total_loss: 2.12163353 d_loss: 1.37070680, g_loss: 0.70115250, ae_loss: 0.04977432\n",
      "Step: [4181] total_loss: 2.09641552 d_loss: 1.35012221, g_loss: 0.69469202, ae_loss: 0.05160120\n",
      "Step: [4182] total_loss: 2.11909056 d_loss: 1.37156558, g_loss: 0.69781339, ae_loss: 0.04971161\n",
      "Step: [4183] total_loss: 2.13439870 d_loss: 1.38255000, g_loss: 0.69957483, ae_loss: 0.05227387\n",
      "Step: [4184] total_loss: 2.10104799 d_loss: 1.35476613, g_loss: 0.70196998, ae_loss: 0.04431186\n",
      "Step: [4185] total_loss: 2.11035538 d_loss: 1.36414623, g_loss: 0.69743252, ae_loss: 0.04877654\n",
      "Step: [4186] total_loss: 2.13559699 d_loss: 1.39747071, g_loss: 0.68585932, ae_loss: 0.05226700\n",
      "Step: [4187] total_loss: 2.13539505 d_loss: 1.40691233, g_loss: 0.67477989, ae_loss: 0.05370289\n",
      "Step: [4188] total_loss: 2.10371375 d_loss: 1.35010707, g_loss: 0.70091844, ae_loss: 0.05268827\n",
      "Step: [4189] total_loss: 2.11808968 d_loss: 1.36276758, g_loss: 0.70049667, ae_loss: 0.05482557\n",
      "Step: [4190] total_loss: 2.14104271 d_loss: 1.40464771, g_loss: 0.68707758, ae_loss: 0.04931737\n",
      "Step: [4191] total_loss: 2.14073610 d_loss: 1.39055586, g_loss: 0.69989979, ae_loss: 0.05028042\n",
      "Step: [4192] total_loss: 2.14448524 d_loss: 1.39701974, g_loss: 0.69508797, ae_loss: 0.05237761\n",
      "Step: [4193] total_loss: 2.14363289 d_loss: 1.37030184, g_loss: 0.72254813, ae_loss: 0.05078308\n",
      "Step: [4194] total_loss: 2.15738750 d_loss: 1.41416168, g_loss: 0.68927306, ae_loss: 0.05395268\n",
      "Step: [4195] total_loss: 2.13943672 d_loss: 1.38118315, g_loss: 0.70385170, ae_loss: 0.05440186\n",
      "Step: [4196] total_loss: 2.12411666 d_loss: 1.36890757, g_loss: 0.70137453, ae_loss: 0.05383460\n",
      "Step: [4197] total_loss: 2.12150908 d_loss: 1.36639237, g_loss: 0.70511281, ae_loss: 0.05000394\n",
      "Step: [4198] total_loss: 2.11951113 d_loss: 1.38424945, g_loss: 0.68670791, ae_loss: 0.04855376\n",
      "Step: [4199] total_loss: 2.12328458 d_loss: 1.38474357, g_loss: 0.68689781, ae_loss: 0.05164330\n",
      "Step: [4200] total_loss: 2.14216423 d_loss: 1.37002790, g_loss: 0.72121739, ae_loss: 0.05091900\n",
      "Step: [4201] total_loss: 2.12763715 d_loss: 1.37400627, g_loss: 0.69924235, ae_loss: 0.05438855\n",
      "Step: [4202] total_loss: 2.15628767 d_loss: 1.40076113, g_loss: 0.69949448, ae_loss: 0.05603199\n",
      "Step: [4203] total_loss: 2.14720702 d_loss: 1.39161062, g_loss: 0.70480669, ae_loss: 0.05078968\n",
      "Step: [4204] total_loss: 2.10950637 d_loss: 1.36634982, g_loss: 0.69506568, ae_loss: 0.04809083\n",
      "Step: [4205] total_loss: 2.13767672 d_loss: 1.36215627, g_loss: 0.72356188, ae_loss: 0.05195850\n",
      "Step: [4206] total_loss: 2.10205650 d_loss: 1.36866271, g_loss: 0.67725474, ae_loss: 0.05613913\n",
      "Step: [4207] total_loss: 2.13159132 d_loss: 1.40352750, g_loss: 0.67667258, ae_loss: 0.05139116\n",
      "Step: [4208] total_loss: 2.11375618 d_loss: 1.37047124, g_loss: 0.69166315, ae_loss: 0.05162172\n",
      "Step: [4209] total_loss: 2.12158442 d_loss: 1.36680150, g_loss: 0.70350248, ae_loss: 0.05128053\n",
      "Step: [4210] total_loss: 2.13885641 d_loss: 1.39643383, g_loss: 0.68878424, ae_loss: 0.05363841\n",
      "Step: [4211] total_loss: 2.14415216 d_loss: 1.39183187, g_loss: 0.69619304, ae_loss: 0.05612719\n",
      "Step: [4212] total_loss: 2.13359022 d_loss: 1.39569688, g_loss: 0.69014370, ae_loss: 0.04774947\n",
      "Step: [4213] total_loss: 2.12867928 d_loss: 1.39396262, g_loss: 0.68603849, ae_loss: 0.04867825\n",
      "Step: [4214] total_loss: 2.12872720 d_loss: 1.36975050, g_loss: 0.70383650, ae_loss: 0.05514009\n",
      "Step: [4215] total_loss: 2.10864305 d_loss: 1.37117088, g_loss: 0.69262850, ae_loss: 0.04484351\n",
      "Step: [4216] total_loss: 2.10331678 d_loss: 1.36109161, g_loss: 0.68838799, ae_loss: 0.05383730\n",
      "Step: [4217] total_loss: 2.12001276 d_loss: 1.38505244, g_loss: 0.68600345, ae_loss: 0.04895681\n",
      "Step: [4218] total_loss: 2.10447025 d_loss: 1.37172174, g_loss: 0.68049461, ae_loss: 0.05225396\n",
      "Step: [4219] total_loss: 2.12392855 d_loss: 1.38121355, g_loss: 0.69023514, ae_loss: 0.05247996\n",
      "Step: [4220] total_loss: 2.11724901 d_loss: 1.37389159, g_loss: 0.69527662, ae_loss: 0.04808085\n",
      "Step: [4221] total_loss: 2.14056897 d_loss: 1.38453591, g_loss: 0.70199442, ae_loss: 0.05403862\n",
      "Step: [4222] total_loss: 2.11501408 d_loss: 1.34551752, g_loss: 0.71848089, ae_loss: 0.05101568\n",
      "Step: [4223] total_loss: 2.11928225 d_loss: 1.38234913, g_loss: 0.68544561, ae_loss: 0.05148762\n",
      "Step: [4224] total_loss: 2.12625551 d_loss: 1.37723660, g_loss: 0.70218050, ae_loss: 0.04683853\n",
      "Step: [4225] total_loss: 2.11555791 d_loss: 1.40547109, g_loss: 0.65614986, ae_loss: 0.05393693\n",
      "Step: [4226] total_loss: 2.12419987 d_loss: 1.39676285, g_loss: 0.67928779, ae_loss: 0.04814909\n",
      "Step: [4227] total_loss: 2.11786699 d_loss: 1.38001657, g_loss: 0.68961823, ae_loss: 0.04823221\n",
      "Step: [4228] total_loss: 2.12554598 d_loss: 1.37999082, g_loss: 0.69414413, ae_loss: 0.05141116\n",
      "Step: [4229] total_loss: 2.12312007 d_loss: 1.37433410, g_loss: 0.69915920, ae_loss: 0.04962681\n",
      "Step: [4230] total_loss: 2.13847589 d_loss: 1.39632797, g_loss: 0.69159794, ae_loss: 0.05055006\n",
      "Step: [4231] total_loss: 2.11341739 d_loss: 1.36207819, g_loss: 0.70212513, ae_loss: 0.04921404\n",
      "Step: [4232] total_loss: 2.11587858 d_loss: 1.37317395, g_loss: 0.69284117, ae_loss: 0.04986347\n",
      "Step: [4233] total_loss: 2.13254070 d_loss: 1.38303149, g_loss: 0.70170009, ae_loss: 0.04780896\n",
      "Step: [4234] total_loss: 2.10985756 d_loss: 1.37523353, g_loss: 0.68038857, ae_loss: 0.05423548\n",
      "Step: [4235] total_loss: 2.13328719 d_loss: 1.39519429, g_loss: 0.68423545, ae_loss: 0.05385743\n",
      "Step: [4236] total_loss: 2.14203382 d_loss: 1.38610649, g_loss: 0.70339537, ae_loss: 0.05253191\n",
      "Step: [4237] total_loss: 2.13045382 d_loss: 1.37607026, g_loss: 0.70175833, ae_loss: 0.05262531\n",
      "Step: [4238] total_loss: 2.12680125 d_loss: 1.38564849, g_loss: 0.69105607, ae_loss: 0.05009674\n",
      "Step: [4239] total_loss: 2.12817287 d_loss: 1.38334537, g_loss: 0.69130707, ae_loss: 0.05352031\n",
      "Step: [4240] total_loss: 2.12454748 d_loss: 1.37085748, g_loss: 0.70161808, ae_loss: 0.05207192\n",
      "Step: [4241] total_loss: 2.10744476 d_loss: 1.36981654, g_loss: 0.68762064, ae_loss: 0.05000750\n",
      "Step: [4242] total_loss: 2.11628294 d_loss: 1.37523544, g_loss: 0.68950719, ae_loss: 0.05154020\n",
      "Step: [4243] total_loss: 2.11841822 d_loss: 1.37495303, g_loss: 0.69308329, ae_loss: 0.05038193\n",
      "Step: [4244] total_loss: 2.13249850 d_loss: 1.38461709, g_loss: 0.69593924, ae_loss: 0.05194210\n",
      "Step: [4245] total_loss: 2.09477758 d_loss: 1.34061456, g_loss: 0.70380133, ae_loss: 0.05036173\n",
      "Step: [4246] total_loss: 2.11388373 d_loss: 1.38082504, g_loss: 0.68125474, ae_loss: 0.05180389\n",
      "Step: [4247] total_loss: 2.13873005 d_loss: 1.39196026, g_loss: 0.69814092, ae_loss: 0.04862887\n",
      "Step: [4248] total_loss: 2.12928820 d_loss: 1.36527765, g_loss: 0.71017110, ae_loss: 0.05383942\n",
      "Step: [4249] total_loss: 2.14005256 d_loss: 1.38608336, g_loss: 0.70101130, ae_loss: 0.05295787\n",
      "Step: [4250] total_loss: 2.11010456 d_loss: 1.37201262, g_loss: 0.68812478, ae_loss: 0.04996724\n",
      "Step: [4251] total_loss: 2.12517405 d_loss: 1.38959932, g_loss: 0.68207586, ae_loss: 0.05349874\n",
      "Step: [4252] total_loss: 2.15944839 d_loss: 1.41256046, g_loss: 0.69553316, ae_loss: 0.05135482\n",
      "Step: [4253] total_loss: 2.13425684 d_loss: 1.37251890, g_loss: 0.71145046, ae_loss: 0.05028742\n",
      "Step: [4254] total_loss: 2.12039804 d_loss: 1.38126755, g_loss: 0.68809223, ae_loss: 0.05103838\n",
      "Step: [4255] total_loss: 2.12554789 d_loss: 1.36871743, g_loss: 0.70699501, ae_loss: 0.04983528\n",
      "Step: [4256] total_loss: 2.14727187 d_loss: 1.40701962, g_loss: 0.68930268, ae_loss: 0.05094963\n",
      "Step: [4257] total_loss: 2.13894176 d_loss: 1.37948442, g_loss: 0.70078421, ae_loss: 0.05867329\n",
      "Step: [4258] total_loss: 2.13279486 d_loss: 1.37329996, g_loss: 0.71003443, ae_loss: 0.04946043\n",
      "Step: [4259] total_loss: 2.10229349 d_loss: 1.36124682, g_loss: 0.69386703, ae_loss: 0.04717955\n",
      "Step: [4260] total_loss: 2.11979866 d_loss: 1.38767505, g_loss: 0.68223888, ae_loss: 0.04988481\n",
      "Step: [4261] total_loss: 2.11158586 d_loss: 1.38080704, g_loss: 0.68342280, ae_loss: 0.04735598\n",
      "Step: [4262] total_loss: 2.12186146 d_loss: 1.37973809, g_loss: 0.69062841, ae_loss: 0.05149495\n",
      "Step: [4263] total_loss: 2.12040401 d_loss: 1.39691532, g_loss: 0.67169428, ae_loss: 0.05179439\n",
      "Step: [4264] total_loss: 2.13463068 d_loss: 1.38019228, g_loss: 0.70582342, ae_loss: 0.04861501\n",
      "Step: [4265] total_loss: 2.13733673 d_loss: 1.38142848, g_loss: 0.70156592, ae_loss: 0.05434224\n",
      "Step: [4266] total_loss: 2.12675261 d_loss: 1.36847651, g_loss: 0.70921361, ae_loss: 0.04906249\n",
      "Step: [4267] total_loss: 2.10050178 d_loss: 1.34787655, g_loss: 0.70292783, ae_loss: 0.04969735\n",
      "Step: [4268] total_loss: 2.12147546 d_loss: 1.37615705, g_loss: 0.68790925, ae_loss: 0.05740915\n",
      "Step: [4269] total_loss: 2.13540792 d_loss: 1.35844719, g_loss: 0.72574502, ae_loss: 0.05121562\n",
      "Step: [4270] total_loss: 2.13369799 d_loss: 1.38673532, g_loss: 0.69479108, ae_loss: 0.05217153\n",
      "Step: [4271] total_loss: 2.13050151 d_loss: 1.38435769, g_loss: 0.69590503, ae_loss: 0.05023868\n",
      "Step: [4272] total_loss: 2.13634825 d_loss: 1.38476682, g_loss: 0.70475990, ae_loss: 0.04682154\n",
      "Step: [4273] total_loss: 2.11217475 d_loss: 1.37952769, g_loss: 0.68208760, ae_loss: 0.05055938\n",
      "Step: [4274] total_loss: 2.12237835 d_loss: 1.35929060, g_loss: 0.70755720, ae_loss: 0.05553070\n",
      "Step: [4275] total_loss: 2.09733772 d_loss: 1.36175966, g_loss: 0.68465793, ae_loss: 0.05092030\n",
      "Step: [4276] total_loss: 2.10996962 d_loss: 1.37737012, g_loss: 0.68467367, ae_loss: 0.04792581\n",
      "Step: [4277] total_loss: 2.15842772 d_loss: 1.38108277, g_loss: 0.72838485, ae_loss: 0.04895993\n",
      "Step: [4278] total_loss: 2.11439466 d_loss: 1.36287463, g_loss: 0.70176566, ae_loss: 0.04975445\n",
      "Step: [4279] total_loss: 2.11343575 d_loss: 1.39245200, g_loss: 0.67220604, ae_loss: 0.04877772\n",
      "Step: [4280] total_loss: 2.13530469 d_loss: 1.38329434, g_loss: 0.70188463, ae_loss: 0.05012574\n",
      "Step: [4281] total_loss: 2.11454058 d_loss: 1.36836278, g_loss: 0.69526517, ae_loss: 0.05091259\n",
      "Step: [4282] total_loss: 2.11108422 d_loss: 1.38290119, g_loss: 0.67763537, ae_loss: 0.05054773\n",
      "Step: [4283] total_loss: 2.10216665 d_loss: 1.35470426, g_loss: 0.70187235, ae_loss: 0.04559016\n",
      "Step: [4284] total_loss: 2.13132691 d_loss: 1.39048243, g_loss: 0.69120824, ae_loss: 0.04963625\n",
      "Step: [4285] total_loss: 2.14836407 d_loss: 1.38659811, g_loss: 0.70747113, ae_loss: 0.05429482\n",
      "Step: [4286] total_loss: 2.13952017 d_loss: 1.38499212, g_loss: 0.69986618, ae_loss: 0.05466203\n",
      "Step: [4287] total_loss: 2.13984203 d_loss: 1.38469863, g_loss: 0.70506799, ae_loss: 0.05007556\n",
      "Step: [4288] total_loss: 2.11642647 d_loss: 1.37954855, g_loss: 0.68652725, ae_loss: 0.05035076\n",
      "Step: [4289] total_loss: 2.13019228 d_loss: 1.37550604, g_loss: 0.70380187, ae_loss: 0.05088437\n",
      "Step: [4290] total_loss: 2.10488582 d_loss: 1.36819470, g_loss: 0.68646967, ae_loss: 0.05022143\n",
      "Step: [4291] total_loss: 2.13613820 d_loss: 1.39761162, g_loss: 0.69044566, ae_loss: 0.04808092\n",
      "Step: [4292] total_loss: 2.12159753 d_loss: 1.37232399, g_loss: 0.69554961, ae_loss: 0.05372396\n",
      "Step: [4293] total_loss: 2.12279248 d_loss: 1.36797535, g_loss: 0.70327395, ae_loss: 0.05154322\n",
      "Step: [4294] total_loss: 2.11547089 d_loss: 1.37370896, g_loss: 0.69195092, ae_loss: 0.04981107\n",
      "Step: [4295] total_loss: 2.12391925 d_loss: 1.38223469, g_loss: 0.68911672, ae_loss: 0.05256786\n",
      "Step: [4296] total_loss: 2.15791392 d_loss: 1.36573577, g_loss: 0.73856026, ae_loss: 0.05361788\n",
      "Step: [4297] total_loss: 2.13353729 d_loss: 1.37536192, g_loss: 0.70489085, ae_loss: 0.05328443\n",
      "Step: [4298] total_loss: 2.13281107 d_loss: 1.38832438, g_loss: 0.69697374, ae_loss: 0.04751300\n",
      "Step: [4299] total_loss: 2.13923931 d_loss: 1.39570093, g_loss: 0.69022697, ae_loss: 0.05331146\n",
      "Step: [4300] total_loss: 2.12286806 d_loss: 1.37038934, g_loss: 0.69709110, ae_loss: 0.05538762\n",
      "Step: [4301] total_loss: 2.16220045 d_loss: 1.39581823, g_loss: 0.71524811, ae_loss: 0.05113428\n",
      "Step: [4302] total_loss: 2.11513042 d_loss: 1.38684940, g_loss: 0.67575371, ae_loss: 0.05252730\n",
      "Step: [4303] total_loss: 2.13802123 d_loss: 1.40704882, g_loss: 0.67778075, ae_loss: 0.05319166\n",
      "Step: [4304] total_loss: 2.13191652 d_loss: 1.40616727, g_loss: 0.67496645, ae_loss: 0.05078272\n",
      "Step: [4305] total_loss: 2.12281370 d_loss: 1.39464700, g_loss: 0.68089855, ae_loss: 0.04726816\n",
      "Step: [4306] total_loss: 2.11889935 d_loss: 1.39469528, g_loss: 0.67695171, ae_loss: 0.04725232\n",
      "Step: [4307] total_loss: 2.12957716 d_loss: 1.39642978, g_loss: 0.68321788, ae_loss: 0.04992946\n",
      "Step: [4308] total_loss: 2.12902379 d_loss: 1.38083386, g_loss: 0.69929492, ae_loss: 0.04889499\n",
      "Step: [4309] total_loss: 2.11833191 d_loss: 1.38694072, g_loss: 0.68133056, ae_loss: 0.05006060\n",
      "Step: [4310] total_loss: 2.14392138 d_loss: 1.39273095, g_loss: 0.69902992, ae_loss: 0.05216059\n",
      "Step: [4311] total_loss: 2.11726618 d_loss: 1.37254953, g_loss: 0.69348276, ae_loss: 0.05123371\n",
      "Step: [4312] total_loss: 2.16522598 d_loss: 1.39943886, g_loss: 0.71497965, ae_loss: 0.05080737\n",
      "Step: [4313] total_loss: 2.12620831 d_loss: 1.38700330, g_loss: 0.68767214, ae_loss: 0.05153301\n",
      "Step: [4314] total_loss: 2.13071704 d_loss: 1.37842536, g_loss: 0.70174366, ae_loss: 0.05054802\n",
      "Step: [4315] total_loss: 2.11773658 d_loss: 1.38320315, g_loss: 0.68307459, ae_loss: 0.05145887\n",
      "Step: [4316] total_loss: 2.12997961 d_loss: 1.39102077, g_loss: 0.68212700, ae_loss: 0.05683191\n",
      "Step: [4317] total_loss: 2.14011240 d_loss: 1.38566995, g_loss: 0.70138472, ae_loss: 0.05305764\n",
      "Step: [4318] total_loss: 2.10872126 d_loss: 1.37123823, g_loss: 0.68800914, ae_loss: 0.04947385\n",
      "Step: [4319] total_loss: 2.08730173 d_loss: 1.36127210, g_loss: 0.67417902, ae_loss: 0.05185056\n",
      "Step: [4320] total_loss: 2.11304855 d_loss: 1.37702513, g_loss: 0.68423951, ae_loss: 0.05178398\n",
      "Step: [4321] total_loss: 2.14388132 d_loss: 1.40140891, g_loss: 0.69381368, ae_loss: 0.04865879\n",
      "Step: [4322] total_loss: 2.11149836 d_loss: 1.36820972, g_loss: 0.69404757, ae_loss: 0.04924124\n",
      "Step: [4323] total_loss: 2.12004805 d_loss: 1.38577747, g_loss: 0.68202722, ae_loss: 0.05224343\n",
      "Step: [4324] total_loss: 2.12043166 d_loss: 1.37512851, g_loss: 0.69355154, ae_loss: 0.05175167\n",
      "Step: [4325] total_loss: 2.12704849 d_loss: 1.38080788, g_loss: 0.69641668, ae_loss: 0.04982390\n",
      "Step: [4326] total_loss: 2.10213518 d_loss: 1.35787570, g_loss: 0.69671285, ae_loss: 0.04754653\n",
      "Step: [4327] total_loss: 2.10487437 d_loss: 1.37387764, g_loss: 0.68534791, ae_loss: 0.04564886\n",
      "Step: [4328] total_loss: 2.12886715 d_loss: 1.39642799, g_loss: 0.68617404, ae_loss: 0.04626511\n",
      "Step: [4329] total_loss: 2.10497284 d_loss: 1.37253034, g_loss: 0.68177897, ae_loss: 0.05066345\n",
      "Step: [4330] total_loss: 2.12021041 d_loss: 1.39381647, g_loss: 0.67407936, ae_loss: 0.05231451\n",
      "Step: [4331] total_loss: 2.11325002 d_loss: 1.37180877, g_loss: 0.68989146, ae_loss: 0.05154980\n",
      "Step: [4332] total_loss: 2.14290047 d_loss: 1.41142344, g_loss: 0.67869145, ae_loss: 0.05278567\n",
      "Step: [4333] total_loss: 2.11956000 d_loss: 1.38463283, g_loss: 0.68638277, ae_loss: 0.04854440\n",
      "Step: [4334] total_loss: 2.10908532 d_loss: 1.36888790, g_loss: 0.68753392, ae_loss: 0.05266353\n",
      "Step: [4335] total_loss: 2.14567375 d_loss: 1.40474701, g_loss: 0.68898845, ae_loss: 0.05193847\n",
      "Step: [4336] total_loss: 2.12941790 d_loss: 1.39156151, g_loss: 0.68606102, ae_loss: 0.05179539\n",
      "Step: [4337] total_loss: 2.12597799 d_loss: 1.38595462, g_loss: 0.68815768, ae_loss: 0.05186578\n",
      "Step: [4338] total_loss: 2.13688278 d_loss: 1.38224292, g_loss: 0.70164496, ae_loss: 0.05299482\n",
      "Step: [4339] total_loss: 2.12877107 d_loss: 1.38953352, g_loss: 0.68713963, ae_loss: 0.05209789\n",
      "Step: [4340] total_loss: 2.13995957 d_loss: 1.40158713, g_loss: 0.68695939, ae_loss: 0.05141306\n",
      "Step: [4341] total_loss: 2.11921573 d_loss: 1.37329102, g_loss: 0.69486791, ae_loss: 0.05105672\n",
      "Step: [4342] total_loss: 2.13976717 d_loss: 1.38795245, g_loss: 0.70274103, ae_loss: 0.04907363\n",
      "Step: [4343] total_loss: 2.13961411 d_loss: 1.39544332, g_loss: 0.69435346, ae_loss: 0.04981731\n",
      "Step: [4344] total_loss: 2.16453838 d_loss: 1.42104864, g_loss: 0.68829179, ae_loss: 0.05519781\n",
      "Step: [4345] total_loss: 2.13386512 d_loss: 1.38222432, g_loss: 0.69970441, ae_loss: 0.05193637\n",
      "Step: [4346] total_loss: 2.12089324 d_loss: 1.37795019, g_loss: 0.69617540, ae_loss: 0.04676771\n",
      "Step: [4347] total_loss: 2.14572716 d_loss: 1.40661967, g_loss: 0.68749809, ae_loss: 0.05160956\n",
      "Step: [4348] total_loss: 2.11627483 d_loss: 1.38458347, g_loss: 0.68215430, ae_loss: 0.04953711\n",
      "Step: [4349] total_loss: 2.14878106 d_loss: 1.38706875, g_loss: 0.70737457, ae_loss: 0.05433774\n",
      "Step: [4350] total_loss: 2.11867714 d_loss: 1.37325406, g_loss: 0.69200218, ae_loss: 0.05342103\n",
      "Step: [4351] total_loss: 2.12169552 d_loss: 1.37079799, g_loss: 0.70435637, ae_loss: 0.04654104\n",
      "Step: [4352] total_loss: 2.13124728 d_loss: 1.38826478, g_loss: 0.69289249, ae_loss: 0.05009007\n",
      "Step: [4353] total_loss: 2.13340330 d_loss: 1.36803424, g_loss: 0.71726263, ae_loss: 0.04810640\n",
      "Step: [4354] total_loss: 2.11219358 d_loss: 1.36600137, g_loss: 0.69310236, ae_loss: 0.05308990\n",
      "Step: [4355] total_loss: 2.12041426 d_loss: 1.39618671, g_loss: 0.67328727, ae_loss: 0.05094014\n",
      "Step: [4356] total_loss: 2.13501453 d_loss: 1.39459395, g_loss: 0.69185781, ae_loss: 0.04856281\n",
      "Step: [4357] total_loss: 2.13068700 d_loss: 1.38042855, g_loss: 0.69556189, ae_loss: 0.05469652\n",
      "Step: [4358] total_loss: 2.11554193 d_loss: 1.37519705, g_loss: 0.69129455, ae_loss: 0.04905035\n",
      "Step: [4359] total_loss: 2.11434770 d_loss: 1.37144566, g_loss: 0.69609243, ae_loss: 0.04680964\n",
      "Step: [4360] total_loss: 2.12337947 d_loss: 1.36870027, g_loss: 0.70457906, ae_loss: 0.05010020\n",
      "Step: [4361] total_loss: 2.11677003 d_loss: 1.38608360, g_loss: 0.67879242, ae_loss: 0.05189408\n",
      "Step: [4362] total_loss: 2.11080265 d_loss: 1.36476374, g_loss: 0.69812232, ae_loss: 0.04791652\n",
      "Step: [4363] total_loss: 2.12330675 d_loss: 1.38614368, g_loss: 0.68511909, ae_loss: 0.05204402\n",
      "Step: [4364] total_loss: 2.12170362 d_loss: 1.39058042, g_loss: 0.68102753, ae_loss: 0.05009555\n",
      "Step: [4365] total_loss: 2.13296032 d_loss: 1.37627780, g_loss: 0.70853853, ae_loss: 0.04814389\n",
      "Step: [4366] total_loss: 2.12524557 d_loss: 1.39604187, g_loss: 0.68280762, ae_loss: 0.04639615\n",
      "Step: [4367] total_loss: 2.12316418 d_loss: 1.37425101, g_loss: 0.69473243, ae_loss: 0.05418061\n",
      "Step: [4368] total_loss: 2.12356639 d_loss: 1.37136269, g_loss: 0.70058084, ae_loss: 0.05162290\n",
      "Step: [4369] total_loss: 2.10848355 d_loss: 1.35951281, g_loss: 0.70017445, ae_loss: 0.04879631\n",
      "Step: [4370] total_loss: 2.12625837 d_loss: 1.38011515, g_loss: 0.69967031, ae_loss: 0.04647275\n",
      "Step: [4371] total_loss: 2.13266730 d_loss: 1.36326575, g_loss: 0.72142130, ae_loss: 0.04798032\n",
      "Step: [4372] total_loss: 2.10421968 d_loss: 1.36895561, g_loss: 0.68411970, ae_loss: 0.05114438\n",
      "Step: [4373] total_loss: 2.11069131 d_loss: 1.38988137, g_loss: 0.66754925, ae_loss: 0.05326074\n",
      "Step: [4374] total_loss: 2.13184857 d_loss: 1.39313078, g_loss: 0.68738186, ae_loss: 0.05133592\n",
      "Step: [4375] total_loss: 2.09964752 d_loss: 1.36908126, g_loss: 0.67794991, ae_loss: 0.05261646\n",
      "Step: [4376] total_loss: 2.13159156 d_loss: 1.37804973, g_loss: 0.70226872, ae_loss: 0.05127306\n",
      "Step: [4377] total_loss: 2.11042690 d_loss: 1.37601888, g_loss: 0.68305635, ae_loss: 0.05135161\n",
      "Step: [4378] total_loss: 2.12264490 d_loss: 1.38489103, g_loss: 0.69188142, ae_loss: 0.04587262\n",
      "Step: [4379] total_loss: 2.12864089 d_loss: 1.38804746, g_loss: 0.68888891, ae_loss: 0.05170455\n",
      "Step: [4380] total_loss: 2.12916803 d_loss: 1.37521791, g_loss: 0.70367897, ae_loss: 0.05027110\n",
      "Step: [4381] total_loss: 2.13359880 d_loss: 1.40854180, g_loss: 0.67605758, ae_loss: 0.04899952\n",
      "Step: [4382] total_loss: 2.13859081 d_loss: 1.40638804, g_loss: 0.68128932, ae_loss: 0.05091337\n",
      "Step: [4383] total_loss: 2.12196803 d_loss: 1.38047457, g_loss: 0.69007325, ae_loss: 0.05142027\n",
      "Step: [4384] total_loss: 2.10791874 d_loss: 1.35834599, g_loss: 0.69934261, ae_loss: 0.05023001\n",
      "Step: [4385] total_loss: 2.10754204 d_loss: 1.36045218, g_loss: 0.69618797, ae_loss: 0.05090183\n",
      "Step: [4386] total_loss: 2.11462450 d_loss: 1.37557197, g_loss: 0.69028485, ae_loss: 0.04876754\n",
      "Step: [4387] total_loss: 2.13045120 d_loss: 1.38545012, g_loss: 0.69332457, ae_loss: 0.05167666\n",
      "Step: [4388] total_loss: 2.14300728 d_loss: 1.40431798, g_loss: 0.68624973, ae_loss: 0.05243944\n",
      "Step: [4389] total_loss: 2.15353107 d_loss: 1.38558531, g_loss: 0.72039700, ae_loss: 0.04754873\n",
      "Step: [4390] total_loss: 2.15459538 d_loss: 1.39846802, g_loss: 0.70257276, ae_loss: 0.05355448\n",
      "Step: [4391] total_loss: 2.11736560 d_loss: 1.36890364, g_loss: 0.69820029, ae_loss: 0.05026156\n",
      "Step: [4392] total_loss: 2.11215448 d_loss: 1.36565471, g_loss: 0.69264323, ae_loss: 0.05385653\n",
      "Step: [4393] total_loss: 2.11650133 d_loss: 1.38219726, g_loss: 0.68202698, ae_loss: 0.05227719\n",
      "Step: [4394] total_loss: 2.12335896 d_loss: 1.37816322, g_loss: 0.68891954, ae_loss: 0.05627624\n",
      "Step: [4395] total_loss: 2.10166216 d_loss: 1.36514711, g_loss: 0.68470383, ae_loss: 0.05181138\n",
      "Step: [4396] total_loss: 2.11905527 d_loss: 1.37462258, g_loss: 0.69428122, ae_loss: 0.05015140\n",
      "Step: [4397] total_loss: 2.12467718 d_loss: 1.37953198, g_loss: 0.69420093, ae_loss: 0.05094424\n",
      "Step: [4398] total_loss: 2.09450555 d_loss: 1.36313224, g_loss: 0.68314505, ae_loss: 0.04822828\n",
      "Step: [4399] total_loss: 2.13983393 d_loss: 1.40924215, g_loss: 0.67572659, ae_loss: 0.05486516\n",
      "Step: [4400] total_loss: 2.11326742 d_loss: 1.36304545, g_loss: 0.70335460, ae_loss: 0.04686732\n",
      "Step: [4401] total_loss: 2.12324500 d_loss: 1.38053107, g_loss: 0.69360495, ae_loss: 0.04910894\n",
      "Step: [4402] total_loss: 2.12969899 d_loss: 1.36601329, g_loss: 0.71735281, ae_loss: 0.04633300\n",
      "Step: [4403] total_loss: 2.13814306 d_loss: 1.39498472, g_loss: 0.69249904, ae_loss: 0.05065934\n",
      "Step: [4404] total_loss: 2.11769485 d_loss: 1.37073803, g_loss: 0.69964194, ae_loss: 0.04731478\n",
      "Step: [4405] total_loss: 2.12249899 d_loss: 1.38181138, g_loss: 0.68898582, ae_loss: 0.05170191\n",
      "Step: [4406] total_loss: 2.14130330 d_loss: 1.39428282, g_loss: 0.69591016, ae_loss: 0.05111029\n",
      "Step: [4407] total_loss: 2.12510443 d_loss: 1.38751042, g_loss: 0.68868786, ae_loss: 0.04890620\n",
      "Step: [4408] total_loss: 2.12650442 d_loss: 1.39078999, g_loss: 0.68741065, ae_loss: 0.04830378\n",
      "Step: [4409] total_loss: 2.12862277 d_loss: 1.36814046, g_loss: 0.71609664, ae_loss: 0.04438563\n",
      "Step: [4410] total_loss: 2.12700081 d_loss: 1.38195205, g_loss: 0.69564968, ae_loss: 0.04939897\n",
      "Step: [4411] total_loss: 2.13410902 d_loss: 1.39824116, g_loss: 0.68321550, ae_loss: 0.05265244\n",
      "Step: [4412] total_loss: 2.13316464 d_loss: 1.38812947, g_loss: 0.69389760, ae_loss: 0.05113761\n",
      "Step: [4413] total_loss: 2.13632059 d_loss: 1.38761020, g_loss: 0.69967270, ae_loss: 0.04903777\n",
      "Step: [4414] total_loss: 2.12329292 d_loss: 1.37510800, g_loss: 0.69850194, ae_loss: 0.04968302\n",
      "Step: [4415] total_loss: 2.11970258 d_loss: 1.38380921, g_loss: 0.68228459, ae_loss: 0.05360872\n",
      "Step: [4416] total_loss: 2.12836123 d_loss: 1.38969350, g_loss: 0.68789613, ae_loss: 0.05077155\n",
      "Step: [4417] total_loss: 2.10655117 d_loss: 1.36517406, g_loss: 0.68523085, ae_loss: 0.05614635\n",
      "Step: [4418] total_loss: 2.12255192 d_loss: 1.38073587, g_loss: 0.68933797, ae_loss: 0.05247800\n",
      "Step: [4419] total_loss: 2.12753773 d_loss: 1.39637792, g_loss: 0.68069160, ae_loss: 0.05046820\n",
      "Step: [4420] total_loss: 2.12583780 d_loss: 1.38542342, g_loss: 0.68953550, ae_loss: 0.05087897\n",
      "Step: [4421] total_loss: 2.11557889 d_loss: 1.37226319, g_loss: 0.69142652, ae_loss: 0.05188913\n",
      "Step: [4422] total_loss: 2.12364841 d_loss: 1.37693143, g_loss: 0.70076537, ae_loss: 0.04595161\n",
      "Step: [4423] total_loss: 2.13964725 d_loss: 1.40835261, g_loss: 0.68054986, ae_loss: 0.05074474\n",
      "Step: [4424] total_loss: 2.13022852 d_loss: 1.38965607, g_loss: 0.68928218, ae_loss: 0.05129045\n",
      "Step: [4425] total_loss: 2.14159846 d_loss: 1.38131285, g_loss: 0.70834774, ae_loss: 0.05193793\n",
      "Step: [4426] total_loss: 2.13602066 d_loss: 1.39662313, g_loss: 0.68698275, ae_loss: 0.05241472\n",
      "Step: [4427] total_loss: 2.12016296 d_loss: 1.38371921, g_loss: 0.68363625, ae_loss: 0.05280755\n",
      "Step: [4428] total_loss: 2.11709642 d_loss: 1.38052773, g_loss: 0.68630695, ae_loss: 0.05026165\n",
      "Step: [4429] total_loss: 2.10978127 d_loss: 1.36947358, g_loss: 0.69030094, ae_loss: 0.05000670\n",
      "Step: [4430] total_loss: 2.11523962 d_loss: 1.38135993, g_loss: 0.68065476, ae_loss: 0.05322480\n",
      "Step: [4431] total_loss: 2.12141228 d_loss: 1.36583400, g_loss: 0.70162797, ae_loss: 0.05395041\n",
      "Step: [4432] total_loss: 2.14800692 d_loss: 1.40821874, g_loss: 0.69156557, ae_loss: 0.04822266\n",
      "Step: [4433] total_loss: 2.12690759 d_loss: 1.39873624, g_loss: 0.67787659, ae_loss: 0.05029472\n",
      "Step: [4434] total_loss: 2.11606383 d_loss: 1.37198412, g_loss: 0.69067609, ae_loss: 0.05340363\n",
      "Step: [4435] total_loss: 2.12169313 d_loss: 1.36840081, g_loss: 0.70224160, ae_loss: 0.05105077\n",
      "Step: [4436] total_loss: 2.13267612 d_loss: 1.37394333, g_loss: 0.70707607, ae_loss: 0.05165656\n",
      "Step: [4437] total_loss: 2.13000298 d_loss: 1.37775421, g_loss: 0.70128572, ae_loss: 0.05096292\n",
      "Step: [4438] total_loss: 2.10972977 d_loss: 1.37317204, g_loss: 0.69016135, ae_loss: 0.04639643\n",
      "Step: [4439] total_loss: 2.12176847 d_loss: 1.36746931, g_loss: 0.70079911, ae_loss: 0.05350010\n",
      "Step: [4440] total_loss: 2.12856627 d_loss: 1.39382076, g_loss: 0.68148839, ae_loss: 0.05325721\n",
      "Step: [4441] total_loss: 2.11843157 d_loss: 1.38273501, g_loss: 0.68782115, ae_loss: 0.04787550\n",
      "Step: [4442] total_loss: 2.14970922 d_loss: 1.39615619, g_loss: 0.69890696, ae_loss: 0.05464619\n",
      "Step: [4443] total_loss: 2.11570549 d_loss: 1.37848854, g_loss: 0.68142819, ae_loss: 0.05578867\n",
      "Step: [4444] total_loss: 2.13335109 d_loss: 1.38674605, g_loss: 0.69989127, ae_loss: 0.04671383\n",
      "Step: [4445] total_loss: 2.13283491 d_loss: 1.39689505, g_loss: 0.68534529, ae_loss: 0.05059444\n",
      "Step: [4446] total_loss: 2.13241506 d_loss: 1.38681257, g_loss: 0.69548231, ae_loss: 0.05012019\n",
      "Step: [4447] total_loss: 2.12654638 d_loss: 1.38576388, g_loss: 0.68887794, ae_loss: 0.05190443\n",
      "Step: [4448] total_loss: 2.13326955 d_loss: 1.37328315, g_loss: 0.70858771, ae_loss: 0.05139875\n",
      "Step: [4449] total_loss: 2.13804913 d_loss: 1.40409088, g_loss: 0.68043011, ae_loss: 0.05352813\n",
      "Step: [4450] total_loss: 2.12566853 d_loss: 1.37584138, g_loss: 0.69509971, ae_loss: 0.05472757\n",
      "Step: [4451] total_loss: 2.11581874 d_loss: 1.38419485, g_loss: 0.67465812, ae_loss: 0.05696579\n",
      "Step: [4452] total_loss: 2.11977434 d_loss: 1.39146233, g_loss: 0.67521310, ae_loss: 0.05309897\n",
      "Step: [4453] total_loss: 2.12707376 d_loss: 1.40077972, g_loss: 0.67593038, ae_loss: 0.05036368\n",
      "Step: [4454] total_loss: 2.12508583 d_loss: 1.38269699, g_loss: 0.69326901, ae_loss: 0.04911976\n",
      "Step: [4455] total_loss: 2.12859774 d_loss: 1.37632799, g_loss: 0.69943070, ae_loss: 0.05283897\n",
      "Step: [4456] total_loss: 2.12602377 d_loss: 1.36555290, g_loss: 0.70843822, ae_loss: 0.05203265\n",
      "Step: [4457] total_loss: 2.12547159 d_loss: 1.39234114, g_loss: 0.68206275, ae_loss: 0.05106784\n",
      "Step: [4458] total_loss: 2.11919641 d_loss: 1.37319696, g_loss: 0.69787276, ae_loss: 0.04812673\n",
      "Step: [4459] total_loss: 2.12589741 d_loss: 1.39859915, g_loss: 0.67837882, ae_loss: 0.04891953\n",
      "Step: [4460] total_loss: 2.12715626 d_loss: 1.38311529, g_loss: 0.69696820, ae_loss: 0.04707260\n",
      "Step: [4461] total_loss: 2.12690783 d_loss: 1.39233017, g_loss: 0.68539762, ae_loss: 0.04918019\n",
      "Step: [4462] total_loss: 2.11465287 d_loss: 1.36148667, g_loss: 0.70442915, ae_loss: 0.04873711\n",
      "Step: [4463] total_loss: 2.12084579 d_loss: 1.38601494, g_loss: 0.68491793, ae_loss: 0.04991301\n",
      "Step: [4464] total_loss: 2.12567568 d_loss: 1.38011241, g_loss: 0.69461471, ae_loss: 0.05094846\n",
      "Step: [4465] total_loss: 2.12434101 d_loss: 1.39410210, g_loss: 0.68386424, ae_loss: 0.04637483\n",
      "Step: [4466] total_loss: 2.12064409 d_loss: 1.38085914, g_loss: 0.68818331, ae_loss: 0.05160157\n",
      "Step: [4467] total_loss: 2.10877109 d_loss: 1.37121868, g_loss: 0.68732977, ae_loss: 0.05022264\n",
      "Step: [4468] total_loss: 2.11324620 d_loss: 1.36659551, g_loss: 0.69244391, ae_loss: 0.05420685\n",
      "Step: [4469] total_loss: 2.11078763 d_loss: 1.38264298, g_loss: 0.67690444, ae_loss: 0.05124020\n",
      "Step: [4470] total_loss: 2.13477945 d_loss: 1.39941525, g_loss: 0.68426251, ae_loss: 0.05110156\n",
      "Step: [4471] total_loss: 2.13006163 d_loss: 1.39117837, g_loss: 0.68402898, ae_loss: 0.05485444\n",
      "Step: [4472] total_loss: 2.18778777 d_loss: 1.35758567, g_loss: 0.77986801, ae_loss: 0.05033410\n",
      "Step: [4473] total_loss: 2.10735321 d_loss: 1.37270474, g_loss: 0.68603361, ae_loss: 0.04861477\n",
      "Step: [4474] total_loss: 2.11368489 d_loss: 1.38549817, g_loss: 0.68145645, ae_loss: 0.04673028\n",
      "Step: [4475] total_loss: 2.13330507 d_loss: 1.38440228, g_loss: 0.69516796, ae_loss: 0.05373494\n",
      "Step: [4476] total_loss: 2.12875938 d_loss: 1.36950195, g_loss: 0.70964873, ae_loss: 0.04960863\n",
      "Step: [4477] total_loss: 2.16158748 d_loss: 1.40808892, g_loss: 0.70080084, ae_loss: 0.05269772\n",
      "Step: [4478] total_loss: 2.11787033 d_loss: 1.37562013, g_loss: 0.69293422, ae_loss: 0.04931602\n",
      "Step: [4479] total_loss: 2.12384653 d_loss: 1.38760877, g_loss: 0.68432498, ae_loss: 0.05191268\n",
      "Step: [4480] total_loss: 2.11592436 d_loss: 1.34890556, g_loss: 0.71820664, ae_loss: 0.04881229\n",
      "Step: [4481] total_loss: 2.10410357 d_loss: 1.36651850, g_loss: 0.68831336, ae_loss: 0.04927167\n",
      "Step: [4482] total_loss: 2.10272431 d_loss: 1.37763739, g_loss: 0.67510712, ae_loss: 0.04997984\n",
      "Step: [4483] total_loss: 2.10578799 d_loss: 1.36471140, g_loss: 0.69544119, ae_loss: 0.04563530\n",
      "Step: [4484] total_loss: 2.13151455 d_loss: 1.39319253, g_loss: 0.68306160, ae_loss: 0.05526042\n",
      "Step: [4485] total_loss: 2.12087131 d_loss: 1.38303149, g_loss: 0.68565178, ae_loss: 0.05218809\n",
      "Step: [4486] total_loss: 2.13565707 d_loss: 1.39342284, g_loss: 0.68810117, ae_loss: 0.05413307\n",
      "Step: [4487] total_loss: 2.15879798 d_loss: 1.38693810, g_loss: 0.72306669, ae_loss: 0.04879316\n",
      "Step: [4488] total_loss: 2.12522721 d_loss: 1.38225508, g_loss: 0.69217497, ae_loss: 0.05079728\n",
      "Step: [4489] total_loss: 2.12896013 d_loss: 1.37609112, g_loss: 0.69736683, ae_loss: 0.05550226\n",
      "Step: [4490] total_loss: 2.13080168 d_loss: 1.38588083, g_loss: 0.69178629, ae_loss: 0.05313473\n",
      "Step: [4491] total_loss: 2.10379577 d_loss: 1.37594247, g_loss: 0.67947036, ae_loss: 0.04838290\n",
      "Step: [4492] total_loss: 2.11981153 d_loss: 1.38872123, g_loss: 0.68122947, ae_loss: 0.04986072\n",
      "Step: [4493] total_loss: 2.12197971 d_loss: 1.37587857, g_loss: 0.68782234, ae_loss: 0.05827881\n",
      "Step: [4494] total_loss: 2.11345577 d_loss: 1.36822438, g_loss: 0.69058466, ae_loss: 0.05464668\n",
      "Step: [4495] total_loss: 2.10497952 d_loss: 1.36914217, g_loss: 0.68059862, ae_loss: 0.05523883\n",
      "Step: [4496] total_loss: 2.10876918 d_loss: 1.39382219, g_loss: 0.66239202, ae_loss: 0.05255492\n",
      "Step: [4497] total_loss: 2.13566828 d_loss: 1.38567245, g_loss: 0.70101672, ae_loss: 0.04897909\n",
      "Step: [4498] total_loss: 2.10605240 d_loss: 1.36880946, g_loss: 0.68760258, ae_loss: 0.04964034\n",
      "Step: [4499] total_loss: 2.12146521 d_loss: 1.38523901, g_loss: 0.68721378, ae_loss: 0.04901243\n",
      "Step: [4500] total_loss: 2.11736703 d_loss: 1.36699116, g_loss: 0.70053130, ae_loss: 0.04984465\n",
      "Step: [4501] total_loss: 2.12937403 d_loss: 1.38023233, g_loss: 0.70309496, ae_loss: 0.04604680\n",
      "Step: [4502] total_loss: 2.11946893 d_loss: 1.34851229, g_loss: 0.71606982, ae_loss: 0.05488687\n",
      "Step: [4503] total_loss: 2.14982557 d_loss: 1.38889229, g_loss: 0.70857483, ae_loss: 0.05235852\n",
      "Step: [4504] total_loss: 2.14941692 d_loss: 1.39337957, g_loss: 0.70295346, ae_loss: 0.05308390\n",
      "Step: [4505] total_loss: 2.12689972 d_loss: 1.35913539, g_loss: 0.71625292, ae_loss: 0.05151134\n",
      "Step: [4506] total_loss: 2.12266159 d_loss: 1.38288045, g_loss: 0.68935943, ae_loss: 0.05042180\n",
      "Step: [4507] total_loss: 2.12198162 d_loss: 1.39086974, g_loss: 0.68130410, ae_loss: 0.04980774\n",
      "Step: [4508] total_loss: 2.11148620 d_loss: 1.37558091, g_loss: 0.68482268, ae_loss: 0.05108259\n",
      "Step: [4509] total_loss: 2.12135506 d_loss: 1.38443971, g_loss: 0.68976724, ae_loss: 0.04714815\n",
      "Step: [4510] total_loss: 2.13009715 d_loss: 1.37425184, g_loss: 0.70208943, ae_loss: 0.05375590\n",
      "Step: [4511] total_loss: 2.11247969 d_loss: 1.36447382, g_loss: 0.69404066, ae_loss: 0.05396511\n",
      "Step: [4512] total_loss: 2.11927748 d_loss: 1.37554979, g_loss: 0.69278800, ae_loss: 0.05093959\n",
      "Step: [4513] total_loss: 2.13838267 d_loss: 1.39347422, g_loss: 0.69550598, ae_loss: 0.04940242\n",
      "Step: [4514] total_loss: 2.12766218 d_loss: 1.37981200, g_loss: 0.69731110, ae_loss: 0.05053917\n",
      "Step: [4515] total_loss: 2.14356136 d_loss: 1.40018547, g_loss: 0.69342250, ae_loss: 0.04995337\n",
      "Step: [4516] total_loss: 2.12684774 d_loss: 1.38401890, g_loss: 0.69444048, ae_loss: 0.04838828\n",
      "Step: [4517] total_loss: 2.12078881 d_loss: 1.37461352, g_loss: 0.69864446, ae_loss: 0.04753092\n",
      "Step: [4518] total_loss: 2.14542842 d_loss: 1.37953055, g_loss: 0.71199924, ae_loss: 0.05389865\n",
      "Step: [4519] total_loss: 2.12613249 d_loss: 1.39233899, g_loss: 0.68430609, ae_loss: 0.04948748\n",
      "Step: [4520] total_loss: 2.14986157 d_loss: 1.37173510, g_loss: 0.72642696, ae_loss: 0.05169955\n",
      "Step: [4521] total_loss: 2.12762165 d_loss: 1.37696719, g_loss: 0.69709885, ae_loss: 0.05355549\n",
      "Step: [4522] total_loss: 2.12257719 d_loss: 1.36528873, g_loss: 0.70908123, ae_loss: 0.04820728\n",
      "Step: [4523] total_loss: 2.11067009 d_loss: 1.37724400, g_loss: 0.67713928, ae_loss: 0.05628685\n",
      "Step: [4524] total_loss: 2.14077234 d_loss: 1.39984322, g_loss: 0.68845552, ae_loss: 0.05247371\n",
      "Step: [4525] total_loss: 2.11271405 d_loss: 1.37509656, g_loss: 0.68662900, ae_loss: 0.05098844\n",
      "Step: [4526] total_loss: 2.11882877 d_loss: 1.37568426, g_loss: 0.69311130, ae_loss: 0.05003312\n",
      "Step: [4527] total_loss: 2.11440897 d_loss: 1.36581707, g_loss: 0.69804180, ae_loss: 0.05055015\n",
      "Step: [4528] total_loss: 2.12465072 d_loss: 1.36307383, g_loss: 0.70743632, ae_loss: 0.05414056\n",
      "Step: [4529] total_loss: 2.09883118 d_loss: 1.36290717, g_loss: 0.68412316, ae_loss: 0.05180101\n",
      "Step: [4530] total_loss: 2.11581898 d_loss: 1.38903022, g_loss: 0.67956007, ae_loss: 0.04722860\n",
      "Step: [4531] total_loss: 2.08593011 d_loss: 1.36066389, g_loss: 0.67284840, ae_loss: 0.05241770\n",
      "Step: [4532] total_loss: 2.11629391 d_loss: 1.38841462, g_loss: 0.67365587, ae_loss: 0.05422348\n",
      "Step: [4533] total_loss: 2.13929152 d_loss: 1.41010237, g_loss: 0.68102837, ae_loss: 0.04816081\n",
      "Step: [4534] total_loss: 2.13541603 d_loss: 1.39990771, g_loss: 0.68190610, ae_loss: 0.05360210\n",
      "Step: [4535] total_loss: 2.11553121 d_loss: 1.38298833, g_loss: 0.68291867, ae_loss: 0.04962417\n",
      "Step: [4536] total_loss: 2.12342834 d_loss: 1.37927210, g_loss: 0.69038999, ae_loss: 0.05376623\n",
      "Step: [4537] total_loss: 2.13478613 d_loss: 1.40263700, g_loss: 0.68091643, ae_loss: 0.05123277\n",
      "Step: [4538] total_loss: 2.11479878 d_loss: 1.37375855, g_loss: 0.69120586, ae_loss: 0.04983433\n",
      "Step: [4539] total_loss: 2.15709782 d_loss: 1.40728998, g_loss: 0.69673753, ae_loss: 0.05307048\n",
      "Step: [4540] total_loss: 2.11472058 d_loss: 1.35414016, g_loss: 0.70938379, ae_loss: 0.05119665\n",
      "Step: [4541] total_loss: 2.14575362 d_loss: 1.41189337, g_loss: 0.68611753, ae_loss: 0.04774277\n",
      "Step: [4542] total_loss: 2.12110734 d_loss: 1.38010204, g_loss: 0.68785667, ae_loss: 0.05314864\n",
      "Step: [4543] total_loss: 2.10976934 d_loss: 1.36960900, g_loss: 0.68833441, ae_loss: 0.05182592\n",
      "Step: [4544] total_loss: 2.13190222 d_loss: 1.39441705, g_loss: 0.68931949, ae_loss: 0.04816554\n",
      "Step: [4545] total_loss: 2.12863278 d_loss: 1.37845159, g_loss: 0.69589168, ae_loss: 0.05428962\n",
      "Step: [4546] total_loss: 2.11922669 d_loss: 1.37257051, g_loss: 0.69915432, ae_loss: 0.04750180\n",
      "Step: [4547] total_loss: 2.14464593 d_loss: 1.39063358, g_loss: 0.70076036, ae_loss: 0.05325195\n",
      "Step: [4548] total_loss: 2.11325026 d_loss: 1.37182832, g_loss: 0.68909371, ae_loss: 0.05232808\n",
      "Step: [4549] total_loss: 2.11963797 d_loss: 1.37606847, g_loss: 0.69215518, ae_loss: 0.05141440\n",
      "Step: [4550] total_loss: 2.15646505 d_loss: 1.41492093, g_loss: 0.68465734, ae_loss: 0.05688664\n",
      "Step: [4551] total_loss: 2.14367676 d_loss: 1.39199400, g_loss: 0.69909275, ae_loss: 0.05259004\n",
      "Step: [4552] total_loss: 2.11796927 d_loss: 1.37797713, g_loss: 0.68901777, ae_loss: 0.05097431\n",
      "Step: [4553] total_loss: 2.12249804 d_loss: 1.39931822, g_loss: 0.67355049, ae_loss: 0.04962922\n",
      "Step: [4554] total_loss: 2.12230062 d_loss: 1.37109733, g_loss: 0.69973195, ae_loss: 0.05147151\n",
      "Step: [4555] total_loss: 2.14585114 d_loss: 1.40625131, g_loss: 0.69131511, ae_loss: 0.04828464\n",
      "Step: [4556] total_loss: 2.10842752 d_loss: 1.36597753, g_loss: 0.68978953, ae_loss: 0.05266045\n",
      "Step: [4557] total_loss: 2.12192249 d_loss: 1.37754965, g_loss: 0.69340730, ae_loss: 0.05096570\n",
      "Step: [4558] total_loss: 2.11515760 d_loss: 1.38318598, g_loss: 0.68374181, ae_loss: 0.04822993\n",
      "Step: [4559] total_loss: 2.14102006 d_loss: 1.39182544, g_loss: 0.69872832, ae_loss: 0.05046631\n",
      "Step: [4560] total_loss: 2.13392782 d_loss: 1.38373709, g_loss: 0.70069456, ae_loss: 0.04949631\n",
      "Step: [4561] total_loss: 2.14244723 d_loss: 1.40160632, g_loss: 0.68901902, ae_loss: 0.05182197\n",
      "Step: [4562] total_loss: 2.14808035 d_loss: 1.38195860, g_loss: 0.71539092, ae_loss: 0.05073067\n",
      "Step: [4563] total_loss: 2.12692642 d_loss: 1.38769984, g_loss: 0.68800932, ae_loss: 0.05121729\n",
      "Step: [4564] total_loss: 2.13025427 d_loss: 1.37334311, g_loss: 0.70050383, ae_loss: 0.05640731\n",
      "Step: [4565] total_loss: 2.14069200 d_loss: 1.39847636, g_loss: 0.69290882, ae_loss: 0.04930677\n",
      "Step: [4566] total_loss: 2.10281301 d_loss: 1.36045146, g_loss: 0.69492239, ae_loss: 0.04743924\n",
      "Step: [4567] total_loss: 2.12932014 d_loss: 1.37581873, g_loss: 0.70234823, ae_loss: 0.05115309\n",
      "Step: [4568] total_loss: 2.11788464 d_loss: 1.38225961, g_loss: 0.68179989, ae_loss: 0.05382507\n",
      "Step: [4569] total_loss: 2.11613369 d_loss: 1.37267041, g_loss: 0.69246614, ae_loss: 0.05099700\n",
      "Step: [4570] total_loss: 2.12717247 d_loss: 1.39469850, g_loss: 0.68164420, ae_loss: 0.05082968\n",
      "Step: [4571] total_loss: 2.11967516 d_loss: 1.37847686, g_loss: 0.68604887, ae_loss: 0.05514955\n",
      "Step: [4572] total_loss: 2.12190294 d_loss: 1.37642145, g_loss: 0.69727272, ae_loss: 0.04820868\n",
      "Step: [4573] total_loss: 2.12921238 d_loss: 1.38500738, g_loss: 0.69406843, ae_loss: 0.05013654\n",
      "Step: [4574] total_loss: 2.12049055 d_loss: 1.37811732, g_loss: 0.69156820, ae_loss: 0.05080509\n",
      "Step: [4575] total_loss: 2.13752389 d_loss: 1.40944517, g_loss: 0.67880380, ae_loss: 0.04927492\n",
      "Step: [4576] total_loss: 2.13327980 d_loss: 1.40077519, g_loss: 0.68266207, ae_loss: 0.04984250\n",
      "Step: [4577] total_loss: 2.15966010 d_loss: 1.40980613, g_loss: 0.69832695, ae_loss: 0.05152700\n",
      "Step: [4578] total_loss: 2.12789702 d_loss: 1.37590885, g_loss: 0.70228684, ae_loss: 0.04970137\n",
      "Step: [4579] total_loss: 2.12508583 d_loss: 1.36192799, g_loss: 0.71446240, ae_loss: 0.04869555\n",
      "Step: [4580] total_loss: 2.10113668 d_loss: 1.34563649, g_loss: 0.70563161, ae_loss: 0.04986841\n",
      "Step: [4581] total_loss: 2.13649845 d_loss: 1.39715648, g_loss: 0.69175732, ae_loss: 0.04758460\n",
      "Step: [4582] total_loss: 2.11918759 d_loss: 1.38324404, g_loss: 0.68830413, ae_loss: 0.04763947\n",
      "Step: [4583] total_loss: 2.14002872 d_loss: 1.37745547, g_loss: 0.71216482, ae_loss: 0.05040833\n",
      "Step: [4584] total_loss: 2.11114311 d_loss: 1.36608970, g_loss: 0.69571793, ae_loss: 0.04933539\n",
      "Step: [4585] total_loss: 2.10598731 d_loss: 1.33576286, g_loss: 0.71832001, ae_loss: 0.05190440\n",
      "Step: [4586] total_loss: 2.11351156 d_loss: 1.36523139, g_loss: 0.69886327, ae_loss: 0.04941706\n",
      "Step: [4587] total_loss: 2.11752224 d_loss: 1.35952663, g_loss: 0.70574099, ae_loss: 0.05225465\n",
      "Step: [4588] total_loss: 2.14673233 d_loss: 1.39726305, g_loss: 0.69723558, ae_loss: 0.05223376\n",
      "Step: [4589] total_loss: 2.12209678 d_loss: 1.38226771, g_loss: 0.68896097, ae_loss: 0.05086800\n",
      "Step: [4590] total_loss: 2.13106537 d_loss: 1.39695597, g_loss: 0.67970520, ae_loss: 0.05440427\n",
      "Step: [4591] total_loss: 2.11799121 d_loss: 1.38671327, g_loss: 0.67982632, ae_loss: 0.05145158\n",
      "Step: [4592] total_loss: 2.13442111 d_loss: 1.37485540, g_loss: 0.70958918, ae_loss: 0.04997646\n",
      "Step: [4593] total_loss: 2.14296293 d_loss: 1.39239526, g_loss: 0.69973922, ae_loss: 0.05082857\n",
      "Step: [4594] total_loss: 2.10859442 d_loss: 1.37247300, g_loss: 0.68618870, ae_loss: 0.04993285\n",
      "Step: [4595] total_loss: 2.12592459 d_loss: 1.38510454, g_loss: 0.68574893, ae_loss: 0.05507118\n",
      "Step: [4596] total_loss: 2.12859797 d_loss: 1.40336704, g_loss: 0.67438924, ae_loss: 0.05084173\n",
      "Step: [4597] total_loss: 2.13458896 d_loss: 1.37988138, g_loss: 0.70568323, ae_loss: 0.04902430\n",
      "Step: [4598] total_loss: 2.14297056 d_loss: 1.39943957, g_loss: 0.69396901, ae_loss: 0.04956182\n",
      "Step: [4599] total_loss: 2.14104462 d_loss: 1.39139247, g_loss: 0.69917560, ae_loss: 0.05047669\n",
      "Step: [4600] total_loss: 2.13912344 d_loss: 1.38336289, g_loss: 0.70343721, ae_loss: 0.05232325\n",
      "Step: [4601] total_loss: 2.11464977 d_loss: 1.37887335, g_loss: 0.68412411, ae_loss: 0.05165216\n",
      "Step: [4602] total_loss: 2.12806273 d_loss: 1.36936545, g_loss: 0.70551533, ae_loss: 0.05318193\n",
      "Step: [4603] total_loss: 2.13668203 d_loss: 1.39392018, g_loss: 0.69034529, ae_loss: 0.05241662\n",
      "Step: [4604] total_loss: 2.16205835 d_loss: 1.41886544, g_loss: 0.68911475, ae_loss: 0.05407814\n",
      "Step: [4605] total_loss: 2.11999226 d_loss: 1.37956476, g_loss: 0.69406283, ae_loss: 0.04636459\n",
      "Step: [4606] total_loss: 2.12493396 d_loss: 1.36794901, g_loss: 0.70486635, ae_loss: 0.05211850\n",
      "Step: [4607] total_loss: 2.12424302 d_loss: 1.39018393, g_loss: 0.68481570, ae_loss: 0.04924329\n",
      "Step: [4608] total_loss: 2.11657906 d_loss: 1.36711812, g_loss: 0.70139253, ae_loss: 0.04806843\n",
      "Step: [4609] total_loss: 2.14165592 d_loss: 1.37006450, g_loss: 0.72180188, ae_loss: 0.04978969\n",
      "Step: [4610] total_loss: 2.11822081 d_loss: 1.37152386, g_loss: 0.70017350, ae_loss: 0.04652338\n",
      "Step: [4611] total_loss: 2.13166547 d_loss: 1.36926675, g_loss: 0.70592397, ae_loss: 0.05647483\n",
      "Step: [4612] total_loss: 2.15747929 d_loss: 1.41250443, g_loss: 0.69344890, ae_loss: 0.05152592\n",
      "Step: [4613] total_loss: 2.11902690 d_loss: 1.37709236, g_loss: 0.68870288, ae_loss: 0.05323167\n",
      "Step: [4614] total_loss: 2.11235046 d_loss: 1.36051619, g_loss: 0.70027030, ae_loss: 0.05156400\n",
      "Step: [4615] total_loss: 2.15158534 d_loss: 1.39759350, g_loss: 0.70110983, ae_loss: 0.05288198\n",
      "Step: [4616] total_loss: 2.12823105 d_loss: 1.39226627, g_loss: 0.68568486, ae_loss: 0.05027984\n",
      "Step: [4617] total_loss: 2.12479758 d_loss: 1.38400173, g_loss: 0.69041461, ae_loss: 0.05038115\n",
      "Step: [4618] total_loss: 2.14007759 d_loss: 1.39855731, g_loss: 0.69194603, ae_loss: 0.04957443\n",
      "Step: [4619] total_loss: 2.12259483 d_loss: 1.38311434, g_loss: 0.68771851, ae_loss: 0.05176197\n",
      "Step: [4620] total_loss: 2.12334919 d_loss: 1.38592052, g_loss: 0.68793434, ae_loss: 0.04949444\n",
      "Step: [4621] total_loss: 2.12401462 d_loss: 1.36557913, g_loss: 0.70987755, ae_loss: 0.04855786\n",
      "Step: [4622] total_loss: 2.12337661 d_loss: 1.35750973, g_loss: 0.71813059, ae_loss: 0.04773629\n",
      "Step: [4623] total_loss: 2.12915969 d_loss: 1.39249706, g_loss: 0.68723881, ae_loss: 0.04942385\n",
      "Step: [4624] total_loss: 2.11531019 d_loss: 1.37441468, g_loss: 0.68850780, ae_loss: 0.05238781\n",
      "Step: [4625] total_loss: 2.12272215 d_loss: 1.37486434, g_loss: 0.69606167, ae_loss: 0.05179623\n",
      "Step: [4626] total_loss: 2.11568141 d_loss: 1.37667751, g_loss: 0.68959105, ae_loss: 0.04941290\n",
      "Step: [4627] total_loss: 2.11525488 d_loss: 1.38660526, g_loss: 0.67963058, ae_loss: 0.04901892\n",
      "Step: [4628] total_loss: 2.11253667 d_loss: 1.37124586, g_loss: 0.69026577, ae_loss: 0.05102504\n",
      "Step: [4629] total_loss: 2.12615943 d_loss: 1.37765789, g_loss: 0.69916892, ae_loss: 0.04933266\n",
      "Step: [4630] total_loss: 2.12855768 d_loss: 1.39479256, g_loss: 0.68452394, ae_loss: 0.04924130\n",
      "Step: [4631] total_loss: 2.14543772 d_loss: 1.39676821, g_loss: 0.69773376, ae_loss: 0.05093582\n",
      "Step: [4632] total_loss: 2.11596870 d_loss: 1.37788749, g_loss: 0.68928307, ae_loss: 0.04879808\n",
      "Step: [4633] total_loss: 2.12039924 d_loss: 1.37700152, g_loss: 0.69437814, ae_loss: 0.04901958\n",
      "Step: [4634] total_loss: 2.13613367 d_loss: 1.37003899, g_loss: 0.71084368, ae_loss: 0.05525101\n",
      "Step: [4635] total_loss: 2.14155340 d_loss: 1.40514064, g_loss: 0.68847251, ae_loss: 0.04794041\n",
      "Step: [4636] total_loss: 2.11897612 d_loss: 1.38053560, g_loss: 0.68935716, ae_loss: 0.04908337\n",
      "Step: [4637] total_loss: 2.12017632 d_loss: 1.39546037, g_loss: 0.67378116, ae_loss: 0.05093464\n",
      "Step: [4638] total_loss: 2.12566972 d_loss: 1.37282562, g_loss: 0.70270848, ae_loss: 0.05013566\n",
      "Step: [4639] total_loss: 2.12177563 d_loss: 1.38408804, g_loss: 0.68488038, ae_loss: 0.05280723\n",
      "Step: [4640] total_loss: 2.10672140 d_loss: 1.36468780, g_loss: 0.69072819, ae_loss: 0.05130548\n",
      "Step: [4641] total_loss: 2.13925982 d_loss: 1.39196932, g_loss: 0.69428015, ae_loss: 0.05301024\n",
      "Step: [4642] total_loss: 2.11382890 d_loss: 1.39048231, g_loss: 0.67340642, ae_loss: 0.04994014\n",
      "Step: [4643] total_loss: 2.09416389 d_loss: 1.36606836, g_loss: 0.67864895, ae_loss: 0.04944643\n",
      "Step: [4644] total_loss: 2.10347342 d_loss: 1.37050939, g_loss: 0.68028474, ae_loss: 0.05267934\n",
      "Step: [4645] total_loss: 2.12911081 d_loss: 1.39348328, g_loss: 0.68221998, ae_loss: 0.05340740\n",
      "Step: [4646] total_loss: 2.11352897 d_loss: 1.38533926, g_loss: 0.67617351, ae_loss: 0.05201626\n",
      "Step: [4647] total_loss: 2.11743188 d_loss: 1.36505866, g_loss: 0.69680518, ae_loss: 0.05556805\n",
      "Step: [4648] total_loss: 2.12371659 d_loss: 1.38094461, g_loss: 0.68694407, ae_loss: 0.05582782\n",
      "Step: [4649] total_loss: 2.12614036 d_loss: 1.38407755, g_loss: 0.69174141, ae_loss: 0.05032150\n",
      "Step: [4650] total_loss: 2.11613035 d_loss: 1.37356174, g_loss: 0.69008207, ae_loss: 0.05248668\n",
      "Step: [4651] total_loss: 2.12098217 d_loss: 1.37131846, g_loss: 0.70086122, ae_loss: 0.04880238\n",
      "Step: [4652] total_loss: 2.09436989 d_loss: 1.35946667, g_loss: 0.68360740, ae_loss: 0.05129574\n",
      "Step: [4653] total_loss: 2.13356447 d_loss: 1.41060817, g_loss: 0.67097354, ae_loss: 0.05198286\n",
      "Step: [4654] total_loss: 2.11647177 d_loss: 1.37818384, g_loss: 0.68468535, ae_loss: 0.05360273\n",
      "Step: [4655] total_loss: 2.11586428 d_loss: 1.38031173, g_loss: 0.68519533, ae_loss: 0.05035708\n",
      "Step: [4656] total_loss: 2.10635018 d_loss: 1.38176656, g_loss: 0.67241722, ae_loss: 0.05216649\n",
      "Step: [4657] total_loss: 2.11827517 d_loss: 1.36898160, g_loss: 0.69618124, ae_loss: 0.05311236\n",
      "Step: [4658] total_loss: 2.11857772 d_loss: 1.38085365, g_loss: 0.69036233, ae_loss: 0.04736170\n",
      "Step: [4659] total_loss: 2.12463856 d_loss: 1.37137544, g_loss: 0.69831479, ae_loss: 0.05494815\n",
      "Step: [4660] total_loss: 2.12494564 d_loss: 1.36681211, g_loss: 0.70927131, ae_loss: 0.04886231\n",
      "Step: [4661] total_loss: 2.11958504 d_loss: 1.37173700, g_loss: 0.69353795, ae_loss: 0.05431016\n",
      "Step: [4662] total_loss: 2.12814999 d_loss: 1.39394736, g_loss: 0.68357182, ae_loss: 0.05063085\n",
      "Step: [4663] total_loss: 2.11788201 d_loss: 1.36882401, g_loss: 0.70009702, ae_loss: 0.04896105\n",
      "Step: [4664] total_loss: 2.11631298 d_loss: 1.36149216, g_loss: 0.70284784, ae_loss: 0.05197305\n",
      "Step: [4665] total_loss: 2.13547730 d_loss: 1.39704227, g_loss: 0.68490756, ae_loss: 0.05352742\n",
      "Step: [4666] total_loss: 2.10724854 d_loss: 1.36243093, g_loss: 0.69295698, ae_loss: 0.05186057\n",
      "Step: [4667] total_loss: 2.13742399 d_loss: 1.40605593, g_loss: 0.67786121, ae_loss: 0.05350679\n",
      "Step: [4668] total_loss: 2.14227438 d_loss: 1.38373780, g_loss: 0.70663583, ae_loss: 0.05190072\n",
      "Step: [4669] total_loss: 2.13179445 d_loss: 1.38256097, g_loss: 0.70192361, ae_loss: 0.04730989\n",
      "Step: [4670] total_loss: 2.13123989 d_loss: 1.37384701, g_loss: 0.70606959, ae_loss: 0.05132314\n",
      "Step: [4671] total_loss: 2.13408399 d_loss: 1.38823783, g_loss: 0.69342983, ae_loss: 0.05241634\n",
      "Step: [4672] total_loss: 2.13536048 d_loss: 1.37436175, g_loss: 0.70766556, ae_loss: 0.05333314\n",
      "Step: [4673] total_loss: 2.15260315 d_loss: 1.39282131, g_loss: 0.70925730, ae_loss: 0.05052444\n",
      "Step: [4674] total_loss: 2.13579702 d_loss: 1.38770080, g_loss: 0.69486749, ae_loss: 0.05322887\n",
      "Step: [4675] total_loss: 2.12686253 d_loss: 1.38563204, g_loss: 0.68842089, ae_loss: 0.05280949\n",
      "Step: [4676] total_loss: 2.11755919 d_loss: 1.36864519, g_loss: 0.69637346, ae_loss: 0.05254050\n",
      "Step: [4677] total_loss: 2.11229491 d_loss: 1.37580848, g_loss: 0.68883514, ae_loss: 0.04765128\n",
      "Step: [4678] total_loss: 2.12887287 d_loss: 1.38937759, g_loss: 0.69044554, ae_loss: 0.04904966\n",
      "Step: [4679] total_loss: 2.11638951 d_loss: 1.35786033, g_loss: 0.70829111, ae_loss: 0.05023799\n",
      "Step: [4680] total_loss: 2.13167953 d_loss: 1.35447311, g_loss: 0.72637206, ae_loss: 0.05083428\n",
      "Step: [4681] total_loss: 2.10597110 d_loss: 1.36022854, g_loss: 0.69262952, ae_loss: 0.05311311\n",
      "Step: [4682] total_loss: 2.12558985 d_loss: 1.39671493, g_loss: 0.68186545, ae_loss: 0.04700936\n",
      "Step: [4683] total_loss: 2.11837077 d_loss: 1.38167834, g_loss: 0.68809557, ae_loss: 0.04859683\n",
      "Step: [4684] total_loss: 2.13122177 d_loss: 1.38376939, g_loss: 0.69449657, ae_loss: 0.05295579\n",
      "Step: [4685] total_loss: 2.13047576 d_loss: 1.38912201, g_loss: 0.69035590, ae_loss: 0.05099787\n",
      "Step: [4686] total_loss: 2.12265134 d_loss: 1.38832307, g_loss: 0.68584865, ae_loss: 0.04847969\n",
      "Step: [4687] total_loss: 2.12940598 d_loss: 1.37339854, g_loss: 0.70489883, ae_loss: 0.05110876\n",
      "Step: [4688] total_loss: 2.13866663 d_loss: 1.39498496, g_loss: 0.69627988, ae_loss: 0.04740195\n",
      "Step: [4689] total_loss: 2.11947870 d_loss: 1.38209343, g_loss: 0.68928742, ae_loss: 0.04809791\n",
      "Step: [4690] total_loss: 2.12952137 d_loss: 1.39815390, g_loss: 0.67784154, ae_loss: 0.05352601\n",
      "Step: [4691] total_loss: 2.12083316 d_loss: 1.38067055, g_loss: 0.69106275, ae_loss: 0.04909993\n",
      "Step: [4692] total_loss: 2.12444425 d_loss: 1.36598182, g_loss: 0.70813918, ae_loss: 0.05032329\n",
      "Step: [4693] total_loss: 2.10182047 d_loss: 1.35280788, g_loss: 0.69602966, ae_loss: 0.05298303\n",
      "Step: [4694] total_loss: 2.12148571 d_loss: 1.37287807, g_loss: 0.70190603, ae_loss: 0.04670152\n",
      "Step: [4695] total_loss: 2.12662053 d_loss: 1.39610040, g_loss: 0.68017858, ae_loss: 0.05034153\n",
      "Step: [4696] total_loss: 2.13966084 d_loss: 1.39101231, g_loss: 0.70093149, ae_loss: 0.04771699\n",
      "Step: [4697] total_loss: 2.13001275 d_loss: 1.38547277, g_loss: 0.68651223, ae_loss: 0.05802771\n",
      "Step: [4698] total_loss: 2.14511299 d_loss: 1.39201427, g_loss: 0.70242834, ae_loss: 0.05067037\n",
      "Step: [4699] total_loss: 2.13049960 d_loss: 1.39146399, g_loss: 0.69031262, ae_loss: 0.04872300\n",
      "Step: [4700] total_loss: 2.13840938 d_loss: 1.36330545, g_loss: 0.72415596, ae_loss: 0.05094797\n",
      "Step: [4701] total_loss: 2.10324454 d_loss: 1.37467146, g_loss: 0.67739081, ae_loss: 0.05118229\n",
      "Step: [4702] total_loss: 2.09411192 d_loss: 1.37535191, g_loss: 0.66768122, ae_loss: 0.05107886\n",
      "Step: [4703] total_loss: 2.12623334 d_loss: 1.40191364, g_loss: 0.67680317, ae_loss: 0.04751651\n",
      "Step: [4704] total_loss: 2.11486435 d_loss: 1.38697135, g_loss: 0.67595363, ae_loss: 0.05193922\n",
      "Step: [4705] total_loss: 2.11627245 d_loss: 1.38931859, g_loss: 0.67764628, ae_loss: 0.04930742\n",
      "Step: [4706] total_loss: 2.09766054 d_loss: 1.33771443, g_loss: 0.70772642, ae_loss: 0.05221972\n",
      "Step: [4707] total_loss: 2.12017012 d_loss: 1.36815763, g_loss: 0.70124418, ae_loss: 0.05076835\n",
      "Step: [4708] total_loss: 2.13847446 d_loss: 1.36345994, g_loss: 0.71937966, ae_loss: 0.05563501\n",
      "Step: [4709] total_loss: 2.13095260 d_loss: 1.38834620, g_loss: 0.68804789, ae_loss: 0.05455854\n",
      "Step: [4710] total_loss: 2.12334442 d_loss: 1.37825871, g_loss: 0.69432431, ae_loss: 0.05076139\n",
      "Step: [4711] total_loss: 2.09197521 d_loss: 1.32965910, g_loss: 0.70875001, ae_loss: 0.05356623\n",
      "Step: [4712] total_loss: 2.12263775 d_loss: 1.39683056, g_loss: 0.67709506, ae_loss: 0.04871225\n",
      "Step: [4713] total_loss: 2.12469268 d_loss: 1.38022482, g_loss: 0.69208431, ae_loss: 0.05238358\n",
      "Step: [4714] total_loss: 2.10824919 d_loss: 1.37196136, g_loss: 0.68381500, ae_loss: 0.05247274\n",
      "Step: [4715] total_loss: 2.13310766 d_loss: 1.39000666, g_loss: 0.69017363, ae_loss: 0.05292724\n",
      "Step: [4716] total_loss: 2.11199927 d_loss: 1.37922931, g_loss: 0.68271339, ae_loss: 0.05005658\n",
      "Step: [4717] total_loss: 2.13625765 d_loss: 1.38198328, g_loss: 0.70501900, ae_loss: 0.04925548\n",
      "Step: [4718] total_loss: 2.11884856 d_loss: 1.36770904, g_loss: 0.70020592, ae_loss: 0.05093359\n",
      "Step: [4719] total_loss: 2.12476325 d_loss: 1.38752103, g_loss: 0.68244576, ae_loss: 0.05479646\n",
      "Step: [4720] total_loss: 2.12271070 d_loss: 1.38801050, g_loss: 0.68196338, ae_loss: 0.05273681\n",
      "Step: [4721] total_loss: 2.12201214 d_loss: 1.37809706, g_loss: 0.68916416, ae_loss: 0.05475106\n",
      "Step: [4722] total_loss: 2.11540866 d_loss: 1.37932205, g_loss: 0.68369085, ae_loss: 0.05239572\n",
      "Step: [4723] total_loss: 2.12731934 d_loss: 1.37354469, g_loss: 0.69683456, ae_loss: 0.05694006\n",
      "Step: [4724] total_loss: 2.16211939 d_loss: 1.40661430, g_loss: 0.70543408, ae_loss: 0.05007099\n",
      "Step: [4725] total_loss: 2.12868857 d_loss: 1.36692023, g_loss: 0.70670849, ae_loss: 0.05505979\n",
      "Step: [4726] total_loss: 2.14291525 d_loss: 1.39405739, g_loss: 0.69956708, ae_loss: 0.04929069\n",
      "Step: [4727] total_loss: 2.12671518 d_loss: 1.39228725, g_loss: 0.68593049, ae_loss: 0.04849760\n",
      "Step: [4728] total_loss: 2.16136408 d_loss: 1.39574265, g_loss: 0.71364427, ae_loss: 0.05197725\n",
      "Step: [4729] total_loss: 2.12121797 d_loss: 1.38732767, g_loss: 0.68291920, ae_loss: 0.05097106\n",
      "Step: [4730] total_loss: 2.11286116 d_loss: 1.38182473, g_loss: 0.68230844, ae_loss: 0.04872815\n",
      "Step: [4731] total_loss: 2.14183283 d_loss: 1.39805794, g_loss: 0.69548297, ae_loss: 0.04829204\n",
      "Step: [4732] total_loss: 2.12786937 d_loss: 1.39729357, g_loss: 0.67590576, ae_loss: 0.05467015\n",
      "Step: [4733] total_loss: 2.11339283 d_loss: 1.37746572, g_loss: 0.68560803, ae_loss: 0.05031900\n",
      "Step: [4734] total_loss: 2.11759758 d_loss: 1.38655782, g_loss: 0.68063438, ae_loss: 0.05040528\n",
      "Step: [4735] total_loss: 2.13164377 d_loss: 1.38693523, g_loss: 0.69144726, ae_loss: 0.05326123\n",
      "Step: [4736] total_loss: 2.10409331 d_loss: 1.36915421, g_loss: 0.68396968, ae_loss: 0.05096939\n",
      "Step: [4737] total_loss: 2.16074872 d_loss: 1.39804173, g_loss: 0.71167034, ae_loss: 0.05103672\n",
      "Step: [4738] total_loss: 2.13267112 d_loss: 1.38053668, g_loss: 0.70225585, ae_loss: 0.04987865\n",
      "Step: [4739] total_loss: 2.14355683 d_loss: 1.39772058, g_loss: 0.69606209, ae_loss: 0.04977422\n",
      "Step: [4740] total_loss: 2.11664677 d_loss: 1.35891926, g_loss: 0.70433933, ae_loss: 0.05338817\n",
      "Step: [4741] total_loss: 2.12533116 d_loss: 1.38727820, g_loss: 0.68896043, ae_loss: 0.04909250\n",
      "Step: [4742] total_loss: 2.10043526 d_loss: 1.35051942, g_loss: 0.69427359, ae_loss: 0.05564240\n",
      "Step: [4743] total_loss: 2.11573792 d_loss: 1.38355362, g_loss: 0.68722123, ae_loss: 0.04496315\n",
      "Step: [4744] total_loss: 2.14898968 d_loss: 1.35465813, g_loss: 0.74125743, ae_loss: 0.05307403\n",
      "Step: [4745] total_loss: 2.13404536 d_loss: 1.38485003, g_loss: 0.69322807, ae_loss: 0.05596728\n",
      "Step: [4746] total_loss: 2.11973739 d_loss: 1.39152527, g_loss: 0.67718911, ae_loss: 0.05102299\n",
      "Step: [4747] total_loss: 2.11479163 d_loss: 1.38527262, g_loss: 0.67485315, ae_loss: 0.05466593\n",
      "Step: [4748] total_loss: 2.11981034 d_loss: 1.39072835, g_loss: 0.68079460, ae_loss: 0.04828740\n",
      "Step: [4749] total_loss: 2.13492966 d_loss: 1.37842202, g_loss: 0.70496809, ae_loss: 0.05153939\n",
      "Step: [4750] total_loss: 2.13758278 d_loss: 1.37435341, g_loss: 0.71273923, ae_loss: 0.05049029\n",
      "Step: [4751] total_loss: 2.10383892 d_loss: 1.36401963, g_loss: 0.69084775, ae_loss: 0.04897141\n",
      "Step: [4752] total_loss: 2.12434077 d_loss: 1.38380063, g_loss: 0.69213706, ae_loss: 0.04840320\n",
      "Step: [4753] total_loss: 2.12986422 d_loss: 1.38178396, g_loss: 0.69637942, ae_loss: 0.05170083\n",
      "Step: [4754] total_loss: 2.14554691 d_loss: 1.36334682, g_loss: 0.72529238, ae_loss: 0.05690761\n",
      "Step: [4755] total_loss: 2.13065863 d_loss: 1.37519598, g_loss: 0.70701468, ae_loss: 0.04844789\n",
      "Step: [4756] total_loss: 2.12276506 d_loss: 1.38289285, g_loss: 0.68589866, ae_loss: 0.05397347\n",
      "Step: [4757] total_loss: 2.12547421 d_loss: 1.38950455, g_loss: 0.68552727, ae_loss: 0.05044230\n",
      "Step: [4758] total_loss: 2.13260937 d_loss: 1.36919498, g_loss: 0.71647727, ae_loss: 0.04693702\n",
      "Step: [4759] total_loss: 2.12978983 d_loss: 1.37871194, g_loss: 0.70100856, ae_loss: 0.05006924\n",
      "Step: [4760] total_loss: 2.12737751 d_loss: 1.37047172, g_loss: 0.70600271, ae_loss: 0.05090312\n",
      "Step: [4761] total_loss: 2.12080026 d_loss: 1.38430035, g_loss: 0.68628854, ae_loss: 0.05021125\n",
      "Step: [4762] total_loss: 2.09439969 d_loss: 1.35774803, g_loss: 0.68779182, ae_loss: 0.04885979\n",
      "Step: [4763] total_loss: 2.12117290 d_loss: 1.37784123, g_loss: 0.69271636, ae_loss: 0.05061528\n",
      "Step: [4764] total_loss: 2.11983538 d_loss: 1.37425709, g_loss: 0.69363385, ae_loss: 0.05194436\n",
      "Step: [4765] total_loss: 2.11596656 d_loss: 1.34296989, g_loss: 0.72312337, ae_loss: 0.04987341\n",
      "Step: [4766] total_loss: 2.11300874 d_loss: 1.37684071, g_loss: 0.68420744, ae_loss: 0.05196056\n",
      "Step: [4767] total_loss: 2.14379311 d_loss: 1.39751959, g_loss: 0.69509721, ae_loss: 0.05117632\n",
      "Step: [4768] total_loss: 2.11867905 d_loss: 1.37412095, g_loss: 0.69217443, ae_loss: 0.05238357\n",
      "Step: [4769] total_loss: 2.14142847 d_loss: 1.39814687, g_loss: 0.68840802, ae_loss: 0.05487362\n",
      "Step: [4770] total_loss: 2.11559439 d_loss: 1.36479485, g_loss: 0.69971263, ae_loss: 0.05108689\n",
      "Step: [4771] total_loss: 2.12906313 d_loss: 1.35889769, g_loss: 0.72005612, ae_loss: 0.05010939\n",
      "Step: [4772] total_loss: 2.12675738 d_loss: 1.38766050, g_loss: 0.68777674, ae_loss: 0.05132011\n",
      "Step: [4773] total_loss: 2.13889074 d_loss: 1.39515710, g_loss: 0.68927014, ae_loss: 0.05446333\n",
      "Step: [4774] total_loss: 2.15198207 d_loss: 1.38820624, g_loss: 0.71015215, ae_loss: 0.05362374\n",
      "Step: [4775] total_loss: 2.10930395 d_loss: 1.35841572, g_loss: 0.69515133, ae_loss: 0.05573696\n",
      "Step: [4776] total_loss: 2.12169743 d_loss: 1.37400079, g_loss: 0.69250298, ae_loss: 0.05519382\n",
      "Step: [4777] total_loss: 2.12139773 d_loss: 1.39149380, g_loss: 0.68209857, ae_loss: 0.04780542\n",
      "Step: [4778] total_loss: 2.09293437 d_loss: 1.35564649, g_loss: 0.68606180, ae_loss: 0.05122598\n",
      "Step: [4779] total_loss: 2.13770247 d_loss: 1.40553236, g_loss: 0.67970723, ae_loss: 0.05246286\n",
      "Step: [4780] total_loss: 2.10959029 d_loss: 1.36066127, g_loss: 0.69754946, ae_loss: 0.05137958\n",
      "Step: [4781] total_loss: 2.12136364 d_loss: 1.40054870, g_loss: 0.67293221, ae_loss: 0.04788274\n",
      "Step: [4782] total_loss: 2.12585711 d_loss: 1.37482989, g_loss: 0.69730306, ae_loss: 0.05372416\n",
      "Step: [4783] total_loss: 2.12098670 d_loss: 1.38992977, g_loss: 0.68181586, ae_loss: 0.04924110\n",
      "Step: [4784] total_loss: 2.14358187 d_loss: 1.40944445, g_loss: 0.68479300, ae_loss: 0.04934430\n",
      "Step: [4785] total_loss: 2.12360954 d_loss: 1.39907265, g_loss: 0.67531967, ae_loss: 0.04921734\n",
      "Step: [4786] total_loss: 2.14291525 d_loss: 1.38854909, g_loss: 0.70593154, ae_loss: 0.04843467\n",
      "Step: [4787] total_loss: 2.13312912 d_loss: 1.38862467, g_loss: 0.69472575, ae_loss: 0.04977856\n",
      "Step: [4788] total_loss: 2.10875487 d_loss: 1.36997008, g_loss: 0.68913245, ae_loss: 0.04965239\n",
      "Step: [4789] total_loss: 2.15495300 d_loss: 1.38097990, g_loss: 0.72021341, ae_loss: 0.05375960\n",
      "Step: [4790] total_loss: 2.13074064 d_loss: 1.39589953, g_loss: 0.68005109, ae_loss: 0.05479012\n",
      "Step: [4791] total_loss: 2.10947824 d_loss: 1.36650348, g_loss: 0.69148481, ae_loss: 0.05149001\n",
      "Step: [4792] total_loss: 2.12654591 d_loss: 1.36683166, g_loss: 0.70652562, ae_loss: 0.05318857\n",
      "Step: [4793] total_loss: 2.12246418 d_loss: 1.38311791, g_loss: 0.69073749, ae_loss: 0.04860869\n",
      "Step: [4794] total_loss: 2.12497878 d_loss: 1.37596238, g_loss: 0.70060140, ae_loss: 0.04841496\n",
      "Step: [4795] total_loss: 2.12871265 d_loss: 1.39077401, g_loss: 0.68933153, ae_loss: 0.04860701\n",
      "Step: [4796] total_loss: 2.13386679 d_loss: 1.36198282, g_loss: 0.72455561, ae_loss: 0.04732853\n",
      "Step: [4797] total_loss: 2.11720657 d_loss: 1.37539148, g_loss: 0.69201362, ae_loss: 0.04980141\n",
      "Step: [4798] total_loss: 2.12486601 d_loss: 1.39173818, g_loss: 0.68520463, ae_loss: 0.04792315\n",
      "Step: [4799] total_loss: 2.13627768 d_loss: 1.39194584, g_loss: 0.69238901, ae_loss: 0.05194286\n",
      "Step: [4800] total_loss: 2.13152099 d_loss: 1.38170624, g_loss: 0.70565110, ae_loss: 0.04416359\n",
      "Step: [4801] total_loss: 2.15801096 d_loss: 1.40256381, g_loss: 0.70088983, ae_loss: 0.05455715\n",
      "Step: [4802] total_loss: 2.15788341 d_loss: 1.41179109, g_loss: 0.69592369, ae_loss: 0.05016859\n",
      "Step: [4803] total_loss: 2.13319588 d_loss: 1.39267731, g_loss: 0.69232708, ae_loss: 0.04819140\n",
      "Step: [4804] total_loss: 2.14730501 d_loss: 1.38601971, g_loss: 0.70260733, ae_loss: 0.05867799\n",
      "Step: [4805] total_loss: 2.13453197 d_loss: 1.39239478, g_loss: 0.69078594, ae_loss: 0.05135113\n",
      "Step: [4806] total_loss: 2.11574531 d_loss: 1.37803864, g_loss: 0.68712634, ae_loss: 0.05058037\n",
      "Step: [4807] total_loss: 2.12622118 d_loss: 1.38547611, g_loss: 0.69145060, ae_loss: 0.04929440\n",
      "Step: [4808] total_loss: 2.10950112 d_loss: 1.36844409, g_loss: 0.68742454, ae_loss: 0.05363250\n",
      "Step: [4809] total_loss: 2.10420442 d_loss: 1.37960339, g_loss: 0.67698777, ae_loss: 0.04761324\n",
      "Step: [4810] total_loss: 2.15449238 d_loss: 1.36209941, g_loss: 0.73950326, ae_loss: 0.05288988\n",
      "Step: [4811] total_loss: 2.12907887 d_loss: 1.37914038, g_loss: 0.69561458, ae_loss: 0.05432386\n",
      "Step: [4812] total_loss: 2.12531757 d_loss: 1.39648700, g_loss: 0.67590612, ae_loss: 0.05292452\n",
      "Step: [4813] total_loss: 2.13158846 d_loss: 1.39654291, g_loss: 0.68539011, ae_loss: 0.04965533\n",
      "Step: [4814] total_loss: 2.14650631 d_loss: 1.39918578, g_loss: 0.69663429, ae_loss: 0.05068631\n",
      "Step: [4815] total_loss: 2.12437487 d_loss: 1.37245083, g_loss: 0.69850934, ae_loss: 0.05341457\n",
      "Step: [4816] total_loss: 2.11952209 d_loss: 1.36881232, g_loss: 0.70284975, ae_loss: 0.04785997\n",
      "Step: [4817] total_loss: 2.11649489 d_loss: 1.35670626, g_loss: 0.71596330, ae_loss: 0.04382536\n",
      "Step: [4818] total_loss: 2.13091183 d_loss: 1.37649024, g_loss: 0.70478117, ae_loss: 0.04964050\n",
      "Step: [4819] total_loss: 2.11718130 d_loss: 1.38871837, g_loss: 0.68100178, ae_loss: 0.04746109\n",
      "Step: [4820] total_loss: 2.13769484 d_loss: 1.38792682, g_loss: 0.70154536, ae_loss: 0.04822277\n",
      "Step: [4821] total_loss: 2.12464952 d_loss: 1.37763572, g_loss: 0.69832623, ae_loss: 0.04868755\n",
      "Step: [4822] total_loss: 2.12285805 d_loss: 1.37106192, g_loss: 0.69911861, ae_loss: 0.05267760\n",
      "Step: [4823] total_loss: 2.13706779 d_loss: 1.37610304, g_loss: 0.70535481, ae_loss: 0.05561012\n",
      "Step: [4824] total_loss: 2.12563086 d_loss: 1.40075779, g_loss: 0.67299283, ae_loss: 0.05188015\n",
      "Step: [4825] total_loss: 2.13636494 d_loss: 1.40089679, g_loss: 0.68992054, ae_loss: 0.04554750\n",
      "Step: [4826] total_loss: 2.13210583 d_loss: 1.38859439, g_loss: 0.69404244, ae_loss: 0.04946889\n",
      "Step: [4827] total_loss: 2.09890223 d_loss: 1.36695528, g_loss: 0.68381864, ae_loss: 0.04812822\n",
      "Step: [4828] total_loss: 2.11562300 d_loss: 1.36051941, g_loss: 0.70522904, ae_loss: 0.04987455\n",
      "Step: [4829] total_loss: 2.12332869 d_loss: 1.38000417, g_loss: 0.69100463, ae_loss: 0.05231975\n",
      "Step: [4830] total_loss: 2.14388657 d_loss: 1.39614868, g_loss: 0.69472969, ae_loss: 0.05300834\n",
      "Step: [4831] total_loss: 2.13441753 d_loss: 1.37991595, g_loss: 0.70616913, ae_loss: 0.04833253\n",
      "Step: [4832] total_loss: 2.11371422 d_loss: 1.36774707, g_loss: 0.69445169, ae_loss: 0.05151539\n",
      "Step: [4833] total_loss: 2.11944342 d_loss: 1.37523532, g_loss: 0.69306481, ae_loss: 0.05114319\n",
      "Step: [4834] total_loss: 2.11900091 d_loss: 1.38039958, g_loss: 0.69164443, ae_loss: 0.04695692\n",
      "Step: [4835] total_loss: 2.12248945 d_loss: 1.38170290, g_loss: 0.69270217, ae_loss: 0.04808451\n",
      "Step: [4836] total_loss: 2.11125350 d_loss: 1.36541283, g_loss: 0.69700575, ae_loss: 0.04883496\n",
      "Step: [4837] total_loss: 2.12567282 d_loss: 1.38766468, g_loss: 0.69064236, ae_loss: 0.04736588\n",
      "Step: [4838] total_loss: 2.12458849 d_loss: 1.38569856, g_loss: 0.68770868, ae_loss: 0.05118126\n",
      "Step: [4839] total_loss: 2.13320589 d_loss: 1.37094522, g_loss: 0.71244371, ae_loss: 0.04981705\n",
      "Step: [4840] total_loss: 2.11556387 d_loss: 1.36842704, g_loss: 0.69190168, ae_loss: 0.05523504\n",
      "Step: [4841] total_loss: 2.14837146 d_loss: 1.39365411, g_loss: 0.70689487, ae_loss: 0.04782249\n",
      "Step: [4842] total_loss: 2.12034106 d_loss: 1.37138224, g_loss: 0.69660985, ae_loss: 0.05234893\n",
      "Step: [4843] total_loss: 2.11368918 d_loss: 1.36382890, g_loss: 0.69585961, ae_loss: 0.05400062\n",
      "Step: [4844] total_loss: 2.13255358 d_loss: 1.38219309, g_loss: 0.70146275, ae_loss: 0.04889784\n",
      "Step: [4845] total_loss: 2.08937430 d_loss: 1.34627986, g_loss: 0.69432354, ae_loss: 0.04877089\n",
      "Step: [4846] total_loss: 2.11562681 d_loss: 1.37885070, g_loss: 0.68305731, ae_loss: 0.05371863\n",
      "Step: [4847] total_loss: 2.12357330 d_loss: 1.37830234, g_loss: 0.69458055, ae_loss: 0.05069051\n",
      "Step: [4848] total_loss: 2.13101101 d_loss: 1.40459704, g_loss: 0.67408288, ae_loss: 0.05233120\n",
      "Step: [4849] total_loss: 2.13311386 d_loss: 1.39249802, g_loss: 0.68741035, ae_loss: 0.05320558\n",
      "Step: [4850] total_loss: 2.13634133 d_loss: 1.39506459, g_loss: 0.68924767, ae_loss: 0.05202901\n",
      "Step: [4851] total_loss: 2.12110758 d_loss: 1.38765121, g_loss: 0.68148208, ae_loss: 0.05197439\n",
      "Step: [4852] total_loss: 2.13771534 d_loss: 1.39303136, g_loss: 0.69361198, ae_loss: 0.05107193\n",
      "Step: [4853] total_loss: 2.11891055 d_loss: 1.36200440, g_loss: 0.70728481, ae_loss: 0.04962139\n",
      "Step: [4854] total_loss: 2.11692953 d_loss: 1.37114573, g_loss: 0.69288707, ae_loss: 0.05289688\n",
      "Step: [4855] total_loss: 2.13078403 d_loss: 1.37833226, g_loss: 0.70165962, ae_loss: 0.05079220\n",
      "Step: [4856] total_loss: 2.09703207 d_loss: 1.35603607, g_loss: 0.68801832, ae_loss: 0.05297768\n",
      "Step: [4857] total_loss: 2.12221289 d_loss: 1.37321091, g_loss: 0.70057762, ae_loss: 0.04842441\n",
      "Step: [4858] total_loss: 2.13571954 d_loss: 1.39915574, g_loss: 0.68228143, ae_loss: 0.05428239\n",
      "Step: [4859] total_loss: 2.12435985 d_loss: 1.38081026, g_loss: 0.69107366, ae_loss: 0.05247593\n",
      "Step: [4860] total_loss: 2.11275578 d_loss: 1.37646723, g_loss: 0.68393922, ae_loss: 0.05234917\n",
      "Step: [4861] total_loss: 2.13143635 d_loss: 1.40410566, g_loss: 0.67608833, ae_loss: 0.05124244\n",
      "Step: [4862] total_loss: 2.13295531 d_loss: 1.37533319, g_loss: 0.70636004, ae_loss: 0.05126211\n",
      "Step: [4863] total_loss: 2.10571170 d_loss: 1.35516751, g_loss: 0.69872993, ae_loss: 0.05181421\n",
      "Step: [4864] total_loss: 2.12547565 d_loss: 1.36658382, g_loss: 0.71085054, ae_loss: 0.04804137\n",
      "Step: [4865] total_loss: 2.13416600 d_loss: 1.36422253, g_loss: 0.71473902, ae_loss: 0.05520444\n",
      "Step: [4866] total_loss: 2.13607860 d_loss: 1.38984632, g_loss: 0.69484407, ae_loss: 0.05138827\n",
      "Step: [4867] total_loss: 2.10239816 d_loss: 1.35110331, g_loss: 0.69923991, ae_loss: 0.05205488\n",
      "Step: [4868] total_loss: 2.12340212 d_loss: 1.34259880, g_loss: 0.72662175, ae_loss: 0.05418159\n",
      "Step: [4869] total_loss: 2.13809967 d_loss: 1.38014162, g_loss: 0.70286560, ae_loss: 0.05509248\n",
      "Step: [4870] total_loss: 2.12562299 d_loss: 1.37827218, g_loss: 0.69769365, ae_loss: 0.04965719\n",
      "Step: [4871] total_loss: 2.13793182 d_loss: 1.38798499, g_loss: 0.69921130, ae_loss: 0.05073546\n",
      "Step: [4872] total_loss: 2.10142159 d_loss: 1.36425483, g_loss: 0.68494338, ae_loss: 0.05222342\n",
      "Step: [4873] total_loss: 2.15690231 d_loss: 1.40409219, g_loss: 0.69629872, ae_loss: 0.05651131\n",
      "Step: [4874] total_loss: 2.13760448 d_loss: 1.38036823, g_loss: 0.70831507, ae_loss: 0.04892114\n",
      "Step: [4875] total_loss: 2.12912250 d_loss: 1.37382793, g_loss: 0.70222533, ae_loss: 0.05306922\n",
      "Step: [4876] total_loss: 2.14598441 d_loss: 1.39993191, g_loss: 0.69139183, ae_loss: 0.05466068\n",
      "Step: [4877] total_loss: 2.12686729 d_loss: 1.37490845, g_loss: 0.69735491, ae_loss: 0.05460399\n",
      "Step: [4878] total_loss: 2.12157869 d_loss: 1.38370585, g_loss: 0.68743670, ae_loss: 0.05043621\n",
      "Step: [4879] total_loss: 2.11712646 d_loss: 1.38102674, g_loss: 0.68926573, ae_loss: 0.04683410\n",
      "Step: [4880] total_loss: 2.09957552 d_loss: 1.35885370, g_loss: 0.68860185, ae_loss: 0.05212008\n",
      "Step: [4881] total_loss: 2.12983656 d_loss: 1.37123621, g_loss: 0.70903713, ae_loss: 0.04956318\n",
      "Step: [4882] total_loss: 2.11537862 d_loss: 1.37997460, g_loss: 0.68319482, ae_loss: 0.05220925\n",
      "Step: [4883] total_loss: 2.12626314 d_loss: 1.40029573, g_loss: 0.67784786, ae_loss: 0.04811938\n",
      "Step: [4884] total_loss: 2.12036228 d_loss: 1.36415553, g_loss: 0.70635569, ae_loss: 0.04985093\n",
      "Step: [4885] total_loss: 2.12557411 d_loss: 1.38882613, g_loss: 0.68354547, ae_loss: 0.05320247\n",
      "Step: [4886] total_loss: 2.12799072 d_loss: 1.38811374, g_loss: 0.68740267, ae_loss: 0.05247423\n",
      "Step: [4887] total_loss: 2.11587095 d_loss: 1.36149371, g_loss: 0.70432514, ae_loss: 0.05005208\n",
      "Step: [4888] total_loss: 2.11098766 d_loss: 1.38388693, g_loss: 0.67752355, ae_loss: 0.04957718\n",
      "Step: [4889] total_loss: 2.11938429 d_loss: 1.37314522, g_loss: 0.69851983, ae_loss: 0.04771916\n",
      "Step: [4890] total_loss: 2.12193346 d_loss: 1.37707543, g_loss: 0.69421279, ae_loss: 0.05064533\n",
      "Step: [4891] total_loss: 2.16383219 d_loss: 1.37960649, g_loss: 0.72953844, ae_loss: 0.05468713\n",
      "Step: [4892] total_loss: 2.11668968 d_loss: 1.37194300, g_loss: 0.69146293, ae_loss: 0.05328368\n",
      "Step: [4893] total_loss: 2.11539984 d_loss: 1.37898731, g_loss: 0.68359214, ae_loss: 0.05282046\n",
      "Step: [4894] total_loss: 2.10928607 d_loss: 1.36364555, g_loss: 0.69597590, ae_loss: 0.04966464\n",
      "Step: [4895] total_loss: 2.12147641 d_loss: 1.38712442, g_loss: 0.67744648, ae_loss: 0.05690556\n",
      "Step: [4896] total_loss: 2.12883186 d_loss: 1.38770199, g_loss: 0.69148362, ae_loss: 0.04964608\n",
      "Step: [4897] total_loss: 2.13205266 d_loss: 1.38081956, g_loss: 0.69879639, ae_loss: 0.05243674\n",
      "Step: [4898] total_loss: 2.12334681 d_loss: 1.37968528, g_loss: 0.69441783, ae_loss: 0.04924351\n",
      "Step: [4899] total_loss: 2.10721922 d_loss: 1.38447881, g_loss: 0.67131209, ae_loss: 0.05142815\n",
      "Step: [4900] total_loss: 2.12084579 d_loss: 1.37883592, g_loss: 0.69026589, ae_loss: 0.05174386\n",
      "Step: [4901] total_loss: 2.10903096 d_loss: 1.37039590, g_loss: 0.68870956, ae_loss: 0.04992554\n",
      "Step: [4902] total_loss: 2.13690448 d_loss: 1.40382528, g_loss: 0.68128717, ae_loss: 0.05179205\n",
      "Step: [4903] total_loss: 2.15600014 d_loss: 1.39222646, g_loss: 0.70775664, ae_loss: 0.05601713\n",
      "Step: [4904] total_loss: 2.13294268 d_loss: 1.39375877, g_loss: 0.68627357, ae_loss: 0.05291016\n",
      "Step: [4905] total_loss: 2.11239815 d_loss: 1.36490285, g_loss: 0.69740129, ae_loss: 0.05009392\n",
      "Step: [4906] total_loss: 2.12932873 d_loss: 1.38323855, g_loss: 0.69462454, ae_loss: 0.05146565\n",
      "Step: [4907] total_loss: 2.10060549 d_loss: 1.34488893, g_loss: 0.70256996, ae_loss: 0.05314659\n",
      "Step: [4908] total_loss: 2.14497805 d_loss: 1.41962206, g_loss: 0.67342877, ae_loss: 0.05192712\n",
      "Step: [4909] total_loss: 2.11270237 d_loss: 1.34057617, g_loss: 0.72121036, ae_loss: 0.05091574\n",
      "Step: [4910] total_loss: 2.12831759 d_loss: 1.39139915, g_loss: 0.68437576, ae_loss: 0.05254264\n",
      "Step: [4911] total_loss: 2.10354853 d_loss: 1.37128758, g_loss: 0.68233097, ae_loss: 0.04993002\n",
      "Step: [4912] total_loss: 2.12658501 d_loss: 1.38925624, g_loss: 0.68634808, ae_loss: 0.05098063\n",
      "Step: [4913] total_loss: 2.12956047 d_loss: 1.39366651, g_loss: 0.68706179, ae_loss: 0.04883228\n",
      "Step: [4914] total_loss: 2.12589097 d_loss: 1.39438570, g_loss: 0.67743456, ae_loss: 0.05407070\n",
      "Step: [4915] total_loss: 2.11203051 d_loss: 1.39071536, g_loss: 0.67027861, ae_loss: 0.05103642\n",
      "Step: [4916] total_loss: 2.10307336 d_loss: 1.36418414, g_loss: 0.68496180, ae_loss: 0.05392745\n",
      "Step: [4917] total_loss: 2.11032176 d_loss: 1.34521091, g_loss: 0.71371710, ae_loss: 0.05139372\n",
      "Step: [4918] total_loss: 2.10351181 d_loss: 1.37551165, g_loss: 0.67732447, ae_loss: 0.05067568\n",
      "Step: [4919] total_loss: 2.11938238 d_loss: 1.38902164, g_loss: 0.68157065, ae_loss: 0.04879018\n",
      "Step: [4920] total_loss: 2.12209821 d_loss: 1.39355052, g_loss: 0.68122196, ae_loss: 0.04732571\n",
      "Step: [4921] total_loss: 2.11969948 d_loss: 1.37806821, g_loss: 0.68562406, ae_loss: 0.05600721\n",
      "Step: [4922] total_loss: 2.12233877 d_loss: 1.33931255, g_loss: 0.73550957, ae_loss: 0.04751676\n",
      "Step: [4923] total_loss: 2.11314249 d_loss: 1.37165236, g_loss: 0.68775976, ae_loss: 0.05373045\n",
      "Step: [4924] total_loss: 2.15144730 d_loss: 1.40315366, g_loss: 0.69536364, ae_loss: 0.05293016\n",
      "Step: [4925] total_loss: 2.13683367 d_loss: 1.39165843, g_loss: 0.69249821, ae_loss: 0.05267698\n",
      "Step: [4926] total_loss: 2.13925457 d_loss: 1.37583435, g_loss: 0.70289755, ae_loss: 0.06052284\n",
      "Step: [4927] total_loss: 2.12668896 d_loss: 1.36110067, g_loss: 0.71346676, ae_loss: 0.05212160\n",
      "Step: [4928] total_loss: 2.12043738 d_loss: 1.37351096, g_loss: 0.69364929, ae_loss: 0.05327714\n",
      "Step: [4929] total_loss: 2.11937857 d_loss: 1.38397312, g_loss: 0.68724108, ae_loss: 0.04816446\n",
      "Step: [4930] total_loss: 2.12349463 d_loss: 1.38575625, g_loss: 0.68799543, ae_loss: 0.04974284\n",
      "Step: [4931] total_loss: 2.11845398 d_loss: 1.37234211, g_loss: 0.69980383, ae_loss: 0.04630819\n",
      "Step: [4932] total_loss: 2.14840889 d_loss: 1.39405453, g_loss: 0.70839334, ae_loss: 0.04596101\n",
      "Step: [4933] total_loss: 2.15299964 d_loss: 1.40273607, g_loss: 0.69938064, ae_loss: 0.05088297\n",
      "Step: [4934] total_loss: 2.13205147 d_loss: 1.38606763, g_loss: 0.69604456, ae_loss: 0.04993922\n",
      "Step: [4935] total_loss: 2.14413190 d_loss: 1.40015531, g_loss: 0.69083488, ae_loss: 0.05314172\n",
      "Step: [4936] total_loss: 2.12844038 d_loss: 1.38462591, g_loss: 0.69229448, ae_loss: 0.05151983\n",
      "Step: [4937] total_loss: 2.12432766 d_loss: 1.38832283, g_loss: 0.68664587, ae_loss: 0.04935910\n",
      "Step: [4938] total_loss: 2.12386608 d_loss: 1.38243914, g_loss: 0.69003224, ae_loss: 0.05139463\n",
      "Step: [4939] total_loss: 2.10804558 d_loss: 1.36246145, g_loss: 0.69403362, ae_loss: 0.05155065\n",
      "Step: [4940] total_loss: 2.11797714 d_loss: 1.38349414, g_loss: 0.68420851, ae_loss: 0.05027449\n",
      "Step: [4941] total_loss: 2.11768007 d_loss: 1.38270247, g_loss: 0.68685317, ae_loss: 0.04812450\n",
      "Step: [4942] total_loss: 2.14437652 d_loss: 1.37264991, g_loss: 0.72149903, ae_loss: 0.05022767\n",
      "Step: [4943] total_loss: 2.13408947 d_loss: 1.39772034, g_loss: 0.68129134, ae_loss: 0.05507795\n",
      "Step: [4944] total_loss: 2.12419558 d_loss: 1.38142180, g_loss: 0.69020319, ae_loss: 0.05257053\n",
      "Step: [4945] total_loss: 2.11652422 d_loss: 1.38609099, g_loss: 0.68450904, ae_loss: 0.04592428\n",
      "Step: [4946] total_loss: 2.12261200 d_loss: 1.37387753, g_loss: 0.69833130, ae_loss: 0.05040312\n",
      "Step: [4947] total_loss: 2.11507583 d_loss: 1.37452900, g_loss: 0.69328278, ae_loss: 0.04726396\n",
      "Step: [4948] total_loss: 2.12111568 d_loss: 1.38145876, g_loss: 0.68983877, ae_loss: 0.04981821\n",
      "Step: [4949] total_loss: 2.10905933 d_loss: 1.37277865, g_loss: 0.68571329, ae_loss: 0.05056740\n",
      "Step: [4950] total_loss: 2.12008905 d_loss: 1.35794401, g_loss: 0.71125859, ae_loss: 0.05088642\n",
      "Step: [4951] total_loss: 2.12533879 d_loss: 1.38381124, g_loss: 0.69166064, ae_loss: 0.04986689\n",
      "Step: [4952] total_loss: 2.14143872 d_loss: 1.38192439, g_loss: 0.70965070, ae_loss: 0.04986356\n",
      "Step: [4953] total_loss: 2.12790203 d_loss: 1.39127350, g_loss: 0.68580157, ae_loss: 0.05082689\n",
      "Step: [4954] total_loss: 2.13934064 d_loss: 1.39830291, g_loss: 0.68926209, ae_loss: 0.05177573\n",
      "Step: [4955] total_loss: 2.11104345 d_loss: 1.37684214, g_loss: 0.68390709, ae_loss: 0.05029416\n",
      "Step: [4956] total_loss: 2.11471343 d_loss: 1.36230505, g_loss: 0.70498574, ae_loss: 0.04742266\n",
      "Step: [4957] total_loss: 2.11124969 d_loss: 1.37027657, g_loss: 0.68898809, ae_loss: 0.05198506\n",
      "Step: [4958] total_loss: 2.12934446 d_loss: 1.41026664, g_loss: 0.66675210, ae_loss: 0.05232564\n",
      "Step: [4959] total_loss: 2.08942366 d_loss: 1.33628225, g_loss: 0.70394492, ae_loss: 0.04919657\n",
      "Step: [4960] total_loss: 2.14777327 d_loss: 1.37417746, g_loss: 0.71914172, ae_loss: 0.05445421\n",
      "Step: [4961] total_loss: 2.15084338 d_loss: 1.39213741, g_loss: 0.71076894, ae_loss: 0.04793709\n",
      "Step: [4962] total_loss: 2.14376593 d_loss: 1.38579273, g_loss: 0.70821834, ae_loss: 0.04975483\n",
      "Step: [4963] total_loss: 2.13587284 d_loss: 1.37877691, g_loss: 0.70253205, ae_loss: 0.05456387\n",
      "Step: [4964] total_loss: 2.16935945 d_loss: 1.41403437, g_loss: 0.70513749, ae_loss: 0.05018755\n",
      "Step: [4965] total_loss: 2.14632058 d_loss: 1.40289164, g_loss: 0.68961847, ae_loss: 0.05381051\n",
      "Step: [4966] total_loss: 2.11500144 d_loss: 1.38254666, g_loss: 0.68027008, ae_loss: 0.05218465\n",
      "Step: [4967] total_loss: 2.12696648 d_loss: 1.38394356, g_loss: 0.69056875, ae_loss: 0.05245411\n",
      "Step: [4968] total_loss: 2.12070847 d_loss: 1.39172220, g_loss: 0.67862755, ae_loss: 0.05035863\n",
      "Step: [4969] total_loss: 2.11318398 d_loss: 1.37524748, g_loss: 0.68812704, ae_loss: 0.04980947\n",
      "Step: [4970] total_loss: 2.12244225 d_loss: 1.37732482, g_loss: 0.69133413, ae_loss: 0.05378341\n",
      "Step: [4971] total_loss: 2.14483905 d_loss: 1.38490677, g_loss: 0.70756298, ae_loss: 0.05236934\n",
      "Step: [4972] total_loss: 2.12857723 d_loss: 1.38284683, g_loss: 0.69574779, ae_loss: 0.04998258\n",
      "Step: [4973] total_loss: 2.13006496 d_loss: 1.38863528, g_loss: 0.68935275, ae_loss: 0.05207703\n",
      "Step: [4974] total_loss: 2.13252497 d_loss: 1.39136600, g_loss: 0.68896300, ae_loss: 0.05219606\n",
      "Step: [4975] total_loss: 2.12340307 d_loss: 1.38630772, g_loss: 0.68673384, ae_loss: 0.05036154\n",
      "Step: [4976] total_loss: 2.13863850 d_loss: 1.37229288, g_loss: 0.71131206, ae_loss: 0.05503355\n",
      "Step: [4977] total_loss: 2.12261248 d_loss: 1.38448906, g_loss: 0.69030237, ae_loss: 0.04782121\n",
      "Step: [4978] total_loss: 2.12199163 d_loss: 1.37754273, g_loss: 0.69498807, ae_loss: 0.04946093\n",
      "Step: [4979] total_loss: 2.12076998 d_loss: 1.38646269, g_loss: 0.68032527, ae_loss: 0.05398187\n",
      "Step: [4980] total_loss: 2.09697723 d_loss: 1.37212443, g_loss: 0.67326975, ae_loss: 0.05158309\n",
      "Step: [4981] total_loss: 2.10665107 d_loss: 1.37704015, g_loss: 0.68033403, ae_loss: 0.04927697\n",
      "Step: [4982] total_loss: 2.10224247 d_loss: 1.34309149, g_loss: 0.70798612, ae_loss: 0.05116484\n",
      "Step: [4983] total_loss: 2.10131621 d_loss: 1.35325885, g_loss: 0.69753146, ae_loss: 0.05052589\n",
      "Step: [4984] total_loss: 2.13796473 d_loss: 1.38845944, g_loss: 0.69549036, ae_loss: 0.05401491\n",
      "Step: [4985] total_loss: 2.12323141 d_loss: 1.38902378, g_loss: 0.68252230, ae_loss: 0.05168528\n",
      "Step: [4986] total_loss: 2.10803556 d_loss: 1.36730838, g_loss: 0.68488657, ae_loss: 0.05584076\n",
      "Step: [4987] total_loss: 2.13004088 d_loss: 1.39618230, g_loss: 0.68139744, ae_loss: 0.05246118\n",
      "Step: [4988] total_loss: 2.11108732 d_loss: 1.36000228, g_loss: 0.69695055, ae_loss: 0.05413451\n",
      "Step: [4989] total_loss: 2.11154819 d_loss: 1.38250494, g_loss: 0.68007219, ae_loss: 0.04897106\n",
      "Step: [4990] total_loss: 2.11572361 d_loss: 1.38351536, g_loss: 0.68294919, ae_loss: 0.04925923\n",
      "Step: [4991] total_loss: 2.14690065 d_loss: 1.40288162, g_loss: 0.68876004, ae_loss: 0.05525913\n",
      "Step: [4992] total_loss: 2.12358522 d_loss: 1.38848746, g_loss: 0.68223941, ae_loss: 0.05285835\n",
      "Step: [4993] total_loss: 2.11647916 d_loss: 1.37552500, g_loss: 0.68961883, ae_loss: 0.05133537\n",
      "Step: [4994] total_loss: 2.13203859 d_loss: 1.38053548, g_loss: 0.69915128, ae_loss: 0.05235190\n",
      "Step: [4995] total_loss: 2.12937760 d_loss: 1.38012218, g_loss: 0.69797403, ae_loss: 0.05128129\n",
      "Step: [4996] total_loss: 2.10700321 d_loss: 1.36726916, g_loss: 0.68725210, ae_loss: 0.05248204\n",
      "Step: [4997] total_loss: 2.14576674 d_loss: 1.39818621, g_loss: 0.69402266, ae_loss: 0.05355771\n",
      "Step: [4998] total_loss: 2.12578964 d_loss: 1.37414515, g_loss: 0.70131063, ae_loss: 0.05033400\n",
      "Step: [4999] total_loss: 2.13276863 d_loss: 1.38604164, g_loss: 0.69649470, ae_loss: 0.05023231\n",
      "Step: [5000] total_loss: 2.11111617 d_loss: 1.36884141, g_loss: 0.69228500, ae_loss: 0.04998970\n",
      "Step: [5001] total_loss: 2.13277841 d_loss: 1.38623321, g_loss: 0.69662827, ae_loss: 0.04991683\n",
      "Step: [5002] total_loss: 2.12916446 d_loss: 1.35271442, g_loss: 0.72409487, ae_loss: 0.05235522\n",
      "Step: [5003] total_loss: 2.12580156 d_loss: 1.40514994, g_loss: 0.66906774, ae_loss: 0.05158379\n",
      "Step: [5004] total_loss: 2.11329722 d_loss: 1.35765529, g_loss: 0.69917542, ae_loss: 0.05646649\n",
      "Step: [5005] total_loss: 2.13345098 d_loss: 1.39739847, g_loss: 0.68557465, ae_loss: 0.05047801\n",
      "Step: [5006] total_loss: 2.13157701 d_loss: 1.39636660, g_loss: 0.68416309, ae_loss: 0.05104742\n",
      "Step: [5007] total_loss: 2.09987354 d_loss: 1.36026084, g_loss: 0.69353116, ae_loss: 0.04608149\n",
      "Step: [5008] total_loss: 2.11364126 d_loss: 1.36764526, g_loss: 0.69580758, ae_loss: 0.05018825\n",
      "Step: [5009] total_loss: 2.12656975 d_loss: 1.38089895, g_loss: 0.69270742, ae_loss: 0.05296343\n",
      "Step: [5010] total_loss: 2.14178133 d_loss: 1.39249980, g_loss: 0.69993258, ae_loss: 0.04934880\n",
      "Step: [5011] total_loss: 2.12727976 d_loss: 1.38763762, g_loss: 0.68880963, ae_loss: 0.05083252\n",
      "Step: [5012] total_loss: 2.14203310 d_loss: 1.39434373, g_loss: 0.69765985, ae_loss: 0.05002954\n",
      "Step: [5013] total_loss: 2.11752176 d_loss: 1.37952900, g_loss: 0.68692350, ae_loss: 0.05106938\n",
      "Step: [5014] total_loss: 2.10014868 d_loss: 1.37181902, g_loss: 0.67369777, ae_loss: 0.05463181\n",
      "Step: [5015] total_loss: 2.12633228 d_loss: 1.38797045, g_loss: 0.68747771, ae_loss: 0.05088418\n",
      "Step: [5016] total_loss: 2.12196565 d_loss: 1.37822938, g_loss: 0.69374996, ae_loss: 0.04998637\n",
      "Step: [5017] total_loss: 2.14421177 d_loss: 1.38800621, g_loss: 0.70578492, ae_loss: 0.05042063\n",
      "Step: [5018] total_loss: 2.11507034 d_loss: 1.37551141, g_loss: 0.68992960, ae_loss: 0.04962949\n",
      "Step: [5019] total_loss: 2.13750172 d_loss: 1.40557051, g_loss: 0.67825675, ae_loss: 0.05367440\n",
      "Step: [5020] total_loss: 2.11634827 d_loss: 1.38654435, g_loss: 0.68098879, ae_loss: 0.04881527\n",
      "Step: [5021] total_loss: 2.10624576 d_loss: 1.36543512, g_loss: 0.69308102, ae_loss: 0.04772965\n",
      "Step: [5022] total_loss: 2.12584949 d_loss: 1.37022305, g_loss: 0.70717192, ae_loss: 0.04845458\n",
      "Step: [5023] total_loss: 2.13834786 d_loss: 1.36970854, g_loss: 0.71767879, ae_loss: 0.05096057\n",
      "Step: [5024] total_loss: 2.12415171 d_loss: 1.39241076, g_loss: 0.67584622, ae_loss: 0.05589463\n",
      "Step: [5025] total_loss: 2.12968063 d_loss: 1.37064123, g_loss: 0.71023309, ae_loss: 0.04880637\n",
      "Step: [5026] total_loss: 2.11806107 d_loss: 1.37390912, g_loss: 0.69328046, ae_loss: 0.05087141\n",
      "Step: [5027] total_loss: 2.10600805 d_loss: 1.36778772, g_loss: 0.68494236, ae_loss: 0.05327800\n",
      "Step: [5028] total_loss: 2.12053156 d_loss: 1.37141919, g_loss: 0.69677448, ae_loss: 0.05233777\n",
      "Step: [5029] total_loss: 2.13240147 d_loss: 1.40884411, g_loss: 0.67515492, ae_loss: 0.04840256\n",
      "Step: [5030] total_loss: 2.12234211 d_loss: 1.37263775, g_loss: 0.69600236, ae_loss: 0.05370193\n",
      "Step: [5031] total_loss: 2.12339282 d_loss: 1.37102640, g_loss: 0.70244956, ae_loss: 0.04991690\n",
      "Step: [5032] total_loss: 2.12952518 d_loss: 1.38424623, g_loss: 0.69502139, ae_loss: 0.05025742\n",
      "Step: [5033] total_loss: 2.14011669 d_loss: 1.37733936, g_loss: 0.71490514, ae_loss: 0.04787232\n",
      "Step: [5034] total_loss: 2.13334179 d_loss: 1.39939952, g_loss: 0.68729025, ae_loss: 0.04665212\n",
      "Step: [5035] total_loss: 2.14421606 d_loss: 1.39019966, g_loss: 0.70331645, ae_loss: 0.05069987\n",
      "Step: [5036] total_loss: 2.12032938 d_loss: 1.37731612, g_loss: 0.69479120, ae_loss: 0.04822203\n",
      "Step: [5037] total_loss: 2.10199022 d_loss: 1.37083960, g_loss: 0.67963898, ae_loss: 0.05151167\n",
      "Step: [5038] total_loss: 2.12172079 d_loss: 1.34464145, g_loss: 0.72703826, ae_loss: 0.05004092\n",
      "Step: [5039] total_loss: 2.11389160 d_loss: 1.38802910, g_loss: 0.67352962, ae_loss: 0.05233276\n",
      "Step: [5040] total_loss: 2.11948848 d_loss: 1.37079549, g_loss: 0.70093721, ae_loss: 0.04775581\n",
      "Step: [5041] total_loss: 2.15272617 d_loss: 1.39676070, g_loss: 0.70478702, ae_loss: 0.05117843\n",
      "Step: [5042] total_loss: 2.13635349 d_loss: 1.37954569, g_loss: 0.70561445, ae_loss: 0.05119330\n",
      "Step: [5043] total_loss: 2.12223601 d_loss: 1.38876820, g_loss: 0.68418127, ae_loss: 0.04928652\n",
      "Step: [5044] total_loss: 2.11148214 d_loss: 1.37731552, g_loss: 0.68519700, ae_loss: 0.04896956\n",
      "Step: [5045] total_loss: 2.12195182 d_loss: 1.38241470, g_loss: 0.68605447, ae_loss: 0.05348263\n",
      "Step: [5046] total_loss: 2.14477491 d_loss: 1.36022902, g_loss: 0.73291063, ae_loss: 0.05163513\n",
      "Step: [5047] total_loss: 2.12226486 d_loss: 1.37681782, g_loss: 0.69844449, ae_loss: 0.04700242\n",
      "Step: [5048] total_loss: 2.11316919 d_loss: 1.37619495, g_loss: 0.68361074, ae_loss: 0.05336351\n",
      "Step: [5049] total_loss: 2.10551214 d_loss: 1.38408363, g_loss: 0.67204976, ae_loss: 0.04937892\n",
      "Step: [5050] total_loss: 2.12174511 d_loss: 1.37226188, g_loss: 0.70042020, ae_loss: 0.04906302\n",
      "Step: [5051] total_loss: 2.12000990 d_loss: 1.39299345, g_loss: 0.67477596, ae_loss: 0.05224043\n",
      "Step: [5052] total_loss: 2.14630842 d_loss: 1.40488553, g_loss: 0.68676734, ae_loss: 0.05465550\n",
      "Step: [5053] total_loss: 2.12185740 d_loss: 1.35374427, g_loss: 0.71611327, ae_loss: 0.05199979\n",
      "Step: [5054] total_loss: 2.14340448 d_loss: 1.39255869, g_loss: 0.69758284, ae_loss: 0.05326298\n",
      "Step: [5055] total_loss: 2.12694502 d_loss: 1.38981652, g_loss: 0.68605417, ae_loss: 0.05107434\n",
      "Step: [5056] total_loss: 2.12098551 d_loss: 1.36948967, g_loss: 0.70205677, ae_loss: 0.04943924\n",
      "Step: [5057] total_loss: 2.13676834 d_loss: 1.39141142, g_loss: 0.69008672, ae_loss: 0.05527017\n",
      "Step: [5058] total_loss: 2.13168120 d_loss: 1.38507748, g_loss: 0.69572031, ae_loss: 0.05088339\n",
      "Step: [5059] total_loss: 2.12985373 d_loss: 1.39481902, g_loss: 0.68269348, ae_loss: 0.05234135\n",
      "Step: [5060] total_loss: 2.11676192 d_loss: 1.36829162, g_loss: 0.70041823, ae_loss: 0.04805204\n",
      "Step: [5061] total_loss: 2.12781024 d_loss: 1.36983943, g_loss: 0.71002537, ae_loss: 0.04794547\n",
      "Step: [5062] total_loss: 2.11386323 d_loss: 1.39140093, g_loss: 0.67775291, ae_loss: 0.04470944\n",
      "Step: [5063] total_loss: 2.12072444 d_loss: 1.38227773, g_loss: 0.68818551, ae_loss: 0.05026113\n",
      "Step: [5064] total_loss: 2.16100883 d_loss: 1.38889575, g_loss: 0.71784651, ae_loss: 0.05426648\n",
      "Step: [5065] total_loss: 2.10802889 d_loss: 1.37592578, g_loss: 0.67913204, ae_loss: 0.05297114\n",
      "Step: [5066] total_loss: 2.10560799 d_loss: 1.37462556, g_loss: 0.67960358, ae_loss: 0.05137889\n",
      "Step: [5067] total_loss: 2.13317394 d_loss: 1.38013089, g_loss: 0.70116723, ae_loss: 0.05187566\n",
      "Step: [5068] total_loss: 2.13249421 d_loss: 1.38505995, g_loss: 0.69767576, ae_loss: 0.04975852\n",
      "Step: [5069] total_loss: 2.11254692 d_loss: 1.36401141, g_loss: 0.69273257, ae_loss: 0.05580296\n",
      "Step: [5070] total_loss: 2.11476898 d_loss: 1.36820889, g_loss: 0.69604611, ae_loss: 0.05051401\n",
      "Step: [5071] total_loss: 2.13354945 d_loss: 1.39232659, g_loss: 0.68781215, ae_loss: 0.05341074\n",
      "Step: [5072] total_loss: 2.12032270 d_loss: 1.38219714, g_loss: 0.68525630, ae_loss: 0.05286935\n",
      "Step: [5073] total_loss: 2.10920811 d_loss: 1.34722209, g_loss: 0.71104276, ae_loss: 0.05094314\n",
      "Step: [5074] total_loss: 2.12423992 d_loss: 1.39418614, g_loss: 0.67619216, ae_loss: 0.05386160\n",
      "Step: [5075] total_loss: 2.11338186 d_loss: 1.36383986, g_loss: 0.69490695, ae_loss: 0.05463493\n",
      "Step: [5076] total_loss: 2.12557602 d_loss: 1.38199425, g_loss: 0.69654769, ae_loss: 0.04703397\n",
      "Step: [5077] total_loss: 2.12676382 d_loss: 1.37052286, g_loss: 0.70421183, ae_loss: 0.05202923\n",
      "Step: [5078] total_loss: 2.10227156 d_loss: 1.36249566, g_loss: 0.69176179, ae_loss: 0.04801421\n",
      "Step: [5079] total_loss: 2.15203738 d_loss: 1.39449954, g_loss: 0.70703322, ae_loss: 0.05050462\n",
      "Step: [5080] total_loss: 2.13454676 d_loss: 1.38557386, g_loss: 0.69777828, ae_loss: 0.05119457\n",
      "Step: [5081] total_loss: 2.11347556 d_loss: 1.34745097, g_loss: 0.71192235, ae_loss: 0.05410235\n",
      "Step: [5082] total_loss: 2.12762737 d_loss: 1.37232447, g_loss: 0.70768917, ae_loss: 0.04761384\n",
      "Step: [5083] total_loss: 2.12864017 d_loss: 1.38228238, g_loss: 0.69224072, ae_loss: 0.05411705\n",
      "Step: [5084] total_loss: 2.14412141 d_loss: 1.40193558, g_loss: 0.68939447, ae_loss: 0.05279136\n",
      "Step: [5085] total_loss: 2.11095929 d_loss: 1.37546670, g_loss: 0.68169498, ae_loss: 0.05379756\n",
      "Step: [5086] total_loss: 2.13585305 d_loss: 1.38693500, g_loss: 0.69499516, ae_loss: 0.05392290\n",
      "Step: [5087] total_loss: 2.14051771 d_loss: 1.39485931, g_loss: 0.69339991, ae_loss: 0.05225835\n",
      "Step: [5088] total_loss: 2.11326337 d_loss: 1.36856437, g_loss: 0.69991738, ae_loss: 0.04478174\n",
      "Step: [5089] total_loss: 2.13498807 d_loss: 1.39756465, g_loss: 0.68626195, ae_loss: 0.05116148\n",
      "Step: [5090] total_loss: 2.13865948 d_loss: 1.38733411, g_loss: 0.70392728, ae_loss: 0.04739822\n",
      "Step: [5091] total_loss: 2.12900257 d_loss: 1.39370537, g_loss: 0.68378639, ae_loss: 0.05151079\n",
      "Step: [5092] total_loss: 2.14314795 d_loss: 1.40034628, g_loss: 0.68804216, ae_loss: 0.05475944\n",
      "Step: [5093] total_loss: 2.13048792 d_loss: 1.40317237, g_loss: 0.67800820, ae_loss: 0.04930744\n",
      "Step: [5094] total_loss: 2.11991143 d_loss: 1.38406062, g_loss: 0.68625760, ae_loss: 0.04959316\n",
      "Step: [5095] total_loss: 2.14356995 d_loss: 1.37337494, g_loss: 0.72208571, ae_loss: 0.04810930\n",
      "Step: [5096] total_loss: 2.12373996 d_loss: 1.38530540, g_loss: 0.69050086, ae_loss: 0.04793375\n",
      "Step: [5097] total_loss: 2.14145398 d_loss: 1.37466860, g_loss: 0.71635008, ae_loss: 0.05043527\n",
      "Step: [5098] total_loss: 2.12932491 d_loss: 1.38840103, g_loss: 0.69066715, ae_loss: 0.05025672\n",
      "Step: [5099] total_loss: 2.12568140 d_loss: 1.38377237, g_loss: 0.69097054, ae_loss: 0.05093832\n",
      "Step: [5100] total_loss: 2.14816356 d_loss: 1.38526464, g_loss: 0.71092314, ae_loss: 0.05197578\n",
      "Step: [5101] total_loss: 2.14731479 d_loss: 1.39590216, g_loss: 0.70215231, ae_loss: 0.04926043\n",
      "Step: [5102] total_loss: 2.11729455 d_loss: 1.36011553, g_loss: 0.70249331, ae_loss: 0.05468567\n",
      "Step: [5103] total_loss: 2.11916780 d_loss: 1.37957835, g_loss: 0.68950987, ae_loss: 0.05007957\n",
      "Step: [5104] total_loss: 2.13503098 d_loss: 1.39855409, g_loss: 0.68871421, ae_loss: 0.04776259\n",
      "Step: [5105] total_loss: 2.11773682 d_loss: 1.38414836, g_loss: 0.68152058, ae_loss: 0.05206791\n",
      "Step: [5106] total_loss: 2.13103771 d_loss: 1.38786495, g_loss: 0.69611669, ae_loss: 0.04705593\n",
      "Step: [5107] total_loss: 2.10736823 d_loss: 1.37717843, g_loss: 0.67888689, ae_loss: 0.05130291\n",
      "Step: [5108] total_loss: 2.10145903 d_loss: 1.35570908, g_loss: 0.69857121, ae_loss: 0.04717876\n",
      "Step: [5109] total_loss: 2.11211538 d_loss: 1.38304448, g_loss: 0.67703652, ae_loss: 0.05203426\n",
      "Step: [5110] total_loss: 2.10001826 d_loss: 1.37052560, g_loss: 0.67994916, ae_loss: 0.04954352\n",
      "Step: [5111] total_loss: 2.10519361 d_loss: 1.37561262, g_loss: 0.68090940, ae_loss: 0.04867156\n",
      "Step: [5112] total_loss: 2.10758758 d_loss: 1.38723016, g_loss: 0.66842389, ae_loss: 0.05193352\n",
      "Step: [5113] total_loss: 2.14056134 d_loss: 1.38818419, g_loss: 0.69977611, ae_loss: 0.05260108\n",
      "Step: [5114] total_loss: 2.12394571 d_loss: 1.38509703, g_loss: 0.68821073, ae_loss: 0.05063809\n",
      "Step: [5115] total_loss: 2.11128497 d_loss: 1.36425042, g_loss: 0.70172977, ae_loss: 0.04530476\n",
      "Step: [5116] total_loss: 2.11955118 d_loss: 1.38845837, g_loss: 0.67963445, ae_loss: 0.05145847\n",
      "Step: [5117] total_loss: 2.12668681 d_loss: 1.36010790, g_loss: 0.71239150, ae_loss: 0.05418746\n",
      "Step: [5118] total_loss: 2.15127993 d_loss: 1.38258421, g_loss: 0.71424603, ae_loss: 0.05444951\n",
      "Step: [5119] total_loss: 2.11236978 d_loss: 1.36725426, g_loss: 0.69492030, ae_loss: 0.05019523\n",
      "Step: [5120] total_loss: 2.14709568 d_loss: 1.38358176, g_loss: 0.71010643, ae_loss: 0.05340759\n",
      "Step: [5121] total_loss: 2.13470602 d_loss: 1.39430332, g_loss: 0.68934184, ae_loss: 0.05106095\n",
      "Step: [5122] total_loss: 2.10930371 d_loss: 1.37952292, g_loss: 0.67518759, ae_loss: 0.05459316\n",
      "Step: [5123] total_loss: 2.12972784 d_loss: 1.38446474, g_loss: 0.69713926, ae_loss: 0.04812375\n",
      "Step: [5124] total_loss: 2.12314725 d_loss: 1.39187467, g_loss: 0.68144590, ae_loss: 0.04982674\n",
      "Step: [5125] total_loss: 2.15530825 d_loss: 1.39446974, g_loss: 0.70458865, ae_loss: 0.05624976\n",
      "Step: [5126] total_loss: 2.14166808 d_loss: 1.40220118, g_loss: 0.68433338, ae_loss: 0.05513346\n",
      "Step: [5127] total_loss: 2.13773012 d_loss: 1.38821828, g_loss: 0.69874722, ae_loss: 0.05076452\n",
      "Step: [5128] total_loss: 2.14288616 d_loss: 1.35511303, g_loss: 0.73552275, ae_loss: 0.05225050\n",
      "Step: [5129] total_loss: 2.14460897 d_loss: 1.39314365, g_loss: 0.69458461, ae_loss: 0.05688070\n",
      "Step: [5130] total_loss: 2.15120983 d_loss: 1.40195918, g_loss: 0.69931757, ae_loss: 0.04993305\n",
      "Step: [5131] total_loss: 2.13999128 d_loss: 1.38029838, g_loss: 0.70583940, ae_loss: 0.05385353\n",
      "Step: [5132] total_loss: 2.13822412 d_loss: 1.41580951, g_loss: 0.67140996, ae_loss: 0.05100460\n",
      "Step: [5133] total_loss: 2.11149931 d_loss: 1.37967229, g_loss: 0.67750192, ae_loss: 0.05432517\n",
      "Step: [5134] total_loss: 2.10226560 d_loss: 1.36717248, g_loss: 0.68675125, ae_loss: 0.04834184\n",
      "Step: [5135] total_loss: 2.11650229 d_loss: 1.37778640, g_loss: 0.68774700, ae_loss: 0.05096890\n",
      "Step: [5136] total_loss: 2.12833595 d_loss: 1.37231541, g_loss: 0.70528620, ae_loss: 0.05073443\n",
      "Step: [5137] total_loss: 2.13326406 d_loss: 1.40028739, g_loss: 0.67959028, ae_loss: 0.05338646\n",
      "Step: [5138] total_loss: 2.12329531 d_loss: 1.37417412, g_loss: 0.70093966, ae_loss: 0.04818154\n",
      "Step: [5139] total_loss: 2.09839487 d_loss: 1.36094892, g_loss: 0.68783295, ae_loss: 0.04961309\n",
      "Step: [5140] total_loss: 2.10530901 d_loss: 1.37186146, g_loss: 0.68139440, ae_loss: 0.05205317\n",
      "Step: [5141] total_loss: 2.11022139 d_loss: 1.38189495, g_loss: 0.67506397, ae_loss: 0.05326235\n",
      "Step: [5142] total_loss: 2.11159134 d_loss: 1.37867320, g_loss: 0.68532741, ae_loss: 0.04759077\n",
      "Step: [5143] total_loss: 2.14342117 d_loss: 1.36040401, g_loss: 0.73314250, ae_loss: 0.04987475\n",
      "Step: [5144] total_loss: 2.11749125 d_loss: 1.38435698, g_loss: 0.68467611, ae_loss: 0.04845818\n",
      "Step: [5145] total_loss: 2.12052536 d_loss: 1.37387109, g_loss: 0.70021832, ae_loss: 0.04643593\n",
      "Step: [5146] total_loss: 2.16868210 d_loss: 1.38677549, g_loss: 0.73000258, ae_loss: 0.05190396\n",
      "Step: [5147] total_loss: 2.13925600 d_loss: 1.38647175, g_loss: 0.70083869, ae_loss: 0.05194543\n",
      "Step: [5148] total_loss: 2.11615419 d_loss: 1.35543013, g_loss: 0.71207070, ae_loss: 0.04865327\n",
      "Step: [5149] total_loss: 2.12007904 d_loss: 1.36612844, g_loss: 0.70060927, ae_loss: 0.05334131\n",
      "Step: [5150] total_loss: 2.12804842 d_loss: 1.38062501, g_loss: 0.69935942, ae_loss: 0.04806384\n",
      "Step: [5151] total_loss: 2.10283756 d_loss: 1.38111067, g_loss: 0.66630781, ae_loss: 0.05541924\n",
      "Step: [5152] total_loss: 2.10779977 d_loss: 1.37171042, g_loss: 0.68691075, ae_loss: 0.04917857\n",
      "Step: [5153] total_loss: 2.11622238 d_loss: 1.36390877, g_loss: 0.70096874, ae_loss: 0.05134474\n",
      "Step: [5154] total_loss: 2.11165881 d_loss: 1.35510361, g_loss: 0.70705807, ae_loss: 0.04949715\n",
      "Step: [5155] total_loss: 2.12316132 d_loss: 1.37686729, g_loss: 0.69607049, ae_loss: 0.05022356\n",
      "Step: [5156] total_loss: 2.12647414 d_loss: 1.34691453, g_loss: 0.73275781, ae_loss: 0.04680184\n",
      "Step: [5157] total_loss: 2.13349915 d_loss: 1.39527798, g_loss: 0.68919498, ae_loss: 0.04902628\n",
      "Step: [5158] total_loss: 2.12932444 d_loss: 1.39133108, g_loss: 0.68546116, ae_loss: 0.05253207\n",
      "Step: [5159] total_loss: 2.12108254 d_loss: 1.38787556, g_loss: 0.68248296, ae_loss: 0.05072405\n",
      "Step: [5160] total_loss: 2.11701155 d_loss: 1.39169753, g_loss: 0.67704773, ae_loss: 0.04826614\n",
      "Step: [5161] total_loss: 2.16483068 d_loss: 1.42154443, g_loss: 0.69563401, ae_loss: 0.04765237\n",
      "Step: [5162] total_loss: 2.13543272 d_loss: 1.38719463, g_loss: 0.69880104, ae_loss: 0.04943692\n",
      "Step: [5163] total_loss: 2.11879134 d_loss: 1.37162876, g_loss: 0.69602191, ae_loss: 0.05114068\n",
      "Step: [5164] total_loss: 2.12424231 d_loss: 1.37762320, g_loss: 0.69883060, ae_loss: 0.04778840\n",
      "Step: [5165] total_loss: 2.13747525 d_loss: 1.39811873, g_loss: 0.68808192, ae_loss: 0.05127456\n",
      "Step: [5166] total_loss: 2.12061262 d_loss: 1.38367105, g_loss: 0.68763196, ae_loss: 0.04930954\n",
      "Step: [5167] total_loss: 2.13512230 d_loss: 1.40242887, g_loss: 0.68151414, ae_loss: 0.05117932\n",
      "Step: [5168] total_loss: 2.12025976 d_loss: 1.37559724, g_loss: 0.69400835, ae_loss: 0.05065422\n",
      "Step: [5169] total_loss: 2.13885975 d_loss: 1.38983667, g_loss: 0.69918764, ae_loss: 0.04983551\n",
      "Step: [5170] total_loss: 2.12399578 d_loss: 1.37421894, g_loss: 0.69448709, ae_loss: 0.05528967\n",
      "Step: [5171] total_loss: 2.10482168 d_loss: 1.36771369, g_loss: 0.68516028, ae_loss: 0.05194786\n",
      "Step: [5172] total_loss: 2.14454126 d_loss: 1.41502452, g_loss: 0.67809427, ae_loss: 0.05142255\n",
      "Step: [5173] total_loss: 2.13068986 d_loss: 1.39499927, g_loss: 0.68367594, ae_loss: 0.05201467\n",
      "Step: [5174] total_loss: 2.11799026 d_loss: 1.38282824, g_loss: 0.68345457, ae_loss: 0.05170736\n",
      "Step: [5175] total_loss: 2.12165737 d_loss: 1.37792814, g_loss: 0.69556439, ae_loss: 0.04816500\n",
      "Step: [5176] total_loss: 2.14247966 d_loss: 1.40585756, g_loss: 0.68673038, ae_loss: 0.04989166\n",
      "Step: [5177] total_loss: 2.11545610 d_loss: 1.38209307, g_loss: 0.68268847, ae_loss: 0.05067444\n",
      "Step: [5178] total_loss: 2.15690136 d_loss: 1.41390193, g_loss: 0.69041467, ae_loss: 0.05258467\n",
      "Step: [5179] total_loss: 2.12287903 d_loss: 1.36868882, g_loss: 0.69872808, ae_loss: 0.05546227\n",
      "Step: [5180] total_loss: 2.14657235 d_loss: 1.36032283, g_loss: 0.73373365, ae_loss: 0.05251586\n",
      "Step: [5181] total_loss: 2.12206411 d_loss: 1.36449194, g_loss: 0.70823747, ae_loss: 0.04933481\n",
      "Step: [5182] total_loss: 2.12981987 d_loss: 1.38741183, g_loss: 0.69369173, ae_loss: 0.04871624\n",
      "Step: [5183] total_loss: 2.12254620 d_loss: 1.37819600, g_loss: 0.69816756, ae_loss: 0.04618277\n",
      "Step: [5184] total_loss: 2.13550282 d_loss: 1.39876580, g_loss: 0.68001556, ae_loss: 0.05672144\n",
      "Step: [5185] total_loss: 2.11869526 d_loss: 1.38021708, g_loss: 0.68875849, ae_loss: 0.04971953\n",
      "Step: [5186] total_loss: 2.10450840 d_loss: 1.38273358, g_loss: 0.66721630, ae_loss: 0.05455839\n",
      "Step: [5187] total_loss: 2.15643787 d_loss: 1.39224458, g_loss: 0.71061254, ae_loss: 0.05358068\n",
      "Step: [5188] total_loss: 2.14005518 d_loss: 1.40357828, g_loss: 0.68311143, ae_loss: 0.05336537\n",
      "Step: [5189] total_loss: 2.12976980 d_loss: 1.37267804, g_loss: 0.70421839, ae_loss: 0.05287341\n",
      "Step: [5190] total_loss: 2.13085985 d_loss: 1.38926780, g_loss: 0.69123471, ae_loss: 0.05035750\n",
      "Step: [5191] total_loss: 2.12079287 d_loss: 1.38641334, g_loss: 0.67865366, ae_loss: 0.05572578\n",
      "Step: [5192] total_loss: 2.12324977 d_loss: 1.37615919, g_loss: 0.69311661, ae_loss: 0.05397404\n",
      "Step: [5193] total_loss: 2.10645914 d_loss: 1.38247657, g_loss: 0.67319965, ae_loss: 0.05078284\n",
      "Step: [5194] total_loss: 2.11878610 d_loss: 1.36609197, g_loss: 0.69918191, ae_loss: 0.05351224\n",
      "Step: [5195] total_loss: 2.13113618 d_loss: 1.40457511, g_loss: 0.68264234, ae_loss: 0.04391872\n",
      "Step: [5196] total_loss: 2.12066841 d_loss: 1.36763024, g_loss: 0.70492971, ae_loss: 0.04810858\n",
      "Step: [5197] total_loss: 2.14812326 d_loss: 1.38522589, g_loss: 0.71337688, ae_loss: 0.04952045\n",
      "Step: [5198] total_loss: 2.13279438 d_loss: 1.37845409, g_loss: 0.70238435, ae_loss: 0.05195589\n",
      "Step: [5199] total_loss: 2.11191416 d_loss: 1.37015605, g_loss: 0.69169313, ae_loss: 0.05006490\n",
      "Step: [5200] total_loss: 2.12890482 d_loss: 1.39145064, g_loss: 0.68664491, ae_loss: 0.05080911\n",
      "Step: [5201] total_loss: 2.10584235 d_loss: 1.37005138, g_loss: 0.68778932, ae_loss: 0.04800159\n",
      "Step: [5202] total_loss: 2.13440609 d_loss: 1.39629030, g_loss: 0.68883812, ae_loss: 0.04927773\n",
      "Step: [5203] total_loss: 2.12689137 d_loss: 1.37945092, g_loss: 0.69767576, ae_loss: 0.04976474\n",
      "Step: [5204] total_loss: 2.12149692 d_loss: 1.37570250, g_loss: 0.69836617, ae_loss: 0.04742829\n",
      "Step: [5205] total_loss: 2.13347864 d_loss: 1.38777971, g_loss: 0.69180334, ae_loss: 0.05389554\n",
      "Step: [5206] total_loss: 2.13241339 d_loss: 1.39249158, g_loss: 0.69085646, ae_loss: 0.04906521\n",
      "Step: [5207] total_loss: 2.12095451 d_loss: 1.38573289, g_loss: 0.68468344, ae_loss: 0.05053812\n",
      "Step: [5208] total_loss: 2.13295412 d_loss: 1.36945128, g_loss: 0.71170855, ae_loss: 0.05179422\n",
      "Step: [5209] total_loss: 2.15041161 d_loss: 1.40745461, g_loss: 0.68758029, ae_loss: 0.05537666\n",
      "Step: [5210] total_loss: 2.10245109 d_loss: 1.35633373, g_loss: 0.69638634, ae_loss: 0.04973105\n",
      "Step: [5211] total_loss: 2.12099075 d_loss: 1.38379025, g_loss: 0.69037753, ae_loss: 0.04682286\n",
      "Step: [5212] total_loss: 2.12894440 d_loss: 1.39934158, g_loss: 0.68107343, ae_loss: 0.04852955\n",
      "Step: [5213] total_loss: 2.12384295 d_loss: 1.36801934, g_loss: 0.70537657, ae_loss: 0.05044710\n",
      "Step: [5214] total_loss: 2.13475704 d_loss: 1.41237378, g_loss: 0.67110360, ae_loss: 0.05127963\n",
      "Step: [5215] total_loss: 2.15744567 d_loss: 1.39494848, g_loss: 0.71099174, ae_loss: 0.05150539\n",
      "Step: [5216] total_loss: 2.13248587 d_loss: 1.38404429, g_loss: 0.69672543, ae_loss: 0.05171615\n",
      "Step: [5217] total_loss: 2.13840270 d_loss: 1.36998379, g_loss: 0.71960849, ae_loss: 0.04881037\n",
      "Step: [5218] total_loss: 2.13756061 d_loss: 1.39691150, g_loss: 0.69481820, ae_loss: 0.04583089\n",
      "Step: [5219] total_loss: 2.15291643 d_loss: 1.39646733, g_loss: 0.70369619, ae_loss: 0.05275289\n",
      "Step: [5220] total_loss: 2.14622068 d_loss: 1.38963532, g_loss: 0.70121264, ae_loss: 0.05537267\n",
      "Step: [5221] total_loss: 2.12894678 d_loss: 1.39147198, g_loss: 0.68945932, ae_loss: 0.04801563\n",
      "Step: [5222] total_loss: 2.12965989 d_loss: 1.38507104, g_loss: 0.69369507, ae_loss: 0.05089383\n",
      "Step: [5223] total_loss: 2.13223314 d_loss: 1.38818169, g_loss: 0.69329870, ae_loss: 0.05075270\n",
      "Step: [5224] total_loss: 2.13015461 d_loss: 1.35895514, g_loss: 0.71839321, ae_loss: 0.05280643\n",
      "Step: [5225] total_loss: 2.15717173 d_loss: 1.40049517, g_loss: 0.70488656, ae_loss: 0.05178982\n",
      "Step: [5226] total_loss: 2.13163567 d_loss: 1.38236761, g_loss: 0.69410628, ae_loss: 0.05516167\n",
      "Step: [5227] total_loss: 2.12838936 d_loss: 1.39214444, g_loss: 0.68088323, ae_loss: 0.05536167\n",
      "Step: [5228] total_loss: 2.12959909 d_loss: 1.39312327, g_loss: 0.68747962, ae_loss: 0.04899632\n",
      "Step: [5229] total_loss: 2.13071179 d_loss: 1.38167560, g_loss: 0.70029688, ae_loss: 0.04873932\n",
      "Step: [5230] total_loss: 2.11135054 d_loss: 1.37761831, g_loss: 0.68541187, ae_loss: 0.04832046\n",
      "Step: [5231] total_loss: 2.14947677 d_loss: 1.39276505, g_loss: 0.70449078, ae_loss: 0.05222093\n",
      "Step: [5232] total_loss: 2.11835647 d_loss: 1.36942625, g_loss: 0.70181388, ae_loss: 0.04711637\n",
      "Step: [5233] total_loss: 2.13488865 d_loss: 1.39383519, g_loss: 0.69052136, ae_loss: 0.05053205\n",
      "Step: [5234] total_loss: 2.11897922 d_loss: 1.38935447, g_loss: 0.68123275, ae_loss: 0.04839190\n",
      "Step: [5235] total_loss: 2.09943104 d_loss: 1.37463009, g_loss: 0.67953902, ae_loss: 0.04526202\n",
      "Step: [5236] total_loss: 2.10698032 d_loss: 1.37835002, g_loss: 0.67774892, ae_loss: 0.05088148\n",
      "Step: [5237] total_loss: 2.11235738 d_loss: 1.38370693, g_loss: 0.67951035, ae_loss: 0.04914013\n",
      "Step: [5238] total_loss: 2.14298582 d_loss: 1.40604472, g_loss: 0.68578398, ae_loss: 0.05115699\n",
      "Step: [5239] total_loss: 2.12764549 d_loss: 1.39049339, g_loss: 0.68574333, ae_loss: 0.05140883\n",
      "Step: [5240] total_loss: 2.12162781 d_loss: 1.36385131, g_loss: 0.70765197, ae_loss: 0.05012455\n",
      "Step: [5241] total_loss: 2.12550592 d_loss: 1.38069701, g_loss: 0.69435275, ae_loss: 0.05045622\n",
      "Step: [5242] total_loss: 2.12351751 d_loss: 1.38011682, g_loss: 0.68853277, ae_loss: 0.05486786\n",
      "Step: [5243] total_loss: 2.12938118 d_loss: 1.37505662, g_loss: 0.70509422, ae_loss: 0.04923026\n",
      "Step: [5244] total_loss: 2.11156940 d_loss: 1.36551642, g_loss: 0.69365668, ae_loss: 0.05239643\n",
      "Step: [5245] total_loss: 2.10651398 d_loss: 1.35852933, g_loss: 0.69432378, ae_loss: 0.05366075\n",
      "Step: [5246] total_loss: 2.12588382 d_loss: 1.37462878, g_loss: 0.70114094, ae_loss: 0.05011404\n",
      "Step: [5247] total_loss: 2.11996937 d_loss: 1.38456011, g_loss: 0.68499142, ae_loss: 0.05041781\n",
      "Step: [5248] total_loss: 2.12198782 d_loss: 1.38170373, g_loss: 0.68470502, ae_loss: 0.05557900\n",
      "Step: [5249] total_loss: 2.11706114 d_loss: 1.38624406, g_loss: 0.68113112, ae_loss: 0.04968594\n",
      "Step: [5250] total_loss: 2.10395026 d_loss: 1.36665440, g_loss: 0.68853122, ae_loss: 0.04876463\n",
      "Step: [5251] total_loss: 2.11304855 d_loss: 1.37085259, g_loss: 0.69403303, ae_loss: 0.04816300\n",
      "Step: [5252] total_loss: 2.11020470 d_loss: 1.37622631, g_loss: 0.68712395, ae_loss: 0.04685438\n",
      "Step: [5253] total_loss: 2.11335087 d_loss: 1.36680758, g_loss: 0.69876742, ae_loss: 0.04777595\n",
      "Step: [5254] total_loss: 2.12236118 d_loss: 1.37588847, g_loss: 0.69930708, ae_loss: 0.04716551\n",
      "Step: [5255] total_loss: 2.11331558 d_loss: 1.37763882, g_loss: 0.68596721, ae_loss: 0.04970942\n",
      "Step: [5256] total_loss: 2.12887335 d_loss: 1.38155127, g_loss: 0.69319987, ae_loss: 0.05412210\n",
      "Step: [5257] total_loss: 2.13443780 d_loss: 1.38095140, g_loss: 0.70333362, ae_loss: 0.05015277\n",
      "Step: [5258] total_loss: 2.12032294 d_loss: 1.37374222, g_loss: 0.70018280, ae_loss: 0.04639789\n",
      "Step: [5259] total_loss: 2.14022446 d_loss: 1.37609911, g_loss: 0.71281713, ae_loss: 0.05130822\n",
      "Step: [5260] total_loss: 2.11340475 d_loss: 1.35728979, g_loss: 0.70550168, ae_loss: 0.05061323\n",
      "Step: [5261] total_loss: 2.12616110 d_loss: 1.36992598, g_loss: 0.70616907, ae_loss: 0.05006593\n",
      "Step: [5262] total_loss: 2.11226010 d_loss: 1.38341391, g_loss: 0.67816770, ae_loss: 0.05067849\n",
      "Step: [5263] total_loss: 2.10656834 d_loss: 1.36269665, g_loss: 0.69356769, ae_loss: 0.05030393\n",
      "Step: [5264] total_loss: 2.10790038 d_loss: 1.37542534, g_loss: 0.68591648, ae_loss: 0.04655862\n",
      "Step: [5265] total_loss: 2.12832761 d_loss: 1.35322034, g_loss: 0.72574329, ae_loss: 0.04936393\n",
      "Step: [5266] total_loss: 2.13495111 d_loss: 1.37066483, g_loss: 0.71039528, ae_loss: 0.05389090\n",
      "Step: [5267] total_loss: 2.14758301 d_loss: 1.37935126, g_loss: 0.71750760, ae_loss: 0.05072407\n",
      "Step: [5268] total_loss: 2.13433075 d_loss: 1.37690473, g_loss: 0.70806742, ae_loss: 0.04935864\n",
      "Step: [5269] total_loss: 2.12256622 d_loss: 1.38004935, g_loss: 0.68918753, ae_loss: 0.05332941\n",
      "Step: [5270] total_loss: 2.13701320 d_loss: 1.40166402, g_loss: 0.68636525, ae_loss: 0.04898394\n",
      "Step: [5271] total_loss: 2.13533664 d_loss: 1.38812900, g_loss: 0.69515026, ae_loss: 0.05205736\n",
      "Step: [5272] total_loss: 2.12112045 d_loss: 1.35429394, g_loss: 0.71152151, ae_loss: 0.05530503\n",
      "Step: [5273] total_loss: 2.13439393 d_loss: 1.38519335, g_loss: 0.69352913, ae_loss: 0.05567140\n",
      "Step: [5274] total_loss: 2.12350798 d_loss: 1.38357401, g_loss: 0.68880332, ae_loss: 0.05113080\n",
      "Step: [5275] total_loss: 2.13294482 d_loss: 1.39343333, g_loss: 0.68470776, ae_loss: 0.05480369\n",
      "Step: [5276] total_loss: 2.12030888 d_loss: 1.38370800, g_loss: 0.68540037, ae_loss: 0.05120060\n",
      "Step: [5277] total_loss: 2.13685966 d_loss: 1.37965441, g_loss: 0.70152730, ae_loss: 0.05567800\n",
      "Step: [5278] total_loss: 2.12937880 d_loss: 1.37945092, g_loss: 0.69863653, ae_loss: 0.05129150\n",
      "Step: [5279] total_loss: 2.11046076 d_loss: 1.37681770, g_loss: 0.68172717, ae_loss: 0.05191580\n",
      "Step: [5280] total_loss: 2.12620044 d_loss: 1.39228857, g_loss: 0.68237627, ae_loss: 0.05153556\n",
      "Step: [5281] total_loss: 2.10558319 d_loss: 1.37404168, g_loss: 0.67914546, ae_loss: 0.05239601\n",
      "Step: [5282] total_loss: 2.12188625 d_loss: 1.37056696, g_loss: 0.69792724, ae_loss: 0.05339222\n",
      "Step: [5283] total_loss: 2.10830164 d_loss: 1.35846853, g_loss: 0.69990706, ae_loss: 0.04992612\n",
      "Step: [5284] total_loss: 2.12332582 d_loss: 1.36110163, g_loss: 0.71042675, ae_loss: 0.05179742\n",
      "Step: [5285] total_loss: 2.12516880 d_loss: 1.39377451, g_loss: 0.68420768, ae_loss: 0.04718652\n",
      "Step: [5286] total_loss: 2.11316895 d_loss: 1.37008119, g_loss: 0.69436163, ae_loss: 0.04872610\n",
      "Step: [5287] total_loss: 2.13181233 d_loss: 1.39048862, g_loss: 0.69026208, ae_loss: 0.05106163\n",
      "Step: [5288] total_loss: 2.14840889 d_loss: 1.39026320, g_loss: 0.70616519, ae_loss: 0.05198044\n",
      "Step: [5289] total_loss: 2.13422894 d_loss: 1.37953687, g_loss: 0.70211655, ae_loss: 0.05257550\n",
      "Step: [5290] total_loss: 2.11604691 d_loss: 1.37458098, g_loss: 0.68837953, ae_loss: 0.05308634\n",
      "Step: [5291] total_loss: 2.12112904 d_loss: 1.36549461, g_loss: 0.70480537, ae_loss: 0.05082904\n",
      "Step: [5292] total_loss: 2.13023925 d_loss: 1.39630532, g_loss: 0.68363309, ae_loss: 0.05030085\n",
      "Step: [5293] total_loss: 2.13681269 d_loss: 1.38088346, g_loss: 0.70119679, ae_loss: 0.05473261\n",
      "Step: [5294] total_loss: 2.12304735 d_loss: 1.38837838, g_loss: 0.68313044, ae_loss: 0.05153859\n",
      "Step: [5295] total_loss: 2.10897636 d_loss: 1.38000476, g_loss: 0.67472267, ae_loss: 0.05424907\n",
      "Step: [5296] total_loss: 2.09914207 d_loss: 1.36377549, g_loss: 0.68274963, ae_loss: 0.05261711\n",
      "Step: [5297] total_loss: 2.11611485 d_loss: 1.36302555, g_loss: 0.70250207, ae_loss: 0.05058734\n",
      "Step: [5298] total_loss: 2.09863091 d_loss: 1.37555099, g_loss: 0.66946173, ae_loss: 0.05361823\n",
      "Step: [5299] total_loss: 2.13067341 d_loss: 1.37428308, g_loss: 0.70474350, ae_loss: 0.05164681\n",
      "Step: [5300] total_loss: 2.13562417 d_loss: 1.39211941, g_loss: 0.69141424, ae_loss: 0.05209054\n",
      "Step: [5301] total_loss: 2.13751125 d_loss: 1.39044952, g_loss: 0.69275969, ae_loss: 0.05430199\n",
      "Step: [5302] total_loss: 2.12527108 d_loss: 1.37188780, g_loss: 0.70463717, ae_loss: 0.04874611\n",
      "Step: [5303] total_loss: 2.13877583 d_loss: 1.38012433, g_loss: 0.70341599, ae_loss: 0.05523553\n",
      "Step: [5304] total_loss: 2.13834000 d_loss: 1.38346481, g_loss: 0.70570862, ae_loss: 0.04916669\n",
      "Step: [5305] total_loss: 2.13625622 d_loss: 1.37826729, g_loss: 0.70640373, ae_loss: 0.05158536\n",
      "Step: [5306] total_loss: 2.10538816 d_loss: 1.35942817, g_loss: 0.69320166, ae_loss: 0.05275818\n",
      "Step: [5307] total_loss: 2.14558077 d_loss: 1.39564812, g_loss: 0.69619602, ae_loss: 0.05373670\n",
      "Step: [5308] total_loss: 2.11235523 d_loss: 1.37959921, g_loss: 0.67934430, ae_loss: 0.05341190\n",
      "Step: [5309] total_loss: 2.10835028 d_loss: 1.36103928, g_loss: 0.69750905, ae_loss: 0.04980190\n",
      "Step: [5310] total_loss: 2.11521626 d_loss: 1.38395894, g_loss: 0.67953110, ae_loss: 0.05172614\n",
      "Step: [5311] total_loss: 2.13358998 d_loss: 1.37570786, g_loss: 0.70563799, ae_loss: 0.05224417\n",
      "Step: [5312] total_loss: 2.11099434 d_loss: 1.38987124, g_loss: 0.66858780, ae_loss: 0.05253544\n",
      "Step: [5313] total_loss: 2.13357234 d_loss: 1.38784671, g_loss: 0.68936956, ae_loss: 0.05635605\n",
      "Step: [5314] total_loss: 2.09664702 d_loss: 1.36995029, g_loss: 0.67519867, ae_loss: 0.05149805\n",
      "Step: [5315] total_loss: 2.13850403 d_loss: 1.40977621, g_loss: 0.67665184, ae_loss: 0.05207599\n",
      "Step: [5316] total_loss: 2.10560393 d_loss: 1.37721789, g_loss: 0.67899287, ae_loss: 0.04939319\n",
      "Step: [5317] total_loss: 2.11985826 d_loss: 1.38710880, g_loss: 0.68290657, ae_loss: 0.04984297\n",
      "Step: [5318] total_loss: 2.12739253 d_loss: 1.38907290, g_loss: 0.68900019, ae_loss: 0.04931950\n",
      "Step: [5319] total_loss: 2.13887286 d_loss: 1.38796961, g_loss: 0.70389485, ae_loss: 0.04700839\n",
      "Step: [5320] total_loss: 2.11816955 d_loss: 1.37517798, g_loss: 0.69361949, ae_loss: 0.04937205\n",
      "Step: [5321] total_loss: 2.14383078 d_loss: 1.36398184, g_loss: 0.72992992, ae_loss: 0.04991896\n",
      "Step: [5322] total_loss: 2.13106513 d_loss: 1.38344455, g_loss: 0.69474864, ae_loss: 0.05287192\n",
      "Step: [5323] total_loss: 2.14198971 d_loss: 1.41205847, g_loss: 0.68054295, ae_loss: 0.04938841\n",
      "Step: [5324] total_loss: 2.09833431 d_loss: 1.35631084, g_loss: 0.69312871, ae_loss: 0.04889466\n",
      "Step: [5325] total_loss: 2.12028551 d_loss: 1.38538992, g_loss: 0.68597639, ae_loss: 0.04891927\n",
      "Step: [5326] total_loss: 2.12163568 d_loss: 1.39547563, g_loss: 0.67406559, ae_loss: 0.05209443\n",
      "Step: [5327] total_loss: 2.10880899 d_loss: 1.38242555, g_loss: 0.67980373, ae_loss: 0.04657975\n",
      "Step: [5328] total_loss: 2.11758280 d_loss: 1.36991894, g_loss: 0.69761050, ae_loss: 0.05005338\n",
      "Step: [5329] total_loss: 2.14942622 d_loss: 1.39628077, g_loss: 0.70104533, ae_loss: 0.05210005\n",
      "Step: [5330] total_loss: 2.10667992 d_loss: 1.37695575, g_loss: 0.67480600, ae_loss: 0.05491831\n",
      "Step: [5331] total_loss: 2.12188625 d_loss: 1.39942765, g_loss: 0.66849738, ae_loss: 0.05396117\n",
      "Step: [5332] total_loss: 2.13513470 d_loss: 1.38333118, g_loss: 0.70115542, ae_loss: 0.05064812\n",
      "Step: [5333] total_loss: 2.10749793 d_loss: 1.36842918, g_loss: 0.69204098, ae_loss: 0.04702777\n",
      "Step: [5334] total_loss: 2.13185096 d_loss: 1.38668573, g_loss: 0.69085145, ae_loss: 0.05431384\n",
      "Step: [5335] total_loss: 2.13542342 d_loss: 1.39365578, g_loss: 0.69013304, ae_loss: 0.05163458\n",
      "Step: [5336] total_loss: 2.12865710 d_loss: 1.38182139, g_loss: 0.69382358, ae_loss: 0.05301218\n",
      "Step: [5337] total_loss: 2.12601089 d_loss: 1.38588071, g_loss: 0.68774372, ae_loss: 0.05238656\n",
      "Step: [5338] total_loss: 2.12014866 d_loss: 1.38470984, g_loss: 0.68576425, ae_loss: 0.04967465\n",
      "Step: [5339] total_loss: 2.11500025 d_loss: 1.38818073, g_loss: 0.67987782, ae_loss: 0.04694163\n",
      "Step: [5340] total_loss: 2.11994839 d_loss: 1.39561582, g_loss: 0.67166895, ae_loss: 0.05266366\n",
      "Step: [5341] total_loss: 2.11107254 d_loss: 1.37877822, g_loss: 0.68310344, ae_loss: 0.04919080\n",
      "Step: [5342] total_loss: 2.16105413 d_loss: 1.38804674, g_loss: 0.72091961, ae_loss: 0.05208782\n",
      "Step: [5343] total_loss: 2.12719011 d_loss: 1.39137530, g_loss: 0.68753338, ae_loss: 0.04828140\n",
      "Step: [5344] total_loss: 2.13481188 d_loss: 1.38412380, g_loss: 0.69807214, ae_loss: 0.05261598\n",
      "Step: [5345] total_loss: 2.14963007 d_loss: 1.40736723, g_loss: 0.69163567, ae_loss: 0.05062712\n",
      "Step: [5346] total_loss: 2.10885859 d_loss: 1.37765288, g_loss: 0.67970264, ae_loss: 0.05150308\n",
      "Step: [5347] total_loss: 2.10778284 d_loss: 1.36006427, g_loss: 0.69646049, ae_loss: 0.05125795\n",
      "Step: [5348] total_loss: 2.12593985 d_loss: 1.38655150, g_loss: 0.68914807, ae_loss: 0.05024015\n",
      "Step: [5349] total_loss: 2.11435270 d_loss: 1.37201321, g_loss: 0.69082868, ae_loss: 0.05151097\n",
      "Step: [5350] total_loss: 2.11117983 d_loss: 1.37400639, g_loss: 0.68559659, ae_loss: 0.05157684\n",
      "Step: [5351] total_loss: 2.12478399 d_loss: 1.38083959, g_loss: 0.69486368, ae_loss: 0.04908065\n",
      "Step: [5352] total_loss: 2.11127687 d_loss: 1.37158656, g_loss: 0.69267672, ae_loss: 0.04701360\n",
      "Step: [5353] total_loss: 2.11267495 d_loss: 1.35733676, g_loss: 0.70625627, ae_loss: 0.04908197\n",
      "Step: [5354] total_loss: 2.11827469 d_loss: 1.37967825, g_loss: 0.68706226, ae_loss: 0.05153427\n",
      "Step: [5355] total_loss: 2.10982370 d_loss: 1.35301352, g_loss: 0.71001661, ae_loss: 0.04679355\n",
      "Step: [5356] total_loss: 2.13127017 d_loss: 1.37366116, g_loss: 0.71001112, ae_loss: 0.04759791\n",
      "Step: [5357] total_loss: 2.14676857 d_loss: 1.39102101, g_loss: 0.70931339, ae_loss: 0.04643418\n",
      "Step: [5358] total_loss: 2.14821291 d_loss: 1.36272454, g_loss: 0.73735738, ae_loss: 0.04813083\n",
      "Step: [5359] total_loss: 2.13305497 d_loss: 1.39394855, g_loss: 0.69208264, ae_loss: 0.04702376\n",
      "Step: [5360] total_loss: 2.12936783 d_loss: 1.39740622, g_loss: 0.68180180, ae_loss: 0.05015997\n",
      "Step: [5361] total_loss: 2.09585619 d_loss: 1.35364306, g_loss: 0.68833673, ae_loss: 0.05387640\n",
      "Step: [5362] total_loss: 2.13676095 d_loss: 1.38553214, g_loss: 0.70224184, ae_loss: 0.04898699\n",
      "Step: [5363] total_loss: 2.11132908 d_loss: 1.34359598, g_loss: 0.71925795, ae_loss: 0.04847525\n",
      "Step: [5364] total_loss: 2.12097454 d_loss: 1.37483644, g_loss: 0.69464254, ae_loss: 0.05149573\n",
      "Step: [5365] total_loss: 2.12961817 d_loss: 1.39814961, g_loss: 0.67897534, ae_loss: 0.05249327\n",
      "Step: [5366] total_loss: 2.12318754 d_loss: 1.38464367, g_loss: 0.68530387, ae_loss: 0.05323993\n",
      "Step: [5367] total_loss: 2.13164043 d_loss: 1.37188804, g_loss: 0.70488101, ae_loss: 0.05487149\n",
      "Step: [5368] total_loss: 2.12976170 d_loss: 1.38444018, g_loss: 0.69331861, ae_loss: 0.05200286\n",
      "Step: [5369] total_loss: 2.11594844 d_loss: 1.37771928, g_loss: 0.68952340, ae_loss: 0.04870587\n",
      "Step: [5370] total_loss: 2.11297321 d_loss: 1.38991523, g_loss: 0.67352843, ae_loss: 0.04952956\n",
      "Step: [5371] total_loss: 2.12090755 d_loss: 1.39211547, g_loss: 0.67948496, ae_loss: 0.04930714\n",
      "Step: [5372] total_loss: 2.14698124 d_loss: 1.38168716, g_loss: 0.71024740, ae_loss: 0.05504665\n",
      "Step: [5373] total_loss: 2.11161208 d_loss: 1.37609315, g_loss: 0.68745595, ae_loss: 0.04806304\n",
      "Step: [5374] total_loss: 2.14234018 d_loss: 1.39151525, g_loss: 0.69960475, ae_loss: 0.05122033\n",
      "Step: [5375] total_loss: 2.12417054 d_loss: 1.38397610, g_loss: 0.69103223, ae_loss: 0.04916225\n",
      "Step: [5376] total_loss: 2.12917948 d_loss: 1.37795234, g_loss: 0.69782019, ae_loss: 0.05340684\n",
      "Step: [5377] total_loss: 2.13630962 d_loss: 1.40254772, g_loss: 0.68308687, ae_loss: 0.05067507\n",
      "Step: [5378] total_loss: 2.12016439 d_loss: 1.35578966, g_loss: 0.71028256, ae_loss: 0.05409228\n",
      "Step: [5379] total_loss: 2.14091969 d_loss: 1.39890099, g_loss: 0.69288880, ae_loss: 0.04912989\n",
      "Step: [5380] total_loss: 2.11831045 d_loss: 1.37511241, g_loss: 0.69487017, ae_loss: 0.04832786\n",
      "Step: [5381] total_loss: 2.12788916 d_loss: 1.38454521, g_loss: 0.69696236, ae_loss: 0.04638154\n",
      "Step: [5382] total_loss: 2.12372422 d_loss: 1.38645196, g_loss: 0.68721688, ae_loss: 0.05005539\n",
      "Step: [5383] total_loss: 2.11049914 d_loss: 1.36038625, g_loss: 0.69993949, ae_loss: 0.05017345\n",
      "Step: [5384] total_loss: 2.14662790 d_loss: 1.36933148, g_loss: 0.72959340, ae_loss: 0.04770313\n",
      "Step: [5385] total_loss: 2.12851858 d_loss: 1.37831807, g_loss: 0.69888735, ae_loss: 0.05131321\n",
      "Step: [5386] total_loss: 2.13345623 d_loss: 1.37950027, g_loss: 0.70188868, ae_loss: 0.05206735\n",
      "Step: [5387] total_loss: 2.12668800 d_loss: 1.38282943, g_loss: 0.69475591, ae_loss: 0.04910268\n",
      "Step: [5388] total_loss: 2.12083888 d_loss: 1.36993718, g_loss: 0.69686878, ae_loss: 0.05403295\n",
      "Step: [5389] total_loss: 2.15388298 d_loss: 1.40450597, g_loss: 0.69433534, ae_loss: 0.05504163\n",
      "Step: [5390] total_loss: 2.13236594 d_loss: 1.38317037, g_loss: 0.69387221, ae_loss: 0.05532340\n",
      "Step: [5391] total_loss: 2.13992643 d_loss: 1.39003825, g_loss: 0.69630373, ae_loss: 0.05358435\n",
      "Step: [5392] total_loss: 2.11579442 d_loss: 1.37520695, g_loss: 0.69291735, ae_loss: 0.04767007\n",
      "Step: [5393] total_loss: 2.13535118 d_loss: 1.38277650, g_loss: 0.69982791, ae_loss: 0.05274665\n",
      "Step: [5394] total_loss: 2.12866592 d_loss: 1.37640071, g_loss: 0.70394790, ae_loss: 0.04831725\n",
      "Step: [5395] total_loss: 2.11429977 d_loss: 1.36588621, g_loss: 0.70041454, ae_loss: 0.04799887\n",
      "Step: [5396] total_loss: 2.14611912 d_loss: 1.38356078, g_loss: 0.71022826, ae_loss: 0.05233018\n",
      "Step: [5397] total_loss: 2.10847902 d_loss: 1.34680438, g_loss: 0.71077812, ae_loss: 0.05089656\n",
      "Step: [5398] total_loss: 2.12106204 d_loss: 1.36386573, g_loss: 0.70884210, ae_loss: 0.04835415\n",
      "Step: [5399] total_loss: 2.11609745 d_loss: 1.38169825, g_loss: 0.68472576, ae_loss: 0.04967339\n",
      "Step: [5400] total_loss: 2.08365679 d_loss: 1.34163690, g_loss: 0.69135427, ae_loss: 0.05066547\n",
      "Step: [5401] total_loss: 2.14893484 d_loss: 1.39845657, g_loss: 0.69912207, ae_loss: 0.05135624\n",
      "Step: [5402] total_loss: 2.13109446 d_loss: 1.38303578, g_loss: 0.69752169, ae_loss: 0.05053713\n",
      "Step: [5403] total_loss: 2.12915349 d_loss: 1.35638547, g_loss: 0.71970308, ae_loss: 0.05306498\n",
      "Step: [5404] total_loss: 2.09848475 d_loss: 1.36381090, g_loss: 0.68139416, ae_loss: 0.05327964\n",
      "Step: [5405] total_loss: 2.11688471 d_loss: 1.38254404, g_loss: 0.68458438, ae_loss: 0.04975640\n",
      "Step: [5406] total_loss: 2.09004593 d_loss: 1.34528923, g_loss: 0.69621307, ae_loss: 0.04854374\n",
      "Step: [5407] total_loss: 2.12142587 d_loss: 1.38898361, g_loss: 0.68548995, ae_loss: 0.04695242\n",
      "Step: [5408] total_loss: 2.15573430 d_loss: 1.37921154, g_loss: 0.72625864, ae_loss: 0.05026416\n",
      "Step: [5409] total_loss: 2.12454700 d_loss: 1.36420548, g_loss: 0.71174538, ae_loss: 0.04859624\n",
      "Step: [5410] total_loss: 2.13216233 d_loss: 1.39052165, g_loss: 0.68609911, ae_loss: 0.05554149\n",
      "Step: [5411] total_loss: 2.11940074 d_loss: 1.39036417, g_loss: 0.68016440, ae_loss: 0.04887223\n",
      "Step: [5412] total_loss: 2.11610007 d_loss: 1.38278580, g_loss: 0.68713033, ae_loss: 0.04618393\n",
      "Step: [5413] total_loss: 2.10387897 d_loss: 1.36411715, g_loss: 0.69117057, ae_loss: 0.04859139\n",
      "Step: [5414] total_loss: 2.11256027 d_loss: 1.37092733, g_loss: 0.68688118, ae_loss: 0.05475189\n",
      "Step: [5415] total_loss: 2.11989188 d_loss: 1.36356878, g_loss: 0.70868456, ae_loss: 0.04763851\n",
      "Step: [5416] total_loss: 2.14222145 d_loss: 1.35512304, g_loss: 0.73571080, ae_loss: 0.05138754\n",
      "Step: [5417] total_loss: 2.10905766 d_loss: 1.38843024, g_loss: 0.67478555, ae_loss: 0.04584187\n",
      "Step: [5418] total_loss: 2.15107012 d_loss: 1.40762770, g_loss: 0.69021678, ae_loss: 0.05322571\n",
      "Step: [5419] total_loss: 2.15295649 d_loss: 1.40991712, g_loss: 0.69387859, ae_loss: 0.04916080\n",
      "Step: [5420] total_loss: 2.12084365 d_loss: 1.39264262, g_loss: 0.67471361, ae_loss: 0.05348740\n",
      "Step: [5421] total_loss: 2.12933779 d_loss: 1.36989069, g_loss: 0.70947802, ae_loss: 0.04996891\n",
      "Step: [5422] total_loss: 2.15881729 d_loss: 1.36197615, g_loss: 0.75020099, ae_loss: 0.04664006\n",
      "Step: [5423] total_loss: 2.09970713 d_loss: 1.35918713, g_loss: 0.68948889, ae_loss: 0.05103119\n",
      "Step: [5424] total_loss: 2.12320280 d_loss: 1.38819313, g_loss: 0.68588090, ae_loss: 0.04912874\n",
      "Step: [5425] total_loss: 2.13660169 d_loss: 1.37727213, g_loss: 0.70868778, ae_loss: 0.05064173\n",
      "Step: [5426] total_loss: 2.13763380 d_loss: 1.38929629, g_loss: 0.69493055, ae_loss: 0.05340700\n",
      "Step: [5427] total_loss: 2.13422108 d_loss: 1.38970387, g_loss: 0.69073164, ae_loss: 0.05378574\n",
      "Step: [5428] total_loss: 2.12154579 d_loss: 1.37330174, g_loss: 0.69788969, ae_loss: 0.05035428\n",
      "Step: [5429] total_loss: 2.12391138 d_loss: 1.38750839, g_loss: 0.68514287, ae_loss: 0.05126005\n",
      "Step: [5430] total_loss: 2.11913013 d_loss: 1.38115561, g_loss: 0.68798703, ae_loss: 0.04998741\n",
      "Step: [5431] total_loss: 2.09742641 d_loss: 1.36877131, g_loss: 0.68024051, ae_loss: 0.04841458\n",
      "Step: [5432] total_loss: 2.13049984 d_loss: 1.40294719, g_loss: 0.67479962, ae_loss: 0.05275303\n",
      "Step: [5433] total_loss: 2.14885902 d_loss: 1.38408446, g_loss: 0.71346974, ae_loss: 0.05130498\n",
      "Step: [5434] total_loss: 2.12889528 d_loss: 1.37006450, g_loss: 0.70670414, ae_loss: 0.05212647\n",
      "Step: [5435] total_loss: 2.14324093 d_loss: 1.40743268, g_loss: 0.68798208, ae_loss: 0.04782623\n",
      "Step: [5436] total_loss: 2.15301275 d_loss: 1.39847481, g_loss: 0.69971466, ae_loss: 0.05482337\n",
      "Step: [5437] total_loss: 2.10464478 d_loss: 1.34886193, g_loss: 0.70462191, ae_loss: 0.05116077\n",
      "Step: [5438] total_loss: 2.12228251 d_loss: 1.37179899, g_loss: 0.69630665, ae_loss: 0.05417692\n",
      "Step: [5439] total_loss: 2.12322998 d_loss: 1.37690020, g_loss: 0.69719398, ae_loss: 0.04913584\n",
      "Step: [5440] total_loss: 2.11813045 d_loss: 1.38678169, g_loss: 0.68027067, ae_loss: 0.05107808\n",
      "Step: [5441] total_loss: 2.12245584 d_loss: 1.37747455, g_loss: 0.69523734, ae_loss: 0.04974396\n",
      "Step: [5442] total_loss: 2.16666603 d_loss: 1.40615678, g_loss: 0.70450628, ae_loss: 0.05600300\n",
      "Step: [5443] total_loss: 2.13664675 d_loss: 1.39015961, g_loss: 0.70184183, ae_loss: 0.04464524\n",
      "Step: [5444] total_loss: 2.09299898 d_loss: 1.36592579, g_loss: 0.67823875, ae_loss: 0.04883455\n",
      "Step: [5445] total_loss: 2.09914756 d_loss: 1.36263573, g_loss: 0.68647408, ae_loss: 0.05003772\n",
      "Step: [5446] total_loss: 2.12437105 d_loss: 1.38923144, g_loss: 0.68371844, ae_loss: 0.05142127\n",
      "Step: [5447] total_loss: 2.10726047 d_loss: 1.37362671, g_loss: 0.68543333, ae_loss: 0.04820032\n",
      "Step: [5448] total_loss: 2.11572552 d_loss: 1.36754870, g_loss: 0.69874185, ae_loss: 0.04943493\n",
      "Step: [5449] total_loss: 2.11438608 d_loss: 1.38564122, g_loss: 0.67933869, ae_loss: 0.04940603\n",
      "Step: [5450] total_loss: 2.09916306 d_loss: 1.36642218, g_loss: 0.68545210, ae_loss: 0.04728865\n",
      "Step: [5451] total_loss: 2.15635443 d_loss: 1.37440777, g_loss: 0.73114824, ae_loss: 0.05079832\n",
      "Step: [5452] total_loss: 2.15839863 d_loss: 1.37394524, g_loss: 0.73437148, ae_loss: 0.05008186\n",
      "Step: [5453] total_loss: 2.11463690 d_loss: 1.36825430, g_loss: 0.69592482, ae_loss: 0.05045785\n",
      "Step: [5454] total_loss: 2.11341572 d_loss: 1.35936093, g_loss: 0.70088434, ae_loss: 0.05317059\n",
      "Step: [5455] total_loss: 2.10093212 d_loss: 1.37113047, g_loss: 0.68329555, ae_loss: 0.04650615\n",
      "Step: [5456] total_loss: 2.12896609 d_loss: 1.35574436, g_loss: 0.72373360, ae_loss: 0.04948803\n",
      "Step: [5457] total_loss: 2.13508582 d_loss: 1.40464258, g_loss: 0.67940474, ae_loss: 0.05103854\n",
      "Step: [5458] total_loss: 2.11597586 d_loss: 1.36585164, g_loss: 0.69993997, ae_loss: 0.05018435\n",
      "Step: [5459] total_loss: 2.15432763 d_loss: 1.39597750, g_loss: 0.70641613, ae_loss: 0.05193397\n",
      "Step: [5460] total_loss: 2.13884163 d_loss: 1.40083408, g_loss: 0.68957138, ae_loss: 0.04843619\n",
      "Step: [5461] total_loss: 2.13903475 d_loss: 1.38406301, g_loss: 0.70220685, ae_loss: 0.05276492\n",
      "Step: [5462] total_loss: 2.14358473 d_loss: 1.37139559, g_loss: 0.72347820, ae_loss: 0.04871094\n",
      "Step: [5463] total_loss: 2.12004089 d_loss: 1.38069940, g_loss: 0.68843371, ae_loss: 0.05090790\n",
      "Step: [5464] total_loss: 2.13099837 d_loss: 1.37988925, g_loss: 0.69526452, ae_loss: 0.05584465\n",
      "Step: [5465] total_loss: 2.11428237 d_loss: 1.38671970, g_loss: 0.67829061, ae_loss: 0.04927205\n",
      "Step: [5466] total_loss: 2.10345554 d_loss: 1.35646486, g_loss: 0.69531250, ae_loss: 0.05167816\n",
      "Step: [5467] total_loss: 2.12465048 d_loss: 1.39278889, g_loss: 0.68124378, ae_loss: 0.05061789\n",
      "Step: [5468] total_loss: 2.12980032 d_loss: 1.40188539, g_loss: 0.67657781, ae_loss: 0.05133703\n",
      "Step: [5469] total_loss: 2.11655450 d_loss: 1.38311398, g_loss: 0.68227637, ae_loss: 0.05116417\n",
      "Step: [5470] total_loss: 2.10934639 d_loss: 1.36131763, g_loss: 0.69982696, ae_loss: 0.04820176\n",
      "Step: [5471] total_loss: 2.11086106 d_loss: 1.37676620, g_loss: 0.68487340, ae_loss: 0.04922137\n",
      "Step: [5472] total_loss: 2.12915254 d_loss: 1.36910152, g_loss: 0.70846683, ae_loss: 0.05158426\n",
      "Step: [5473] total_loss: 2.15633988 d_loss: 1.40308154, g_loss: 0.70014745, ae_loss: 0.05311081\n",
      "Step: [5474] total_loss: 2.11071563 d_loss: 1.36459053, g_loss: 0.69494200, ae_loss: 0.05118311\n",
      "Step: [5475] total_loss: 2.13719487 d_loss: 1.37768853, g_loss: 0.70466173, ae_loss: 0.05484458\n",
      "Step: [5476] total_loss: 2.10991049 d_loss: 1.37660718, g_loss: 0.68444586, ae_loss: 0.04885749\n",
      "Step: [5477] total_loss: 2.12139535 d_loss: 1.36939847, g_loss: 0.69882280, ae_loss: 0.05317411\n",
      "Step: [5478] total_loss: 2.12737894 d_loss: 1.37008762, g_loss: 0.70513755, ae_loss: 0.05215374\n",
      "Step: [5479] total_loss: 2.14007306 d_loss: 1.38940036, g_loss: 0.69957572, ae_loss: 0.05109688\n",
      "Step: [5480] total_loss: 2.12719488 d_loss: 1.38972139, g_loss: 0.68610585, ae_loss: 0.05136780\n",
      "Step: [5481] total_loss: 2.15056062 d_loss: 1.39801133, g_loss: 0.70238614, ae_loss: 0.05016319\n",
      "Step: [5482] total_loss: 2.12163305 d_loss: 1.37018621, g_loss: 0.70425236, ae_loss: 0.04719461\n",
      "Step: [5483] total_loss: 2.12477255 d_loss: 1.38265622, g_loss: 0.69093293, ae_loss: 0.05118335\n",
      "Step: [5484] total_loss: 2.14370966 d_loss: 1.38619852, g_loss: 0.70916152, ae_loss: 0.04834950\n",
      "Step: [5485] total_loss: 2.15147877 d_loss: 1.38354826, g_loss: 0.71376252, ae_loss: 0.05416805\n",
      "Step: [5486] total_loss: 2.14119768 d_loss: 1.38465929, g_loss: 0.70561433, ae_loss: 0.05092392\n",
      "Step: [5487] total_loss: 2.10854268 d_loss: 1.38471437, g_loss: 0.66943055, ae_loss: 0.05439778\n",
      "Step: [5488] total_loss: 2.13395500 d_loss: 1.37695456, g_loss: 0.70178860, ae_loss: 0.05521185\n",
      "Step: [5489] total_loss: 2.10650778 d_loss: 1.37087917, g_loss: 0.68302125, ae_loss: 0.05260746\n",
      "Step: [5490] total_loss: 2.14038563 d_loss: 1.36473012, g_loss: 0.72010410, ae_loss: 0.05555126\n",
      "Step: [5491] total_loss: 2.10872078 d_loss: 1.36832571, g_loss: 0.69052112, ae_loss: 0.04987380\n",
      "Step: [5492] total_loss: 2.11594629 d_loss: 1.36434364, g_loss: 0.70305270, ae_loss: 0.04854985\n",
      "Step: [5493] total_loss: 2.13564253 d_loss: 1.38482213, g_loss: 0.70051551, ae_loss: 0.05030497\n",
      "Step: [5494] total_loss: 2.11888504 d_loss: 1.37134361, g_loss: 0.69630456, ae_loss: 0.05123698\n",
      "Step: [5495] total_loss: 2.13260865 d_loss: 1.36155820, g_loss: 0.72132528, ae_loss: 0.04972515\n",
      "Step: [5496] total_loss: 2.14726400 d_loss: 1.40184820, g_loss: 0.69292730, ae_loss: 0.05248860\n",
      "Step: [5497] total_loss: 2.12089252 d_loss: 1.37558365, g_loss: 0.69722390, ae_loss: 0.04808496\n",
      "Step: [5498] total_loss: 2.14045596 d_loss: 1.39112973, g_loss: 0.69377953, ae_loss: 0.05554679\n",
      "Step: [5499] total_loss: 2.11998868 d_loss: 1.37779856, g_loss: 0.68380821, ae_loss: 0.05838189\n",
      "Step: [5500] total_loss: 2.12214279 d_loss: 1.36486733, g_loss: 0.70867920, ae_loss: 0.04859635\n",
      "Step: [5501] total_loss: 2.15240049 d_loss: 1.39935470, g_loss: 0.70354527, ae_loss: 0.04950044\n",
      "Step: [5502] total_loss: 2.11873507 d_loss: 1.38056588, g_loss: 0.68473727, ae_loss: 0.05343186\n",
      "Step: [5503] total_loss: 2.13753843 d_loss: 1.40480542, g_loss: 0.67900014, ae_loss: 0.05373286\n",
      "Step: [5504] total_loss: 2.12047362 d_loss: 1.36997199, g_loss: 0.70129097, ae_loss: 0.04921072\n",
      "Step: [5505] total_loss: 2.12495875 d_loss: 1.38436794, g_loss: 0.69187039, ae_loss: 0.04872036\n",
      "Step: [5506] total_loss: 2.11724472 d_loss: 1.39217901, g_loss: 0.67281127, ae_loss: 0.05225459\n",
      "Step: [5507] total_loss: 2.12563419 d_loss: 1.37879872, g_loss: 0.69638836, ae_loss: 0.05044696\n",
      "Step: [5508] total_loss: 2.13946009 d_loss: 1.42350137, g_loss: 0.66822088, ae_loss: 0.04773768\n",
      "Step: [5509] total_loss: 2.11738086 d_loss: 1.38137066, g_loss: 0.68445712, ae_loss: 0.05155309\n",
      "Step: [5510] total_loss: 2.12910986 d_loss: 1.40112972, g_loss: 0.67721617, ae_loss: 0.05076381\n",
      "Step: [5511] total_loss: 2.11571407 d_loss: 1.37971091, g_loss: 0.68674350, ae_loss: 0.04925966\n",
      "Step: [5512] total_loss: 2.14581585 d_loss: 1.37432313, g_loss: 0.72192657, ae_loss: 0.04956629\n",
      "Step: [5513] total_loss: 2.11414051 d_loss: 1.36056709, g_loss: 0.70634025, ae_loss: 0.04723325\n",
      "Step: [5514] total_loss: 2.12411141 d_loss: 1.38500690, g_loss: 0.69005102, ae_loss: 0.04905358\n",
      "Step: [5515] total_loss: 2.10800576 d_loss: 1.36714649, g_loss: 0.69152594, ae_loss: 0.04933337\n",
      "Step: [5516] total_loss: 2.10561991 d_loss: 1.37346816, g_loss: 0.68204498, ae_loss: 0.05010661\n",
      "Step: [5517] total_loss: 2.10602498 d_loss: 1.36713815, g_loss: 0.69451886, ae_loss: 0.04436789\n",
      "Step: [5518] total_loss: 2.14275742 d_loss: 1.41393411, g_loss: 0.67502666, ae_loss: 0.05379678\n",
      "Step: [5519] total_loss: 2.13397002 d_loss: 1.40214276, g_loss: 0.67987418, ae_loss: 0.05195311\n",
      "Step: [5520] total_loss: 2.12488461 d_loss: 1.38380289, g_loss: 0.69195312, ae_loss: 0.04912865\n",
      "Step: [5521] total_loss: 2.12620068 d_loss: 1.38794994, g_loss: 0.68792742, ae_loss: 0.05032342\n",
      "Step: [5522] total_loss: 2.09683084 d_loss: 1.35190928, g_loss: 0.69574213, ae_loss: 0.04917941\n",
      "Step: [5523] total_loss: 2.12673926 d_loss: 1.39553452, g_loss: 0.68165344, ae_loss: 0.04955136\n",
      "Step: [5524] total_loss: 2.12047911 d_loss: 1.37803984, g_loss: 0.69056010, ae_loss: 0.05187913\n",
      "Step: [5525] total_loss: 2.14974403 d_loss: 1.39055371, g_loss: 0.70673686, ae_loss: 0.05245354\n",
      "Step: [5526] total_loss: 2.13999581 d_loss: 1.38009214, g_loss: 0.70713747, ae_loss: 0.05276622\n",
      "Step: [5527] total_loss: 2.13705802 d_loss: 1.37519562, g_loss: 0.70876169, ae_loss: 0.05310075\n",
      "Step: [5528] total_loss: 2.12107873 d_loss: 1.37755203, g_loss: 0.69379604, ae_loss: 0.04973065\n",
      "Step: [5529] total_loss: 2.11640835 d_loss: 1.36538959, g_loss: 0.69968259, ae_loss: 0.05133622\n",
      "Step: [5530] total_loss: 2.11149502 d_loss: 1.37880623, g_loss: 0.68542862, ae_loss: 0.04726003\n",
      "Step: [5531] total_loss: 2.11044407 d_loss: 1.35874081, g_loss: 0.70194775, ae_loss: 0.04975560\n",
      "Step: [5532] total_loss: 2.12283158 d_loss: 1.39480281, g_loss: 0.67757249, ae_loss: 0.05045626\n",
      "Step: [5533] total_loss: 2.11463737 d_loss: 1.38143170, g_loss: 0.67916071, ae_loss: 0.05404485\n",
      "Step: [5534] total_loss: 2.17929983 d_loss: 1.42571723, g_loss: 0.69936407, ae_loss: 0.05421842\n",
      "Step: [5535] total_loss: 2.12743711 d_loss: 1.40075552, g_loss: 0.68046105, ae_loss: 0.04622057\n",
      "Step: [5536] total_loss: 2.12819672 d_loss: 1.36381531, g_loss: 0.71233821, ae_loss: 0.05204335\n",
      "Step: [5537] total_loss: 2.11797714 d_loss: 1.38358235, g_loss: 0.68438780, ae_loss: 0.05000702\n",
      "Step: [5538] total_loss: 2.10880589 d_loss: 1.37441027, g_loss: 0.68779904, ae_loss: 0.04659662\n",
      "Step: [5539] total_loss: 2.10871172 d_loss: 1.37213206, g_loss: 0.68554753, ae_loss: 0.05103213\n",
      "Step: [5540] total_loss: 2.10215449 d_loss: 1.36927962, g_loss: 0.68327796, ae_loss: 0.04959696\n",
      "Step: [5541] total_loss: 2.13964128 d_loss: 1.39614415, g_loss: 0.69519031, ae_loss: 0.04830683\n",
      "Step: [5542] total_loss: 2.11538744 d_loss: 1.37436450, g_loss: 0.69182849, ae_loss: 0.04919432\n",
      "Step: [5543] total_loss: 2.10477352 d_loss: 1.36371982, g_loss: 0.68677068, ae_loss: 0.05428310\n",
      "Step: [5544] total_loss: 2.13844299 d_loss: 1.35283375, g_loss: 0.73237389, ae_loss: 0.05323542\n",
      "Step: [5545] total_loss: 2.14571333 d_loss: 1.38399732, g_loss: 0.70930779, ae_loss: 0.05240822\n",
      "Step: [5546] total_loss: 2.13288879 d_loss: 1.38740063, g_loss: 0.69131362, ae_loss: 0.05417445\n",
      "Step: [5547] total_loss: 2.10742235 d_loss: 1.36203837, g_loss: 0.69158340, ae_loss: 0.05380075\n",
      "Step: [5548] total_loss: 2.10982943 d_loss: 1.36530495, g_loss: 0.69670641, ae_loss: 0.04781792\n",
      "Step: [5549] total_loss: 2.13622642 d_loss: 1.41486979, g_loss: 0.66664648, ae_loss: 0.05471010\n",
      "Step: [5550] total_loss: 2.11246896 d_loss: 1.35174918, g_loss: 0.70861632, ae_loss: 0.05210340\n",
      "Step: [5551] total_loss: 2.13114452 d_loss: 1.40444458, g_loss: 0.67372787, ae_loss: 0.05297217\n",
      "Step: [5552] total_loss: 2.11779547 d_loss: 1.39013314, g_loss: 0.67461509, ae_loss: 0.05304732\n",
      "Step: [5553] total_loss: 2.12205911 d_loss: 1.37645292, g_loss: 0.69504106, ae_loss: 0.05056512\n",
      "Step: [5554] total_loss: 2.11706161 d_loss: 1.37977839, g_loss: 0.68547201, ae_loss: 0.05181126\n",
      "Step: [5555] total_loss: 2.10914874 d_loss: 1.38911319, g_loss: 0.66955400, ae_loss: 0.05048159\n",
      "Step: [5556] total_loss: 2.10422397 d_loss: 1.36922383, g_loss: 0.68755275, ae_loss: 0.04744747\n",
      "Step: [5557] total_loss: 2.12122679 d_loss: 1.38897419, g_loss: 0.68045700, ae_loss: 0.05179556\n",
      "Step: [5558] total_loss: 2.10557270 d_loss: 1.36694074, g_loss: 0.68949091, ae_loss: 0.04914100\n",
      "Step: [5559] total_loss: 2.12425232 d_loss: 1.38304782, g_loss: 0.68768740, ae_loss: 0.05351717\n",
      "Step: [5560] total_loss: 2.10418081 d_loss: 1.38063371, g_loss: 0.67675292, ae_loss: 0.04679405\n",
      "Step: [5561] total_loss: 2.12680244 d_loss: 1.38057435, g_loss: 0.69650787, ae_loss: 0.04972016\n",
      "Step: [5562] total_loss: 2.14188242 d_loss: 1.39396739, g_loss: 0.69614053, ae_loss: 0.05177452\n",
      "Step: [5563] total_loss: 2.11743641 d_loss: 1.34681010, g_loss: 0.72314525, ae_loss: 0.04748115\n",
      "Step: [5564] total_loss: 2.13423157 d_loss: 1.38585258, g_loss: 0.69935322, ae_loss: 0.04902568\n",
      "Step: [5565] total_loss: 2.12116480 d_loss: 1.37364864, g_loss: 0.69796139, ae_loss: 0.04955486\n",
      "Step: [5566] total_loss: 2.13948894 d_loss: 1.39005220, g_loss: 0.69698006, ae_loss: 0.05245659\n",
      "Step: [5567] total_loss: 2.15086675 d_loss: 1.40237939, g_loss: 0.69681966, ae_loss: 0.05166769\n",
      "Step: [5568] total_loss: 2.15287161 d_loss: 1.40111637, g_loss: 0.69979525, ae_loss: 0.05195986\n",
      "Step: [5569] total_loss: 2.12687707 d_loss: 1.36614418, g_loss: 0.71004272, ae_loss: 0.05069013\n",
      "Step: [5570] total_loss: 2.13898015 d_loss: 1.39766145, g_loss: 0.69239646, ae_loss: 0.04892231\n",
      "Step: [5571] total_loss: 2.14022017 d_loss: 1.37096143, g_loss: 0.71781987, ae_loss: 0.05143895\n",
      "Step: [5572] total_loss: 2.15097499 d_loss: 1.41478252, g_loss: 0.68419093, ae_loss: 0.05200148\n",
      "Step: [5573] total_loss: 2.13733387 d_loss: 1.37273264, g_loss: 0.71349192, ae_loss: 0.05110917\n",
      "Step: [5574] total_loss: 2.12094951 d_loss: 1.36721921, g_loss: 0.70176035, ae_loss: 0.05196984\n",
      "Step: [5575] total_loss: 2.13800335 d_loss: 1.40070260, g_loss: 0.68483627, ae_loss: 0.05246465\n",
      "Step: [5576] total_loss: 2.12239456 d_loss: 1.38253164, g_loss: 0.68919092, ae_loss: 0.05067201\n",
      "Step: [5577] total_loss: 2.11013961 d_loss: 1.37754166, g_loss: 0.68352008, ae_loss: 0.04907781\n",
      "Step: [5578] total_loss: 2.11926508 d_loss: 1.38379359, g_loss: 0.68524677, ae_loss: 0.05022462\n",
      "Step: [5579] total_loss: 2.11445570 d_loss: 1.36139274, g_loss: 0.70341307, ae_loss: 0.04964980\n",
      "Step: [5580] total_loss: 2.13626051 d_loss: 1.39032781, g_loss: 0.69563335, ae_loss: 0.05029939\n",
      "Step: [5581] total_loss: 2.11178112 d_loss: 1.36036873, g_loss: 0.70211768, ae_loss: 0.04929468\n",
      "Step: [5582] total_loss: 2.12859869 d_loss: 1.37086165, g_loss: 0.70638466, ae_loss: 0.05135245\n",
      "Step: [5583] total_loss: 2.11960816 d_loss: 1.37607741, g_loss: 0.69058853, ae_loss: 0.05294225\n",
      "Step: [5584] total_loss: 2.10787082 d_loss: 1.38662338, g_loss: 0.67472821, ae_loss: 0.04651929\n",
      "Step: [5585] total_loss: 2.09662366 d_loss: 1.35756671, g_loss: 0.69099265, ae_loss: 0.04806419\n",
      "Step: [5586] total_loss: 2.12355685 d_loss: 1.38834608, g_loss: 0.68563402, ae_loss: 0.04957676\n",
      "Step: [5587] total_loss: 2.11295152 d_loss: 1.37130141, g_loss: 0.69277692, ae_loss: 0.04887324\n",
      "Step: [5588] total_loss: 2.12613082 d_loss: 1.37749076, g_loss: 0.69833773, ae_loss: 0.05030243\n",
      "Step: [5589] total_loss: 2.13066959 d_loss: 1.39024401, g_loss: 0.68960333, ae_loss: 0.05082240\n",
      "Step: [5590] total_loss: 2.14645386 d_loss: 1.39131272, g_loss: 0.70618606, ae_loss: 0.04895491\n",
      "Step: [5591] total_loss: 2.12098837 d_loss: 1.38387680, g_loss: 0.68785667, ae_loss: 0.04925492\n",
      "Step: [5592] total_loss: 2.12622929 d_loss: 1.39206100, g_loss: 0.68703943, ae_loss: 0.04712882\n",
      "Step: [5593] total_loss: 2.10424662 d_loss: 1.35669255, g_loss: 0.69546092, ae_loss: 0.05209322\n",
      "Step: [5594] total_loss: 2.13002348 d_loss: 1.38643813, g_loss: 0.69075191, ae_loss: 0.05283346\n",
      "Step: [5595] total_loss: 2.09855676 d_loss: 1.35815978, g_loss: 0.69210851, ae_loss: 0.04828841\n",
      "Step: [5596] total_loss: 2.12262726 d_loss: 1.38024199, g_loss: 0.69728255, ae_loss: 0.04510274\n",
      "Step: [5597] total_loss: 2.12623096 d_loss: 1.39398348, g_loss: 0.68074256, ae_loss: 0.05150485\n",
      "Step: [5598] total_loss: 2.12145138 d_loss: 1.38862121, g_loss: 0.68243873, ae_loss: 0.05039147\n",
      "Step: [5599] total_loss: 2.11895204 d_loss: 1.38304615, g_loss: 0.68475300, ae_loss: 0.05115281\n",
      "Step: [5600] total_loss: 2.10804939 d_loss: 1.36685574, g_loss: 0.69007087, ae_loss: 0.05112278\n",
      "Step: [5601] total_loss: 2.12354755 d_loss: 1.38235283, g_loss: 0.69139338, ae_loss: 0.04980120\n",
      "Step: [5602] total_loss: 2.12504458 d_loss: 1.37231874, g_loss: 0.70191956, ae_loss: 0.05080627\n",
      "Step: [5603] total_loss: 2.14624262 d_loss: 1.41359496, g_loss: 0.68377763, ae_loss: 0.04887011\n",
      "Step: [5604] total_loss: 2.12359476 d_loss: 1.38366127, g_loss: 0.69154620, ae_loss: 0.04838716\n",
      "Step: [5605] total_loss: 2.14162755 d_loss: 1.40551305, g_loss: 0.68436676, ae_loss: 0.05174767\n",
      "Step: [5606] total_loss: 2.13200903 d_loss: 1.39497864, g_loss: 0.68191040, ae_loss: 0.05511989\n",
      "Step: [5607] total_loss: 2.12557507 d_loss: 1.39430928, g_loss: 0.68165076, ae_loss: 0.04961497\n",
      "Step: [5608] total_loss: 2.12780428 d_loss: 1.37375546, g_loss: 0.70017618, ae_loss: 0.05387253\n",
      "Step: [5609] total_loss: 2.12653399 d_loss: 1.37087476, g_loss: 0.70639658, ae_loss: 0.04926249\n",
      "Step: [5610] total_loss: 2.11428666 d_loss: 1.38073349, g_loss: 0.68192428, ae_loss: 0.05162895\n",
      "Step: [5611] total_loss: 2.12586594 d_loss: 1.38402891, g_loss: 0.69423056, ae_loss: 0.04760635\n",
      "Step: [5612] total_loss: 2.13769603 d_loss: 1.40325451, g_loss: 0.68641984, ae_loss: 0.04802163\n",
      "Step: [5613] total_loss: 2.12936091 d_loss: 1.36391449, g_loss: 0.70999044, ae_loss: 0.05545597\n",
      "Step: [5614] total_loss: 2.13403130 d_loss: 1.37880564, g_loss: 0.70339012, ae_loss: 0.05183550\n",
      "Step: [5615] total_loss: 2.14742374 d_loss: 1.38370013, g_loss: 0.71227980, ae_loss: 0.05144385\n",
      "Step: [5616] total_loss: 2.15128684 d_loss: 1.38747346, g_loss: 0.71224552, ae_loss: 0.05156780\n",
      "Step: [5617] total_loss: 2.13382459 d_loss: 1.38889933, g_loss: 0.69213730, ae_loss: 0.05278789\n",
      "Step: [5618] total_loss: 2.13119936 d_loss: 1.37888098, g_loss: 0.70132333, ae_loss: 0.05099503\n",
      "Step: [5619] total_loss: 2.13738060 d_loss: 1.38232303, g_loss: 0.70348144, ae_loss: 0.05157616\n",
      "Step: [5620] total_loss: 2.13826346 d_loss: 1.40016341, g_loss: 0.68760294, ae_loss: 0.05049716\n",
      "Step: [5621] total_loss: 2.13732362 d_loss: 1.39261508, g_loss: 0.69255400, ae_loss: 0.05215454\n",
      "Step: [5622] total_loss: 2.11763620 d_loss: 1.37604511, g_loss: 0.68978739, ae_loss: 0.05180386\n",
      "Step: [5623] total_loss: 2.12187076 d_loss: 1.37582469, g_loss: 0.69511867, ae_loss: 0.05092739\n",
      "Step: [5624] total_loss: 2.13769436 d_loss: 1.38742447, g_loss: 0.69690192, ae_loss: 0.05336800\n",
      "Step: [5625] total_loss: 2.12858415 d_loss: 1.38948405, g_loss: 0.68942189, ae_loss: 0.04967816\n",
      "Step: [5626] total_loss: 2.12847090 d_loss: 1.37448037, g_loss: 0.70288253, ae_loss: 0.05110815\n",
      "Step: [5627] total_loss: 2.14045763 d_loss: 1.40789998, g_loss: 0.68504226, ae_loss: 0.04751548\n",
      "Step: [5628] total_loss: 2.12094331 d_loss: 1.37890947, g_loss: 0.69514322, ae_loss: 0.04689061\n",
      "Step: [5629] total_loss: 2.13295603 d_loss: 1.39847112, g_loss: 0.68705583, ae_loss: 0.04742896\n",
      "Step: [5630] total_loss: 2.12941408 d_loss: 1.38101828, g_loss: 0.69333804, ae_loss: 0.05505778\n",
      "Step: [5631] total_loss: 2.11368275 d_loss: 1.35507023, g_loss: 0.70565522, ae_loss: 0.05295741\n",
      "Step: [5632] total_loss: 2.12587285 d_loss: 1.37570405, g_loss: 0.70361257, ae_loss: 0.04655629\n",
      "Step: [5633] total_loss: 2.11615634 d_loss: 1.38282514, g_loss: 0.68752950, ae_loss: 0.04580163\n",
      "Step: [5634] total_loss: 2.10811758 d_loss: 1.35820127, g_loss: 0.70183569, ae_loss: 0.04808071\n",
      "Step: [5635] total_loss: 2.11704636 d_loss: 1.37175322, g_loss: 0.69876611, ae_loss: 0.04652693\n",
      "Step: [5636] total_loss: 2.14036298 d_loss: 1.40575385, g_loss: 0.68596125, ae_loss: 0.04864787\n",
      "Step: [5637] total_loss: 2.12983036 d_loss: 1.38486862, g_loss: 0.69538713, ae_loss: 0.04957445\n",
      "Step: [5638] total_loss: 2.12455845 d_loss: 1.37549031, g_loss: 0.69780552, ae_loss: 0.05126273\n",
      "Step: [5639] total_loss: 2.13373518 d_loss: 1.39487219, g_loss: 0.68773794, ae_loss: 0.05112515\n",
      "Step: [5640] total_loss: 2.13105345 d_loss: 1.38847351, g_loss: 0.69404107, ae_loss: 0.04853894\n",
      "Step: [5641] total_loss: 2.11344934 d_loss: 1.37198114, g_loss: 0.69184303, ae_loss: 0.04962517\n",
      "Step: [5642] total_loss: 2.10861874 d_loss: 1.37297201, g_loss: 0.68805134, ae_loss: 0.04759533\n",
      "Step: [5643] total_loss: 2.11809349 d_loss: 1.38602614, g_loss: 0.68078315, ae_loss: 0.05128405\n",
      "Step: [5644] total_loss: 2.12839174 d_loss: 1.39843524, g_loss: 0.68039703, ae_loss: 0.04955939\n",
      "Step: [5645] total_loss: 2.11015654 d_loss: 1.36687303, g_loss: 0.69229031, ae_loss: 0.05099311\n",
      "Step: [5646] total_loss: 2.13114667 d_loss: 1.37154961, g_loss: 0.70782864, ae_loss: 0.05176845\n",
      "Step: [5647] total_loss: 2.11158252 d_loss: 1.36679137, g_loss: 0.69954109, ae_loss: 0.04525008\n",
      "Step: [5648] total_loss: 2.12739396 d_loss: 1.35685539, g_loss: 0.71799129, ae_loss: 0.05254731\n",
      "Step: [5649] total_loss: 2.11473656 d_loss: 1.38799763, g_loss: 0.67828476, ae_loss: 0.04845424\n",
      "Step: [5650] total_loss: 2.13471150 d_loss: 1.39686465, g_loss: 0.68818104, ae_loss: 0.04966585\n",
      "Step: [5651] total_loss: 2.16049767 d_loss: 1.41082263, g_loss: 0.69478571, ae_loss: 0.05488944\n",
      "Step: [5652] total_loss: 2.11829281 d_loss: 1.39731729, g_loss: 0.67094517, ae_loss: 0.05003044\n",
      "Step: [5653] total_loss: 2.14336777 d_loss: 1.38027358, g_loss: 0.71328080, ae_loss: 0.04981321\n",
      "Step: [5654] total_loss: 2.12017727 d_loss: 1.37624693, g_loss: 0.69716227, ae_loss: 0.04676805\n",
      "Step: [5655] total_loss: 2.11476302 d_loss: 1.37032723, g_loss: 0.69676256, ae_loss: 0.04767322\n",
      "Step: [5656] total_loss: 2.09492779 d_loss: 1.36641073, g_loss: 0.67961705, ae_loss: 0.04889994\n",
      "Step: [5657] total_loss: 2.11839676 d_loss: 1.38857627, g_loss: 0.67989695, ae_loss: 0.04992355\n",
      "Step: [5658] total_loss: 2.13941431 d_loss: 1.38195062, g_loss: 0.71047020, ae_loss: 0.04699359\n",
      "Step: [5659] total_loss: 2.13388538 d_loss: 1.38610470, g_loss: 0.69873023, ae_loss: 0.04905039\n",
      "Step: [5660] total_loss: 2.09809875 d_loss: 1.35298467, g_loss: 0.69623125, ae_loss: 0.04888296\n",
      "Step: [5661] total_loss: 2.11327004 d_loss: 1.36689019, g_loss: 0.69275492, ae_loss: 0.05362484\n",
      "Step: [5662] total_loss: 2.11768818 d_loss: 1.37322795, g_loss: 0.69518161, ae_loss: 0.04927866\n",
      "Step: [5663] total_loss: 2.12844658 d_loss: 1.38636708, g_loss: 0.69350421, ae_loss: 0.04857522\n",
      "Step: [5664] total_loss: 2.13423848 d_loss: 1.37137616, g_loss: 0.70986772, ae_loss: 0.05299466\n",
      "Step: [5665] total_loss: 2.12599659 d_loss: 1.37766087, g_loss: 0.70020360, ae_loss: 0.04813223\n",
      "Step: [5666] total_loss: 2.10708666 d_loss: 1.34992611, g_loss: 0.70639408, ae_loss: 0.05076639\n",
      "Step: [5667] total_loss: 2.12941241 d_loss: 1.38283670, g_loss: 0.69357246, ae_loss: 0.05300320\n",
      "Step: [5668] total_loss: 2.12758613 d_loss: 1.38260829, g_loss: 0.69392329, ae_loss: 0.05105449\n",
      "Step: [5669] total_loss: 2.12335634 d_loss: 1.37326956, g_loss: 0.69791067, ae_loss: 0.05217621\n",
      "Step: [5670] total_loss: 2.12927914 d_loss: 1.40243113, g_loss: 0.67155468, ae_loss: 0.05529343\n",
      "Step: [5671] total_loss: 2.11541009 d_loss: 1.37218761, g_loss: 0.69038558, ae_loss: 0.05283695\n",
      "Step: [5672] total_loss: 2.13292122 d_loss: 1.38815677, g_loss: 0.69498634, ae_loss: 0.04977816\n",
      "Step: [5673] total_loss: 2.11995864 d_loss: 1.39642823, g_loss: 0.67141950, ae_loss: 0.05211094\n",
      "Step: [5674] total_loss: 2.12147236 d_loss: 1.38202477, g_loss: 0.68885946, ae_loss: 0.05058803\n",
      "Step: [5675] total_loss: 2.11790776 d_loss: 1.37071264, g_loss: 0.69484639, ae_loss: 0.05234874\n",
      "Step: [5676] total_loss: 2.11627483 d_loss: 1.39127326, g_loss: 0.67484772, ae_loss: 0.05015397\n",
      "Step: [5677] total_loss: 2.10968685 d_loss: 1.37386346, g_loss: 0.68091917, ae_loss: 0.05490433\n",
      "Step: [5678] total_loss: 2.10710001 d_loss: 1.36166584, g_loss: 0.69421339, ae_loss: 0.05122061\n",
      "Step: [5679] total_loss: 2.11189127 d_loss: 1.36957562, g_loss: 0.69268292, ae_loss: 0.04963266\n",
      "Step: [5680] total_loss: 2.12367582 d_loss: 1.38842821, g_loss: 0.68714499, ae_loss: 0.04810245\n",
      "Step: [5681] total_loss: 2.12372708 d_loss: 1.38794088, g_loss: 0.68344706, ae_loss: 0.05233910\n",
      "Step: [5682] total_loss: 2.11556506 d_loss: 1.37539148, g_loss: 0.69196153, ae_loss: 0.04821202\n",
      "Step: [5683] total_loss: 2.13572979 d_loss: 1.38695812, g_loss: 0.69921851, ae_loss: 0.04955318\n",
      "Step: [5684] total_loss: 2.13852835 d_loss: 1.38434970, g_loss: 0.69816518, ae_loss: 0.05601335\n",
      "Step: [5685] total_loss: 2.12359023 d_loss: 1.38241792, g_loss: 0.69234383, ae_loss: 0.04882850\n",
      "Step: [5686] total_loss: 2.13007116 d_loss: 1.35713291, g_loss: 0.71985251, ae_loss: 0.05308581\n",
      "Step: [5687] total_loss: 2.12909532 d_loss: 1.39254379, g_loss: 0.68687546, ae_loss: 0.04967610\n",
      "Step: [5688] total_loss: 2.12129045 d_loss: 1.39466298, g_loss: 0.67493415, ae_loss: 0.05169328\n",
      "Step: [5689] total_loss: 2.12146711 d_loss: 1.37615633, g_loss: 0.69398582, ae_loss: 0.05132512\n",
      "Step: [5690] total_loss: 2.13085365 d_loss: 1.40745437, g_loss: 0.67415452, ae_loss: 0.04924471\n",
      "Step: [5691] total_loss: 2.12069058 d_loss: 1.38553071, g_loss: 0.68480247, ae_loss: 0.05035742\n",
      "Step: [5692] total_loss: 2.13119936 d_loss: 1.37600422, g_loss: 0.70713198, ae_loss: 0.04806307\n",
      "Step: [5693] total_loss: 2.12108135 d_loss: 1.36934030, g_loss: 0.70013899, ae_loss: 0.05160214\n",
      "Step: [5694] total_loss: 2.13180304 d_loss: 1.38621879, g_loss: 0.69829625, ae_loss: 0.04728810\n",
      "Step: [5695] total_loss: 2.14971876 d_loss: 1.38368630, g_loss: 0.71314049, ae_loss: 0.05289203\n",
      "Step: [5696] total_loss: 2.11480188 d_loss: 1.37897301, g_loss: 0.68647325, ae_loss: 0.04935560\n",
      "Step: [5697] total_loss: 2.10565639 d_loss: 1.36575186, g_loss: 0.69030595, ae_loss: 0.04959858\n",
      "Step: [5698] total_loss: 2.11983418 d_loss: 1.38267803, g_loss: 0.68811458, ae_loss: 0.04904161\n",
      "Step: [5699] total_loss: 2.11082506 d_loss: 1.36060905, g_loss: 0.69919515, ae_loss: 0.05102098\n",
      "Step: [5700] total_loss: 2.10204315 d_loss: 1.34976697, g_loss: 0.69831884, ae_loss: 0.05395724\n",
      "Step: [5701] total_loss: 2.10681796 d_loss: 1.37664998, g_loss: 0.68044978, ae_loss: 0.04971823\n",
      "Step: [5702] total_loss: 2.12561202 d_loss: 1.38097692, g_loss: 0.69236237, ae_loss: 0.05227285\n",
      "Step: [5703] total_loss: 2.13869143 d_loss: 1.36589336, g_loss: 0.72324181, ae_loss: 0.04955612\n",
      "Step: [5704] total_loss: 2.12501287 d_loss: 1.37754893, g_loss: 0.69725490, ae_loss: 0.05020916\n",
      "Step: [5705] total_loss: 2.14574122 d_loss: 1.37677217, g_loss: 0.71663165, ae_loss: 0.05233746\n",
      "Step: [5706] total_loss: 2.13365769 d_loss: 1.35894346, g_loss: 0.72494674, ae_loss: 0.04976745\n",
      "Step: [5707] total_loss: 2.12705231 d_loss: 1.37765789, g_loss: 0.69608313, ae_loss: 0.05331127\n",
      "Step: [5708] total_loss: 2.13511443 d_loss: 1.36902583, g_loss: 0.71433991, ae_loss: 0.05174866\n",
      "Step: [5709] total_loss: 2.12381244 d_loss: 1.38053465, g_loss: 0.68891817, ae_loss: 0.05435964\n",
      "Step: [5710] total_loss: 2.12908053 d_loss: 1.37360811, g_loss: 0.70325607, ae_loss: 0.05221644\n",
      "Step: [5711] total_loss: 2.13440466 d_loss: 1.37852609, g_loss: 0.70711011, ae_loss: 0.04876848\n",
      "Step: [5712] total_loss: 2.13526034 d_loss: 1.37772179, g_loss: 0.70648706, ae_loss: 0.05105146\n",
      "Step: [5713] total_loss: 2.12437248 d_loss: 1.37282372, g_loss: 0.69737148, ae_loss: 0.05417744\n",
      "Step: [5714] total_loss: 2.10712671 d_loss: 1.37992716, g_loss: 0.67642820, ae_loss: 0.05077134\n",
      "Step: [5715] total_loss: 2.10857296 d_loss: 1.36553264, g_loss: 0.69146705, ae_loss: 0.05157311\n",
      "Step: [5716] total_loss: 2.16026449 d_loss: 1.37033606, g_loss: 0.74339014, ae_loss: 0.04653835\n",
      "Step: [5717] total_loss: 2.12702560 d_loss: 1.37026215, g_loss: 0.70905960, ae_loss: 0.04770391\n",
      "Step: [5718] total_loss: 2.11465240 d_loss: 1.36336327, g_loss: 0.70400608, ae_loss: 0.04728306\n",
      "Step: [5719] total_loss: 2.15303016 d_loss: 1.40179420, g_loss: 0.69858539, ae_loss: 0.05265053\n",
      "Step: [5720] total_loss: 2.13627791 d_loss: 1.38504457, g_loss: 0.70295030, ae_loss: 0.04828295\n",
      "Step: [5721] total_loss: 2.12983561 d_loss: 1.37871885, g_loss: 0.70025289, ae_loss: 0.05086386\n",
      "Step: [5722] total_loss: 2.13366961 d_loss: 1.38076711, g_loss: 0.69996446, ae_loss: 0.05293816\n",
      "Step: [5723] total_loss: 2.13665652 d_loss: 1.38221312, g_loss: 0.70228362, ae_loss: 0.05215979\n",
      "Step: [5724] total_loss: 2.09346604 d_loss: 1.35249591, g_loss: 0.68658638, ae_loss: 0.05438377\n",
      "Step: [5725] total_loss: 2.11078024 d_loss: 1.38660920, g_loss: 0.67192626, ae_loss: 0.05224463\n",
      "Step: [5726] total_loss: 2.10412812 d_loss: 1.35583067, g_loss: 0.69762892, ae_loss: 0.05066859\n",
      "Step: [5727] total_loss: 2.13130260 d_loss: 1.40420640, g_loss: 0.67294389, ae_loss: 0.05415241\n",
      "Step: [5728] total_loss: 2.10521984 d_loss: 1.37183797, g_loss: 0.68440318, ae_loss: 0.04897853\n",
      "Step: [5729] total_loss: 2.12924910 d_loss: 1.38779283, g_loss: 0.69483185, ae_loss: 0.04662430\n",
      "Step: [5730] total_loss: 2.11808443 d_loss: 1.36376381, g_loss: 0.70349956, ae_loss: 0.05082105\n",
      "Step: [5731] total_loss: 2.12163639 d_loss: 1.36019325, g_loss: 0.70763648, ae_loss: 0.05380653\n",
      "Step: [5732] total_loss: 2.11132336 d_loss: 1.37325358, g_loss: 0.68769914, ae_loss: 0.05037072\n",
      "Step: [5733] total_loss: 2.11881542 d_loss: 1.39005363, g_loss: 0.68227351, ae_loss: 0.04648846\n",
      "Step: [5734] total_loss: 2.13064623 d_loss: 1.38241243, g_loss: 0.69630986, ae_loss: 0.05192401\n",
      "Step: [5735] total_loss: 2.11304617 d_loss: 1.39009118, g_loss: 0.66757464, ae_loss: 0.05538023\n",
      "Step: [5736] total_loss: 2.11031532 d_loss: 1.37535560, g_loss: 0.68392837, ae_loss: 0.05103153\n",
      "Step: [5737] total_loss: 2.12412119 d_loss: 1.38464284, g_loss: 0.68817151, ae_loss: 0.05130676\n",
      "Step: [5738] total_loss: 2.13161135 d_loss: 1.37799776, g_loss: 0.70660770, ae_loss: 0.04700593\n",
      "Step: [5739] total_loss: 2.13813186 d_loss: 1.39153242, g_loss: 0.69470400, ae_loss: 0.05189533\n",
      "Step: [5740] total_loss: 2.13016367 d_loss: 1.39130354, g_loss: 0.68519807, ae_loss: 0.05366201\n",
      "Step: [5741] total_loss: 2.13404179 d_loss: 1.39342356, g_loss: 0.68494427, ae_loss: 0.05567388\n",
      "Step: [5742] total_loss: 2.13308001 d_loss: 1.39325535, g_loss: 0.68823719, ae_loss: 0.05158732\n",
      "Step: [5743] total_loss: 2.10292673 d_loss: 1.36608851, g_loss: 0.68883073, ae_loss: 0.04800731\n",
      "Step: [5744] total_loss: 2.12067962 d_loss: 1.37992764, g_loss: 0.69362938, ae_loss: 0.04712255\n",
      "Step: [5745] total_loss: 2.12391567 d_loss: 1.37892199, g_loss: 0.68999529, ae_loss: 0.05499833\n",
      "Step: [5746] total_loss: 2.13546085 d_loss: 1.39665604, g_loss: 0.68568254, ae_loss: 0.05312219\n",
      "Step: [5747] total_loss: 2.12919855 d_loss: 1.38218856, g_loss: 0.69299316, ae_loss: 0.05401671\n",
      "Step: [5748] total_loss: 2.12914753 d_loss: 1.34815180, g_loss: 0.72918475, ae_loss: 0.05181099\n",
      "Step: [5749] total_loss: 2.12920427 d_loss: 1.38190258, g_loss: 0.69723701, ae_loss: 0.05006484\n",
      "Step: [5750] total_loss: 2.16237307 d_loss: 1.38243425, g_loss: 0.72426796, ae_loss: 0.05567073\n",
      "Step: [5751] total_loss: 2.12618375 d_loss: 1.37428272, g_loss: 0.69802600, ae_loss: 0.05387502\n",
      "Step: [5752] total_loss: 2.11041927 d_loss: 1.35989094, g_loss: 0.69530630, ae_loss: 0.05522205\n",
      "Step: [5753] total_loss: 2.14205360 d_loss: 1.40893674, g_loss: 0.68079549, ae_loss: 0.05232135\n",
      "Step: [5754] total_loss: 2.12052870 d_loss: 1.36771560, g_loss: 0.70183372, ae_loss: 0.05097952\n",
      "Step: [5755] total_loss: 2.10905004 d_loss: 1.37735581, g_loss: 0.68237287, ae_loss: 0.04932138\n",
      "Step: [5756] total_loss: 2.10551357 d_loss: 1.37027431, g_loss: 0.68763995, ae_loss: 0.04759943\n",
      "Step: [5757] total_loss: 2.11765695 d_loss: 1.36636555, g_loss: 0.70097673, ae_loss: 0.05031471\n",
      "Step: [5758] total_loss: 2.14265513 d_loss: 1.36711776, g_loss: 0.72414708, ae_loss: 0.05139032\n",
      "Step: [5759] total_loss: 2.11310029 d_loss: 1.37520874, g_loss: 0.68891126, ae_loss: 0.04898037\n",
      "Step: [5760] total_loss: 2.10819530 d_loss: 1.37792540, g_loss: 0.67705524, ae_loss: 0.05321474\n",
      "Step: [5761] total_loss: 2.12363100 d_loss: 1.37660408, g_loss: 0.69445777, ae_loss: 0.05256920\n",
      "Step: [5762] total_loss: 2.13758659 d_loss: 1.39924812, g_loss: 0.68344325, ae_loss: 0.05489513\n",
      "Step: [5763] total_loss: 2.12265992 d_loss: 1.38656735, g_loss: 0.68157208, ae_loss: 0.05452053\n",
      "Step: [5764] total_loss: 2.12826324 d_loss: 1.38059556, g_loss: 0.69659066, ae_loss: 0.05107697\n",
      "Step: [5765] total_loss: 2.11623073 d_loss: 1.38895774, g_loss: 0.67746460, ae_loss: 0.04980841\n",
      "Step: [5766] total_loss: 2.11197495 d_loss: 1.34607053, g_loss: 0.71271545, ae_loss: 0.05318891\n",
      "Step: [5767] total_loss: 2.11368537 d_loss: 1.37206626, g_loss: 0.69217587, ae_loss: 0.04944328\n",
      "Step: [5768] total_loss: 2.12786436 d_loss: 1.40056527, g_loss: 0.67619199, ae_loss: 0.05110712\n",
      "Step: [5769] total_loss: 2.11650276 d_loss: 1.38836646, g_loss: 0.67838740, ae_loss: 0.04974880\n",
      "Step: [5770] total_loss: 2.12538242 d_loss: 1.39136910, g_loss: 0.68315405, ae_loss: 0.05085923\n",
      "Step: [5771] total_loss: 2.10987234 d_loss: 1.35836840, g_loss: 0.69767904, ae_loss: 0.05382493\n",
      "Step: [5772] total_loss: 2.12907600 d_loss: 1.38545680, g_loss: 0.69480890, ae_loss: 0.04881022\n",
      "Step: [5773] total_loss: 2.12847137 d_loss: 1.38990748, g_loss: 0.68336749, ae_loss: 0.05519635\n",
      "Step: [5774] total_loss: 2.10420465 d_loss: 1.35249960, g_loss: 0.70077628, ae_loss: 0.05092877\n",
      "Step: [5775] total_loss: 2.12461281 d_loss: 1.37344694, g_loss: 0.70097709, ae_loss: 0.05018883\n",
      "Step: [5776] total_loss: 2.13575935 d_loss: 1.37079096, g_loss: 0.71104836, ae_loss: 0.05391999\n",
      "Step: [5777] total_loss: 2.14546347 d_loss: 1.38988817, g_loss: 0.70367628, ae_loss: 0.05189906\n",
      "Step: [5778] total_loss: 2.15840960 d_loss: 1.39841557, g_loss: 0.70771664, ae_loss: 0.05227744\n",
      "Step: [5779] total_loss: 2.13133001 d_loss: 1.39285803, g_loss: 0.68921089, ae_loss: 0.04926111\n",
      "Step: [5780] total_loss: 2.15392685 d_loss: 1.41499138, g_loss: 0.68907297, ae_loss: 0.04986262\n",
      "Step: [5781] total_loss: 2.14278746 d_loss: 1.36006880, g_loss: 0.73317790, ae_loss: 0.04954063\n",
      "Step: [5782] total_loss: 2.10962152 d_loss: 1.36501348, g_loss: 0.69369966, ae_loss: 0.05090830\n",
      "Step: [5783] total_loss: 2.12199926 d_loss: 1.37534809, g_loss: 0.69568515, ae_loss: 0.05096588\n",
      "Step: [5784] total_loss: 2.12741661 d_loss: 1.38174820, g_loss: 0.69263411, ae_loss: 0.05303424\n",
      "Step: [5785] total_loss: 2.10892153 d_loss: 1.37916613, g_loss: 0.68070108, ae_loss: 0.04905441\n",
      "Step: [5786] total_loss: 2.11648512 d_loss: 1.36914277, g_loss: 0.69619894, ae_loss: 0.05114357\n",
      "Step: [5787] total_loss: 2.12805939 d_loss: 1.38126564, g_loss: 0.69557488, ae_loss: 0.05121901\n",
      "Step: [5788] total_loss: 2.14723229 d_loss: 1.41057694, g_loss: 0.68930531, ae_loss: 0.04735002\n",
      "Step: [5789] total_loss: 2.13924456 d_loss: 1.39566004, g_loss: 0.68854654, ae_loss: 0.05503814\n",
      "Step: [5790] total_loss: 2.13068914 d_loss: 1.38667965, g_loss: 0.69583797, ae_loss: 0.04817136\n",
      "Step: [5791] total_loss: 2.13626075 d_loss: 1.38421881, g_loss: 0.70397669, ae_loss: 0.04806535\n",
      "Step: [5792] total_loss: 2.12603831 d_loss: 1.40794063, g_loss: 0.66795689, ae_loss: 0.05014085\n",
      "Step: [5793] total_loss: 2.13260460 d_loss: 1.38426137, g_loss: 0.70144212, ae_loss: 0.04690115\n",
      "Step: [5794] total_loss: 2.12382412 d_loss: 1.39256716, g_loss: 0.68483543, ae_loss: 0.04642156\n",
      "Step: [5795] total_loss: 2.12692118 d_loss: 1.39267635, g_loss: 0.68134558, ae_loss: 0.05289939\n",
      "Step: [5796] total_loss: 2.11033106 d_loss: 1.37244391, g_loss: 0.68611515, ae_loss: 0.05177199\n",
      "Step: [5797] total_loss: 2.11759901 d_loss: 1.36943567, g_loss: 0.69865674, ae_loss: 0.04950657\n",
      "Step: [5798] total_loss: 2.12549281 d_loss: 1.38530982, g_loss: 0.68676859, ae_loss: 0.05341444\n",
      "Step: [5799] total_loss: 2.13806391 d_loss: 1.40607095, g_loss: 0.68037772, ae_loss: 0.05161512\n",
      "Step: [5800] total_loss: 2.16749954 d_loss: 1.42014050, g_loss: 0.69272614, ae_loss: 0.05463276\n",
      "Step: [5801] total_loss: 2.14496326 d_loss: 1.37744808, g_loss: 0.71907717, ae_loss: 0.04843813\n",
      "Step: [5802] total_loss: 2.13455296 d_loss: 1.37958586, g_loss: 0.70608234, ae_loss: 0.04888470\n",
      "Step: [5803] total_loss: 2.12334061 d_loss: 1.38480067, g_loss: 0.68396878, ae_loss: 0.05457102\n",
      "Step: [5804] total_loss: 2.13129544 d_loss: 1.37951410, g_loss: 0.70232242, ae_loss: 0.04945897\n",
      "Step: [5805] total_loss: 2.11136723 d_loss: 1.36533237, g_loss: 0.69375682, ae_loss: 0.05227799\n",
      "Step: [5806] total_loss: 2.11673355 d_loss: 1.39200830, g_loss: 0.67826897, ae_loss: 0.04645623\n",
      "Step: [5807] total_loss: 2.12286758 d_loss: 1.38415492, g_loss: 0.68245876, ae_loss: 0.05625399\n",
      "Step: [5808] total_loss: 2.11318111 d_loss: 1.38248038, g_loss: 0.68370235, ae_loss: 0.04699856\n",
      "Step: [5809] total_loss: 2.12743330 d_loss: 1.39863133, g_loss: 0.67699969, ae_loss: 0.05180214\n",
      "Step: [5810] total_loss: 2.11967850 d_loss: 1.37806296, g_loss: 0.69183981, ae_loss: 0.04977565\n",
      "Step: [5811] total_loss: 2.13924551 d_loss: 1.36330163, g_loss: 0.72762960, ae_loss: 0.04831430\n",
      "Step: [5812] total_loss: 2.12218904 d_loss: 1.37881303, g_loss: 0.69146609, ae_loss: 0.05190981\n",
      "Step: [5813] total_loss: 2.12902594 d_loss: 1.35501635, g_loss: 0.72063863, ae_loss: 0.05337090\n",
      "Step: [5814] total_loss: 2.12768126 d_loss: 1.38305998, g_loss: 0.69140804, ae_loss: 0.05321321\n",
      "Step: [5815] total_loss: 2.14526439 d_loss: 1.40065169, g_loss: 0.69371033, ae_loss: 0.05090240\n",
      "Step: [5816] total_loss: 2.12560797 d_loss: 1.38042462, g_loss: 0.68768466, ae_loss: 0.05749874\n",
      "Step: [5817] total_loss: 2.12900829 d_loss: 1.38948298, g_loss: 0.68579459, ae_loss: 0.05373085\n",
      "Step: [5818] total_loss: 2.10739875 d_loss: 1.35969663, g_loss: 0.69870472, ae_loss: 0.04899741\n",
      "Step: [5819] total_loss: 2.12022018 d_loss: 1.38902974, g_loss: 0.67728722, ae_loss: 0.05390334\n",
      "Step: [5820] total_loss: 2.11015272 d_loss: 1.37984765, g_loss: 0.68087244, ae_loss: 0.04943271\n",
      "Step: [5821] total_loss: 2.12914038 d_loss: 1.39531243, g_loss: 0.68209970, ae_loss: 0.05172809\n",
      "Step: [5822] total_loss: 2.12525868 d_loss: 1.40308523, g_loss: 0.67245746, ae_loss: 0.04971601\n",
      "Step: [5823] total_loss: 2.14985561 d_loss: 1.39266574, g_loss: 0.70259511, ae_loss: 0.05459462\n",
      "Step: [5824] total_loss: 2.14436674 d_loss: 1.37900460, g_loss: 0.72054797, ae_loss: 0.04481424\n",
      "Step: [5825] total_loss: 2.14028120 d_loss: 1.38164699, g_loss: 0.70484948, ae_loss: 0.05378461\n",
      "Step: [5826] total_loss: 2.13957095 d_loss: 1.35727692, g_loss: 0.73052889, ae_loss: 0.05176505\n",
      "Step: [5827] total_loss: 2.12459183 d_loss: 1.38023853, g_loss: 0.69157887, ae_loss: 0.05277435\n",
      "Step: [5828] total_loss: 2.11726022 d_loss: 1.37758362, g_loss: 0.68916619, ae_loss: 0.05051037\n",
      "Step: [5829] total_loss: 2.11920738 d_loss: 1.36542082, g_loss: 0.69879723, ae_loss: 0.05498927\n",
      "Step: [5830] total_loss: 2.11739159 d_loss: 1.37373996, g_loss: 0.69126302, ae_loss: 0.05238859\n",
      "Step: [5831] total_loss: 2.11812615 d_loss: 1.37964451, g_loss: 0.68906564, ae_loss: 0.04941600\n",
      "Step: [5832] total_loss: 2.11673307 d_loss: 1.37771821, g_loss: 0.68546724, ae_loss: 0.05354754\n",
      "Step: [5833] total_loss: 2.10029697 d_loss: 1.36376905, g_loss: 0.68797982, ae_loss: 0.04854816\n",
      "Step: [5834] total_loss: 2.10930777 d_loss: 1.37742496, g_loss: 0.67995036, ae_loss: 0.05193251\n",
      "Step: [5835] total_loss: 2.14788151 d_loss: 1.36460805, g_loss: 0.73089528, ae_loss: 0.05237834\n",
      "Step: [5836] total_loss: 2.12896633 d_loss: 1.38538456, g_loss: 0.68530363, ae_loss: 0.05827813\n",
      "Step: [5837] total_loss: 2.10265875 d_loss: 1.35547578, g_loss: 0.69334638, ae_loss: 0.05383670\n",
      "Step: [5838] total_loss: 2.11161780 d_loss: 1.37593186, g_loss: 0.68846476, ae_loss: 0.04722119\n",
      "Step: [5839] total_loss: 2.13331795 d_loss: 1.38083875, g_loss: 0.70378250, ae_loss: 0.04869669\n",
      "Step: [5840] total_loss: 2.12468481 d_loss: 1.36466670, g_loss: 0.70796812, ae_loss: 0.05205012\n",
      "Step: [5841] total_loss: 2.11740041 d_loss: 1.36779654, g_loss: 0.69954401, ae_loss: 0.05005985\n",
      "Step: [5842] total_loss: 2.11127687 d_loss: 1.36649799, g_loss: 0.69445348, ae_loss: 0.05032540\n",
      "Step: [5843] total_loss: 2.12968755 d_loss: 1.36132455, g_loss: 0.71462238, ae_loss: 0.05374066\n",
      "Step: [5844] total_loss: 2.15261126 d_loss: 1.36997199, g_loss: 0.72953832, ae_loss: 0.05310089\n",
      "Step: [5845] total_loss: 2.13247252 d_loss: 1.38033640, g_loss: 0.69587219, ae_loss: 0.05626408\n",
      "Step: [5846] total_loss: 2.12917089 d_loss: 1.37790239, g_loss: 0.70168471, ae_loss: 0.04958370\n",
      "Step: [5847] total_loss: 2.11862659 d_loss: 1.37850785, g_loss: 0.68982458, ae_loss: 0.05029431\n",
      "Step: [5848] total_loss: 2.10242939 d_loss: 1.37573767, g_loss: 0.67502415, ae_loss: 0.05166743\n",
      "Step: [5849] total_loss: 2.10756159 d_loss: 1.37647319, g_loss: 0.67808765, ae_loss: 0.05300084\n",
      "Step: [5850] total_loss: 2.12427497 d_loss: 1.39826560, g_loss: 0.67647278, ae_loss: 0.04953654\n",
      "Step: [5851] total_loss: 2.13171959 d_loss: 1.38481951, g_loss: 0.69326758, ae_loss: 0.05363245\n",
      "Step: [5852] total_loss: 2.13305664 d_loss: 1.39258456, g_loss: 0.68566859, ae_loss: 0.05480364\n",
      "Step: [5853] total_loss: 2.09782577 d_loss: 1.34876204, g_loss: 0.69300687, ae_loss: 0.05605685\n",
      "Step: [5854] total_loss: 2.15616035 d_loss: 1.39279079, g_loss: 0.71102536, ae_loss: 0.05234422\n",
      "Step: [5855] total_loss: 2.14580750 d_loss: 1.39115036, g_loss: 0.70094234, ae_loss: 0.05371483\n",
      "Step: [5856] total_loss: 2.14807320 d_loss: 1.38346469, g_loss: 0.70619488, ae_loss: 0.05841355\n",
      "Step: [5857] total_loss: 2.13585496 d_loss: 1.38185501, g_loss: 0.70288807, ae_loss: 0.05111185\n",
      "Step: [5858] total_loss: 2.11487913 d_loss: 1.37250745, g_loss: 0.69161332, ae_loss: 0.05075839\n",
      "Step: [5859] total_loss: 2.10509682 d_loss: 1.37095189, g_loss: 0.67965209, ae_loss: 0.05449268\n",
      "Step: [5860] total_loss: 2.12787724 d_loss: 1.39393079, g_loss: 0.68162525, ae_loss: 0.05232131\n",
      "Step: [5861] total_loss: 2.14370537 d_loss: 1.37663245, g_loss: 0.71457744, ae_loss: 0.05249548\n",
      "Step: [5862] total_loss: 2.12649632 d_loss: 1.38923550, g_loss: 0.69157457, ae_loss: 0.04568619\n",
      "Step: [5863] total_loss: 2.13850379 d_loss: 1.37917709, g_loss: 0.70724529, ae_loss: 0.05208132\n",
      "Step: [5864] total_loss: 2.11434078 d_loss: 1.36901820, g_loss: 0.69654918, ae_loss: 0.04877332\n",
      "Step: [5865] total_loss: 2.10787964 d_loss: 1.37688458, g_loss: 0.68326533, ae_loss: 0.04772976\n",
      "Step: [5866] total_loss: 2.13202763 d_loss: 1.39347267, g_loss: 0.68774658, ae_loss: 0.05080826\n",
      "Step: [5867] total_loss: 2.11241746 d_loss: 1.37336099, g_loss: 0.68554997, ae_loss: 0.05350654\n",
      "Step: [5868] total_loss: 2.14982915 d_loss: 1.41173935, g_loss: 0.68528265, ae_loss: 0.05280708\n",
      "Step: [5869] total_loss: 2.12044001 d_loss: 1.37873638, g_loss: 0.69180119, ae_loss: 0.04990241\n",
      "Step: [5870] total_loss: 2.15060377 d_loss: 1.38803840, g_loss: 0.70989299, ae_loss: 0.05267243\n",
      "Step: [5871] total_loss: 2.14135981 d_loss: 1.38770306, g_loss: 0.70205450, ae_loss: 0.05160218\n",
      "Step: [5872] total_loss: 2.14362621 d_loss: 1.37735045, g_loss: 0.71696889, ae_loss: 0.04930688\n",
      "Step: [5873] total_loss: 2.13022423 d_loss: 1.38572502, g_loss: 0.69050223, ae_loss: 0.05399696\n",
      "Step: [5874] total_loss: 2.11281013 d_loss: 1.37417984, g_loss: 0.68786478, ae_loss: 0.05076549\n",
      "Step: [5875] total_loss: 2.10326695 d_loss: 1.37539244, g_loss: 0.68061554, ae_loss: 0.04725892\n",
      "Step: [5876] total_loss: 2.13676405 d_loss: 1.41041780, g_loss: 0.67675465, ae_loss: 0.04959170\n",
      "Step: [5877] total_loss: 2.13013697 d_loss: 1.39596939, g_loss: 0.67992103, ae_loss: 0.05424649\n",
      "Step: [5878] total_loss: 2.14371848 d_loss: 1.40365911, g_loss: 0.68940639, ae_loss: 0.05065303\n",
      "Step: [5879] total_loss: 2.11750698 d_loss: 1.37751651, g_loss: 0.68936658, ae_loss: 0.05062389\n",
      "Step: [5880] total_loss: 2.13085842 d_loss: 1.39896703, g_loss: 0.67954934, ae_loss: 0.05234200\n",
      "Step: [5881] total_loss: 2.13568830 d_loss: 1.38905001, g_loss: 0.69237101, ae_loss: 0.05426729\n",
      "Step: [5882] total_loss: 2.12507820 d_loss: 1.37993932, g_loss: 0.69544315, ae_loss: 0.04969576\n",
      "Step: [5883] total_loss: 2.12521577 d_loss: 1.37333524, g_loss: 0.70477277, ae_loss: 0.04710774\n",
      "Step: [5884] total_loss: 2.12784767 d_loss: 1.38100410, g_loss: 0.69519103, ae_loss: 0.05165245\n",
      "Step: [5885] total_loss: 2.15961528 d_loss: 1.41982174, g_loss: 0.68932551, ae_loss: 0.05046806\n",
      "Step: [5886] total_loss: 2.14889050 d_loss: 1.40262485, g_loss: 0.69315898, ae_loss: 0.05310681\n",
      "Step: [5887] total_loss: 2.15286970 d_loss: 1.38134611, g_loss: 0.72140336, ae_loss: 0.05012037\n",
      "Step: [5888] total_loss: 2.16289926 d_loss: 1.39189577, g_loss: 0.71939504, ae_loss: 0.05160844\n",
      "Step: [5889] total_loss: 2.15704203 d_loss: 1.40537477, g_loss: 0.69639838, ae_loss: 0.05526891\n",
      "Step: [5890] total_loss: 2.12787938 d_loss: 1.37647724, g_loss: 0.70005649, ae_loss: 0.05134555\n",
      "Step: [5891] total_loss: 2.14447236 d_loss: 1.36595726, g_loss: 0.72993755, ae_loss: 0.04857760\n",
      "Step: [5892] total_loss: 2.13544226 d_loss: 1.38489747, g_loss: 0.69818878, ae_loss: 0.05235602\n",
      "Step: [5893] total_loss: 2.12088275 d_loss: 1.37148094, g_loss: 0.70088500, ae_loss: 0.04851678\n",
      "Step: [5894] total_loss: 2.12633276 d_loss: 1.36007905, g_loss: 0.71260870, ae_loss: 0.05364512\n",
      "Step: [5895] total_loss: 2.13069463 d_loss: 1.38884413, g_loss: 0.69280440, ae_loss: 0.04904621\n",
      "Step: [5896] total_loss: 2.13787603 d_loss: 1.37706399, g_loss: 0.71303380, ae_loss: 0.04777826\n",
      "Step: [5897] total_loss: 2.12398958 d_loss: 1.38163066, g_loss: 0.69576001, ae_loss: 0.04659884\n",
      "Step: [5898] total_loss: 2.15021491 d_loss: 1.39376664, g_loss: 0.70330578, ae_loss: 0.05314239\n",
      "Step: [5899] total_loss: 2.11567450 d_loss: 1.40100658, g_loss: 0.66608745, ae_loss: 0.04858041\n",
      "Step: [5900] total_loss: 2.11267638 d_loss: 1.35704243, g_loss: 0.70749015, ae_loss: 0.04814387\n",
      "Step: [5901] total_loss: 2.12660456 d_loss: 1.36767244, g_loss: 0.70765626, ae_loss: 0.05127599\n",
      "Step: [5902] total_loss: 2.11281824 d_loss: 1.35723519, g_loss: 0.70850694, ae_loss: 0.04707615\n",
      "Step: [5903] total_loss: 2.11420345 d_loss: 1.38670015, g_loss: 0.67777395, ae_loss: 0.04972949\n",
      "Step: [5904] total_loss: 2.10440731 d_loss: 1.37148583, g_loss: 0.68551660, ae_loss: 0.04740499\n",
      "Step: [5905] total_loss: 2.13649154 d_loss: 1.37621188, g_loss: 0.70695567, ae_loss: 0.05332393\n",
      "Step: [5906] total_loss: 2.15637779 d_loss: 1.38939643, g_loss: 0.71664119, ae_loss: 0.05034030\n",
      "Step: [5907] total_loss: 2.13735652 d_loss: 1.37632918, g_loss: 0.70783234, ae_loss: 0.05319500\n",
      "Step: [5908] total_loss: 2.12823677 d_loss: 1.36942685, g_loss: 0.70737481, ae_loss: 0.05143505\n",
      "Step: [5909] total_loss: 2.12565374 d_loss: 1.38967800, g_loss: 0.68383729, ae_loss: 0.05213849\n",
      "Step: [5910] total_loss: 2.13699818 d_loss: 1.40133846, g_loss: 0.68524885, ae_loss: 0.05041076\n",
      "Step: [5911] total_loss: 2.12194300 d_loss: 1.38518858, g_loss: 0.68688953, ae_loss: 0.04986486\n",
      "Step: [5912] total_loss: 2.12289834 d_loss: 1.37440014, g_loss: 0.69446921, ae_loss: 0.05402894\n",
      "Step: [5913] total_loss: 2.12704754 d_loss: 1.38924348, g_loss: 0.68430769, ae_loss: 0.05349619\n",
      "Step: [5914] total_loss: 2.12590265 d_loss: 1.38209414, g_loss: 0.69234616, ae_loss: 0.05146224\n",
      "Step: [5915] total_loss: 2.10696840 d_loss: 1.36397743, g_loss: 0.69430804, ae_loss: 0.04868288\n",
      "Step: [5916] total_loss: 2.12600517 d_loss: 1.38218021, g_loss: 0.69102550, ae_loss: 0.05279964\n",
      "Step: [5917] total_loss: 2.13407660 d_loss: 1.41104007, g_loss: 0.67205608, ae_loss: 0.05098033\n",
      "Step: [5918] total_loss: 2.11284304 d_loss: 1.35878468, g_loss: 0.70525980, ae_loss: 0.04879873\n",
      "Step: [5919] total_loss: 2.13297892 d_loss: 1.39427602, g_loss: 0.68858123, ae_loss: 0.05012161\n",
      "Step: [5920] total_loss: 2.11445880 d_loss: 1.37595391, g_loss: 0.68531394, ae_loss: 0.05319090\n",
      "Step: [5921] total_loss: 2.11554670 d_loss: 1.38741326, g_loss: 0.68012828, ae_loss: 0.04800515\n",
      "Step: [5922] total_loss: 2.13062787 d_loss: 1.37731147, g_loss: 0.70100075, ae_loss: 0.05231572\n",
      "Step: [5923] total_loss: 2.11267781 d_loss: 1.38516700, g_loss: 0.67568290, ae_loss: 0.05182796\n",
      "Step: [5924] total_loss: 2.11521411 d_loss: 1.37728083, g_loss: 0.69005954, ae_loss: 0.04787377\n",
      "Step: [5925] total_loss: 2.13011694 d_loss: 1.38667440, g_loss: 0.68945169, ae_loss: 0.05399083\n",
      "Step: [5926] total_loss: 2.12554479 d_loss: 1.38647604, g_loss: 0.69194448, ae_loss: 0.04712426\n",
      "Step: [5927] total_loss: 2.13259935 d_loss: 1.39737487, g_loss: 0.68925118, ae_loss: 0.04597320\n",
      "Step: [5928] total_loss: 2.12210083 d_loss: 1.37171400, g_loss: 0.70043898, ae_loss: 0.04994788\n",
      "Step: [5929] total_loss: 2.11707091 d_loss: 1.38057566, g_loss: 0.68523914, ae_loss: 0.05125603\n",
      "Step: [5930] total_loss: 2.13134122 d_loss: 1.38864255, g_loss: 0.69080806, ae_loss: 0.05189059\n",
      "Step: [5931] total_loss: 2.11756802 d_loss: 1.37715864, g_loss: 0.69082761, ae_loss: 0.04958182\n",
      "Step: [5932] total_loss: 2.11372471 d_loss: 1.35675991, g_loss: 0.70407152, ae_loss: 0.05289319\n",
      "Step: [5933] total_loss: 2.14657331 d_loss: 1.39091659, g_loss: 0.70811200, ae_loss: 0.04754471\n",
      "Step: [5934] total_loss: 2.13618612 d_loss: 1.40498948, g_loss: 0.68283141, ae_loss: 0.04836507\n",
      "Step: [5935] total_loss: 2.12881112 d_loss: 1.38048661, g_loss: 0.69938177, ae_loss: 0.04894264\n",
      "Step: [5936] total_loss: 2.11811256 d_loss: 1.36782384, g_loss: 0.69659805, ae_loss: 0.05369059\n",
      "Step: [5937] total_loss: 2.13052773 d_loss: 1.39667344, g_loss: 0.68335271, ae_loss: 0.05050163\n",
      "Step: [5938] total_loss: 2.11952209 d_loss: 1.38516843, g_loss: 0.68372202, ae_loss: 0.05063156\n",
      "Step: [5939] total_loss: 2.12970257 d_loss: 1.39058101, g_loss: 0.69007146, ae_loss: 0.04905004\n",
      "Step: [5940] total_loss: 2.12853813 d_loss: 1.37466121, g_loss: 0.70228648, ae_loss: 0.05159056\n",
      "Step: [5941] total_loss: 2.11623001 d_loss: 1.37867773, g_loss: 0.68432963, ae_loss: 0.05322255\n",
      "Step: [5942] total_loss: 2.10929394 d_loss: 1.38336706, g_loss: 0.68134868, ae_loss: 0.04457805\n",
      "Step: [5943] total_loss: 2.12760925 d_loss: 1.34952021, g_loss: 0.72731763, ae_loss: 0.05077133\n",
      "Step: [5944] total_loss: 2.12200618 d_loss: 1.38983846, g_loss: 0.68452632, ae_loss: 0.04764136\n",
      "Step: [5945] total_loss: 2.10696149 d_loss: 1.36081886, g_loss: 0.69293964, ae_loss: 0.05320299\n",
      "Step: [5946] total_loss: 2.12581420 d_loss: 1.39365113, g_loss: 0.68097359, ae_loss: 0.05118958\n",
      "Step: [5947] total_loss: 2.13576007 d_loss: 1.38810039, g_loss: 0.69857371, ae_loss: 0.04908598\n",
      "Step: [5948] total_loss: 2.12491298 d_loss: 1.37631571, g_loss: 0.70127338, ae_loss: 0.04732394\n",
      "Step: [5949] total_loss: 2.10807371 d_loss: 1.37021255, g_loss: 0.68874210, ae_loss: 0.04911896\n",
      "Step: [5950] total_loss: 2.10715151 d_loss: 1.35777783, g_loss: 0.69374096, ae_loss: 0.05563253\n",
      "Step: [5951] total_loss: 2.12053084 d_loss: 1.37529755, g_loss: 0.69417977, ae_loss: 0.05105351\n",
      "Step: [5952] total_loss: 2.12129784 d_loss: 1.38054776, g_loss: 0.69508147, ae_loss: 0.04566856\n",
      "Step: [5953] total_loss: 2.11167383 d_loss: 1.36653268, g_loss: 0.69479346, ae_loss: 0.05034763\n",
      "Step: [5954] total_loss: 2.12683058 d_loss: 1.37903666, g_loss: 0.69854659, ae_loss: 0.04924724\n",
      "Step: [5955] total_loss: 2.13839006 d_loss: 1.38319993, g_loss: 0.70461547, ae_loss: 0.05057462\n",
      "Step: [5956] total_loss: 2.10167241 d_loss: 1.36729717, g_loss: 0.68366241, ae_loss: 0.05071286\n",
      "Step: [5957] total_loss: 2.12858248 d_loss: 1.36780262, g_loss: 0.70749760, ae_loss: 0.05328227\n",
      "Step: [5958] total_loss: 2.12819386 d_loss: 1.39016092, g_loss: 0.69061571, ae_loss: 0.04741721\n",
      "Step: [5959] total_loss: 2.12032199 d_loss: 1.38464046, g_loss: 0.68407834, ae_loss: 0.05160324\n",
      "Step: [5960] total_loss: 2.12174892 d_loss: 1.38684475, g_loss: 0.68320119, ae_loss: 0.05170311\n",
      "Step: [5961] total_loss: 2.13871479 d_loss: 1.41284776, g_loss: 0.67556167, ae_loss: 0.05030528\n",
      "Step: [5962] total_loss: 2.12045717 d_loss: 1.38262379, g_loss: 0.68636405, ae_loss: 0.05146922\n",
      "Step: [5963] total_loss: 2.10846734 d_loss: 1.37521994, g_loss: 0.68652618, ae_loss: 0.04672119\n",
      "Step: [5964] total_loss: 2.13262892 d_loss: 1.40055180, g_loss: 0.68010402, ae_loss: 0.05197298\n",
      "Step: [5965] total_loss: 2.13346529 d_loss: 1.39774895, g_loss: 0.68004847, ae_loss: 0.05566797\n",
      "Step: [5966] total_loss: 2.09137177 d_loss: 1.35662556, g_loss: 0.68212283, ae_loss: 0.05262344\n",
      "Step: [5967] total_loss: 2.10806227 d_loss: 1.37132072, g_loss: 0.68674159, ae_loss: 0.05000000\n",
      "Step: [5968] total_loss: 2.12389159 d_loss: 1.40128410, g_loss: 0.67428195, ae_loss: 0.04832557\n",
      "Step: [5969] total_loss: 2.10187483 d_loss: 1.36204839, g_loss: 0.68909526, ae_loss: 0.05073116\n",
      "Step: [5970] total_loss: 2.10351229 d_loss: 1.36705661, g_loss: 0.68376970, ae_loss: 0.05268599\n",
      "Step: [5971] total_loss: 2.11625004 d_loss: 1.37123513, g_loss: 0.69446456, ae_loss: 0.05055026\n",
      "Step: [5972] total_loss: 2.12735653 d_loss: 1.39669633, g_loss: 0.68167984, ae_loss: 0.04898027\n",
      "Step: [5973] total_loss: 2.11442590 d_loss: 1.36776018, g_loss: 0.69532615, ae_loss: 0.05133964\n",
      "Step: [5974] total_loss: 2.14042974 d_loss: 1.38353562, g_loss: 0.70873320, ae_loss: 0.04816086\n",
      "Step: [5975] total_loss: 2.12843204 d_loss: 1.39410174, g_loss: 0.68494684, ae_loss: 0.04938340\n",
      "Step: [5976] total_loss: 2.14260697 d_loss: 1.39979315, g_loss: 0.69356775, ae_loss: 0.04924605\n",
      "Step: [5977] total_loss: 2.11099195 d_loss: 1.36641359, g_loss: 0.69650626, ae_loss: 0.04807205\n",
      "Step: [5978] total_loss: 2.11233139 d_loss: 1.37230563, g_loss: 0.69215298, ae_loss: 0.04787286\n",
      "Step: [5979] total_loss: 2.13361788 d_loss: 1.36919904, g_loss: 0.71058834, ae_loss: 0.05383042\n",
      "Step: [5980] total_loss: 2.10651684 d_loss: 1.37417042, g_loss: 0.68508887, ae_loss: 0.04725743\n",
      "Step: [5981] total_loss: 2.13812184 d_loss: 1.40061772, g_loss: 0.68190116, ae_loss: 0.05560290\n",
      "Step: [5982] total_loss: 2.13254356 d_loss: 1.39465952, g_loss: 0.69222057, ae_loss: 0.04566341\n",
      "Step: [5983] total_loss: 2.12444878 d_loss: 1.35865474, g_loss: 0.71682000, ae_loss: 0.04897404\n",
      "Step: [5984] total_loss: 2.12793326 d_loss: 1.39390516, g_loss: 0.67922920, ae_loss: 0.05479882\n",
      "Step: [5985] total_loss: 2.12363672 d_loss: 1.38706112, g_loss: 0.68858695, ae_loss: 0.04798860\n",
      "Step: [5986] total_loss: 2.12431431 d_loss: 1.37049353, g_loss: 0.70313060, ae_loss: 0.05069016\n",
      "Step: [5987] total_loss: 2.12471390 d_loss: 1.38738132, g_loss: 0.68756652, ae_loss: 0.04976610\n",
      "Step: [5988] total_loss: 2.12578630 d_loss: 1.37504005, g_loss: 0.70116341, ae_loss: 0.04958275\n",
      "Step: [5989] total_loss: 2.12609053 d_loss: 1.38103712, g_loss: 0.68857241, ae_loss: 0.05648117\n",
      "Step: [5990] total_loss: 2.13259935 d_loss: 1.38198566, g_loss: 0.70181334, ae_loss: 0.04880042\n",
      "Step: [5991] total_loss: 2.12978601 d_loss: 1.37813747, g_loss: 0.70099962, ae_loss: 0.05064904\n",
      "Step: [5992] total_loss: 2.14407229 d_loss: 1.39363301, g_loss: 0.70261186, ae_loss: 0.04782749\n",
      "Step: [5993] total_loss: 2.10647631 d_loss: 1.35579109, g_loss: 0.69990957, ae_loss: 0.05077567\n",
      "Step: [5994] total_loss: 2.10060763 d_loss: 1.37229967, g_loss: 0.67690319, ae_loss: 0.05140486\n",
      "Step: [5995] total_loss: 2.12686563 d_loss: 1.40066814, g_loss: 0.67460227, ae_loss: 0.05159524\n",
      "Step: [5996] total_loss: 2.12633324 d_loss: 1.40453005, g_loss: 0.67242205, ae_loss: 0.04938113\n",
      "Step: [5997] total_loss: 2.13334966 d_loss: 1.40263057, g_loss: 0.67663890, ae_loss: 0.05408008\n",
      "Step: [5998] total_loss: 2.12771988 d_loss: 1.38921618, g_loss: 0.69092357, ae_loss: 0.04758015\n",
      "Step: [5999] total_loss: 2.12890196 d_loss: 1.38635349, g_loss: 0.68663561, ae_loss: 0.05591300\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={keep_prob : 0.9})\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.astype(np.float32)\n",
    "    #batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    \n",
    "    '''get the noise data'''\n",
    "    if prior_type =='mixGaussian':\n",
    "        z_label_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = gaussian_mixture(batch_size, z_dim, label_indices=z_label_rand)\n",
    "    elif prior_type == 'swiss_roll':\n",
    "        z_label_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = swiss_roll(batch_size, z_dim, label_indices=z_label_rand)\n",
    "    elif prior_type == 'normal':\n",
    "        batch_z, z_label_rand = gaussian(batch_size, z_dim, use_label_info=True)    \n",
    "    else:\n",
    "        raise Exception(f\"[!] There is no option for {prior_type}\" )\n",
    "\n",
    "    z_id_one_hot_vector = np.zeros((batch_size, y_dim))\n",
    "    z_id_one_hot_vector[np.arange(batch_size), z_label_rand] = 1\n",
    "    \n",
    "    feed_dict_input = {\n",
    "        x_input:batch_images,\n",
    "        x_target:batch_images,\n",
    "        x_label:batch_labels,\n",
    "        z_sample:batch_z,\n",
    "        z_label:z_id_one_hot_vector,\n",
    "        keep_prob: 0.9\n",
    "    }\n",
    "    '''update AE network''' \n",
    "    _, loss_likehood = sess.run([ae_optim, neg_marginal_likelihood], feed_dict=feed_dict_input)\n",
    "    '''update discriminator network'''\n",
    "    _, d_loss = sess.run([d_optim, D_loss], feed_dict=feed_dict_input)\n",
    "    '''update generator network, run 2 times'''\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "     \n",
    "    total_loss = loss_likehood + d_loss + g_loss\n",
    "    # display training status\n",
    "    print(\"Step: [%d] total_loss: %.8f d_loss: %.8f, g_loss: %.8f, ae_loss: %.8f\" % (step_ind, total_loss, d_loss, g_loss, loss_likehood) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if  np.mod(step_ind, 300) == 0:\n",
    "        if plot == 'reproduce': #  Plot for reproduce performance\n",
    "            samples = sess.run(images_reconstruction, feed_dict={x_input:test_batch_images, keep_prob:1})\n",
    "            # put the \"batch_size\" images into one big canvas\n",
    "            display(samples, batch_size, img_w, img_h, image_dims, step_ind, plot='reproduce')\n",
    "            \n",
    "        elif plot == 'mainfold learning': # Plot for manifold learning result\n",
    "            samples = sess.run(fake_images, feed_dict={z_hidden: z_pmlr, keep_prob:1})\n",
    "            display(samples, batch_size, img_w, img_h, image_dims, step_ind, plot='mainfold learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
