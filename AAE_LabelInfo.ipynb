{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the network structure borrow from https://github.com/hwalsuklee/tensorflow-mnist-AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from math import sin,cos,sqrt\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net,wscope,bscope,output_depth):\n",
    "    w_init = tf.contrib.layers.xavier_initializer()\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], initializer=w_init)\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=b_init)\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Distribution Functions\n",
    "here define four prior distribution, which will impose onto the hidden code vector of AAE.   \n",
    "Most codes from https://github.com/musyoku/adversarial-autoencoder/blob/master/aae/sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def uniform(batch_size, n_dim, n_labels=10, minv=-1, maxv=1, label_indices=None):\n",
    "    '''if label_indices is None, then uniform will call numpy.random.uniform()'''\n",
    "    if label_indices is not None:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(label, n_labels):\n",
    "            num = int(np.ceil(np.sqrt(n_labels)))\n",
    "            size = (maxv-minv)*1.0/num\n",
    "            x, y = np.random.uniform(-size/2, size/2, (2,))\n",
    "            i = label / num\n",
    "            j = label % num\n",
    "            x += j*size+minv+0.5*size\n",
    "            y += i*size+minv+0.5*size\n",
    "            return np.array([x, y]).reshape((2,))\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "    else:\n",
    "        z = np.random.uniform(minv, maxv, (batch_size, n_dim)).astype(np.float32)\n",
    "    return z\n",
    "\n",
    "def gaussian(batch_size, n_dim, mean=0, var=1, n_labels=10, use_label_info=False):\n",
    "    if use_label_info:\n",
    "        if n_dim != 2 or n_labels != 10:\n",
    "            raise Exception(\"n_dim must be 2 and n_labels must be 10.\")\n",
    "\n",
    "        def sample(n_labels):\n",
    "            x, y = np.random.normal(mean, var, (2,))\n",
    "            angle = np.angle((x-mean) + 1j*(y-mean), deg=True)\n",
    "            dist = np.sqrt((x-mean)**2+(y-mean)**2)\n",
    "\n",
    "            # label 0\n",
    "            if dist <1.0:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = ((int)((n_labels-1)*angle))//360\n",
    "\n",
    "                if label<0:\n",
    "                    label+=n_labels-1\n",
    "\n",
    "                label += 1\n",
    "\n",
    "            return np.array([x, y]).reshape((2,)), label\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        z_id = np.empty((batch_size), dtype=np.int32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    a_sample, a_label = sample(n_labels)\n",
    "                    z[batch, zi*2:zi*2+2] = a_sample\n",
    "                    z_id[batch] = a_label\n",
    "        return z, z_id\n",
    "    else:\n",
    "        z = np.random.normal(mean, var, (batch_size, n_dim)).astype(np.float32)\n",
    "        return z\n",
    "\n",
    "def gaussian_mixture(batch_size, n_dim=2, n_labels=10, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z\n",
    "\n",
    "def swiss_roll(batch_size, n_dim=2, n_labels=10, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(label, n_labels):\n",
    "        uni = np.random.uniform(0.0, 1.0) / float(n_labels) + float(label) / float(n_labels)\n",
    "        r = sqrt(uni) * 3.0\n",
    "        rad = np.pi * 4.0 * sqrt(uni)\n",
    "        x = r * cos(rad)\n",
    "        y = r * sin(rad)\n",
    "        return np.array([x, y]).reshape((2,))\n",
    "\n",
    "    z = np.zeros((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(np.random.randint(0, n_labels), n_labels)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the discriminator part of the AAE is one MLP network, two hidden fully connected layers, which input is [batch_size, z] matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout''' \n",
    "        net = linear(z, 'd_wlinear0', 'd_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'd_wlinear1', 'd_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'd_wlinear2', 'd_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_encoder(x, n_hidden, n_output, keep_prob, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"AE_encoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropout'''\n",
    "        net = linear(x, 'aeE_wlinear0', 'aeE_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeE_wlinear1', 'aeE_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        z = out_logit = linear(net, 'aeE_wlinear2', 'aeE_blinear2', n_output)\n",
    "\n",
    "    return z\n",
    "\n",
    "def AE_decoder(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "\n",
    "    with tf.variable_scope(\"AE_decoder\", reuse=reuse):\n",
    "\n",
    "        '''1st: linear -> relu -> dropput'''\n",
    "        net = linear(z, 'aeD_wlinear0', 'aeD_blinear0', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''2nd: linear -> relu -> dropout'''\n",
    "        net = linear(net, 'aeD_wlinear1', 'aeD_blinear1', n_hidden)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "        '''3th: linear'''\n",
    "        out_logit = linear(net, 'aeD_wlinear2', 'aeD_blinear2', n_output)\n",
    "        '''4th: sigmoid'''\n",
    "        x_pred = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, img_dim, n_hidden):\n",
    "\n",
    "    y = AE_decoder(z, n_hidden, img_dim, 1.0, reuse=True)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "results_path = './result'\n",
    "n_hidden = 1000  # number of hidden units in MLP\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.5\n",
    "n_epochs = 20\n",
    "prior_type = 'mixGaussian' # one of ['mixGaussian', 'swiss_roll', 'normal']\n",
    "batch_size = 64\n",
    "min_tot_loss = 1e99\n",
    "\n",
    "image_dims = [28, 28, 1]\n",
    "img_dim = reduce(lambda x,y:x*y, image_dims)\n",
    "z_dim = 2\n",
    "y_dim = 10\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# In denoising-autoencoder, x_hat == x + noise; otherwise x_hat == x\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, img_dim], name='input_img')\n",
    "x_target = tf.placeholder(tf.float32, shape=[None, img_dim], name='target_img')\n",
    "x_label = tf.placeholder(tf.float32, shape=[None, y_dim], name='input_img_label')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# input for PMLR\n",
    "z_hidden = tf.placeholder(tf.float32, shape=[None, z_dim], name='latent_variable')\n",
    "\n",
    "# samples drawn from prior distribution\n",
    "z_sample = tf.placeholder(tf.float32, shape=[None, z_dim], name='prior_sample')\n",
    "z_label = tf.placeholder(tf.float32, shape=[None, y_dim], name='prior_sample_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "'''Reconstruction Loss''' \n",
    "\n",
    "# encoding\n",
    "z = AE_encoder(x_input, n_hidden, z_dim, keep_prob)\n",
    "# decoding\n",
    "x_pred = AE_decoder(z, n_hidden, img_dim, keep_prob)\n",
    "# loss\n",
    "marginal_likelihood = -tf.reduce_mean(tf.reduce_mean(tf.squared_difference(x_input, x_pred)))\n",
    "\n",
    "'''GAN Loss'''\n",
    "z_real = tf.concat([z_sample, z_label],1)\n",
    "z_fake = tf.concat([z, x_label],1)\n",
    "\n",
    "D_real, D_real_logits = discriminator(z_real, n_hidden, 1, keep_prob)\n",
    "D_fake, D_fake_logits = discriminator(z_fake, n_hidden, 1, keep_prob, reuse=True)\n",
    "\n",
    "# discriminator loss\n",
    "D_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n",
    "D_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n",
    "D_loss = D_loss_real+D_loss_fake\n",
    "\n",
    "# generator loss\n",
    "G_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))\n",
    "\n",
    "marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "D_loss = tf.reduce_mean(D_loss)\n",
    "G_loss = tf.reduce_mean(G_loss)\n",
    "\n",
    "neg_marginal_likelihood = -1*marginal_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G and a group for ae\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "g_vars = [var for var in t_vars if 'AE_encoder' in var.name]\n",
    "ae_vars = [var for var in t_vars if 'AE_encoder' in var.name or 'AE_decoder' in var.name]\n",
    "\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate/5, beta1=beta1).minimize(D_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(G_loss, var_list=g_vars)\n",
    "ae_optim = tf.train.AdamOptimizer(learning_rate).minimize(neg_marginal_likelihood, var_list=ae_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "\n",
    "# for test\n",
    "test_image_batch = mnist.test.next_batch(batch_size)\n",
    "test_batch_images,test_batch_labels = test_image_batch\n",
    "test_batch_images = test_batch_images.astype(np.float32)\n",
    "images_reconstruction = x_pred\n",
    "#fake_images_y = generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] total_loss: 2.21599221 d_loss: 1.38879454, g_loss: 0.59566182, ae_loss: 0.23153590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3/lib/python3.6/site-packages/ipykernel_launcher.py:65: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1] total_loss: 2.67728233 d_loss: 2.06396151, g_loss: 0.39813507, ae_loss: 0.21518576\n",
      "Step: [2] total_loss: 4.12332821 d_loss: 2.60292339, g_loss: 1.37662792, ae_loss: 0.14377700\n",
      "Step: [3] total_loss: 2.59159422 d_loss: 1.04359603, g_loss: 1.43003082, ae_loss: 0.11796747\n",
      "Step: [4] total_loss: 2.14255381 d_loss: 1.06647480, g_loss: 0.92500788, ae_loss: 0.15107122\n",
      "Step: [5] total_loss: 2.18356276 d_loss: 1.21848202, g_loss: 0.79798466, ae_loss: 0.16709606\n",
      "Step: [6] total_loss: 2.17771864 d_loss: 1.26613092, g_loss: 0.74798846, ae_loss: 0.16359928\n",
      "Step: [7] total_loss: 2.18939590 d_loss: 1.33981001, g_loss: 0.69774544, ae_loss: 0.15184033\n",
      "Step: [8] total_loss: 2.18388796 d_loss: 1.46723795, g_loss: 0.57318348, ae_loss: 0.14346646\n",
      "Step: [9] total_loss: 2.32688498 d_loss: 1.76212740, g_loss: 0.44496647, ae_loss: 0.11979098\n",
      "Step: [10] total_loss: 2.78885841 d_loss: 2.36931276, g_loss: 0.33110681, ae_loss: 0.08843864\n",
      "Step: [11] total_loss: 3.47601724 d_loss: 2.86676025, g_loss: 0.51798511, ae_loss: 0.09127185\n",
      "Step: [12] total_loss: 4.00597906 d_loss: 2.14438105, g_loss: 1.77248275, ae_loss: 0.08911514\n",
      "Step: [13] total_loss: 2.33977318 d_loss: 1.24514484, g_loss: 1.01988697, ae_loss: 0.07474127\n",
      "Step: [14] total_loss: 2.28028107 d_loss: 1.18033910, g_loss: 1.00425339, ae_loss: 0.09568841\n",
      "Step: [15] total_loss: 2.38141084 d_loss: 1.40781844, g_loss: 0.84851837, ae_loss: 0.12507409\n",
      "Step: [16] total_loss: 2.32563162 d_loss: 1.34883416, g_loss: 0.85884345, ae_loss: 0.11795416\n",
      "Step: [17] total_loss: 2.38292384 d_loss: 1.45738637, g_loss: 0.80151725, ae_loss: 0.12402025\n",
      "Step: [18] total_loss: 2.30163574 d_loss: 1.36050928, g_loss: 0.82850307, ae_loss: 0.11262350\n",
      "Step: [19] total_loss: 2.36241293 d_loss: 1.45508993, g_loss: 0.79725230, ae_loss: 0.11007077\n",
      "Step: [20] total_loss: 2.30339050 d_loss: 1.39124203, g_loss: 0.81435370, ae_loss: 0.09779465\n",
      "Step: [21] total_loss: 2.33885050 d_loss: 1.45716190, g_loss: 0.78910089, ae_loss: 0.09258766\n",
      "Step: [22] total_loss: 2.33620906 d_loss: 1.41524279, g_loss: 0.83316630, ae_loss: 0.08780003\n",
      "Step: [23] total_loss: 2.29989552 d_loss: 1.43430257, g_loss: 0.78513414, ae_loss: 0.08045875\n",
      "Step: [24] total_loss: 2.29830885 d_loss: 1.41511559, g_loss: 0.80512452, ae_loss: 0.07806879\n",
      "Step: [25] total_loss: 2.26398921 d_loss: 1.41710603, g_loss: 0.77381682, ae_loss: 0.07306638\n",
      "Step: [26] total_loss: 2.29217196 d_loss: 1.42546439, g_loss: 0.79286110, ae_loss: 0.07384640\n",
      "Step: [27] total_loss: 2.25996566 d_loss: 1.42257702, g_loss: 0.76325178, ae_loss: 0.07413685\n",
      "Step: [28] total_loss: 2.27744818 d_loss: 1.42212880, g_loss: 0.77894533, ae_loss: 0.07637419\n",
      "Step: [29] total_loss: 2.25722909 d_loss: 1.42102575, g_loss: 0.76308632, ae_loss: 0.07311696\n",
      "Step: [30] total_loss: 2.24650288 d_loss: 1.41439915, g_loss: 0.76601207, ae_loss: 0.06609159\n",
      "Step: [31] total_loss: 2.23897886 d_loss: 1.40877271, g_loss: 0.75908446, ae_loss: 0.07112171\n",
      "Step: [32] total_loss: 2.23606038 d_loss: 1.40903234, g_loss: 0.75817907, ae_loss: 0.06884896\n",
      "Step: [33] total_loss: 2.21110940 d_loss: 1.40050745, g_loss: 0.74265260, ae_loss: 0.06794938\n",
      "Step: [34] total_loss: 2.21762276 d_loss: 1.39697647, g_loss: 0.75062859, ae_loss: 0.07001787\n",
      "Step: [35] total_loss: 2.20861101 d_loss: 1.40155315, g_loss: 0.73620749, ae_loss: 0.07085021\n",
      "Step: [36] total_loss: 2.20535731 d_loss: 1.39523268, g_loss: 0.73919559, ae_loss: 0.07092906\n",
      "Step: [37] total_loss: 2.19528413 d_loss: 1.38993084, g_loss: 0.73597211, ae_loss: 0.06938108\n",
      "Step: [38] total_loss: 2.21416616 d_loss: 1.41139865, g_loss: 0.73189974, ae_loss: 0.07086767\n",
      "Step: [39] total_loss: 2.17905641 d_loss: 1.38133276, g_loss: 0.72507071, ae_loss: 0.07265288\n",
      "Step: [40] total_loss: 2.19080830 d_loss: 1.38863611, g_loss: 0.72952437, ae_loss: 0.07264797\n",
      "Step: [41] total_loss: 2.17202592 d_loss: 1.38371015, g_loss: 0.72151697, ae_loss: 0.06679886\n",
      "Step: [42] total_loss: 2.17042351 d_loss: 1.38620901, g_loss: 0.71596056, ae_loss: 0.06825405\n",
      "Step: [43] total_loss: 2.15477800 d_loss: 1.36931133, g_loss: 0.71543300, ae_loss: 0.07003381\n",
      "Step: [44] total_loss: 2.15341139 d_loss: 1.36971998, g_loss: 0.71353018, ae_loss: 0.07016135\n",
      "Step: [45] total_loss: 2.15220737 d_loss: 1.37744570, g_loss: 0.70932579, ae_loss: 0.06543581\n",
      "Step: [46] total_loss: 2.14480805 d_loss: 1.37039375, g_loss: 0.70755285, ae_loss: 0.06686140\n",
      "Step: [47] total_loss: 2.16202259 d_loss: 1.37845421, g_loss: 0.71152991, ae_loss: 0.07203849\n",
      "Step: [48] total_loss: 2.14922071 d_loss: 1.36951280, g_loss: 0.70911646, ae_loss: 0.07059141\n",
      "Step: [49] total_loss: 2.14116216 d_loss: 1.37633336, g_loss: 0.69359529, ae_loss: 0.07123352\n",
      "Step: [50] total_loss: 2.14355326 d_loss: 1.36672807, g_loss: 0.70248246, ae_loss: 0.07434259\n",
      "Step: [51] total_loss: 2.12808204 d_loss: 1.36903572, g_loss: 0.69339693, ae_loss: 0.06564939\n",
      "Step: [52] total_loss: 2.13412166 d_loss: 1.36709595, g_loss: 0.70032746, ae_loss: 0.06669817\n",
      "Step: [53] total_loss: 2.13201785 d_loss: 1.35680342, g_loss: 0.70230490, ae_loss: 0.07290944\n",
      "Step: [54] total_loss: 2.13407540 d_loss: 1.36425233, g_loss: 0.70221227, ae_loss: 0.06761090\n",
      "Step: [55] total_loss: 2.13733363 d_loss: 1.36619735, g_loss: 0.69528425, ae_loss: 0.07585201\n",
      "Step: [56] total_loss: 2.11496162 d_loss: 1.35640371, g_loss: 0.69194019, ae_loss: 0.06661784\n",
      "Step: [57] total_loss: 2.12114286 d_loss: 1.36027694, g_loss: 0.69032967, ae_loss: 0.07053633\n",
      "Step: [58] total_loss: 2.10294557 d_loss: 1.34211075, g_loss: 0.69738913, ae_loss: 0.06344566\n",
      "Step: [59] total_loss: 2.12251234 d_loss: 1.35857725, g_loss: 0.69263011, ae_loss: 0.07130495\n",
      "Step: [60] total_loss: 2.11547279 d_loss: 1.35731864, g_loss: 0.68976253, ae_loss: 0.06839170\n",
      "Step: [61] total_loss: 2.12726521 d_loss: 1.36657977, g_loss: 0.68999815, ae_loss: 0.07068729\n",
      "Step: [62] total_loss: 2.12058043 d_loss: 1.36238420, g_loss: 0.69011009, ae_loss: 0.06808616\n",
      "Step: [63] total_loss: 2.10942841 d_loss: 1.35671234, g_loss: 0.68697000, ae_loss: 0.06574622\n",
      "Step: [64] total_loss: 2.12927294 d_loss: 1.36668825, g_loss: 0.69470656, ae_loss: 0.06787824\n",
      "Step: [65] total_loss: 2.10881948 d_loss: 1.35817242, g_loss: 0.68420732, ae_loss: 0.06643988\n",
      "Step: [66] total_loss: 2.10285974 d_loss: 1.35573792, g_loss: 0.68448609, ae_loss: 0.06263573\n",
      "Step: [67] total_loss: 2.11305094 d_loss: 1.36899292, g_loss: 0.67816979, ae_loss: 0.06588830\n",
      "Step: [68] total_loss: 2.10726929 d_loss: 1.35652685, g_loss: 0.68506062, ae_loss: 0.06568179\n",
      "Step: [69] total_loss: 2.11030030 d_loss: 1.34604764, g_loss: 0.69434452, ae_loss: 0.06990810\n",
      "Step: [70] total_loss: 2.11840725 d_loss: 1.37053108, g_loss: 0.68400115, ae_loss: 0.06387501\n",
      "Step: [71] total_loss: 2.10520554 d_loss: 1.34853363, g_loss: 0.68986583, ae_loss: 0.06680607\n",
      "Step: [72] total_loss: 2.10703325 d_loss: 1.35685492, g_loss: 0.67829692, ae_loss: 0.07188144\n",
      "Step: [73] total_loss: 2.10914540 d_loss: 1.35477149, g_loss: 0.67870998, ae_loss: 0.07566392\n",
      "Step: [74] total_loss: 2.11056447 d_loss: 1.35691893, g_loss: 0.68534422, ae_loss: 0.06830138\n",
      "Step: [75] total_loss: 2.11979771 d_loss: 1.36618757, g_loss: 0.68221927, ae_loss: 0.07139094\n",
      "Step: [76] total_loss: 2.11671209 d_loss: 1.35492492, g_loss: 0.69278789, ae_loss: 0.06899923\n",
      "Step: [77] total_loss: 2.12700510 d_loss: 1.35699105, g_loss: 0.69727039, ae_loss: 0.07274363\n",
      "Step: [78] total_loss: 2.11624193 d_loss: 1.35725784, g_loss: 0.69016802, ae_loss: 0.06881610\n",
      "Step: [79] total_loss: 2.12040997 d_loss: 1.36630261, g_loss: 0.68660474, ae_loss: 0.06750273\n",
      "Step: [80] total_loss: 2.11990905 d_loss: 1.36202168, g_loss: 0.69212282, ae_loss: 0.06576456\n",
      "Step: [81] total_loss: 2.10968900 d_loss: 1.35259748, g_loss: 0.68824160, ae_loss: 0.06884991\n",
      "Step: [82] total_loss: 2.10428190 d_loss: 1.34423399, g_loss: 0.69436431, ae_loss: 0.06568358\n",
      "Step: [83] total_loss: 2.11447859 d_loss: 1.36402178, g_loss: 0.68073159, ae_loss: 0.06972527\n",
      "Step: [84] total_loss: 2.11153150 d_loss: 1.35232401, g_loss: 0.68942571, ae_loss: 0.06978183\n",
      "Step: [85] total_loss: 2.12675238 d_loss: 1.37927246, g_loss: 0.67616165, ae_loss: 0.07131820\n",
      "Step: [86] total_loss: 2.10961556 d_loss: 1.34935069, g_loss: 0.69461465, ae_loss: 0.06565019\n",
      "Step: [87] total_loss: 2.10135245 d_loss: 1.35247302, g_loss: 0.68561924, ae_loss: 0.06326024\n",
      "Step: [88] total_loss: 2.10833883 d_loss: 1.36819756, g_loss: 0.67745626, ae_loss: 0.06268490\n",
      "Step: [89] total_loss: 2.12412548 d_loss: 1.35969412, g_loss: 0.70029426, ae_loss: 0.06413721\n",
      "Step: [90] total_loss: 2.10883856 d_loss: 1.35483074, g_loss: 0.69258356, ae_loss: 0.06142433\n",
      "Step: [91] total_loss: 2.10723591 d_loss: 1.35313797, g_loss: 0.68760395, ae_loss: 0.06649382\n",
      "Step: [92] total_loss: 2.12279344 d_loss: 1.36129117, g_loss: 0.69475353, ae_loss: 0.06674878\n",
      "Step: [93] total_loss: 2.10010433 d_loss: 1.34046638, g_loss: 0.69570941, ae_loss: 0.06392854\n",
      "Step: [94] total_loss: 2.10481739 d_loss: 1.34378886, g_loss: 0.69489169, ae_loss: 0.06613675\n",
      "Step: [95] total_loss: 2.12794781 d_loss: 1.37785363, g_loss: 0.68213189, ae_loss: 0.06796236\n",
      "Step: [96] total_loss: 2.11779118 d_loss: 1.36132467, g_loss: 0.69141519, ae_loss: 0.06505126\n",
      "Step: [97] total_loss: 2.11217642 d_loss: 1.34969604, g_loss: 0.69253218, ae_loss: 0.06994836\n",
      "Step: [98] total_loss: 2.11554074 d_loss: 1.36677814, g_loss: 0.68165654, ae_loss: 0.06710614\n",
      "Step: [99] total_loss: 2.10821915 d_loss: 1.35851860, g_loss: 0.68221068, ae_loss: 0.06748992\n",
      "Step: [100] total_loss: 2.12561703 d_loss: 1.36505604, g_loss: 0.69176400, ae_loss: 0.06879701\n",
      "Step: [101] total_loss: 2.12655354 d_loss: 1.37266934, g_loss: 0.68571031, ae_loss: 0.06817392\n",
      "Step: [102] total_loss: 2.12404966 d_loss: 1.36931360, g_loss: 0.68320096, ae_loss: 0.07153496\n",
      "Step: [103] total_loss: 2.12170362 d_loss: 1.36286938, g_loss: 0.69515836, ae_loss: 0.06367596\n",
      "Step: [104] total_loss: 2.12292719 d_loss: 1.34935093, g_loss: 0.70995814, ae_loss: 0.06361807\n",
      "Step: [105] total_loss: 2.12652922 d_loss: 1.35659075, g_loss: 0.70276892, ae_loss: 0.06716939\n",
      "Step: [106] total_loss: 2.13262367 d_loss: 1.36190510, g_loss: 0.70228904, ae_loss: 0.06842951\n",
      "Step: [107] total_loss: 2.12312341 d_loss: 1.36715388, g_loss: 0.69019330, ae_loss: 0.06577620\n",
      "Step: [108] total_loss: 2.12517476 d_loss: 1.36146176, g_loss: 0.69625318, ae_loss: 0.06745978\n",
      "Step: [109] total_loss: 2.12974405 d_loss: 1.35728168, g_loss: 0.70934433, ae_loss: 0.06311814\n",
      "Step: [110] total_loss: 2.12510300 d_loss: 1.36089945, g_loss: 0.69427776, ae_loss: 0.06992573\n",
      "Step: [111] total_loss: 2.12466812 d_loss: 1.36763358, g_loss: 0.68883824, ae_loss: 0.06819636\n",
      "Step: [112] total_loss: 2.12933207 d_loss: 1.36213970, g_loss: 0.70292521, ae_loss: 0.06426726\n",
      "Step: [113] total_loss: 2.13714337 d_loss: 1.38190722, g_loss: 0.69099081, ae_loss: 0.06424533\n",
      "Step: [114] total_loss: 2.11807084 d_loss: 1.36755633, g_loss: 0.68354279, ae_loss: 0.06697163\n",
      "Step: [115] total_loss: 2.12639260 d_loss: 1.38088095, g_loss: 0.67663264, ae_loss: 0.06887906\n",
      "Step: [116] total_loss: 2.12457156 d_loss: 1.36511517, g_loss: 0.69298631, ae_loss: 0.06647004\n",
      "Step: [117] total_loss: 2.10645533 d_loss: 1.35995603, g_loss: 0.68229210, ae_loss: 0.06420723\n",
      "Step: [118] total_loss: 2.12084389 d_loss: 1.34924996, g_loss: 0.70511138, ae_loss: 0.06648255\n",
      "Step: [119] total_loss: 2.12788105 d_loss: 1.37211561, g_loss: 0.69034982, ae_loss: 0.06541570\n",
      "Step: [120] total_loss: 2.13357687 d_loss: 1.36361217, g_loss: 0.70097339, ae_loss: 0.06899147\n",
      "Step: [121] total_loss: 2.12930155 d_loss: 1.36717296, g_loss: 0.69732094, ae_loss: 0.06480773\n",
      "Step: [122] total_loss: 2.12299871 d_loss: 1.37119663, g_loss: 0.68475127, ae_loss: 0.06705067\n",
      "Step: [123] total_loss: 2.13893652 d_loss: 1.38625860, g_loss: 0.68548846, ae_loss: 0.06718937\n",
      "Step: [124] total_loss: 2.12021446 d_loss: 1.37672234, g_loss: 0.67894447, ae_loss: 0.06454763\n",
      "Step: [125] total_loss: 2.12905073 d_loss: 1.38010192, g_loss: 0.68129778, ae_loss: 0.06765091\n",
      "Step: [126] total_loss: 2.13004375 d_loss: 1.36521733, g_loss: 0.70077360, ae_loss: 0.06405277\n",
      "Step: [127] total_loss: 2.12697268 d_loss: 1.36487269, g_loss: 0.69645804, ae_loss: 0.06564204\n",
      "Step: [128] total_loss: 2.13156915 d_loss: 1.36259341, g_loss: 0.70331889, ae_loss: 0.06565691\n",
      "Step: [129] total_loss: 2.14663625 d_loss: 1.38835955, g_loss: 0.69280291, ae_loss: 0.06547374\n",
      "Step: [130] total_loss: 2.12689900 d_loss: 1.36928272, g_loss: 0.68534601, ae_loss: 0.07227032\n",
      "Step: [131] total_loss: 2.11380601 d_loss: 1.35822535, g_loss: 0.69235492, ae_loss: 0.06322569\n",
      "Step: [132] total_loss: 2.11319780 d_loss: 1.36017990, g_loss: 0.68927133, ae_loss: 0.06374656\n",
      "Step: [133] total_loss: 2.12973738 d_loss: 1.37558627, g_loss: 0.68804967, ae_loss: 0.06610125\n",
      "Step: [134] total_loss: 2.11856627 d_loss: 1.35324049, g_loss: 0.70119119, ae_loss: 0.06413460\n",
      "Step: [135] total_loss: 2.11455011 d_loss: 1.36320400, g_loss: 0.69020313, ae_loss: 0.06114295\n",
      "Step: [136] total_loss: 2.12114429 d_loss: 1.36485112, g_loss: 0.69052005, ae_loss: 0.06577323\n",
      "Step: [137] total_loss: 2.11974716 d_loss: 1.35372043, g_loss: 0.69908762, ae_loss: 0.06693925\n",
      "Step: [138] total_loss: 2.11978149 d_loss: 1.37472856, g_loss: 0.67916787, ae_loss: 0.06588519\n",
      "Step: [139] total_loss: 2.13765526 d_loss: 1.37516153, g_loss: 0.69411838, ae_loss: 0.06837540\n",
      "Step: [140] total_loss: 2.11831713 d_loss: 1.35395396, g_loss: 0.70354462, ae_loss: 0.06081856\n",
      "Step: [141] total_loss: 2.11865878 d_loss: 1.36685216, g_loss: 0.69292271, ae_loss: 0.05888385\n",
      "Step: [142] total_loss: 2.12483430 d_loss: 1.37087703, g_loss: 0.68872470, ae_loss: 0.06523266\n",
      "Step: [143] total_loss: 2.10847902 d_loss: 1.35176015, g_loss: 0.69483149, ae_loss: 0.06188723\n",
      "Step: [144] total_loss: 2.12303495 d_loss: 1.36817396, g_loss: 0.69040132, ae_loss: 0.06445960\n",
      "Step: [145] total_loss: 2.11038685 d_loss: 1.35599017, g_loss: 0.69343668, ae_loss: 0.06096002\n",
      "Step: [146] total_loss: 2.11274481 d_loss: 1.36954165, g_loss: 0.67897594, ae_loss: 0.06422719\n",
      "Step: [147] total_loss: 2.11970663 d_loss: 1.36548638, g_loss: 0.69348145, ae_loss: 0.06073885\n",
      "Step: [148] total_loss: 2.10873914 d_loss: 1.34854507, g_loss: 0.69742370, ae_loss: 0.06277039\n",
      "Step: [149] total_loss: 2.11663795 d_loss: 1.37097847, g_loss: 0.68010110, ae_loss: 0.06555831\n",
      "Step: [150] total_loss: 2.12126875 d_loss: 1.37392640, g_loss: 0.68366528, ae_loss: 0.06367715\n",
      "Step: [151] total_loss: 2.12292099 d_loss: 1.38214445, g_loss: 0.67668825, ae_loss: 0.06408831\n",
      "Step: [152] total_loss: 2.11712670 d_loss: 1.35034144, g_loss: 0.70071954, ae_loss: 0.06606575\n",
      "Step: [153] total_loss: 2.12434769 d_loss: 1.35025072, g_loss: 0.70848560, ae_loss: 0.06561147\n",
      "Step: [154] total_loss: 2.12766862 d_loss: 1.36902511, g_loss: 0.69471002, ae_loss: 0.06393354\n",
      "Step: [155] total_loss: 2.14350533 d_loss: 1.38173175, g_loss: 0.69830883, ae_loss: 0.06346476\n",
      "Step: [156] total_loss: 2.14367676 d_loss: 1.38525927, g_loss: 0.69443941, ae_loss: 0.06397816\n",
      "Step: [157] total_loss: 2.14523292 d_loss: 1.37953281, g_loss: 0.70057130, ae_loss: 0.06512883\n",
      "Step: [158] total_loss: 2.12529683 d_loss: 1.37238526, g_loss: 0.69087034, ae_loss: 0.06204114\n",
      "Step: [159] total_loss: 2.11919117 d_loss: 1.37022638, g_loss: 0.68944550, ae_loss: 0.05951929\n",
      "Step: [160] total_loss: 2.12507296 d_loss: 1.36640429, g_loss: 0.69165564, ae_loss: 0.06701308\n",
      "Step: [161] total_loss: 2.12948751 d_loss: 1.37914121, g_loss: 0.68268740, ae_loss: 0.06765902\n",
      "Step: [162] total_loss: 2.12523556 d_loss: 1.36119163, g_loss: 0.70214069, ae_loss: 0.06190315\n",
      "Step: [163] total_loss: 2.12570381 d_loss: 1.37060595, g_loss: 0.69539118, ae_loss: 0.05970668\n",
      "Step: [164] total_loss: 2.12379503 d_loss: 1.35190821, g_loss: 0.70401514, ae_loss: 0.06787172\n",
      "Step: [165] total_loss: 2.13677263 d_loss: 1.38882899, g_loss: 0.68091869, ae_loss: 0.06702501\n",
      "Step: [166] total_loss: 2.11386633 d_loss: 1.36398220, g_loss: 0.68524933, ae_loss: 0.06463487\n",
      "Step: [167] total_loss: 2.13293982 d_loss: 1.37811804, g_loss: 0.69122171, ae_loss: 0.06360009\n",
      "Step: [168] total_loss: 2.13229227 d_loss: 1.37983298, g_loss: 0.68805015, ae_loss: 0.06440926\n",
      "Step: [169] total_loss: 2.14045811 d_loss: 1.39958405, g_loss: 0.67555428, ae_loss: 0.06531978\n",
      "Step: [170] total_loss: 2.12537122 d_loss: 1.37196076, g_loss: 0.69069415, ae_loss: 0.06271638\n",
      "Step: [171] total_loss: 2.13574743 d_loss: 1.37182593, g_loss: 0.70299566, ae_loss: 0.06092568\n",
      "Step: [172] total_loss: 2.12969017 d_loss: 1.36673021, g_loss: 0.70051891, ae_loss: 0.06244106\n",
      "Step: [173] total_loss: 2.13796115 d_loss: 1.38377428, g_loss: 0.69113022, ae_loss: 0.06305670\n",
      "Step: [174] total_loss: 2.12274146 d_loss: 1.34725428, g_loss: 0.71154606, ae_loss: 0.06394113\n",
      "Step: [175] total_loss: 2.13598347 d_loss: 1.38856459, g_loss: 0.68500465, ae_loss: 0.06241418\n",
      "Step: [176] total_loss: 2.12678695 d_loss: 1.35295606, g_loss: 0.71120667, ae_loss: 0.06262425\n",
      "Step: [177] total_loss: 2.13524628 d_loss: 1.37879765, g_loss: 0.69578087, ae_loss: 0.06066784\n",
      "Step: [178] total_loss: 2.12795782 d_loss: 1.35581636, g_loss: 0.71138704, ae_loss: 0.06075437\n",
      "Step: [179] total_loss: 2.13411736 d_loss: 1.36187518, g_loss: 0.70749199, ae_loss: 0.06475023\n",
      "Step: [180] total_loss: 2.11736917 d_loss: 1.36306250, g_loss: 0.69302994, ae_loss: 0.06127665\n",
      "Step: [181] total_loss: 2.11993694 d_loss: 1.36307788, g_loss: 0.69643044, ae_loss: 0.06042870\n",
      "Step: [182] total_loss: 2.12257004 d_loss: 1.35129595, g_loss: 0.70319176, ae_loss: 0.06808237\n",
      "Step: [183] total_loss: 2.11671114 d_loss: 1.37493896, g_loss: 0.67722750, ae_loss: 0.06454477\n",
      "Step: [184] total_loss: 2.11765242 d_loss: 1.37074280, g_loss: 0.68371701, ae_loss: 0.06319246\n",
      "Step: [185] total_loss: 2.11850023 d_loss: 1.37076366, g_loss: 0.68335330, ae_loss: 0.06438319\n",
      "Step: [186] total_loss: 2.12665796 d_loss: 1.36537552, g_loss: 0.69707489, ae_loss: 0.06420740\n",
      "Step: [187] total_loss: 2.13095498 d_loss: 1.37626648, g_loss: 0.69242376, ae_loss: 0.06226473\n",
      "Step: [188] total_loss: 2.10666037 d_loss: 1.34723806, g_loss: 0.69693154, ae_loss: 0.06249088\n",
      "Step: [189] total_loss: 2.12946510 d_loss: 1.36786866, g_loss: 0.69987708, ae_loss: 0.06171945\n",
      "Step: [190] total_loss: 2.11758709 d_loss: 1.36275005, g_loss: 0.69024044, ae_loss: 0.06459652\n",
      "Step: [191] total_loss: 2.13212681 d_loss: 1.37938333, g_loss: 0.68563908, ae_loss: 0.06710428\n",
      "Step: [192] total_loss: 2.14249420 d_loss: 1.38151753, g_loss: 0.69337630, ae_loss: 0.06760028\n",
      "Step: [193] total_loss: 2.12750602 d_loss: 1.39452934, g_loss: 0.67192239, ae_loss: 0.06105426\n",
      "Step: [194] total_loss: 2.14422703 d_loss: 1.38975585, g_loss: 0.69400501, ae_loss: 0.06046618\n",
      "Step: [195] total_loss: 2.12550211 d_loss: 1.36879694, g_loss: 0.69114250, ae_loss: 0.06556258\n",
      "Step: [196] total_loss: 2.11891890 d_loss: 1.36897182, g_loss: 0.69383824, ae_loss: 0.05610899\n",
      "Step: [197] total_loss: 2.11914349 d_loss: 1.37986410, g_loss: 0.67761946, ae_loss: 0.06166006\n",
      "Step: [198] total_loss: 2.13099265 d_loss: 1.37098885, g_loss: 0.69733810, ae_loss: 0.06266569\n",
      "Step: [199] total_loss: 2.12463570 d_loss: 1.36062467, g_loss: 0.69631851, ae_loss: 0.06769241\n",
      "Step: [200] total_loss: 2.13040352 d_loss: 1.38296258, g_loss: 0.67940784, ae_loss: 0.06803320\n",
      "Step: [201] total_loss: 2.12321806 d_loss: 1.37557995, g_loss: 0.68776143, ae_loss: 0.05987670\n",
      "Step: [202] total_loss: 2.11430359 d_loss: 1.37552309, g_loss: 0.67822230, ae_loss: 0.06055833\n",
      "Step: [203] total_loss: 2.12295127 d_loss: 1.37247932, g_loss: 0.68929958, ae_loss: 0.06117241\n",
      "Step: [204] total_loss: 2.14587760 d_loss: 1.38615251, g_loss: 0.69830668, ae_loss: 0.06141844\n",
      "Step: [205] total_loss: 2.12249112 d_loss: 1.37872660, g_loss: 0.68278503, ae_loss: 0.06097950\n",
      "Step: [206] total_loss: 2.11953449 d_loss: 1.37174249, g_loss: 0.68506730, ae_loss: 0.06272487\n",
      "Step: [207] total_loss: 2.10050297 d_loss: 1.34812629, g_loss: 0.69075572, ae_loss: 0.06162100\n",
      "Step: [208] total_loss: 2.11708593 d_loss: 1.38407409, g_loss: 0.67032409, ae_loss: 0.06268760\n",
      "Step: [209] total_loss: 2.12019348 d_loss: 1.36439991, g_loss: 0.69853604, ae_loss: 0.05725740\n",
      "Step: [210] total_loss: 2.12414742 d_loss: 1.38382697, g_loss: 0.67673999, ae_loss: 0.06358049\n",
      "Step: [211] total_loss: 2.11531830 d_loss: 1.37439537, g_loss: 0.67790186, ae_loss: 0.06302116\n",
      "Step: [212] total_loss: 2.10653400 d_loss: 1.34600568, g_loss: 0.70023453, ae_loss: 0.06029390\n",
      "Step: [213] total_loss: 2.10820007 d_loss: 1.35914123, g_loss: 0.69088322, ae_loss: 0.05817553\n",
      "Step: [214] total_loss: 2.10451651 d_loss: 1.35995483, g_loss: 0.67903316, ae_loss: 0.06552847\n",
      "Step: [215] total_loss: 2.10700560 d_loss: 1.37109399, g_loss: 0.67390716, ae_loss: 0.06200431\n",
      "Step: [216] total_loss: 2.12109184 d_loss: 1.38262033, g_loss: 0.67648351, ae_loss: 0.06198798\n",
      "Step: [217] total_loss: 2.10989451 d_loss: 1.37345707, g_loss: 0.67477369, ae_loss: 0.06166377\n",
      "Step: [218] total_loss: 2.12677693 d_loss: 1.38589048, g_loss: 0.67824107, ae_loss: 0.06264532\n",
      "Step: [219] total_loss: 2.12311316 d_loss: 1.37736475, g_loss: 0.68506253, ae_loss: 0.06068591\n",
      "Step: [220] total_loss: 2.10050869 d_loss: 1.35494936, g_loss: 0.68399954, ae_loss: 0.06155992\n",
      "Step: [221] total_loss: 2.11284542 d_loss: 1.37757349, g_loss: 0.67285818, ae_loss: 0.06241379\n",
      "Step: [222] total_loss: 2.12777543 d_loss: 1.38211823, g_loss: 0.68274051, ae_loss: 0.06291678\n",
      "Step: [223] total_loss: 2.11206198 d_loss: 1.36905992, g_loss: 0.68049121, ae_loss: 0.06251074\n",
      "Step: [224] total_loss: 2.12835765 d_loss: 1.37877417, g_loss: 0.68781191, ae_loss: 0.06177149\n",
      "Step: [225] total_loss: 2.10959148 d_loss: 1.35125780, g_loss: 0.69524884, ae_loss: 0.06308493\n",
      "Step: [226] total_loss: 2.12477541 d_loss: 1.39090478, g_loss: 0.67269313, ae_loss: 0.06117762\n",
      "Step: [227] total_loss: 2.12600327 d_loss: 1.36176407, g_loss: 0.70088583, ae_loss: 0.06335338\n",
      "Step: [228] total_loss: 2.11290860 d_loss: 1.36556554, g_loss: 0.68718958, ae_loss: 0.06015354\n",
      "Step: [229] total_loss: 2.13135910 d_loss: 1.37943423, g_loss: 0.68806231, ae_loss: 0.06386251\n",
      "Step: [230] total_loss: 2.11317015 d_loss: 1.33910823, g_loss: 0.71340227, ae_loss: 0.06065957\n",
      "Step: [231] total_loss: 2.11627626 d_loss: 1.37189651, g_loss: 0.68436587, ae_loss: 0.06001395\n",
      "Step: [232] total_loss: 2.13180923 d_loss: 1.38194644, g_loss: 0.68957591, ae_loss: 0.06028681\n",
      "Step: [233] total_loss: 2.10186696 d_loss: 1.36599565, g_loss: 0.67429733, ae_loss: 0.06157396\n",
      "Step: [234] total_loss: 2.12558937 d_loss: 1.38190854, g_loss: 0.68199158, ae_loss: 0.06168930\n",
      "Step: [235] total_loss: 2.11383772 d_loss: 1.37594271, g_loss: 0.67296481, ae_loss: 0.06493037\n",
      "Step: [236] total_loss: 2.12472868 d_loss: 1.38078129, g_loss: 0.67986262, ae_loss: 0.06408484\n",
      "Step: [237] total_loss: 2.12777972 d_loss: 1.38604832, g_loss: 0.68194169, ae_loss: 0.05978983\n",
      "Step: [238] total_loss: 2.12426233 d_loss: 1.37125695, g_loss: 0.69428146, ae_loss: 0.05872379\n",
      "Step: [239] total_loss: 2.12823439 d_loss: 1.37133193, g_loss: 0.69891840, ae_loss: 0.05798403\n",
      "Step: [240] total_loss: 2.13036680 d_loss: 1.37326694, g_loss: 0.69523633, ae_loss: 0.06186343\n",
      "Step: [241] total_loss: 2.12635946 d_loss: 1.37254846, g_loss: 0.68860447, ae_loss: 0.06520644\n",
      "Step: [242] total_loss: 2.12305045 d_loss: 1.37360191, g_loss: 0.68578321, ae_loss: 0.06366543\n",
      "Step: [243] total_loss: 2.13600540 d_loss: 1.38007796, g_loss: 0.69160998, ae_loss: 0.06431733\n",
      "Step: [244] total_loss: 2.11842108 d_loss: 1.37112832, g_loss: 0.68687594, ae_loss: 0.06041670\n",
      "Step: [245] total_loss: 2.11917615 d_loss: 1.37818599, g_loss: 0.68202549, ae_loss: 0.05896455\n",
      "Step: [246] total_loss: 2.12963605 d_loss: 1.38107002, g_loss: 0.68582994, ae_loss: 0.06273598\n",
      "Step: [247] total_loss: 2.12935400 d_loss: 1.37774706, g_loss: 0.69123328, ae_loss: 0.06037354\n",
      "Step: [248] total_loss: 2.11373878 d_loss: 1.36281335, g_loss: 0.69102710, ae_loss: 0.05989838\n",
      "Step: [249] total_loss: 2.13267827 d_loss: 1.39416480, g_loss: 0.67505181, ae_loss: 0.06346169\n",
      "Step: [250] total_loss: 2.12310815 d_loss: 1.36811233, g_loss: 0.69662637, ae_loss: 0.05836944\n",
      "Step: [251] total_loss: 2.12374735 d_loss: 1.39742291, g_loss: 0.66492146, ae_loss: 0.06140302\n",
      "Step: [252] total_loss: 2.12612534 d_loss: 1.37456679, g_loss: 0.68807638, ae_loss: 0.06348211\n",
      "Step: [253] total_loss: 2.12092471 d_loss: 1.37520862, g_loss: 0.68821299, ae_loss: 0.05750313\n",
      "Step: [254] total_loss: 2.12765121 d_loss: 1.38117266, g_loss: 0.68585682, ae_loss: 0.06062175\n",
      "Step: [255] total_loss: 2.11749983 d_loss: 1.37212980, g_loss: 0.68359208, ae_loss: 0.06177812\n",
      "Step: [256] total_loss: 2.12134981 d_loss: 1.37889981, g_loss: 0.67864668, ae_loss: 0.06380336\n",
      "Step: [257] total_loss: 2.13555288 d_loss: 1.37784410, g_loss: 0.69317138, ae_loss: 0.06453723\n",
      "Step: [258] total_loss: 2.12056160 d_loss: 1.36866140, g_loss: 0.69049537, ae_loss: 0.06140485\n",
      "Step: [259] total_loss: 2.12442374 d_loss: 1.38893247, g_loss: 0.67360008, ae_loss: 0.06189116\n",
      "Step: [260] total_loss: 2.13424897 d_loss: 1.38806248, g_loss: 0.68335652, ae_loss: 0.06283002\n",
      "Step: [261] total_loss: 2.11953282 d_loss: 1.36335993, g_loss: 0.69451344, ae_loss: 0.06165947\n",
      "Step: [262] total_loss: 2.12552786 d_loss: 1.37412906, g_loss: 0.68911088, ae_loss: 0.06228799\n",
      "Step: [263] total_loss: 2.11569953 d_loss: 1.36981130, g_loss: 0.68321347, ae_loss: 0.06267475\n",
      "Step: [264] total_loss: 2.10798740 d_loss: 1.35462427, g_loss: 0.69499695, ae_loss: 0.05836620\n",
      "Step: [265] total_loss: 2.12230062 d_loss: 1.36776423, g_loss: 0.69326913, ae_loss: 0.06126716\n",
      "Step: [266] total_loss: 2.10945797 d_loss: 1.35757375, g_loss: 0.69247848, ae_loss: 0.05940582\n",
      "Step: [267] total_loss: 2.11146498 d_loss: 1.37126923, g_loss: 0.67767411, ae_loss: 0.06252162\n",
      "Step: [268] total_loss: 2.11760902 d_loss: 1.38056254, g_loss: 0.67599440, ae_loss: 0.06105218\n",
      "Step: [269] total_loss: 2.13320351 d_loss: 1.37532330, g_loss: 0.69829941, ae_loss: 0.05958069\n",
      "Step: [270] total_loss: 2.11726427 d_loss: 1.36860657, g_loss: 0.68704975, ae_loss: 0.06160796\n",
      "Step: [271] total_loss: 2.12684917 d_loss: 1.38546038, g_loss: 0.67829871, ae_loss: 0.06308997\n",
      "Step: [272] total_loss: 2.12103248 d_loss: 1.37600124, g_loss: 0.68638200, ae_loss: 0.05864918\n",
      "Step: [273] total_loss: 2.12648320 d_loss: 1.38065577, g_loss: 0.68735319, ae_loss: 0.05847416\n",
      "Step: [274] total_loss: 2.10951114 d_loss: 1.35832202, g_loss: 0.68814516, ae_loss: 0.06304391\n",
      "Step: [275] total_loss: 2.10685277 d_loss: 1.35248208, g_loss: 0.69005466, ae_loss: 0.06431606\n",
      "Step: [276] total_loss: 2.12362099 d_loss: 1.38160729, g_loss: 0.68002731, ae_loss: 0.06198628\n",
      "Step: [277] total_loss: 2.11522627 d_loss: 1.37813938, g_loss: 0.67380768, ae_loss: 0.06327920\n",
      "Step: [278] total_loss: 2.11975813 d_loss: 1.36607206, g_loss: 0.68990827, ae_loss: 0.06377780\n",
      "Step: [279] total_loss: 2.11834931 d_loss: 1.37961650, g_loss: 0.67983228, ae_loss: 0.05890045\n",
      "Step: [280] total_loss: 2.11181211 d_loss: 1.36541295, g_loss: 0.68959939, ae_loss: 0.05679980\n",
      "Step: [281] total_loss: 2.11100531 d_loss: 1.37353063, g_loss: 0.67806935, ae_loss: 0.05940533\n",
      "Step: [282] total_loss: 2.11799765 d_loss: 1.38111687, g_loss: 0.67767930, ae_loss: 0.05920159\n",
      "Step: [283] total_loss: 2.11589575 d_loss: 1.35569596, g_loss: 0.69984245, ae_loss: 0.06035728\n",
      "Step: [284] total_loss: 2.11878300 d_loss: 1.38199902, g_loss: 0.67534745, ae_loss: 0.06143656\n",
      "Step: [285] total_loss: 2.11513805 d_loss: 1.37161767, g_loss: 0.68580604, ae_loss: 0.05771440\n",
      "Step: [286] total_loss: 2.13654184 d_loss: 1.39259291, g_loss: 0.68270445, ae_loss: 0.06124461\n",
      "Step: [287] total_loss: 2.11945271 d_loss: 1.36566567, g_loss: 0.69379020, ae_loss: 0.05999683\n",
      "Step: [288] total_loss: 2.11627269 d_loss: 1.36585116, g_loss: 0.68972516, ae_loss: 0.06069637\n",
      "Step: [289] total_loss: 2.12054729 d_loss: 1.36420333, g_loss: 0.69647741, ae_loss: 0.05986661\n",
      "Step: [290] total_loss: 2.10586834 d_loss: 1.34616351, g_loss: 0.69770885, ae_loss: 0.06199603\n",
      "Step: [291] total_loss: 2.13257504 d_loss: 1.36839223, g_loss: 0.69824558, ae_loss: 0.06593716\n",
      "Step: [292] total_loss: 2.12295079 d_loss: 1.36845040, g_loss: 0.69825834, ae_loss: 0.05624193\n",
      "Step: [293] total_loss: 2.11496282 d_loss: 1.37812018, g_loss: 0.67307138, ae_loss: 0.06377122\n",
      "Step: [294] total_loss: 2.13494897 d_loss: 1.39937305, g_loss: 0.67127210, ae_loss: 0.06430393\n",
      "Step: [295] total_loss: 2.11983633 d_loss: 1.37033641, g_loss: 0.68608844, ae_loss: 0.06341136\n",
      "Step: [296] total_loss: 2.13254690 d_loss: 1.38264966, g_loss: 0.68879509, ae_loss: 0.06110222\n",
      "Step: [297] total_loss: 2.12705636 d_loss: 1.37382126, g_loss: 0.69137293, ae_loss: 0.06186207\n",
      "Step: [298] total_loss: 2.13994360 d_loss: 1.37721860, g_loss: 0.70025003, ae_loss: 0.06247507\n",
      "Step: [299] total_loss: 2.12162018 d_loss: 1.37023687, g_loss: 0.69204861, ae_loss: 0.05933469\n",
      "Step: [300] total_loss: 2.11366677 d_loss: 1.36873257, g_loss: 0.68465322, ae_loss: 0.06028101\n",
      "Step: [301] total_loss: 2.11195803 d_loss: 1.36126781, g_loss: 0.69058400, ae_loss: 0.06010617\n",
      "Step: [302] total_loss: 2.12179613 d_loss: 1.37572312, g_loss: 0.68417168, ae_loss: 0.06190117\n",
      "Step: [303] total_loss: 2.12830591 d_loss: 1.37410939, g_loss: 0.69255459, ae_loss: 0.06164206\n",
      "Step: [304] total_loss: 2.11368132 d_loss: 1.36436284, g_loss: 0.68731236, ae_loss: 0.06200621\n",
      "Step: [305] total_loss: 2.12083340 d_loss: 1.36924291, g_loss: 0.68708205, ae_loss: 0.06450831\n",
      "Step: [306] total_loss: 2.14600062 d_loss: 1.41255295, g_loss: 0.67061770, ae_loss: 0.06282996\n",
      "Step: [307] total_loss: 2.13560319 d_loss: 1.38896751, g_loss: 0.68872529, ae_loss: 0.05791045\n",
      "Step: [308] total_loss: 2.12404680 d_loss: 1.38262463, g_loss: 0.67966926, ae_loss: 0.06175289\n",
      "Step: [309] total_loss: 2.13339520 d_loss: 1.38800216, g_loss: 0.68532062, ae_loss: 0.06007249\n",
      "Step: [310] total_loss: 2.11525083 d_loss: 1.36341465, g_loss: 0.68842602, ae_loss: 0.06341021\n",
      "Step: [311] total_loss: 2.13164043 d_loss: 1.36115527, g_loss: 0.70773745, ae_loss: 0.06274776\n",
      "Step: [312] total_loss: 2.14458442 d_loss: 1.39152956, g_loss: 0.69298208, ae_loss: 0.06007276\n",
      "Step: [313] total_loss: 2.13230991 d_loss: 1.37986875, g_loss: 0.68940145, ae_loss: 0.06303962\n",
      "Step: [314] total_loss: 2.13650560 d_loss: 1.38304842, g_loss: 0.69688737, ae_loss: 0.05656986\n",
      "Step: [315] total_loss: 2.11703587 d_loss: 1.36668289, g_loss: 0.69264841, ae_loss: 0.05770459\n",
      "Step: [316] total_loss: 2.14149451 d_loss: 1.38861740, g_loss: 0.69336069, ae_loss: 0.05951643\n",
      "Step: [317] total_loss: 2.12729311 d_loss: 1.38009286, g_loss: 0.68353093, ae_loss: 0.06366919\n",
      "Step: [318] total_loss: 2.11379170 d_loss: 1.35371518, g_loss: 0.70132625, ae_loss: 0.05875033\n",
      "Step: [319] total_loss: 2.12322712 d_loss: 1.38894928, g_loss: 0.67229903, ae_loss: 0.06197897\n",
      "Step: [320] total_loss: 2.11311913 d_loss: 1.37212062, g_loss: 0.68277907, ae_loss: 0.05821951\n",
      "Step: [321] total_loss: 2.12004519 d_loss: 1.38266826, g_loss: 0.67608750, ae_loss: 0.06128950\n",
      "Step: [322] total_loss: 2.10903931 d_loss: 1.36910605, g_loss: 0.67448747, ae_loss: 0.06544562\n",
      "Step: [323] total_loss: 2.10632896 d_loss: 1.35490441, g_loss: 0.69270474, ae_loss: 0.05871986\n",
      "Step: [324] total_loss: 2.12268925 d_loss: 1.38134074, g_loss: 0.67723513, ae_loss: 0.06411321\n",
      "Step: [325] total_loss: 2.10382056 d_loss: 1.36484349, g_loss: 0.67619568, ae_loss: 0.06278134\n",
      "Step: [326] total_loss: 2.12078071 d_loss: 1.36944103, g_loss: 0.68818641, ae_loss: 0.06315323\n",
      "Step: [327] total_loss: 2.12516880 d_loss: 1.37710428, g_loss: 0.68427140, ae_loss: 0.06379317\n",
      "Step: [328] total_loss: 2.11597538 d_loss: 1.37986326, g_loss: 0.67355037, ae_loss: 0.06256173\n",
      "Step: [329] total_loss: 2.11704803 d_loss: 1.36925137, g_loss: 0.69041026, ae_loss: 0.05738641\n",
      "Step: [330] total_loss: 2.13172078 d_loss: 1.39158225, g_loss: 0.68030590, ae_loss: 0.05983256\n",
      "Step: [331] total_loss: 2.13078761 d_loss: 1.37740374, g_loss: 0.68974125, ae_loss: 0.06364258\n",
      "Step: [332] total_loss: 2.10028172 d_loss: 1.37407553, g_loss: 0.66625416, ae_loss: 0.05995198\n",
      "Step: [333] total_loss: 2.10505962 d_loss: 1.37714887, g_loss: 0.66800386, ae_loss: 0.05990696\n",
      "Step: [334] total_loss: 2.11567926 d_loss: 1.37373209, g_loss: 0.68437612, ae_loss: 0.05757102\n",
      "Step: [335] total_loss: 2.10838175 d_loss: 1.38315058, g_loss: 0.66722059, ae_loss: 0.05801065\n",
      "Step: [336] total_loss: 2.11007929 d_loss: 1.36702693, g_loss: 0.68334436, ae_loss: 0.05970788\n",
      "Step: [337] total_loss: 2.12480092 d_loss: 1.37335706, g_loss: 0.69237679, ae_loss: 0.05906701\n",
      "Step: [338] total_loss: 2.11331606 d_loss: 1.37377906, g_loss: 0.68017721, ae_loss: 0.05935985\n",
      "Step: [339] total_loss: 2.13166046 d_loss: 1.37170279, g_loss: 0.69673741, ae_loss: 0.06322014\n",
      "Step: [340] total_loss: 2.13461089 d_loss: 1.39541888, g_loss: 0.67680663, ae_loss: 0.06238550\n",
      "Step: [341] total_loss: 2.12934303 d_loss: 1.37832904, g_loss: 0.68830395, ae_loss: 0.06271015\n",
      "Step: [342] total_loss: 2.13558483 d_loss: 1.39110994, g_loss: 0.68306267, ae_loss: 0.06141230\n",
      "Step: [343] total_loss: 2.11921549 d_loss: 1.37451220, g_loss: 0.68372238, ae_loss: 0.06098101\n",
      "Step: [344] total_loss: 2.11602068 d_loss: 1.37394190, g_loss: 0.68265653, ae_loss: 0.05942216\n",
      "Step: [345] total_loss: 2.11768579 d_loss: 1.38411069, g_loss: 0.67395329, ae_loss: 0.05962177\n",
      "Step: [346] total_loss: 2.11729288 d_loss: 1.38594258, g_loss: 0.66994941, ae_loss: 0.06140096\n",
      "Step: [347] total_loss: 2.11000681 d_loss: 1.36827040, g_loss: 0.68377459, ae_loss: 0.05796181\n",
      "Step: [348] total_loss: 2.12173176 d_loss: 1.38211572, g_loss: 0.67617941, ae_loss: 0.06343646\n",
      "Step: [349] total_loss: 2.13411045 d_loss: 1.38258290, g_loss: 0.69144726, ae_loss: 0.06008037\n",
      "Step: [350] total_loss: 2.12535548 d_loss: 1.38360369, g_loss: 0.68050706, ae_loss: 0.06124477\n",
      "Step: [351] total_loss: 2.10900235 d_loss: 1.35290146, g_loss: 0.69648391, ae_loss: 0.05961708\n",
      "Step: [352] total_loss: 2.11735916 d_loss: 1.35871208, g_loss: 0.69911128, ae_loss: 0.05953577\n",
      "Step: [353] total_loss: 2.13308549 d_loss: 1.40038896, g_loss: 0.66657567, ae_loss: 0.06612082\n",
      "Step: [354] total_loss: 2.13719630 d_loss: 1.39587975, g_loss: 0.67869335, ae_loss: 0.06262324\n",
      "Step: [355] total_loss: 2.12629986 d_loss: 1.37261033, g_loss: 0.69153726, ae_loss: 0.06215219\n",
      "Step: [356] total_loss: 2.12149096 d_loss: 1.37937570, g_loss: 0.68077171, ae_loss: 0.06134346\n",
      "Step: [357] total_loss: 2.12267041 d_loss: 1.35966182, g_loss: 0.70205063, ae_loss: 0.06095802\n",
      "Step: [358] total_loss: 2.13053560 d_loss: 1.38066530, g_loss: 0.69004750, ae_loss: 0.05982272\n",
      "Step: [359] total_loss: 2.11991048 d_loss: 1.37444258, g_loss: 0.68567538, ae_loss: 0.05979250\n",
      "Step: [360] total_loss: 2.12248802 d_loss: 1.37150800, g_loss: 0.69056869, ae_loss: 0.06041123\n",
      "Step: [361] total_loss: 2.11053658 d_loss: 1.35749936, g_loss: 0.69234014, ae_loss: 0.06069694\n",
      "Step: [362] total_loss: 2.14972544 d_loss: 1.40726113, g_loss: 0.68143046, ae_loss: 0.06103376\n",
      "Step: [363] total_loss: 2.13863611 d_loss: 1.38023376, g_loss: 0.69917953, ae_loss: 0.05922293\n",
      "Step: [364] total_loss: 2.12525892 d_loss: 1.39171267, g_loss: 0.67696267, ae_loss: 0.05658351\n",
      "Step: [365] total_loss: 2.13693309 d_loss: 1.38509607, g_loss: 0.68745506, ae_loss: 0.06438191\n",
      "Step: [366] total_loss: 2.13614559 d_loss: 1.38902307, g_loss: 0.68617636, ae_loss: 0.06094613\n",
      "Step: [367] total_loss: 2.13581133 d_loss: 1.38661981, g_loss: 0.68771696, ae_loss: 0.06147468\n",
      "Step: [368] total_loss: 2.12429476 d_loss: 1.38006330, g_loss: 0.68253076, ae_loss: 0.06170056\n",
      "Step: [369] total_loss: 2.10824919 d_loss: 1.37183642, g_loss: 0.67723155, ae_loss: 0.05918108\n",
      "Step: [370] total_loss: 2.11375546 d_loss: 1.37228823, g_loss: 0.67937720, ae_loss: 0.06209004\n",
      "Step: [371] total_loss: 2.13722467 d_loss: 1.38746762, g_loss: 0.68981636, ae_loss: 0.05994061\n",
      "Step: [372] total_loss: 2.11007833 d_loss: 1.37804699, g_loss: 0.67071867, ae_loss: 0.06131253\n",
      "Step: [373] total_loss: 2.13675451 d_loss: 1.37614167, g_loss: 0.69828475, ae_loss: 0.06232803\n",
      "Step: [374] total_loss: 2.12058783 d_loss: 1.38839006, g_loss: 0.67707515, ae_loss: 0.05512252\n",
      "Step: [375] total_loss: 2.12637210 d_loss: 1.38417470, g_loss: 0.68075323, ae_loss: 0.06144411\n",
      "Step: [376] total_loss: 2.12796593 d_loss: 1.38870585, g_loss: 0.68443835, ae_loss: 0.05482190\n",
      "Step: [377] total_loss: 2.12076759 d_loss: 1.36744928, g_loss: 0.69683790, ae_loss: 0.05648040\n",
      "Step: [378] total_loss: 2.12586880 d_loss: 1.36426735, g_loss: 0.70017564, ae_loss: 0.06142575\n",
      "Step: [379] total_loss: 2.13055897 d_loss: 1.38217974, g_loss: 0.69413579, ae_loss: 0.05424350\n",
      "Step: [380] total_loss: 2.12906671 d_loss: 1.37888503, g_loss: 0.68982106, ae_loss: 0.06036070\n",
      "Step: [381] total_loss: 2.13571787 d_loss: 1.38334703, g_loss: 0.69039500, ae_loss: 0.06197594\n",
      "Step: [382] total_loss: 2.12791729 d_loss: 1.36460733, g_loss: 0.70039523, ae_loss: 0.06291456\n",
      "Step: [383] total_loss: 2.11707044 d_loss: 1.37755156, g_loss: 0.68220830, ae_loss: 0.05731060\n",
      "Step: [384] total_loss: 2.12078190 d_loss: 1.38562870, g_loss: 0.67877412, ae_loss: 0.05637898\n",
      "Step: [385] total_loss: 2.12339878 d_loss: 1.38345659, g_loss: 0.67694950, ae_loss: 0.06299286\n",
      "Step: [386] total_loss: 2.11516953 d_loss: 1.38098240, g_loss: 0.67381668, ae_loss: 0.06037046\n",
      "Step: [387] total_loss: 2.12141514 d_loss: 1.38354969, g_loss: 0.68096620, ae_loss: 0.05689934\n",
      "Step: [388] total_loss: 2.10646057 d_loss: 1.36144090, g_loss: 0.68842947, ae_loss: 0.05659016\n",
      "Step: [389] total_loss: 2.12613010 d_loss: 1.37861323, g_loss: 0.68754101, ae_loss: 0.05997599\n",
      "Step: [390] total_loss: 2.12043715 d_loss: 1.36598694, g_loss: 0.69403434, ae_loss: 0.06041594\n",
      "Step: [391] total_loss: 2.12909365 d_loss: 1.38647270, g_loss: 0.68513751, ae_loss: 0.05748332\n",
      "Step: [392] total_loss: 2.12577057 d_loss: 1.38828123, g_loss: 0.68121624, ae_loss: 0.05627324\n",
      "Step: [393] total_loss: 2.11957502 d_loss: 1.39415169, g_loss: 0.66416872, ae_loss: 0.06125478\n",
      "Step: [394] total_loss: 2.11004019 d_loss: 1.36787844, g_loss: 0.68448079, ae_loss: 0.05768086\n",
      "Step: [395] total_loss: 2.11111593 d_loss: 1.38066840, g_loss: 0.66851687, ae_loss: 0.06193070\n",
      "Step: [396] total_loss: 2.12689328 d_loss: 1.37753272, g_loss: 0.68858010, ae_loss: 0.06078056\n",
      "Step: [397] total_loss: 2.12321758 d_loss: 1.37856150, g_loss: 0.68808681, ae_loss: 0.05656919\n",
      "Step: [398] total_loss: 2.11151981 d_loss: 1.36695743, g_loss: 0.68458855, ae_loss: 0.05997369\n",
      "Step: [399] total_loss: 2.11475348 d_loss: 1.37163460, g_loss: 0.68267399, ae_loss: 0.06044478\n",
      "Step: [400] total_loss: 2.11515927 d_loss: 1.37491775, g_loss: 0.68308479, ae_loss: 0.05715664\n",
      "Step: [401] total_loss: 2.11683178 d_loss: 1.38389957, g_loss: 0.67229176, ae_loss: 0.06064040\n",
      "Step: [402] total_loss: 2.12887025 d_loss: 1.38115644, g_loss: 0.68451774, ae_loss: 0.06319602\n",
      "Step: [403] total_loss: 2.11616588 d_loss: 1.37473607, g_loss: 0.68142641, ae_loss: 0.06000346\n",
      "Step: [404] total_loss: 2.10114169 d_loss: 1.37412930, g_loss: 0.66922677, ae_loss: 0.05778565\n",
      "Step: [405] total_loss: 2.10584641 d_loss: 1.37240756, g_loss: 0.67564559, ae_loss: 0.05779338\n",
      "Step: [406] total_loss: 2.11198235 d_loss: 1.37235415, g_loss: 0.67938995, ae_loss: 0.06023841\n",
      "Step: [407] total_loss: 2.11288786 d_loss: 1.38377225, g_loss: 0.66915071, ae_loss: 0.05996497\n",
      "Step: [408] total_loss: 2.13422775 d_loss: 1.39059246, g_loss: 0.68572176, ae_loss: 0.05791347\n",
      "Step: [409] total_loss: 2.13202548 d_loss: 1.38648081, g_loss: 0.69014126, ae_loss: 0.05540336\n",
      "Step: [410] total_loss: 2.13517714 d_loss: 1.38124120, g_loss: 0.69772643, ae_loss: 0.05620942\n",
      "Step: [411] total_loss: 2.15369463 d_loss: 1.40748394, g_loss: 0.68489504, ae_loss: 0.06131567\n",
      "Step: [412] total_loss: 2.14189386 d_loss: 1.38782334, g_loss: 0.69366634, ae_loss: 0.06040412\n",
      "Step: [413] total_loss: 2.11693597 d_loss: 1.37939644, g_loss: 0.68080473, ae_loss: 0.05673481\n",
      "Step: [414] total_loss: 2.10657477 d_loss: 1.35488391, g_loss: 0.69570374, ae_loss: 0.05598710\n",
      "Step: [415] total_loss: 2.12856770 d_loss: 1.37788951, g_loss: 0.69145614, ae_loss: 0.05922216\n",
      "Step: [416] total_loss: 2.13177013 d_loss: 1.38588274, g_loss: 0.68301785, ae_loss: 0.06286953\n",
      "Step: [417] total_loss: 2.11023140 d_loss: 1.35228002, g_loss: 0.69760579, ae_loss: 0.06034564\n",
      "Step: [418] total_loss: 2.13503742 d_loss: 1.37027538, g_loss: 0.70244789, ae_loss: 0.06231416\n",
      "Step: [419] total_loss: 2.13349986 d_loss: 1.37533140, g_loss: 0.69770753, ae_loss: 0.06046098\n",
      "Step: [420] total_loss: 2.13900137 d_loss: 1.38283587, g_loss: 0.69616067, ae_loss: 0.06000498\n",
      "Step: [421] total_loss: 2.12676334 d_loss: 1.35997295, g_loss: 0.70178831, ae_loss: 0.06500192\n",
      "Step: [422] total_loss: 2.11923265 d_loss: 1.36681473, g_loss: 0.69370157, ae_loss: 0.05871625\n",
      "Step: [423] total_loss: 2.16391182 d_loss: 1.38613534, g_loss: 0.71627903, ae_loss: 0.06149741\n",
      "Step: [424] total_loss: 2.11271405 d_loss: 1.37284112, g_loss: 0.68221939, ae_loss: 0.05765351\n",
      "Step: [425] total_loss: 2.12293720 d_loss: 1.35632849, g_loss: 0.70771039, ae_loss: 0.05889829\n",
      "Step: [426] total_loss: 2.15154433 d_loss: 1.38968885, g_loss: 0.69586670, ae_loss: 0.06598879\n",
      "Step: [427] total_loss: 2.12350678 d_loss: 1.38470793, g_loss: 0.67991257, ae_loss: 0.05888624\n",
      "Step: [428] total_loss: 2.12309217 d_loss: 1.38848782, g_loss: 0.67612755, ae_loss: 0.05847687\n",
      "Step: [429] total_loss: 2.14911747 d_loss: 1.39665902, g_loss: 0.69122672, ae_loss: 0.06123168\n",
      "Step: [430] total_loss: 2.12578535 d_loss: 1.37220383, g_loss: 0.69391370, ae_loss: 0.05966776\n",
      "Step: [431] total_loss: 2.12451553 d_loss: 1.36701775, g_loss: 0.69902337, ae_loss: 0.05847428\n",
      "Step: [432] total_loss: 2.11854100 d_loss: 1.37075663, g_loss: 0.68824160, ae_loss: 0.05954277\n",
      "Step: [433] total_loss: 2.10127974 d_loss: 1.35969484, g_loss: 0.68411934, ae_loss: 0.05746567\n",
      "Step: [434] total_loss: 2.12105608 d_loss: 1.38122523, g_loss: 0.68282700, ae_loss: 0.05700400\n",
      "Step: [435] total_loss: 2.09965944 d_loss: 1.36375654, g_loss: 0.67815435, ae_loss: 0.05774866\n",
      "Step: [436] total_loss: 2.10403299 d_loss: 1.36772346, g_loss: 0.68161547, ae_loss: 0.05469417\n",
      "Step: [437] total_loss: 2.11949539 d_loss: 1.37476397, g_loss: 0.68569458, ae_loss: 0.05903701\n",
      "Step: [438] total_loss: 2.11185312 d_loss: 1.37853050, g_loss: 0.67419827, ae_loss: 0.05912434\n",
      "Step: [439] total_loss: 2.11559248 d_loss: 1.38259315, g_loss: 0.67594403, ae_loss: 0.05705527\n",
      "Step: [440] total_loss: 2.11235476 d_loss: 1.36880517, g_loss: 0.68808985, ae_loss: 0.05545987\n",
      "Step: [441] total_loss: 2.14655113 d_loss: 1.39921594, g_loss: 0.68890238, ae_loss: 0.05843294\n",
      "Step: [442] total_loss: 2.12910557 d_loss: 1.38075805, g_loss: 0.68919104, ae_loss: 0.05915650\n",
      "Step: [443] total_loss: 2.13200808 d_loss: 1.39891374, g_loss: 0.67915690, ae_loss: 0.05393746\n",
      "Step: [444] total_loss: 2.12467766 d_loss: 1.36568379, g_loss: 0.70058882, ae_loss: 0.05840487\n",
      "Step: [445] total_loss: 2.13542962 d_loss: 1.36823833, g_loss: 0.70979476, ae_loss: 0.05739656\n",
      "Step: [446] total_loss: 2.13741946 d_loss: 1.39251137, g_loss: 0.68698907, ae_loss: 0.05791900\n",
      "Step: [447] total_loss: 2.13025379 d_loss: 1.36840153, g_loss: 0.69929141, ae_loss: 0.06256087\n",
      "Step: [448] total_loss: 2.12211013 d_loss: 1.39309418, g_loss: 0.67421091, ae_loss: 0.05480503\n",
      "Step: [449] total_loss: 2.09877253 d_loss: 1.34511900, g_loss: 0.69670188, ae_loss: 0.05695179\n",
      "Step: [450] total_loss: 2.09650540 d_loss: 1.36939549, g_loss: 0.67267680, ae_loss: 0.05443312\n",
      "Step: [451] total_loss: 2.10225868 d_loss: 1.38119507, g_loss: 0.66344881, ae_loss: 0.05761485\n",
      "Step: [452] total_loss: 2.09774852 d_loss: 1.36878896, g_loss: 0.66985160, ae_loss: 0.05910794\n",
      "Step: [453] total_loss: 2.09921503 d_loss: 1.37201989, g_loss: 0.67266273, ae_loss: 0.05453241\n",
      "Step: [454] total_loss: 2.10675049 d_loss: 1.36967599, g_loss: 0.68129313, ae_loss: 0.05578136\n",
      "Step: [455] total_loss: 2.11233759 d_loss: 1.38409019, g_loss: 0.66916478, ae_loss: 0.05908248\n",
      "Step: [456] total_loss: 2.14816546 d_loss: 1.40249395, g_loss: 0.68602657, ae_loss: 0.05964497\n",
      "Step: [457] total_loss: 2.12113476 d_loss: 1.38091612, g_loss: 0.67657292, ae_loss: 0.06364566\n",
      "Step: [458] total_loss: 2.13559937 d_loss: 1.38260829, g_loss: 0.69340700, ae_loss: 0.05958410\n",
      "Step: [459] total_loss: 2.14725447 d_loss: 1.39533782, g_loss: 0.69431806, ae_loss: 0.05759850\n",
      "Step: [460] total_loss: 2.13266349 d_loss: 1.37303329, g_loss: 0.70085365, ae_loss: 0.05877654\n",
      "Step: [461] total_loss: 2.12823129 d_loss: 1.38046694, g_loss: 0.68844360, ae_loss: 0.05932072\n",
      "Step: [462] total_loss: 2.10840392 d_loss: 1.36159587, g_loss: 0.69247901, ae_loss: 0.05432902\n",
      "Step: [463] total_loss: 2.11157203 d_loss: 1.36851120, g_loss: 0.68564129, ae_loss: 0.05741951\n",
      "Step: [464] total_loss: 2.11914515 d_loss: 1.38191772, g_loss: 0.67846036, ae_loss: 0.05876711\n",
      "Step: [465] total_loss: 2.11784625 d_loss: 1.35550129, g_loss: 0.70632356, ae_loss: 0.05602129\n",
      "Step: [466] total_loss: 2.11115408 d_loss: 1.36140203, g_loss: 0.69043589, ae_loss: 0.05931619\n",
      "Step: [467] total_loss: 2.13470268 d_loss: 1.38099551, g_loss: 0.69330394, ae_loss: 0.06040323\n",
      "Step: [468] total_loss: 2.13332033 d_loss: 1.38183427, g_loss: 0.69451892, ae_loss: 0.05696699\n",
      "Step: [469] total_loss: 2.12799168 d_loss: 1.37499368, g_loss: 0.69986176, ae_loss: 0.05313635\n",
      "Step: [470] total_loss: 2.12765980 d_loss: 1.38271785, g_loss: 0.68147653, ae_loss: 0.06346538\n",
      "Step: [471] total_loss: 2.11835861 d_loss: 1.37855387, g_loss: 0.68316770, ae_loss: 0.05663691\n",
      "Step: [472] total_loss: 2.10767579 d_loss: 1.37488687, g_loss: 0.67532408, ae_loss: 0.05746479\n",
      "Step: [473] total_loss: 2.12335515 d_loss: 1.36703181, g_loss: 0.69825870, ae_loss: 0.05806460\n",
      "Step: [474] total_loss: 2.13156271 d_loss: 1.38764858, g_loss: 0.68244588, ae_loss: 0.06146806\n",
      "Step: [475] total_loss: 2.12962866 d_loss: 1.38338232, g_loss: 0.68677437, ae_loss: 0.05947207\n",
      "Step: [476] total_loss: 2.11017108 d_loss: 1.36232960, g_loss: 0.69031912, ae_loss: 0.05752225\n",
      "Step: [477] total_loss: 2.10514998 d_loss: 1.37567282, g_loss: 0.67162836, ae_loss: 0.05784884\n",
      "Step: [478] total_loss: 2.12313843 d_loss: 1.38537204, g_loss: 0.67700636, ae_loss: 0.06076017\n",
      "Step: [479] total_loss: 2.13031721 d_loss: 1.37113070, g_loss: 0.70157558, ae_loss: 0.05761096\n",
      "Step: [480] total_loss: 2.11145830 d_loss: 1.37079716, g_loss: 0.68744516, ae_loss: 0.05321590\n",
      "Step: [481] total_loss: 2.12316489 d_loss: 1.36089921, g_loss: 0.70646149, ae_loss: 0.05580418\n",
      "Step: [482] total_loss: 2.13726163 d_loss: 1.39563262, g_loss: 0.68576032, ae_loss: 0.05586866\n",
      "Step: [483] total_loss: 2.11960244 d_loss: 1.36004090, g_loss: 0.70395058, ae_loss: 0.05561104\n",
      "Step: [484] total_loss: 2.13473558 d_loss: 1.38768315, g_loss: 0.68947303, ae_loss: 0.05757940\n",
      "Step: [485] total_loss: 2.11196899 d_loss: 1.37356818, g_loss: 0.68262953, ae_loss: 0.05577137\n",
      "Step: [486] total_loss: 2.13246059 d_loss: 1.37773299, g_loss: 0.69753861, ae_loss: 0.05718905\n",
      "Step: [487] total_loss: 2.12984300 d_loss: 1.38315248, g_loss: 0.69311732, ae_loss: 0.05357317\n",
      "Step: [488] total_loss: 2.12261128 d_loss: 1.38474119, g_loss: 0.68058318, ae_loss: 0.05728685\n",
      "Step: [489] total_loss: 2.12891436 d_loss: 1.37575698, g_loss: 0.69564784, ae_loss: 0.05750944\n",
      "Step: [490] total_loss: 2.14120007 d_loss: 1.38743544, g_loss: 0.69160318, ae_loss: 0.06216133\n",
      "Step: [491] total_loss: 2.13015223 d_loss: 1.36341381, g_loss: 0.71076250, ae_loss: 0.05597607\n",
      "Step: [492] total_loss: 2.13578606 d_loss: 1.38924634, g_loss: 0.69091856, ae_loss: 0.05562110\n",
      "Step: [493] total_loss: 2.11725140 d_loss: 1.35180187, g_loss: 0.70535302, ae_loss: 0.06009634\n",
      "Step: [494] total_loss: 2.11788249 d_loss: 1.37380743, g_loss: 0.68547332, ae_loss: 0.05860176\n",
      "Step: [495] total_loss: 2.11339855 d_loss: 1.36657119, g_loss: 0.69040394, ae_loss: 0.05642330\n",
      "Step: [496] total_loss: 2.10343242 d_loss: 1.38267553, g_loss: 0.66363955, ae_loss: 0.05711730\n",
      "Step: [497] total_loss: 2.11212921 d_loss: 1.37033761, g_loss: 0.67929173, ae_loss: 0.06249996\n",
      "Step: [498] total_loss: 2.12361979 d_loss: 1.37248230, g_loss: 0.69415575, ae_loss: 0.05698172\n",
      "Step: [499] total_loss: 2.12821865 d_loss: 1.38141334, g_loss: 0.69344008, ae_loss: 0.05336527\n",
      "Step: [500] total_loss: 2.12761545 d_loss: 1.36786747, g_loss: 0.70090842, ae_loss: 0.05883939\n",
      "Step: [501] total_loss: 2.14045548 d_loss: 1.39979792, g_loss: 0.68418980, ae_loss: 0.05646771\n",
      "Step: [502] total_loss: 2.10928464 d_loss: 1.36926544, g_loss: 0.68371075, ae_loss: 0.05630851\n",
      "Step: [503] total_loss: 2.11894107 d_loss: 1.38466203, g_loss: 0.67489231, ae_loss: 0.05938673\n",
      "Step: [504] total_loss: 2.11550617 d_loss: 1.36671245, g_loss: 0.69155782, ae_loss: 0.05723581\n",
      "Step: [505] total_loss: 2.12086654 d_loss: 1.37651253, g_loss: 0.69075656, ae_loss: 0.05359749\n",
      "Step: [506] total_loss: 2.12410069 d_loss: 1.40385222, g_loss: 0.66432011, ae_loss: 0.05592851\n",
      "Step: [507] total_loss: 2.12005472 d_loss: 1.37890804, g_loss: 0.68491626, ae_loss: 0.05623042\n",
      "Step: [508] total_loss: 2.11171436 d_loss: 1.35670042, g_loss: 0.69861734, ae_loss: 0.05639644\n",
      "Step: [509] total_loss: 2.10667682 d_loss: 1.35745418, g_loss: 0.69097054, ae_loss: 0.05825213\n",
      "Step: [510] total_loss: 2.11118984 d_loss: 1.35872436, g_loss: 0.69204319, ae_loss: 0.06042237\n",
      "Step: [511] total_loss: 2.13649559 d_loss: 1.38664603, g_loss: 0.69102532, ae_loss: 0.05882430\n",
      "Step: [512] total_loss: 2.11014915 d_loss: 1.37385345, g_loss: 0.67928892, ae_loss: 0.05700679\n",
      "Step: [513] total_loss: 2.12560081 d_loss: 1.37573051, g_loss: 0.69656593, ae_loss: 0.05330448\n",
      "Step: [514] total_loss: 2.13538408 d_loss: 1.38725424, g_loss: 0.69032812, ae_loss: 0.05780178\n",
      "Step: [515] total_loss: 2.14129734 d_loss: 1.40653491, g_loss: 0.67823589, ae_loss: 0.05652652\n",
      "Step: [516] total_loss: 2.12538934 d_loss: 1.36862302, g_loss: 0.69722253, ae_loss: 0.05954387\n",
      "Step: [517] total_loss: 2.12134624 d_loss: 1.36903799, g_loss: 0.69499457, ae_loss: 0.05731364\n",
      "Step: [518] total_loss: 2.12242413 d_loss: 1.37962353, g_loss: 0.68482083, ae_loss: 0.05797964\n",
      "Step: [519] total_loss: 2.12033820 d_loss: 1.38845599, g_loss: 0.67523479, ae_loss: 0.05664744\n",
      "Step: [520] total_loss: 2.11283827 d_loss: 1.38013196, g_loss: 0.67096263, ae_loss: 0.06174375\n",
      "Step: [521] total_loss: 2.11865973 d_loss: 1.36603057, g_loss: 0.69572937, ae_loss: 0.05689974\n",
      "Step: [522] total_loss: 2.14317656 d_loss: 1.39965200, g_loss: 0.68580604, ae_loss: 0.05771836\n",
      "Step: [523] total_loss: 2.13939238 d_loss: 1.37579012, g_loss: 0.70362186, ae_loss: 0.05998024\n",
      "Step: [524] total_loss: 2.11732578 d_loss: 1.37840188, g_loss: 0.68291867, ae_loss: 0.05600509\n",
      "Step: [525] total_loss: 2.12718058 d_loss: 1.38434315, g_loss: 0.68620616, ae_loss: 0.05663128\n",
      "Step: [526] total_loss: 2.12172937 d_loss: 1.36624908, g_loss: 0.69984519, ae_loss: 0.05563510\n",
      "Step: [527] total_loss: 2.11957765 d_loss: 1.38230026, g_loss: 0.68204343, ae_loss: 0.05523396\n",
      "Step: [528] total_loss: 2.11855459 d_loss: 1.37498379, g_loss: 0.68769288, ae_loss: 0.05587791\n",
      "Step: [529] total_loss: 2.12252879 d_loss: 1.36510301, g_loss: 0.70214033, ae_loss: 0.05528542\n",
      "Step: [530] total_loss: 2.12444878 d_loss: 1.37469244, g_loss: 0.69341475, ae_loss: 0.05634170\n",
      "Step: [531] total_loss: 2.10981750 d_loss: 1.35366535, g_loss: 0.69655287, ae_loss: 0.05959926\n",
      "Step: [532] total_loss: 2.12637496 d_loss: 1.37876201, g_loss: 0.69156849, ae_loss: 0.05604445\n",
      "Step: [533] total_loss: 2.12454462 d_loss: 1.36173058, g_loss: 0.70553726, ae_loss: 0.05727687\n",
      "Step: [534] total_loss: 2.14058781 d_loss: 1.38720071, g_loss: 0.69376230, ae_loss: 0.05962468\n",
      "Step: [535] total_loss: 2.14040184 d_loss: 1.39546895, g_loss: 0.68932259, ae_loss: 0.05561018\n",
      "Step: [536] total_loss: 2.11595035 d_loss: 1.38305807, g_loss: 0.67683697, ae_loss: 0.05605537\n",
      "Step: [537] total_loss: 2.13868523 d_loss: 1.39152551, g_loss: 0.68984580, ae_loss: 0.05731389\n",
      "Step: [538] total_loss: 2.11878204 d_loss: 1.36009037, g_loss: 0.70199853, ae_loss: 0.05669317\n",
      "Step: [539] total_loss: 2.10266638 d_loss: 1.36118221, g_loss: 0.68556452, ae_loss: 0.05591971\n",
      "Step: [540] total_loss: 2.12666583 d_loss: 1.37097716, g_loss: 0.69950378, ae_loss: 0.05618486\n",
      "Step: [541] total_loss: 2.10396290 d_loss: 1.37439239, g_loss: 0.67273831, ae_loss: 0.05683234\n",
      "Step: [542] total_loss: 2.11876488 d_loss: 1.37926280, g_loss: 0.68599147, ae_loss: 0.05351062\n",
      "Step: [543] total_loss: 2.12984228 d_loss: 1.40350926, g_loss: 0.67157125, ae_loss: 0.05476169\n",
      "Step: [544] total_loss: 2.12686491 d_loss: 1.36909413, g_loss: 0.70159900, ae_loss: 0.05617173\n",
      "Step: [545] total_loss: 2.10644579 d_loss: 1.37092614, g_loss: 0.68112552, ae_loss: 0.05439403\n",
      "Step: [546] total_loss: 2.12580371 d_loss: 1.37630534, g_loss: 0.69267499, ae_loss: 0.05682338\n",
      "Step: [547] total_loss: 2.11740184 d_loss: 1.39068174, g_loss: 0.67177987, ae_loss: 0.05494020\n",
      "Step: [548] total_loss: 2.12293863 d_loss: 1.37693572, g_loss: 0.68808508, ae_loss: 0.05791779\n",
      "Step: [549] total_loss: 2.10418701 d_loss: 1.35273290, g_loss: 0.69916236, ae_loss: 0.05229165\n",
      "Step: [550] total_loss: 2.12344456 d_loss: 1.37683475, g_loss: 0.69031590, ae_loss: 0.05629383\n",
      "Step: [551] total_loss: 2.11281776 d_loss: 1.35216355, g_loss: 0.70109546, ae_loss: 0.05955891\n",
      "Step: [552] total_loss: 2.10077786 d_loss: 1.37097824, g_loss: 0.67415553, ae_loss: 0.05564399\n",
      "Step: [553] total_loss: 2.11873245 d_loss: 1.35603809, g_loss: 0.70029581, ae_loss: 0.06239854\n",
      "Step: [554] total_loss: 2.11103749 d_loss: 1.36525500, g_loss: 0.68827057, ae_loss: 0.05751187\n",
      "Step: [555] total_loss: 2.12674642 d_loss: 1.38551581, g_loss: 0.68265533, ae_loss: 0.05857530\n",
      "Step: [556] total_loss: 2.11300588 d_loss: 1.37868917, g_loss: 0.68146437, ae_loss: 0.05285229\n",
      "Step: [557] total_loss: 2.13132143 d_loss: 1.37214994, g_loss: 0.70416528, ae_loss: 0.05500625\n",
      "Step: [558] total_loss: 2.13892508 d_loss: 1.39147174, g_loss: 0.69158787, ae_loss: 0.05586538\n",
      "Step: [559] total_loss: 2.12044549 d_loss: 1.35161889, g_loss: 0.70985752, ae_loss: 0.05896897\n",
      "Step: [560] total_loss: 2.13385415 d_loss: 1.39965081, g_loss: 0.67815906, ae_loss: 0.05604428\n",
      "Step: [561] total_loss: 2.11878562 d_loss: 1.37645316, g_loss: 0.68478870, ae_loss: 0.05754379\n",
      "Step: [562] total_loss: 2.09922838 d_loss: 1.34990072, g_loss: 0.69596696, ae_loss: 0.05336081\n",
      "Step: [563] total_loss: 2.12193251 d_loss: 1.39501762, g_loss: 0.67293394, ae_loss: 0.05398084\n",
      "Step: [564] total_loss: 2.12386227 d_loss: 1.37740541, g_loss: 0.68879509, ae_loss: 0.05766182\n",
      "Step: [565] total_loss: 2.12018180 d_loss: 1.35732961, g_loss: 0.70389867, ae_loss: 0.05895356\n",
      "Step: [566] total_loss: 2.12565565 d_loss: 1.37962723, g_loss: 0.68939853, ae_loss: 0.05662981\n",
      "Step: [567] total_loss: 2.12580490 d_loss: 1.37537122, g_loss: 0.69031715, ae_loss: 0.06011636\n",
      "Step: [568] total_loss: 2.12513065 d_loss: 1.38523269, g_loss: 0.68339956, ae_loss: 0.05649854\n",
      "Step: [569] total_loss: 2.11711097 d_loss: 1.38601458, g_loss: 0.68007571, ae_loss: 0.05102075\n",
      "Step: [570] total_loss: 2.13274336 d_loss: 1.38491249, g_loss: 0.69170451, ae_loss: 0.05612636\n",
      "Step: [571] total_loss: 2.11363673 d_loss: 1.37024617, g_loss: 0.68433517, ae_loss: 0.05905538\n",
      "Step: [572] total_loss: 2.13010406 d_loss: 1.38754964, g_loss: 0.68786371, ae_loss: 0.05469055\n",
      "Step: [573] total_loss: 2.12812376 d_loss: 1.38385916, g_loss: 0.68969023, ae_loss: 0.05457423\n",
      "Step: [574] total_loss: 2.12989688 d_loss: 1.38761961, g_loss: 0.68394595, ae_loss: 0.05833142\n",
      "Step: [575] total_loss: 2.12003398 d_loss: 1.37929165, g_loss: 0.68606257, ae_loss: 0.05467974\n",
      "Step: [576] total_loss: 2.10811424 d_loss: 1.35848618, g_loss: 0.69065696, ae_loss: 0.05897115\n",
      "Step: [577] total_loss: 2.09890079 d_loss: 1.35563958, g_loss: 0.68824613, ae_loss: 0.05501518\n",
      "Step: [578] total_loss: 2.10189271 d_loss: 1.35833347, g_loss: 0.68510580, ae_loss: 0.05845347\n",
      "Step: [579] total_loss: 2.12643909 d_loss: 1.38084102, g_loss: 0.69094777, ae_loss: 0.05465023\n",
      "Step: [580] total_loss: 2.10062027 d_loss: 1.36042976, g_loss: 0.68609291, ae_loss: 0.05409749\n",
      "Step: [581] total_loss: 2.09955072 d_loss: 1.36039329, g_loss: 0.68358040, ae_loss: 0.05557713\n",
      "Step: [582] total_loss: 2.12792039 d_loss: 1.38583791, g_loss: 0.68641794, ae_loss: 0.05566454\n",
      "Step: [583] total_loss: 2.11252832 d_loss: 1.38468122, g_loss: 0.67074955, ae_loss: 0.05709738\n",
      "Step: [584] total_loss: 2.12776875 d_loss: 1.36043334, g_loss: 0.71106046, ae_loss: 0.05627489\n",
      "Step: [585] total_loss: 2.09816051 d_loss: 1.35963917, g_loss: 0.68379855, ae_loss: 0.05472274\n",
      "Step: [586] total_loss: 2.12164378 d_loss: 1.39305496, g_loss: 0.67097169, ae_loss: 0.05761701\n",
      "Step: [587] total_loss: 2.13359880 d_loss: 1.39215732, g_loss: 0.68296719, ae_loss: 0.05847435\n",
      "Step: [588] total_loss: 2.12523890 d_loss: 1.37097144, g_loss: 0.69897187, ae_loss: 0.05529565\n",
      "Step: [589] total_loss: 2.12687969 d_loss: 1.36767459, g_loss: 0.70227820, ae_loss: 0.05692684\n",
      "Step: [590] total_loss: 2.13367772 d_loss: 1.39406300, g_loss: 0.68377960, ae_loss: 0.05583517\n",
      "Step: [591] total_loss: 2.12443161 d_loss: 1.38206446, g_loss: 0.68719798, ae_loss: 0.05516922\n",
      "Step: [592] total_loss: 2.11304855 d_loss: 1.38211441, g_loss: 0.67545420, ae_loss: 0.05547994\n",
      "Step: [593] total_loss: 2.12067175 d_loss: 1.37542713, g_loss: 0.68838537, ae_loss: 0.05685915\n",
      "Step: [594] total_loss: 2.11376429 d_loss: 1.34773743, g_loss: 0.70656621, ae_loss: 0.05946060\n",
      "Step: [595] total_loss: 2.12876153 d_loss: 1.38336110, g_loss: 0.68686455, ae_loss: 0.05853576\n",
      "Step: [596] total_loss: 2.14245605 d_loss: 1.37789130, g_loss: 0.70234799, ae_loss: 0.06221683\n",
      "Step: [597] total_loss: 2.13061571 d_loss: 1.38450062, g_loss: 0.69163173, ae_loss: 0.05448330\n",
      "Step: [598] total_loss: 2.11729503 d_loss: 1.37699223, g_loss: 0.68436033, ae_loss: 0.05594242\n",
      "Step: [599] total_loss: 2.11644650 d_loss: 1.34457624, g_loss: 0.71433157, ae_loss: 0.05753878\n",
      "Step: [600] total_loss: 2.12496519 d_loss: 1.37903285, g_loss: 0.69209111, ae_loss: 0.05384107\n",
      "Step: [601] total_loss: 2.09097958 d_loss: 1.34333777, g_loss: 0.69276237, ae_loss: 0.05487949\n",
      "Step: [602] total_loss: 2.11216021 d_loss: 1.35372663, g_loss: 0.70146221, ae_loss: 0.05697130\n",
      "Step: [603] total_loss: 2.11719322 d_loss: 1.36530375, g_loss: 0.68954265, ae_loss: 0.06234675\n",
      "Step: [604] total_loss: 2.10985756 d_loss: 1.34851921, g_loss: 0.70254385, ae_loss: 0.05879457\n",
      "Step: [605] total_loss: 2.14831209 d_loss: 1.38383484, g_loss: 0.71003830, ae_loss: 0.05443899\n",
      "Step: [606] total_loss: 2.12452030 d_loss: 1.37557054, g_loss: 0.69010139, ae_loss: 0.05884854\n",
      "Step: [607] total_loss: 2.11271000 d_loss: 1.36804914, g_loss: 0.69081223, ae_loss: 0.05384854\n",
      "Step: [608] total_loss: 2.12249851 d_loss: 1.36487806, g_loss: 0.70073402, ae_loss: 0.05688634\n",
      "Step: [609] total_loss: 2.11768484 d_loss: 1.36342669, g_loss: 0.69353789, ae_loss: 0.06072031\n",
      "Step: [610] total_loss: 2.13467360 d_loss: 1.38659596, g_loss: 0.69314826, ae_loss: 0.05492923\n",
      "Step: [611] total_loss: 2.12092113 d_loss: 1.36451983, g_loss: 0.69869339, ae_loss: 0.05770798\n",
      "Step: [612] total_loss: 2.10691214 d_loss: 1.36614120, g_loss: 0.68647921, ae_loss: 0.05429160\n",
      "Step: [613] total_loss: 2.11729026 d_loss: 1.37413001, g_loss: 0.68523681, ae_loss: 0.05792344\n",
      "Step: [614] total_loss: 2.12732410 d_loss: 1.39474702, g_loss: 0.68141663, ae_loss: 0.05116051\n",
      "Step: [615] total_loss: 2.11140871 d_loss: 1.36080074, g_loss: 0.69289887, ae_loss: 0.05770927\n",
      "Step: [616] total_loss: 2.13110399 d_loss: 1.37997127, g_loss: 0.69422233, ae_loss: 0.05691023\n",
      "Step: [617] total_loss: 2.11475587 d_loss: 1.37759495, g_loss: 0.68080008, ae_loss: 0.05636080\n",
      "Step: [618] total_loss: 2.12475300 d_loss: 1.38126159, g_loss: 0.68707603, ae_loss: 0.05641527\n",
      "Step: [619] total_loss: 2.11852837 d_loss: 1.39162481, g_loss: 0.67113996, ae_loss: 0.05576363\n",
      "Step: [620] total_loss: 2.13080812 d_loss: 1.39760256, g_loss: 0.68081450, ae_loss: 0.05239108\n",
      "Step: [621] total_loss: 2.11769629 d_loss: 1.37802219, g_loss: 0.68003607, ae_loss: 0.05963818\n",
      "Step: [622] total_loss: 2.10171700 d_loss: 1.38217223, g_loss: 0.66507435, ae_loss: 0.05447052\n",
      "Step: [623] total_loss: 2.11371398 d_loss: 1.35607374, g_loss: 0.69794208, ae_loss: 0.05969810\n",
      "Step: [624] total_loss: 2.10638475 d_loss: 1.38597393, g_loss: 0.66420913, ae_loss: 0.05620154\n",
      "Step: [625] total_loss: 2.11564493 d_loss: 1.37489808, g_loss: 0.68614316, ae_loss: 0.05460356\n",
      "Step: [626] total_loss: 2.11865187 d_loss: 1.35911226, g_loss: 0.70407724, ae_loss: 0.05546245\n",
      "Step: [627] total_loss: 2.12546468 d_loss: 1.37142205, g_loss: 0.70221591, ae_loss: 0.05182667\n",
      "Step: [628] total_loss: 2.12428999 d_loss: 1.38609624, g_loss: 0.68892038, ae_loss: 0.04927345\n",
      "Step: [629] total_loss: 2.12686610 d_loss: 1.36152852, g_loss: 0.70739532, ae_loss: 0.05794227\n",
      "Step: [630] total_loss: 2.11048961 d_loss: 1.36435270, g_loss: 0.68844604, ae_loss: 0.05769083\n",
      "Step: [631] total_loss: 2.11038399 d_loss: 1.36261916, g_loss: 0.69452119, ae_loss: 0.05324373\n",
      "Step: [632] total_loss: 2.11964607 d_loss: 1.35885489, g_loss: 0.70163876, ae_loss: 0.05915243\n",
      "Step: [633] total_loss: 2.11975193 d_loss: 1.37911570, g_loss: 0.68945163, ae_loss: 0.05118448\n",
      "Step: [634] total_loss: 2.11748409 d_loss: 1.38247180, g_loss: 0.68379033, ae_loss: 0.05122181\n",
      "Step: [635] total_loss: 2.11034012 d_loss: 1.35991740, g_loss: 0.69437671, ae_loss: 0.05604608\n",
      "Step: [636] total_loss: 2.11140013 d_loss: 1.37762702, g_loss: 0.68001533, ae_loss: 0.05375764\n",
      "Step: [637] total_loss: 2.12617517 d_loss: 1.40160251, g_loss: 0.66567707, ae_loss: 0.05889556\n",
      "Step: [638] total_loss: 2.11121607 d_loss: 1.36558795, g_loss: 0.68632668, ae_loss: 0.05930153\n",
      "Step: [639] total_loss: 2.13166380 d_loss: 1.36584377, g_loss: 0.70892531, ae_loss: 0.05689468\n",
      "Step: [640] total_loss: 2.12431717 d_loss: 1.38188636, g_loss: 0.68947971, ae_loss: 0.05295125\n",
      "Step: [641] total_loss: 2.13238549 d_loss: 1.39630437, g_loss: 0.68284005, ae_loss: 0.05324107\n",
      "Step: [642] total_loss: 2.12896776 d_loss: 1.37401152, g_loss: 0.69989586, ae_loss: 0.05506052\n",
      "Step: [643] total_loss: 2.12102079 d_loss: 1.37603462, g_loss: 0.68831301, ae_loss: 0.05667304\n",
      "Step: [644] total_loss: 2.12255144 d_loss: 1.36837959, g_loss: 0.69728971, ae_loss: 0.05688210\n",
      "Step: [645] total_loss: 2.11745119 d_loss: 1.38046467, g_loss: 0.68372995, ae_loss: 0.05325650\n",
      "Step: [646] total_loss: 2.12952471 d_loss: 1.39215016, g_loss: 0.68275225, ae_loss: 0.05462244\n",
      "Step: [647] total_loss: 2.11717653 d_loss: 1.36382079, g_loss: 0.69389844, ae_loss: 0.05945745\n",
      "Step: [648] total_loss: 2.12398076 d_loss: 1.37588632, g_loss: 0.69215423, ae_loss: 0.05594028\n",
      "Step: [649] total_loss: 2.13112187 d_loss: 1.37924576, g_loss: 0.69087917, ae_loss: 0.06099703\n",
      "Step: [650] total_loss: 2.12774420 d_loss: 1.38066220, g_loss: 0.69022679, ae_loss: 0.05685507\n",
      "Step: [651] total_loss: 2.12120223 d_loss: 1.36450708, g_loss: 0.70039380, ae_loss: 0.05630130\n",
      "Step: [652] total_loss: 2.13080478 d_loss: 1.37217510, g_loss: 0.70751959, ae_loss: 0.05111013\n",
      "Step: [653] total_loss: 2.13405371 d_loss: 1.37796879, g_loss: 0.69761533, ae_loss: 0.05846951\n",
      "Step: [654] total_loss: 2.12418437 d_loss: 1.38908482, g_loss: 0.67930973, ae_loss: 0.05578978\n",
      "Step: [655] total_loss: 2.10818100 d_loss: 1.36031079, g_loss: 0.69017780, ae_loss: 0.05769255\n",
      "Step: [656] total_loss: 2.12850428 d_loss: 1.37074399, g_loss: 0.70488924, ae_loss: 0.05287100\n",
      "Step: [657] total_loss: 2.12831211 d_loss: 1.39721298, g_loss: 0.67634642, ae_loss: 0.05475268\n",
      "Step: [658] total_loss: 2.11697054 d_loss: 1.37918186, g_loss: 0.68026161, ae_loss: 0.05752708\n",
      "Step: [659] total_loss: 2.10536361 d_loss: 1.37607741, g_loss: 0.67587280, ae_loss: 0.05341339\n",
      "Step: [660] total_loss: 2.14319062 d_loss: 1.38269520, g_loss: 0.70577234, ae_loss: 0.05472301\n",
      "Step: [661] total_loss: 2.12840533 d_loss: 1.36983657, g_loss: 0.70716083, ae_loss: 0.05140794\n",
      "Step: [662] total_loss: 2.12602925 d_loss: 1.37257767, g_loss: 0.69757241, ae_loss: 0.05587910\n",
      "Step: [663] total_loss: 2.11701441 d_loss: 1.36003017, g_loss: 0.70217335, ae_loss: 0.05481103\n",
      "Step: [664] total_loss: 2.13220453 d_loss: 1.38760376, g_loss: 0.68928128, ae_loss: 0.05531942\n",
      "Step: [665] total_loss: 2.11368227 d_loss: 1.37384307, g_loss: 0.68712330, ae_loss: 0.05271593\n",
      "Step: [666] total_loss: 2.12222052 d_loss: 1.37522197, g_loss: 0.68799114, ae_loss: 0.05900728\n",
      "Step: [667] total_loss: 2.12266636 d_loss: 1.37609506, g_loss: 0.69238770, ae_loss: 0.05418370\n",
      "Step: [668] total_loss: 2.13476515 d_loss: 1.38599992, g_loss: 0.69553304, ae_loss: 0.05323226\n",
      "Step: [669] total_loss: 2.11608744 d_loss: 1.37919283, g_loss: 0.67905807, ae_loss: 0.05783657\n",
      "Step: [670] total_loss: 2.13316250 d_loss: 1.37109876, g_loss: 0.70528984, ae_loss: 0.05677400\n",
      "Step: [671] total_loss: 2.12007332 d_loss: 1.38945866, g_loss: 0.67489243, ae_loss: 0.05572227\n",
      "Step: [672] total_loss: 2.14473295 d_loss: 1.41910470, g_loss: 0.66657108, ae_loss: 0.05905707\n",
      "Step: [673] total_loss: 2.11762452 d_loss: 1.36025488, g_loss: 0.70224744, ae_loss: 0.05512220\n",
      "Step: [674] total_loss: 2.14713597 d_loss: 1.38755965, g_loss: 0.70370907, ae_loss: 0.05586726\n",
      "Step: [675] total_loss: 2.13666224 d_loss: 1.39154458, g_loss: 0.69195122, ae_loss: 0.05316633\n",
      "Step: [676] total_loss: 2.12303162 d_loss: 1.36970019, g_loss: 0.69776493, ae_loss: 0.05556658\n",
      "Step: [677] total_loss: 2.13286281 d_loss: 1.39597213, g_loss: 0.68452305, ae_loss: 0.05236762\n",
      "Step: [678] total_loss: 2.12380219 d_loss: 1.35787058, g_loss: 0.70940000, ae_loss: 0.05653167\n",
      "Step: [679] total_loss: 2.11611962 d_loss: 1.37101293, g_loss: 0.69267088, ae_loss: 0.05243586\n",
      "Step: [680] total_loss: 2.12636423 d_loss: 1.38081837, g_loss: 0.69211709, ae_loss: 0.05342884\n",
      "Step: [681] total_loss: 2.11141634 d_loss: 1.35470951, g_loss: 0.70369732, ae_loss: 0.05300942\n",
      "Step: [682] total_loss: 2.10954571 d_loss: 1.34872663, g_loss: 0.69808525, ae_loss: 0.06273387\n",
      "Step: [683] total_loss: 2.12080574 d_loss: 1.38292110, g_loss: 0.68493247, ae_loss: 0.05295229\n",
      "Step: [684] total_loss: 2.12112117 d_loss: 1.36501002, g_loss: 0.70221376, ae_loss: 0.05389743\n",
      "Step: [685] total_loss: 2.12036467 d_loss: 1.37393844, g_loss: 0.69432294, ae_loss: 0.05210312\n",
      "Step: [686] total_loss: 2.12908792 d_loss: 1.38759208, g_loss: 0.68668675, ae_loss: 0.05480900\n",
      "Step: [687] total_loss: 2.10927010 d_loss: 1.36963761, g_loss: 0.68589449, ae_loss: 0.05373786\n",
      "Step: [688] total_loss: 2.12239909 d_loss: 1.36953020, g_loss: 0.69782048, ae_loss: 0.05504831\n",
      "Step: [689] total_loss: 2.13809156 d_loss: 1.36875451, g_loss: 0.71068710, ae_loss: 0.05865003\n",
      "Step: [690] total_loss: 2.14051533 d_loss: 1.38564038, g_loss: 0.69853103, ae_loss: 0.05634404\n",
      "Step: [691] total_loss: 2.12914705 d_loss: 1.37227178, g_loss: 0.70459378, ae_loss: 0.05228149\n",
      "Step: [692] total_loss: 2.12874365 d_loss: 1.37194085, g_loss: 0.70056570, ae_loss: 0.05623712\n",
      "Step: [693] total_loss: 2.11352110 d_loss: 1.35688496, g_loss: 0.70194423, ae_loss: 0.05469181\n",
      "Step: [694] total_loss: 2.12093830 d_loss: 1.37901485, g_loss: 0.68487036, ae_loss: 0.05705316\n",
      "Step: [695] total_loss: 2.12876296 d_loss: 1.39183974, g_loss: 0.68242085, ae_loss: 0.05450235\n",
      "Step: [696] total_loss: 2.13205314 d_loss: 1.38543415, g_loss: 0.69129878, ae_loss: 0.05532011\n",
      "Step: [697] total_loss: 2.12075210 d_loss: 1.36105704, g_loss: 0.70301902, ae_loss: 0.05667605\n",
      "Step: [698] total_loss: 2.14086628 d_loss: 1.39512897, g_loss: 0.69395185, ae_loss: 0.05178536\n",
      "Step: [699] total_loss: 2.13363624 d_loss: 1.39833498, g_loss: 0.68243206, ae_loss: 0.05286916\n",
      "Step: [700] total_loss: 2.11727548 d_loss: 1.38333559, g_loss: 0.68259871, ae_loss: 0.05134119\n",
      "Step: [701] total_loss: 2.13098717 d_loss: 1.37980509, g_loss: 0.70019907, ae_loss: 0.05098303\n",
      "Step: [702] total_loss: 2.09951878 d_loss: 1.37653732, g_loss: 0.66749048, ae_loss: 0.05549113\n",
      "Step: [703] total_loss: 2.12626195 d_loss: 1.38949621, g_loss: 0.67755455, ae_loss: 0.05921127\n",
      "Step: [704] total_loss: 2.10590506 d_loss: 1.38657713, g_loss: 0.66787541, ae_loss: 0.05145236\n",
      "Step: [705] total_loss: 2.12895155 d_loss: 1.37627697, g_loss: 0.70022660, ae_loss: 0.05244797\n",
      "Step: [706] total_loss: 2.10766840 d_loss: 1.37432647, g_loss: 0.68114221, ae_loss: 0.05219980\n",
      "Step: [707] total_loss: 2.12813401 d_loss: 1.37718534, g_loss: 0.69732875, ae_loss: 0.05361983\n",
      "Step: [708] total_loss: 2.11799574 d_loss: 1.38258219, g_loss: 0.68141073, ae_loss: 0.05400281\n",
      "Step: [709] total_loss: 2.11102176 d_loss: 1.38420200, g_loss: 0.67286742, ae_loss: 0.05395233\n",
      "Step: [710] total_loss: 2.12239218 d_loss: 1.37041318, g_loss: 0.69474065, ae_loss: 0.05723852\n",
      "Step: [711] total_loss: 2.10546231 d_loss: 1.36973178, g_loss: 0.68511927, ae_loss: 0.05061124\n",
      "Step: [712] total_loss: 2.10832453 d_loss: 1.38781929, g_loss: 0.67070550, ae_loss: 0.04979963\n",
      "Step: [713] total_loss: 2.11076856 d_loss: 1.36925566, g_loss: 0.68902934, ae_loss: 0.05248353\n",
      "Step: [714] total_loss: 2.11840296 d_loss: 1.38690829, g_loss: 0.67494500, ae_loss: 0.05654980\n",
      "Step: [715] total_loss: 2.12434888 d_loss: 1.37088656, g_loss: 0.69827735, ae_loss: 0.05518499\n",
      "Step: [716] total_loss: 2.12903929 d_loss: 1.37487841, g_loss: 0.70081234, ae_loss: 0.05334850\n",
      "Step: [717] total_loss: 2.11067128 d_loss: 1.36104894, g_loss: 0.69123256, ae_loss: 0.05838976\n",
      "Step: [718] total_loss: 2.11691332 d_loss: 1.38483787, g_loss: 0.68092275, ae_loss: 0.05115253\n",
      "Step: [719] total_loss: 2.10800505 d_loss: 1.37424731, g_loss: 0.67981869, ae_loss: 0.05393894\n",
      "Step: [720] total_loss: 2.12876701 d_loss: 1.38782287, g_loss: 0.68255258, ae_loss: 0.05839170\n",
      "Step: [721] total_loss: 2.11678290 d_loss: 1.38403702, g_loss: 0.68177295, ae_loss: 0.05097290\n",
      "Step: [722] total_loss: 2.13102007 d_loss: 1.39155900, g_loss: 0.68330908, ae_loss: 0.05615204\n",
      "Step: [723] total_loss: 2.11537027 d_loss: 1.36540413, g_loss: 0.69178307, ae_loss: 0.05818296\n",
      "Step: [724] total_loss: 2.11220789 d_loss: 1.34062672, g_loss: 0.71543896, ae_loss: 0.05614234\n",
      "Step: [725] total_loss: 2.12417459 d_loss: 1.39099920, g_loss: 0.67699528, ae_loss: 0.05618019\n",
      "Step: [726] total_loss: 2.11277056 d_loss: 1.38572025, g_loss: 0.67252964, ae_loss: 0.05452064\n",
      "Step: [727] total_loss: 2.12473035 d_loss: 1.37686479, g_loss: 0.68560714, ae_loss: 0.06225853\n",
      "Step: [728] total_loss: 2.11116266 d_loss: 1.35743427, g_loss: 0.69744718, ae_loss: 0.05628136\n",
      "Step: [729] total_loss: 2.13436127 d_loss: 1.37822580, g_loss: 0.69971037, ae_loss: 0.05642492\n",
      "Step: [730] total_loss: 2.14410424 d_loss: 1.40221310, g_loss: 0.68768322, ae_loss: 0.05420790\n",
      "Step: [731] total_loss: 2.14749050 d_loss: 1.38258362, g_loss: 0.71001250, ae_loss: 0.05489445\n",
      "Step: [732] total_loss: 2.13391447 d_loss: 1.38766921, g_loss: 0.69409221, ae_loss: 0.05215294\n",
      "Step: [733] total_loss: 2.14203572 d_loss: 1.38210988, g_loss: 0.70660812, ae_loss: 0.05331773\n",
      "Step: [734] total_loss: 2.13849497 d_loss: 1.37622797, g_loss: 0.70598841, ae_loss: 0.05627853\n",
      "Step: [735] total_loss: 2.11681771 d_loss: 1.35964954, g_loss: 0.70152873, ae_loss: 0.05563942\n",
      "Step: [736] total_loss: 2.12407994 d_loss: 1.36446548, g_loss: 0.70369798, ae_loss: 0.05591648\n",
      "Step: [737] total_loss: 2.12207460 d_loss: 1.38510144, g_loss: 0.68079209, ae_loss: 0.05618116\n",
      "Step: [738] total_loss: 2.12621975 d_loss: 1.38579762, g_loss: 0.68676382, ae_loss: 0.05365830\n",
      "Step: [739] total_loss: 2.11166668 d_loss: 1.34557307, g_loss: 0.70691895, ae_loss: 0.05917482\n",
      "Step: [740] total_loss: 2.10796595 d_loss: 1.37172914, g_loss: 0.68128806, ae_loss: 0.05494869\n",
      "Step: [741] total_loss: 2.12622976 d_loss: 1.37656450, g_loss: 0.69517326, ae_loss: 0.05449211\n",
      "Step: [742] total_loss: 2.11260080 d_loss: 1.36217380, g_loss: 0.69545698, ae_loss: 0.05497005\n",
      "Step: [743] total_loss: 2.11493421 d_loss: 1.37057066, g_loss: 0.68923992, ae_loss: 0.05512354\n",
      "Step: [744] total_loss: 2.10823536 d_loss: 1.37455201, g_loss: 0.67754066, ae_loss: 0.05614267\n",
      "Step: [745] total_loss: 2.11817026 d_loss: 1.38561201, g_loss: 0.67908120, ae_loss: 0.05347689\n",
      "Step: [746] total_loss: 2.14334679 d_loss: 1.38934684, g_loss: 0.69525784, ae_loss: 0.05874211\n",
      "Step: [747] total_loss: 2.12394691 d_loss: 1.37704873, g_loss: 0.69010681, ae_loss: 0.05679132\n",
      "Step: [748] total_loss: 2.14195967 d_loss: 1.39026165, g_loss: 0.70001006, ae_loss: 0.05168793\n",
      "Step: [749] total_loss: 2.11324859 d_loss: 1.36705589, g_loss: 0.68882191, ae_loss: 0.05737080\n",
      "Step: [750] total_loss: 2.14400196 d_loss: 1.37386179, g_loss: 0.71250904, ae_loss: 0.05763128\n",
      "Step: [751] total_loss: 2.12230682 d_loss: 1.36341918, g_loss: 0.70492291, ae_loss: 0.05396463\n",
      "Step: [752] total_loss: 2.12700701 d_loss: 1.37453139, g_loss: 0.69638109, ae_loss: 0.05609461\n",
      "Step: [753] total_loss: 2.11630201 d_loss: 1.37428749, g_loss: 0.69016075, ae_loss: 0.05185360\n",
      "Step: [754] total_loss: 2.10831070 d_loss: 1.38733780, g_loss: 0.66627276, ae_loss: 0.05470027\n",
      "Step: [755] total_loss: 2.10243130 d_loss: 1.36663544, g_loss: 0.68223745, ae_loss: 0.05355845\n",
      "Step: [756] total_loss: 2.11685133 d_loss: 1.36188591, g_loss: 0.70289230, ae_loss: 0.05207302\n",
      "Step: [757] total_loss: 2.14827204 d_loss: 1.39001977, g_loss: 0.70447278, ae_loss: 0.05377962\n",
      "Step: [758] total_loss: 2.13742495 d_loss: 1.39026678, g_loss: 0.69209456, ae_loss: 0.05506347\n",
      "Step: [759] total_loss: 2.12908220 d_loss: 1.39001381, g_loss: 0.68068963, ae_loss: 0.05837882\n",
      "Step: [760] total_loss: 2.12659931 d_loss: 1.39108968, g_loss: 0.67899132, ae_loss: 0.05651837\n",
      "Step: [761] total_loss: 2.13019395 d_loss: 1.40042317, g_loss: 0.67832476, ae_loss: 0.05144608\n",
      "Step: [762] total_loss: 2.15201139 d_loss: 1.40452766, g_loss: 0.69347322, ae_loss: 0.05401044\n",
      "Step: [763] total_loss: 2.14483809 d_loss: 1.39394391, g_loss: 0.69676745, ae_loss: 0.05412671\n",
      "Step: [764] total_loss: 2.11860847 d_loss: 1.38108623, g_loss: 0.68291080, ae_loss: 0.05461138\n",
      "Step: [765] total_loss: 2.15753078 d_loss: 1.39644253, g_loss: 0.70611024, ae_loss: 0.05497798\n",
      "Step: [766] total_loss: 2.12857056 d_loss: 1.38098383, g_loss: 0.69062245, ae_loss: 0.05696438\n",
      "Step: [767] total_loss: 2.13735271 d_loss: 1.37990415, g_loss: 0.70431864, ae_loss: 0.05312990\n",
      "Step: [768] total_loss: 2.14556408 d_loss: 1.37440753, g_loss: 0.71628582, ae_loss: 0.05487071\n",
      "Step: [769] total_loss: 2.12953377 d_loss: 1.37284720, g_loss: 0.69910926, ae_loss: 0.05757738\n",
      "Step: [770] total_loss: 2.12547445 d_loss: 1.36882401, g_loss: 0.70191830, ae_loss: 0.05473211\n",
      "Step: [771] total_loss: 2.12190199 d_loss: 1.35405231, g_loss: 0.71298969, ae_loss: 0.05485996\n",
      "Step: [772] total_loss: 2.11889601 d_loss: 1.36983466, g_loss: 0.69375694, ae_loss: 0.05530435\n",
      "Step: [773] total_loss: 2.10578394 d_loss: 1.36785293, g_loss: 0.68233657, ae_loss: 0.05559430\n",
      "Step: [774] total_loss: 2.10665607 d_loss: 1.37676728, g_loss: 0.67645419, ae_loss: 0.05343444\n",
      "Step: [775] total_loss: 2.11052895 d_loss: 1.37298036, g_loss: 0.68389726, ae_loss: 0.05365143\n",
      "Step: [776] total_loss: 2.10761571 d_loss: 1.37143362, g_loss: 0.68259388, ae_loss: 0.05358810\n",
      "Step: [777] total_loss: 2.11333513 d_loss: 1.38372898, g_loss: 0.67775404, ae_loss: 0.05185221\n",
      "Step: [778] total_loss: 2.13060021 d_loss: 1.39517522, g_loss: 0.68464237, ae_loss: 0.05078255\n",
      "Step: [779] total_loss: 2.13573050 d_loss: 1.38522196, g_loss: 0.70030177, ae_loss: 0.05020679\n",
      "Step: [780] total_loss: 2.13673925 d_loss: 1.38562250, g_loss: 0.69786310, ae_loss: 0.05325354\n",
      "Step: [781] total_loss: 2.15054035 d_loss: 1.39562035, g_loss: 0.69939518, ae_loss: 0.05552486\n",
      "Step: [782] total_loss: 2.11596632 d_loss: 1.36923015, g_loss: 0.69324231, ae_loss: 0.05349374\n",
      "Step: [783] total_loss: 2.13927078 d_loss: 1.39661193, g_loss: 0.69221473, ae_loss: 0.05044419\n",
      "Step: [784] total_loss: 2.11739254 d_loss: 1.37720656, g_loss: 0.68715763, ae_loss: 0.05302822\n",
      "Step: [785] total_loss: 2.13961267 d_loss: 1.37930727, g_loss: 0.70479935, ae_loss: 0.05550605\n",
      "Step: [786] total_loss: 2.11406755 d_loss: 1.38514113, g_loss: 0.67331898, ae_loss: 0.05560743\n",
      "Step: [787] total_loss: 2.11725497 d_loss: 1.37161779, g_loss: 0.69730216, ae_loss: 0.04833494\n",
      "Step: [788] total_loss: 2.13113666 d_loss: 1.39480102, g_loss: 0.68065768, ae_loss: 0.05567795\n",
      "Step: [789] total_loss: 2.11666298 d_loss: 1.37372613, g_loss: 0.69132113, ae_loss: 0.05161569\n",
      "Step: [790] total_loss: 2.13429070 d_loss: 1.37758970, g_loss: 0.70218533, ae_loss: 0.05451560\n",
      "Step: [791] total_loss: 2.12475443 d_loss: 1.37159252, g_loss: 0.70026994, ae_loss: 0.05289183\n",
      "Step: [792] total_loss: 2.11872959 d_loss: 1.35883367, g_loss: 0.70702088, ae_loss: 0.05287500\n",
      "Step: [793] total_loss: 2.11468577 d_loss: 1.37433875, g_loss: 0.68390876, ae_loss: 0.05643832\n",
      "Step: [794] total_loss: 2.12169647 d_loss: 1.36740041, g_loss: 0.70320457, ae_loss: 0.05109146\n",
      "Step: [795] total_loss: 2.11942959 d_loss: 1.38076830, g_loss: 0.68570173, ae_loss: 0.05295948\n",
      "Step: [796] total_loss: 2.13142014 d_loss: 1.39836216, g_loss: 0.67922723, ae_loss: 0.05383080\n",
      "Step: [797] total_loss: 2.09173322 d_loss: 1.35909748, g_loss: 0.67593175, ae_loss: 0.05670395\n",
      "Step: [798] total_loss: 2.12068200 d_loss: 1.37746429, g_loss: 0.68587750, ae_loss: 0.05734023\n",
      "Step: [799] total_loss: 2.11172938 d_loss: 1.37526011, g_loss: 0.68229830, ae_loss: 0.05417102\n",
      "Step: [800] total_loss: 2.10912037 d_loss: 1.36363626, g_loss: 0.69126570, ae_loss: 0.05421848\n",
      "Step: [801] total_loss: 2.13662696 d_loss: 1.37337673, g_loss: 0.70493346, ae_loss: 0.05831688\n",
      "Step: [802] total_loss: 2.14555073 d_loss: 1.39669037, g_loss: 0.69272459, ae_loss: 0.05613562\n",
      "Step: [803] total_loss: 2.11561251 d_loss: 1.36972356, g_loss: 0.69153798, ae_loss: 0.05435110\n",
      "Step: [804] total_loss: 2.13332391 d_loss: 1.38631845, g_loss: 0.69275141, ae_loss: 0.05425408\n",
      "Step: [805] total_loss: 2.12894344 d_loss: 1.35604095, g_loss: 0.71799731, ae_loss: 0.05490523\n",
      "Step: [806] total_loss: 2.12788773 d_loss: 1.39552176, g_loss: 0.68011034, ae_loss: 0.05225561\n",
      "Step: [807] total_loss: 2.13606238 d_loss: 1.36984491, g_loss: 0.71487814, ae_loss: 0.05133929\n",
      "Step: [808] total_loss: 2.13673234 d_loss: 1.37455249, g_loss: 0.70596147, ae_loss: 0.05621839\n",
      "Step: [809] total_loss: 2.12553596 d_loss: 1.37372017, g_loss: 0.69831944, ae_loss: 0.05349620\n",
      "Step: [810] total_loss: 2.10602236 d_loss: 1.34161592, g_loss: 0.70899141, ae_loss: 0.05541515\n",
      "Step: [811] total_loss: 2.11686468 d_loss: 1.37478018, g_loss: 0.68695652, ae_loss: 0.05512815\n",
      "Step: [812] total_loss: 2.11660457 d_loss: 1.37896156, g_loss: 0.69085872, ae_loss: 0.04678424\n",
      "Step: [813] total_loss: 2.11452627 d_loss: 1.37817740, g_loss: 0.67832530, ae_loss: 0.05802369\n",
      "Step: [814] total_loss: 2.12714624 d_loss: 1.38061452, g_loss: 0.68976527, ae_loss: 0.05676640\n",
      "Step: [815] total_loss: 2.12568903 d_loss: 1.38800108, g_loss: 0.68312573, ae_loss: 0.05456205\n",
      "Step: [816] total_loss: 2.13758039 d_loss: 1.38892019, g_loss: 0.69215280, ae_loss: 0.05650732\n",
      "Step: [817] total_loss: 2.13836694 d_loss: 1.39332891, g_loss: 0.69367653, ae_loss: 0.05136145\n",
      "Step: [818] total_loss: 2.11605310 d_loss: 1.37134850, g_loss: 0.69166130, ae_loss: 0.05304337\n",
      "Step: [819] total_loss: 2.14571667 d_loss: 1.38972449, g_loss: 0.70014751, ae_loss: 0.05584462\n",
      "Step: [820] total_loss: 2.10991478 d_loss: 1.38013947, g_loss: 0.67898810, ae_loss: 0.05078724\n",
      "Step: [821] total_loss: 2.14060998 d_loss: 1.37090409, g_loss: 0.71455383, ae_loss: 0.05515201\n",
      "Step: [822] total_loss: 2.16328144 d_loss: 1.40356803, g_loss: 0.70173025, ae_loss: 0.05798325\n",
      "Step: [823] total_loss: 2.14397979 d_loss: 1.39664316, g_loss: 0.69273245, ae_loss: 0.05460412\n",
      "Step: [824] total_loss: 2.14250898 d_loss: 1.40004539, g_loss: 0.68438470, ae_loss: 0.05807872\n",
      "Step: [825] total_loss: 2.15024662 d_loss: 1.36497080, g_loss: 0.72780204, ae_loss: 0.05747377\n",
      "Step: [826] total_loss: 2.12266135 d_loss: 1.39432108, g_loss: 0.67699790, ae_loss: 0.05134233\n",
      "Step: [827] total_loss: 2.13002992 d_loss: 1.36719882, g_loss: 0.70384943, ae_loss: 0.05898166\n",
      "Step: [828] total_loss: 2.10073948 d_loss: 1.36818850, g_loss: 0.68041039, ae_loss: 0.05214070\n",
      "Step: [829] total_loss: 2.09757400 d_loss: 1.38506389, g_loss: 0.65915698, ae_loss: 0.05335312\n",
      "Step: [830] total_loss: 2.12994313 d_loss: 1.38758540, g_loss: 0.68873805, ae_loss: 0.05361979\n",
      "Step: [831] total_loss: 2.13759398 d_loss: 1.37938023, g_loss: 0.70242620, ae_loss: 0.05578760\n",
      "Step: [832] total_loss: 2.12916565 d_loss: 1.38676322, g_loss: 0.68920445, ae_loss: 0.05319792\n",
      "Step: [833] total_loss: 2.15016484 d_loss: 1.36594093, g_loss: 0.73168534, ae_loss: 0.05253857\n",
      "Step: [834] total_loss: 2.11829853 d_loss: 1.35219812, g_loss: 0.71329123, ae_loss: 0.05280918\n",
      "Step: [835] total_loss: 2.11148286 d_loss: 1.36290574, g_loss: 0.69396114, ae_loss: 0.05461603\n",
      "Step: [836] total_loss: 2.12463570 d_loss: 1.36825109, g_loss: 0.69538289, ae_loss: 0.06100165\n",
      "Step: [837] total_loss: 2.11761713 d_loss: 1.37982285, g_loss: 0.68470979, ae_loss: 0.05308433\n",
      "Step: [838] total_loss: 2.12928915 d_loss: 1.39681375, g_loss: 0.67646587, ae_loss: 0.05600971\n",
      "Step: [839] total_loss: 2.10442805 d_loss: 1.37375510, g_loss: 0.67606169, ae_loss: 0.05461130\n",
      "Step: [840] total_loss: 2.11729813 d_loss: 1.37809324, g_loss: 0.68385863, ae_loss: 0.05534618\n",
      "Step: [841] total_loss: 2.11979294 d_loss: 1.37944531, g_loss: 0.68760705, ae_loss: 0.05274064\n",
      "Step: [842] total_loss: 2.11879277 d_loss: 1.37057817, g_loss: 0.69596028, ae_loss: 0.05225436\n",
      "Step: [843] total_loss: 2.13250732 d_loss: 1.37938118, g_loss: 0.69998050, ae_loss: 0.05314571\n",
      "Step: [844] total_loss: 2.13134909 d_loss: 1.36827588, g_loss: 0.70773637, ae_loss: 0.05533670\n",
      "Step: [845] total_loss: 2.13208055 d_loss: 1.36207390, g_loss: 0.71448576, ae_loss: 0.05552071\n",
      "Step: [846] total_loss: 2.12189341 d_loss: 1.37553215, g_loss: 0.69322872, ae_loss: 0.05313263\n",
      "Step: [847] total_loss: 2.09347963 d_loss: 1.34101033, g_loss: 0.69710279, ae_loss: 0.05536666\n",
      "Step: [848] total_loss: 2.16934538 d_loss: 1.42158461, g_loss: 0.68977582, ae_loss: 0.05798503\n",
      "Step: [849] total_loss: 2.13697934 d_loss: 1.39273000, g_loss: 0.69303071, ae_loss: 0.05121859\n",
      "Step: [850] total_loss: 2.12072611 d_loss: 1.37320995, g_loss: 0.69443369, ae_loss: 0.05308240\n",
      "Step: [851] total_loss: 2.14345312 d_loss: 1.38907921, g_loss: 0.69821870, ae_loss: 0.05615511\n",
      "Step: [852] total_loss: 2.13345480 d_loss: 1.37104297, g_loss: 0.70625371, ae_loss: 0.05615811\n",
      "Step: [853] total_loss: 2.13096571 d_loss: 1.39133787, g_loss: 0.68682373, ae_loss: 0.05280428\n",
      "Step: [854] total_loss: 2.10413647 d_loss: 1.37318385, g_loss: 0.67813814, ae_loss: 0.05281457\n",
      "Step: [855] total_loss: 2.11602116 d_loss: 1.36196637, g_loss: 0.70041430, ae_loss: 0.05364052\n",
      "Step: [856] total_loss: 2.12606001 d_loss: 1.37365103, g_loss: 0.70015156, ae_loss: 0.05225750\n",
      "Step: [857] total_loss: 2.10854197 d_loss: 1.36410582, g_loss: 0.69075370, ae_loss: 0.05368239\n",
      "Step: [858] total_loss: 2.13632965 d_loss: 1.39118934, g_loss: 0.68947810, ae_loss: 0.05566212\n",
      "Step: [859] total_loss: 2.13119364 d_loss: 1.37167025, g_loss: 0.70483530, ae_loss: 0.05468804\n",
      "Step: [860] total_loss: 2.13619685 d_loss: 1.38044488, g_loss: 0.70398313, ae_loss: 0.05176889\n",
      "Step: [861] total_loss: 2.11829662 d_loss: 1.38862014, g_loss: 0.67791080, ae_loss: 0.05176556\n",
      "Step: [862] total_loss: 2.13404465 d_loss: 1.39428174, g_loss: 0.68178469, ae_loss: 0.05797814\n",
      "Step: [863] total_loss: 2.14051867 d_loss: 1.38215160, g_loss: 0.70384842, ae_loss: 0.05451872\n",
      "Step: [864] total_loss: 2.13950324 d_loss: 1.37728333, g_loss: 0.70576060, ae_loss: 0.05645931\n",
      "Step: [865] total_loss: 2.12332129 d_loss: 1.37020969, g_loss: 0.69854760, ae_loss: 0.05456405\n",
      "Step: [866] total_loss: 2.12852287 d_loss: 1.36901164, g_loss: 0.70565534, ae_loss: 0.05385576\n",
      "Step: [867] total_loss: 2.11732125 d_loss: 1.36852503, g_loss: 0.69749624, ae_loss: 0.05129987\n",
      "Step: [868] total_loss: 2.12610316 d_loss: 1.37062716, g_loss: 0.70207995, ae_loss: 0.05339614\n",
      "Step: [869] total_loss: 2.14210224 d_loss: 1.38608789, g_loss: 0.70216215, ae_loss: 0.05385226\n",
      "Step: [870] total_loss: 2.14364672 d_loss: 1.40877771, g_loss: 0.67693102, ae_loss: 0.05793788\n",
      "Step: [871] total_loss: 2.13639688 d_loss: 1.39209640, g_loss: 0.69328821, ae_loss: 0.05101215\n",
      "Step: [872] total_loss: 2.12905216 d_loss: 1.39223230, g_loss: 0.68424970, ae_loss: 0.05257015\n",
      "Step: [873] total_loss: 2.12464190 d_loss: 1.37625980, g_loss: 0.69852036, ae_loss: 0.04986177\n",
      "Step: [874] total_loss: 2.10458565 d_loss: 1.35710478, g_loss: 0.69487667, ae_loss: 0.05260425\n",
      "Step: [875] total_loss: 2.12413359 d_loss: 1.37967825, g_loss: 0.68706870, ae_loss: 0.05738681\n",
      "Step: [876] total_loss: 2.11568213 d_loss: 1.37297177, g_loss: 0.69001365, ae_loss: 0.05269664\n",
      "Step: [877] total_loss: 2.13313532 d_loss: 1.37251616, g_loss: 0.70154130, ae_loss: 0.05907781\n",
      "Step: [878] total_loss: 2.11784315 d_loss: 1.36537850, g_loss: 0.69598973, ae_loss: 0.05647498\n",
      "Step: [879] total_loss: 2.13161874 d_loss: 1.40087461, g_loss: 0.67423034, ae_loss: 0.05651380\n",
      "Step: [880] total_loss: 2.13427114 d_loss: 1.37821758, g_loss: 0.70208299, ae_loss: 0.05397066\n",
      "Step: [881] total_loss: 2.15133929 d_loss: 1.39230144, g_loss: 0.70381868, ae_loss: 0.05521921\n",
      "Step: [882] total_loss: 2.14935493 d_loss: 1.39207506, g_loss: 0.70538592, ae_loss: 0.05189411\n",
      "Step: [883] total_loss: 2.13495994 d_loss: 1.38990664, g_loss: 0.69168758, ae_loss: 0.05336567\n",
      "Step: [884] total_loss: 2.11835861 d_loss: 1.37635541, g_loss: 0.69135904, ae_loss: 0.05064433\n",
      "Step: [885] total_loss: 2.12333727 d_loss: 1.37657428, g_loss: 0.69630408, ae_loss: 0.05045889\n",
      "Step: [886] total_loss: 2.12216616 d_loss: 1.36641335, g_loss: 0.70326674, ae_loss: 0.05248599\n",
      "Step: [887] total_loss: 2.13483977 d_loss: 1.39198160, g_loss: 0.68889654, ae_loss: 0.05396166\n",
      "Step: [888] total_loss: 2.11655283 d_loss: 1.37842917, g_loss: 0.68220168, ae_loss: 0.05592206\n",
      "Step: [889] total_loss: 2.12716103 d_loss: 1.38314271, g_loss: 0.68960923, ae_loss: 0.05440916\n",
      "Step: [890] total_loss: 2.17589474 d_loss: 1.42380571, g_loss: 0.69239354, ae_loss: 0.05969561\n",
      "Step: [891] total_loss: 2.15089941 d_loss: 1.35957325, g_loss: 0.73472238, ae_loss: 0.05660370\n",
      "Step: [892] total_loss: 2.14837980 d_loss: 1.37731099, g_loss: 0.71384692, ae_loss: 0.05722184\n",
      "Step: [893] total_loss: 2.12913656 d_loss: 1.37900209, g_loss: 0.69257218, ae_loss: 0.05756220\n",
      "Step: [894] total_loss: 2.11854887 d_loss: 1.37730527, g_loss: 0.68731207, ae_loss: 0.05393153\n",
      "Step: [895] total_loss: 2.11552215 d_loss: 1.37621439, g_loss: 0.68627417, ae_loss: 0.05303364\n",
      "Step: [896] total_loss: 2.12545013 d_loss: 1.37213516, g_loss: 0.70210183, ae_loss: 0.05121331\n",
      "Step: [897] total_loss: 2.10653901 d_loss: 1.38348460, g_loss: 0.66901714, ae_loss: 0.05403731\n",
      "Step: [898] total_loss: 2.11361074 d_loss: 1.37937617, g_loss: 0.67902088, ae_loss: 0.05521358\n",
      "Step: [899] total_loss: 2.13364983 d_loss: 1.37283385, g_loss: 0.70636153, ae_loss: 0.05445435\n",
      "Step: [900] total_loss: 2.13417888 d_loss: 1.38122702, g_loss: 0.69916415, ae_loss: 0.05378773\n",
      "Step: [901] total_loss: 2.15471840 d_loss: 1.39981401, g_loss: 0.70572102, ae_loss: 0.04918353\n",
      "Step: [902] total_loss: 2.13326168 d_loss: 1.37068057, g_loss: 0.70810020, ae_loss: 0.05448088\n",
      "Step: [903] total_loss: 2.13752937 d_loss: 1.37665510, g_loss: 0.70165277, ae_loss: 0.05922151\n",
      "Step: [904] total_loss: 2.12609291 d_loss: 1.36812949, g_loss: 0.70576137, ae_loss: 0.05220197\n",
      "Step: [905] total_loss: 2.14290237 d_loss: 1.37420416, g_loss: 0.71620452, ae_loss: 0.05249358\n",
      "Step: [906] total_loss: 2.13420415 d_loss: 1.38796532, g_loss: 0.69306982, ae_loss: 0.05316901\n",
      "Step: [907] total_loss: 2.12912035 d_loss: 1.39325285, g_loss: 0.68116629, ae_loss: 0.05470134\n",
      "Step: [908] total_loss: 2.11716604 d_loss: 1.36359549, g_loss: 0.69812274, ae_loss: 0.05544788\n",
      "Step: [909] total_loss: 2.11582661 d_loss: 1.38716054, g_loss: 0.67459083, ae_loss: 0.05407542\n",
      "Step: [910] total_loss: 2.13629580 d_loss: 1.39414155, g_loss: 0.69043684, ae_loss: 0.05171751\n",
      "Step: [911] total_loss: 2.12258387 d_loss: 1.36140788, g_loss: 0.70228350, ae_loss: 0.05889237\n",
      "Step: [912] total_loss: 2.11807203 d_loss: 1.35620999, g_loss: 0.71016645, ae_loss: 0.05169557\n",
      "Step: [913] total_loss: 2.12035346 d_loss: 1.38726962, g_loss: 0.68157494, ae_loss: 0.05150887\n",
      "Step: [914] total_loss: 2.11010337 d_loss: 1.36678147, g_loss: 0.68971670, ae_loss: 0.05360522\n",
      "Step: [915] total_loss: 2.14743876 d_loss: 1.39933920, g_loss: 0.69621271, ae_loss: 0.05188689\n",
      "Step: [916] total_loss: 2.13622689 d_loss: 1.37109542, g_loss: 0.71307021, ae_loss: 0.05206132\n",
      "Step: [917] total_loss: 2.16402507 d_loss: 1.38145351, g_loss: 0.72485113, ae_loss: 0.05772047\n",
      "Step: [918] total_loss: 2.11664677 d_loss: 1.37006819, g_loss: 0.69235003, ae_loss: 0.05422840\n",
      "Step: [919] total_loss: 2.11670089 d_loss: 1.37512350, g_loss: 0.68932897, ae_loss: 0.05224849\n",
      "Step: [920] total_loss: 2.12360144 d_loss: 1.38011241, g_loss: 0.68770015, ae_loss: 0.05578870\n",
      "Step: [921] total_loss: 2.11244226 d_loss: 1.37624311, g_loss: 0.68329179, ae_loss: 0.05290730\n",
      "Step: [922] total_loss: 2.12746525 d_loss: 1.39195752, g_loss: 0.68027425, ae_loss: 0.05523356\n",
      "Step: [923] total_loss: 2.14656162 d_loss: 1.39406109, g_loss: 0.69938475, ae_loss: 0.05311579\n",
      "Step: [924] total_loss: 2.12372017 d_loss: 1.37569284, g_loss: 0.69449019, ae_loss: 0.05353703\n",
      "Step: [925] total_loss: 2.12664533 d_loss: 1.38876939, g_loss: 0.68352950, ae_loss: 0.05434641\n",
      "Step: [926] total_loss: 2.10390139 d_loss: 1.36192846, g_loss: 0.69193661, ae_loss: 0.05003639\n",
      "Step: [927] total_loss: 2.12712240 d_loss: 1.35716355, g_loss: 0.71693110, ae_loss: 0.05302773\n",
      "Step: [928] total_loss: 2.13098955 d_loss: 1.39452922, g_loss: 0.68746179, ae_loss: 0.04899842\n",
      "Step: [929] total_loss: 2.11474490 d_loss: 1.37055159, g_loss: 0.69001806, ae_loss: 0.05417524\n",
      "Step: [930] total_loss: 2.10653591 d_loss: 1.36345243, g_loss: 0.69203830, ae_loss: 0.05104513\n",
      "Step: [931] total_loss: 2.15121460 d_loss: 1.42357826, g_loss: 0.67371583, ae_loss: 0.05392034\n",
      "Step: [932] total_loss: 2.14304519 d_loss: 1.40343213, g_loss: 0.68674749, ae_loss: 0.05286565\n",
      "Step: [933] total_loss: 2.13718677 d_loss: 1.38912559, g_loss: 0.69708550, ae_loss: 0.05097574\n",
      "Step: [934] total_loss: 2.12016630 d_loss: 1.39018118, g_loss: 0.67865324, ae_loss: 0.05133198\n",
      "Step: [935] total_loss: 2.13111901 d_loss: 1.38756299, g_loss: 0.69282132, ae_loss: 0.05073468\n",
      "Step: [936] total_loss: 2.14533567 d_loss: 1.39082432, g_loss: 0.69963461, ae_loss: 0.05487667\n",
      "Step: [937] total_loss: 2.13367200 d_loss: 1.38311470, g_loss: 0.69625723, ae_loss: 0.05430005\n",
      "Step: [938] total_loss: 2.15617990 d_loss: 1.38172174, g_loss: 0.71747226, ae_loss: 0.05698583\n",
      "Step: [939] total_loss: 2.13917875 d_loss: 1.39928889, g_loss: 0.68634009, ae_loss: 0.05354986\n",
      "Step: [940] total_loss: 2.13043547 d_loss: 1.35301471, g_loss: 0.72748911, ae_loss: 0.04993157\n",
      "Step: [941] total_loss: 2.11281896 d_loss: 1.37253761, g_loss: 0.68869996, ae_loss: 0.05158136\n",
      "Step: [942] total_loss: 2.12832665 d_loss: 1.39344406, g_loss: 0.68389702, ae_loss: 0.05098563\n",
      "Step: [943] total_loss: 2.13326454 d_loss: 1.38484037, g_loss: 0.69318539, ae_loss: 0.05523870\n",
      "Step: [944] total_loss: 2.11933184 d_loss: 1.37813139, g_loss: 0.68881845, ae_loss: 0.05238193\n",
      "Step: [945] total_loss: 2.12957525 d_loss: 1.37201536, g_loss: 0.70387775, ae_loss: 0.05368219\n",
      "Step: [946] total_loss: 2.10130215 d_loss: 1.36799812, g_loss: 0.67538559, ae_loss: 0.05791827\n",
      "Step: [947] total_loss: 2.10973835 d_loss: 1.38095784, g_loss: 0.67764068, ae_loss: 0.05113995\n",
      "Step: [948] total_loss: 2.11761737 d_loss: 1.37059450, g_loss: 0.69546765, ae_loss: 0.05155516\n",
      "Step: [949] total_loss: 2.12872314 d_loss: 1.38345051, g_loss: 0.69182932, ae_loss: 0.05344334\n",
      "Step: [950] total_loss: 2.12263250 d_loss: 1.38280797, g_loss: 0.68949938, ae_loss: 0.05032509\n",
      "Step: [951] total_loss: 2.10971880 d_loss: 1.36268544, g_loss: 0.69436860, ae_loss: 0.05266490\n",
      "Step: [952] total_loss: 2.14733338 d_loss: 1.38436902, g_loss: 0.70801121, ae_loss: 0.05495321\n",
      "Step: [953] total_loss: 2.12339783 d_loss: 1.36855602, g_loss: 0.70164168, ae_loss: 0.05320004\n",
      "Step: [954] total_loss: 2.13772535 d_loss: 1.38607097, g_loss: 0.70319426, ae_loss: 0.04845996\n",
      "Step: [955] total_loss: 2.13815832 d_loss: 1.38063002, g_loss: 0.70386839, ae_loss: 0.05365990\n",
      "Step: [956] total_loss: 2.12691617 d_loss: 1.38000607, g_loss: 0.69198859, ae_loss: 0.05492151\n",
      "Step: [957] total_loss: 2.11437273 d_loss: 1.36938763, g_loss: 0.69191229, ae_loss: 0.05307274\n",
      "Step: [958] total_loss: 2.12201452 d_loss: 1.37363029, g_loss: 0.69629610, ae_loss: 0.05208800\n",
      "Step: [959] total_loss: 2.16340375 d_loss: 1.40357661, g_loss: 0.70457667, ae_loss: 0.05525037\n",
      "Step: [960] total_loss: 2.11815476 d_loss: 1.39341962, g_loss: 0.67178148, ae_loss: 0.05295357\n",
      "Step: [961] total_loss: 2.12570477 d_loss: 1.38417244, g_loss: 0.69031465, ae_loss: 0.05121754\n",
      "Step: [962] total_loss: 2.12170792 d_loss: 1.37494683, g_loss: 0.69405508, ae_loss: 0.05270601\n",
      "Step: [963] total_loss: 2.12960625 d_loss: 1.37494934, g_loss: 0.70209789, ae_loss: 0.05255906\n",
      "Step: [964] total_loss: 2.12617970 d_loss: 1.38897634, g_loss: 0.68416297, ae_loss: 0.05304032\n",
      "Step: [965] total_loss: 2.10409641 d_loss: 1.36877823, g_loss: 0.67880189, ae_loss: 0.05651642\n",
      "Step: [966] total_loss: 2.12151051 d_loss: 1.39425278, g_loss: 0.67365450, ae_loss: 0.05360332\n",
      "Step: [967] total_loss: 2.13489079 d_loss: 1.39379239, g_loss: 0.68650299, ae_loss: 0.05459539\n",
      "Step: [968] total_loss: 2.12083673 d_loss: 1.37441421, g_loss: 0.69667161, ae_loss: 0.04975091\n",
      "Step: [969] total_loss: 2.09863210 d_loss: 1.35041440, g_loss: 0.69780785, ae_loss: 0.05040983\n",
      "Step: [970] total_loss: 2.13721180 d_loss: 1.40529430, g_loss: 0.67619991, ae_loss: 0.05571745\n",
      "Step: [971] total_loss: 2.12831879 d_loss: 1.39266062, g_loss: 0.68502539, ae_loss: 0.05063266\n",
      "Step: [972] total_loss: 2.13648105 d_loss: 1.37637520, g_loss: 0.71043551, ae_loss: 0.04967031\n",
      "Step: [973] total_loss: 2.12303352 d_loss: 1.37309670, g_loss: 0.69734704, ae_loss: 0.05258967\n",
      "Step: [974] total_loss: 2.12415981 d_loss: 1.36758924, g_loss: 0.70451164, ae_loss: 0.05205879\n",
      "Step: [975] total_loss: 2.14066887 d_loss: 1.36154199, g_loss: 0.72544122, ae_loss: 0.05368570\n",
      "Step: [976] total_loss: 2.12761402 d_loss: 1.37433767, g_loss: 0.69933939, ae_loss: 0.05393699\n",
      "Step: [977] total_loss: 2.12083554 d_loss: 1.37466669, g_loss: 0.68835354, ae_loss: 0.05781532\n",
      "Step: [978] total_loss: 2.12365913 d_loss: 1.38628113, g_loss: 0.68126869, ae_loss: 0.05610919\n",
      "Step: [979] total_loss: 2.12652254 d_loss: 1.38464081, g_loss: 0.68724835, ae_loss: 0.05463353\n",
      "Step: [980] total_loss: 2.12140989 d_loss: 1.38291228, g_loss: 0.68662190, ae_loss: 0.05187555\n",
      "Step: [981] total_loss: 2.10914660 d_loss: 1.36911809, g_loss: 0.68237430, ae_loss: 0.05765425\n",
      "Step: [982] total_loss: 2.12138414 d_loss: 1.38251972, g_loss: 0.68223083, ae_loss: 0.05663344\n",
      "Step: [983] total_loss: 2.11817074 d_loss: 1.38909078, g_loss: 0.67391551, ae_loss: 0.05516449\n",
      "Step: [984] total_loss: 2.12354898 d_loss: 1.37748313, g_loss: 0.69629276, ae_loss: 0.04977298\n",
      "Step: [985] total_loss: 2.12746239 d_loss: 1.38064027, g_loss: 0.69102228, ae_loss: 0.05579976\n",
      "Step: [986] total_loss: 2.14312983 d_loss: 1.37615323, g_loss: 0.70916706, ae_loss: 0.05780962\n",
      "Step: [987] total_loss: 2.13098955 d_loss: 1.37949872, g_loss: 0.69733012, ae_loss: 0.05416058\n",
      "Step: [988] total_loss: 2.16201234 d_loss: 1.40018046, g_loss: 0.70706159, ae_loss: 0.05477038\n",
      "Step: [989] total_loss: 2.13987446 d_loss: 1.37350976, g_loss: 0.70929444, ae_loss: 0.05707018\n",
      "Step: [990] total_loss: 2.13978958 d_loss: 1.39391935, g_loss: 0.69221807, ae_loss: 0.05365219\n",
      "Step: [991] total_loss: 2.13074923 d_loss: 1.38836050, g_loss: 0.68895370, ae_loss: 0.05343504\n",
      "Step: [992] total_loss: 2.13118553 d_loss: 1.39030063, g_loss: 0.68559432, ae_loss: 0.05529073\n",
      "Step: [993] total_loss: 2.11664653 d_loss: 1.37675416, g_loss: 0.68739319, ae_loss: 0.05249912\n",
      "Step: [994] total_loss: 2.13342381 d_loss: 1.37198222, g_loss: 0.70467269, ae_loss: 0.05676893\n",
      "Step: [995] total_loss: 2.11952066 d_loss: 1.38777995, g_loss: 0.68088317, ae_loss: 0.05085772\n",
      "Step: [996] total_loss: 2.13795900 d_loss: 1.40322638, g_loss: 0.68248796, ae_loss: 0.05224473\n",
      "Step: [997] total_loss: 2.14619923 d_loss: 1.39452446, g_loss: 0.70076901, ae_loss: 0.05090581\n",
      "Step: [998] total_loss: 2.15197396 d_loss: 1.38044953, g_loss: 0.71781802, ae_loss: 0.05370635\n",
      "Step: [999] total_loss: 2.12438369 d_loss: 1.36170554, g_loss: 0.71015167, ae_loss: 0.05252651\n",
      "Step: [1000] total_loss: 2.12595463 d_loss: 1.37913060, g_loss: 0.69416142, ae_loss: 0.05266244\n",
      "Step: [1001] total_loss: 2.13342476 d_loss: 1.38132763, g_loss: 0.69727248, ae_loss: 0.05482470\n",
      "Step: [1002] total_loss: 2.13357782 d_loss: 1.38060856, g_loss: 0.70303404, ae_loss: 0.04993520\n",
      "Step: [1003] total_loss: 2.13970375 d_loss: 1.39067328, g_loss: 0.69560421, ae_loss: 0.05342637\n",
      "Step: [1004] total_loss: 2.12007499 d_loss: 1.38276935, g_loss: 0.68044031, ae_loss: 0.05686538\n",
      "Step: [1005] total_loss: 2.10777664 d_loss: 1.38140976, g_loss: 0.67666900, ae_loss: 0.04969795\n",
      "Step: [1006] total_loss: 2.11557245 d_loss: 1.37691927, g_loss: 0.68422443, ae_loss: 0.05442888\n",
      "Step: [1007] total_loss: 2.11217856 d_loss: 1.38180399, g_loss: 0.67379165, ae_loss: 0.05658288\n",
      "Step: [1008] total_loss: 2.14995742 d_loss: 1.42275178, g_loss: 0.67127591, ae_loss: 0.05592982\n",
      "Step: [1009] total_loss: 2.14823580 d_loss: 1.39803648, g_loss: 0.69419205, ae_loss: 0.05600724\n",
      "Step: [1010] total_loss: 2.12896681 d_loss: 1.37292075, g_loss: 0.70143664, ae_loss: 0.05460960\n",
      "Step: [1011] total_loss: 2.13921452 d_loss: 1.39389515, g_loss: 0.69286489, ae_loss: 0.05245448\n",
      "Step: [1012] total_loss: 2.12808394 d_loss: 1.36856580, g_loss: 0.70902395, ae_loss: 0.05049416\n",
      "Step: [1013] total_loss: 2.13250256 d_loss: 1.37371373, g_loss: 0.70490253, ae_loss: 0.05388612\n",
      "Step: [1014] total_loss: 2.11856508 d_loss: 1.38576233, g_loss: 0.67958045, ae_loss: 0.05322246\n",
      "Step: [1015] total_loss: 2.13430619 d_loss: 1.36899257, g_loss: 0.71307397, ae_loss: 0.05223970\n",
      "Step: [1016] total_loss: 2.13621211 d_loss: 1.38886213, g_loss: 0.69370043, ae_loss: 0.05364958\n",
      "Step: [1017] total_loss: 2.12323737 d_loss: 1.38784647, g_loss: 0.67925227, ae_loss: 0.05613868\n",
      "Step: [1018] total_loss: 2.12015224 d_loss: 1.38370371, g_loss: 0.68784404, ae_loss: 0.04860450\n",
      "Step: [1019] total_loss: 2.12141109 d_loss: 1.37423015, g_loss: 0.69642925, ae_loss: 0.05075169\n",
      "Step: [1020] total_loss: 2.11534071 d_loss: 1.36546910, g_loss: 0.69956899, ae_loss: 0.05030247\n",
      "Step: [1021] total_loss: 2.12995744 d_loss: 1.37811732, g_loss: 0.69869119, ae_loss: 0.05314882\n",
      "Step: [1022] total_loss: 2.10895824 d_loss: 1.37510717, g_loss: 0.68320686, ae_loss: 0.05064427\n",
      "Step: [1023] total_loss: 2.12036562 d_loss: 1.36407173, g_loss: 0.70012385, ae_loss: 0.05617007\n",
      "Step: [1024] total_loss: 2.11556458 d_loss: 1.35478854, g_loss: 0.70797122, ae_loss: 0.05280484\n",
      "Step: [1025] total_loss: 2.14272141 d_loss: 1.37383771, g_loss: 0.71355069, ae_loss: 0.05533297\n",
      "Step: [1026] total_loss: 2.09874058 d_loss: 1.35958862, g_loss: 0.69139951, ae_loss: 0.04775246\n",
      "Step: [1027] total_loss: 2.13348508 d_loss: 1.39719748, g_loss: 0.68294024, ae_loss: 0.05334738\n",
      "Step: [1028] total_loss: 2.12284374 d_loss: 1.38283646, g_loss: 0.68683565, ae_loss: 0.05317158\n",
      "Step: [1029] total_loss: 2.11017871 d_loss: 1.36368537, g_loss: 0.69482750, ae_loss: 0.05166586\n",
      "Step: [1030] total_loss: 2.11708713 d_loss: 1.37179613, g_loss: 0.69248742, ae_loss: 0.05280348\n",
      "Step: [1031] total_loss: 2.12683344 d_loss: 1.37615192, g_loss: 0.70133740, ae_loss: 0.04934420\n",
      "Step: [1032] total_loss: 2.13573551 d_loss: 1.39433908, g_loss: 0.69009256, ae_loss: 0.05130383\n",
      "Step: [1033] total_loss: 2.12730360 d_loss: 1.36366844, g_loss: 0.71042609, ae_loss: 0.05320910\n",
      "Step: [1034] total_loss: 2.11252928 d_loss: 1.35115492, g_loss: 0.70806813, ae_loss: 0.05330620\n",
      "Step: [1035] total_loss: 2.11208129 d_loss: 1.36677015, g_loss: 0.69424641, ae_loss: 0.05106476\n",
      "Step: [1036] total_loss: 2.13430953 d_loss: 1.39527476, g_loss: 0.68919444, ae_loss: 0.04984029\n",
      "Step: [1037] total_loss: 2.14685345 d_loss: 1.38650870, g_loss: 0.70657873, ae_loss: 0.05376591\n",
      "Step: [1038] total_loss: 2.14016080 d_loss: 1.37017488, g_loss: 0.71696812, ae_loss: 0.05301777\n",
      "Step: [1039] total_loss: 2.14006305 d_loss: 1.38710010, g_loss: 0.69828713, ae_loss: 0.05467584\n",
      "Step: [1040] total_loss: 2.12340021 d_loss: 1.38312638, g_loss: 0.68652016, ae_loss: 0.05375363\n",
      "Step: [1041] total_loss: 2.14105892 d_loss: 1.37021244, g_loss: 0.71652806, ae_loss: 0.05431831\n",
      "Step: [1042] total_loss: 2.13926601 d_loss: 1.40561390, g_loss: 0.68223977, ae_loss: 0.05141236\n",
      "Step: [1043] total_loss: 2.12989640 d_loss: 1.37196267, g_loss: 0.69942224, ae_loss: 0.05851154\n",
      "Step: [1044] total_loss: 2.13778496 d_loss: 1.38690555, g_loss: 0.69836926, ae_loss: 0.05251012\n",
      "Step: [1045] total_loss: 2.13641310 d_loss: 1.39427710, g_loss: 0.68709826, ae_loss: 0.05503767\n",
      "Step: [1046] total_loss: 2.13246059 d_loss: 1.35677552, g_loss: 0.72342646, ae_loss: 0.05225855\n",
      "Step: [1047] total_loss: 2.11737108 d_loss: 1.35370851, g_loss: 0.71129298, ae_loss: 0.05236945\n",
      "Step: [1048] total_loss: 2.13149452 d_loss: 1.35435224, g_loss: 0.71876192, ae_loss: 0.05838019\n",
      "Step: [1049] total_loss: 2.12804365 d_loss: 1.37279785, g_loss: 0.70256221, ae_loss: 0.05268348\n",
      "Step: [1050] total_loss: 2.10771918 d_loss: 1.38059592, g_loss: 0.67349344, ae_loss: 0.05362973\n",
      "Step: [1051] total_loss: 2.11083317 d_loss: 1.36229253, g_loss: 0.69735193, ae_loss: 0.05118874\n",
      "Step: [1052] total_loss: 2.12726688 d_loss: 1.36132336, g_loss: 0.71434379, ae_loss: 0.05159962\n",
      "Step: [1053] total_loss: 2.14181614 d_loss: 1.40236068, g_loss: 0.68386304, ae_loss: 0.05559225\n",
      "Step: [1054] total_loss: 2.12573576 d_loss: 1.35161781, g_loss: 0.72032392, ae_loss: 0.05379420\n",
      "Step: [1055] total_loss: 2.14092565 d_loss: 1.37059450, g_loss: 0.71067512, ae_loss: 0.05965599\n",
      "Step: [1056] total_loss: 2.11497426 d_loss: 1.36662734, g_loss: 0.69722027, ae_loss: 0.05112671\n",
      "Step: [1057] total_loss: 2.13779831 d_loss: 1.39536774, g_loss: 0.68886828, ae_loss: 0.05356219\n",
      "Step: [1058] total_loss: 2.10773563 d_loss: 1.36812055, g_loss: 0.68755293, ae_loss: 0.05206201\n",
      "Step: [1059] total_loss: 2.11662626 d_loss: 1.36491370, g_loss: 0.69948697, ae_loss: 0.05222572\n",
      "Step: [1060] total_loss: 2.12874269 d_loss: 1.38150024, g_loss: 0.69404668, ae_loss: 0.05319569\n",
      "Step: [1061] total_loss: 2.13571429 d_loss: 1.39195704, g_loss: 0.68940026, ae_loss: 0.05435700\n",
      "Step: [1062] total_loss: 2.14191198 d_loss: 1.39361620, g_loss: 0.69339848, ae_loss: 0.05489714\n",
      "Step: [1063] total_loss: 2.11320305 d_loss: 1.35853291, g_loss: 0.69993639, ae_loss: 0.05473390\n",
      "Step: [1064] total_loss: 2.13537788 d_loss: 1.37514567, g_loss: 0.70737660, ae_loss: 0.05285562\n",
      "Step: [1065] total_loss: 2.13910675 d_loss: 1.39045203, g_loss: 0.69543624, ae_loss: 0.05321866\n",
      "Step: [1066] total_loss: 2.11427999 d_loss: 1.36821961, g_loss: 0.69336879, ae_loss: 0.05269160\n",
      "Step: [1067] total_loss: 2.10183859 d_loss: 1.37058878, g_loss: 0.67752707, ae_loss: 0.05372281\n",
      "Step: [1068] total_loss: 2.13257575 d_loss: 1.37477434, g_loss: 0.70789349, ae_loss: 0.04990789\n",
      "Step: [1069] total_loss: 2.15114355 d_loss: 1.39410818, g_loss: 0.70374978, ae_loss: 0.05328569\n",
      "Step: [1070] total_loss: 2.11391497 d_loss: 1.36385286, g_loss: 0.69217181, ae_loss: 0.05789025\n",
      "Step: [1071] total_loss: 2.14140558 d_loss: 1.40057671, g_loss: 0.69288355, ae_loss: 0.04794526\n",
      "Step: [1072] total_loss: 2.10689950 d_loss: 1.36742365, g_loss: 0.68826294, ae_loss: 0.05121290\n",
      "Step: [1073] total_loss: 2.12601042 d_loss: 1.37580943, g_loss: 0.70156503, ae_loss: 0.04863601\n",
      "Step: [1074] total_loss: 2.12676382 d_loss: 1.37956333, g_loss: 0.69703901, ae_loss: 0.05016148\n",
      "Step: [1075] total_loss: 2.13332486 d_loss: 1.37049246, g_loss: 0.71440160, ae_loss: 0.04843085\n",
      "Step: [1076] total_loss: 2.13200426 d_loss: 1.37236857, g_loss: 0.70921397, ae_loss: 0.05042168\n",
      "Step: [1077] total_loss: 2.12788558 d_loss: 1.36398387, g_loss: 0.71114045, ae_loss: 0.05276126\n",
      "Step: [1078] total_loss: 2.12267590 d_loss: 1.38119137, g_loss: 0.68420255, ae_loss: 0.05728206\n",
      "Step: [1079] total_loss: 2.14442635 d_loss: 1.39363432, g_loss: 0.69743550, ae_loss: 0.05335660\n",
      "Step: [1080] total_loss: 2.13525462 d_loss: 1.39747345, g_loss: 0.68690199, ae_loss: 0.05087912\n",
      "Step: [1081] total_loss: 2.14276266 d_loss: 1.37438285, g_loss: 0.71560442, ae_loss: 0.05277527\n",
      "Step: [1082] total_loss: 2.11651373 d_loss: 1.37694371, g_loss: 0.68704545, ae_loss: 0.05252460\n",
      "Step: [1083] total_loss: 2.09079933 d_loss: 1.34183729, g_loss: 0.69643414, ae_loss: 0.05252807\n",
      "Step: [1084] total_loss: 2.11310387 d_loss: 1.37848341, g_loss: 0.68437874, ae_loss: 0.05024175\n",
      "Step: [1085] total_loss: 2.15134501 d_loss: 1.39829171, g_loss: 0.69929481, ae_loss: 0.05375848\n",
      "Step: [1086] total_loss: 2.12277985 d_loss: 1.37547636, g_loss: 0.69131821, ae_loss: 0.05598520\n",
      "Step: [1087] total_loss: 2.13768101 d_loss: 1.39078033, g_loss: 0.69207382, ae_loss: 0.05482692\n",
      "Step: [1088] total_loss: 2.13327599 d_loss: 1.36735964, g_loss: 0.71327436, ae_loss: 0.05264187\n",
      "Step: [1089] total_loss: 2.14516687 d_loss: 1.37649083, g_loss: 0.70922333, ae_loss: 0.05945281\n",
      "Step: [1090] total_loss: 2.13635540 d_loss: 1.39548039, g_loss: 0.68944788, ae_loss: 0.05142703\n",
      "Step: [1091] total_loss: 2.13103700 d_loss: 1.38821244, g_loss: 0.68993223, ae_loss: 0.05289233\n",
      "Step: [1092] total_loss: 2.13087940 d_loss: 1.39666271, g_loss: 0.68196285, ae_loss: 0.05225373\n",
      "Step: [1093] total_loss: 2.13128304 d_loss: 1.38448060, g_loss: 0.69167101, ae_loss: 0.05513147\n",
      "Step: [1094] total_loss: 2.11589241 d_loss: 1.37055600, g_loss: 0.68938518, ae_loss: 0.05595119\n",
      "Step: [1095] total_loss: 2.11760616 d_loss: 1.37366164, g_loss: 0.68782270, ae_loss: 0.05612177\n",
      "Step: [1096] total_loss: 2.13594389 d_loss: 1.37773418, g_loss: 0.70733166, ae_loss: 0.05087795\n",
      "Step: [1097] total_loss: 2.11928606 d_loss: 1.38965297, g_loss: 0.67690122, ae_loss: 0.05273183\n",
      "Step: [1098] total_loss: 2.10035086 d_loss: 1.37599897, g_loss: 0.67450362, ae_loss: 0.04984831\n",
      "Step: [1099] total_loss: 2.12090874 d_loss: 1.37326229, g_loss: 0.69782341, ae_loss: 0.04982318\n",
      "Step: [1100] total_loss: 2.11515951 d_loss: 1.38210976, g_loss: 0.67966300, ae_loss: 0.05338669\n",
      "Step: [1101] total_loss: 2.13157725 d_loss: 1.37552428, g_loss: 0.70095748, ae_loss: 0.05509547\n",
      "Step: [1102] total_loss: 2.14714241 d_loss: 1.38290930, g_loss: 0.71004760, ae_loss: 0.05418563\n",
      "Step: [1103] total_loss: 2.14545059 d_loss: 1.38926005, g_loss: 0.70065355, ae_loss: 0.05553686\n",
      "Step: [1104] total_loss: 2.12850332 d_loss: 1.38677049, g_loss: 0.68697679, ae_loss: 0.05475598\n",
      "Step: [1105] total_loss: 2.13256788 d_loss: 1.37840688, g_loss: 0.69770688, ae_loss: 0.05645415\n",
      "Step: [1106] total_loss: 2.13976288 d_loss: 1.37145138, g_loss: 0.71723998, ae_loss: 0.05107150\n",
      "Step: [1107] total_loss: 2.11951733 d_loss: 1.37354827, g_loss: 0.69370770, ae_loss: 0.05226145\n",
      "Step: [1108] total_loss: 2.12919044 d_loss: 1.38841748, g_loss: 0.68851483, ae_loss: 0.05225826\n",
      "Step: [1109] total_loss: 2.11868238 d_loss: 1.35345280, g_loss: 0.70922291, ae_loss: 0.05600671\n",
      "Step: [1110] total_loss: 2.11136246 d_loss: 1.36938882, g_loss: 0.69346201, ae_loss: 0.04851168\n",
      "Step: [1111] total_loss: 2.12151241 d_loss: 1.37560272, g_loss: 0.69611526, ae_loss: 0.04979461\n",
      "Step: [1112] total_loss: 2.12468576 d_loss: 1.37456846, g_loss: 0.69283676, ae_loss: 0.05728068\n",
      "Step: [1113] total_loss: 2.13837719 d_loss: 1.36230731, g_loss: 0.72510433, ae_loss: 0.05096570\n",
      "Step: [1114] total_loss: 2.12145424 d_loss: 1.38517725, g_loss: 0.68499345, ae_loss: 0.05128363\n",
      "Step: [1115] total_loss: 2.12510777 d_loss: 1.37461495, g_loss: 0.69819808, ae_loss: 0.05229462\n",
      "Step: [1116] total_loss: 2.12452459 d_loss: 1.38260174, g_loss: 0.68608922, ae_loss: 0.05583375\n",
      "Step: [1117] total_loss: 2.12890577 d_loss: 1.38226843, g_loss: 0.69208419, ae_loss: 0.05455321\n",
      "Step: [1118] total_loss: 2.09304667 d_loss: 1.36697805, g_loss: 0.67545319, ae_loss: 0.05061536\n",
      "Step: [1119] total_loss: 2.12741709 d_loss: 1.37641037, g_loss: 0.69749182, ae_loss: 0.05351488\n",
      "Step: [1120] total_loss: 2.12302399 d_loss: 1.37463617, g_loss: 0.69349277, ae_loss: 0.05489508\n",
      "Step: [1121] total_loss: 2.11699319 d_loss: 1.37288940, g_loss: 0.68738365, ae_loss: 0.05672010\n",
      "Step: [1122] total_loss: 2.12942719 d_loss: 1.38005662, g_loss: 0.69459027, ae_loss: 0.05478039\n",
      "Step: [1123] total_loss: 2.14729285 d_loss: 1.38904965, g_loss: 0.70636332, ae_loss: 0.05187993\n",
      "Step: [1124] total_loss: 2.15829277 d_loss: 1.41263294, g_loss: 0.68964493, ae_loss: 0.05601474\n",
      "Step: [1125] total_loss: 2.14200354 d_loss: 1.39917409, g_loss: 0.69466543, ae_loss: 0.04816414\n",
      "Step: [1126] total_loss: 2.13081622 d_loss: 1.38159680, g_loss: 0.69470686, ae_loss: 0.05451252\n",
      "Step: [1127] total_loss: 2.13127255 d_loss: 1.36420965, g_loss: 0.71267003, ae_loss: 0.05439281\n",
      "Step: [1128] total_loss: 2.11603975 d_loss: 1.37395489, g_loss: 0.68681657, ae_loss: 0.05526835\n",
      "Step: [1129] total_loss: 2.11008096 d_loss: 1.35990810, g_loss: 0.69649649, ae_loss: 0.05367631\n",
      "Step: [1130] total_loss: 2.11238909 d_loss: 1.37221909, g_loss: 0.68763995, ae_loss: 0.05252998\n",
      "Step: [1131] total_loss: 2.13362837 d_loss: 1.38704729, g_loss: 0.69235861, ae_loss: 0.05422257\n",
      "Step: [1132] total_loss: 2.12638807 d_loss: 1.36975884, g_loss: 0.70255423, ae_loss: 0.05407514\n",
      "Step: [1133] total_loss: 2.11389184 d_loss: 1.38284588, g_loss: 0.67794722, ae_loss: 0.05309863\n",
      "Step: [1134] total_loss: 2.15119219 d_loss: 1.40393639, g_loss: 0.69533658, ae_loss: 0.05191910\n",
      "Step: [1135] total_loss: 2.14262056 d_loss: 1.38310218, g_loss: 0.70575279, ae_loss: 0.05376566\n",
      "Step: [1136] total_loss: 2.12241888 d_loss: 1.38862586, g_loss: 0.68233371, ae_loss: 0.05145936\n",
      "Step: [1137] total_loss: 2.13755035 d_loss: 1.38655925, g_loss: 0.69701546, ae_loss: 0.05397572\n",
      "Step: [1138] total_loss: 2.13432074 d_loss: 1.37861025, g_loss: 0.70600176, ae_loss: 0.04970881\n",
      "Step: [1139] total_loss: 2.11787367 d_loss: 1.38424063, g_loss: 0.68040538, ae_loss: 0.05322760\n",
      "Step: [1140] total_loss: 2.12781143 d_loss: 1.39198256, g_loss: 0.68569195, ae_loss: 0.05013699\n",
      "Step: [1141] total_loss: 2.12807131 d_loss: 1.38489747, g_loss: 0.68750453, ae_loss: 0.05566924\n",
      "Step: [1142] total_loss: 2.10474968 d_loss: 1.36105776, g_loss: 0.69186187, ae_loss: 0.05182999\n",
      "Step: [1143] total_loss: 2.12975168 d_loss: 1.39245057, g_loss: 0.68754905, ae_loss: 0.04975195\n",
      "Step: [1144] total_loss: 2.12731743 d_loss: 1.36169398, g_loss: 0.70992601, ae_loss: 0.05569737\n",
      "Step: [1145] total_loss: 2.12925291 d_loss: 1.39020348, g_loss: 0.68616641, ae_loss: 0.05288288\n",
      "Step: [1146] total_loss: 2.11834002 d_loss: 1.36626101, g_loss: 0.69911706, ae_loss: 0.05296183\n",
      "Step: [1147] total_loss: 2.15072727 d_loss: 1.40140951, g_loss: 0.69072223, ae_loss: 0.05859564\n",
      "Step: [1148] total_loss: 2.13812494 d_loss: 1.38283956, g_loss: 0.70596516, ae_loss: 0.04932030\n",
      "Step: [1149] total_loss: 2.12656164 d_loss: 1.37883615, g_loss: 0.69715774, ae_loss: 0.05056759\n",
      "Step: [1150] total_loss: 2.12828326 d_loss: 1.37155473, g_loss: 0.70625299, ae_loss: 0.05047563\n",
      "Step: [1151] total_loss: 2.13456011 d_loss: 1.38696837, g_loss: 0.69470346, ae_loss: 0.05288825\n",
      "Step: [1152] total_loss: 2.14099526 d_loss: 1.38786399, g_loss: 0.69560879, ae_loss: 0.05752248\n",
      "Step: [1153] total_loss: 2.13785768 d_loss: 1.37811279, g_loss: 0.70110935, ae_loss: 0.05863562\n",
      "Step: [1154] total_loss: 2.09287548 d_loss: 1.35217535, g_loss: 0.68979657, ae_loss: 0.05090367\n",
      "Step: [1155] total_loss: 2.13245082 d_loss: 1.40670300, g_loss: 0.67257303, ae_loss: 0.05317475\n",
      "Step: [1156] total_loss: 2.12304592 d_loss: 1.38707697, g_loss: 0.68139100, ae_loss: 0.05457797\n",
      "Step: [1157] total_loss: 2.11560035 d_loss: 1.37642360, g_loss: 0.68609941, ae_loss: 0.05307731\n",
      "Step: [1158] total_loss: 2.10689020 d_loss: 1.36847389, g_loss: 0.68531799, ae_loss: 0.05309846\n",
      "Step: [1159] total_loss: 2.12158275 d_loss: 1.38250196, g_loss: 0.68611813, ae_loss: 0.05296264\n",
      "Step: [1160] total_loss: 2.11373711 d_loss: 1.37815690, g_loss: 0.67878348, ae_loss: 0.05679680\n",
      "Step: [1161] total_loss: 2.12741232 d_loss: 1.39584315, g_loss: 0.67670894, ae_loss: 0.05486033\n",
      "Step: [1162] total_loss: 2.11962318 d_loss: 1.36501873, g_loss: 0.70059776, ae_loss: 0.05400659\n",
      "Step: [1163] total_loss: 2.13372183 d_loss: 1.40101743, g_loss: 0.68228769, ae_loss: 0.05041686\n",
      "Step: [1164] total_loss: 2.12426901 d_loss: 1.38789105, g_loss: 0.68215632, ae_loss: 0.05422153\n",
      "Step: [1165] total_loss: 2.15054131 d_loss: 1.38132596, g_loss: 0.71197855, ae_loss: 0.05723695\n",
      "Step: [1166] total_loss: 2.11914015 d_loss: 1.35998535, g_loss: 0.71359694, ae_loss: 0.04555790\n",
      "Step: [1167] total_loss: 2.13116074 d_loss: 1.36315966, g_loss: 0.71781230, ae_loss: 0.05018884\n",
      "Step: [1168] total_loss: 2.11970949 d_loss: 1.36077154, g_loss: 0.70278931, ae_loss: 0.05614859\n",
      "Step: [1169] total_loss: 2.15071201 d_loss: 1.39126933, g_loss: 0.70180643, ae_loss: 0.05763630\n",
      "Step: [1170] total_loss: 2.13041735 d_loss: 1.41107917, g_loss: 0.66054541, ae_loss: 0.05879272\n",
      "Step: [1171] total_loss: 2.11259007 d_loss: 1.36251640, g_loss: 0.69814938, ae_loss: 0.05192427\n",
      "Step: [1172] total_loss: 2.11408615 d_loss: 1.36705709, g_loss: 0.69505835, ae_loss: 0.05197086\n",
      "Step: [1173] total_loss: 2.12893867 d_loss: 1.37649632, g_loss: 0.69976926, ae_loss: 0.05267325\n",
      "Step: [1174] total_loss: 2.12474179 d_loss: 1.37052739, g_loss: 0.70034260, ae_loss: 0.05387177\n",
      "Step: [1175] total_loss: 2.14121127 d_loss: 1.36511827, g_loss: 0.71972650, ae_loss: 0.05636638\n",
      "Step: [1176] total_loss: 2.14054656 d_loss: 1.39451492, g_loss: 0.69416809, ae_loss: 0.05186353\n",
      "Step: [1177] total_loss: 2.13436842 d_loss: 1.39011395, g_loss: 0.69103193, ae_loss: 0.05322256\n",
      "Step: [1178] total_loss: 2.12775850 d_loss: 1.37993693, g_loss: 0.69586217, ae_loss: 0.05195926\n",
      "Step: [1179] total_loss: 2.11864042 d_loss: 1.38092446, g_loss: 0.68405968, ae_loss: 0.05365622\n",
      "Step: [1180] total_loss: 2.11884022 d_loss: 1.37558806, g_loss: 0.68857008, ae_loss: 0.05468216\n",
      "Step: [1181] total_loss: 2.11441565 d_loss: 1.37599325, g_loss: 0.68424940, ae_loss: 0.05417296\n",
      "Step: [1182] total_loss: 2.11393237 d_loss: 1.35310829, g_loss: 0.71039134, ae_loss: 0.05043278\n",
      "Step: [1183] total_loss: 2.13769102 d_loss: 1.40063131, g_loss: 0.68330508, ae_loss: 0.05375458\n",
      "Step: [1184] total_loss: 2.14128137 d_loss: 1.37855053, g_loss: 0.70755517, ae_loss: 0.05517566\n",
      "Step: [1185] total_loss: 2.13676786 d_loss: 1.39203143, g_loss: 0.69500816, ae_loss: 0.04972814\n",
      "Step: [1186] total_loss: 2.14101553 d_loss: 1.38490951, g_loss: 0.69780391, ae_loss: 0.05830211\n",
      "Step: [1187] total_loss: 2.14779878 d_loss: 1.40885842, g_loss: 0.68985701, ae_loss: 0.04908338\n",
      "Step: [1188] total_loss: 2.11724973 d_loss: 1.35621405, g_loss: 0.70853555, ae_loss: 0.05250009\n",
      "Step: [1189] total_loss: 2.11423111 d_loss: 1.37313402, g_loss: 0.68790001, ae_loss: 0.05319706\n",
      "Step: [1190] total_loss: 2.15774345 d_loss: 1.39590764, g_loss: 0.70790029, ae_loss: 0.05393567\n",
      "Step: [1191] total_loss: 2.14008379 d_loss: 1.36994219, g_loss: 0.71658564, ae_loss: 0.05355583\n",
      "Step: [1192] total_loss: 2.13189745 d_loss: 1.38081777, g_loss: 0.70320451, ae_loss: 0.04787499\n",
      "Step: [1193] total_loss: 2.12097764 d_loss: 1.38045907, g_loss: 0.69151974, ae_loss: 0.04899880\n",
      "Step: [1194] total_loss: 2.13285136 d_loss: 1.40317321, g_loss: 0.67888427, ae_loss: 0.05079383\n",
      "Step: [1195] total_loss: 2.11946797 d_loss: 1.38491488, g_loss: 0.68080038, ae_loss: 0.05375271\n",
      "Step: [1196] total_loss: 2.11317039 d_loss: 1.36003709, g_loss: 0.69647932, ae_loss: 0.05665394\n",
      "Step: [1197] total_loss: 2.13474131 d_loss: 1.39478636, g_loss: 0.68942142, ae_loss: 0.05053350\n",
      "Step: [1198] total_loss: 2.14770031 d_loss: 1.40205169, g_loss: 0.69128150, ae_loss: 0.05436707\n",
      "Step: [1199] total_loss: 2.13028955 d_loss: 1.37705827, g_loss: 0.70198703, ae_loss: 0.05124422\n",
      "Step: [1200] total_loss: 2.16276908 d_loss: 1.41006041, g_loss: 0.69686115, ae_loss: 0.05584757\n",
      "Step: [1201] total_loss: 2.13619566 d_loss: 1.38499427, g_loss: 0.69762409, ae_loss: 0.05357748\n",
      "Step: [1202] total_loss: 2.15361452 d_loss: 1.38586986, g_loss: 0.71019328, ae_loss: 0.05755135\n",
      "Step: [1203] total_loss: 2.11110592 d_loss: 1.38196635, g_loss: 0.67379928, ae_loss: 0.05534045\n",
      "Step: [1204] total_loss: 2.11635113 d_loss: 1.37060416, g_loss: 0.69373274, ae_loss: 0.05201432\n",
      "Step: [1205] total_loss: 2.12301254 d_loss: 1.39599609, g_loss: 0.67463052, ae_loss: 0.05238582\n",
      "Step: [1206] total_loss: 2.12410831 d_loss: 1.35793114, g_loss: 0.70952237, ae_loss: 0.05665470\n",
      "Step: [1207] total_loss: 2.11763048 d_loss: 1.37915111, g_loss: 0.68352616, ae_loss: 0.05495335\n",
      "Step: [1208] total_loss: 2.11619139 d_loss: 1.39353859, g_loss: 0.67053461, ae_loss: 0.05211826\n",
      "Step: [1209] total_loss: 2.12890482 d_loss: 1.38886011, g_loss: 0.68526077, ae_loss: 0.05478380\n",
      "Step: [1210] total_loss: 2.12843800 d_loss: 1.38318741, g_loss: 0.69153059, ae_loss: 0.05371992\n",
      "Step: [1211] total_loss: 2.13737440 d_loss: 1.38145745, g_loss: 0.70391285, ae_loss: 0.05200427\n",
      "Step: [1212] total_loss: 2.13674736 d_loss: 1.39264250, g_loss: 0.69424319, ae_loss: 0.04986170\n",
      "Step: [1213] total_loss: 2.14139986 d_loss: 1.38858795, g_loss: 0.70155811, ae_loss: 0.05125378\n",
      "Step: [1214] total_loss: 2.11783552 d_loss: 1.35719323, g_loss: 0.70802116, ae_loss: 0.05262098\n",
      "Step: [1215] total_loss: 2.12460518 d_loss: 1.39342487, g_loss: 0.67843306, ae_loss: 0.05274742\n",
      "Step: [1216] total_loss: 2.12842250 d_loss: 1.36891794, g_loss: 0.70524871, ae_loss: 0.05425582\n",
      "Step: [1217] total_loss: 2.13185120 d_loss: 1.39052010, g_loss: 0.68932617, ae_loss: 0.05200486\n",
      "Step: [1218] total_loss: 2.12224483 d_loss: 1.36799705, g_loss: 0.69952738, ae_loss: 0.05472057\n",
      "Step: [1219] total_loss: 2.13960147 d_loss: 1.36584449, g_loss: 0.71525395, ae_loss: 0.05850308\n",
      "Step: [1220] total_loss: 2.11100197 d_loss: 1.36655688, g_loss: 0.69404173, ae_loss: 0.05040338\n",
      "Step: [1221] total_loss: 2.10877085 d_loss: 1.35279083, g_loss: 0.70977819, ae_loss: 0.04620187\n",
      "Step: [1222] total_loss: 2.12153101 d_loss: 1.38395596, g_loss: 0.68667907, ae_loss: 0.05089610\n",
      "Step: [1223] total_loss: 2.11541128 d_loss: 1.36029959, g_loss: 0.70602363, ae_loss: 0.04908797\n",
      "Step: [1224] total_loss: 2.12759233 d_loss: 1.39027762, g_loss: 0.68873805, ae_loss: 0.04857669\n",
      "Step: [1225] total_loss: 2.12810421 d_loss: 1.37216854, g_loss: 0.70240259, ae_loss: 0.05353300\n",
      "Step: [1226] total_loss: 2.09269810 d_loss: 1.36919940, g_loss: 0.67277265, ae_loss: 0.05072614\n",
      "Step: [1227] total_loss: 2.11627412 d_loss: 1.37601626, g_loss: 0.68841237, ae_loss: 0.05184558\n",
      "Step: [1228] total_loss: 2.13626146 d_loss: 1.38875556, g_loss: 0.69646710, ae_loss: 0.05103885\n",
      "Step: [1229] total_loss: 2.13740921 d_loss: 1.40025949, g_loss: 0.68259597, ae_loss: 0.05455361\n",
      "Step: [1230] total_loss: 2.14390588 d_loss: 1.39312768, g_loss: 0.70019996, ae_loss: 0.05057828\n",
      "Step: [1231] total_loss: 2.12116480 d_loss: 1.35897684, g_loss: 0.70599389, ae_loss: 0.05619418\n",
      "Step: [1232] total_loss: 2.12630296 d_loss: 1.38031101, g_loss: 0.69520468, ae_loss: 0.05078735\n",
      "Step: [1233] total_loss: 2.14928865 d_loss: 1.41727304, g_loss: 0.67793655, ae_loss: 0.05407906\n",
      "Step: [1234] total_loss: 2.15367532 d_loss: 1.39873743, g_loss: 0.70112199, ae_loss: 0.05381584\n",
      "Step: [1235] total_loss: 2.13328505 d_loss: 1.38517118, g_loss: 0.69014132, ae_loss: 0.05797272\n",
      "Step: [1236] total_loss: 2.14165401 d_loss: 1.39910841, g_loss: 0.68971729, ae_loss: 0.05282832\n",
      "Step: [1237] total_loss: 2.12937212 d_loss: 1.39479876, g_loss: 0.68219048, ae_loss: 0.05238289\n",
      "Step: [1238] total_loss: 2.11457443 d_loss: 1.37100148, g_loss: 0.69016707, ae_loss: 0.05340596\n",
      "Step: [1239] total_loss: 2.12365794 d_loss: 1.37893343, g_loss: 0.69092059, ae_loss: 0.05380387\n",
      "Step: [1240] total_loss: 2.12301683 d_loss: 1.39000940, g_loss: 0.68211007, ae_loss: 0.05089749\n",
      "Step: [1241] total_loss: 2.11385775 d_loss: 1.37857950, g_loss: 0.68261766, ae_loss: 0.05266043\n",
      "Step: [1242] total_loss: 2.13449836 d_loss: 1.35729742, g_loss: 0.72158915, ae_loss: 0.05561187\n",
      "Step: [1243] total_loss: 2.12375259 d_loss: 1.37971818, g_loss: 0.68985438, ae_loss: 0.05418008\n",
      "Step: [1244] total_loss: 2.13220716 d_loss: 1.39258444, g_loss: 0.68617046, ae_loss: 0.05345224\n",
      "Step: [1245] total_loss: 2.13787770 d_loss: 1.39359951, g_loss: 0.69184351, ae_loss: 0.05243470\n",
      "Step: [1246] total_loss: 2.12788343 d_loss: 1.38128793, g_loss: 0.69212663, ae_loss: 0.05446894\n",
      "Step: [1247] total_loss: 2.12380838 d_loss: 1.36699128, g_loss: 0.70136130, ae_loss: 0.05545590\n",
      "Step: [1248] total_loss: 2.13768673 d_loss: 1.39546919, g_loss: 0.68807095, ae_loss: 0.05414660\n",
      "Step: [1249] total_loss: 2.13538575 d_loss: 1.37258291, g_loss: 0.70454824, ae_loss: 0.05825463\n",
      "Step: [1250] total_loss: 2.14949799 d_loss: 1.40357637, g_loss: 0.69033366, ae_loss: 0.05558796\n",
      "Step: [1251] total_loss: 2.12033558 d_loss: 1.38667309, g_loss: 0.67932165, ae_loss: 0.05434073\n",
      "Step: [1252] total_loss: 2.12810969 d_loss: 1.39213526, g_loss: 0.68639368, ae_loss: 0.04958086\n",
      "Step: [1253] total_loss: 2.12934971 d_loss: 1.37792957, g_loss: 0.70013750, ae_loss: 0.05128250\n",
      "Step: [1254] total_loss: 2.15171409 d_loss: 1.35839725, g_loss: 0.74060106, ae_loss: 0.05271573\n",
      "Step: [1255] total_loss: 2.12052202 d_loss: 1.37538326, g_loss: 0.69318736, ae_loss: 0.05195141\n",
      "Step: [1256] total_loss: 2.10763216 d_loss: 1.37310696, g_loss: 0.68443727, ae_loss: 0.05008789\n",
      "Step: [1257] total_loss: 2.11098695 d_loss: 1.36995399, g_loss: 0.69212693, ae_loss: 0.04890606\n",
      "Step: [1258] total_loss: 2.12835693 d_loss: 1.37373018, g_loss: 0.70191222, ae_loss: 0.05271463\n",
      "Step: [1259] total_loss: 2.13960266 d_loss: 1.39567828, g_loss: 0.69069314, ae_loss: 0.05323122\n",
      "Step: [1260] total_loss: 2.11849141 d_loss: 1.37375379, g_loss: 0.69555551, ae_loss: 0.04918204\n",
      "Step: [1261] total_loss: 2.12684107 d_loss: 1.36546469, g_loss: 0.70512044, ae_loss: 0.05625607\n",
      "Step: [1262] total_loss: 2.13175869 d_loss: 1.38274527, g_loss: 0.69513845, ae_loss: 0.05387508\n",
      "Step: [1263] total_loss: 2.12094498 d_loss: 1.36918759, g_loss: 0.70002890, ae_loss: 0.05172866\n",
      "Step: [1264] total_loss: 2.11437249 d_loss: 1.38383937, g_loss: 0.67803150, ae_loss: 0.05250158\n",
      "Step: [1265] total_loss: 2.12151027 d_loss: 1.38419950, g_loss: 0.68639630, ae_loss: 0.05091458\n",
      "Step: [1266] total_loss: 2.10905123 d_loss: 1.36667919, g_loss: 0.68886387, ae_loss: 0.05350799\n",
      "Step: [1267] total_loss: 2.10057807 d_loss: 1.36361980, g_loss: 0.68771851, ae_loss: 0.04923981\n",
      "Step: [1268] total_loss: 2.12087822 d_loss: 1.36157227, g_loss: 0.70930743, ae_loss: 0.04999854\n",
      "Step: [1269] total_loss: 2.10731077 d_loss: 1.38372254, g_loss: 0.67432123, ae_loss: 0.04926701\n",
      "Step: [1270] total_loss: 2.11898160 d_loss: 1.36865735, g_loss: 0.69503552, ae_loss: 0.05528884\n",
      "Step: [1271] total_loss: 2.14190197 d_loss: 1.40052450, g_loss: 0.69010258, ae_loss: 0.05127480\n",
      "Step: [1272] total_loss: 2.11849856 d_loss: 1.37044144, g_loss: 0.69819891, ae_loss: 0.04985820\n",
      "Step: [1273] total_loss: 2.13135004 d_loss: 1.38156414, g_loss: 0.70135784, ae_loss: 0.04842810\n",
      "Step: [1274] total_loss: 2.13633728 d_loss: 1.38780785, g_loss: 0.69772470, ae_loss: 0.05080488\n",
      "Step: [1275] total_loss: 2.15808678 d_loss: 1.39769304, g_loss: 0.70806903, ae_loss: 0.05232476\n",
      "Step: [1276] total_loss: 2.12988567 d_loss: 1.38067794, g_loss: 0.69744182, ae_loss: 0.05176606\n",
      "Step: [1277] total_loss: 2.12927413 d_loss: 1.38641286, g_loss: 0.68939608, ae_loss: 0.05346529\n",
      "Step: [1278] total_loss: 2.11927128 d_loss: 1.37027061, g_loss: 0.69178373, ae_loss: 0.05721688\n",
      "Step: [1279] total_loss: 2.14721131 d_loss: 1.40532243, g_loss: 0.68824059, ae_loss: 0.05364827\n",
      "Step: [1280] total_loss: 2.12907648 d_loss: 1.38765860, g_loss: 0.69232327, ae_loss: 0.04909457\n",
      "Step: [1281] total_loss: 2.11344051 d_loss: 1.36734271, g_loss: 0.69566262, ae_loss: 0.05043531\n",
      "Step: [1282] total_loss: 2.11322832 d_loss: 1.37555432, g_loss: 0.68627113, ae_loss: 0.05140287\n",
      "Step: [1283] total_loss: 2.13133764 d_loss: 1.39498174, g_loss: 0.68238878, ae_loss: 0.05396725\n",
      "Step: [1284] total_loss: 2.11809635 d_loss: 1.36035204, g_loss: 0.70598006, ae_loss: 0.05176438\n",
      "Step: [1285] total_loss: 2.10251617 d_loss: 1.37308121, g_loss: 0.67799801, ae_loss: 0.05143689\n",
      "Step: [1286] total_loss: 2.11662054 d_loss: 1.36590719, g_loss: 0.69910687, ae_loss: 0.05160639\n",
      "Step: [1287] total_loss: 2.09986830 d_loss: 1.35781741, g_loss: 0.68872738, ae_loss: 0.05332349\n",
      "Step: [1288] total_loss: 2.11801481 d_loss: 1.39324713, g_loss: 0.67425776, ae_loss: 0.05050985\n",
      "Step: [1289] total_loss: 2.11710954 d_loss: 1.36642516, g_loss: 0.69618535, ae_loss: 0.05449897\n",
      "Step: [1290] total_loss: 2.12792110 d_loss: 1.36392355, g_loss: 0.70659918, ae_loss: 0.05739836\n",
      "Step: [1291] total_loss: 2.13167262 d_loss: 1.38050747, g_loss: 0.69551438, ae_loss: 0.05565077\n",
      "Step: [1292] total_loss: 2.11595726 d_loss: 1.35666037, g_loss: 0.71245432, ae_loss: 0.04684275\n",
      "Step: [1293] total_loss: 2.14890718 d_loss: 1.40372884, g_loss: 0.68916464, ae_loss: 0.05601387\n",
      "Step: [1294] total_loss: 2.12864518 d_loss: 1.39177859, g_loss: 0.68485469, ae_loss: 0.05201181\n",
      "Step: [1295] total_loss: 2.12374210 d_loss: 1.39639187, g_loss: 0.67909020, ae_loss: 0.04826000\n",
      "Step: [1296] total_loss: 2.13474894 d_loss: 1.37796068, g_loss: 0.70408505, ae_loss: 0.05270327\n",
      "Step: [1297] total_loss: 2.13094330 d_loss: 1.38688409, g_loss: 0.69248331, ae_loss: 0.05157591\n",
      "Step: [1298] total_loss: 2.14173985 d_loss: 1.36400127, g_loss: 0.72570729, ae_loss: 0.05203114\n",
      "Step: [1299] total_loss: 2.13935757 d_loss: 1.38201272, g_loss: 0.70393312, ae_loss: 0.05341157\n",
      "Step: [1300] total_loss: 2.13392210 d_loss: 1.37566459, g_loss: 0.70402122, ae_loss: 0.05423636\n",
      "Step: [1301] total_loss: 2.13844466 d_loss: 1.39308631, g_loss: 0.69610393, ae_loss: 0.04925447\n",
      "Step: [1302] total_loss: 2.12274528 d_loss: 1.36924434, g_loss: 0.69921488, ae_loss: 0.05428605\n",
      "Step: [1303] total_loss: 2.13487697 d_loss: 1.39223695, g_loss: 0.69232041, ae_loss: 0.05031967\n",
      "Step: [1304] total_loss: 2.11821175 d_loss: 1.38678014, g_loss: 0.68197536, ae_loss: 0.04945632\n",
      "Step: [1305] total_loss: 2.10582399 d_loss: 1.37393737, g_loss: 0.68143034, ae_loss: 0.05045645\n",
      "Step: [1306] total_loss: 2.10024452 d_loss: 1.37989438, g_loss: 0.66890913, ae_loss: 0.05144108\n",
      "Step: [1307] total_loss: 2.12732697 d_loss: 1.37952721, g_loss: 0.69624895, ae_loss: 0.05155089\n",
      "Step: [1308] total_loss: 2.12403059 d_loss: 1.38150287, g_loss: 0.68845063, ae_loss: 0.05407703\n",
      "Step: [1309] total_loss: 2.13906240 d_loss: 1.37948358, g_loss: 0.70695513, ae_loss: 0.05262365\n",
      "Step: [1310] total_loss: 2.12593198 d_loss: 1.38989782, g_loss: 0.68482697, ae_loss: 0.05120718\n",
      "Step: [1311] total_loss: 2.12839699 d_loss: 1.37363076, g_loss: 0.70274699, ae_loss: 0.05201937\n",
      "Step: [1312] total_loss: 2.12216520 d_loss: 1.37954116, g_loss: 0.69347787, ae_loss: 0.04914612\n",
      "Step: [1313] total_loss: 2.12553596 d_loss: 1.38874936, g_loss: 0.68345016, ae_loss: 0.05333644\n",
      "Step: [1314] total_loss: 2.12040472 d_loss: 1.37254655, g_loss: 0.69216126, ae_loss: 0.05569696\n",
      "Step: [1315] total_loss: 2.12115574 d_loss: 1.39364457, g_loss: 0.67344022, ae_loss: 0.05407093\n",
      "Step: [1316] total_loss: 2.13851333 d_loss: 1.38848805, g_loss: 0.69966459, ae_loss: 0.05036074\n",
      "Step: [1317] total_loss: 2.13167477 d_loss: 1.37406325, g_loss: 0.70545030, ae_loss: 0.05216137\n",
      "Step: [1318] total_loss: 2.11378145 d_loss: 1.36928535, g_loss: 0.69182867, ae_loss: 0.05266754\n",
      "Step: [1319] total_loss: 2.14560342 d_loss: 1.38915801, g_loss: 0.70550752, ae_loss: 0.05093786\n",
      "Step: [1320] total_loss: 2.13243198 d_loss: 1.38839340, g_loss: 0.68956828, ae_loss: 0.05447044\n",
      "Step: [1321] total_loss: 2.13644886 d_loss: 1.39288235, g_loss: 0.68832308, ae_loss: 0.05524337\n",
      "Step: [1322] total_loss: 2.13043737 d_loss: 1.38448668, g_loss: 0.69216180, ae_loss: 0.05378877\n",
      "Step: [1323] total_loss: 2.14029574 d_loss: 1.38420105, g_loss: 0.70449311, ae_loss: 0.05160166\n",
      "Step: [1324] total_loss: 2.14002657 d_loss: 1.39833164, g_loss: 0.68845570, ae_loss: 0.05323921\n",
      "Step: [1325] total_loss: 2.12243485 d_loss: 1.36864901, g_loss: 0.70132953, ae_loss: 0.05245627\n",
      "Step: [1326] total_loss: 2.13961339 d_loss: 1.40754402, g_loss: 0.67966586, ae_loss: 0.05240348\n",
      "Step: [1327] total_loss: 2.15474319 d_loss: 1.39969039, g_loss: 0.70445478, ae_loss: 0.05059801\n",
      "Step: [1328] total_loss: 2.11904383 d_loss: 1.36673379, g_loss: 0.69759631, ae_loss: 0.05471375\n",
      "Step: [1329] total_loss: 2.12365055 d_loss: 1.38514495, g_loss: 0.68578601, ae_loss: 0.05271976\n",
      "Step: [1330] total_loss: 2.11352921 d_loss: 1.36342955, g_loss: 0.69301546, ae_loss: 0.05708413\n",
      "Step: [1331] total_loss: 2.12257338 d_loss: 1.38185358, g_loss: 0.68901181, ae_loss: 0.05170815\n",
      "Step: [1332] total_loss: 2.13082170 d_loss: 1.39958823, g_loss: 0.67191958, ae_loss: 0.05931389\n",
      "Step: [1333] total_loss: 2.13131380 d_loss: 1.38541031, g_loss: 0.69461274, ae_loss: 0.05129072\n",
      "Step: [1334] total_loss: 2.13535643 d_loss: 1.39251292, g_loss: 0.69029623, ae_loss: 0.05254735\n",
      "Step: [1335] total_loss: 2.13433361 d_loss: 1.36622691, g_loss: 0.71171331, ae_loss: 0.05639330\n",
      "Step: [1336] total_loss: 2.14372778 d_loss: 1.39345133, g_loss: 0.69997919, ae_loss: 0.05029740\n",
      "Step: [1337] total_loss: 2.14983606 d_loss: 1.38343263, g_loss: 0.71580666, ae_loss: 0.05059685\n",
      "Step: [1338] total_loss: 2.13593650 d_loss: 1.37536955, g_loss: 0.71184254, ae_loss: 0.04872445\n",
      "Step: [1339] total_loss: 2.13159132 d_loss: 1.38219249, g_loss: 0.69588411, ae_loss: 0.05351463\n",
      "Step: [1340] total_loss: 2.11591744 d_loss: 1.38622820, g_loss: 0.67505455, ae_loss: 0.05463473\n",
      "Step: [1341] total_loss: 2.10828924 d_loss: 1.37566447, g_loss: 0.67882681, ae_loss: 0.05379780\n",
      "Step: [1342] total_loss: 2.11887622 d_loss: 1.38738179, g_loss: 0.67754990, ae_loss: 0.05394456\n",
      "Step: [1343] total_loss: 2.11874390 d_loss: 1.36816573, g_loss: 0.69611561, ae_loss: 0.05446253\n",
      "Step: [1344] total_loss: 2.11418200 d_loss: 1.35905647, g_loss: 0.70399553, ae_loss: 0.05113004\n",
      "Step: [1345] total_loss: 2.14982986 d_loss: 1.40191412, g_loss: 0.69426721, ae_loss: 0.05364850\n",
      "Step: [1346] total_loss: 2.13231492 d_loss: 1.37065315, g_loss: 0.71030915, ae_loss: 0.05135268\n",
      "Step: [1347] total_loss: 2.13336039 d_loss: 1.37245798, g_loss: 0.71179092, ae_loss: 0.04911142\n",
      "Step: [1348] total_loss: 2.14312148 d_loss: 1.38550782, g_loss: 0.70746195, ae_loss: 0.05015168\n",
      "Step: [1349] total_loss: 2.13291597 d_loss: 1.37269092, g_loss: 0.70724797, ae_loss: 0.05297722\n",
      "Step: [1350] total_loss: 2.14979124 d_loss: 1.38214791, g_loss: 0.71582299, ae_loss: 0.05182032\n",
      "Step: [1351] total_loss: 2.13778806 d_loss: 1.39431310, g_loss: 0.68666697, ae_loss: 0.05680795\n",
      "Step: [1352] total_loss: 2.12894917 d_loss: 1.40575993, g_loss: 0.66913170, ae_loss: 0.05405753\n",
      "Step: [1353] total_loss: 2.13864470 d_loss: 1.38284993, g_loss: 0.70143127, ae_loss: 0.05436356\n",
      "Step: [1354] total_loss: 2.12387538 d_loss: 1.36862540, g_loss: 0.70221645, ae_loss: 0.05303356\n",
      "Step: [1355] total_loss: 2.14049935 d_loss: 1.38997912, g_loss: 0.69495034, ae_loss: 0.05556987\n",
      "Step: [1356] total_loss: 2.14639139 d_loss: 1.39141011, g_loss: 0.70092136, ae_loss: 0.05405999\n",
      "Step: [1357] total_loss: 2.13796163 d_loss: 1.39231110, g_loss: 0.68821549, ae_loss: 0.05743502\n",
      "Step: [1358] total_loss: 2.15326285 d_loss: 1.39923239, g_loss: 0.70470905, ae_loss: 0.04932136\n",
      "Step: [1359] total_loss: 2.13435793 d_loss: 1.38316178, g_loss: 0.70185554, ae_loss: 0.04934055\n",
      "Step: [1360] total_loss: 2.12328362 d_loss: 1.37103724, g_loss: 0.70222068, ae_loss: 0.05002573\n",
      "Step: [1361] total_loss: 2.13050914 d_loss: 1.40372920, g_loss: 0.67365390, ae_loss: 0.05312597\n",
      "Step: [1362] total_loss: 2.11665368 d_loss: 1.38119209, g_loss: 0.67950207, ae_loss: 0.05595959\n",
      "Step: [1363] total_loss: 2.11760378 d_loss: 1.38234520, g_loss: 0.68383968, ae_loss: 0.05141887\n",
      "Step: [1364] total_loss: 2.11240625 d_loss: 1.38565063, g_loss: 0.67302632, ae_loss: 0.05372944\n",
      "Step: [1365] total_loss: 2.11454129 d_loss: 1.37889266, g_loss: 0.68134850, ae_loss: 0.05430004\n",
      "Step: [1366] total_loss: 2.11367035 d_loss: 1.38004363, g_loss: 0.68184447, ae_loss: 0.05178209\n",
      "Step: [1367] total_loss: 2.14369226 d_loss: 1.37763155, g_loss: 0.71170878, ae_loss: 0.05435187\n",
      "Step: [1368] total_loss: 2.13471365 d_loss: 1.37520206, g_loss: 0.70737720, ae_loss: 0.05213422\n",
      "Step: [1369] total_loss: 2.12308908 d_loss: 1.36346757, g_loss: 0.70510387, ae_loss: 0.05451765\n",
      "Step: [1370] total_loss: 2.12703657 d_loss: 1.38683081, g_loss: 0.68949318, ae_loss: 0.05071263\n",
      "Step: [1371] total_loss: 2.11762810 d_loss: 1.36613560, g_loss: 0.70082825, ae_loss: 0.05066426\n",
      "Step: [1372] total_loss: 2.11459589 d_loss: 1.39965177, g_loss: 0.66616368, ae_loss: 0.04878030\n",
      "Step: [1373] total_loss: 2.10278463 d_loss: 1.37832487, g_loss: 0.67147398, ae_loss: 0.05298582\n",
      "Step: [1374] total_loss: 2.11469269 d_loss: 1.39008975, g_loss: 0.66773140, ae_loss: 0.05687149\n",
      "Step: [1375] total_loss: 2.13890076 d_loss: 1.36657369, g_loss: 0.72062105, ae_loss: 0.05170590\n",
      "Step: [1376] total_loss: 2.13134909 d_loss: 1.38296175, g_loss: 0.69521195, ae_loss: 0.05317535\n",
      "Step: [1377] total_loss: 2.15258837 d_loss: 1.38325858, g_loss: 0.71553123, ae_loss: 0.05379847\n",
      "Step: [1378] total_loss: 2.13854527 d_loss: 1.35996401, g_loss: 0.72371411, ae_loss: 0.05486716\n",
      "Step: [1379] total_loss: 2.14905596 d_loss: 1.37983537, g_loss: 0.71776354, ae_loss: 0.05145718\n",
      "Step: [1380] total_loss: 2.15577936 d_loss: 1.39010859, g_loss: 0.71706623, ae_loss: 0.04860447\n",
      "Step: [1381] total_loss: 2.13847780 d_loss: 1.38044059, g_loss: 0.70634657, ae_loss: 0.05169064\n",
      "Step: [1382] total_loss: 2.15618205 d_loss: 1.38230443, g_loss: 0.71957082, ae_loss: 0.05430683\n",
      "Step: [1383] total_loss: 2.13613319 d_loss: 1.39731526, g_loss: 0.68431568, ae_loss: 0.05450236\n",
      "Step: [1384] total_loss: 2.11945438 d_loss: 1.36703539, g_loss: 0.70129454, ae_loss: 0.05112460\n",
      "Step: [1385] total_loss: 2.11012411 d_loss: 1.36701322, g_loss: 0.69242835, ae_loss: 0.05068257\n",
      "Step: [1386] total_loss: 2.11972141 d_loss: 1.38565469, g_loss: 0.68129408, ae_loss: 0.05277271\n",
      "Step: [1387] total_loss: 2.11989260 d_loss: 1.37638545, g_loss: 0.69275415, ae_loss: 0.05075309\n",
      "Step: [1388] total_loss: 2.14015341 d_loss: 1.37194943, g_loss: 0.71894014, ae_loss: 0.04926376\n",
      "Step: [1389] total_loss: 2.12969327 d_loss: 1.39256811, g_loss: 0.68757433, ae_loss: 0.04955082\n",
      "Step: [1390] total_loss: 2.12940264 d_loss: 1.37747216, g_loss: 0.70097286, ae_loss: 0.05095768\n",
      "Step: [1391] total_loss: 2.12916183 d_loss: 1.38614976, g_loss: 0.69703770, ae_loss: 0.04597430\n",
      "Step: [1392] total_loss: 2.13786650 d_loss: 1.38129449, g_loss: 0.70514703, ae_loss: 0.05142516\n",
      "Step: [1393] total_loss: 2.14118242 d_loss: 1.40631533, g_loss: 0.68329978, ae_loss: 0.05156723\n",
      "Step: [1394] total_loss: 2.13617158 d_loss: 1.38434720, g_loss: 0.69874966, ae_loss: 0.05307470\n",
      "Step: [1395] total_loss: 2.12446976 d_loss: 1.36923957, g_loss: 0.70598596, ae_loss: 0.04924430\n",
      "Step: [1396] total_loss: 2.11092424 d_loss: 1.37807107, g_loss: 0.67844522, ae_loss: 0.05440788\n",
      "Step: [1397] total_loss: 2.11201906 d_loss: 1.38380301, g_loss: 0.67890823, ae_loss: 0.04930795\n",
      "Step: [1398] total_loss: 2.13223171 d_loss: 1.39507663, g_loss: 0.68390077, ae_loss: 0.05325429\n",
      "Step: [1399] total_loss: 2.11652422 d_loss: 1.36685431, g_loss: 0.69964981, ae_loss: 0.05002012\n",
      "Step: [1400] total_loss: 2.13464785 d_loss: 1.38157523, g_loss: 0.70277292, ae_loss: 0.05029973\n",
      "Step: [1401] total_loss: 2.13011694 d_loss: 1.37230873, g_loss: 0.70394325, ae_loss: 0.05386494\n",
      "Step: [1402] total_loss: 2.14777112 d_loss: 1.39410174, g_loss: 0.70131356, ae_loss: 0.05235592\n",
      "Step: [1403] total_loss: 2.15302610 d_loss: 1.39112258, g_loss: 0.70401651, ae_loss: 0.05788714\n",
      "Step: [1404] total_loss: 2.12769413 d_loss: 1.35750771, g_loss: 0.72012556, ae_loss: 0.05006074\n",
      "Step: [1405] total_loss: 2.13630342 d_loss: 1.39293790, g_loss: 0.68969381, ae_loss: 0.05367166\n",
      "Step: [1406] total_loss: 2.12063456 d_loss: 1.38442922, g_loss: 0.68176639, ae_loss: 0.05443898\n",
      "Step: [1407] total_loss: 2.12448311 d_loss: 1.39953303, g_loss: 0.67359209, ae_loss: 0.05135795\n",
      "Step: [1408] total_loss: 2.10811734 d_loss: 1.36836410, g_loss: 0.68722528, ae_loss: 0.05252806\n",
      "Step: [1409] total_loss: 2.13651013 d_loss: 1.38721776, g_loss: 0.69920260, ae_loss: 0.05008973\n",
      "Step: [1410] total_loss: 2.11995792 d_loss: 1.37533116, g_loss: 0.69405884, ae_loss: 0.05056784\n",
      "Step: [1411] total_loss: 2.12412262 d_loss: 1.38339448, g_loss: 0.69023377, ae_loss: 0.05049447\n",
      "Step: [1412] total_loss: 2.12485409 d_loss: 1.39139247, g_loss: 0.68031156, ae_loss: 0.05314991\n",
      "Step: [1413] total_loss: 2.12040043 d_loss: 1.36695445, g_loss: 0.70735568, ae_loss: 0.04609037\n",
      "Step: [1414] total_loss: 2.12884092 d_loss: 1.37440097, g_loss: 0.70317316, ae_loss: 0.05126691\n",
      "Step: [1415] total_loss: 2.14104319 d_loss: 1.36154962, g_loss: 0.72518480, ae_loss: 0.05430863\n",
      "Step: [1416] total_loss: 2.13337851 d_loss: 1.39585841, g_loss: 0.68394810, ae_loss: 0.05357191\n",
      "Step: [1417] total_loss: 2.11751080 d_loss: 1.36769772, g_loss: 0.70097673, ae_loss: 0.04883619\n",
      "Step: [1418] total_loss: 2.13378811 d_loss: 1.38260794, g_loss: 0.69735754, ae_loss: 0.05382268\n",
      "Step: [1419] total_loss: 2.12418938 d_loss: 1.36337733, g_loss: 0.70544326, ae_loss: 0.05536887\n",
      "Step: [1420] total_loss: 2.09842944 d_loss: 1.36461020, g_loss: 0.68428040, ae_loss: 0.04953883\n",
      "Step: [1421] total_loss: 2.12954807 d_loss: 1.38865066, g_loss: 0.68904543, ae_loss: 0.05185186\n",
      "Step: [1422] total_loss: 2.09552479 d_loss: 1.37078357, g_loss: 0.67219275, ae_loss: 0.05254842\n",
      "Step: [1423] total_loss: 2.10948277 d_loss: 1.37428665, g_loss: 0.68513691, ae_loss: 0.05005927\n",
      "Step: [1424] total_loss: 2.11741638 d_loss: 1.36739314, g_loss: 0.69735610, ae_loss: 0.05266697\n",
      "Step: [1425] total_loss: 2.12387681 d_loss: 1.37100172, g_loss: 0.70372152, ae_loss: 0.04915352\n",
      "Step: [1426] total_loss: 2.15159869 d_loss: 1.39944458, g_loss: 0.69930613, ae_loss: 0.05284800\n",
      "Step: [1427] total_loss: 2.11864352 d_loss: 1.34697795, g_loss: 0.72392470, ae_loss: 0.04774081\n",
      "Step: [1428] total_loss: 2.13326764 d_loss: 1.37944722, g_loss: 0.70649761, ae_loss: 0.04732277\n",
      "Step: [1429] total_loss: 2.11955166 d_loss: 1.36896896, g_loss: 0.69901752, ae_loss: 0.05156501\n",
      "Step: [1430] total_loss: 2.11454773 d_loss: 1.37669349, g_loss: 0.68275219, ae_loss: 0.05510197\n",
      "Step: [1431] total_loss: 2.12989902 d_loss: 1.39999831, g_loss: 0.68195367, ae_loss: 0.04794717\n",
      "Step: [1432] total_loss: 2.15993261 d_loss: 1.38403535, g_loss: 0.72057056, ae_loss: 0.05532684\n",
      "Step: [1433] total_loss: 2.09884214 d_loss: 1.34295344, g_loss: 0.70280015, ae_loss: 0.05308862\n",
      "Step: [1434] total_loss: 2.12871075 d_loss: 1.39020634, g_loss: 0.68616176, ae_loss: 0.05234257\n",
      "Step: [1435] total_loss: 2.11379504 d_loss: 1.36758375, g_loss: 0.69173861, ae_loss: 0.05447273\n",
      "Step: [1436] total_loss: 2.10590649 d_loss: 1.36346388, g_loss: 0.69142222, ae_loss: 0.05102026\n",
      "Step: [1437] total_loss: 2.10494328 d_loss: 1.36027837, g_loss: 0.69400036, ae_loss: 0.05066454\n",
      "Step: [1438] total_loss: 2.10118079 d_loss: 1.36297059, g_loss: 0.68405712, ae_loss: 0.05415304\n",
      "Step: [1439] total_loss: 2.10293341 d_loss: 1.37391329, g_loss: 0.67755389, ae_loss: 0.05146638\n",
      "Step: [1440] total_loss: 2.10317492 d_loss: 1.35033190, g_loss: 0.70044518, ae_loss: 0.05239785\n",
      "Step: [1441] total_loss: 2.12516761 d_loss: 1.39011407, g_loss: 0.68371946, ae_loss: 0.05133398\n",
      "Step: [1442] total_loss: 2.13470721 d_loss: 1.40830207, g_loss: 0.66874748, ae_loss: 0.05765777\n",
      "Step: [1443] total_loss: 2.11836338 d_loss: 1.37016010, g_loss: 0.69439512, ae_loss: 0.05380819\n",
      "Step: [1444] total_loss: 2.12532544 d_loss: 1.36757612, g_loss: 0.70309293, ae_loss: 0.05465636\n",
      "Step: [1445] total_loss: 2.13529348 d_loss: 1.39069641, g_loss: 0.69055092, ae_loss: 0.05404633\n",
      "Step: [1446] total_loss: 2.11477947 d_loss: 1.35510170, g_loss: 0.70515251, ae_loss: 0.05452517\n",
      "Step: [1447] total_loss: 2.12074780 d_loss: 1.37715447, g_loss: 0.68813556, ae_loss: 0.05545786\n",
      "Step: [1448] total_loss: 2.13335896 d_loss: 1.38549805, g_loss: 0.69246590, ae_loss: 0.05539509\n",
      "Step: [1449] total_loss: 2.11232996 d_loss: 1.38380480, g_loss: 0.67695308, ae_loss: 0.05157201\n",
      "Step: [1450] total_loss: 2.10140705 d_loss: 1.35725427, g_loss: 0.69135165, ae_loss: 0.05280126\n",
      "Step: [1451] total_loss: 2.16273522 d_loss: 1.41733170, g_loss: 0.69064760, ae_loss: 0.05475598\n",
      "Step: [1452] total_loss: 2.11018944 d_loss: 1.36391044, g_loss: 0.69470108, ae_loss: 0.05157793\n",
      "Step: [1453] total_loss: 2.14044523 d_loss: 1.38326240, g_loss: 0.70665556, ae_loss: 0.05052725\n",
      "Step: [1454] total_loss: 2.13363671 d_loss: 1.37763369, g_loss: 0.70217580, ae_loss: 0.05382732\n",
      "Step: [1455] total_loss: 2.12979269 d_loss: 1.35509527, g_loss: 0.72275162, ae_loss: 0.05194576\n",
      "Step: [1456] total_loss: 2.14530396 d_loss: 1.39558530, g_loss: 0.69915432, ae_loss: 0.05056443\n",
      "Step: [1457] total_loss: 2.15608883 d_loss: 1.42357779, g_loss: 0.68335867, ae_loss: 0.04915225\n",
      "Step: [1458] total_loss: 2.12679935 d_loss: 1.37381899, g_loss: 0.70487374, ae_loss: 0.04810657\n",
      "Step: [1459] total_loss: 2.12893152 d_loss: 1.36386836, g_loss: 0.71084499, ae_loss: 0.05421805\n",
      "Step: [1460] total_loss: 2.11137080 d_loss: 1.36860418, g_loss: 0.68700206, ae_loss: 0.05576459\n",
      "Step: [1461] total_loss: 2.13747454 d_loss: 1.38258791, g_loss: 0.69840717, ae_loss: 0.05647929\n",
      "Step: [1462] total_loss: 2.13034248 d_loss: 1.36841130, g_loss: 0.70812392, ae_loss: 0.05380727\n",
      "Step: [1463] total_loss: 2.12637281 d_loss: 1.35917401, g_loss: 0.71413177, ae_loss: 0.05306702\n",
      "Step: [1464] total_loss: 2.13160491 d_loss: 1.37898064, g_loss: 0.69881129, ae_loss: 0.05381304\n",
      "Step: [1465] total_loss: 2.14763975 d_loss: 1.39639044, g_loss: 0.69599998, ae_loss: 0.05524934\n",
      "Step: [1466] total_loss: 2.14901924 d_loss: 1.37013757, g_loss: 0.72856236, ae_loss: 0.05031930\n",
      "Step: [1467] total_loss: 2.15114450 d_loss: 1.39817595, g_loss: 0.69729638, ae_loss: 0.05567209\n",
      "Step: [1468] total_loss: 2.11349344 d_loss: 1.36060762, g_loss: 0.70091343, ae_loss: 0.05197253\n",
      "Step: [1469] total_loss: 2.11068296 d_loss: 1.35730577, g_loss: 0.70002759, ae_loss: 0.05334960\n",
      "Step: [1470] total_loss: 2.12881517 d_loss: 1.36995745, g_loss: 0.70641226, ae_loss: 0.05244552\n",
      "Step: [1471] total_loss: 2.14189601 d_loss: 1.37165260, g_loss: 0.72176015, ae_loss: 0.04848326\n",
      "Step: [1472] total_loss: 2.14987206 d_loss: 1.40433621, g_loss: 0.69569129, ae_loss: 0.04984447\n",
      "Step: [1473] total_loss: 2.13391590 d_loss: 1.39187348, g_loss: 0.69301152, ae_loss: 0.04903103\n",
      "Step: [1474] total_loss: 2.13108373 d_loss: 1.38207388, g_loss: 0.69818562, ae_loss: 0.05082411\n",
      "Step: [1475] total_loss: 2.13064742 d_loss: 1.37695432, g_loss: 0.70266873, ae_loss: 0.05102436\n",
      "Step: [1476] total_loss: 2.12227440 d_loss: 1.37289333, g_loss: 0.69868255, ae_loss: 0.05069847\n",
      "Step: [1477] total_loss: 2.13641739 d_loss: 1.38069892, g_loss: 0.70257378, ae_loss: 0.05314467\n",
      "Step: [1478] total_loss: 2.14274025 d_loss: 1.37583721, g_loss: 0.71555495, ae_loss: 0.05134797\n",
      "Step: [1479] total_loss: 2.13408709 d_loss: 1.35687411, g_loss: 0.72690976, ae_loss: 0.05030325\n",
      "Step: [1480] total_loss: 2.11903238 d_loss: 1.36457920, g_loss: 0.70114720, ae_loss: 0.05330596\n",
      "Step: [1481] total_loss: 2.14113665 d_loss: 1.36690760, g_loss: 0.71974754, ae_loss: 0.05448165\n",
      "Step: [1482] total_loss: 2.11668396 d_loss: 1.37285256, g_loss: 0.69593167, ae_loss: 0.04789969\n",
      "Step: [1483] total_loss: 2.12009621 d_loss: 1.36750460, g_loss: 0.69712877, ae_loss: 0.05546279\n",
      "Step: [1484] total_loss: 2.12396622 d_loss: 1.40016830, g_loss: 0.67195916, ae_loss: 0.05183889\n",
      "Step: [1485] total_loss: 2.13908195 d_loss: 1.38220596, g_loss: 0.70567811, ae_loss: 0.05119783\n",
      "Step: [1486] total_loss: 2.13606215 d_loss: 1.38206053, g_loss: 0.69828892, ae_loss: 0.05571269\n",
      "Step: [1487] total_loss: 2.15230227 d_loss: 1.41048443, g_loss: 0.69058096, ae_loss: 0.05123689\n",
      "Step: [1488] total_loss: 2.10560632 d_loss: 1.35852957, g_loss: 0.69576389, ae_loss: 0.05131294\n",
      "Step: [1489] total_loss: 2.13786054 d_loss: 1.38640356, g_loss: 0.69760525, ae_loss: 0.05385177\n",
      "Step: [1490] total_loss: 2.12946320 d_loss: 1.39911449, g_loss: 0.67633474, ae_loss: 0.05401400\n",
      "Step: [1491] total_loss: 2.14019585 d_loss: 1.38419437, g_loss: 0.70023596, ae_loss: 0.05576559\n",
      "Step: [1492] total_loss: 2.10327697 d_loss: 1.35253406, g_loss: 0.69690639, ae_loss: 0.05383648\n",
      "Step: [1493] total_loss: 2.11634731 d_loss: 1.37632108, g_loss: 0.68900287, ae_loss: 0.05102350\n",
      "Step: [1494] total_loss: 2.12321639 d_loss: 1.38399434, g_loss: 0.68474126, ae_loss: 0.05448084\n",
      "Step: [1495] total_loss: 2.10382104 d_loss: 1.37464118, g_loss: 0.67609096, ae_loss: 0.05308894\n",
      "Step: [1496] total_loss: 2.11302423 d_loss: 1.39628887, g_loss: 0.66732502, ae_loss: 0.04941050\n",
      "Step: [1497] total_loss: 2.08741093 d_loss: 1.34706020, g_loss: 0.68755233, ae_loss: 0.05279840\n",
      "Step: [1498] total_loss: 2.10109973 d_loss: 1.35263252, g_loss: 0.69487059, ae_loss: 0.05359665\n",
      "Step: [1499] total_loss: 2.14069843 d_loss: 1.38689232, g_loss: 0.69983822, ae_loss: 0.05396800\n",
      "Step: [1500] total_loss: 2.10378790 d_loss: 1.36594343, g_loss: 0.68338728, ae_loss: 0.05445719\n",
      "Step: [1501] total_loss: 2.13204217 d_loss: 1.37408721, g_loss: 0.70297956, ae_loss: 0.05497535\n",
      "Step: [1502] total_loss: 2.14572239 d_loss: 1.37950850, g_loss: 0.71206909, ae_loss: 0.05414480\n",
      "Step: [1503] total_loss: 2.13421869 d_loss: 1.37330830, g_loss: 0.70723760, ae_loss: 0.05367271\n",
      "Step: [1504] total_loss: 2.12828135 d_loss: 1.39439738, g_loss: 0.68460858, ae_loss: 0.04927539\n",
      "Step: [1505] total_loss: 2.10724998 d_loss: 1.36757541, g_loss: 0.68508387, ae_loss: 0.05459067\n",
      "Step: [1506] total_loss: 2.10673523 d_loss: 1.36341667, g_loss: 0.69231462, ae_loss: 0.05100376\n",
      "Step: [1507] total_loss: 2.12945509 d_loss: 1.40697455, g_loss: 0.66846228, ae_loss: 0.05401808\n",
      "Step: [1508] total_loss: 2.14320111 d_loss: 1.40198708, g_loss: 0.69055110, ae_loss: 0.05066292\n",
      "Step: [1509] total_loss: 2.13182378 d_loss: 1.39694417, g_loss: 0.68581289, ae_loss: 0.04906663\n",
      "Step: [1510] total_loss: 2.12025809 d_loss: 1.37377489, g_loss: 0.69499129, ae_loss: 0.05149200\n",
      "Step: [1511] total_loss: 2.11653686 d_loss: 1.37224722, g_loss: 0.69340414, ae_loss: 0.05088552\n",
      "Step: [1512] total_loss: 2.12672877 d_loss: 1.37242782, g_loss: 0.70463532, ae_loss: 0.04966572\n",
      "Step: [1513] total_loss: 2.14239407 d_loss: 1.38065577, g_loss: 0.70643222, ae_loss: 0.05530593\n",
      "Step: [1514] total_loss: 2.12064743 d_loss: 1.36670947, g_loss: 0.70329732, ae_loss: 0.05064076\n",
      "Step: [1515] total_loss: 2.14737606 d_loss: 1.39239311, g_loss: 0.69569099, ae_loss: 0.05929187\n",
      "Step: [1516] total_loss: 2.11763978 d_loss: 1.36852562, g_loss: 0.70104438, ae_loss: 0.04806984\n",
      "Step: [1517] total_loss: 2.13421345 d_loss: 1.40436006, g_loss: 0.67521566, ae_loss: 0.05463767\n",
      "Step: [1518] total_loss: 2.12756395 d_loss: 1.38201046, g_loss: 0.69083726, ae_loss: 0.05471637\n",
      "Step: [1519] total_loss: 2.11914635 d_loss: 1.36493337, g_loss: 0.70333040, ae_loss: 0.05088255\n",
      "Step: [1520] total_loss: 2.12503099 d_loss: 1.37883341, g_loss: 0.69589096, ae_loss: 0.05030661\n",
      "Step: [1521] total_loss: 2.09598780 d_loss: 1.36469686, g_loss: 0.68037522, ae_loss: 0.05091581\n",
      "Step: [1522] total_loss: 2.12870789 d_loss: 1.38928080, g_loss: 0.68707609, ae_loss: 0.05235104\n",
      "Step: [1523] total_loss: 2.14586782 d_loss: 1.39409137, g_loss: 0.69548488, ae_loss: 0.05629156\n",
      "Step: [1524] total_loss: 2.13392591 d_loss: 1.38494873, g_loss: 0.69342566, ae_loss: 0.05555158\n",
      "Step: [1525] total_loss: 2.13174939 d_loss: 1.36934912, g_loss: 0.71201968, ae_loss: 0.05038055\n",
      "Step: [1526] total_loss: 2.11546278 d_loss: 1.36688638, g_loss: 0.69208485, ae_loss: 0.05649166\n",
      "Step: [1527] total_loss: 2.10522461 d_loss: 1.35559487, g_loss: 0.69496173, ae_loss: 0.05466807\n",
      "Step: [1528] total_loss: 2.12230968 d_loss: 1.39245117, g_loss: 0.67356086, ae_loss: 0.05629764\n",
      "Step: [1529] total_loss: 2.10774374 d_loss: 1.35979652, g_loss: 0.69548023, ae_loss: 0.05246689\n",
      "Step: [1530] total_loss: 2.13042164 d_loss: 1.39626825, g_loss: 0.68055135, ae_loss: 0.05360208\n",
      "Step: [1531] total_loss: 2.13820696 d_loss: 1.40120053, g_loss: 0.68736708, ae_loss: 0.04963943\n",
      "Step: [1532] total_loss: 2.14337730 d_loss: 1.38454843, g_loss: 0.70285320, ae_loss: 0.05597571\n",
      "Step: [1533] total_loss: 2.13001609 d_loss: 1.34765697, g_loss: 0.73191583, ae_loss: 0.05044334\n",
      "Step: [1534] total_loss: 2.11143565 d_loss: 1.36517477, g_loss: 0.69433141, ae_loss: 0.05192944\n",
      "Step: [1535] total_loss: 2.14949179 d_loss: 1.39075375, g_loss: 0.70227003, ae_loss: 0.05646802\n",
      "Step: [1536] total_loss: 2.12946033 d_loss: 1.37400639, g_loss: 0.70023203, ae_loss: 0.05522208\n",
      "Step: [1537] total_loss: 2.09739494 d_loss: 1.36297917, g_loss: 0.68323600, ae_loss: 0.05117970\n",
      "Step: [1538] total_loss: 2.11076641 d_loss: 1.36919010, g_loss: 0.68965328, ae_loss: 0.05192286\n",
      "Step: [1539] total_loss: 2.12345147 d_loss: 1.39208794, g_loss: 0.67843282, ae_loss: 0.05293069\n",
      "Step: [1540] total_loss: 2.12023520 d_loss: 1.37830758, g_loss: 0.69292730, ae_loss: 0.04900023\n",
      "Step: [1541] total_loss: 2.13177633 d_loss: 1.39777362, g_loss: 0.68215883, ae_loss: 0.05184372\n",
      "Step: [1542] total_loss: 2.15052485 d_loss: 1.38308764, g_loss: 0.71406078, ae_loss: 0.05337648\n",
      "Step: [1543] total_loss: 2.12367439 d_loss: 1.36298692, g_loss: 0.70670360, ae_loss: 0.05398379\n",
      "Step: [1544] total_loss: 2.13707256 d_loss: 1.38977647, g_loss: 0.69300258, ae_loss: 0.05429338\n",
      "Step: [1545] total_loss: 2.11933708 d_loss: 1.37324357, g_loss: 0.69309300, ae_loss: 0.05300047\n",
      "Step: [1546] total_loss: 2.11654115 d_loss: 1.35004604, g_loss: 0.71280164, ae_loss: 0.05369339\n",
      "Step: [1547] total_loss: 2.13536668 d_loss: 1.39937711, g_loss: 0.68361098, ae_loss: 0.05237848\n",
      "Step: [1548] total_loss: 2.13854885 d_loss: 1.39271593, g_loss: 0.69211543, ae_loss: 0.05371756\n",
      "Step: [1549] total_loss: 2.12386084 d_loss: 1.38809562, g_loss: 0.68351638, ae_loss: 0.05224874\n",
      "Step: [1550] total_loss: 2.13158560 d_loss: 1.38749337, g_loss: 0.68974495, ae_loss: 0.05434715\n",
      "Step: [1551] total_loss: 2.12315750 d_loss: 1.39941061, g_loss: 0.67132366, ae_loss: 0.05242318\n",
      "Step: [1552] total_loss: 2.13673854 d_loss: 1.38483667, g_loss: 0.70079261, ae_loss: 0.05110930\n",
      "Step: [1553] total_loss: 2.14725232 d_loss: 1.37121964, g_loss: 0.72104615, ae_loss: 0.05498642\n",
      "Step: [1554] total_loss: 2.13797712 d_loss: 1.38302255, g_loss: 0.69985825, ae_loss: 0.05509636\n",
      "Step: [1555] total_loss: 2.12290740 d_loss: 1.37768245, g_loss: 0.69139010, ae_loss: 0.05383484\n",
      "Step: [1556] total_loss: 2.17212200 d_loss: 1.41112673, g_loss: 0.70505810, ae_loss: 0.05593704\n",
      "Step: [1557] total_loss: 2.13079381 d_loss: 1.38055873, g_loss: 0.69716072, ae_loss: 0.05307432\n",
      "Step: [1558] total_loss: 2.14158392 d_loss: 1.38873982, g_loss: 0.69631481, ae_loss: 0.05652933\n",
      "Step: [1559] total_loss: 2.13051033 d_loss: 1.38273644, g_loss: 0.69673836, ae_loss: 0.05103555\n",
      "Step: [1560] total_loss: 2.13011909 d_loss: 1.39869916, g_loss: 0.67993379, ae_loss: 0.05148610\n",
      "Step: [1561] total_loss: 2.11830664 d_loss: 1.38141918, g_loss: 0.68576348, ae_loss: 0.05112383\n",
      "Step: [1562] total_loss: 2.12375736 d_loss: 1.36441052, g_loss: 0.70584804, ae_loss: 0.05349869\n",
      "Step: [1563] total_loss: 2.11201668 d_loss: 1.37603641, g_loss: 0.68078870, ae_loss: 0.05519168\n",
      "Step: [1564] total_loss: 2.13108015 d_loss: 1.36435580, g_loss: 0.71616411, ae_loss: 0.05056040\n",
      "Step: [1565] total_loss: 2.14388680 d_loss: 1.39250445, g_loss: 0.69760597, ae_loss: 0.05377635\n",
      "Step: [1566] total_loss: 2.13139939 d_loss: 1.38845837, g_loss: 0.68864697, ae_loss: 0.05429405\n",
      "Step: [1567] total_loss: 2.11988258 d_loss: 1.37464762, g_loss: 0.69885284, ae_loss: 0.04638208\n",
      "Step: [1568] total_loss: 2.13456845 d_loss: 1.38928795, g_loss: 0.69348240, ae_loss: 0.05179807\n",
      "Step: [1569] total_loss: 2.09809494 d_loss: 1.37415314, g_loss: 0.67011046, ae_loss: 0.05383125\n",
      "Step: [1570] total_loss: 2.11998463 d_loss: 1.39512753, g_loss: 0.67289871, ae_loss: 0.05195841\n",
      "Step: [1571] total_loss: 2.12243271 d_loss: 1.37844396, g_loss: 0.69612169, ae_loss: 0.04786722\n",
      "Step: [1572] total_loss: 2.13704252 d_loss: 1.40712857, g_loss: 0.67660260, ae_loss: 0.05331133\n",
      "Step: [1573] total_loss: 2.15076017 d_loss: 1.38271022, g_loss: 0.71612167, ae_loss: 0.05192816\n",
      "Step: [1574] total_loss: 2.12130260 d_loss: 1.36714673, g_loss: 0.70391500, ae_loss: 0.05024105\n",
      "Step: [1575] total_loss: 2.15066123 d_loss: 1.41003752, g_loss: 0.68412143, ae_loss: 0.05650218\n",
      "Step: [1576] total_loss: 2.12316322 d_loss: 1.38269198, g_loss: 0.69052792, ae_loss: 0.04994346\n",
      "Step: [1577] total_loss: 2.10381413 d_loss: 1.35873890, g_loss: 0.68995953, ae_loss: 0.05511576\n",
      "Step: [1578] total_loss: 2.12094355 d_loss: 1.37811434, g_loss: 0.69125307, ae_loss: 0.05157610\n",
      "Step: [1579] total_loss: 2.13310623 d_loss: 1.38147473, g_loss: 0.70175928, ae_loss: 0.04987211\n",
      "Step: [1580] total_loss: 2.12047029 d_loss: 1.38870740, g_loss: 0.67864752, ae_loss: 0.05311532\n",
      "Step: [1581] total_loss: 2.11858940 d_loss: 1.37213469, g_loss: 0.69680870, ae_loss: 0.04964595\n",
      "Step: [1582] total_loss: 2.11761165 d_loss: 1.36393166, g_loss: 0.70245701, ae_loss: 0.05122287\n",
      "Step: [1583] total_loss: 2.13642025 d_loss: 1.39250016, g_loss: 0.68981999, ae_loss: 0.05410010\n",
      "Step: [1584] total_loss: 2.12280369 d_loss: 1.36181891, g_loss: 0.70574993, ae_loss: 0.05523485\n",
      "Step: [1585] total_loss: 2.11336112 d_loss: 1.36728072, g_loss: 0.69345742, ae_loss: 0.05262286\n",
      "Step: [1586] total_loss: 2.14214325 d_loss: 1.40131521, g_loss: 0.68788302, ae_loss: 0.05294504\n",
      "Step: [1587] total_loss: 2.12087941 d_loss: 1.38091493, g_loss: 0.68961728, ae_loss: 0.05034720\n",
      "Step: [1588] total_loss: 2.13327861 d_loss: 1.38820958, g_loss: 0.69209892, ae_loss: 0.05297010\n",
      "Step: [1589] total_loss: 2.14240456 d_loss: 1.39073706, g_loss: 0.69757748, ae_loss: 0.05409018\n",
      "Step: [1590] total_loss: 2.12808752 d_loss: 1.35364485, g_loss: 0.71926332, ae_loss: 0.05517934\n",
      "Step: [1591] total_loss: 2.11919355 d_loss: 1.37024546, g_loss: 0.69560230, ae_loss: 0.05334577\n",
      "Step: [1592] total_loss: 2.13568997 d_loss: 1.39762342, g_loss: 0.68268085, ae_loss: 0.05538570\n",
      "Step: [1593] total_loss: 2.11048317 d_loss: 1.35425115, g_loss: 0.70218146, ae_loss: 0.05405048\n",
      "Step: [1594] total_loss: 2.13170481 d_loss: 1.38614130, g_loss: 0.69544363, ae_loss: 0.05011973\n",
      "Step: [1595] total_loss: 2.11551428 d_loss: 1.37096012, g_loss: 0.69089496, ae_loss: 0.05365922\n",
      "Step: [1596] total_loss: 2.13420153 d_loss: 1.38519073, g_loss: 0.69658566, ae_loss: 0.05242521\n",
      "Step: [1597] total_loss: 2.12931609 d_loss: 1.37986040, g_loss: 0.69918889, ae_loss: 0.05026689\n",
      "Step: [1598] total_loss: 2.11657643 d_loss: 1.37696922, g_loss: 0.68701613, ae_loss: 0.05259113\n",
      "Step: [1599] total_loss: 2.14892650 d_loss: 1.39692187, g_loss: 0.70022613, ae_loss: 0.05177851\n",
      "Step: [1600] total_loss: 2.13992572 d_loss: 1.40998518, g_loss: 0.67782545, ae_loss: 0.05211506\n",
      "Step: [1601] total_loss: 2.13180590 d_loss: 1.38245034, g_loss: 0.69762790, ae_loss: 0.05172749\n",
      "Step: [1602] total_loss: 2.10705376 d_loss: 1.36403728, g_loss: 0.69302475, ae_loss: 0.04999177\n",
      "Step: [1603] total_loss: 2.13038898 d_loss: 1.39414835, g_loss: 0.68593442, ae_loss: 0.05030621\n",
      "Step: [1604] total_loss: 2.12506390 d_loss: 1.38613033, g_loss: 0.68734837, ae_loss: 0.05158517\n",
      "Step: [1605] total_loss: 2.14074469 d_loss: 1.38375163, g_loss: 0.70466006, ae_loss: 0.05233293\n",
      "Step: [1606] total_loss: 2.15140533 d_loss: 1.38145697, g_loss: 0.71317697, ae_loss: 0.05677145\n",
      "Step: [1607] total_loss: 2.12678957 d_loss: 1.36350989, g_loss: 0.71002960, ae_loss: 0.05325004\n",
      "Step: [1608] total_loss: 2.12459517 d_loss: 1.36132073, g_loss: 0.70743030, ae_loss: 0.05584404\n",
      "Step: [1609] total_loss: 2.14077520 d_loss: 1.40320134, g_loss: 0.68221068, ae_loss: 0.05536332\n",
      "Step: [1610] total_loss: 2.13220882 d_loss: 1.38193393, g_loss: 0.69852042, ae_loss: 0.05175454\n",
      "Step: [1611] total_loss: 2.13037586 d_loss: 1.36950755, g_loss: 0.70794570, ae_loss: 0.05292271\n",
      "Step: [1612] total_loss: 2.13969731 d_loss: 1.37544179, g_loss: 0.70265102, ae_loss: 0.06160450\n",
      "Step: [1613] total_loss: 2.10992885 d_loss: 1.35872269, g_loss: 0.69932288, ae_loss: 0.05188327\n",
      "Step: [1614] total_loss: 2.10900903 d_loss: 1.37640882, g_loss: 0.67939389, ae_loss: 0.05320637\n",
      "Step: [1615] total_loss: 2.11908579 d_loss: 1.35423815, g_loss: 0.71265608, ae_loss: 0.05219164\n",
      "Step: [1616] total_loss: 2.14958954 d_loss: 1.41657305, g_loss: 0.67770398, ae_loss: 0.05531254\n",
      "Step: [1617] total_loss: 2.13561153 d_loss: 1.38960969, g_loss: 0.69439852, ae_loss: 0.05160318\n",
      "Step: [1618] total_loss: 2.14807987 d_loss: 1.39360619, g_loss: 0.70166433, ae_loss: 0.05280946\n",
      "Step: [1619] total_loss: 2.10421705 d_loss: 1.35290790, g_loss: 0.70030665, ae_loss: 0.05100249\n",
      "Step: [1620] total_loss: 2.11143470 d_loss: 1.37447143, g_loss: 0.68208021, ae_loss: 0.05488299\n",
      "Step: [1621] total_loss: 2.11770868 d_loss: 1.36431575, g_loss: 0.70345342, ae_loss: 0.04993964\n",
      "Step: [1622] total_loss: 2.12847137 d_loss: 1.41765237, g_loss: 0.65873772, ae_loss: 0.05208137\n",
      "Step: [1623] total_loss: 2.10793066 d_loss: 1.36932075, g_loss: 0.68461430, ae_loss: 0.05399544\n",
      "Step: [1624] total_loss: 2.15991712 d_loss: 1.39655769, g_loss: 0.71155012, ae_loss: 0.05180931\n",
      "Step: [1625] total_loss: 2.13938332 d_loss: 1.39241433, g_loss: 0.69314116, ae_loss: 0.05382780\n",
      "Step: [1626] total_loss: 2.15587139 d_loss: 1.38073063, g_loss: 0.72002912, ae_loss: 0.05511166\n",
      "Step: [1627] total_loss: 2.15118504 d_loss: 1.38685846, g_loss: 0.70990908, ae_loss: 0.05441745\n",
      "Step: [1628] total_loss: 2.11241484 d_loss: 1.36142612, g_loss: 0.70234901, ae_loss: 0.04863976\n",
      "Step: [1629] total_loss: 2.15402031 d_loss: 1.38724399, g_loss: 0.71387035, ae_loss: 0.05290591\n",
      "Step: [1630] total_loss: 2.14499545 d_loss: 1.36012721, g_loss: 0.73225027, ae_loss: 0.05261788\n",
      "Step: [1631] total_loss: 2.13404131 d_loss: 1.39601994, g_loss: 0.68647546, ae_loss: 0.05154585\n",
      "Step: [1632] total_loss: 2.12620258 d_loss: 1.38871074, g_loss: 0.68756872, ae_loss: 0.04992305\n",
      "Step: [1633] total_loss: 2.13018560 d_loss: 1.38368988, g_loss: 0.69454616, ae_loss: 0.05194960\n",
      "Step: [1634] total_loss: 2.12039828 d_loss: 1.38273144, g_loss: 0.68713403, ae_loss: 0.05053282\n",
      "Step: [1635] total_loss: 2.13588953 d_loss: 1.39231622, g_loss: 0.69263589, ae_loss: 0.05093743\n",
      "Step: [1636] total_loss: 2.12486124 d_loss: 1.39066172, g_loss: 0.67899400, ae_loss: 0.05520558\n",
      "Step: [1637] total_loss: 2.13080978 d_loss: 1.38119888, g_loss: 0.69392204, ae_loss: 0.05568891\n",
      "Step: [1638] total_loss: 2.10979748 d_loss: 1.36555028, g_loss: 0.69203800, ae_loss: 0.05220931\n",
      "Step: [1639] total_loss: 2.12680101 d_loss: 1.39920545, g_loss: 0.67730141, ae_loss: 0.05029399\n",
      "Step: [1640] total_loss: 2.13372564 d_loss: 1.40958178, g_loss: 0.67311954, ae_loss: 0.05102417\n",
      "Step: [1641] total_loss: 2.12831855 d_loss: 1.37871051, g_loss: 0.69568479, ae_loss: 0.05392322\n",
      "Step: [1642] total_loss: 2.12636518 d_loss: 1.38691831, g_loss: 0.69547808, ae_loss: 0.04396890\n",
      "Step: [1643] total_loss: 2.14723206 d_loss: 1.38988769, g_loss: 0.70589232, ae_loss: 0.05145199\n",
      "Step: [1644] total_loss: 2.11751318 d_loss: 1.37746584, g_loss: 0.69221234, ae_loss: 0.04783508\n",
      "Step: [1645] total_loss: 2.12082601 d_loss: 1.36792803, g_loss: 0.69762081, ae_loss: 0.05527712\n",
      "Step: [1646] total_loss: 2.14573574 d_loss: 1.39195514, g_loss: 0.70200336, ae_loss: 0.05177722\n",
      "Step: [1647] total_loss: 2.11536002 d_loss: 1.37802219, g_loss: 0.68833792, ae_loss: 0.04899995\n",
      "Step: [1648] total_loss: 2.11661720 d_loss: 1.38189769, g_loss: 0.68502176, ae_loss: 0.04969770\n",
      "Step: [1649] total_loss: 2.10545921 d_loss: 1.37513542, g_loss: 0.67523950, ae_loss: 0.05508418\n",
      "Step: [1650] total_loss: 2.12244368 d_loss: 1.34900343, g_loss: 0.71772039, ae_loss: 0.05572002\n",
      "Step: [1651] total_loss: 2.09304523 d_loss: 1.34908473, g_loss: 0.68973053, ae_loss: 0.05423000\n",
      "Step: [1652] total_loss: 2.12927866 d_loss: 1.38967943, g_loss: 0.68703938, ae_loss: 0.05255999\n",
      "Step: [1653] total_loss: 2.11061478 d_loss: 1.37268615, g_loss: 0.68642104, ae_loss: 0.05150746\n",
      "Step: [1654] total_loss: 2.12729192 d_loss: 1.38659585, g_loss: 0.68938988, ae_loss: 0.05130625\n",
      "Step: [1655] total_loss: 2.12763023 d_loss: 1.36925602, g_loss: 0.70143509, ae_loss: 0.05693898\n",
      "Step: [1656] total_loss: 2.14927268 d_loss: 1.39784074, g_loss: 0.69865513, ae_loss: 0.05277684\n",
      "Step: [1657] total_loss: 2.12200260 d_loss: 1.37806678, g_loss: 0.69571936, ae_loss: 0.04821656\n",
      "Step: [1658] total_loss: 2.13864064 d_loss: 1.35295248, g_loss: 0.73344529, ae_loss: 0.05224289\n",
      "Step: [1659] total_loss: 2.14712572 d_loss: 1.36185169, g_loss: 0.73433590, ae_loss: 0.05093821\n",
      "Step: [1660] total_loss: 2.13430142 d_loss: 1.38303733, g_loss: 0.70402604, ae_loss: 0.04723815\n",
      "Step: [1661] total_loss: 2.12311864 d_loss: 1.36928070, g_loss: 0.70364273, ae_loss: 0.05019522\n",
      "Step: [1662] total_loss: 2.12118053 d_loss: 1.37052572, g_loss: 0.69884080, ae_loss: 0.05181405\n",
      "Step: [1663] total_loss: 2.11728954 d_loss: 1.36896586, g_loss: 0.69662046, ae_loss: 0.05170323\n",
      "Step: [1664] total_loss: 2.12112117 d_loss: 1.38174534, g_loss: 0.68498045, ae_loss: 0.05439534\n",
      "Step: [1665] total_loss: 2.10024691 d_loss: 1.38041449, g_loss: 0.67009687, ae_loss: 0.04973546\n",
      "Step: [1666] total_loss: 2.11593056 d_loss: 1.37506008, g_loss: 0.69039685, ae_loss: 0.05047373\n",
      "Step: [1667] total_loss: 2.11680079 d_loss: 1.38418794, g_loss: 0.67691195, ae_loss: 0.05570106\n",
      "Step: [1668] total_loss: 2.14986181 d_loss: 1.40823877, g_loss: 0.68297750, ae_loss: 0.05864546\n",
      "Step: [1669] total_loss: 2.14782238 d_loss: 1.40925431, g_loss: 0.68740833, ae_loss: 0.05115964\n",
      "Step: [1670] total_loss: 2.11895466 d_loss: 1.34884763, g_loss: 0.71887660, ae_loss: 0.05123048\n",
      "Step: [1671] total_loss: 2.15127254 d_loss: 1.39106953, g_loss: 0.70764303, ae_loss: 0.05256002\n",
      "Step: [1672] total_loss: 2.15106869 d_loss: 1.39820600, g_loss: 0.70132554, ae_loss: 0.05153707\n",
      "Step: [1673] total_loss: 2.14477253 d_loss: 1.39259601, g_loss: 0.69920945, ae_loss: 0.05296712\n",
      "Step: [1674] total_loss: 2.13743114 d_loss: 1.38923979, g_loss: 0.69462621, ae_loss: 0.05356516\n",
      "Step: [1675] total_loss: 2.15225530 d_loss: 1.37174392, g_loss: 0.72636271, ae_loss: 0.05414873\n",
      "Step: [1676] total_loss: 2.11961174 d_loss: 1.38227010, g_loss: 0.68796098, ae_loss: 0.04938070\n",
      "Step: [1677] total_loss: 2.12595034 d_loss: 1.37036693, g_loss: 0.70252919, ae_loss: 0.05305437\n",
      "Step: [1678] total_loss: 2.11573648 d_loss: 1.35194039, g_loss: 0.70963937, ae_loss: 0.05415668\n",
      "Step: [1679] total_loss: 2.13989401 d_loss: 1.38955331, g_loss: 0.69592470, ae_loss: 0.05441590\n",
      "Step: [1680] total_loss: 2.14305258 d_loss: 1.36550236, g_loss: 0.72451204, ae_loss: 0.05303823\n",
      "Step: [1681] total_loss: 2.09280992 d_loss: 1.33898497, g_loss: 0.70247275, ae_loss: 0.05135227\n",
      "Step: [1682] total_loss: 2.10628033 d_loss: 1.36471593, g_loss: 0.68951035, ae_loss: 0.05205419\n",
      "Step: [1683] total_loss: 2.10410523 d_loss: 1.38837552, g_loss: 0.66167033, ae_loss: 0.05405943\n",
      "Step: [1684] total_loss: 2.11945772 d_loss: 1.38231051, g_loss: 0.68424243, ae_loss: 0.05290468\n",
      "Step: [1685] total_loss: 2.12495422 d_loss: 1.39010310, g_loss: 0.67951059, ae_loss: 0.05534068\n",
      "Step: [1686] total_loss: 2.14444065 d_loss: 1.41806448, g_loss: 0.67374408, ae_loss: 0.05263191\n",
      "Step: [1687] total_loss: 2.14500284 d_loss: 1.38484383, g_loss: 0.70731544, ae_loss: 0.05284363\n",
      "Step: [1688] total_loss: 2.14497590 d_loss: 1.38998055, g_loss: 0.69723660, ae_loss: 0.05775866\n",
      "Step: [1689] total_loss: 2.13125992 d_loss: 1.38304663, g_loss: 0.69851422, ae_loss: 0.04969911\n",
      "Step: [1690] total_loss: 2.14217067 d_loss: 1.38918376, g_loss: 0.70181066, ae_loss: 0.05117635\n",
      "Step: [1691] total_loss: 2.13394666 d_loss: 1.37792766, g_loss: 0.70702446, ae_loss: 0.04899458\n",
      "Step: [1692] total_loss: 2.13599110 d_loss: 1.38008189, g_loss: 0.70460939, ae_loss: 0.05129976\n",
      "Step: [1693] total_loss: 2.12402153 d_loss: 1.37917888, g_loss: 0.69002438, ae_loss: 0.05481835\n",
      "Step: [1694] total_loss: 2.12531352 d_loss: 1.36028469, g_loss: 0.70913601, ae_loss: 0.05589278\n",
      "Step: [1695] total_loss: 2.12426329 d_loss: 1.37801862, g_loss: 0.69342244, ae_loss: 0.05282225\n",
      "Step: [1696] total_loss: 2.13202786 d_loss: 1.37091875, g_loss: 0.70310223, ae_loss: 0.05800687\n",
      "Step: [1697] total_loss: 2.13335323 d_loss: 1.40680540, g_loss: 0.67576230, ae_loss: 0.05078565\n",
      "Step: [1698] total_loss: 2.10739207 d_loss: 1.36645889, g_loss: 0.69286370, ae_loss: 0.04806951\n",
      "Step: [1699] total_loss: 2.11753583 d_loss: 1.38291287, g_loss: 0.68223310, ae_loss: 0.05238984\n",
      "Step: [1700] total_loss: 2.12600279 d_loss: 1.38254094, g_loss: 0.69475603, ae_loss: 0.04870583\n",
      "Step: [1701] total_loss: 2.11926794 d_loss: 1.37230802, g_loss: 0.69000912, ae_loss: 0.05695076\n",
      "Step: [1702] total_loss: 2.13606644 d_loss: 1.39348674, g_loss: 0.69021857, ae_loss: 0.05236128\n",
      "Step: [1703] total_loss: 2.15145540 d_loss: 1.37847221, g_loss: 0.71978104, ae_loss: 0.05320201\n",
      "Step: [1704] total_loss: 2.12378311 d_loss: 1.38266504, g_loss: 0.68784720, ae_loss: 0.05327094\n",
      "Step: [1705] total_loss: 2.12908506 d_loss: 1.38289106, g_loss: 0.68864381, ae_loss: 0.05755009\n",
      "Step: [1706] total_loss: 2.13065815 d_loss: 1.37311530, g_loss: 0.70589334, ae_loss: 0.05164953\n",
      "Step: [1707] total_loss: 2.13091183 d_loss: 1.39585543, g_loss: 0.68427956, ae_loss: 0.05077691\n",
      "Step: [1708] total_loss: 2.13669872 d_loss: 1.38765740, g_loss: 0.69214123, ae_loss: 0.05690001\n",
      "Step: [1709] total_loss: 2.13352346 d_loss: 1.39719355, g_loss: 0.68322295, ae_loss: 0.05310689\n",
      "Step: [1710] total_loss: 2.14263940 d_loss: 1.40030360, g_loss: 0.69001961, ae_loss: 0.05231619\n",
      "Step: [1711] total_loss: 2.10834265 d_loss: 1.36454952, g_loss: 0.68869966, ae_loss: 0.05509338\n",
      "Step: [1712] total_loss: 2.13222694 d_loss: 1.38672495, g_loss: 0.69651330, ae_loss: 0.04898858\n",
      "Step: [1713] total_loss: 2.12953854 d_loss: 1.38213658, g_loss: 0.69270271, ae_loss: 0.05469935\n",
      "Step: [1714] total_loss: 2.11519814 d_loss: 1.35592675, g_loss: 0.70589018, ae_loss: 0.05338116\n",
      "Step: [1715] total_loss: 2.12806511 d_loss: 1.38518310, g_loss: 0.68909419, ae_loss: 0.05378801\n",
      "Step: [1716] total_loss: 2.11467457 d_loss: 1.38555598, g_loss: 0.67707658, ae_loss: 0.05204212\n",
      "Step: [1717] total_loss: 2.12612009 d_loss: 1.39642930, g_loss: 0.67604554, ae_loss: 0.05364510\n",
      "Step: [1718] total_loss: 2.12650323 d_loss: 1.40623987, g_loss: 0.66842210, ae_loss: 0.05184129\n",
      "Step: [1719] total_loss: 2.14051294 d_loss: 1.39904833, g_loss: 0.69063342, ae_loss: 0.05083132\n",
      "Step: [1720] total_loss: 2.14484596 d_loss: 1.38680673, g_loss: 0.70504522, ae_loss: 0.05299408\n",
      "Step: [1721] total_loss: 2.12862468 d_loss: 1.38833213, g_loss: 0.68722367, ae_loss: 0.05306885\n",
      "Step: [1722] total_loss: 2.11794996 d_loss: 1.37825060, g_loss: 0.68611324, ae_loss: 0.05358609\n",
      "Step: [1723] total_loss: 2.12243462 d_loss: 1.37814307, g_loss: 0.69379973, ae_loss: 0.05049197\n",
      "Step: [1724] total_loss: 2.13200855 d_loss: 1.37126803, g_loss: 0.71056736, ae_loss: 0.05017319\n",
      "Step: [1725] total_loss: 2.13842916 d_loss: 1.39145255, g_loss: 0.69505191, ae_loss: 0.05192485\n",
      "Step: [1726] total_loss: 2.14543915 d_loss: 1.39874840, g_loss: 0.69342917, ae_loss: 0.05326167\n",
      "Step: [1727] total_loss: 2.12921882 d_loss: 1.38252306, g_loss: 0.69676661, ae_loss: 0.04992915\n",
      "Step: [1728] total_loss: 2.11207700 d_loss: 1.37681925, g_loss: 0.68408692, ae_loss: 0.05117084\n",
      "Step: [1729] total_loss: 2.10552096 d_loss: 1.36025286, g_loss: 0.69468814, ae_loss: 0.05057992\n",
      "Step: [1730] total_loss: 2.11704922 d_loss: 1.38403559, g_loss: 0.67823303, ae_loss: 0.05478060\n",
      "Step: [1731] total_loss: 2.12294483 d_loss: 1.38689137, g_loss: 0.68272424, ae_loss: 0.05332927\n",
      "Step: [1732] total_loss: 2.13530588 d_loss: 1.38982892, g_loss: 0.69115132, ae_loss: 0.05432555\n",
      "Step: [1733] total_loss: 2.13401103 d_loss: 1.38287222, g_loss: 0.70001376, ae_loss: 0.05112501\n",
      "Step: [1734] total_loss: 2.14255905 d_loss: 1.40576720, g_loss: 0.68602109, ae_loss: 0.05077081\n",
      "Step: [1735] total_loss: 2.14040208 d_loss: 1.37608540, g_loss: 0.71594101, ae_loss: 0.04837575\n",
      "Step: [1736] total_loss: 2.14287996 d_loss: 1.39523363, g_loss: 0.69810128, ae_loss: 0.04954508\n",
      "Step: [1737] total_loss: 2.14333391 d_loss: 1.37292027, g_loss: 0.71668679, ae_loss: 0.05372679\n",
      "Step: [1738] total_loss: 2.12150002 d_loss: 1.37612855, g_loss: 0.69102395, ae_loss: 0.05434758\n",
      "Step: [1739] total_loss: 2.13490152 d_loss: 1.36706066, g_loss: 0.71457976, ae_loss: 0.05326098\n",
      "Step: [1740] total_loss: 2.12978220 d_loss: 1.39783812, g_loss: 0.68214208, ae_loss: 0.04980206\n",
      "Step: [1741] total_loss: 2.10783482 d_loss: 1.34583426, g_loss: 0.70726728, ae_loss: 0.05473327\n",
      "Step: [1742] total_loss: 2.12037063 d_loss: 1.38406682, g_loss: 0.68436784, ae_loss: 0.05193593\n",
      "Step: [1743] total_loss: 2.13277578 d_loss: 1.40129352, g_loss: 0.68170357, ae_loss: 0.04977886\n",
      "Step: [1744] total_loss: 2.12563825 d_loss: 1.38062763, g_loss: 0.69355357, ae_loss: 0.05145704\n",
      "Step: [1745] total_loss: 2.15710258 d_loss: 1.39600694, g_loss: 0.70792222, ae_loss: 0.05317357\n",
      "Step: [1746] total_loss: 2.12370539 d_loss: 1.37390494, g_loss: 0.70152992, ae_loss: 0.04827061\n",
      "Step: [1747] total_loss: 2.13890290 d_loss: 1.38319540, g_loss: 0.70409763, ae_loss: 0.05160983\n",
      "Step: [1748] total_loss: 2.12902927 d_loss: 1.38674307, g_loss: 0.69242036, ae_loss: 0.04986579\n",
      "Step: [1749] total_loss: 2.13568759 d_loss: 1.39895666, g_loss: 0.68637985, ae_loss: 0.05035108\n",
      "Step: [1750] total_loss: 2.12964582 d_loss: 1.39361262, g_loss: 0.68496281, ae_loss: 0.05107031\n",
      "Step: [1751] total_loss: 2.11030388 d_loss: 1.35193932, g_loss: 0.70616674, ae_loss: 0.05219775\n",
      "Step: [1752] total_loss: 2.12417221 d_loss: 1.39073932, g_loss: 0.68669713, ae_loss: 0.04673578\n",
      "Step: [1753] total_loss: 2.13205838 d_loss: 1.36208296, g_loss: 0.71719027, ae_loss: 0.05278510\n",
      "Step: [1754] total_loss: 2.12597513 d_loss: 1.36972988, g_loss: 0.70661914, ae_loss: 0.04962612\n",
      "Step: [1755] total_loss: 2.12886429 d_loss: 1.39675117, g_loss: 0.68163097, ae_loss: 0.05048210\n",
      "Step: [1756] total_loss: 2.12777424 d_loss: 1.38510370, g_loss: 0.69131923, ae_loss: 0.05135122\n",
      "Step: [1757] total_loss: 2.12402558 d_loss: 1.36250949, g_loss: 0.71048892, ae_loss: 0.05102715\n",
      "Step: [1758] total_loss: 2.12363625 d_loss: 1.37402654, g_loss: 0.69635606, ae_loss: 0.05325376\n",
      "Step: [1759] total_loss: 2.12570691 d_loss: 1.37681186, g_loss: 0.69873500, ae_loss: 0.05015999\n",
      "Step: [1760] total_loss: 2.12798572 d_loss: 1.39541543, g_loss: 0.67481279, ae_loss: 0.05775750\n",
      "Step: [1761] total_loss: 2.12234879 d_loss: 1.37768197, g_loss: 0.68755281, ae_loss: 0.05711406\n",
      "Step: [1762] total_loss: 2.11266375 d_loss: 1.38662291, g_loss: 0.67754722, ae_loss: 0.04849345\n",
      "Step: [1763] total_loss: 2.10963583 d_loss: 1.36886239, g_loss: 0.68621016, ae_loss: 0.05456311\n",
      "Step: [1764] total_loss: 2.10961461 d_loss: 1.36063683, g_loss: 0.70002079, ae_loss: 0.04895697\n",
      "Step: [1765] total_loss: 2.11010599 d_loss: 1.38461328, g_loss: 0.67505586, ae_loss: 0.05043686\n",
      "Step: [1766] total_loss: 2.12279773 d_loss: 1.38190746, g_loss: 0.69112843, ae_loss: 0.04976185\n",
      "Step: [1767] total_loss: 2.12023544 d_loss: 1.36129117, g_loss: 0.70466042, ae_loss: 0.05428401\n",
      "Step: [1768] total_loss: 2.12791014 d_loss: 1.38487744, g_loss: 0.68961895, ae_loss: 0.05341361\n",
      "Step: [1769] total_loss: 2.12825489 d_loss: 1.37301981, g_loss: 0.70259452, ae_loss: 0.05264046\n",
      "Step: [1770] total_loss: 2.13823509 d_loss: 1.38816881, g_loss: 0.69473255, ae_loss: 0.05533389\n",
      "Step: [1771] total_loss: 2.12649107 d_loss: 1.37232339, g_loss: 0.70317751, ae_loss: 0.05099006\n",
      "Step: [1772] total_loss: 2.13499355 d_loss: 1.40432811, g_loss: 0.67790592, ae_loss: 0.05275958\n",
      "Step: [1773] total_loss: 2.12178898 d_loss: 1.35969162, g_loss: 0.71139479, ae_loss: 0.05070251\n",
      "Step: [1774] total_loss: 2.11714792 d_loss: 1.36654449, g_loss: 0.69970351, ae_loss: 0.05089988\n",
      "Step: [1775] total_loss: 2.11067224 d_loss: 1.36338866, g_loss: 0.69784021, ae_loss: 0.04944330\n",
      "Step: [1776] total_loss: 2.11218214 d_loss: 1.36370468, g_loss: 0.69909000, ae_loss: 0.04938751\n",
      "Step: [1777] total_loss: 2.14758515 d_loss: 1.36637628, g_loss: 0.73134547, ae_loss: 0.04986339\n",
      "Step: [1778] total_loss: 2.11833549 d_loss: 1.35339284, g_loss: 0.71336925, ae_loss: 0.05157337\n",
      "Step: [1779] total_loss: 2.10424399 d_loss: 1.35194564, g_loss: 0.70272559, ae_loss: 0.04957285\n",
      "Step: [1780] total_loss: 2.14979434 d_loss: 1.37240505, g_loss: 0.72348511, ae_loss: 0.05390423\n",
      "Step: [1781] total_loss: 2.13044214 d_loss: 1.39228392, g_loss: 0.68744266, ae_loss: 0.05071567\n",
      "Step: [1782] total_loss: 2.14453125 d_loss: 1.39218843, g_loss: 0.69867158, ae_loss: 0.05367125\n",
      "Step: [1783] total_loss: 2.11742687 d_loss: 1.37514973, g_loss: 0.69415230, ae_loss: 0.04812488\n",
      "Step: [1784] total_loss: 2.11712980 d_loss: 1.34968746, g_loss: 0.71452701, ae_loss: 0.05291532\n",
      "Step: [1785] total_loss: 2.14862633 d_loss: 1.37990344, g_loss: 0.71499646, ae_loss: 0.05372640\n",
      "Step: [1786] total_loss: 2.12531996 d_loss: 1.38550985, g_loss: 0.68890750, ae_loss: 0.05090249\n",
      "Step: [1787] total_loss: 2.12926197 d_loss: 1.38322401, g_loss: 0.70027447, ae_loss: 0.04576332\n",
      "Step: [1788] total_loss: 2.10817385 d_loss: 1.35248065, g_loss: 0.70283681, ae_loss: 0.05285643\n",
      "Step: [1789] total_loss: 2.13824368 d_loss: 1.39919496, g_loss: 0.68232840, ae_loss: 0.05672040\n",
      "Step: [1790] total_loss: 2.12776613 d_loss: 1.36134005, g_loss: 0.71423835, ae_loss: 0.05218772\n",
      "Step: [1791] total_loss: 2.14359903 d_loss: 1.36406457, g_loss: 0.72808176, ae_loss: 0.05145266\n",
      "Step: [1792] total_loss: 2.13202238 d_loss: 1.37985158, g_loss: 0.70262414, ae_loss: 0.04954666\n",
      "Step: [1793] total_loss: 2.15288925 d_loss: 1.40280366, g_loss: 0.69972348, ae_loss: 0.05036196\n",
      "Step: [1794] total_loss: 2.15988827 d_loss: 1.42428541, g_loss: 0.68200529, ae_loss: 0.05359769\n",
      "Step: [1795] total_loss: 2.12841606 d_loss: 1.38134193, g_loss: 0.69507128, ae_loss: 0.05200281\n",
      "Step: [1796] total_loss: 2.09547329 d_loss: 1.33828735, g_loss: 0.70512938, ae_loss: 0.05205663\n",
      "Step: [1797] total_loss: 2.15985107 d_loss: 1.40081358, g_loss: 0.70431966, ae_loss: 0.05471786\n",
      "Step: [1798] total_loss: 2.13226080 d_loss: 1.38535714, g_loss: 0.70004869, ae_loss: 0.04685489\n",
      "Step: [1799] total_loss: 2.11620998 d_loss: 1.39017415, g_loss: 0.67313337, ae_loss: 0.05290246\n",
      "Step: [1800] total_loss: 2.13372397 d_loss: 1.38213563, g_loss: 0.69915414, ae_loss: 0.05243423\n",
      "Step: [1801] total_loss: 2.13714933 d_loss: 1.38712358, g_loss: 0.69982576, ae_loss: 0.05020015\n",
      "Step: [1802] total_loss: 2.12834883 d_loss: 1.38705707, g_loss: 0.68884969, ae_loss: 0.05244220\n",
      "Step: [1803] total_loss: 2.14226103 d_loss: 1.38483667, g_loss: 0.70215583, ae_loss: 0.05526855\n",
      "Step: [1804] total_loss: 2.12373447 d_loss: 1.38133228, g_loss: 0.68902910, ae_loss: 0.05337299\n",
      "Step: [1805] total_loss: 2.11919284 d_loss: 1.37492359, g_loss: 0.69280630, ae_loss: 0.05146284\n",
      "Step: [1806] total_loss: 2.09731889 d_loss: 1.36564171, g_loss: 0.68168604, ae_loss: 0.04999113\n",
      "Step: [1807] total_loss: 2.12845325 d_loss: 1.38327718, g_loss: 0.69101560, ae_loss: 0.05416058\n",
      "Step: [1808] total_loss: 2.13082743 d_loss: 1.38236082, g_loss: 0.69821227, ae_loss: 0.05025429\n",
      "Step: [1809] total_loss: 2.13593245 d_loss: 1.38907623, g_loss: 0.69290614, ae_loss: 0.05394992\n",
      "Step: [1810] total_loss: 2.12828445 d_loss: 1.38909352, g_loss: 0.68853921, ae_loss: 0.05065162\n",
      "Step: [1811] total_loss: 2.10344601 d_loss: 1.37665915, g_loss: 0.67563772, ae_loss: 0.05114899\n",
      "Step: [1812] total_loss: 2.13249278 d_loss: 1.37901139, g_loss: 0.70320904, ae_loss: 0.05027230\n",
      "Step: [1813] total_loss: 2.14249015 d_loss: 1.36803162, g_loss: 0.72464961, ae_loss: 0.04980899\n",
      "Step: [1814] total_loss: 2.14657688 d_loss: 1.39015532, g_loss: 0.70501482, ae_loss: 0.05140656\n",
      "Step: [1815] total_loss: 2.15302229 d_loss: 1.38473403, g_loss: 0.71734273, ae_loss: 0.05094542\n",
      "Step: [1816] total_loss: 2.16339254 d_loss: 1.39742303, g_loss: 0.71261656, ae_loss: 0.05335287\n",
      "Step: [1817] total_loss: 2.12820339 d_loss: 1.37663054, g_loss: 0.69600689, ae_loss: 0.05556580\n",
      "Step: [1818] total_loss: 2.11858797 d_loss: 1.35862565, g_loss: 0.70909810, ae_loss: 0.05086430\n",
      "Step: [1819] total_loss: 2.14282084 d_loss: 1.38503313, g_loss: 0.70572710, ae_loss: 0.05206067\n",
      "Step: [1820] total_loss: 2.13723946 d_loss: 1.36373258, g_loss: 0.72575080, ae_loss: 0.04775605\n",
      "Step: [1821] total_loss: 2.13516951 d_loss: 1.40530062, g_loss: 0.67917824, ae_loss: 0.05069051\n",
      "Step: [1822] total_loss: 2.12291431 d_loss: 1.36737323, g_loss: 0.70480263, ae_loss: 0.05073839\n",
      "Step: [1823] total_loss: 2.13867760 d_loss: 1.38802242, g_loss: 0.69746542, ae_loss: 0.05318964\n",
      "Step: [1824] total_loss: 2.14690375 d_loss: 1.38240421, g_loss: 0.71154326, ae_loss: 0.05295628\n",
      "Step: [1825] total_loss: 2.12367916 d_loss: 1.37892985, g_loss: 0.69319570, ae_loss: 0.05155376\n",
      "Step: [1826] total_loss: 2.11645555 d_loss: 1.36277723, g_loss: 0.69996071, ae_loss: 0.05371755\n",
      "Step: [1827] total_loss: 2.12457037 d_loss: 1.39019823, g_loss: 0.68176901, ae_loss: 0.05260304\n",
      "Step: [1828] total_loss: 2.13479114 d_loss: 1.39586675, g_loss: 0.68412948, ae_loss: 0.05479489\n",
      "Step: [1829] total_loss: 2.11835718 d_loss: 1.36720431, g_loss: 0.69779253, ae_loss: 0.05336019\n",
      "Step: [1830] total_loss: 2.12713051 d_loss: 1.37716162, g_loss: 0.69212699, ae_loss: 0.05784205\n",
      "Step: [1831] total_loss: 2.14779592 d_loss: 1.40117097, g_loss: 0.69462657, ae_loss: 0.05199841\n",
      "Step: [1832] total_loss: 2.12221098 d_loss: 1.38234568, g_loss: 0.68406272, ae_loss: 0.05580271\n",
      "Step: [1833] total_loss: 2.12522101 d_loss: 1.37738991, g_loss: 0.69068086, ae_loss: 0.05715027\n",
      "Step: [1834] total_loss: 2.14500117 d_loss: 1.40701747, g_loss: 0.68694627, ae_loss: 0.05103738\n",
      "Step: [1835] total_loss: 2.14088297 d_loss: 1.37021780, g_loss: 0.71737432, ae_loss: 0.05329091\n",
      "Step: [1836] total_loss: 2.12954640 d_loss: 1.39368749, g_loss: 0.68686104, ae_loss: 0.04899792\n",
      "Step: [1837] total_loss: 2.12106228 d_loss: 1.38137794, g_loss: 0.68876648, ae_loss: 0.05091791\n",
      "Step: [1838] total_loss: 2.11319137 d_loss: 1.37810183, g_loss: 0.67983335, ae_loss: 0.05525615\n",
      "Step: [1839] total_loss: 2.12776184 d_loss: 1.38636458, g_loss: 0.69009852, ae_loss: 0.05129890\n",
      "Step: [1840] total_loss: 2.12721491 d_loss: 1.39649498, g_loss: 0.67564011, ae_loss: 0.05507995\n",
      "Step: [1841] total_loss: 2.12377429 d_loss: 1.38426328, g_loss: 0.68624675, ae_loss: 0.05326430\n",
      "Step: [1842] total_loss: 2.13481092 d_loss: 1.38074565, g_loss: 0.69741970, ae_loss: 0.05664548\n",
      "Step: [1843] total_loss: 2.14073396 d_loss: 1.38621044, g_loss: 0.70360219, ae_loss: 0.05092127\n",
      "Step: [1844] total_loss: 2.14980507 d_loss: 1.37527657, g_loss: 0.71637094, ae_loss: 0.05815765\n",
      "Step: [1845] total_loss: 2.15192103 d_loss: 1.39090919, g_loss: 0.70860052, ae_loss: 0.05241127\n",
      "Step: [1846] total_loss: 2.14894485 d_loss: 1.38928604, g_loss: 0.70730364, ae_loss: 0.05235501\n",
      "Step: [1847] total_loss: 2.14553237 d_loss: 1.39271116, g_loss: 0.69948542, ae_loss: 0.05333576\n",
      "Step: [1848] total_loss: 2.12735939 d_loss: 1.38229191, g_loss: 0.69267464, ae_loss: 0.05239270\n",
      "Step: [1849] total_loss: 2.12788677 d_loss: 1.38305068, g_loss: 0.69005609, ae_loss: 0.05477996\n",
      "Step: [1850] total_loss: 2.11589956 d_loss: 1.38727021, g_loss: 0.67522424, ae_loss: 0.05340511\n",
      "Step: [1851] total_loss: 2.11021376 d_loss: 1.36670542, g_loss: 0.69138038, ae_loss: 0.05212790\n",
      "Step: [1852] total_loss: 2.11820817 d_loss: 1.37135410, g_loss: 0.69502431, ae_loss: 0.05182980\n",
      "Step: [1853] total_loss: 2.10785341 d_loss: 1.37600183, g_loss: 0.67814469, ae_loss: 0.05370686\n",
      "Step: [1854] total_loss: 2.12428951 d_loss: 1.39141393, g_loss: 0.68288869, ae_loss: 0.04998688\n",
      "Step: [1855] total_loss: 2.12740850 d_loss: 1.38391542, g_loss: 0.68837428, ae_loss: 0.05511879\n",
      "Step: [1856] total_loss: 2.13904381 d_loss: 1.39624977, g_loss: 0.69096816, ae_loss: 0.05182582\n",
      "Step: [1857] total_loss: 2.14754581 d_loss: 1.40319884, g_loss: 0.69366860, ae_loss: 0.05067822\n",
      "Step: [1858] total_loss: 2.14988041 d_loss: 1.39310038, g_loss: 0.70192766, ae_loss: 0.05485242\n",
      "Step: [1859] total_loss: 2.13321424 d_loss: 1.38473451, g_loss: 0.69672585, ae_loss: 0.05175392\n",
      "Step: [1860] total_loss: 2.12894607 d_loss: 1.38429356, g_loss: 0.69259191, ae_loss: 0.05206057\n",
      "Step: [1861] total_loss: 2.13073158 d_loss: 1.38079691, g_loss: 0.69673276, ae_loss: 0.05320177\n",
      "Step: [1862] total_loss: 2.11416054 d_loss: 1.37714386, g_loss: 0.68452728, ae_loss: 0.05248943\n",
      "Step: [1863] total_loss: 2.12463593 d_loss: 1.37523317, g_loss: 0.69803655, ae_loss: 0.05136625\n",
      "Step: [1864] total_loss: 2.14352131 d_loss: 1.37325990, g_loss: 0.71704906, ae_loss: 0.05321238\n",
      "Step: [1865] total_loss: 2.10910225 d_loss: 1.37753451, g_loss: 0.67752719, ae_loss: 0.05404040\n",
      "Step: [1866] total_loss: 2.12284470 d_loss: 1.41268277, g_loss: 0.66160142, ae_loss: 0.04856043\n",
      "Step: [1867] total_loss: 2.11564660 d_loss: 1.38134420, g_loss: 0.68289530, ae_loss: 0.05140707\n",
      "Step: [1868] total_loss: 2.10881972 d_loss: 1.36901116, g_loss: 0.69106972, ae_loss: 0.04873889\n",
      "Step: [1869] total_loss: 2.09786510 d_loss: 1.38566637, g_loss: 0.66027081, ae_loss: 0.05192789\n",
      "Step: [1870] total_loss: 2.12356806 d_loss: 1.37787604, g_loss: 0.69361997, ae_loss: 0.05207188\n",
      "Step: [1871] total_loss: 2.12782192 d_loss: 1.40643084, g_loss: 0.67167079, ae_loss: 0.04972029\n",
      "Step: [1872] total_loss: 2.12912035 d_loss: 1.38405919, g_loss: 0.69438970, ae_loss: 0.05067160\n",
      "Step: [1873] total_loss: 2.13988781 d_loss: 1.38761723, g_loss: 0.69612038, ae_loss: 0.05615008\n",
      "Step: [1874] total_loss: 2.11526871 d_loss: 1.36630130, g_loss: 0.69676721, ae_loss: 0.05220031\n",
      "Step: [1875] total_loss: 2.13625598 d_loss: 1.39500666, g_loss: 0.68223125, ae_loss: 0.05901799\n",
      "Step: [1876] total_loss: 2.12233114 d_loss: 1.36635852, g_loss: 0.70207131, ae_loss: 0.05390133\n",
      "Step: [1877] total_loss: 2.16205788 d_loss: 1.39175653, g_loss: 0.71462953, ae_loss: 0.05567193\n",
      "Step: [1878] total_loss: 2.14147639 d_loss: 1.38072777, g_loss: 0.71332949, ae_loss: 0.04741907\n",
      "Step: [1879] total_loss: 2.14894271 d_loss: 1.38087118, g_loss: 0.71882105, ae_loss: 0.04925054\n",
      "Step: [1880] total_loss: 2.14895964 d_loss: 1.40674508, g_loss: 0.69103360, ae_loss: 0.05118091\n",
      "Step: [1881] total_loss: 2.14633465 d_loss: 1.38124406, g_loss: 0.71129572, ae_loss: 0.05379468\n",
      "Step: [1882] total_loss: 2.14894962 d_loss: 1.38588297, g_loss: 0.71124792, ae_loss: 0.05181886\n",
      "Step: [1883] total_loss: 2.13339138 d_loss: 1.39328313, g_loss: 0.68975151, ae_loss: 0.05035658\n",
      "Step: [1884] total_loss: 2.11175299 d_loss: 1.38287401, g_loss: 0.68108678, ae_loss: 0.04779208\n",
      "Step: [1885] total_loss: 2.12660217 d_loss: 1.37732112, g_loss: 0.69732594, ae_loss: 0.05195509\n",
      "Step: [1886] total_loss: 2.14909983 d_loss: 1.38401437, g_loss: 0.71210051, ae_loss: 0.05298486\n",
      "Step: [1887] total_loss: 2.14534545 d_loss: 1.38437676, g_loss: 0.71006680, ae_loss: 0.05090194\n",
      "Step: [1888] total_loss: 2.13300276 d_loss: 1.37304008, g_loss: 0.70230877, ae_loss: 0.05765381\n",
      "Step: [1889] total_loss: 2.12702370 d_loss: 1.37025094, g_loss: 0.70190364, ae_loss: 0.05486905\n",
      "Step: [1890] total_loss: 2.10960889 d_loss: 1.36216187, g_loss: 0.69837594, ae_loss: 0.04907104\n",
      "Step: [1891] total_loss: 2.12516451 d_loss: 1.37957537, g_loss: 0.69073462, ae_loss: 0.05485441\n",
      "Step: [1892] total_loss: 2.11828947 d_loss: 1.34123945, g_loss: 0.72494155, ae_loss: 0.05210849\n",
      "Step: [1893] total_loss: 2.10815334 d_loss: 1.35049844, g_loss: 0.70714271, ae_loss: 0.05051230\n",
      "Step: [1894] total_loss: 2.09313107 d_loss: 1.36893404, g_loss: 0.67491466, ae_loss: 0.04928238\n",
      "Step: [1895] total_loss: 2.11160231 d_loss: 1.38804507, g_loss: 0.67229128, ae_loss: 0.05126596\n",
      "Step: [1896] total_loss: 2.11646652 d_loss: 1.34679604, g_loss: 0.71794832, ae_loss: 0.05172219\n",
      "Step: [1897] total_loss: 2.10367060 d_loss: 1.36662209, g_loss: 0.68501496, ae_loss: 0.05203359\n",
      "Step: [1898] total_loss: 2.12256336 d_loss: 1.37796974, g_loss: 0.69186306, ae_loss: 0.05273064\n",
      "Step: [1899] total_loss: 2.11559105 d_loss: 1.37226796, g_loss: 0.69146264, ae_loss: 0.05186037\n",
      "Step: [1900] total_loss: 2.12313890 d_loss: 1.37522459, g_loss: 0.69635189, ae_loss: 0.05156230\n",
      "Step: [1901] total_loss: 2.11499858 d_loss: 1.35218108, g_loss: 0.70921105, ae_loss: 0.05360645\n",
      "Step: [1902] total_loss: 2.11622953 d_loss: 1.37418163, g_loss: 0.69336236, ae_loss: 0.04868571\n",
      "Step: [1903] total_loss: 2.12575245 d_loss: 1.37441051, g_loss: 0.70101511, ae_loss: 0.05032695\n",
      "Step: [1904] total_loss: 2.12338519 d_loss: 1.36514080, g_loss: 0.70600855, ae_loss: 0.05223587\n",
      "Step: [1905] total_loss: 2.13151646 d_loss: 1.37415314, g_loss: 0.70351142, ae_loss: 0.05385187\n",
      "Step: [1906] total_loss: 2.16167974 d_loss: 1.38538563, g_loss: 0.72139597, ae_loss: 0.05489801\n",
      "Step: [1907] total_loss: 2.14301157 d_loss: 1.39737868, g_loss: 0.69415832, ae_loss: 0.05147464\n",
      "Step: [1908] total_loss: 2.13310146 d_loss: 1.36226416, g_loss: 0.71198779, ae_loss: 0.05884962\n",
      "Step: [1909] total_loss: 2.12592220 d_loss: 1.37355149, g_loss: 0.69520962, ae_loss: 0.05716112\n",
      "Step: [1910] total_loss: 2.14840364 d_loss: 1.40167618, g_loss: 0.68905914, ae_loss: 0.05766837\n",
      "Step: [1911] total_loss: 2.11959386 d_loss: 1.37625194, g_loss: 0.68856579, ae_loss: 0.05477617\n",
      "Step: [1912] total_loss: 2.14509010 d_loss: 1.40177381, g_loss: 0.68964815, ae_loss: 0.05366818\n",
      "Step: [1913] total_loss: 2.12778854 d_loss: 1.37359977, g_loss: 0.70357001, ae_loss: 0.05061865\n",
      "Step: [1914] total_loss: 2.13337278 d_loss: 1.37695456, g_loss: 0.70772886, ae_loss: 0.04868923\n",
      "Step: [1915] total_loss: 2.14779282 d_loss: 1.38849068, g_loss: 0.70543766, ae_loss: 0.05386457\n",
      "Step: [1916] total_loss: 2.13343668 d_loss: 1.36530256, g_loss: 0.71309310, ae_loss: 0.05504107\n",
      "Step: [1917] total_loss: 2.13787413 d_loss: 1.38583612, g_loss: 0.70022810, ae_loss: 0.05181003\n",
      "Step: [1918] total_loss: 2.14069676 d_loss: 1.40025067, g_loss: 0.68597746, ae_loss: 0.05446862\n",
      "Step: [1919] total_loss: 2.11670089 d_loss: 1.38363481, g_loss: 0.68304515, ae_loss: 0.05002091\n",
      "Step: [1920] total_loss: 2.10809016 d_loss: 1.39271951, g_loss: 0.66415155, ae_loss: 0.05121914\n",
      "Step: [1921] total_loss: 2.12643075 d_loss: 1.40101647, g_loss: 0.67516065, ae_loss: 0.05025360\n",
      "Step: [1922] total_loss: 2.11888123 d_loss: 1.38654852, g_loss: 0.68456423, ae_loss: 0.04776853\n",
      "Step: [1923] total_loss: 2.11658573 d_loss: 1.38711977, g_loss: 0.67942441, ae_loss: 0.05004167\n",
      "Step: [1924] total_loss: 2.14282441 d_loss: 1.39461446, g_loss: 0.69642258, ae_loss: 0.05178737\n",
      "Step: [1925] total_loss: 2.12298822 d_loss: 1.37806225, g_loss: 0.68984330, ae_loss: 0.05508251\n",
      "Step: [1926] total_loss: 2.12756252 d_loss: 1.38702273, g_loss: 0.68804300, ae_loss: 0.05249697\n",
      "Step: [1927] total_loss: 2.12345695 d_loss: 1.38892519, g_loss: 0.68233454, ae_loss: 0.05219705\n",
      "Step: [1928] total_loss: 2.11887932 d_loss: 1.38184178, g_loss: 0.68059659, ae_loss: 0.05644090\n",
      "Step: [1929] total_loss: 2.16142845 d_loss: 1.38811100, g_loss: 0.71717310, ae_loss: 0.05614430\n",
      "Step: [1930] total_loss: 2.15665364 d_loss: 1.39881611, g_loss: 0.70264840, ae_loss: 0.05518912\n",
      "Step: [1931] total_loss: 2.13923573 d_loss: 1.39158607, g_loss: 0.69309431, ae_loss: 0.05455543\n",
      "Step: [1932] total_loss: 2.11330819 d_loss: 1.38163185, g_loss: 0.68023086, ae_loss: 0.05144554\n",
      "Step: [1933] total_loss: 2.11611104 d_loss: 1.37578130, g_loss: 0.69083154, ae_loss: 0.04949816\n",
      "Step: [1934] total_loss: 2.13424397 d_loss: 1.39918566, g_loss: 0.68270802, ae_loss: 0.05235016\n",
      "Step: [1935] total_loss: 2.12468553 d_loss: 1.38621986, g_loss: 0.68246907, ae_loss: 0.05599648\n",
      "Step: [1936] total_loss: 2.13720608 d_loss: 1.38808835, g_loss: 0.69756079, ae_loss: 0.05155681\n",
      "Step: [1937] total_loss: 2.12093663 d_loss: 1.37573814, g_loss: 0.69536400, ae_loss: 0.04983454\n",
      "Step: [1938] total_loss: 2.12195921 d_loss: 1.37406158, g_loss: 0.69958299, ae_loss: 0.04831454\n",
      "Step: [1939] total_loss: 2.12775826 d_loss: 1.37600935, g_loss: 0.70064914, ae_loss: 0.05109975\n",
      "Step: [1940] total_loss: 2.11495900 d_loss: 1.38379312, g_loss: 0.68099695, ae_loss: 0.05016900\n",
      "Step: [1941] total_loss: 2.13027430 d_loss: 1.38883877, g_loss: 0.68860424, ae_loss: 0.05283117\n",
      "Step: [1942] total_loss: 2.17100906 d_loss: 1.40965164, g_loss: 0.70792317, ae_loss: 0.05343428\n",
      "Step: [1943] total_loss: 2.11354733 d_loss: 1.35972643, g_loss: 0.70374966, ae_loss: 0.05007126\n",
      "Step: [1944] total_loss: 2.12428355 d_loss: 1.36597550, g_loss: 0.70649987, ae_loss: 0.05180823\n",
      "Step: [1945] total_loss: 2.12460709 d_loss: 1.36926270, g_loss: 0.70170701, ae_loss: 0.05363749\n",
      "Step: [1946] total_loss: 2.10060525 d_loss: 1.36853743, g_loss: 0.68770665, ae_loss: 0.04436115\n",
      "Step: [1947] total_loss: 2.11167836 d_loss: 1.37780952, g_loss: 0.68360895, ae_loss: 0.05025990\n",
      "Step: [1948] total_loss: 2.13279724 d_loss: 1.36700439, g_loss: 0.71589255, ae_loss: 0.04990019\n",
      "Step: [1949] total_loss: 2.13408804 d_loss: 1.37149155, g_loss: 0.71332479, ae_loss: 0.04927164\n",
      "Step: [1950] total_loss: 2.10863972 d_loss: 1.35483265, g_loss: 0.70314741, ae_loss: 0.05065957\n",
      "Step: [1951] total_loss: 2.14613557 d_loss: 1.39268088, g_loss: 0.69691885, ae_loss: 0.05653586\n",
      "Step: [1952] total_loss: 2.13410997 d_loss: 1.40953290, g_loss: 0.67627448, ae_loss: 0.04830267\n",
      "Step: [1953] total_loss: 2.09828162 d_loss: 1.36677241, g_loss: 0.67913699, ae_loss: 0.05237218\n",
      "Step: [1954] total_loss: 2.11278296 d_loss: 1.38524628, g_loss: 0.67687345, ae_loss: 0.05066323\n",
      "Step: [1955] total_loss: 2.13498735 d_loss: 1.40413308, g_loss: 0.67740226, ae_loss: 0.05345187\n",
      "Step: [1956] total_loss: 2.14396095 d_loss: 1.38951206, g_loss: 0.70059884, ae_loss: 0.05384989\n",
      "Step: [1957] total_loss: 2.15316963 d_loss: 1.39498711, g_loss: 0.70315337, ae_loss: 0.05502908\n",
      "Step: [1958] total_loss: 2.15712690 d_loss: 1.41058564, g_loss: 0.69720507, ae_loss: 0.04933621\n",
      "Step: [1959] total_loss: 2.14318562 d_loss: 1.39013696, g_loss: 0.70107663, ae_loss: 0.05197192\n",
      "Step: [1960] total_loss: 2.11940765 d_loss: 1.36079609, g_loss: 0.70524442, ae_loss: 0.05336713\n",
      "Step: [1961] total_loss: 2.15429068 d_loss: 1.39024603, g_loss: 0.70896024, ae_loss: 0.05508441\n",
      "Step: [1962] total_loss: 2.14007998 d_loss: 1.38950038, g_loss: 0.69745588, ae_loss: 0.05312372\n",
      "Step: [1963] total_loss: 2.14406848 d_loss: 1.39933181, g_loss: 0.69349611, ae_loss: 0.05124052\n",
      "Step: [1964] total_loss: 2.13314724 d_loss: 1.38796723, g_loss: 0.69562542, ae_loss: 0.04955457\n",
      "Step: [1965] total_loss: 2.14722681 d_loss: 1.40454507, g_loss: 0.69146824, ae_loss: 0.05121354\n",
      "Step: [1966] total_loss: 2.12262964 d_loss: 1.37165403, g_loss: 0.69809449, ae_loss: 0.05288124\n",
      "Step: [1967] total_loss: 2.12610722 d_loss: 1.37790227, g_loss: 0.69945753, ae_loss: 0.04874739\n",
      "Step: [1968] total_loss: 2.12683296 d_loss: 1.39051437, g_loss: 0.68895149, ae_loss: 0.04736721\n",
      "Step: [1969] total_loss: 2.11553907 d_loss: 1.38279748, g_loss: 0.68357539, ae_loss: 0.04916633\n",
      "Step: [1970] total_loss: 2.10982227 d_loss: 1.37448919, g_loss: 0.68288243, ae_loss: 0.05245074\n",
      "Step: [1971] total_loss: 2.12095380 d_loss: 1.38105929, g_loss: 0.68706143, ae_loss: 0.05283311\n",
      "Step: [1972] total_loss: 2.12210560 d_loss: 1.36614108, g_loss: 0.70706046, ae_loss: 0.04890419\n",
      "Step: [1973] total_loss: 2.11405635 d_loss: 1.36958575, g_loss: 0.69034427, ae_loss: 0.05412626\n",
      "Step: [1974] total_loss: 2.11264277 d_loss: 1.38003111, g_loss: 0.67897588, ae_loss: 0.05363576\n",
      "Step: [1975] total_loss: 2.11583543 d_loss: 1.37582374, g_loss: 0.68947089, ae_loss: 0.05054085\n",
      "Step: [1976] total_loss: 2.11504316 d_loss: 1.37789297, g_loss: 0.68676913, ae_loss: 0.05038123\n",
      "Step: [1977] total_loss: 2.12513542 d_loss: 1.38022709, g_loss: 0.69475389, ae_loss: 0.05015430\n",
      "Step: [1978] total_loss: 2.13704109 d_loss: 1.37558937, g_loss: 0.71516269, ae_loss: 0.04628896\n",
      "Step: [1979] total_loss: 2.11376143 d_loss: 1.36895120, g_loss: 0.69238544, ae_loss: 0.05242493\n",
      "Step: [1980] total_loss: 2.14813781 d_loss: 1.39273965, g_loss: 0.69991744, ae_loss: 0.05548077\n",
      "Step: [1981] total_loss: 2.13115764 d_loss: 1.37465107, g_loss: 0.70690417, ae_loss: 0.04960235\n",
      "Step: [1982] total_loss: 2.15824890 d_loss: 1.37710690, g_loss: 0.72751999, ae_loss: 0.05362200\n",
      "Step: [1983] total_loss: 2.14548206 d_loss: 1.38686585, g_loss: 0.70182747, ae_loss: 0.05678875\n",
      "Step: [1984] total_loss: 2.14112496 d_loss: 1.39340591, g_loss: 0.69379479, ae_loss: 0.05392432\n",
      "Step: [1985] total_loss: 2.13608670 d_loss: 1.39567447, g_loss: 0.68698341, ae_loss: 0.05342877\n",
      "Step: [1986] total_loss: 2.10997295 d_loss: 1.37825227, g_loss: 0.68386614, ae_loss: 0.04785438\n",
      "Step: [1987] total_loss: 2.13700891 d_loss: 1.38212276, g_loss: 0.70218563, ae_loss: 0.05270049\n",
      "Step: [1988] total_loss: 2.13887119 d_loss: 1.37955689, g_loss: 0.70900309, ae_loss: 0.05031125\n",
      "Step: [1989] total_loss: 2.14858675 d_loss: 1.38349724, g_loss: 0.71126044, ae_loss: 0.05382904\n",
      "Step: [1990] total_loss: 2.13063383 d_loss: 1.37334061, g_loss: 0.70860386, ae_loss: 0.04868951\n",
      "Step: [1991] total_loss: 2.12375927 d_loss: 1.37585437, g_loss: 0.69825321, ae_loss: 0.04965159\n",
      "Step: [1992] total_loss: 2.13812017 d_loss: 1.36756516, g_loss: 0.71543300, ae_loss: 0.05512219\n",
      "Step: [1993] total_loss: 2.12529874 d_loss: 1.36422253, g_loss: 0.70837045, ae_loss: 0.05270575\n",
      "Step: [1994] total_loss: 2.11397648 d_loss: 1.35691595, g_loss: 0.70400894, ae_loss: 0.05305148\n",
      "Step: [1995] total_loss: 2.14687943 d_loss: 1.39809442, g_loss: 0.69635963, ae_loss: 0.05242532\n",
      "Step: [1996] total_loss: 2.14687943 d_loss: 1.39019048, g_loss: 0.70841599, ae_loss: 0.04827295\n",
      "Step: [1997] total_loss: 2.14264774 d_loss: 1.36379600, g_loss: 0.72793967, ae_loss: 0.05091204\n",
      "Step: [1998] total_loss: 2.14226508 d_loss: 1.36535287, g_loss: 0.72279966, ae_loss: 0.05411251\n",
      "Step: [1999] total_loss: 2.09253550 d_loss: 1.35808384, g_loss: 0.68026161, ae_loss: 0.05418988\n",
      "Step: [2000] total_loss: 2.13996625 d_loss: 1.39070952, g_loss: 0.69425935, ae_loss: 0.05499738\n",
      "Step: [2001] total_loss: 2.09315395 d_loss: 1.35493958, g_loss: 0.68878400, ae_loss: 0.04943034\n",
      "Step: [2002] total_loss: 2.11258650 d_loss: 1.39485669, g_loss: 0.66627800, ae_loss: 0.05145184\n",
      "Step: [2003] total_loss: 2.10879445 d_loss: 1.36919928, g_loss: 0.68986613, ae_loss: 0.04972895\n",
      "Step: [2004] total_loss: 2.14222813 d_loss: 1.38114440, g_loss: 0.70766461, ae_loss: 0.05341894\n",
      "Step: [2005] total_loss: 2.11025023 d_loss: 1.36589265, g_loss: 0.69325936, ae_loss: 0.05109819\n",
      "Step: [2006] total_loss: 2.11626148 d_loss: 1.37385702, g_loss: 0.68880826, ae_loss: 0.05359621\n",
      "Step: [2007] total_loss: 2.14503241 d_loss: 1.39770520, g_loss: 0.69119728, ae_loss: 0.05612997\n",
      "Step: [2008] total_loss: 2.13406181 d_loss: 1.39850187, g_loss: 0.68849653, ae_loss: 0.04706334\n",
      "Step: [2009] total_loss: 2.14122629 d_loss: 1.39284658, g_loss: 0.69456828, ae_loss: 0.05381129\n",
      "Step: [2010] total_loss: 2.12781024 d_loss: 1.37483549, g_loss: 0.70335543, ae_loss: 0.04961929\n",
      "Step: [2011] total_loss: 2.11902571 d_loss: 1.36527967, g_loss: 0.70042992, ae_loss: 0.05331628\n",
      "Step: [2012] total_loss: 2.12183547 d_loss: 1.37182701, g_loss: 0.69729745, ae_loss: 0.05271100\n",
      "Step: [2013] total_loss: 2.15051126 d_loss: 1.37496758, g_loss: 0.72232056, ae_loss: 0.05322304\n",
      "Step: [2014] total_loss: 2.14555359 d_loss: 1.39087296, g_loss: 0.70645410, ae_loss: 0.04822643\n",
      "Step: [2015] total_loss: 2.13682127 d_loss: 1.39327312, g_loss: 0.69394219, ae_loss: 0.04960614\n",
      "Step: [2016] total_loss: 2.10924578 d_loss: 1.35992742, g_loss: 0.69275725, ae_loss: 0.05656120\n",
      "Step: [2017] total_loss: 2.14002275 d_loss: 1.39182329, g_loss: 0.69882214, ae_loss: 0.04937718\n",
      "Step: [2018] total_loss: 2.11956692 d_loss: 1.36523032, g_loss: 0.70226592, ae_loss: 0.05207060\n",
      "Step: [2019] total_loss: 2.11357784 d_loss: 1.37705636, g_loss: 0.68381619, ae_loss: 0.05270522\n",
      "Step: [2020] total_loss: 2.11257792 d_loss: 1.37876630, g_loss: 0.68448496, ae_loss: 0.04932651\n",
      "Step: [2021] total_loss: 2.14065862 d_loss: 1.40469885, g_loss: 0.68095934, ae_loss: 0.05500040\n",
      "Step: [2022] total_loss: 2.11378336 d_loss: 1.38588202, g_loss: 0.67582154, ae_loss: 0.05207996\n",
      "Step: [2023] total_loss: 2.11781049 d_loss: 1.38741970, g_loss: 0.67943180, ae_loss: 0.05095896\n",
      "Step: [2024] total_loss: 2.12656689 d_loss: 1.37582922, g_loss: 0.69791019, ae_loss: 0.05282747\n",
      "Step: [2025] total_loss: 2.13251972 d_loss: 1.36567664, g_loss: 0.71437871, ae_loss: 0.05246432\n",
      "Step: [2026] total_loss: 2.11744523 d_loss: 1.36505687, g_loss: 0.69511080, ae_loss: 0.05727754\n",
      "Step: [2027] total_loss: 2.15695238 d_loss: 1.41291833, g_loss: 0.69100678, ae_loss: 0.05302712\n",
      "Step: [2028] total_loss: 2.11756992 d_loss: 1.36779141, g_loss: 0.69772875, ae_loss: 0.05204980\n",
      "Step: [2029] total_loss: 2.12847328 d_loss: 1.38092446, g_loss: 0.69562829, ae_loss: 0.05192067\n",
      "Step: [2030] total_loss: 2.15246129 d_loss: 1.40100765, g_loss: 0.69937134, ae_loss: 0.05208227\n",
      "Step: [2031] total_loss: 2.14163256 d_loss: 1.39734745, g_loss: 0.69133949, ae_loss: 0.05294548\n",
      "Step: [2032] total_loss: 2.16066837 d_loss: 1.38641548, g_loss: 0.72221875, ae_loss: 0.05203403\n",
      "Step: [2033] total_loss: 2.10908842 d_loss: 1.35276401, g_loss: 0.70091975, ae_loss: 0.05540457\n",
      "Step: [2034] total_loss: 2.11588740 d_loss: 1.38131857, g_loss: 0.67938387, ae_loss: 0.05518493\n",
      "Step: [2035] total_loss: 2.09982991 d_loss: 1.35617936, g_loss: 0.69446588, ae_loss: 0.04918467\n",
      "Step: [2036] total_loss: 2.11071301 d_loss: 1.38055849, g_loss: 0.67944145, ae_loss: 0.05071308\n",
      "Step: [2037] total_loss: 2.10626459 d_loss: 1.36936903, g_loss: 0.68122053, ae_loss: 0.05567493\n",
      "Step: [2038] total_loss: 2.09720850 d_loss: 1.36579323, g_loss: 0.67789507, ae_loss: 0.05352035\n",
      "Step: [2039] total_loss: 2.10932231 d_loss: 1.35484171, g_loss: 0.70319492, ae_loss: 0.05128564\n",
      "Step: [2040] total_loss: 2.12083435 d_loss: 1.37542129, g_loss: 0.69180763, ae_loss: 0.05360527\n",
      "Step: [2041] total_loss: 2.11774206 d_loss: 1.36060870, g_loss: 0.70083666, ae_loss: 0.05629661\n",
      "Step: [2042] total_loss: 2.11436224 d_loss: 1.36203682, g_loss: 0.69651937, ae_loss: 0.05580587\n",
      "Step: [2043] total_loss: 2.13710237 d_loss: 1.39639485, g_loss: 0.68978900, ae_loss: 0.05091851\n",
      "Step: [2044] total_loss: 2.12342930 d_loss: 1.38292360, g_loss: 0.69008648, ae_loss: 0.05041916\n",
      "Step: [2045] total_loss: 2.11497355 d_loss: 1.36805308, g_loss: 0.69169497, ae_loss: 0.05522542\n",
      "Step: [2046] total_loss: 2.14468050 d_loss: 1.36709189, g_loss: 0.72690356, ae_loss: 0.05068517\n",
      "Step: [2047] total_loss: 2.09498453 d_loss: 1.34127498, g_loss: 0.69981432, ae_loss: 0.05389524\n",
      "Step: [2048] total_loss: 2.11419129 d_loss: 1.38043714, g_loss: 0.67993736, ae_loss: 0.05381683\n",
      "Step: [2049] total_loss: 2.13652086 d_loss: 1.39405572, g_loss: 0.69099665, ae_loss: 0.05146838\n",
      "Step: [2050] total_loss: 2.12617207 d_loss: 1.36942542, g_loss: 0.70659387, ae_loss: 0.05015272\n",
      "Step: [2051] total_loss: 2.11519289 d_loss: 1.38922203, g_loss: 0.67755210, ae_loss: 0.04841893\n",
      "Step: [2052] total_loss: 2.13752317 d_loss: 1.39584303, g_loss: 0.69323587, ae_loss: 0.04844423\n",
      "Step: [2053] total_loss: 2.14160252 d_loss: 1.39070082, g_loss: 0.69915724, ae_loss: 0.05174446\n",
      "Step: [2054] total_loss: 2.13899183 d_loss: 1.40964389, g_loss: 0.67781961, ae_loss: 0.05152841\n",
      "Step: [2055] total_loss: 2.13873649 d_loss: 1.38689506, g_loss: 0.69793224, ae_loss: 0.05390913\n",
      "Step: [2056] total_loss: 2.13102484 d_loss: 1.37434745, g_loss: 0.70442009, ae_loss: 0.05225728\n",
      "Step: [2057] total_loss: 2.14064503 d_loss: 1.38770008, g_loss: 0.70112944, ae_loss: 0.05181535\n",
      "Step: [2058] total_loss: 2.15659475 d_loss: 1.40144765, g_loss: 0.70409054, ae_loss: 0.05105656\n",
      "Step: [2059] total_loss: 2.13089061 d_loss: 1.37134027, g_loss: 0.70865959, ae_loss: 0.05089071\n",
      "Step: [2060] total_loss: 2.14224267 d_loss: 1.38563871, g_loss: 0.70611453, ae_loss: 0.05048946\n",
      "Step: [2061] total_loss: 2.13604975 d_loss: 1.40089893, g_loss: 0.68348086, ae_loss: 0.05167011\n",
      "Step: [2062] total_loss: 2.12238836 d_loss: 1.39385283, g_loss: 0.67855883, ae_loss: 0.04997687\n",
      "Step: [2063] total_loss: 2.11962128 d_loss: 1.38237047, g_loss: 0.68542123, ae_loss: 0.05182957\n",
      "Step: [2064] total_loss: 2.11556077 d_loss: 1.37930632, g_loss: 0.68318033, ae_loss: 0.05307417\n",
      "Step: [2065] total_loss: 2.13439155 d_loss: 1.40161121, g_loss: 0.68323171, ae_loss: 0.04954861\n",
      "Step: [2066] total_loss: 2.12666011 d_loss: 1.39778054, g_loss: 0.68010259, ae_loss: 0.04877704\n",
      "Step: [2067] total_loss: 2.12313700 d_loss: 1.38743973, g_loss: 0.68891793, ae_loss: 0.04677937\n",
      "Step: [2068] total_loss: 2.11125731 d_loss: 1.36193311, g_loss: 0.69755483, ae_loss: 0.05176943\n",
      "Step: [2069] total_loss: 2.13175368 d_loss: 1.38968444, g_loss: 0.69461572, ae_loss: 0.04745357\n",
      "Step: [2070] total_loss: 2.13940430 d_loss: 1.37890172, g_loss: 0.70755869, ae_loss: 0.05294378\n",
      "Step: [2071] total_loss: 2.13475895 d_loss: 1.38308144, g_loss: 0.70306474, ae_loss: 0.04861266\n",
      "Step: [2072] total_loss: 2.09361410 d_loss: 1.36044121, g_loss: 0.68331361, ae_loss: 0.04985927\n",
      "Step: [2073] total_loss: 2.10776067 d_loss: 1.38459551, g_loss: 0.67343163, ae_loss: 0.04973355\n",
      "Step: [2074] total_loss: 2.12009382 d_loss: 1.36904550, g_loss: 0.69973421, ae_loss: 0.05131426\n",
      "Step: [2075] total_loss: 2.14355922 d_loss: 1.38405931, g_loss: 0.71116996, ae_loss: 0.04832994\n",
      "Step: [2076] total_loss: 2.13454556 d_loss: 1.37236500, g_loss: 0.70661992, ae_loss: 0.05556072\n",
      "Step: [2077] total_loss: 2.15671778 d_loss: 1.42006826, g_loss: 0.68875551, ae_loss: 0.04789407\n",
      "Step: [2078] total_loss: 2.14018488 d_loss: 1.38061547, g_loss: 0.70606983, ae_loss: 0.05349943\n",
      "Step: [2079] total_loss: 2.15372849 d_loss: 1.36599648, g_loss: 0.73182780, ae_loss: 0.05590421\n",
      "Step: [2080] total_loss: 2.14183378 d_loss: 1.36717486, g_loss: 0.72339237, ae_loss: 0.05126641\n",
      "Step: [2081] total_loss: 2.13724327 d_loss: 1.37500763, g_loss: 0.70774692, ae_loss: 0.05448874\n",
      "Step: [2082] total_loss: 2.12692666 d_loss: 1.38148463, g_loss: 0.69038779, ae_loss: 0.05505435\n",
      "Step: [2083] total_loss: 2.13457704 d_loss: 1.38505113, g_loss: 0.69529706, ae_loss: 0.05422880\n",
      "Step: [2084] total_loss: 2.12114573 d_loss: 1.38236856, g_loss: 0.68240976, ae_loss: 0.05636730\n",
      "Step: [2085] total_loss: 2.13459325 d_loss: 1.40060747, g_loss: 0.68036544, ae_loss: 0.05362037\n",
      "Step: [2086] total_loss: 2.12528801 d_loss: 1.38922572, g_loss: 0.68668902, ae_loss: 0.04937313\n",
      "Step: [2087] total_loss: 2.13104510 d_loss: 1.38161159, g_loss: 0.69920945, ae_loss: 0.05022410\n",
      "Step: [2088] total_loss: 2.12470436 d_loss: 1.38475251, g_loss: 0.68576980, ae_loss: 0.05418215\n",
      "Step: [2089] total_loss: 2.12881136 d_loss: 1.35680485, g_loss: 0.71734536, ae_loss: 0.05466131\n",
      "Step: [2090] total_loss: 2.11942697 d_loss: 1.38166308, g_loss: 0.68620700, ae_loss: 0.05155700\n",
      "Step: [2091] total_loss: 2.11056995 d_loss: 1.38536775, g_loss: 0.67397231, ae_loss: 0.05122989\n",
      "Step: [2092] total_loss: 2.11375523 d_loss: 1.37153900, g_loss: 0.69210160, ae_loss: 0.05011462\n",
      "Step: [2093] total_loss: 2.13388991 d_loss: 1.38142705, g_loss: 0.69939524, ae_loss: 0.05306773\n",
      "Step: [2094] total_loss: 2.13013220 d_loss: 1.39015222, g_loss: 0.68426168, ae_loss: 0.05571816\n",
      "Step: [2095] total_loss: 2.11848259 d_loss: 1.38583708, g_loss: 0.67935407, ae_loss: 0.05329145\n",
      "Step: [2096] total_loss: 2.10528374 d_loss: 1.38187647, g_loss: 0.67077601, ae_loss: 0.05263137\n",
      "Step: [2097] total_loss: 2.12593412 d_loss: 1.38018680, g_loss: 0.69187510, ae_loss: 0.05387231\n",
      "Step: [2098] total_loss: 2.17077160 d_loss: 1.35477674, g_loss: 0.76435006, ae_loss: 0.05164490\n",
      "Step: [2099] total_loss: 2.13487291 d_loss: 1.38890815, g_loss: 0.69249630, ae_loss: 0.05346853\n",
      "Step: [2100] total_loss: 2.12740612 d_loss: 1.37633634, g_loss: 0.70089042, ae_loss: 0.05017943\n",
      "Step: [2101] total_loss: 2.11923552 d_loss: 1.38927782, g_loss: 0.67630208, ae_loss: 0.05365548\n",
      "Step: [2102] total_loss: 2.10267305 d_loss: 1.35161078, g_loss: 0.70062709, ae_loss: 0.05043513\n",
      "Step: [2103] total_loss: 2.11684346 d_loss: 1.39107895, g_loss: 0.67205280, ae_loss: 0.05371163\n",
      "Step: [2104] total_loss: 2.12596846 d_loss: 1.38035071, g_loss: 0.69148165, ae_loss: 0.05413602\n",
      "Step: [2105] total_loss: 2.11501026 d_loss: 1.38693714, g_loss: 0.67913693, ae_loss: 0.04893628\n",
      "Step: [2106] total_loss: 2.13696957 d_loss: 1.39511061, g_loss: 0.68767262, ae_loss: 0.05418620\n",
      "Step: [2107] total_loss: 2.12816906 d_loss: 1.37279284, g_loss: 0.70283842, ae_loss: 0.05253776\n",
      "Step: [2108] total_loss: 2.11306286 d_loss: 1.38261819, g_loss: 0.68141693, ae_loss: 0.04902783\n",
      "Step: [2109] total_loss: 2.12152457 d_loss: 1.37864554, g_loss: 0.69084537, ae_loss: 0.05203364\n",
      "Step: [2110] total_loss: 2.13074517 d_loss: 1.37478137, g_loss: 0.69986409, ae_loss: 0.05609974\n",
      "Step: [2111] total_loss: 2.12799692 d_loss: 1.38300657, g_loss: 0.69225061, ae_loss: 0.05273992\n",
      "Step: [2112] total_loss: 2.12656474 d_loss: 1.37527978, g_loss: 0.69884413, ae_loss: 0.05244090\n",
      "Step: [2113] total_loss: 2.12283659 d_loss: 1.37411094, g_loss: 0.69818544, ae_loss: 0.05054038\n",
      "Step: [2114] total_loss: 2.11862612 d_loss: 1.38276625, g_loss: 0.68585169, ae_loss: 0.05000822\n",
      "Step: [2115] total_loss: 2.12457538 d_loss: 1.39033759, g_loss: 0.68310350, ae_loss: 0.05113439\n",
      "Step: [2116] total_loss: 2.12706208 d_loss: 1.37495947, g_loss: 0.70001805, ae_loss: 0.05208454\n",
      "Step: [2117] total_loss: 2.11798906 d_loss: 1.37839925, g_loss: 0.69050306, ae_loss: 0.04908676\n",
      "Step: [2118] total_loss: 2.13452554 d_loss: 1.38483214, g_loss: 0.69554287, ae_loss: 0.05415047\n",
      "Step: [2119] total_loss: 2.11165237 d_loss: 1.37263489, g_loss: 0.68968296, ae_loss: 0.04933450\n",
      "Step: [2120] total_loss: 2.12809134 d_loss: 1.36383867, g_loss: 0.71226740, ae_loss: 0.05198517\n",
      "Step: [2121] total_loss: 2.11196899 d_loss: 1.36487389, g_loss: 0.69922924, ae_loss: 0.04786588\n",
      "Step: [2122] total_loss: 2.12980032 d_loss: 1.34345794, g_loss: 0.73592252, ae_loss: 0.05041986\n",
      "Step: [2123] total_loss: 2.11780405 d_loss: 1.36623466, g_loss: 0.70238817, ae_loss: 0.04918117\n",
      "Step: [2124] total_loss: 2.10701799 d_loss: 1.35645592, g_loss: 0.70056033, ae_loss: 0.05000163\n",
      "Step: [2125] total_loss: 2.10188890 d_loss: 1.35671186, g_loss: 0.69158274, ae_loss: 0.05359421\n",
      "Step: [2126] total_loss: 2.13370895 d_loss: 1.38159645, g_loss: 0.69707406, ae_loss: 0.05503860\n",
      "Step: [2127] total_loss: 2.10907197 d_loss: 1.35397363, g_loss: 0.70415169, ae_loss: 0.05094668\n",
      "Step: [2128] total_loss: 2.12518978 d_loss: 1.39883304, g_loss: 0.67271870, ae_loss: 0.05363801\n",
      "Step: [2129] total_loss: 2.14119482 d_loss: 1.38265133, g_loss: 0.71076041, ae_loss: 0.04778315\n",
      "Step: [2130] total_loss: 2.13096476 d_loss: 1.36825430, g_loss: 0.71028233, ae_loss: 0.05242796\n",
      "Step: [2131] total_loss: 2.12397766 d_loss: 1.37456560, g_loss: 0.70413196, ae_loss: 0.04527999\n",
      "Step: [2132] total_loss: 2.11399603 d_loss: 1.34382832, g_loss: 0.71760893, ae_loss: 0.05255871\n",
      "Step: [2133] total_loss: 2.13393879 d_loss: 1.36948442, g_loss: 0.71075863, ae_loss: 0.05369565\n",
      "Step: [2134] total_loss: 2.13181663 d_loss: 1.36761999, g_loss: 0.71193624, ae_loss: 0.05226035\n",
      "Step: [2135] total_loss: 2.12793446 d_loss: 1.38170302, g_loss: 0.69359940, ae_loss: 0.05263210\n",
      "Step: [2136] total_loss: 2.14656353 d_loss: 1.38758516, g_loss: 0.70614076, ae_loss: 0.05283761\n",
      "Step: [2137] total_loss: 2.14076900 d_loss: 1.38342810, g_loss: 0.70468163, ae_loss: 0.05265919\n",
      "Step: [2138] total_loss: 2.10832214 d_loss: 1.39539146, g_loss: 0.66390777, ae_loss: 0.04902298\n",
      "Step: [2139] total_loss: 2.16941786 d_loss: 1.39667678, g_loss: 0.72004437, ae_loss: 0.05269662\n",
      "Step: [2140] total_loss: 2.12749362 d_loss: 1.40084958, g_loss: 0.67323864, ae_loss: 0.05340536\n",
      "Step: [2141] total_loss: 2.11857319 d_loss: 1.38233757, g_loss: 0.68708861, ae_loss: 0.04914718\n",
      "Step: [2142] total_loss: 2.13184881 d_loss: 1.37897611, g_loss: 0.70074022, ae_loss: 0.05213240\n",
      "Step: [2143] total_loss: 2.13759184 d_loss: 1.38231039, g_loss: 0.70331520, ae_loss: 0.05196618\n",
      "Step: [2144] total_loss: 2.13270521 d_loss: 1.36558747, g_loss: 0.71778321, ae_loss: 0.04933440\n",
      "Step: [2145] total_loss: 2.12020779 d_loss: 1.37629580, g_loss: 0.68872833, ae_loss: 0.05518365\n",
      "Step: [2146] total_loss: 2.12753296 d_loss: 1.37465370, g_loss: 0.69482577, ae_loss: 0.05805366\n",
      "Step: [2147] total_loss: 2.10674882 d_loss: 1.38078713, g_loss: 0.67596620, ae_loss: 0.04999540\n",
      "Step: [2148] total_loss: 2.12606597 d_loss: 1.37739921, g_loss: 0.69240654, ae_loss: 0.05626021\n",
      "Step: [2149] total_loss: 2.10528183 d_loss: 1.37371135, g_loss: 0.67934167, ae_loss: 0.05222888\n",
      "Step: [2150] total_loss: 2.10912752 d_loss: 1.39117920, g_loss: 0.66953850, ae_loss: 0.04840982\n",
      "Step: [2151] total_loss: 2.11697292 d_loss: 1.35360837, g_loss: 0.71666431, ae_loss: 0.04670033\n",
      "Step: [2152] total_loss: 2.12153935 d_loss: 1.39968848, g_loss: 0.67407531, ae_loss: 0.04777545\n",
      "Step: [2153] total_loss: 2.12405491 d_loss: 1.34794831, g_loss: 0.72435331, ae_loss: 0.05175317\n",
      "Step: [2154] total_loss: 2.13279438 d_loss: 1.36219096, g_loss: 0.71984279, ae_loss: 0.05076068\n",
      "Step: [2155] total_loss: 2.13637114 d_loss: 1.37661147, g_loss: 0.70758975, ae_loss: 0.05216983\n",
      "Step: [2156] total_loss: 2.12448525 d_loss: 1.36713922, g_loss: 0.70269704, ae_loss: 0.05464905\n",
      "Step: [2157] total_loss: 2.11721015 d_loss: 1.37043893, g_loss: 0.69515389, ae_loss: 0.05161740\n",
      "Step: [2158] total_loss: 2.14811182 d_loss: 1.37772274, g_loss: 0.71777713, ae_loss: 0.05261208\n",
      "Step: [2159] total_loss: 2.11331654 d_loss: 1.38807225, g_loss: 0.66861433, ae_loss: 0.05662998\n",
      "Step: [2160] total_loss: 2.12320447 d_loss: 1.37002802, g_loss: 0.70155138, ae_loss: 0.05162496\n",
      "Step: [2161] total_loss: 2.11559772 d_loss: 1.38824892, g_loss: 0.67377901, ae_loss: 0.05356988\n",
      "Step: [2162] total_loss: 2.10816240 d_loss: 1.37758350, g_loss: 0.68008506, ae_loss: 0.05049371\n",
      "Step: [2163] total_loss: 2.11737013 d_loss: 1.37357473, g_loss: 0.68993700, ae_loss: 0.05385851\n",
      "Step: [2164] total_loss: 2.13807011 d_loss: 1.38813829, g_loss: 0.69650835, ae_loss: 0.05342357\n",
      "Step: [2165] total_loss: 2.13705206 d_loss: 1.36400032, g_loss: 0.72172225, ae_loss: 0.05132944\n",
      "Step: [2166] total_loss: 2.12136984 d_loss: 1.37686002, g_loss: 0.69285154, ae_loss: 0.05165817\n",
      "Step: [2167] total_loss: 2.13179207 d_loss: 1.36917686, g_loss: 0.71196079, ae_loss: 0.05065455\n",
      "Step: [2168] total_loss: 2.14252639 d_loss: 1.38342023, g_loss: 0.70749801, ae_loss: 0.05160823\n",
      "Step: [2169] total_loss: 2.13632846 d_loss: 1.38598859, g_loss: 0.69757360, ae_loss: 0.05276627\n",
      "Step: [2170] total_loss: 2.15727806 d_loss: 1.38992167, g_loss: 0.71079195, ae_loss: 0.05656444\n",
      "Step: [2171] total_loss: 2.13445330 d_loss: 1.38094831, g_loss: 0.70217425, ae_loss: 0.05133083\n",
      "Step: [2172] total_loss: 2.14402437 d_loss: 1.38706398, g_loss: 0.70101708, ae_loss: 0.05594339\n",
      "Step: [2173] total_loss: 2.11390495 d_loss: 1.38756824, g_loss: 0.67519498, ae_loss: 0.05114166\n",
      "Step: [2174] total_loss: 2.10890865 d_loss: 1.36933184, g_loss: 0.69196630, ae_loss: 0.04761057\n",
      "Step: [2175] total_loss: 2.10853100 d_loss: 1.37287688, g_loss: 0.68545294, ae_loss: 0.05020129\n",
      "Step: [2176] total_loss: 2.11402082 d_loss: 1.37479019, g_loss: 0.68765557, ae_loss: 0.05157496\n",
      "Step: [2177] total_loss: 2.11226010 d_loss: 1.35274148, g_loss: 0.71053356, ae_loss: 0.04898509\n",
      "Step: [2178] total_loss: 2.10643864 d_loss: 1.36278379, g_loss: 0.69116873, ae_loss: 0.05248623\n",
      "Step: [2179] total_loss: 2.14250255 d_loss: 1.38433862, g_loss: 0.70603907, ae_loss: 0.05212486\n",
      "Step: [2180] total_loss: 2.13414192 d_loss: 1.38707876, g_loss: 0.69540036, ae_loss: 0.05166263\n",
      "Step: [2181] total_loss: 2.13328218 d_loss: 1.38472867, g_loss: 0.69913876, ae_loss: 0.04941483\n",
      "Step: [2182] total_loss: 2.14134622 d_loss: 1.37884986, g_loss: 0.71051735, ae_loss: 0.05197894\n",
      "Step: [2183] total_loss: 2.11057496 d_loss: 1.36318815, g_loss: 0.69783938, ae_loss: 0.04954746\n",
      "Step: [2184] total_loss: 2.14858770 d_loss: 1.37350166, g_loss: 0.72310650, ae_loss: 0.05197944\n",
      "Step: [2185] total_loss: 2.13063049 d_loss: 1.37061584, g_loss: 0.71023834, ae_loss: 0.04977646\n",
      "Step: [2186] total_loss: 2.12295675 d_loss: 1.38121271, g_loss: 0.68811405, ae_loss: 0.05363015\n",
      "Step: [2187] total_loss: 2.13004303 d_loss: 1.39870465, g_loss: 0.68004370, ae_loss: 0.05129459\n",
      "Step: [2188] total_loss: 2.12485552 d_loss: 1.39533579, g_loss: 0.67319524, ae_loss: 0.05632460\n",
      "Step: [2189] total_loss: 2.13195276 d_loss: 1.39182460, g_loss: 0.68492830, ae_loss: 0.05519988\n",
      "Step: [2190] total_loss: 2.14377594 d_loss: 1.38944483, g_loss: 0.70512104, ae_loss: 0.04920997\n",
      "Step: [2191] total_loss: 2.13347602 d_loss: 1.40775359, g_loss: 0.67209888, ae_loss: 0.05362356\n",
      "Step: [2192] total_loss: 2.11521220 d_loss: 1.37914729, g_loss: 0.68628109, ae_loss: 0.04978383\n",
      "Step: [2193] total_loss: 2.11690974 d_loss: 1.37166512, g_loss: 0.69081479, ae_loss: 0.05442995\n",
      "Step: [2194] total_loss: 2.13428831 d_loss: 1.39613032, g_loss: 0.68663967, ae_loss: 0.05151825\n",
      "Step: [2195] total_loss: 2.14297438 d_loss: 1.39502764, g_loss: 0.69584960, ae_loss: 0.05209704\n",
      "Step: [2196] total_loss: 2.11991167 d_loss: 1.37116909, g_loss: 0.69556367, ae_loss: 0.05317885\n",
      "Step: [2197] total_loss: 2.15449619 d_loss: 1.40337181, g_loss: 0.69804329, ae_loss: 0.05308098\n",
      "Step: [2198] total_loss: 2.14709234 d_loss: 1.39624858, g_loss: 0.70148778, ae_loss: 0.04935595\n",
      "Step: [2199] total_loss: 2.13578582 d_loss: 1.36528218, g_loss: 0.71692848, ae_loss: 0.05357511\n",
      "Step: [2200] total_loss: 2.15201664 d_loss: 1.38729620, g_loss: 0.70992237, ae_loss: 0.05479798\n",
      "Step: [2201] total_loss: 2.13747191 d_loss: 1.37343919, g_loss: 0.71216524, ae_loss: 0.05186751\n",
      "Step: [2202] total_loss: 2.14952183 d_loss: 1.38375306, g_loss: 0.70776987, ae_loss: 0.05799900\n",
      "Step: [2203] total_loss: 2.10936356 d_loss: 1.36278510, g_loss: 0.69642913, ae_loss: 0.05014946\n",
      "Step: [2204] total_loss: 2.09553146 d_loss: 1.35769701, g_loss: 0.68139857, ae_loss: 0.05643588\n",
      "Step: [2205] total_loss: 2.10425878 d_loss: 1.36108518, g_loss: 0.69103575, ae_loss: 0.05213781\n",
      "Step: [2206] total_loss: 2.10532880 d_loss: 1.37313020, g_loss: 0.67933077, ae_loss: 0.05286783\n",
      "Step: [2207] total_loss: 2.10450101 d_loss: 1.36152470, g_loss: 0.68946552, ae_loss: 0.05351082\n",
      "Step: [2208] total_loss: 2.08682275 d_loss: 1.36615181, g_loss: 0.66894031, ae_loss: 0.05173061\n",
      "Step: [2209] total_loss: 2.09092879 d_loss: 1.34818220, g_loss: 0.69435740, ae_loss: 0.04838916\n",
      "Step: [2210] total_loss: 2.11730313 d_loss: 1.39900827, g_loss: 0.66702926, ae_loss: 0.05126559\n",
      "Step: [2211] total_loss: 2.12261367 d_loss: 1.38018882, g_loss: 0.68931097, ae_loss: 0.05311378\n",
      "Step: [2212] total_loss: 2.14094067 d_loss: 1.39101386, g_loss: 0.69699502, ae_loss: 0.05293180\n",
      "Step: [2213] total_loss: 2.12783670 d_loss: 1.37560880, g_loss: 0.70285809, ae_loss: 0.04936998\n",
      "Step: [2214] total_loss: 2.12800646 d_loss: 1.38794827, g_loss: 0.68991560, ae_loss: 0.05014266\n",
      "Step: [2215] total_loss: 2.12612009 d_loss: 1.37832069, g_loss: 0.69535887, ae_loss: 0.05244037\n",
      "Step: [2216] total_loss: 2.13799524 d_loss: 1.37338293, g_loss: 0.70865834, ae_loss: 0.05595409\n",
      "Step: [2217] total_loss: 2.12658763 d_loss: 1.37337422, g_loss: 0.69740862, ae_loss: 0.05580467\n",
      "Step: [2218] total_loss: 2.14759207 d_loss: 1.37915850, g_loss: 0.71601701, ae_loss: 0.05241643\n",
      "Step: [2219] total_loss: 2.15581656 d_loss: 1.36774147, g_loss: 0.73611391, ae_loss: 0.05196131\n",
      "Step: [2220] total_loss: 2.13421774 d_loss: 1.39911270, g_loss: 0.68572032, ae_loss: 0.04938455\n",
      "Step: [2221] total_loss: 2.10843849 d_loss: 1.37234592, g_loss: 0.68504584, ae_loss: 0.05104671\n",
      "Step: [2222] total_loss: 2.10840940 d_loss: 1.36609960, g_loss: 0.68834090, ae_loss: 0.05396894\n",
      "Step: [2223] total_loss: 2.09567595 d_loss: 1.35447514, g_loss: 0.68794346, ae_loss: 0.05325745\n",
      "Step: [2224] total_loss: 2.12355518 d_loss: 1.39042640, g_loss: 0.68060708, ae_loss: 0.05252161\n",
      "Step: [2225] total_loss: 2.10509014 d_loss: 1.39269137, g_loss: 0.66199231, ae_loss: 0.05040653\n",
      "Step: [2226] total_loss: 2.10839772 d_loss: 1.36832714, g_loss: 0.68772525, ae_loss: 0.05234540\n",
      "Step: [2227] total_loss: 2.11877584 d_loss: 1.37699437, g_loss: 0.68992406, ae_loss: 0.05185738\n",
      "Step: [2228] total_loss: 2.12647438 d_loss: 1.37200928, g_loss: 0.70158172, ae_loss: 0.05288342\n",
      "Step: [2229] total_loss: 2.11815858 d_loss: 1.36949360, g_loss: 0.70005465, ae_loss: 0.04861035\n",
      "Step: [2230] total_loss: 2.12034631 d_loss: 1.38598824, g_loss: 0.68386829, ae_loss: 0.05048981\n",
      "Step: [2231] total_loss: 2.12558079 d_loss: 1.35577178, g_loss: 0.71693206, ae_loss: 0.05287706\n",
      "Step: [2232] total_loss: 2.10320234 d_loss: 1.37450886, g_loss: 0.67618227, ae_loss: 0.05251130\n",
      "Step: [2233] total_loss: 2.10845709 d_loss: 1.35915983, g_loss: 0.69556320, ae_loss: 0.05373394\n",
      "Step: [2234] total_loss: 2.11490536 d_loss: 1.37580347, g_loss: 0.68675888, ae_loss: 0.05234285\n",
      "Step: [2235] total_loss: 2.13767147 d_loss: 1.40590024, g_loss: 0.67869198, ae_loss: 0.05307941\n",
      "Step: [2236] total_loss: 2.15527225 d_loss: 1.40395534, g_loss: 0.70135099, ae_loss: 0.04996603\n",
      "Step: [2237] total_loss: 2.13827014 d_loss: 1.39918900, g_loss: 0.68986011, ae_loss: 0.04922104\n",
      "Step: [2238] total_loss: 2.12438536 d_loss: 1.37524271, g_loss: 0.70073986, ae_loss: 0.04840277\n",
      "Step: [2239] total_loss: 2.12435007 d_loss: 1.37979054, g_loss: 0.69278169, ae_loss: 0.05177779\n",
      "Step: [2240] total_loss: 2.13177943 d_loss: 1.38307118, g_loss: 0.69806755, ae_loss: 0.05064073\n",
      "Step: [2241] total_loss: 2.12979388 d_loss: 1.37857878, g_loss: 0.69841075, ae_loss: 0.05280429\n",
      "Step: [2242] total_loss: 2.13078451 d_loss: 1.39130139, g_loss: 0.68702805, ae_loss: 0.05245498\n",
      "Step: [2243] total_loss: 2.13849258 d_loss: 1.39652157, g_loss: 0.69099182, ae_loss: 0.05097925\n",
      "Step: [2244] total_loss: 2.12866378 d_loss: 1.37577558, g_loss: 0.69769895, ae_loss: 0.05518923\n",
      "Step: [2245] total_loss: 2.13907027 d_loss: 1.40423369, g_loss: 0.68226141, ae_loss: 0.05257528\n",
      "Step: [2246] total_loss: 2.14185619 d_loss: 1.39092875, g_loss: 0.69673026, ae_loss: 0.05419713\n",
      "Step: [2247] total_loss: 2.12674618 d_loss: 1.38355410, g_loss: 0.68807822, ae_loss: 0.05511394\n",
      "Step: [2248] total_loss: 2.12431812 d_loss: 1.35990834, g_loss: 0.71056175, ae_loss: 0.05384819\n",
      "Step: [2249] total_loss: 2.10880232 d_loss: 1.36184359, g_loss: 0.69266176, ae_loss: 0.05429713\n",
      "Step: [2250] total_loss: 2.13215017 d_loss: 1.37687039, g_loss: 0.69745350, ae_loss: 0.05782627\n",
      "Step: [2251] total_loss: 2.11598563 d_loss: 1.36707115, g_loss: 0.69811195, ae_loss: 0.05080250\n",
      "Step: [2252] total_loss: 2.11317301 d_loss: 1.38014293, g_loss: 0.68217337, ae_loss: 0.05085662\n",
      "Step: [2253] total_loss: 2.12016201 d_loss: 1.37214351, g_loss: 0.69493949, ae_loss: 0.05307890\n",
      "Step: [2254] total_loss: 2.12099600 d_loss: 1.35992861, g_loss: 0.70972782, ae_loss: 0.05133963\n",
      "Step: [2255] total_loss: 2.11021352 d_loss: 1.37431717, g_loss: 0.68057591, ae_loss: 0.05532050\n",
      "Step: [2256] total_loss: 2.10852337 d_loss: 1.38709712, g_loss: 0.67100239, ae_loss: 0.05042376\n",
      "Step: [2257] total_loss: 2.11644578 d_loss: 1.40041065, g_loss: 0.66697407, ae_loss: 0.04906110\n",
      "Step: [2258] total_loss: 2.12857747 d_loss: 1.36753631, g_loss: 0.70985287, ae_loss: 0.05118840\n",
      "Step: [2259] total_loss: 2.12225962 d_loss: 1.37687349, g_loss: 0.69566154, ae_loss: 0.04972451\n",
      "Step: [2260] total_loss: 2.12647891 d_loss: 1.38964796, g_loss: 0.68485767, ae_loss: 0.05197335\n",
      "Step: [2261] total_loss: 2.12958026 d_loss: 1.37532234, g_loss: 0.70059365, ae_loss: 0.05366415\n",
      "Step: [2262] total_loss: 2.10820818 d_loss: 1.38163257, g_loss: 0.67619491, ae_loss: 0.05038056\n",
      "Step: [2263] total_loss: 2.12420392 d_loss: 1.36270332, g_loss: 0.70658034, ae_loss: 0.05492030\n",
      "Step: [2264] total_loss: 2.12059307 d_loss: 1.36540604, g_loss: 0.70329332, ae_loss: 0.05189364\n",
      "Step: [2265] total_loss: 2.12761641 d_loss: 1.38933790, g_loss: 0.68619084, ae_loss: 0.05208751\n",
      "Step: [2266] total_loss: 2.13271236 d_loss: 1.38115263, g_loss: 0.69662917, ae_loss: 0.05493040\n",
      "Step: [2267] total_loss: 2.13448381 d_loss: 1.36275542, g_loss: 0.71937048, ae_loss: 0.05235778\n",
      "Step: [2268] total_loss: 2.13458872 d_loss: 1.38066602, g_loss: 0.70321834, ae_loss: 0.05070454\n",
      "Step: [2269] total_loss: 2.15174055 d_loss: 1.39010119, g_loss: 0.70084894, ae_loss: 0.06079037\n",
      "Step: [2270] total_loss: 2.11525822 d_loss: 1.35540605, g_loss: 0.70833302, ae_loss: 0.05151933\n",
      "Step: [2271] total_loss: 2.13700056 d_loss: 1.39346969, g_loss: 0.69188356, ae_loss: 0.05164721\n",
      "Step: [2272] total_loss: 2.10871267 d_loss: 1.36846888, g_loss: 0.69080877, ae_loss: 0.04943496\n",
      "Step: [2273] total_loss: 2.12311721 d_loss: 1.38153601, g_loss: 0.68751401, ae_loss: 0.05406716\n",
      "Step: [2274] total_loss: 2.13128710 d_loss: 1.38085473, g_loss: 0.69660360, ae_loss: 0.05382876\n",
      "Step: [2275] total_loss: 2.12276220 d_loss: 1.35392761, g_loss: 0.72119009, ae_loss: 0.04764444\n",
      "Step: [2276] total_loss: 2.14608884 d_loss: 1.38596928, g_loss: 0.70979929, ae_loss: 0.05032028\n",
      "Step: [2277] total_loss: 2.14704990 d_loss: 1.38980722, g_loss: 0.70646226, ae_loss: 0.05078032\n",
      "Step: [2278] total_loss: 2.13256168 d_loss: 1.37689948, g_loss: 0.70171154, ae_loss: 0.05395070\n",
      "Step: [2279] total_loss: 2.12801528 d_loss: 1.39885402, g_loss: 0.68047631, ae_loss: 0.04868500\n",
      "Step: [2280] total_loss: 2.11044741 d_loss: 1.37798595, g_loss: 0.68017602, ae_loss: 0.05228559\n",
      "Step: [2281] total_loss: 2.10065675 d_loss: 1.36564934, g_loss: 0.68295747, ae_loss: 0.05204986\n",
      "Step: [2282] total_loss: 2.13450241 d_loss: 1.38387585, g_loss: 0.69834173, ae_loss: 0.05228477\n",
      "Step: [2283] total_loss: 2.14735031 d_loss: 1.39803982, g_loss: 0.69897133, ae_loss: 0.05033914\n",
      "Step: [2284] total_loss: 2.13498425 d_loss: 1.38599610, g_loss: 0.70010561, ae_loss: 0.04888259\n",
      "Step: [2285] total_loss: 2.15180302 d_loss: 1.37957323, g_loss: 0.71862727, ae_loss: 0.05360255\n",
      "Step: [2286] total_loss: 2.12227368 d_loss: 1.37001467, g_loss: 0.70270449, ae_loss: 0.04955446\n",
      "Step: [2287] total_loss: 2.13246202 d_loss: 1.40088725, g_loss: 0.68384874, ae_loss: 0.04772609\n",
      "Step: [2288] total_loss: 2.12090373 d_loss: 1.39267504, g_loss: 0.67693579, ae_loss: 0.05129285\n",
      "Step: [2289] total_loss: 2.12224627 d_loss: 1.36335468, g_loss: 0.71095717, ae_loss: 0.04793451\n",
      "Step: [2290] total_loss: 2.10195160 d_loss: 1.35656381, g_loss: 0.69268006, ae_loss: 0.05270777\n",
      "Step: [2291] total_loss: 2.12041664 d_loss: 1.37207818, g_loss: 0.69442976, ae_loss: 0.05390860\n",
      "Step: [2292] total_loss: 2.13283062 d_loss: 1.38294888, g_loss: 0.69199371, ae_loss: 0.05788794\n",
      "Step: [2293] total_loss: 2.12829852 d_loss: 1.39345551, g_loss: 0.68076122, ae_loss: 0.05408180\n",
      "Step: [2294] total_loss: 2.10581398 d_loss: 1.34755945, g_loss: 0.70800865, ae_loss: 0.05024586\n",
      "Step: [2295] total_loss: 2.11394644 d_loss: 1.37886310, g_loss: 0.68462861, ae_loss: 0.05045480\n",
      "Step: [2296] total_loss: 2.14200377 d_loss: 1.36930346, g_loss: 0.71973640, ae_loss: 0.05296380\n",
      "Step: [2297] total_loss: 2.12564015 d_loss: 1.38720846, g_loss: 0.68659902, ae_loss: 0.05183265\n",
      "Step: [2298] total_loss: 2.12052417 d_loss: 1.37709594, g_loss: 0.69187486, ae_loss: 0.05155336\n",
      "Step: [2299] total_loss: 2.13363075 d_loss: 1.39651489, g_loss: 0.68397701, ae_loss: 0.05313902\n",
      "Step: [2300] total_loss: 2.14293909 d_loss: 1.40700424, g_loss: 0.68578649, ae_loss: 0.05014826\n",
      "Step: [2301] total_loss: 2.15215707 d_loss: 1.40624583, g_loss: 0.69025481, ae_loss: 0.05565644\n",
      "Step: [2302] total_loss: 2.15519667 d_loss: 1.39553726, g_loss: 0.70241964, ae_loss: 0.05723977\n",
      "Step: [2303] total_loss: 2.11776090 d_loss: 1.37026811, g_loss: 0.69821978, ae_loss: 0.04927304\n",
      "Step: [2304] total_loss: 2.11030650 d_loss: 1.36382532, g_loss: 0.69136363, ae_loss: 0.05511766\n",
      "Step: [2305] total_loss: 2.11432362 d_loss: 1.36995053, g_loss: 0.69112825, ae_loss: 0.05324489\n",
      "Step: [2306] total_loss: 2.09507823 d_loss: 1.36476254, g_loss: 0.67887568, ae_loss: 0.05143997\n",
      "Step: [2307] total_loss: 2.10325050 d_loss: 1.36928773, g_loss: 0.68269110, ae_loss: 0.05127185\n",
      "Step: [2308] total_loss: 2.11144066 d_loss: 1.37889695, g_loss: 0.68262768, ae_loss: 0.04991604\n",
      "Step: [2309] total_loss: 2.10755920 d_loss: 1.34930634, g_loss: 0.69964099, ae_loss: 0.05861169\n",
      "Step: [2310] total_loss: 2.09154081 d_loss: 1.36972976, g_loss: 0.67395103, ae_loss: 0.04786019\n",
      "Step: [2311] total_loss: 2.14009190 d_loss: 1.39661646, g_loss: 0.69311261, ae_loss: 0.05036267\n",
      "Step: [2312] total_loss: 2.15490603 d_loss: 1.39523029, g_loss: 0.70063746, ae_loss: 0.05903823\n",
      "Step: [2313] total_loss: 2.15246868 d_loss: 1.39284444, g_loss: 0.70753610, ae_loss: 0.05208822\n",
      "Step: [2314] total_loss: 2.13425493 d_loss: 1.37072396, g_loss: 0.71140671, ae_loss: 0.05212430\n",
      "Step: [2315] total_loss: 2.10229301 d_loss: 1.37208450, g_loss: 0.67962062, ae_loss: 0.05058780\n",
      "Step: [2316] total_loss: 2.11422515 d_loss: 1.37082505, g_loss: 0.69000614, ae_loss: 0.05339393\n",
      "Step: [2317] total_loss: 2.14329481 d_loss: 1.40034604, g_loss: 0.68643701, ae_loss: 0.05651171\n",
      "Step: [2318] total_loss: 2.14455986 d_loss: 1.40201592, g_loss: 0.68716562, ae_loss: 0.05537846\n",
      "Step: [2319] total_loss: 2.13730097 d_loss: 1.39121556, g_loss: 0.69372642, ae_loss: 0.05235894\n",
      "Step: [2320] total_loss: 2.11285949 d_loss: 1.35645127, g_loss: 0.70337111, ae_loss: 0.05303715\n",
      "Step: [2321] total_loss: 2.12245846 d_loss: 1.37811589, g_loss: 0.69375789, ae_loss: 0.05058483\n",
      "Step: [2322] total_loss: 2.12598467 d_loss: 1.38253641, g_loss: 0.69233018, ae_loss: 0.05111799\n",
      "Step: [2323] total_loss: 2.12300444 d_loss: 1.39193225, g_loss: 0.67886770, ae_loss: 0.05220442\n",
      "Step: [2324] total_loss: 2.12461948 d_loss: 1.38189507, g_loss: 0.68981171, ae_loss: 0.05291265\n",
      "Step: [2325] total_loss: 2.11660480 d_loss: 1.36776757, g_loss: 0.69859886, ae_loss: 0.05023823\n",
      "Step: [2326] total_loss: 2.14162111 d_loss: 1.37869477, g_loss: 0.71373975, ae_loss: 0.04918666\n",
      "Step: [2327] total_loss: 2.11562991 d_loss: 1.36137795, g_loss: 0.70134389, ae_loss: 0.05290812\n",
      "Step: [2328] total_loss: 2.14843965 d_loss: 1.39471197, g_loss: 0.70228654, ae_loss: 0.05144116\n",
      "Step: [2329] total_loss: 2.13292265 d_loss: 1.39072609, g_loss: 0.69042444, ae_loss: 0.05177218\n",
      "Step: [2330] total_loss: 2.14762759 d_loss: 1.38174284, g_loss: 0.71137148, ae_loss: 0.05451321\n",
      "Step: [2331] total_loss: 2.12957382 d_loss: 1.35586679, g_loss: 0.72217965, ae_loss: 0.05152740\n",
      "Step: [2332] total_loss: 2.10097694 d_loss: 1.37078357, g_loss: 0.67661786, ae_loss: 0.05357558\n",
      "Step: [2333] total_loss: 2.10907030 d_loss: 1.37655020, g_loss: 0.68196076, ae_loss: 0.05055924\n",
      "Step: [2334] total_loss: 2.11361408 d_loss: 1.38216066, g_loss: 0.67947900, ae_loss: 0.05197437\n",
      "Step: [2335] total_loss: 2.13512516 d_loss: 1.41994035, g_loss: 0.66343510, ae_loss: 0.05174984\n",
      "Step: [2336] total_loss: 2.11626434 d_loss: 1.36201954, g_loss: 0.70458162, ae_loss: 0.04966303\n",
      "Step: [2337] total_loss: 2.12972546 d_loss: 1.39042723, g_loss: 0.68705082, ae_loss: 0.05224726\n",
      "Step: [2338] total_loss: 2.15413237 d_loss: 1.40428519, g_loss: 0.69557381, ae_loss: 0.05427327\n",
      "Step: [2339] total_loss: 2.12725329 d_loss: 1.37761927, g_loss: 0.69795620, ae_loss: 0.05167785\n",
      "Step: [2340] total_loss: 2.15245676 d_loss: 1.36063075, g_loss: 0.73779571, ae_loss: 0.05403018\n",
      "Step: [2341] total_loss: 2.12923908 d_loss: 1.37199569, g_loss: 0.70641375, ae_loss: 0.05082952\n",
      "Step: [2342] total_loss: 2.13076639 d_loss: 1.38741374, g_loss: 0.69037795, ae_loss: 0.05297479\n",
      "Step: [2343] total_loss: 2.12384820 d_loss: 1.38788462, g_loss: 0.67980683, ae_loss: 0.05615672\n",
      "Step: [2344] total_loss: 2.11935759 d_loss: 1.40025902, g_loss: 0.66760254, ae_loss: 0.05149596\n",
      "Step: [2345] total_loss: 2.12145901 d_loss: 1.38146448, g_loss: 0.68590045, ae_loss: 0.05409407\n",
      "Step: [2346] total_loss: 2.16684055 d_loss: 1.40677238, g_loss: 0.70797193, ae_loss: 0.05209631\n",
      "Step: [2347] total_loss: 2.14334273 d_loss: 1.38845015, g_loss: 0.70222479, ae_loss: 0.05266783\n",
      "Step: [2348] total_loss: 2.13135338 d_loss: 1.38378811, g_loss: 0.69126743, ae_loss: 0.05629782\n",
      "Step: [2349] total_loss: 2.12401915 d_loss: 1.39611554, g_loss: 0.67581570, ae_loss: 0.05208802\n",
      "Step: [2350] total_loss: 2.12571573 d_loss: 1.38154912, g_loss: 0.69249558, ae_loss: 0.05167108\n",
      "Step: [2351] total_loss: 2.10490131 d_loss: 1.36813712, g_loss: 0.68598920, ae_loss: 0.05077496\n",
      "Step: [2352] total_loss: 2.12506628 d_loss: 1.38117146, g_loss: 0.69103688, ae_loss: 0.05285782\n",
      "Step: [2353] total_loss: 2.10552788 d_loss: 1.36007810, g_loss: 0.69541991, ae_loss: 0.05002991\n",
      "Step: [2354] total_loss: 2.10746908 d_loss: 1.37048972, g_loss: 0.68469143, ae_loss: 0.05228787\n",
      "Step: [2355] total_loss: 2.14732695 d_loss: 1.41263199, g_loss: 0.68399131, ae_loss: 0.05070363\n",
      "Step: [2356] total_loss: 2.12625074 d_loss: 1.36257505, g_loss: 0.71201283, ae_loss: 0.05166297\n",
      "Step: [2357] total_loss: 2.12217474 d_loss: 1.37109983, g_loss: 0.70160824, ae_loss: 0.04946658\n",
      "Step: [2358] total_loss: 2.10800934 d_loss: 1.37022662, g_loss: 0.68725884, ae_loss: 0.05052397\n",
      "Step: [2359] total_loss: 2.14050770 d_loss: 1.36798978, g_loss: 0.71851969, ae_loss: 0.05399833\n",
      "Step: [2360] total_loss: 2.13132381 d_loss: 1.38514888, g_loss: 0.69313538, ae_loss: 0.05303969\n",
      "Step: [2361] total_loss: 2.11750507 d_loss: 1.36890793, g_loss: 0.70216519, ae_loss: 0.04643199\n",
      "Step: [2362] total_loss: 2.13348007 d_loss: 1.38837695, g_loss: 0.69413090, ae_loss: 0.05097235\n",
      "Step: [2363] total_loss: 2.11697793 d_loss: 1.37181354, g_loss: 0.69434059, ae_loss: 0.05082383\n",
      "Step: [2364] total_loss: 2.14691162 d_loss: 1.40221691, g_loss: 0.69494188, ae_loss: 0.04975270\n",
      "Step: [2365] total_loss: 2.14228439 d_loss: 1.40439403, g_loss: 0.68690705, ae_loss: 0.05098343\n",
      "Step: [2366] total_loss: 2.15425873 d_loss: 1.37898743, g_loss: 0.72613466, ae_loss: 0.04913656\n",
      "Step: [2367] total_loss: 2.14977217 d_loss: 1.37923312, g_loss: 0.71870857, ae_loss: 0.05183041\n",
      "Step: [2368] total_loss: 2.13663578 d_loss: 1.37808096, g_loss: 0.70812190, ae_loss: 0.05043301\n",
      "Step: [2369] total_loss: 2.12018514 d_loss: 1.37309074, g_loss: 0.69483471, ae_loss: 0.05225974\n",
      "Step: [2370] total_loss: 2.11238194 d_loss: 1.35114789, g_loss: 0.71012700, ae_loss: 0.05110692\n",
      "Step: [2371] total_loss: 2.11693048 d_loss: 1.38583875, g_loss: 0.67864889, ae_loss: 0.05244278\n",
      "Step: [2372] total_loss: 2.12219048 d_loss: 1.39377618, g_loss: 0.67405272, ae_loss: 0.05436153\n",
      "Step: [2373] total_loss: 2.12986898 d_loss: 1.38184226, g_loss: 0.69336152, ae_loss: 0.05466528\n",
      "Step: [2374] total_loss: 2.13811374 d_loss: 1.38728690, g_loss: 0.70117635, ae_loss: 0.04965058\n",
      "Step: [2375] total_loss: 2.15894318 d_loss: 1.40166688, g_loss: 0.70517576, ae_loss: 0.05210069\n",
      "Step: [2376] total_loss: 2.12012386 d_loss: 1.37681961, g_loss: 0.69250762, ae_loss: 0.05079654\n",
      "Step: [2377] total_loss: 2.14170909 d_loss: 1.39776266, g_loss: 0.69304907, ae_loss: 0.05089733\n",
      "Step: [2378] total_loss: 2.12442040 d_loss: 1.37001872, g_loss: 0.69819319, ae_loss: 0.05620850\n",
      "Step: [2379] total_loss: 2.12065005 d_loss: 1.36459279, g_loss: 0.70490152, ae_loss: 0.05115567\n",
      "Step: [2380] total_loss: 2.15029335 d_loss: 1.38773537, g_loss: 0.71003497, ae_loss: 0.05252310\n",
      "Step: [2381] total_loss: 2.13412380 d_loss: 1.40224457, g_loss: 0.68251729, ae_loss: 0.04936203\n",
      "Step: [2382] total_loss: 2.11139870 d_loss: 1.38143063, g_loss: 0.67979848, ae_loss: 0.05016956\n",
      "Step: [2383] total_loss: 2.11420345 d_loss: 1.37265623, g_loss: 0.69195282, ae_loss: 0.04959426\n",
      "Step: [2384] total_loss: 2.11541700 d_loss: 1.37568676, g_loss: 0.69130594, ae_loss: 0.04842439\n",
      "Step: [2385] total_loss: 2.13243747 d_loss: 1.40565062, g_loss: 0.67138356, ae_loss: 0.05540333\n",
      "Step: [2386] total_loss: 2.14814568 d_loss: 1.37225056, g_loss: 0.72304666, ae_loss: 0.05284850\n",
      "Step: [2387] total_loss: 2.10875034 d_loss: 1.36753273, g_loss: 0.68751371, ae_loss: 0.05370382\n",
      "Step: [2388] total_loss: 2.10926628 d_loss: 1.37242341, g_loss: 0.68596411, ae_loss: 0.05087874\n",
      "Step: [2389] total_loss: 2.11518884 d_loss: 1.38945937, g_loss: 0.67644572, ae_loss: 0.04928373\n",
      "Step: [2390] total_loss: 2.13370585 d_loss: 1.36724806, g_loss: 0.71280777, ae_loss: 0.05364997\n",
      "Step: [2391] total_loss: 2.14836955 d_loss: 1.41910720, g_loss: 0.67764747, ae_loss: 0.05161486\n",
      "Step: [2392] total_loss: 2.11143255 d_loss: 1.35812211, g_loss: 0.70266867, ae_loss: 0.05064166\n",
      "Step: [2393] total_loss: 2.11855102 d_loss: 1.37881637, g_loss: 0.68722689, ae_loss: 0.05250775\n",
      "Step: [2394] total_loss: 2.14457059 d_loss: 1.41435075, g_loss: 0.67936367, ae_loss: 0.05085609\n",
      "Step: [2395] total_loss: 2.12629271 d_loss: 1.37028146, g_loss: 0.70933580, ae_loss: 0.04667554\n",
      "Step: [2396] total_loss: 2.11134315 d_loss: 1.36506867, g_loss: 0.69304353, ae_loss: 0.05323084\n",
      "Step: [2397] total_loss: 2.11111307 d_loss: 1.37767398, g_loss: 0.68317115, ae_loss: 0.05026803\n",
      "Step: [2398] total_loss: 2.11949587 d_loss: 1.38409996, g_loss: 0.68347824, ae_loss: 0.05191754\n",
      "Step: [2399] total_loss: 2.12231183 d_loss: 1.37857139, g_loss: 0.69575477, ae_loss: 0.04798562\n",
      "Step: [2400] total_loss: 2.12029195 d_loss: 1.34713173, g_loss: 0.72391534, ae_loss: 0.04924492\n",
      "Step: [2401] total_loss: 2.12737012 d_loss: 1.38934922, g_loss: 0.68585819, ae_loss: 0.05216260\n",
      "Step: [2402] total_loss: 2.14040756 d_loss: 1.39723706, g_loss: 0.69157016, ae_loss: 0.05160039\n",
      "Step: [2403] total_loss: 2.09225345 d_loss: 1.36047792, g_loss: 0.67925322, ae_loss: 0.05252235\n",
      "Step: [2404] total_loss: 2.12729502 d_loss: 1.38427424, g_loss: 0.69098973, ae_loss: 0.05203113\n",
      "Step: [2405] total_loss: 2.13026667 d_loss: 1.36313796, g_loss: 0.71545947, ae_loss: 0.05166911\n",
      "Step: [2406] total_loss: 2.11494064 d_loss: 1.37924194, g_loss: 0.68359053, ae_loss: 0.05210805\n",
      "Step: [2407] total_loss: 2.12506557 d_loss: 1.38466322, g_loss: 0.68809038, ae_loss: 0.05231201\n",
      "Step: [2408] total_loss: 2.12262487 d_loss: 1.39035511, g_loss: 0.68178266, ae_loss: 0.05048712\n",
      "Step: [2409] total_loss: 2.12401175 d_loss: 1.35832644, g_loss: 0.71231359, ae_loss: 0.05337178\n",
      "Step: [2410] total_loss: 2.09887934 d_loss: 1.37073851, g_loss: 0.67919099, ae_loss: 0.04894986\n",
      "Step: [2411] total_loss: 2.11846733 d_loss: 1.37981808, g_loss: 0.69013965, ae_loss: 0.04850947\n",
      "Step: [2412] total_loss: 2.12534976 d_loss: 1.38436306, g_loss: 0.68981946, ae_loss: 0.05116725\n",
      "Step: [2413] total_loss: 2.13159084 d_loss: 1.38969779, g_loss: 0.68814081, ae_loss: 0.05375225\n",
      "Step: [2414] total_loss: 2.12482524 d_loss: 1.36526215, g_loss: 0.70413160, ae_loss: 0.05543149\n",
      "Step: [2415] total_loss: 2.14456034 d_loss: 1.40400791, g_loss: 0.68806577, ae_loss: 0.05248663\n",
      "Step: [2416] total_loss: 2.11509442 d_loss: 1.36385298, g_loss: 0.70055592, ae_loss: 0.05068550\n",
      "Step: [2417] total_loss: 2.14302015 d_loss: 1.37267017, g_loss: 0.71872723, ae_loss: 0.05162290\n",
      "Step: [2418] total_loss: 2.12446785 d_loss: 1.36391616, g_loss: 0.71009517, ae_loss: 0.05045649\n",
      "Step: [2419] total_loss: 2.13689232 d_loss: 1.36383009, g_loss: 0.72023469, ae_loss: 0.05282763\n",
      "Step: [2420] total_loss: 2.13794041 d_loss: 1.37743866, g_loss: 0.70804393, ae_loss: 0.05245788\n",
      "Step: [2421] total_loss: 2.14035511 d_loss: 1.39409995, g_loss: 0.69617975, ae_loss: 0.05007527\n",
      "Step: [2422] total_loss: 2.13057804 d_loss: 1.38154304, g_loss: 0.69180220, ae_loss: 0.05723279\n",
      "Step: [2423] total_loss: 2.12368751 d_loss: 1.39317930, g_loss: 0.68012905, ae_loss: 0.05037921\n",
      "Step: [2424] total_loss: 2.09495330 d_loss: 1.36379838, g_loss: 0.68383718, ae_loss: 0.04731771\n",
      "Step: [2425] total_loss: 2.11740327 d_loss: 1.36768031, g_loss: 0.70033520, ae_loss: 0.04938783\n",
      "Step: [2426] total_loss: 2.11412287 d_loss: 1.37126577, g_loss: 0.69289148, ae_loss: 0.04996562\n",
      "Step: [2427] total_loss: 2.14690256 d_loss: 1.38291073, g_loss: 0.71375036, ae_loss: 0.05024150\n",
      "Step: [2428] total_loss: 2.13721228 d_loss: 1.38981557, g_loss: 0.69221139, ae_loss: 0.05518541\n",
      "Step: [2429] total_loss: 2.12753415 d_loss: 1.37270868, g_loss: 0.70262790, ae_loss: 0.05219757\n",
      "Step: [2430] total_loss: 2.14368796 d_loss: 1.39344871, g_loss: 0.69962090, ae_loss: 0.05061844\n",
      "Step: [2431] total_loss: 2.14625263 d_loss: 1.39720023, g_loss: 0.69683319, ae_loss: 0.05221922\n",
      "Step: [2432] total_loss: 2.11061287 d_loss: 1.38704252, g_loss: 0.67321265, ae_loss: 0.05035765\n",
      "Step: [2433] total_loss: 2.12367725 d_loss: 1.37411594, g_loss: 0.69564569, ae_loss: 0.05391558\n",
      "Step: [2434] total_loss: 2.13214827 d_loss: 1.38473678, g_loss: 0.69358766, ae_loss: 0.05382367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ae60bac12ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m'''update generator network, run 2 times'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={keep_prob : 0.9})\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.astype(np.float32)\n",
    "    #batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    \n",
    "    '''get the noise data'''\n",
    "    if prior_type =='mixGaussian':\n",
    "        z_label_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = gaussian_mixture(batch_size, z_dim, label_indices=z_label_rand)\n",
    "    elif prior_type == 'swiss_roll':\n",
    "        z_label_rand = np.random.randint(0, 10, size=[batch_size])\n",
    "        batch_z = swiss_roll(batch_size, z_dim, label_indices=z_label_rand)\n",
    "    elif prior_type == 'normal':\n",
    "        batch_z, z_label_rand = gaussian(batch_size, z_dim, use_label_info=True)    \n",
    "    else:\n",
    "        raise Exception(f\"[!] There is no option for {prior_type}\" )\n",
    "\n",
    "    z_id_one_hot_vector = np.zeros((batch_size, y_dim))\n",
    "    z_id_one_hot_vector[np.arange(batch_size), z_label_rand] = 1\n",
    "    \n",
    "    feed_dict_input = {\n",
    "        x_input:batch_images,\n",
    "        x_target:batch_images,\n",
    "        x_label:batch_labels,\n",
    "        z_sample:batch_z,\n",
    "        z_label:z_id_one_hot_vector,\n",
    "        keep_prob: 0.9\n",
    "    }\n",
    "    '''update AE network''' \n",
    "    _, loss_likehood = sess.run([ae_optim, neg_marginal_likelihood], feed_dict=feed_dict_input)\n",
    "    '''update discriminator network'''\n",
    "    _, d_loss = sess.run([d_optim, D_loss], feed_dict=feed_dict_input)\n",
    "    '''update generator network, run 2 times'''\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "    _, g_loss = sess.run([g_optim, G_loss], feed_dict=feed_dict_input)\n",
    "     \n",
    "    total_loss = loss_likehood + d_loss + g_loss\n",
    "    # display training status\n",
    "    print(\"Step: [%d] total_loss: %.8f d_loss: %.8f, g_loss: %.8f, ae_loss: %.8f\" % (step_ind, total_loss, d_loss, g_loss, loss_likehood) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "        samples = sess.run(images_reconstruction, feed_dict={x_input:test_batch_images, keep_prob:1})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :].reshape(image_dims[:2])\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
