{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most codes from https://github.com/hwalsuklee/tensorflow-generative-model-collections/blob/master/EBGAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn(net,scope,is_training):\n",
    "    return tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope=scope)\n",
    "\n",
    "def conv(net,wscope,bscope,output_depth = 64, receptive_field=[5,5],stride=[2,2]):\n",
    "    shape = net.get_shape()\n",
    "    weights = tf.get_variable(wscope, receptive_field+[shape[-1], output_depth],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.nn.conv2d(net, weights, strides=[1]+stride+[1], padding='SAME')\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def linear(net,wscope,bscope,output_depth):\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "\n",
    "def deconv(net,wscope,bscope,output_shape,receptive_field=[5,5],stride=[2,2]):\n",
    "    weights = tf.get_variable(wscope, receptive_field+[output_shape[-1], net.get_shape()[-1]],\n",
    "                                       initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "    net = tf.nn.conv2d_transpose(net, weights, output_shape=output_shape, strides=[1]+stride+[1])\n",
    "    biases = tf.get_variable(bscope, [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def get_shape(net):\n",
    "    return net.get_shape().as_list()\n",
    "\n",
    "# borrowed from https://github.com/shekkizh/EBGAN.tensorflow/blob/master/EBGAN/Faces_EBGAN.py\n",
    "def pullaway_loss(embeddings):\n",
    "    \"\"\"\n",
    "    Pull Away loss calculation\n",
    "    :param embeddings: The embeddings to be orthogonalized for varied faces. Shape [batch_size, embeddings_dim]\n",
    "    :return: pull away term loss\n",
    "    \"\"\"\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    similarity = tf.matmul(normalized_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    batch_size = tf.cast(tf.shape(embeddings)[0], tf.float32)\n",
    "    pt_loss = (tf.reduce_sum(similarity) - batch_size) / (batch_size * (batch_size - 1))\n",
    "    return pt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"discriminator\" function, the \"discriminator_y\" only has more one \"y\" parameter than \"discriminator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inputs, batch_size, output_height=28, output_width=28,output_depth=1,is_training=True, reuse=False):\n",
    "    # It must be Auto-Encoder style architecture\n",
    "    # Architecture : (64)4c2s-FC32-FC64*14*14_BR-(1)4dc2s_S    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: Conv -> relu'''\n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',output_depth=64,receptive_field=[4,4])\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''2nd: reshape -> linear'''\n",
    "        net = tf.reshape(net,[batch_size,-1])\n",
    "        code = linear(net,'d_wlinear2','d_blinear2',32)\n",
    "\n",
    "        '''3th: linear -> bn -> relu'''\n",
    "        net = linear(code,'d_wlinear3','d_blinear3',64*14*14)\n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''4th: reshape -> deconv -> sigmoid'''\n",
    "        net = tf.reshape(net,[batch_size,14,14,64])\n",
    "        output_shape = [batch_size, output_height, output_width, output_depth]\n",
    "        net = deconv(net,'d_wdeconv4','d_bdeconv4',output_shape, receptive_field=[4,4])\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        #recon loss\n",
    "        recon_error = tf.sqrt(2 * tf.nn.l2_loss(out - inputs)) / batch_size\n",
    "\n",
    "        return out, recon_error, code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,i will write two \"generator\" function, the \"generator_y\" only has more one \"y\" parameter than \"generator\". Spliting them into two function in order to easily understanding the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator( z, batch_size, output_height=28, output_width=28,output_depth=1, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S    \n",
    "    height0,width0 = output_height,output_width\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2)\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2)\n",
    "    height8,width8 = math.ceil(float(height4)/2),math.ceil(float(width4)/2)\n",
    "    height16,width16 = math.ceil(float(height8)/2),math.ceil(float(width8)/2)\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: linear -> bn -> relu '''\n",
    "        net = linear(z,\"g_wlinear1\",\"g_blinear1\",1024)\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        '''2nd: linear -> bn -> relu '''\n",
    "        net = linear(net,'g_wlinear2','g_blinear2',128*7*7)\n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''3th: reshape -> deconv -> bn -> relu '''\n",
    "        net = tf.reshape(net, [batch_size, 7, 7, 128])\n",
    "        output_shape = [batch_size, 14, 14, 64]\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape, receptive_field=[4,4])\n",
    "        net = bn(net,scope='g_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        '''4th: deconv -> sigmoid '''\n",
    "        output_shape = [batch_size, output_height, output_width, output_depth]\n",
    "        net = deconv(net,'g_wdeconv4','g_bdeconv4',output_shape,receptive_field=[4,4])\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "y_dim = 10\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "pt_loss_weight = 0.1\n",
    "# usually margin of 1 is enough, but for large batch size it must be larger than 1\n",
    "margin = max(1,batch_size/64.)\n",
    "output_height,output_width,output_depth = [28,28,1]\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "#labels\n",
    "#y = tf.placeholder(tf.float32, [batch_size,y_dim], name='y')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "# output of D for real images\n",
    "D_real, D_real_err, D_real_code = discriminator(inputs, batch_size, is_training=True, reuse=False)\n",
    "# output of D for fake images\n",
    "G = generator(z, batch_size, is_training=True, reuse=False)\n",
    "D_fake, D_fake_err, D_fake_code = discriminator(G, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "    \n",
    "# get loss for discriminator\n",
    "#d_loss_real = tf.reduce_mean(\n",
    "#              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "#d_loss_fake = tf.reduce_mean(\n",
    "#              tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "#d_loss = d_loss_real + d_loss_fake\n",
    "d_loss = D_real_err + tf.maximum(margin-D_fake_err, 0)\n",
    "\n",
    "\n",
    "\n",
    "# get loss for generator\n",
    "#g_loss = tf.reduce_mean(\n",
    "#         tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
    "g_loss = D_fake_err + pt_loss_weight*pullaway_loss(D_fake_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate*5, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "fake_images = generator(z,batch_size, is_training=False, reuse=True)\n",
    "\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim))\n",
    "test_labels_onehot = np.zeros([batch_size, y_dim],dtype = np.float32)\n",
    "test_labels_onehot[np.arange(batch_size), np.arange(batch_size)%int(np.sqrt(batch_size))] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 2.45882535, g_loss: 0.34812006\n",
      "Step: [1] d_loss: 2.53511095, g_loss: 0.21076630\n",
      "Step: [2] d_loss: 2.56526852, g_loss: 0.18194437\n",
      "Step: [3] d_loss: 2.54498911, g_loss: 0.16014083\n",
      "Step: [4] d_loss: 2.57091236, g_loss: 0.15100284\n",
      "Step: [5] d_loss: 2.56231451, g_loss: 0.14720303\n",
      "Step: [6] d_loss: 2.54625654, g_loss: 0.14526439\n",
      "Step: [7] d_loss: 2.54714274, g_loss: 0.14290920\n",
      "Step: [8] d_loss: 2.53388119, g_loss: 0.14021786\n",
      "Step: [9] d_loss: 2.52435565, g_loss: 0.13336349\n",
      "Step: [10] d_loss: 2.52245092, g_loss: 0.13507384\n",
      "Step: [11] d_loss: 2.49683666, g_loss: 0.13153760\n",
      "Step: [12] d_loss: 2.48992157, g_loss: 0.13418341\n",
      "Step: [13] d_loss: 2.46706533, g_loss: 0.12897038\n",
      "Step: [14] d_loss: 2.46490669, g_loss: 0.13702089\n",
      "Step: [15] d_loss: 2.43763924, g_loss: 0.12864146\n",
      "Step: [16] d_loss: 2.43481350, g_loss: 0.13071549\n",
      "Step: [17] d_loss: 2.42003536, g_loss: 0.13110904\n",
      "Step: [18] d_loss: 2.40537620, g_loss: 0.15050592\n",
      "Step: [19] d_loss: 2.38636112, g_loss: 0.15532562\n",
      "Step: [20] d_loss: 2.37999511, g_loss: 0.14610128\n",
      "Step: [21] d_loss: 2.38525534, g_loss: 0.14221027\n",
      "Step: [22] d_loss: 2.36490941, g_loss: 0.13766219\n",
      "Step: [23] d_loss: 2.34745502, g_loss: 0.13906778\n",
      "Step: [24] d_loss: 2.34596443, g_loss: 0.13307711\n",
      "Step: [25] d_loss: 2.32730961, g_loss: 0.12842514\n",
      "Step: [26] d_loss: 2.32027960, g_loss: 0.13113399\n",
      "Step: [27] d_loss: 2.30786204, g_loss: 0.13000500\n",
      "Step: [28] d_loss: 2.30064344, g_loss: 0.12727286\n",
      "Step: [29] d_loss: 2.29580569, g_loss: 0.12923412\n",
      "Step: [30] d_loss: 2.27304935, g_loss: 0.13440938\n",
      "Step: [31] d_loss: 2.26331234, g_loss: 0.15136319\n",
      "Step: [32] d_loss: 2.23851824, g_loss: 0.13940960\n",
      "Step: [33] d_loss: 2.24229097, g_loss: 0.16675460\n",
      "Step: [34] d_loss: 2.21786118, g_loss: 0.13437235\n",
      "Step: [35] d_loss: 2.23597050, g_loss: 0.15560678\n",
      "Step: [36] d_loss: 2.16600800, g_loss: 0.17513508\n",
      "Step: [37] d_loss: 2.13789439, g_loss: 0.16404003\n",
      "Step: [38] d_loss: 2.16062784, g_loss: 0.32232133\n",
      "Step: [39] d_loss: 2.02312565, g_loss: 0.21549095\n",
      "Step: [40] d_loss: 2.11058998, g_loss: 0.21289262\n",
      "Step: [41] d_loss: 1.99418056, g_loss: 0.30122480\n",
      "Step: [42] d_loss: 1.95899582, g_loss: 0.46867442\n",
      "Step: [43] d_loss: 1.99651122, g_loss: 0.35840848\n",
      "Step: [44] d_loss: 2.02061319, g_loss: 0.27240881\n",
      "Step: [45] d_loss: 1.98358274, g_loss: 0.31080619\n",
      "Step: [46] d_loss: 2.06555390, g_loss: 0.32135293\n",
      "Step: [47] d_loss: 2.08487415, g_loss: 0.19322181\n",
      "Step: [48] d_loss: 2.11136031, g_loss: 0.18424517\n",
      "Step: [49] d_loss: 1.98637581, g_loss: 0.15839021\n",
      "Step: [50] d_loss: 2.00157356, g_loss: 0.16882125\n",
      "Step: [51] d_loss: 2.02687836, g_loss: 0.17866665\n",
      "Step: [52] d_loss: 2.06449413, g_loss: 0.26313215\n",
      "Step: [53] d_loss: 2.05677843, g_loss: 0.18406433\n",
      "Step: [54] d_loss: 2.00213814, g_loss: 0.22070369\n",
      "Step: [55] d_loss: 2.01275539, g_loss: 0.28579229\n",
      "Step: [56] d_loss: 2.02959347, g_loss: 0.19193172\n",
      "Step: [57] d_loss: 2.04439735, g_loss: 0.18342753\n",
      "Step: [58] d_loss: 2.02393866, g_loss: 0.20016372\n",
      "Step: [59] d_loss: 1.94847775, g_loss: 0.21869519\n",
      "Step: [60] d_loss: 2.03152418, g_loss: 0.18794340\n",
      "Step: [61] d_loss: 1.98099351, g_loss: 0.18594575\n",
      "Step: [62] d_loss: 1.98290229, g_loss: 0.20022392\n",
      "Step: [63] d_loss: 2.00258327, g_loss: 0.22947489\n",
      "Step: [64] d_loss: 1.88517046, g_loss: 0.18975076\n",
      "Step: [65] d_loss: 1.99906707, g_loss: 0.18367204\n",
      "Step: [66] d_loss: 2.01572037, g_loss: 0.18217704\n",
      "Step: [67] d_loss: 1.99126720, g_loss: 0.19389695\n",
      "Step: [68] d_loss: 1.96146679, g_loss: 0.17507413\n",
      "Step: [69] d_loss: 1.94141352, g_loss: 0.22663480\n",
      "Step: [70] d_loss: 1.98148775, g_loss: 0.16919382\n",
      "Step: [71] d_loss: 1.96628606, g_loss: 0.20372033\n",
      "Step: [72] d_loss: 1.97449791, g_loss: 0.20225379\n",
      "Step: [73] d_loss: 1.96564853, g_loss: 0.19031236\n",
      "Step: [74] d_loss: 1.89924312, g_loss: 0.20603514\n",
      "Step: [75] d_loss: 1.94875574, g_loss: 0.21315622\n",
      "Step: [76] d_loss: 1.89363790, g_loss: 0.19533798\n",
      "Step: [77] d_loss: 1.93903852, g_loss: 0.20899066\n",
      "Step: [78] d_loss: 1.95275378, g_loss: 0.21100336\n",
      "Step: [79] d_loss: 1.91017938, g_loss: 0.20298404\n",
      "Step: [80] d_loss: 1.87579203, g_loss: 0.22722468\n",
      "Step: [81] d_loss: 1.92575288, g_loss: 0.18086782\n",
      "Step: [82] d_loss: 1.94258571, g_loss: 0.19847754\n",
      "Step: [83] d_loss: 1.88317072, g_loss: 0.18865380\n",
      "Step: [84] d_loss: 1.90968287, g_loss: 0.17815806\n",
      "Step: [85] d_loss: 1.95238197, g_loss: 0.19281670\n",
      "Step: [86] d_loss: 1.85400271, g_loss: 0.19582421\n",
      "Step: [87] d_loss: 1.88253665, g_loss: 0.19741467\n",
      "Step: [88] d_loss: 1.89296043, g_loss: 0.27770787\n",
      "Step: [89] d_loss: 1.81214404, g_loss: 0.22942361\n",
      "Step: [90] d_loss: 1.82310963, g_loss: 0.40121782\n",
      "Step: [91] d_loss: 1.74435031, g_loss: 0.23910472\n",
      "Step: [92] d_loss: 1.74596620, g_loss: 0.36769038\n",
      "Step: [93] d_loss: 1.79513156, g_loss: 0.39397436\n",
      "Step: [94] d_loss: 1.69764853, g_loss: 0.35659903\n",
      "Step: [95] d_loss: 1.74221277, g_loss: 0.35221982\n",
      "Step: [96] d_loss: 1.78040504, g_loss: 0.44267258\n",
      "Step: [97] d_loss: 1.75621557, g_loss: 0.56488413\n",
      "Step: [98] d_loss: 1.49419379, g_loss: 0.56068426\n",
      "Step: [99] d_loss: 1.39755487, g_loss: 0.48455009\n",
      "Step: [100] d_loss: 1.44276929, g_loss: 0.68139708\n",
      "Step: [101] d_loss: 1.60982478, g_loss: 0.54700816\n",
      "Step: [102] d_loss: 1.45373058, g_loss: 0.60994786\n",
      "Step: [103] d_loss: 1.53028703, g_loss: 0.44117975\n",
      "Step: [104] d_loss: 1.65563297, g_loss: 0.41663429\n",
      "Step: [105] d_loss: 1.75128531, g_loss: 0.36182702\n",
      "Step: [106] d_loss: 1.60297132, g_loss: 0.31169298\n",
      "Step: [107] d_loss: 1.70307016, g_loss: 0.40635705\n",
      "Step: [108] d_loss: 1.68461752, g_loss: 0.50061285\n",
      "Step: [109] d_loss: 1.69068289, g_loss: 0.42134467\n",
      "Step: [110] d_loss: 1.63655663, g_loss: 0.50248426\n",
      "Step: [111] d_loss: 1.55629516, g_loss: 0.51685929\n",
      "Step: [112] d_loss: 1.55884099, g_loss: 0.52441257\n",
      "Step: [113] d_loss: 1.60667384, g_loss: 0.38286373\n",
      "Step: [114] d_loss: 1.64637172, g_loss: 0.54647022\n",
      "Step: [115] d_loss: 1.72073674, g_loss: 0.43600160\n",
      "Step: [116] d_loss: 1.57749045, g_loss: 0.36821994\n",
      "Step: [117] d_loss: 1.71150661, g_loss: 0.49182633\n",
      "Step: [118] d_loss: 1.70167637, g_loss: 0.54129308\n",
      "Step: [119] d_loss: 1.70774961, g_loss: 0.61453766\n",
      "Step: [120] d_loss: 1.54697776, g_loss: 0.61522722\n",
      "Step: [121] d_loss: 1.50359023, g_loss: 0.57916474\n",
      "Step: [122] d_loss: 1.58333933, g_loss: 0.49282783\n",
      "Step: [123] d_loss: 1.65545273, g_loss: 0.62840784\n",
      "Step: [124] d_loss: 1.54402113, g_loss: 0.70017707\n",
      "Step: [125] d_loss: 1.37901521, g_loss: 0.70532364\n",
      "Step: [126] d_loss: 1.32435226, g_loss: 0.47572663\n",
      "Step: [127] d_loss: 1.45489705, g_loss: 0.61096966\n",
      "Step: [128] d_loss: 1.49270093, g_loss: 0.63864410\n",
      "Step: [129] d_loss: 1.50882065, g_loss: 0.59941453\n",
      "Step: [130] d_loss: 1.42003536, g_loss: 0.48700354\n",
      "Step: [131] d_loss: 1.55941653, g_loss: 0.51590556\n",
      "Step: [132] d_loss: 1.45491076, g_loss: 0.58561736\n",
      "Step: [133] d_loss: 1.58861792, g_loss: 0.60395509\n",
      "Step: [134] d_loss: 1.51701736, g_loss: 0.73336601\n",
      "Step: [135] d_loss: 1.14723277, g_loss: 0.87985319\n",
      "Step: [136] d_loss: 1.35753083, g_loss: 0.83366197\n",
      "Step: [137] d_loss: 1.20920932, g_loss: 0.57406545\n",
      "Step: [138] d_loss: 1.33588040, g_loss: 0.74414688\n",
      "Step: [139] d_loss: 1.33470082, g_loss: 0.66262621\n",
      "Step: [140] d_loss: 1.19320726, g_loss: 0.56351465\n",
      "Step: [141] d_loss: 1.18324137, g_loss: 0.56598294\n",
      "Step: [142] d_loss: 1.28667676, g_loss: 0.60589987\n",
      "Step: [143] d_loss: 1.30878377, g_loss: 0.49921280\n",
      "Step: [144] d_loss: 1.27400112, g_loss: 0.43579605\n",
      "Step: [145] d_loss: 1.24510801, g_loss: 0.61399019\n",
      "Step: [146] d_loss: 1.29361558, g_loss: 0.55820656\n",
      "Step: [147] d_loss: 1.34819674, g_loss: 0.53044510\n",
      "Step: [148] d_loss: 1.36528015, g_loss: 0.46881616\n",
      "Step: [149] d_loss: 1.40055406, g_loss: 0.47360027\n",
      "Step: [150] d_loss: 1.31095159, g_loss: 0.58017951\n",
      "Step: [151] d_loss: 1.34325981, g_loss: 0.46970931\n",
      "Step: [152] d_loss: 1.28679585, g_loss: 0.54065287\n",
      "Step: [153] d_loss: 1.33774638, g_loss: 0.54265922\n",
      "Step: [154] d_loss: 1.35705090, g_loss: 0.50085050\n",
      "Step: [155] d_loss: 1.27179897, g_loss: 0.57391638\n",
      "Step: [156] d_loss: 1.27748322, g_loss: 0.54676670\n",
      "Step: [157] d_loss: 1.40440512, g_loss: 0.51599693\n",
      "Step: [158] d_loss: 1.45996547, g_loss: 0.47792727\n",
      "Step: [159] d_loss: 1.38997746, g_loss: 0.53678775\n",
      "Step: [160] d_loss: 1.38074315, g_loss: 0.51273137\n",
      "Step: [161] d_loss: 1.28865075, g_loss: 0.55479741\n",
      "Step: [162] d_loss: 1.19403839, g_loss: 0.58116752\n",
      "Step: [163] d_loss: 1.37519979, g_loss: 0.47280067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [164] d_loss: 1.32165456, g_loss: 0.54878497\n",
      "Step: [165] d_loss: 1.25413704, g_loss: 0.48045337\n",
      "Step: [166] d_loss: 1.34613323, g_loss: 0.51363897\n",
      "Step: [167] d_loss: 1.32709050, g_loss: 0.47334892\n",
      "Step: [168] d_loss: 1.38996387, g_loss: 0.45970866\n",
      "Step: [169] d_loss: 1.29151344, g_loss: 0.54355234\n",
      "Step: [170] d_loss: 1.11024451, g_loss: 0.54805684\n",
      "Step: [171] d_loss: 1.19837451, g_loss: 0.52248418\n",
      "Step: [172] d_loss: 1.30640519, g_loss: 0.51799989\n",
      "Step: [173] d_loss: 1.35418868, g_loss: 0.59663481\n",
      "Step: [174] d_loss: 1.33863974, g_loss: 0.55863345\n",
      "Step: [175] d_loss: 1.33385777, g_loss: 0.48582935\n",
      "Step: [176] d_loss: 1.23627281, g_loss: 0.48883051\n",
      "Step: [177] d_loss: 1.09822226, g_loss: 0.59771180\n",
      "Step: [178] d_loss: 1.26156020, g_loss: 0.61711007\n",
      "Step: [179] d_loss: 1.17876911, g_loss: 0.52101731\n",
      "Step: [180] d_loss: 1.28948784, g_loss: 0.51226658\n",
      "Step: [181] d_loss: 1.20011377, g_loss: 0.63708514\n",
      "Step: [182] d_loss: 1.38231683, g_loss: 0.48766226\n",
      "Step: [183] d_loss: 1.32755280, g_loss: 0.52552682\n",
      "Step: [184] d_loss: 1.32605767, g_loss: 0.56563115\n",
      "Step: [185] d_loss: 1.16519523, g_loss: 0.57944357\n",
      "Step: [186] d_loss: 1.22500312, g_loss: 0.58684683\n",
      "Step: [187] d_loss: 1.15432954, g_loss: 0.60735732\n",
      "Step: [188] d_loss: 1.31677127, g_loss: 0.50405693\n",
      "Step: [189] d_loss: 1.17487526, g_loss: 0.62011623\n",
      "Step: [190] d_loss: 1.24885666, g_loss: 0.62678665\n",
      "Step: [191] d_loss: 1.32653987, g_loss: 0.49651182\n",
      "Step: [192] d_loss: 1.22059727, g_loss: 0.59450853\n",
      "Step: [193] d_loss: 1.42220378, g_loss: 0.49908692\n",
      "Step: [194] d_loss: 1.27969790, g_loss: 0.53550684\n",
      "Step: [195] d_loss: 1.23033893, g_loss: 0.58882308\n",
      "Step: [196] d_loss: 1.33956099, g_loss: 0.49428198\n",
      "Step: [197] d_loss: 1.29290986, g_loss: 0.60121202\n",
      "Step: [198] d_loss: 1.27732038, g_loss: 0.55014360\n",
      "Step: [199] d_loss: 1.30545259, g_loss: 0.50741452\n",
      "Step: [200] d_loss: 1.27530599, g_loss: 0.50694996\n",
      "Step: [201] d_loss: 1.20494437, g_loss: 0.60144937\n",
      "Step: [202] d_loss: 1.13689160, g_loss: 0.59866190\n",
      "Step: [203] d_loss: 1.14374232, g_loss: 0.57308197\n",
      "Step: [204] d_loss: 1.37103415, g_loss: 0.50469840\n",
      "Step: [205] d_loss: 1.19720793, g_loss: 0.66194957\n",
      "Step: [206] d_loss: 1.30643153, g_loss: 0.50387263\n",
      "Step: [207] d_loss: 1.22702551, g_loss: 0.53357756\n",
      "Step: [208] d_loss: 1.17508268, g_loss: 0.58874798\n",
      "Step: [209] d_loss: 1.20224595, g_loss: 0.58849525\n",
      "Step: [210] d_loss: 1.28619576, g_loss: 0.49752787\n",
      "Step: [211] d_loss: 1.22401011, g_loss: 0.52955157\n",
      "Step: [212] d_loss: 1.29542828, g_loss: 0.50764734\n",
      "Step: [213] d_loss: 1.21916151, g_loss: 0.57701504\n",
      "Step: [214] d_loss: 1.28193498, g_loss: 0.52484864\n",
      "Step: [215] d_loss: 1.23190594, g_loss: 0.52841473\n",
      "Step: [216] d_loss: 1.18601513, g_loss: 0.56716782\n",
      "Step: [217] d_loss: 1.29844117, g_loss: 0.49580657\n",
      "Step: [218] d_loss: 1.19889557, g_loss: 0.56706131\n",
      "Step: [219] d_loss: 1.21064353, g_loss: 0.51086676\n",
      "Step: [220] d_loss: 1.22264552, g_loss: 0.57745433\n",
      "Step: [221] d_loss: 1.15691805, g_loss: 0.62482554\n",
      "Step: [222] d_loss: 1.11865556, g_loss: 0.60843265\n",
      "Step: [223] d_loss: 1.04093122, g_loss: 0.59986418\n",
      "Step: [224] d_loss: 1.26466393, g_loss: 0.57821196\n",
      "Step: [225] d_loss: 1.27211642, g_loss: 0.54344302\n",
      "Step: [226] d_loss: 1.15737677, g_loss: 0.54270196\n",
      "Step: [227] d_loss: 1.26129603, g_loss: 0.52381176\n",
      "Step: [228] d_loss: 1.15700293, g_loss: 0.55652159\n",
      "Step: [229] d_loss: 1.11917746, g_loss: 0.60801315\n",
      "Step: [230] d_loss: 1.26409483, g_loss: 0.56912667\n",
      "Step: [231] d_loss: 1.31319880, g_loss: 0.51992184\n",
      "Step: [232] d_loss: 1.29900897, g_loss: 0.54339284\n",
      "Step: [233] d_loss: 1.20314860, g_loss: 0.54220700\n",
      "Step: [234] d_loss: 1.24504495, g_loss: 0.50288552\n",
      "Step: [235] d_loss: 1.22772050, g_loss: 0.45990011\n",
      "Step: [236] d_loss: 1.22335744, g_loss: 0.52257973\n",
      "Step: [237] d_loss: 1.09206843, g_loss: 0.53219354\n",
      "Step: [238] d_loss: 1.28830636, g_loss: 0.50668800\n",
      "Step: [239] d_loss: 0.99702823, g_loss: 0.53330338\n",
      "Step: [240] d_loss: 1.10134006, g_loss: 0.68161255\n",
      "Step: [241] d_loss: 1.28444624, g_loss: 0.54172343\n",
      "Step: [242] d_loss: 1.18738008, g_loss: 0.60499698\n",
      "Step: [243] d_loss: 1.26755047, g_loss: 0.57652938\n",
      "Step: [244] d_loss: 1.21379280, g_loss: 0.55985552\n",
      "Step: [245] d_loss: 1.19425869, g_loss: 0.55464119\n",
      "Step: [246] d_loss: 1.29081368, g_loss: 0.52952677\n",
      "Step: [247] d_loss: 1.34861934, g_loss: 0.46008340\n",
      "Step: [248] d_loss: 1.18520653, g_loss: 0.54515314\n",
      "Step: [249] d_loss: 1.17825770, g_loss: 0.57651120\n",
      "Step: [250] d_loss: 1.10729671, g_loss: 0.59732372\n",
      "Step: [251] d_loss: 1.24983144, g_loss: 0.46237510\n",
      "Step: [252] d_loss: 1.26332998, g_loss: 0.53000122\n",
      "Step: [253] d_loss: 1.27996778, g_loss: 0.40510565\n",
      "Step: [254] d_loss: 0.71929759, g_loss: 0.88096678\n",
      "Step: [255] d_loss: 1.27948880, g_loss: 0.47005415\n",
      "Step: [256] d_loss: 1.10192394, g_loss: 0.45842159\n",
      "Step: [257] d_loss: 1.31939256, g_loss: 0.52049071\n",
      "Step: [258] d_loss: 1.16909635, g_loss: 0.59487832\n",
      "Step: [259] d_loss: 1.35975921, g_loss: 0.44863504\n",
      "Step: [260] d_loss: 1.29109812, g_loss: 0.50497192\n",
      "Step: [261] d_loss: 1.10506225, g_loss: 0.56421733\n",
      "Step: [262] d_loss: 1.07938921, g_loss: 0.68575454\n",
      "Step: [263] d_loss: 1.15318513, g_loss: 0.57790118\n",
      "Step: [264] d_loss: 1.15111446, g_loss: 0.56443739\n",
      "Step: [265] d_loss: 1.09593248, g_loss: 0.62658238\n",
      "Step: [266] d_loss: 1.05614901, g_loss: 0.58482850\n",
      "Step: [267] d_loss: 1.11735475, g_loss: 0.54612225\n",
      "Step: [268] d_loss: 1.14003658, g_loss: 0.57480222\n",
      "Step: [269] d_loss: 0.81320500, g_loss: 0.86582142\n",
      "Step: [270] d_loss: 1.13297117, g_loss: 0.57518476\n",
      "Step: [271] d_loss: 1.10937738, g_loss: 0.52743447\n",
      "Step: [272] d_loss: 1.06630754, g_loss: 0.56832659\n",
      "Step: [273] d_loss: 1.15909600, g_loss: 0.60990453\n",
      "Step: [274] d_loss: 1.06697321, g_loss: 0.63808978\n",
      "Step: [275] d_loss: 1.17524195, g_loss: 0.58817792\n",
      "Step: [276] d_loss: 1.16056824, g_loss: 0.64344400\n",
      "Step: [277] d_loss: 1.20093560, g_loss: 0.52124339\n",
      "Step: [278] d_loss: 1.13961291, g_loss: 0.52318162\n",
      "Step: [279] d_loss: 1.20916057, g_loss: 0.52325368\n",
      "Step: [280] d_loss: 1.17442012, g_loss: 0.55132329\n",
      "Step: [281] d_loss: 1.21956313, g_loss: 0.51843542\n",
      "Step: [282] d_loss: 1.20063674, g_loss: 0.52396774\n",
      "Step: [283] d_loss: 1.27323067, g_loss: 0.47148421\n",
      "Step: [284] d_loss: 1.32949305, g_loss: 0.45592433\n",
      "Step: [285] d_loss: 1.22091460, g_loss: 0.47058707\n",
      "Step: [286] d_loss: 0.84869933, g_loss: 0.83121467\n",
      "Step: [287] d_loss: 1.09981632, g_loss: 0.66023934\n",
      "Step: [288] d_loss: 1.01205254, g_loss: 0.66827440\n",
      "Step: [289] d_loss: 1.07202029, g_loss: 0.66117275\n",
      "Step: [290] d_loss: 0.97308588, g_loss: 0.71722627\n",
      "Step: [291] d_loss: 1.13866484, g_loss: 0.56972253\n",
      "Step: [292] d_loss: 1.20367014, g_loss: 0.53919774\n",
      "Step: [293] d_loss: 1.20082664, g_loss: 0.50573206\n",
      "Step: [294] d_loss: 1.14320612, g_loss: 0.62223029\n",
      "Step: [295] d_loss: 1.16082048, g_loss: 0.48928642\n",
      "Step: [296] d_loss: 1.20236826, g_loss: 0.53134227\n",
      "Step: [297] d_loss: 1.11635351, g_loss: 0.61741143\n",
      "Step: [298] d_loss: 1.19775736, g_loss: 0.53722465\n",
      "Step: [299] d_loss: 1.09864759, g_loss: 0.64892644\n",
      "Step: [300] d_loss: 1.21429014, g_loss: 0.50215852\n",
      "Step: [301] d_loss: 1.24038970, g_loss: 0.49875173\n",
      "Step: [302] d_loss: 0.76297826, g_loss: 0.52996629\n",
      "Step: [303] d_loss: 1.18903565, g_loss: 0.53103626\n",
      "Step: [304] d_loss: 1.16865110, g_loss: 0.45510039\n",
      "Step: [305] d_loss: 1.04097891, g_loss: 0.64520061\n",
      "Step: [306] d_loss: 0.86376590, g_loss: 0.56355250\n",
      "Step: [307] d_loss: 1.11979413, g_loss: 0.61999696\n",
      "Step: [308] d_loss: 1.21818829, g_loss: 0.50600278\n",
      "Step: [309] d_loss: 1.12860990, g_loss: 0.53539443\n",
      "Step: [310] d_loss: 1.16050208, g_loss: 0.53880745\n",
      "Step: [311] d_loss: 1.24964523, g_loss: 0.46922454\n",
      "Step: [312] d_loss: 0.97416925, g_loss: 0.48668861\n",
      "Step: [313] d_loss: 0.80233836, g_loss: 0.62477881\n",
      "Step: [314] d_loss: 1.10150898, g_loss: 0.58073068\n",
      "Step: [315] d_loss: 1.17999196, g_loss: 0.53788060\n",
      "Step: [316] d_loss: 1.13954926, g_loss: 0.54140067\n",
      "Step: [317] d_loss: 1.01203847, g_loss: 0.54030031\n",
      "Step: [318] d_loss: 0.99757904, g_loss: 0.59700775\n",
      "Step: [319] d_loss: 1.01169968, g_loss: 0.64018679\n",
      "Step: [320] d_loss: 1.12555349, g_loss: 0.54745042\n",
      "Step: [321] d_loss: 1.14064312, g_loss: 0.52410966\n",
      "Step: [322] d_loss: 0.85014224, g_loss: 0.56515676\n",
      "Step: [323] d_loss: 1.14509833, g_loss: 0.56519538\n",
      "Step: [324] d_loss: 0.76131946, g_loss: 0.65574020\n",
      "Step: [325] d_loss: 1.02165985, g_loss: 0.59691471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [326] d_loss: 1.06667662, g_loss: 0.56945294\n",
      "Step: [327] d_loss: 1.10414577, g_loss: 0.60959315\n",
      "Step: [328] d_loss: 0.85938483, g_loss: 0.57110018\n",
      "Step: [329] d_loss: 0.92329788, g_loss: 0.57803214\n",
      "Step: [330] d_loss: 1.02195740, g_loss: 0.63913214\n",
      "Step: [331] d_loss: 1.11049843, g_loss: 0.57073140\n",
      "Step: [332] d_loss: 0.98846269, g_loss: 0.62286174\n",
      "Step: [333] d_loss: 0.67113847, g_loss: 0.60892993\n",
      "Step: [334] d_loss: 1.18679309, g_loss: 0.53126270\n",
      "Step: [335] d_loss: 1.05282307, g_loss: 0.59312737\n",
      "Step: [336] d_loss: 1.10918164, g_loss: 0.55066556\n",
      "Step: [337] d_loss: 1.22292197, g_loss: 0.44300890\n",
      "Step: [338] d_loss: 1.18808103, g_loss: 0.50395459\n",
      "Step: [339] d_loss: 1.19304466, g_loss: 0.54529923\n",
      "Step: [340] d_loss: 1.13306189, g_loss: 0.48967084\n",
      "Step: [341] d_loss: 1.06111836, g_loss: 0.63746172\n",
      "Step: [342] d_loss: 0.74199414, g_loss: 0.83928913\n",
      "Step: [343] d_loss: 0.76327306, g_loss: 0.82171708\n",
      "Step: [344] d_loss: 1.03622353, g_loss: 0.66235054\n",
      "Step: [345] d_loss: 0.99033320, g_loss: 0.67501497\n",
      "Step: [346] d_loss: 1.10257602, g_loss: 0.58290136\n",
      "Step: [347] d_loss: 0.84802592, g_loss: 0.62781638\n",
      "Step: [348] d_loss: 0.92432559, g_loss: 0.64939606\n",
      "Step: [349] d_loss: 0.88949454, g_loss: 0.77791250\n",
      "Step: [350] d_loss: 1.02908289, g_loss: 0.64631641\n",
      "Step: [351] d_loss: 1.02414322, g_loss: 0.66684788\n",
      "Step: [352] d_loss: 0.95911556, g_loss: 0.73025334\n",
      "Step: [353] d_loss: 1.04644275, g_loss: 0.61495239\n",
      "Step: [354] d_loss: 1.12652111, g_loss: 0.60190427\n",
      "Step: [355] d_loss: 1.07358885, g_loss: 0.60478020\n",
      "Step: [356] d_loss: 1.08994269, g_loss: 0.58060527\n",
      "Step: [357] d_loss: 1.10936666, g_loss: 0.54012215\n",
      "Step: [358] d_loss: 0.90517038, g_loss: 0.67350954\n",
      "Step: [359] d_loss: 0.85192585, g_loss: 0.67742944\n",
      "Step: [360] d_loss: 0.99135983, g_loss: 0.69826043\n",
      "Step: [361] d_loss: 0.98568380, g_loss: 0.62321877\n",
      "Step: [362] d_loss: 0.98508686, g_loss: 0.66891468\n",
      "Step: [363] d_loss: 1.04072618, g_loss: 0.63116527\n",
      "Step: [364] d_loss: 1.10541272, g_loss: 0.57702398\n",
      "Step: [365] d_loss: 1.06151736, g_loss: 0.58237493\n",
      "Step: [366] d_loss: 1.12799692, g_loss: 0.58417475\n",
      "Step: [367] d_loss: 0.97899491, g_loss: 0.66806614\n",
      "Step: [368] d_loss: 1.12391806, g_loss: 0.45199859\n",
      "Step: [369] d_loss: 1.13690174, g_loss: 0.55524027\n",
      "Step: [370] d_loss: 0.94246459, g_loss: 0.51599956\n",
      "Step: [371] d_loss: 0.92763215, g_loss: 0.57255483\n",
      "Step: [372] d_loss: 0.96855915, g_loss: 0.68330115\n",
      "Step: [373] d_loss: 0.96123379, g_loss: 0.62764788\n",
      "Step: [374] d_loss: 0.99980181, g_loss: 0.62894601\n",
      "Step: [375] d_loss: 1.00944281, g_loss: 0.64432693\n",
      "Step: [376] d_loss: 1.01236200, g_loss: 0.67825758\n",
      "Step: [377] d_loss: 0.91544664, g_loss: 0.70877671\n",
      "Step: [378] d_loss: 1.07907414, g_loss: 0.55552649\n",
      "Step: [379] d_loss: 1.06373715, g_loss: 0.66176021\n",
      "Step: [380] d_loss: 1.20935106, g_loss: 0.51774204\n",
      "Step: [381] d_loss: 1.14792275, g_loss: 0.48273942\n",
      "Step: [382] d_loss: 1.16040063, g_loss: 0.50246143\n",
      "Step: [383] d_loss: 1.09498036, g_loss: 0.47407678\n",
      "Step: [384] d_loss: 0.59325409, g_loss: 0.71211338\n",
      "Step: [385] d_loss: 0.63207924, g_loss: 0.75895375\n",
      "Step: [386] d_loss: 0.59505910, g_loss: 0.71067870\n",
      "Step: [387] d_loss: 0.89562076, g_loss: 0.69004154\n",
      "Step: [388] d_loss: 0.99423814, g_loss: 0.63036489\n",
      "Step: [389] d_loss: 1.07521653, g_loss: 0.57271618\n",
      "Step: [390] d_loss: 0.97045207, g_loss: 0.68535972\n",
      "Step: [391] d_loss: 1.14394176, g_loss: 0.49999329\n",
      "Step: [392] d_loss: 1.07816017, g_loss: 0.48763758\n",
      "Step: [393] d_loss: 0.87273324, g_loss: 0.66967332\n",
      "Step: [394] d_loss: 0.73987567, g_loss: 0.85409701\n",
      "Step: [395] d_loss: 0.58541983, g_loss: 0.77654546\n",
      "Step: [396] d_loss: 0.75370508, g_loss: 0.81743717\n",
      "Step: [397] d_loss: 0.93406582, g_loss: 0.69223982\n",
      "Step: [398] d_loss: 0.93078965, g_loss: 0.71674728\n",
      "Step: [399] d_loss: 1.00685620, g_loss: 0.51938146\n",
      "Step: [400] d_loss: 0.99778426, g_loss: 0.65238571\n",
      "Step: [401] d_loss: 0.92387444, g_loss: 0.70570385\n",
      "Step: [402] d_loss: 1.06787550, g_loss: 0.55193573\n",
      "Step: [403] d_loss: 1.04465723, g_loss: 0.59303540\n",
      "Step: [404] d_loss: 1.02936602, g_loss: 0.54649150\n",
      "Step: [405] d_loss: 1.05421400, g_loss: 0.56273496\n",
      "Step: [406] d_loss: 0.99725693, g_loss: 0.68159050\n",
      "Step: [407] d_loss: 1.01475286, g_loss: 0.52837169\n",
      "Step: [408] d_loss: 0.92463571, g_loss: 0.58850503\n",
      "Step: [409] d_loss: 1.12185907, g_loss: 0.62837452\n",
      "Step: [410] d_loss: 0.97872990, g_loss: 0.64024490\n",
      "Step: [411] d_loss: 0.58971244, g_loss: 0.64698440\n",
      "Step: [412] d_loss: 1.03072143, g_loss: 0.56602234\n",
      "Step: [413] d_loss: 0.97575802, g_loss: 0.63411087\n",
      "Step: [414] d_loss: 1.08362293, g_loss: 0.52968615\n",
      "Step: [415] d_loss: 1.14118171, g_loss: 0.49604315\n",
      "Step: [416] d_loss: 1.01839638, g_loss: 0.59235394\n",
      "Step: [417] d_loss: 0.69769806, g_loss: 0.55171132\n",
      "Step: [418] d_loss: 1.15547800, g_loss: 0.49937379\n",
      "Step: [419] d_loss: 0.91990829, g_loss: 0.63205290\n",
      "Step: [420] d_loss: 0.97207636, g_loss: 0.69330108\n",
      "Step: [421] d_loss: 0.93220645, g_loss: 0.60363960\n",
      "Step: [422] d_loss: 1.03287172, g_loss: 0.54108822\n",
      "Step: [423] d_loss: 0.92713153, g_loss: 0.60426968\n",
      "Step: [424] d_loss: 0.61453098, g_loss: 1.03520298\n",
      "Step: [425] d_loss: 0.62361449, g_loss: 0.61561406\n",
      "Step: [426] d_loss: 0.90026546, g_loss: 0.70785850\n",
      "Step: [427] d_loss: 0.61368692, g_loss: 0.73733950\n",
      "Step: [428] d_loss: 0.88729751, g_loss: 0.72180361\n",
      "Step: [429] d_loss: 0.66632438, g_loss: 0.63606972\n",
      "Step: [430] d_loss: 0.89882255, g_loss: 0.75202137\n",
      "Step: [431] d_loss: 0.82905865, g_loss: 0.65926743\n",
      "Step: [432] d_loss: 0.80048472, g_loss: 0.83315766\n",
      "Step: [433] d_loss: 0.75878912, g_loss: 0.82131302\n",
      "Step: [434] d_loss: 0.81079680, g_loss: 0.78783107\n",
      "Step: [435] d_loss: 0.84033924, g_loss: 0.74614948\n",
      "Step: [436] d_loss: 0.76848453, g_loss: 0.78723693\n",
      "Step: [437] d_loss: 0.90488350, g_loss: 0.72557610\n",
      "Step: [438] d_loss: 0.89154053, g_loss: 0.75280523\n",
      "Step: [439] d_loss: 0.86343598, g_loss: 0.76144254\n",
      "Step: [440] d_loss: 0.93842638, g_loss: 0.68065554\n",
      "Step: [441] d_loss: 0.93678540, g_loss: 0.70755279\n",
      "Step: [442] d_loss: 0.94136673, g_loss: 0.68130982\n",
      "Step: [443] d_loss: 0.94733644, g_loss: 0.62635541\n",
      "Step: [444] d_loss: 0.93528259, g_loss: 0.64424849\n",
      "Step: [445] d_loss: 0.98566711, g_loss: 0.61035132\n",
      "Step: [446] d_loss: 0.90638167, g_loss: 0.65531105\n",
      "Step: [447] d_loss: 1.02052343, g_loss: 0.57030237\n",
      "Step: [448] d_loss: 1.04089105, g_loss: 0.55340600\n",
      "Step: [449] d_loss: 1.01307273, g_loss: 0.56954604\n",
      "Step: [450] d_loss: 0.94138354, g_loss: 0.60713571\n",
      "Step: [451] d_loss: 1.11836863, g_loss: 0.52989638\n",
      "Step: [452] d_loss: 1.02399671, g_loss: 0.59181535\n",
      "Step: [453] d_loss: 1.03703070, g_loss: 0.54599541\n",
      "Step: [454] d_loss: 0.67681879, g_loss: 0.71140790\n",
      "Step: [455] d_loss: 0.55472898, g_loss: 0.73421353\n",
      "Step: [456] d_loss: 0.84693152, g_loss: 0.74066842\n",
      "Step: [457] d_loss: 0.72202271, g_loss: 0.71909171\n",
      "Step: [458] d_loss: 0.61458516, g_loss: 0.60456550\n",
      "Step: [459] d_loss: 0.90886724, g_loss: 0.69077951\n",
      "Step: [460] d_loss: 1.03953958, g_loss: 0.61144918\n",
      "Step: [461] d_loss: 1.00506067, g_loss: 0.60859680\n",
      "Step: [462] d_loss: 0.85272449, g_loss: 0.69319719\n",
      "Step: [463] d_loss: 0.79489970, g_loss: 0.74301893\n",
      "Step: [464] d_loss: 0.94976926, g_loss: 0.69753444\n",
      "Step: [465] d_loss: 0.90429497, g_loss: 0.61718678\n",
      "Step: [466] d_loss: 0.55562019, g_loss: 0.66028833\n",
      "Step: [467] d_loss: 0.53621113, g_loss: 0.78788447\n",
      "Step: [468] d_loss: 0.80673182, g_loss: 0.71458489\n",
      "Step: [469] d_loss: 0.91266513, g_loss: 0.70870763\n",
      "Step: [470] d_loss: 0.81203550, g_loss: 0.74097645\n",
      "Step: [471] d_loss: 1.00556111, g_loss: 0.65352428\n",
      "Step: [472] d_loss: 0.87136525, g_loss: 0.70856053\n",
      "Step: [473] d_loss: 0.90484262, g_loss: 0.67320222\n",
      "Step: [474] d_loss: 0.98699272, g_loss: 0.59293115\n",
      "Step: [475] d_loss: 0.53732270, g_loss: 0.60761154\n",
      "Step: [476] d_loss: 0.84467101, g_loss: 0.73884308\n",
      "Step: [477] d_loss: 0.75081509, g_loss: 0.68051875\n",
      "Step: [478] d_loss: 0.77527952, g_loss: 0.72265548\n",
      "Step: [479] d_loss: 0.93878192, g_loss: 0.62280160\n",
      "Step: [480] d_loss: 0.89679629, g_loss: 0.66934586\n",
      "Step: [481] d_loss: 0.82564479, g_loss: 0.64936769\n",
      "Step: [482] d_loss: 0.69230455, g_loss: 0.66064405\n",
      "Step: [483] d_loss: 0.82373649, g_loss: 0.68599010\n",
      "Step: [484] d_loss: 0.76228851, g_loss: 0.66164511\n",
      "Step: [485] d_loss: 0.71190119, g_loss: 0.73031437\n",
      "Step: [486] d_loss: 0.80974156, g_loss: 0.67925233\n",
      "Step: [487] d_loss: 0.87750590, g_loss: 0.68421358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [488] d_loss: 0.92878717, g_loss: 0.61327732\n",
      "Step: [489] d_loss: 0.94215906, g_loss: 0.64717281\n",
      "Step: [490] d_loss: 0.98465264, g_loss: 0.54440641\n",
      "Step: [491] d_loss: 0.66394049, g_loss: 0.87985122\n",
      "Step: [492] d_loss: 0.54623944, g_loss: 0.78626114\n",
      "Step: [493] d_loss: 0.75887632, g_loss: 0.79782242\n",
      "Step: [494] d_loss: 0.75299889, g_loss: 0.69951588\n",
      "Step: [495] d_loss: 0.83094758, g_loss: 0.75469899\n",
      "Step: [496] d_loss: 0.88223451, g_loss: 0.70271993\n",
      "Step: [497] d_loss: 0.92009383, g_loss: 0.58017606\n",
      "Step: [498] d_loss: 0.76646417, g_loss: 0.73145759\n",
      "Step: [499] d_loss: 0.88301301, g_loss: 0.63161778\n",
      "Step: [500] d_loss: 0.52840722, g_loss: 0.58522695\n",
      "Step: [501] d_loss: 0.92561769, g_loss: 0.60091770\n",
      "Step: [502] d_loss: 0.65387774, g_loss: 0.64896905\n",
      "Step: [503] d_loss: 0.89896357, g_loss: 0.66665035\n",
      "Step: [504] d_loss: 0.89427221, g_loss: 0.64321893\n",
      "Step: [505] d_loss: 0.73007458, g_loss: 0.73001945\n",
      "Step: [506] d_loss: 0.79878169, g_loss: 0.69900584\n",
      "Step: [507] d_loss: 0.85939068, g_loss: 0.67252111\n",
      "Step: [508] d_loss: 0.88812798, g_loss: 0.67642438\n",
      "Step: [509] d_loss: 0.59478813, g_loss: 0.76132739\n",
      "Step: [510] d_loss: 0.62559146, g_loss: 0.73253453\n",
      "Step: [511] d_loss: 0.78232586, g_loss: 0.74367934\n",
      "Step: [512] d_loss: 0.77076656, g_loss: 0.74216473\n",
      "Step: [513] d_loss: 0.86419803, g_loss: 0.63066995\n",
      "Step: [514] d_loss: 0.76571727, g_loss: 0.72987276\n",
      "Step: [515] d_loss: 0.73711824, g_loss: 0.68498665\n",
      "Step: [516] d_loss: 0.82078332, g_loss: 0.77206731\n",
      "Step: [517] d_loss: 0.84303522, g_loss: 0.66409278\n",
      "Step: [518] d_loss: 0.81372178, g_loss: 0.67386520\n",
      "Step: [519] d_loss: 0.90472913, g_loss: 0.64610618\n",
      "Step: [520] d_loss: 0.70257008, g_loss: 0.76481503\n",
      "Step: [521] d_loss: 0.85173237, g_loss: 0.64258689\n",
      "Step: [522] d_loss: 0.82766855, g_loss: 0.62743348\n",
      "Step: [523] d_loss: 0.93888307, g_loss: 0.67391008\n",
      "Step: [524] d_loss: 0.76614690, g_loss: 0.74023622\n",
      "Step: [525] d_loss: 0.78460598, g_loss: 0.66503912\n",
      "Step: [526] d_loss: 0.77089906, g_loss: 0.80944395\n",
      "Step: [527] d_loss: 0.71622485, g_loss: 0.85865748\n",
      "Step: [528] d_loss: 0.73110098, g_loss: 0.77688861\n",
      "Step: [529] d_loss: 0.77419138, g_loss: 0.74207079\n",
      "Step: [530] d_loss: 0.85599232, g_loss: 0.66150135\n",
      "Step: [531] d_loss: 0.85604835, g_loss: 0.65323669\n",
      "Step: [532] d_loss: 0.94676679, g_loss: 0.57377779\n",
      "Step: [533] d_loss: 0.81872910, g_loss: 0.65380472\n",
      "Step: [534] d_loss: 0.77848095, g_loss: 0.70382643\n",
      "Step: [535] d_loss: 0.87949789, g_loss: 0.65987110\n",
      "Step: [536] d_loss: 0.92375892, g_loss: 0.65416127\n",
      "Step: [537] d_loss: 0.85504413, g_loss: 0.63825297\n",
      "Step: [538] d_loss: 0.90425599, g_loss: 0.65991819\n",
      "Step: [539] d_loss: 0.81940520, g_loss: 0.70067674\n",
      "Step: [540] d_loss: 0.79257244, g_loss: 0.62129509\n",
      "Step: [541] d_loss: 0.89045668, g_loss: 0.66337627\n",
      "Step: [542] d_loss: 0.82923502, g_loss: 0.69325852\n",
      "Step: [543] d_loss: 0.52407479, g_loss: 0.75358498\n",
      "Step: [544] d_loss: 0.53518003, g_loss: 0.78113288\n",
      "Step: [545] d_loss: 0.72541589, g_loss: 0.72140175\n",
      "Step: [546] d_loss: 0.51737666, g_loss: 0.79674506\n",
      "Step: [547] d_loss: 0.51417112, g_loss: 0.79625750\n",
      "Step: [548] d_loss: 0.61927062, g_loss: 0.93572700\n",
      "Step: [549] d_loss: 0.58239508, g_loss: 0.89697123\n",
      "Step: [550] d_loss: 0.70945370, g_loss: 0.76644748\n",
      "Step: [551] d_loss: 0.77004564, g_loss: 0.80567223\n",
      "Step: [552] d_loss: 0.73256814, g_loss: 0.74392003\n",
      "Step: [553] d_loss: 0.73056167, g_loss: 0.74767631\n",
      "Step: [554] d_loss: 0.79695755, g_loss: 0.70283461\n",
      "Step: [555] d_loss: 0.76281768, g_loss: 0.83333308\n",
      "Step: [556] d_loss: 0.74811852, g_loss: 0.70277727\n",
      "Step: [557] d_loss: 0.90536815, g_loss: 0.67861682\n",
      "Step: [558] d_loss: 0.77820432, g_loss: 0.69569808\n",
      "Step: [559] d_loss: 0.63533509, g_loss: 0.73573995\n",
      "Step: [560] d_loss: 0.79401851, g_loss: 0.76883072\n",
      "Step: [561] d_loss: 0.62362581, g_loss: 0.74063098\n",
      "Step: [562] d_loss: 0.77706343, g_loss: 0.63211560\n",
      "Step: [563] d_loss: 0.75334513, g_loss: 0.78550547\n",
      "Step: [564] d_loss: 0.49917528, g_loss: 0.70513499\n",
      "Step: [565] d_loss: 0.71496046, g_loss: 0.75255024\n",
      "Step: [566] d_loss: 0.75918674, g_loss: 0.73768318\n",
      "Step: [567] d_loss: 0.88168490, g_loss: 0.67617118\n",
      "Step: [568] d_loss: 0.71238184, g_loss: 0.80419320\n",
      "Step: [569] d_loss: 0.78121531, g_loss: 0.72983700\n",
      "Step: [570] d_loss: 0.82133889, g_loss: 0.72254765\n",
      "Step: [571] d_loss: 0.94863057, g_loss: 0.59651005\n",
      "Step: [572] d_loss: 0.45282203, g_loss: 0.74198461\n",
      "Step: [573] d_loss: 0.69404745, g_loss: 0.76779968\n",
      "Step: [574] d_loss: 0.64908230, g_loss: 0.63997221\n",
      "Step: [575] d_loss: 0.81790221, g_loss: 0.62864369\n",
      "Step: [576] d_loss: 0.78893530, g_loss: 0.72973931\n",
      "Step: [577] d_loss: 0.76085341, g_loss: 0.78069913\n",
      "Step: [578] d_loss: 0.82161927, g_loss: 0.70799470\n",
      "Step: [579] d_loss: 0.73610651, g_loss: 0.71125156\n",
      "Step: [580] d_loss: 0.87333059, g_loss: 0.61388326\n",
      "Step: [581] d_loss: 0.88034385, g_loss: 0.62445378\n",
      "Step: [582] d_loss: 0.84158099, g_loss: 0.69645023\n",
      "Step: [583] d_loss: 0.84942651, g_loss: 0.61327410\n",
      "Step: [584] d_loss: 0.81448317, g_loss: 0.72385633\n",
      "Step: [585] d_loss: 0.80614758, g_loss: 0.74394870\n",
      "Step: [586] d_loss: 0.62339401, g_loss: 0.79515392\n",
      "Step: [587] d_loss: 0.68740487, g_loss: 0.80215120\n",
      "Step: [588] d_loss: 0.80782419, g_loss: 0.66838491\n",
      "Step: [589] d_loss: 0.65510118, g_loss: 0.69466823\n",
      "Step: [590] d_loss: 0.71496975, g_loss: 0.76024711\n",
      "Step: [591] d_loss: 0.81486213, g_loss: 0.73786992\n",
      "Step: [592] d_loss: 0.78698421, g_loss: 0.64829880\n",
      "Step: [593] d_loss: 0.76349354, g_loss: 0.73124230\n",
      "Step: [594] d_loss: 0.82626820, g_loss: 0.68608624\n",
      "Step: [595] d_loss: 0.85068911, g_loss: 0.63505435\n",
      "Step: [596] d_loss: 0.92590696, g_loss: 0.63397157\n",
      "Step: [597] d_loss: 0.79365206, g_loss: 0.64860594\n",
      "Step: [598] d_loss: 0.82777798, g_loss: 0.63332075\n",
      "Step: [599] d_loss: 0.79872501, g_loss: 0.69052941\n",
      "Step: [600] d_loss: 0.84089464, g_loss: 0.78137791\n",
      "Step: [601] d_loss: 0.47760943, g_loss: 0.75848961\n",
      "Step: [602] d_loss: 0.68248284, g_loss: 0.84583718\n",
      "Step: [603] d_loss: 0.71399426, g_loss: 0.75246441\n",
      "Step: [604] d_loss: 0.66004276, g_loss: 0.77178901\n",
      "Step: [605] d_loss: 0.79114711, g_loss: 0.73839492\n",
      "Step: [606] d_loss: 0.74219227, g_loss: 0.68628275\n",
      "Step: [607] d_loss: 0.67253351, g_loss: 0.77510142\n",
      "Step: [608] d_loss: 0.67139560, g_loss: 0.86298501\n",
      "Step: [609] d_loss: 0.45664471, g_loss: 0.85119039\n",
      "Step: [610] d_loss: 0.53452027, g_loss: 0.88327855\n",
      "Step: [611] d_loss: 0.63370323, g_loss: 0.81487000\n",
      "Step: [612] d_loss: 0.65889722, g_loss: 0.81961185\n",
      "Step: [613] d_loss: 0.57810283, g_loss: 0.89505768\n",
      "Step: [614] d_loss: 0.79451513, g_loss: 0.67821473\n",
      "Step: [615] d_loss: 0.79495525, g_loss: 0.68087417\n",
      "Step: [616] d_loss: 0.75389397, g_loss: 0.73993349\n",
      "Step: [617] d_loss: 0.87258244, g_loss: 0.66061592\n",
      "Step: [618] d_loss: 0.88734484, g_loss: 0.66075909\n",
      "Step: [619] d_loss: 0.75457335, g_loss: 0.71176904\n",
      "Step: [620] d_loss: 0.84775102, g_loss: 0.66878629\n",
      "Step: [621] d_loss: 0.76619941, g_loss: 0.78352696\n",
      "Step: [622] d_loss: 0.47064489, g_loss: 0.92097747\n",
      "Step: [623] d_loss: 0.52739680, g_loss: 0.87253827\n",
      "Step: [624] d_loss: 0.65096211, g_loss: 0.81826824\n",
      "Step: [625] d_loss: 0.70574045, g_loss: 0.81637692\n",
      "Step: [626] d_loss: 0.60928321, g_loss: 0.88320404\n",
      "Step: [627] d_loss: 0.63113832, g_loss: 0.85963440\n",
      "Step: [628] d_loss: 0.67253339, g_loss: 0.75250375\n",
      "Step: [629] d_loss: 0.63629222, g_loss: 0.84478903\n",
      "Step: [630] d_loss: 0.62877077, g_loss: 0.85219258\n",
      "Step: [631] d_loss: 0.67004526, g_loss: 0.86493921\n",
      "Step: [632] d_loss: 0.58090901, g_loss: 0.85500371\n",
      "Step: [633] d_loss: 0.73258388, g_loss: 0.81270963\n",
      "Step: [634] d_loss: 0.44691709, g_loss: 0.90445811\n",
      "Step: [635] d_loss: 0.47480717, g_loss: 0.84093678\n",
      "Step: [636] d_loss: 0.72905445, g_loss: 0.79996127\n",
      "Step: [637] d_loss: 0.72045535, g_loss: 0.73554909\n",
      "Step: [638] d_loss: 0.65885544, g_loss: 0.79012585\n",
      "Step: [639] d_loss: 0.64077806, g_loss: 0.74409288\n",
      "Step: [640] d_loss: 0.62386394, g_loss: 0.88397044\n",
      "Step: [641] d_loss: 0.66127187, g_loss: 0.71498054\n",
      "Step: [642] d_loss: 0.63227952, g_loss: 0.78651357\n",
      "Step: [643] d_loss: 0.64597750, g_loss: 0.79442042\n",
      "Step: [644] d_loss: 0.63368368, g_loss: 0.83355486\n",
      "Step: [645] d_loss: 0.68970609, g_loss: 0.75545478\n",
      "Step: [646] d_loss: 0.71763104, g_loss: 0.72075009\n",
      "Step: [647] d_loss: 0.69847846, g_loss: 0.80077207\n",
      "Step: [648] d_loss: 0.48309085, g_loss: 0.89680332\n",
      "Step: [649] d_loss: 0.57178950, g_loss: 0.90616035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [650] d_loss: 0.67782640, g_loss: 0.75032741\n",
      "Step: [651] d_loss: 0.69665504, g_loss: 0.78109568\n",
      "Step: [652] d_loss: 0.48471510, g_loss: 0.94889641\n",
      "Step: [653] d_loss: 0.54092014, g_loss: 0.95462883\n",
      "Step: [654] d_loss: 0.41437969, g_loss: 0.94452631\n",
      "Step: [655] d_loss: 0.42846659, g_loss: 0.92819536\n",
      "Step: [656] d_loss: 0.58256078, g_loss: 0.85913754\n",
      "Step: [657] d_loss: 0.48061880, g_loss: 0.98623461\n",
      "Step: [658] d_loss: 0.62535429, g_loss: 0.85466260\n",
      "Step: [659] d_loss: 0.62304068, g_loss: 0.87828386\n",
      "Step: [660] d_loss: 0.47891578, g_loss: 0.85174108\n",
      "Step: [661] d_loss: 0.49367005, g_loss: 0.88079011\n",
      "Step: [662] d_loss: 0.57273805, g_loss: 0.90715814\n",
      "Step: [663] d_loss: 0.45724261, g_loss: 0.93516135\n",
      "Step: [664] d_loss: 0.51604736, g_loss: 0.83874398\n",
      "Step: [665] d_loss: 0.61930645, g_loss: 0.85830724\n",
      "Step: [666] d_loss: 0.63294113, g_loss: 0.85836500\n",
      "Step: [667] d_loss: 0.58371520, g_loss: 0.81017780\n",
      "Step: [668] d_loss: 0.70406497, g_loss: 0.85445255\n",
      "Step: [669] d_loss: 0.62872088, g_loss: 0.80977416\n",
      "Step: [670] d_loss: 0.51725578, g_loss: 0.88552600\n",
      "Step: [671] d_loss: 0.46683654, g_loss: 1.03975725\n",
      "Step: [672] d_loss: 0.63466287, g_loss: 0.84635639\n",
      "Step: [673] d_loss: 0.44930595, g_loss: 0.96681130\n",
      "Step: [674] d_loss: 0.41861218, g_loss: 0.79367381\n",
      "Step: [675] d_loss: 0.64716423, g_loss: 0.79570085\n",
      "Step: [676] d_loss: 0.58491844, g_loss: 0.94320577\n",
      "Step: [677] d_loss: 0.62455237, g_loss: 0.83763021\n",
      "Step: [678] d_loss: 0.66073745, g_loss: 0.78714252\n",
      "Step: [679] d_loss: 0.51367688, g_loss: 0.89488989\n",
      "Step: [680] d_loss: 0.54511154, g_loss: 0.96177590\n",
      "Step: [681] d_loss: 0.42329380, g_loss: 0.92796230\n",
      "Step: [682] d_loss: 0.62800002, g_loss: 0.79949683\n",
      "Step: [683] d_loss: 0.71861386, g_loss: 0.80326545\n",
      "Step: [684] d_loss: 0.67912555, g_loss: 0.79542303\n",
      "Step: [685] d_loss: 0.64484882, g_loss: 0.79592580\n",
      "Step: [686] d_loss: 0.45100039, g_loss: 0.97993571\n",
      "Step: [687] d_loss: 0.46387526, g_loss: 0.94623697\n",
      "Step: [688] d_loss: 0.41752103, g_loss: 0.96120256\n",
      "Step: [689] d_loss: 0.43429539, g_loss: 0.90050995\n",
      "Step: [690] d_loss: 0.50590295, g_loss: 0.97094941\n",
      "Step: [691] d_loss: 0.53090805, g_loss: 0.84005421\n",
      "Step: [692] d_loss: 0.55609781, g_loss: 0.92724264\n",
      "Step: [693] d_loss: 0.57641339, g_loss: 0.85144931\n",
      "Step: [694] d_loss: 0.56783032, g_loss: 0.91575199\n",
      "Step: [695] d_loss: 0.60730684, g_loss: 0.90660119\n",
      "Step: [696] d_loss: 0.57208121, g_loss: 0.90554762\n",
      "Step: [697] d_loss: 0.44879717, g_loss: 0.89771336\n",
      "Step: [698] d_loss: 0.44340271, g_loss: 0.98881596\n",
      "Step: [699] d_loss: 0.44034767, g_loss: 0.92778140\n",
      "Step: [700] d_loss: 0.46887851, g_loss: 0.95791405\n",
      "Step: [701] d_loss: 0.49960676, g_loss: 0.97445822\n",
      "Step: [702] d_loss: 0.42860913, g_loss: 1.01696730\n",
      "Step: [703] d_loss: 0.52090490, g_loss: 0.87717247\n",
      "Step: [704] d_loss: 0.63382983, g_loss: 0.86731750\n",
      "Step: [705] d_loss: 0.54703164, g_loss: 0.90525371\n",
      "Step: [706] d_loss: 0.60094649, g_loss: 0.88188732\n",
      "Step: [707] d_loss: 0.65754533, g_loss: 0.82862788\n",
      "Step: [708] d_loss: 0.58950245, g_loss: 0.89975083\n",
      "Step: [709] d_loss: 0.50974953, g_loss: 1.01208997\n",
      "Step: [710] d_loss: 0.54235566, g_loss: 0.95501852\n",
      "Step: [711] d_loss: 0.42669564, g_loss: 0.91606015\n",
      "Step: [712] d_loss: 0.47637460, g_loss: 0.92158294\n",
      "Step: [713] d_loss: 0.52964818, g_loss: 0.97398043\n",
      "Step: [714] d_loss: 0.45432609, g_loss: 1.01583505\n",
      "Step: [715] d_loss: 0.46696281, g_loss: 0.99810767\n",
      "Step: [716] d_loss: 0.54986447, g_loss: 0.85899580\n",
      "Step: [717] d_loss: 0.45433173, g_loss: 0.98106349\n",
      "Step: [718] d_loss: 0.46596357, g_loss: 1.06768060\n",
      "Step: [719] d_loss: 0.53530306, g_loss: 0.92096299\n",
      "Step: [720] d_loss: 0.43546537, g_loss: 0.93520832\n",
      "Step: [721] d_loss: 0.42542499, g_loss: 1.05180466\n",
      "Step: [722] d_loss: 0.43406683, g_loss: 1.06066740\n",
      "Step: [723] d_loss: 0.42325237, g_loss: 1.06519270\n",
      "Step: [724] d_loss: 0.44773677, g_loss: 1.06474817\n",
      "Step: [725] d_loss: 0.41586977, g_loss: 1.06418347\n",
      "Step: [726] d_loss: 0.44211960, g_loss: 1.01378834\n",
      "Step: [727] d_loss: 0.41041437, g_loss: 0.87496221\n",
      "Step: [728] d_loss: 0.50028825, g_loss: 1.00365293\n",
      "Step: [729] d_loss: 0.42207438, g_loss: 1.01119936\n",
      "Step: [730] d_loss: 0.45922443, g_loss: 1.04726636\n",
      "Step: [731] d_loss: 0.45577604, g_loss: 1.02896798\n",
      "Step: [732] d_loss: 0.49649975, g_loss: 0.93827432\n",
      "Step: [733] d_loss: 0.44869882, g_loss: 0.94472766\n",
      "Step: [734] d_loss: 0.44348973, g_loss: 1.01179302\n",
      "Step: [735] d_loss: 0.45395902, g_loss: 0.95917296\n",
      "Step: [736] d_loss: 0.47275949, g_loss: 1.03265262\n",
      "Step: [737] d_loss: 0.44020787, g_loss: 1.00215912\n",
      "Step: [738] d_loss: 0.47532639, g_loss: 0.99372303\n",
      "Step: [739] d_loss: 0.51911485, g_loss: 0.94906706\n",
      "Step: [740] d_loss: 0.58977950, g_loss: 0.89708614\n",
      "Step: [741] d_loss: 0.50441182, g_loss: 0.99270713\n",
      "Step: [742] d_loss: 0.49533084, g_loss: 0.95585579\n",
      "Step: [743] d_loss: 0.43203503, g_loss: 0.96518111\n",
      "Step: [744] d_loss: 0.44245160, g_loss: 1.04485214\n",
      "Step: [745] d_loss: 0.41462970, g_loss: 1.00557816\n",
      "Step: [746] d_loss: 0.57503617, g_loss: 0.92989844\n",
      "Step: [747] d_loss: 0.45356119, g_loss: 0.95198661\n",
      "Step: [748] d_loss: 0.46037108, g_loss: 1.00101852\n",
      "Step: [749] d_loss: 0.52650982, g_loss: 0.97094166\n",
      "Step: [750] d_loss: 0.48376015, g_loss: 1.02674329\n",
      "Step: [751] d_loss: 0.44325837, g_loss: 1.04613686\n",
      "Step: [752] d_loss: 0.52077496, g_loss: 0.91547823\n",
      "Step: [753] d_loss: 0.51428264, g_loss: 0.90376508\n",
      "Step: [754] d_loss: 0.47207376, g_loss: 0.98731959\n",
      "Step: [755] d_loss: 0.53250313, g_loss: 1.00328398\n",
      "Step: [756] d_loss: 0.42111149, g_loss: 0.93461967\n",
      "Step: [757] d_loss: 0.48354018, g_loss: 0.94406033\n",
      "Step: [758] d_loss: 0.42259672, g_loss: 1.05846500\n",
      "Step: [759] d_loss: 0.44963911, g_loss: 1.00703704\n",
      "Step: [760] d_loss: 0.41176054, g_loss: 1.05926681\n",
      "Step: [761] d_loss: 0.46469325, g_loss: 0.98350048\n",
      "Step: [762] d_loss: 0.50289965, g_loss: 0.98153746\n",
      "Step: [763] d_loss: 0.59535909, g_loss: 0.90150553\n",
      "Step: [764] d_loss: 0.55191255, g_loss: 0.90350157\n",
      "Step: [765] d_loss: 0.61006862, g_loss: 0.90526420\n",
      "Step: [766] d_loss: 0.50082797, g_loss: 0.93731642\n",
      "Step: [767] d_loss: 0.43342257, g_loss: 1.04555821\n",
      "Step: [768] d_loss: 0.40508297, g_loss: 1.10628641\n",
      "Step: [769] d_loss: 0.42949411, g_loss: 1.08494496\n",
      "Step: [770] d_loss: 0.38177231, g_loss: 1.02659571\n",
      "Step: [771] d_loss: 0.39285547, g_loss: 1.16286588\n",
      "Step: [772] d_loss: 0.42333764, g_loss: 1.04051924\n",
      "Step: [773] d_loss: 0.42222291, g_loss: 1.01721811\n",
      "Step: [774] d_loss: 0.42218390, g_loss: 0.99500674\n",
      "Step: [775] d_loss: 0.54369563, g_loss: 0.93093073\n",
      "Step: [776] d_loss: 0.47269210, g_loss: 0.93832767\n",
      "Step: [777] d_loss: 0.41974413, g_loss: 1.01903379\n",
      "Step: [778] d_loss: 0.41297069, g_loss: 0.95732975\n",
      "Step: [779] d_loss: 0.46909630, g_loss: 0.96792352\n",
      "Step: [780] d_loss: 0.40088260, g_loss: 1.01275969\n",
      "Step: [781] d_loss: 0.41475782, g_loss: 1.07434916\n",
      "Step: [782] d_loss: 0.42257616, g_loss: 1.01370049\n",
      "Step: [783] d_loss: 0.45063373, g_loss: 0.95925272\n",
      "Step: [784] d_loss: 0.42137399, g_loss: 1.03955948\n",
      "Step: [785] d_loss: 0.49955133, g_loss: 0.95166701\n",
      "Step: [786] d_loss: 0.43884552, g_loss: 1.01105034\n",
      "Step: [787] d_loss: 0.41118166, g_loss: 1.00183141\n",
      "Step: [788] d_loss: 0.40049431, g_loss: 1.16203272\n",
      "Step: [789] d_loss: 0.40411487, g_loss: 1.08327293\n",
      "Step: [790] d_loss: 0.41285291, g_loss: 1.00801229\n",
      "Step: [791] d_loss: 0.42450503, g_loss: 1.07078433\n",
      "Step: [792] d_loss: 0.43712214, g_loss: 0.97613102\n",
      "Step: [793] d_loss: 0.43086210, g_loss: 1.11619043\n",
      "Step: [794] d_loss: 0.44070619, g_loss: 0.97550666\n",
      "Step: [795] d_loss: 0.46541190, g_loss: 1.02482581\n",
      "Step: [796] d_loss: 0.40405321, g_loss: 1.12128937\n",
      "Step: [797] d_loss: 0.41731492, g_loss: 1.01087034\n",
      "Step: [798] d_loss: 0.42214674, g_loss: 1.03093529\n",
      "Step: [799] d_loss: 0.47412470, g_loss: 0.99281192\n",
      "Step: [800] d_loss: 0.44272450, g_loss: 0.96553069\n",
      "Step: [801] d_loss: 0.46356022, g_loss: 0.94856519\n",
      "Step: [802] d_loss: 0.50295925, g_loss: 0.98480374\n",
      "Step: [803] d_loss: 0.37209031, g_loss: 0.98121685\n",
      "Step: [804] d_loss: 0.39484066, g_loss: 1.08341312\n",
      "Step: [805] d_loss: 0.42497030, g_loss: 1.05361164\n",
      "Step: [806] d_loss: 0.39643222, g_loss: 1.18974948\n",
      "Step: [807] d_loss: 0.42523918, g_loss: 1.05524135\n",
      "Step: [808] d_loss: 0.39751342, g_loss: 1.06837475\n",
      "Step: [809] d_loss: 0.48208460, g_loss: 0.99035794\n",
      "Step: [810] d_loss: 0.41390654, g_loss: 0.98380476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [811] d_loss: 0.38468492, g_loss: 0.95505774\n",
      "Step: [812] d_loss: 0.42301452, g_loss: 1.02920139\n",
      "Step: [813] d_loss: 0.41915992, g_loss: 1.12081969\n",
      "Step: [814] d_loss: 0.41971031, g_loss: 0.99404866\n",
      "Step: [815] d_loss: 0.39801893, g_loss: 1.08435118\n",
      "Step: [816] d_loss: 0.38800740, g_loss: 1.08191848\n",
      "Step: [817] d_loss: 0.41891211, g_loss: 0.94767487\n",
      "Step: [818] d_loss: 0.43047795, g_loss: 0.96817654\n",
      "Step: [819] d_loss: 0.46380660, g_loss: 0.98258746\n",
      "Step: [820] d_loss: 0.45762867, g_loss: 0.98786283\n",
      "Step: [821] d_loss: 0.39401561, g_loss: 1.33345056\n",
      "Step: [822] d_loss: 0.38823053, g_loss: 1.00911593\n",
      "Step: [823] d_loss: 0.40650105, g_loss: 1.05078924\n",
      "Step: [824] d_loss: 0.45681074, g_loss: 0.97442281\n",
      "Step: [825] d_loss: 0.46395189, g_loss: 1.01754236\n",
      "Step: [826] d_loss: 0.52361596, g_loss: 0.95410138\n",
      "Step: [827] d_loss: 0.51853102, g_loss: 0.98506266\n",
      "Step: [828] d_loss: 0.55424380, g_loss: 0.95388508\n",
      "Step: [829] d_loss: 0.42167628, g_loss: 1.02178669\n",
      "Step: [830] d_loss: 0.50034881, g_loss: 0.96608365\n",
      "Step: [831] d_loss: 0.40535977, g_loss: 0.97789192\n",
      "Step: [832] d_loss: 0.38806966, g_loss: 1.14197135\n",
      "Step: [833] d_loss: 0.38512209, g_loss: 1.07537651\n",
      "Step: [834] d_loss: 0.39096165, g_loss: 1.11258388\n",
      "Step: [835] d_loss: 0.39096573, g_loss: 1.09376740\n",
      "Step: [836] d_loss: 0.40486214, g_loss: 1.01566851\n",
      "Step: [837] d_loss: 0.47116438, g_loss: 0.95659220\n",
      "Step: [838] d_loss: 0.39654225, g_loss: 1.13709342\n",
      "Step: [839] d_loss: 0.43544820, g_loss: 1.01197290\n",
      "Step: [840] d_loss: 0.47880009, g_loss: 0.95692581\n",
      "Step: [841] d_loss: 0.39366689, g_loss: 0.99359018\n",
      "Step: [842] d_loss: 0.45969915, g_loss: 1.02197635\n",
      "Step: [843] d_loss: 0.39025930, g_loss: 1.06336904\n",
      "Step: [844] d_loss: 0.38341409, g_loss: 1.04014719\n",
      "Step: [845] d_loss: 0.49903509, g_loss: 0.97236240\n",
      "Step: [846] d_loss: 0.44195023, g_loss: 1.04926872\n",
      "Step: [847] d_loss: 0.50797844, g_loss: 0.98263228\n",
      "Step: [848] d_loss: 0.40410990, g_loss: 1.04568624\n",
      "Step: [849] d_loss: 0.38871312, g_loss: 1.05834758\n",
      "Step: [850] d_loss: 0.45057371, g_loss: 0.98530054\n",
      "Step: [851] d_loss: 0.44999555, g_loss: 0.98536760\n",
      "Step: [852] d_loss: 0.45402157, g_loss: 1.01413345\n",
      "Step: [853] d_loss: 0.38222450, g_loss: 1.29859078\n",
      "Step: [854] d_loss: 0.37026450, g_loss: 1.30820739\n",
      "Step: [855] d_loss: 0.38899255, g_loss: 1.03677118\n",
      "Step: [856] d_loss: 0.40725052, g_loss: 1.06009519\n",
      "Step: [857] d_loss: 0.39609212, g_loss: 1.06055534\n",
      "Step: [858] d_loss: 0.40392917, g_loss: 1.04036093\n",
      "Step: [859] d_loss: 0.38452530, g_loss: 1.00667381\n",
      "Step: [860] d_loss: 0.37445986, g_loss: 1.01856506\n",
      "Step: [861] d_loss: 0.44327086, g_loss: 0.98635495\n",
      "Step: [862] d_loss: 0.49710238, g_loss: 0.97400570\n",
      "Step: [863] d_loss: 0.44292784, g_loss: 1.00129795\n",
      "Step: [864] d_loss: 0.38797772, g_loss: 1.01528537\n",
      "Step: [865] d_loss: 0.39680430, g_loss: 1.04295182\n",
      "Step: [866] d_loss: 0.39365456, g_loss: 1.06411362\n",
      "Step: [867] d_loss: 0.45885199, g_loss: 0.98271674\n",
      "Step: [868] d_loss: 0.41440636, g_loss: 1.02005100\n",
      "Step: [869] d_loss: 0.51307863, g_loss: 0.98928905\n",
      "Step: [870] d_loss: 0.42160502, g_loss: 1.01488340\n",
      "Step: [871] d_loss: 0.38067865, g_loss: 1.03411412\n",
      "Step: [872] d_loss: 0.39474046, g_loss: 1.08752859\n",
      "Step: [873] d_loss: 0.39340326, g_loss: 1.05192363\n",
      "Step: [874] d_loss: 0.43399513, g_loss: 1.05793464\n",
      "Step: [875] d_loss: 0.45105371, g_loss: 1.01591158\n",
      "Step: [876] d_loss: 0.38165286, g_loss: 1.09752548\n",
      "Step: [877] d_loss: 0.38643718, g_loss: 1.07700324\n",
      "Step: [878] d_loss: 0.37902668, g_loss: 1.16096258\n",
      "Step: [879] d_loss: 0.38265240, g_loss: 1.08740234\n",
      "Step: [880] d_loss: 0.42782471, g_loss: 1.08735764\n",
      "Step: [881] d_loss: 0.44171768, g_loss: 1.02140772\n",
      "Step: [882] d_loss: 0.45879799, g_loss: 1.00936091\n",
      "Step: [883] d_loss: 0.38768050, g_loss: 1.03192639\n",
      "Step: [884] d_loss: 0.45492736, g_loss: 1.01070452\n",
      "Step: [885] d_loss: 0.37130377, g_loss: 1.06411660\n",
      "Step: [886] d_loss: 0.37143257, g_loss: 1.07998323\n",
      "Step: [887] d_loss: 0.37989736, g_loss: 1.05018258\n",
      "Step: [888] d_loss: 0.38138345, g_loss: 1.15635860\n",
      "Step: [889] d_loss: 0.39848515, g_loss: 1.10382259\n",
      "Step: [890] d_loss: 0.36675289, g_loss: 1.07091415\n",
      "Step: [891] d_loss: 0.37308246, g_loss: 1.11130464\n",
      "Step: [892] d_loss: 0.35864803, g_loss: 1.04149830\n",
      "Step: [893] d_loss: 0.39093202, g_loss: 1.23726189\n",
      "Step: [894] d_loss: 0.34117687, g_loss: 1.07424772\n",
      "Step: [895] d_loss: 0.38752961, g_loss: 1.06036043\n",
      "Step: [896] d_loss: 0.40395314, g_loss: 1.01270902\n",
      "Step: [897] d_loss: 0.40495265, g_loss: 1.02642882\n",
      "Step: [898] d_loss: 0.38839385, g_loss: 1.15353715\n",
      "Step: [899] d_loss: 0.39563179, g_loss: 1.14763141\n",
      "Step: [900] d_loss: 0.39967918, g_loss: 1.07933187\n",
      "Step: [901] d_loss: 0.38800672, g_loss: 1.06943345\n",
      "Step: [902] d_loss: 0.39507991, g_loss: 1.00417995\n",
      "Step: [903] d_loss: 0.41142106, g_loss: 1.05590057\n",
      "Step: [904] d_loss: 0.41454449, g_loss: 1.00457585\n",
      "Step: [905] d_loss: 0.44438860, g_loss: 0.99947000\n",
      "Step: [906] d_loss: 0.41349623, g_loss: 1.05105364\n",
      "Step: [907] d_loss: 0.40193743, g_loss: 1.02260208\n",
      "Step: [908] d_loss: 0.37541205, g_loss: 1.05413353\n",
      "Step: [909] d_loss: 0.36918664, g_loss: 1.10985148\n",
      "Step: [910] d_loss: 0.36989617, g_loss: 1.11830604\n",
      "Step: [911] d_loss: 0.37298691, g_loss: 1.08396339\n",
      "Step: [912] d_loss: 0.37488565, g_loss: 1.07263470\n",
      "Step: [913] d_loss: 0.37665930, g_loss: 1.07848465\n",
      "Step: [914] d_loss: 0.38729531, g_loss: 1.09962559\n",
      "Step: [915] d_loss: 0.38673005, g_loss: 1.04047883\n",
      "Step: [916] d_loss: 0.37170404, g_loss: 1.05287361\n",
      "Step: [917] d_loss: 0.37018818, g_loss: 1.08238840\n",
      "Step: [918] d_loss: 0.40168375, g_loss: 1.12509525\n",
      "Step: [919] d_loss: 0.43370938, g_loss: 1.01817155\n",
      "Step: [920] d_loss: 0.40442041, g_loss: 1.06530070\n",
      "Step: [921] d_loss: 0.37492251, g_loss: 1.07597065\n",
      "Step: [922] d_loss: 0.40493697, g_loss: 1.06708562\n",
      "Step: [923] d_loss: 0.37608039, g_loss: 1.15614891\n",
      "Step: [924] d_loss: 0.37711370, g_loss: 1.06936944\n",
      "Step: [925] d_loss: 0.37255225, g_loss: 1.14367223\n",
      "Step: [926] d_loss: 0.38292462, g_loss: 1.07158899\n",
      "Step: [927] d_loss: 0.44729760, g_loss: 1.00563002\n",
      "Step: [928] d_loss: 0.36444581, g_loss: 1.08658242\n",
      "Step: [929] d_loss: 0.37008619, g_loss: 1.06614101\n",
      "Step: [930] d_loss: 0.45272842, g_loss: 1.01265883\n",
      "Step: [931] d_loss: 0.36240736, g_loss: 1.12839937\n",
      "Step: [932] d_loss: 0.40525711, g_loss: 1.07650793\n",
      "Step: [933] d_loss: 0.37384179, g_loss: 1.11822367\n",
      "Step: [934] d_loss: 0.39458477, g_loss: 1.07525134\n",
      "Step: [935] d_loss: 0.42512020, g_loss: 1.06074107\n",
      "Step: [936] d_loss: 0.43718043, g_loss: 1.00394666\n",
      "Step: [937] d_loss: 0.39106983, g_loss: 1.00273907\n",
      "Step: [938] d_loss: 0.44536981, g_loss: 1.00366044\n",
      "Step: [939] d_loss: 0.35749462, g_loss: 1.12081182\n",
      "Step: [940] d_loss: 0.37702045, g_loss: 1.04812896\n",
      "Step: [941] d_loss: 0.37975600, g_loss: 1.13026118\n",
      "Step: [942] d_loss: 0.37866145, g_loss: 1.10460424\n",
      "Step: [943] d_loss: 0.38478020, g_loss: 1.15594518\n",
      "Step: [944] d_loss: 0.35195792, g_loss: 1.07870531\n",
      "Step: [945] d_loss: 0.39081508, g_loss: 1.16730535\n",
      "Step: [946] d_loss: 0.38151836, g_loss: 1.02973151\n",
      "Step: [947] d_loss: 0.37406689, g_loss: 1.04554224\n",
      "Step: [948] d_loss: 0.37934613, g_loss: 1.02898550\n",
      "Step: [949] d_loss: 0.36727872, g_loss: 1.04174984\n",
      "Step: [950] d_loss: 0.36417747, g_loss: 1.06233406\n",
      "Step: [951] d_loss: 0.35821804, g_loss: 1.04280436\n",
      "Step: [952] d_loss: 0.35239941, g_loss: 1.07224524\n",
      "Step: [953] d_loss: 0.37976024, g_loss: 1.02234030\n",
      "Step: [954] d_loss: 0.46126932, g_loss: 0.98690462\n",
      "Step: [955] d_loss: 0.36820760, g_loss: 1.11634338\n",
      "Step: [956] d_loss: 0.37017116, g_loss: 1.05726123\n",
      "Step: [957] d_loss: 0.36376676, g_loss: 1.11042786\n",
      "Step: [958] d_loss: 0.36229366, g_loss: 1.04111958\n",
      "Step: [959] d_loss: 0.37141332, g_loss: 1.09012175\n",
      "Step: [960] d_loss: 0.37475392, g_loss: 1.06418419\n",
      "Step: [961] d_loss: 0.37130499, g_loss: 1.05438054\n",
      "Step: [962] d_loss: 0.36464596, g_loss: 1.12815154\n",
      "Step: [963] d_loss: 0.39420393, g_loss: 1.04070759\n",
      "Step: [964] d_loss: 0.36436802, g_loss: 1.00612319\n",
      "Step: [965] d_loss: 0.45499000, g_loss: 0.98605698\n",
      "Step: [966] d_loss: 0.36547375, g_loss: 1.06648469\n",
      "Step: [967] d_loss: 0.38803029, g_loss: 1.06277418\n",
      "Step: [968] d_loss: 0.38566241, g_loss: 1.07774758\n",
      "Step: [969] d_loss: 0.37105292, g_loss: 1.08387446\n",
      "Step: [970] d_loss: 0.40908721, g_loss: 1.03679657\n",
      "Step: [971] d_loss: 0.37238830, g_loss: 1.15005553\n",
      "Step: [972] d_loss: 0.38583547, g_loss: 1.13208067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [973] d_loss: 0.38989860, g_loss: 1.07490015\n",
      "Step: [974] d_loss: 0.36957848, g_loss: 1.15623641\n",
      "Step: [975] d_loss: 0.37745032, g_loss: 1.06790149\n",
      "Step: [976] d_loss: 0.36538827, g_loss: 1.09690166\n",
      "Step: [977] d_loss: 0.36189443, g_loss: 1.01370072\n",
      "Step: [978] d_loss: 0.50215888, g_loss: 1.01172066\n",
      "Step: [979] d_loss: 0.38123244, g_loss: 1.18925416\n",
      "Step: [980] d_loss: 0.48237354, g_loss: 0.99275744\n",
      "Step: [981] d_loss: 0.43934456, g_loss: 1.05534065\n",
      "Step: [982] d_loss: 0.37100250, g_loss: 1.03016722\n",
      "Step: [983] d_loss: 0.36387649, g_loss: 1.06330359\n",
      "Step: [984] d_loss: 0.38066742, g_loss: 1.09783828\n",
      "Step: [985] d_loss: 0.36871111, g_loss: 1.04920650\n",
      "Step: [986] d_loss: 0.36586279, g_loss: 1.13706040\n",
      "Step: [987] d_loss: 0.37472627, g_loss: 1.09801090\n",
      "Step: [988] d_loss: 0.36511046, g_loss: 1.09772646\n",
      "Step: [989] d_loss: 0.36222875, g_loss: 1.13336861\n",
      "Step: [990] d_loss: 0.36971933, g_loss: 1.11160719\n",
      "Step: [991] d_loss: 0.35335755, g_loss: 1.04394019\n",
      "Step: [992] d_loss: 0.35991475, g_loss: 1.07206357\n",
      "Step: [993] d_loss: 0.37694684, g_loss: 1.02843308\n",
      "Step: [994] d_loss: 0.35291517, g_loss: 1.09888244\n",
      "Step: [995] d_loss: 0.35657606, g_loss: 1.07999718\n",
      "Step: [996] d_loss: 0.38478562, g_loss: 1.15825963\n",
      "Step: [997] d_loss: 0.36158821, g_loss: 1.05226386\n",
      "Step: [998] d_loss: 0.34549579, g_loss: 1.09910858\n",
      "Step: [999] d_loss: 0.37663025, g_loss: 1.08933628\n",
      "Step: [1000] d_loss: 0.42634520, g_loss: 1.03365433\n",
      "Step: [1001] d_loss: 0.35341552, g_loss: 1.03406942\n",
      "Step: [1002] d_loss: 0.36709276, g_loss: 1.11073339\n",
      "Step: [1003] d_loss: 0.37763005, g_loss: 1.01523054\n",
      "Step: [1004] d_loss: 0.35407481, g_loss: 1.05056882\n",
      "Step: [1005] d_loss: 0.36713651, g_loss: 1.02049470\n",
      "Step: [1006] d_loss: 0.37633520, g_loss: 1.06340981\n",
      "Step: [1007] d_loss: 0.40595627, g_loss: 0.99574178\n",
      "Step: [1008] d_loss: 0.43070519, g_loss: 1.02910125\n",
      "Step: [1009] d_loss: 0.44963524, g_loss: 1.00258374\n",
      "Step: [1010] d_loss: 0.35967964, g_loss: 1.03768480\n",
      "Step: [1011] d_loss: 0.34645027, g_loss: 1.03767824\n",
      "Step: [1012] d_loss: 0.41849360, g_loss: 1.06054068\n",
      "Step: [1013] d_loss: 0.37849256, g_loss: 1.14551830\n",
      "Step: [1014] d_loss: 0.40584174, g_loss: 1.02378654\n",
      "Step: [1015] d_loss: 0.35471958, g_loss: 1.21337926\n",
      "Step: [1016] d_loss: 0.33445999, g_loss: 1.06744623\n",
      "Step: [1017] d_loss: 0.37774599, g_loss: 1.03290331\n",
      "Step: [1018] d_loss: 0.35279980, g_loss: 1.04806364\n",
      "Step: [1019] d_loss: 0.41742301, g_loss: 1.00242507\n",
      "Step: [1020] d_loss: 0.36158565, g_loss: 1.00812984\n",
      "Step: [1021] d_loss: 0.38091558, g_loss: 1.05330062\n",
      "Step: [1022] d_loss: 0.37810439, g_loss: 1.06899512\n",
      "Step: [1023] d_loss: 0.39893603, g_loss: 1.03470373\n",
      "Step: [1024] d_loss: 0.42628637, g_loss: 1.08366942\n",
      "Step: [1025] d_loss: 0.43084985, g_loss: 0.99198300\n",
      "Step: [1026] d_loss: 0.35585049, g_loss: 1.12659991\n",
      "Step: [1027] d_loss: 0.36951721, g_loss: 1.06743348\n",
      "Step: [1028] d_loss: 0.42390406, g_loss: 1.01169062\n",
      "Step: [1029] d_loss: 0.39251709, g_loss: 1.11721516\n",
      "Step: [1030] d_loss: 0.36063811, g_loss: 1.18439889\n",
      "Step: [1031] d_loss: 0.34929559, g_loss: 1.02462649\n",
      "Step: [1032] d_loss: 0.38551348, g_loss: 1.02553391\n",
      "Step: [1033] d_loss: 0.38056323, g_loss: 1.01249349\n",
      "Step: [1034] d_loss: 0.39334276, g_loss: 1.03011382\n",
      "Step: [1035] d_loss: 0.37030616, g_loss: 1.12274611\n",
      "Step: [1036] d_loss: 0.37512487, g_loss: 1.06951475\n",
      "Step: [1037] d_loss: 0.34804696, g_loss: 1.01018226\n",
      "Step: [1038] d_loss: 0.35084948, g_loss: 1.02880359\n",
      "Step: [1039] d_loss: 0.37126353, g_loss: 1.06470907\n",
      "Step: [1040] d_loss: 0.42310399, g_loss: 1.01442063\n",
      "Step: [1041] d_loss: 0.45398164, g_loss: 1.00700712\n",
      "Step: [1042] d_loss: 0.40398210, g_loss: 1.01307142\n",
      "Step: [1043] d_loss: 0.42942575, g_loss: 0.98574257\n",
      "Step: [1044] d_loss: 0.44134748, g_loss: 0.98771071\n",
      "Step: [1045] d_loss: 0.35901982, g_loss: 1.07409811\n",
      "Step: [1046] d_loss: 0.41792351, g_loss: 1.03135538\n",
      "Step: [1047] d_loss: 0.41990811, g_loss: 1.00614798\n",
      "Step: [1048] d_loss: 0.35528260, g_loss: 1.02615380\n",
      "Step: [1049] d_loss: 0.38638672, g_loss: 1.06369185\n",
      "Step: [1050] d_loss: 0.38486430, g_loss: 1.09700859\n",
      "Step: [1051] d_loss: 0.43298402, g_loss: 0.99864399\n",
      "Step: [1052] d_loss: 0.49226525, g_loss: 0.95427775\n",
      "Step: [1053] d_loss: 0.36533961, g_loss: 1.09530497\n",
      "Step: [1054] d_loss: 0.42710528, g_loss: 1.00843275\n",
      "Step: [1055] d_loss: 0.36953482, g_loss: 1.05677152\n",
      "Step: [1056] d_loss: 0.39882100, g_loss: 1.01468480\n",
      "Step: [1057] d_loss: 0.42604288, g_loss: 1.01025331\n",
      "Step: [1058] d_loss: 0.43996239, g_loss: 0.99842596\n",
      "Step: [1059] d_loss: 0.39770657, g_loss: 1.03097975\n",
      "Step: [1060] d_loss: 0.46350184, g_loss: 0.96071619\n",
      "Step: [1061] d_loss: 0.42682558, g_loss: 0.99751157\n",
      "Step: [1062] d_loss: 0.37489468, g_loss: 1.01233935\n",
      "Step: [1063] d_loss: 0.44626552, g_loss: 0.95512927\n",
      "Step: [1064] d_loss: 0.33874565, g_loss: 1.02447855\n",
      "Step: [1065] d_loss: 0.48101151, g_loss: 0.97329479\n",
      "Step: [1066] d_loss: 0.41912827, g_loss: 1.00226641\n",
      "Step: [1067] d_loss: 0.52448124, g_loss: 0.93003428\n",
      "Step: [1068] d_loss: 0.45207688, g_loss: 0.97454089\n",
      "Step: [1069] d_loss: 0.49156234, g_loss: 0.96652961\n",
      "Step: [1070] d_loss: 0.49591213, g_loss: 0.93920809\n",
      "Step: [1071] d_loss: 0.45481136, g_loss: 0.99785447\n",
      "Step: [1072] d_loss: 0.48145595, g_loss: 0.96302980\n",
      "Step: [1073] d_loss: 0.46005735, g_loss: 0.97013199\n",
      "Step: [1074] d_loss: 0.40346166, g_loss: 1.02069807\n",
      "Step: [1075] d_loss: 0.56897509, g_loss: 0.89229298\n",
      "Step: [1076] d_loss: 0.51780409, g_loss: 0.94050056\n",
      "Step: [1077] d_loss: 0.45661151, g_loss: 0.99383920\n",
      "Step: [1078] d_loss: 0.46587998, g_loss: 0.95940804\n",
      "Step: [1079] d_loss: 0.45981973, g_loss: 0.96837491\n",
      "Step: [1080] d_loss: 0.50181782, g_loss: 0.96272731\n",
      "Step: [1081] d_loss: 0.50089455, g_loss: 0.95822811\n",
      "Step: [1082] d_loss: 0.51640189, g_loss: 0.95536214\n",
      "Step: [1083] d_loss: 0.50666726, g_loss: 0.94037825\n",
      "Step: [1084] d_loss: 0.59344715, g_loss: 0.86682123\n",
      "Step: [1085] d_loss: 0.49197307, g_loss: 0.94368488\n",
      "Step: [1086] d_loss: 0.58142024, g_loss: 0.87583268\n",
      "Step: [1087] d_loss: 0.55326819, g_loss: 0.89387679\n",
      "Step: [1088] d_loss: 0.58547294, g_loss: 0.87103248\n",
      "Step: [1089] d_loss: 0.59382808, g_loss: 0.84861273\n",
      "Step: [1090] d_loss: 0.57924926, g_loss: 0.86045581\n",
      "Step: [1091] d_loss: 0.59398007, g_loss: 0.86257070\n",
      "Step: [1092] d_loss: 0.57446510, g_loss: 0.87193704\n",
      "Step: [1093] d_loss: 0.56919777, g_loss: 0.86372346\n",
      "Step: [1094] d_loss: 0.58739257, g_loss: 0.90186542\n",
      "Step: [1095] d_loss: 0.54527628, g_loss: 0.89510357\n",
      "Step: [1096] d_loss: 0.51024771, g_loss: 0.95730138\n",
      "Step: [1097] d_loss: 0.58363801, g_loss: 0.88846260\n",
      "Step: [1098] d_loss: 0.53939080, g_loss: 0.94712704\n",
      "Step: [1099] d_loss: 0.52301413, g_loss: 0.94596231\n",
      "Step: [1100] d_loss: 0.53897429, g_loss: 0.91123110\n",
      "Step: [1101] d_loss: 0.59262341, g_loss: 0.89364880\n",
      "Step: [1102] d_loss: 0.56736529, g_loss: 0.90121281\n",
      "Step: [1103] d_loss: 0.54729414, g_loss: 0.91437513\n",
      "Step: [1104] d_loss: 0.52044201, g_loss: 0.94672871\n",
      "Step: [1105] d_loss: 0.60953885, g_loss: 0.88557935\n",
      "Step: [1106] d_loss: 0.58353758, g_loss: 0.94285357\n",
      "Step: [1107] d_loss: 0.54212642, g_loss: 0.92107987\n",
      "Step: [1108] d_loss: 0.52040058, g_loss: 0.92852348\n",
      "Step: [1109] d_loss: 0.58184397, g_loss: 0.91050893\n",
      "Step: [1110] d_loss: 0.54127038, g_loss: 0.91985381\n",
      "Step: [1111] d_loss: 0.58159679, g_loss: 0.90481937\n",
      "Step: [1112] d_loss: 0.54317653, g_loss: 0.92380720\n",
      "Step: [1113] d_loss: 0.60372275, g_loss: 0.85515958\n",
      "Step: [1114] d_loss: 0.56743133, g_loss: 0.88213140\n",
      "Step: [1115] d_loss: 0.56019694, g_loss: 0.89991915\n",
      "Step: [1116] d_loss: 0.54040277, g_loss: 0.88947934\n",
      "Step: [1117] d_loss: 0.62527001, g_loss: 0.83190489\n",
      "Step: [1118] d_loss: 0.59890985, g_loss: 0.86268264\n",
      "Step: [1119] d_loss: 0.58375639, g_loss: 0.88462210\n",
      "Step: [1120] d_loss: 0.56086707, g_loss: 0.92838132\n",
      "Step: [1121] d_loss: 0.54054117, g_loss: 0.91172278\n",
      "Step: [1122] d_loss: 0.56511807, g_loss: 0.88855886\n",
      "Step: [1123] d_loss: 0.61982685, g_loss: 0.86837435\n",
      "Step: [1124] d_loss: 0.55454087, g_loss: 0.89234066\n",
      "Step: [1125] d_loss: 0.64323795, g_loss: 0.86520743\n",
      "Step: [1126] d_loss: 0.61606932, g_loss: 0.81398994\n",
      "Step: [1127] d_loss: 0.65319347, g_loss: 0.82780927\n",
      "Step: [1128] d_loss: 0.65919423, g_loss: 0.81963634\n",
      "Step: [1129] d_loss: 0.64313638, g_loss: 0.84791523\n",
      "Step: [1130] d_loss: 0.61319304, g_loss: 0.84447348\n",
      "Step: [1131] d_loss: 0.64032722, g_loss: 0.83107054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1132] d_loss: 0.61278403, g_loss: 0.86323500\n",
      "Step: [1133] d_loss: 0.62148297, g_loss: 0.85320276\n",
      "Step: [1134] d_loss: 0.63136387, g_loss: 0.83408415\n",
      "Step: [1135] d_loss: 0.61193717, g_loss: 0.88109362\n",
      "Step: [1136] d_loss: 0.61060536, g_loss: 0.87434649\n",
      "Step: [1137] d_loss: 0.61961067, g_loss: 0.84447825\n",
      "Step: [1138] d_loss: 0.57377481, g_loss: 0.90300274\n",
      "Step: [1139] d_loss: 0.66277421, g_loss: 0.82967019\n",
      "Step: [1140] d_loss: 0.57527328, g_loss: 0.86634737\n",
      "Step: [1141] d_loss: 0.60042775, g_loss: 0.87263155\n",
      "Step: [1142] d_loss: 0.61487699, g_loss: 0.88055718\n",
      "Step: [1143] d_loss: 0.58648908, g_loss: 0.88530552\n",
      "Step: [1144] d_loss: 0.62724006, g_loss: 0.85878813\n",
      "Step: [1145] d_loss: 0.61924362, g_loss: 0.87279981\n",
      "Step: [1146] d_loss: 0.57571191, g_loss: 0.89083040\n",
      "Step: [1147] d_loss: 0.58877182, g_loss: 0.87637049\n",
      "Step: [1148] d_loss: 0.52490258, g_loss: 0.93393564\n",
      "Step: [1149] d_loss: 0.58989984, g_loss: 0.91013193\n",
      "Step: [1150] d_loss: 0.60271060, g_loss: 0.84252059\n",
      "Step: [1151] d_loss: 0.51371789, g_loss: 0.97391945\n",
      "Step: [1152] d_loss: 0.59382880, g_loss: 0.89190596\n",
      "Step: [1153] d_loss: 0.57909417, g_loss: 0.85291123\n",
      "Step: [1154] d_loss: 0.58224124, g_loss: 0.90261942\n",
      "Step: [1155] d_loss: 0.61213529, g_loss: 0.91970140\n",
      "Step: [1156] d_loss: 0.56576347, g_loss: 0.91694736\n",
      "Step: [1157] d_loss: 0.54425085, g_loss: 0.94017243\n",
      "Step: [1158] d_loss: 0.59915257, g_loss: 0.88870722\n",
      "Step: [1159] d_loss: 0.51730382, g_loss: 0.93959141\n",
      "Step: [1160] d_loss: 0.50173706, g_loss: 0.93453765\n",
      "Step: [1161] d_loss: 0.58396935, g_loss: 0.89334214\n",
      "Step: [1162] d_loss: 0.59203249, g_loss: 0.89214724\n",
      "Step: [1163] d_loss: 0.65723920, g_loss: 0.84116638\n",
      "Step: [1164] d_loss: 0.49912170, g_loss: 0.94809371\n",
      "Step: [1165] d_loss: 0.58069694, g_loss: 0.90826952\n",
      "Step: [1166] d_loss: 0.58682144, g_loss: 0.87952107\n",
      "Step: [1167] d_loss: 0.56931543, g_loss: 0.87846708\n",
      "Step: [1168] d_loss: 0.55720460, g_loss: 0.87756872\n",
      "Step: [1169] d_loss: 0.56813282, g_loss: 0.88798797\n",
      "Step: [1170] d_loss: 0.60304701, g_loss: 0.85686213\n",
      "Step: [1171] d_loss: 0.56978726, g_loss: 0.85936403\n",
      "Step: [1172] d_loss: 0.61732548, g_loss: 0.84338015\n",
      "Step: [1173] d_loss: 0.64535248, g_loss: 0.81656051\n",
      "Step: [1174] d_loss: 0.62694573, g_loss: 0.84530348\n",
      "Step: [1175] d_loss: 0.55751812, g_loss: 0.85445285\n",
      "Step: [1176] d_loss: 0.59593803, g_loss: 0.87771678\n",
      "Step: [1177] d_loss: 0.57164240, g_loss: 0.87414801\n",
      "Step: [1178] d_loss: 0.54024321, g_loss: 0.87437242\n",
      "Step: [1179] d_loss: 0.66362810, g_loss: 0.84483129\n",
      "Step: [1180] d_loss: 0.61331999, g_loss: 0.84699702\n",
      "Step: [1181] d_loss: 0.55131805, g_loss: 0.89646983\n",
      "Step: [1182] d_loss: 0.64190316, g_loss: 0.79959631\n",
      "Step: [1183] d_loss: 0.61464238, g_loss: 0.82116908\n",
      "Step: [1184] d_loss: 0.63511431, g_loss: 0.82410651\n",
      "Step: [1185] d_loss: 0.64550769, g_loss: 0.79927212\n",
      "Step: [1186] d_loss: 0.62265658, g_loss: 0.83338815\n",
      "Step: [1187] d_loss: 0.60619354, g_loss: 0.81804329\n",
      "Step: [1188] d_loss: 0.65339911, g_loss: 0.79692006\n",
      "Step: [1189] d_loss: 0.62709427, g_loss: 0.79257411\n",
      "Step: [1190] d_loss: 0.70230722, g_loss: 0.77812600\n",
      "Step: [1191] d_loss: 0.62167531, g_loss: 0.81901813\n",
      "Step: [1192] d_loss: 0.57933295, g_loss: 0.85200214\n",
      "Step: [1193] d_loss: 0.68290579, g_loss: 0.77885574\n",
      "Step: [1194] d_loss: 0.63541162, g_loss: 0.81605482\n",
      "Step: [1195] d_loss: 0.61024278, g_loss: 0.79833227\n",
      "Step: [1196] d_loss: 0.63558280, g_loss: 0.79249829\n",
      "Step: [1197] d_loss: 0.67979223, g_loss: 0.77633798\n",
      "Step: [1198] d_loss: 0.62967134, g_loss: 0.79699910\n",
      "Step: [1199] d_loss: 0.66398108, g_loss: 0.77249652\n",
      "Step: [1200] d_loss: 0.64419186, g_loss: 0.78746307\n",
      "Step: [1201] d_loss: 0.58946967, g_loss: 0.82487297\n",
      "Step: [1202] d_loss: 0.59524757, g_loss: 0.79742837\n",
      "Step: [1203] d_loss: 0.67850542, g_loss: 0.77716744\n",
      "Step: [1204] d_loss: 0.69093370, g_loss: 0.75324517\n",
      "Step: [1205] d_loss: 0.65258056, g_loss: 0.78452706\n",
      "Step: [1206] d_loss: 0.65715158, g_loss: 0.75802612\n",
      "Step: [1207] d_loss: 0.65734231, g_loss: 0.80557585\n",
      "Step: [1208] d_loss: 0.61523896, g_loss: 0.77259976\n",
      "Step: [1209] d_loss: 0.70241845, g_loss: 0.74682486\n",
      "Step: [1210] d_loss: 0.60285139, g_loss: 0.84165365\n",
      "Step: [1211] d_loss: 0.61250973, g_loss: 0.78428143\n",
      "Step: [1212] d_loss: 0.59411478, g_loss: 0.79851055\n",
      "Step: [1213] d_loss: 0.67651391, g_loss: 0.77931321\n",
      "Step: [1214] d_loss: 0.64686924, g_loss: 0.76143074\n",
      "Step: [1215] d_loss: 0.56333721, g_loss: 0.79848933\n",
      "Step: [1216] d_loss: 0.64616978, g_loss: 0.76625997\n",
      "Step: [1217] d_loss: 0.71049541, g_loss: 0.72022521\n",
      "Step: [1218] d_loss: 0.56703198, g_loss: 0.81020617\n",
      "Step: [1219] d_loss: 0.53146559, g_loss: 0.83178264\n",
      "Step: [1220] d_loss: 0.63780111, g_loss: 0.76789081\n",
      "Step: [1221] d_loss: 0.57694817, g_loss: 0.80243546\n",
      "Step: [1222] d_loss: 0.62345433, g_loss: 0.78318912\n",
      "Step: [1223] d_loss: 0.64793652, g_loss: 0.75279558\n",
      "Step: [1224] d_loss: 0.57046103, g_loss: 0.80450600\n",
      "Step: [1225] d_loss: 0.52316272, g_loss: 0.84401512\n",
      "Step: [1226] d_loss: 0.67435980, g_loss: 0.71734858\n",
      "Step: [1227] d_loss: 0.59985226, g_loss: 0.75891578\n",
      "Step: [1228] d_loss: 0.63174331, g_loss: 0.74238175\n",
      "Step: [1229] d_loss: 0.65520275, g_loss: 0.74852538\n",
      "Step: [1230] d_loss: 0.61896443, g_loss: 0.78397071\n",
      "Step: [1231] d_loss: 0.56041747, g_loss: 0.80569828\n",
      "Step: [1232] d_loss: 0.55045176, g_loss: 0.77204651\n",
      "Step: [1233] d_loss: 0.66049254, g_loss: 0.73385519\n",
      "Step: [1234] d_loss: 0.60856891, g_loss: 0.77035743\n",
      "Step: [1235] d_loss: 0.59321481, g_loss: 0.81526101\n",
      "Step: [1236] d_loss: 0.63790143, g_loss: 0.74862105\n",
      "Step: [1237] d_loss: 0.58419865, g_loss: 0.79492551\n",
      "Step: [1238] d_loss: 0.63388729, g_loss: 0.76223081\n",
      "Step: [1239] d_loss: 0.53677046, g_loss: 0.83816731\n",
      "Step: [1240] d_loss: 0.55166745, g_loss: 0.80630827\n",
      "Step: [1241] d_loss: 0.58837980, g_loss: 0.78831095\n",
      "Step: [1242] d_loss: 0.57966810, g_loss: 0.82980031\n",
      "Step: [1243] d_loss: 0.56724513, g_loss: 0.78132641\n",
      "Step: [1244] d_loss: 0.65179998, g_loss: 0.76036423\n",
      "Step: [1245] d_loss: 0.63512820, g_loss: 0.79291403\n",
      "Step: [1246] d_loss: 0.51676929, g_loss: 0.87186062\n",
      "Step: [1247] d_loss: 0.60460937, g_loss: 0.78771651\n",
      "Step: [1248] d_loss: 0.50267279, g_loss: 0.88150090\n",
      "Step: [1249] d_loss: 0.50576431, g_loss: 0.84698904\n",
      "Step: [1250] d_loss: 0.54715383, g_loss: 0.83268744\n",
      "Step: [1251] d_loss: 0.60714602, g_loss: 0.80318755\n",
      "Step: [1252] d_loss: 0.58316672, g_loss: 0.78904557\n",
      "Step: [1253] d_loss: 0.52858353, g_loss: 0.85725570\n",
      "Step: [1254] d_loss: 0.50779527, g_loss: 0.82911074\n",
      "Step: [1255] d_loss: 0.49564105, g_loss: 0.89889479\n",
      "Step: [1256] d_loss: 0.63580489, g_loss: 0.78504050\n",
      "Step: [1257] d_loss: 0.52585000, g_loss: 0.84575683\n",
      "Step: [1258] d_loss: 0.52161241, g_loss: 0.82401234\n",
      "Step: [1259] d_loss: 0.52464581, g_loss: 0.83078498\n",
      "Step: [1260] d_loss: 0.44510147, g_loss: 0.88725299\n",
      "Step: [1261] d_loss: 0.57351273, g_loss: 0.84153771\n",
      "Step: [1262] d_loss: 0.57824266, g_loss: 0.83467937\n",
      "Step: [1263] d_loss: 0.56951779, g_loss: 0.84484947\n",
      "Step: [1264] d_loss: 0.50330055, g_loss: 0.85678428\n",
      "Step: [1265] d_loss: 0.52493459, g_loss: 0.84506303\n",
      "Step: [1266] d_loss: 0.50605142, g_loss: 0.86433923\n",
      "Step: [1267] d_loss: 0.50225711, g_loss: 0.83917165\n",
      "Step: [1268] d_loss: 0.50983739, g_loss: 0.86962277\n",
      "Step: [1269] d_loss: 0.47844678, g_loss: 0.87574482\n",
      "Step: [1270] d_loss: 0.51706117, g_loss: 0.90031809\n",
      "Step: [1271] d_loss: 0.50172794, g_loss: 0.87932515\n",
      "Step: [1272] d_loss: 0.56266248, g_loss: 0.87539339\n",
      "Step: [1273] d_loss: 0.52461475, g_loss: 0.87785953\n",
      "Step: [1274] d_loss: 0.55502778, g_loss: 0.86229795\n",
      "Step: [1275] d_loss: 0.46807930, g_loss: 0.92054713\n",
      "Step: [1276] d_loss: 0.54576302, g_loss: 0.85457253\n",
      "Step: [1277] d_loss: 0.56299484, g_loss: 0.89552391\n",
      "Step: [1278] d_loss: 0.51222658, g_loss: 0.88404596\n",
      "Step: [1279] d_loss: 0.44106132, g_loss: 0.91499609\n",
      "Step: [1280] d_loss: 0.51960683, g_loss: 0.88409686\n",
      "Step: [1281] d_loss: 0.41099820, g_loss: 0.97023660\n",
      "Step: [1282] d_loss: 0.50409198, g_loss: 0.93148452\n",
      "Step: [1283] d_loss: 0.48302713, g_loss: 0.89559102\n",
      "Step: [1284] d_loss: 0.49365979, g_loss: 0.88253272\n",
      "Step: [1285] d_loss: 0.42994395, g_loss: 0.96572578\n",
      "Step: [1286] d_loss: 0.51251805, g_loss: 0.93335927\n",
      "Step: [1287] d_loss: 0.43632427, g_loss: 0.99535173\n",
      "Step: [1288] d_loss: 0.49384654, g_loss: 0.91491914\n",
      "Step: [1289] d_loss: 0.49616414, g_loss: 0.93719923\n",
      "Step: [1290] d_loss: 0.47001716, g_loss: 0.95310301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1291] d_loss: 0.48125935, g_loss: 0.95323616\n",
      "Step: [1292] d_loss: 0.51108944, g_loss: 0.92051405\n",
      "Step: [1293] d_loss: 0.47219950, g_loss: 0.92594332\n",
      "Step: [1294] d_loss: 0.51873600, g_loss: 0.90030605\n",
      "Step: [1295] d_loss: 0.39891008, g_loss: 0.96119076\n",
      "Step: [1296] d_loss: 0.53569603, g_loss: 0.89667833\n",
      "Step: [1297] d_loss: 0.47034442, g_loss: 0.92925125\n",
      "Step: [1298] d_loss: 0.52770454, g_loss: 0.91767108\n",
      "Step: [1299] d_loss: 0.46022916, g_loss: 0.90888011\n",
      "Step: [1300] d_loss: 0.52110887, g_loss: 0.91962665\n",
      "Step: [1301] d_loss: 0.43078640, g_loss: 0.97329116\n",
      "Step: [1302] d_loss: 0.47105217, g_loss: 0.94752157\n",
      "Step: [1303] d_loss: 0.42611912, g_loss: 0.95405799\n",
      "Step: [1304] d_loss: 0.53770852, g_loss: 0.87456000\n",
      "Step: [1305] d_loss: 0.50607002, g_loss: 0.93371952\n",
      "Step: [1306] d_loss: 0.47174931, g_loss: 0.94339991\n",
      "Step: [1307] d_loss: 0.49300006, g_loss: 0.93767166\n",
      "Step: [1308] d_loss: 0.46985120, g_loss: 0.94500375\n",
      "Step: [1309] d_loss: 0.45652995, g_loss: 0.95210755\n",
      "Step: [1310] d_loss: 0.46686554, g_loss: 0.93031293\n",
      "Step: [1311] d_loss: 0.47863030, g_loss: 0.97089070\n",
      "Step: [1312] d_loss: 0.55201924, g_loss: 0.90862924\n",
      "Step: [1313] d_loss: 0.43685713, g_loss: 0.99682581\n",
      "Step: [1314] d_loss: 0.50961888, g_loss: 0.92179888\n",
      "Step: [1315] d_loss: 0.45092523, g_loss: 0.98946208\n",
      "Step: [1316] d_loss: 0.54895991, g_loss: 0.90070558\n",
      "Step: [1317] d_loss: 0.48797274, g_loss: 0.94581419\n",
      "Step: [1318] d_loss: 0.46813557, g_loss: 0.95336848\n",
      "Step: [1319] d_loss: 0.49146786, g_loss: 0.95931733\n",
      "Step: [1320] d_loss: 0.53928351, g_loss: 0.92362440\n",
      "Step: [1321] d_loss: 0.48530892, g_loss: 0.98847586\n",
      "Step: [1322] d_loss: 0.41911623, g_loss: 1.03676486\n",
      "Step: [1323] d_loss: 0.50745720, g_loss: 0.97718066\n",
      "Step: [1324] d_loss: 0.49406469, g_loss: 0.95896113\n",
      "Step: [1325] d_loss: 0.42505983, g_loss: 0.99483824\n",
      "Step: [1326] d_loss: 0.45507255, g_loss: 1.02020550\n",
      "Step: [1327] d_loss: 0.44388098, g_loss: 0.95996571\n",
      "Step: [1328] d_loss: 0.50521040, g_loss: 0.97161913\n",
      "Step: [1329] d_loss: 0.47736317, g_loss: 0.96622193\n",
      "Step: [1330] d_loss: 0.41584620, g_loss: 1.01337743\n",
      "Step: [1331] d_loss: 0.41852570, g_loss: 1.01900768\n",
      "Step: [1332] d_loss: 0.50611722, g_loss: 0.95897233\n",
      "Step: [1333] d_loss: 0.42573327, g_loss: 1.00385273\n",
      "Step: [1334] d_loss: 0.47276950, g_loss: 0.95029062\n",
      "Step: [1335] d_loss: 0.42378631, g_loss: 0.97333062\n",
      "Step: [1336] d_loss: 0.44479746, g_loss: 0.96716470\n",
      "Step: [1337] d_loss: 0.49010143, g_loss: 0.97745174\n",
      "Step: [1338] d_loss: 0.43225533, g_loss: 0.98022372\n",
      "Step: [1339] d_loss: 0.44938162, g_loss: 0.99932265\n",
      "Step: [1340] d_loss: 0.46525750, g_loss: 0.95077175\n",
      "Step: [1341] d_loss: 0.52638704, g_loss: 0.96662718\n",
      "Step: [1342] d_loss: 0.46169248, g_loss: 0.97674215\n",
      "Step: [1343] d_loss: 0.43010879, g_loss: 1.00093889\n",
      "Step: [1344] d_loss: 0.50234652, g_loss: 0.94404191\n",
      "Step: [1345] d_loss: 0.48151538, g_loss: 0.96131486\n",
      "Step: [1346] d_loss: 0.40876013, g_loss: 0.97642684\n",
      "Step: [1347] d_loss: 0.46477637, g_loss: 0.99929512\n",
      "Step: [1348] d_loss: 0.45564669, g_loss: 0.96428090\n",
      "Step: [1349] d_loss: 0.47901952, g_loss: 0.97610205\n",
      "Step: [1350] d_loss: 0.39440832, g_loss: 0.99943805\n",
      "Step: [1351] d_loss: 0.42693427, g_loss: 1.00676799\n",
      "Step: [1352] d_loss: 0.43090564, g_loss: 1.00526631\n",
      "Step: [1353] d_loss: 0.41945097, g_loss: 0.98242903\n",
      "Step: [1354] d_loss: 0.46755046, g_loss: 1.00794220\n",
      "Step: [1355] d_loss: 0.46418720, g_loss: 1.00681257\n",
      "Step: [1356] d_loss: 0.41088644, g_loss: 1.02531254\n",
      "Step: [1357] d_loss: 0.39354011, g_loss: 0.98269308\n",
      "Step: [1358] d_loss: 0.43046486, g_loss: 1.04028308\n",
      "Step: [1359] d_loss: 0.40484494, g_loss: 0.99780661\n",
      "Step: [1360] d_loss: 0.45831537, g_loss: 0.98169595\n",
      "Step: [1361] d_loss: 0.47650629, g_loss: 0.97623158\n",
      "Step: [1362] d_loss: 0.49087068, g_loss: 0.99615902\n",
      "Step: [1363] d_loss: 0.41263902, g_loss: 0.99219716\n",
      "Step: [1364] d_loss: 0.43312141, g_loss: 0.99713421\n",
      "Step: [1365] d_loss: 0.48780352, g_loss: 0.99071467\n",
      "Step: [1366] d_loss: 0.46356040, g_loss: 0.97767127\n",
      "Step: [1367] d_loss: 0.50175697, g_loss: 0.94684547\n",
      "Step: [1368] d_loss: 0.48480186, g_loss: 0.97687614\n",
      "Step: [1369] d_loss: 0.48404658, g_loss: 0.98353595\n",
      "Step: [1370] d_loss: 0.49872562, g_loss: 0.94953495\n",
      "Step: [1371] d_loss: 0.46220478, g_loss: 1.00752795\n",
      "Step: [1372] d_loss: 0.46310335, g_loss: 0.95638144\n",
      "Step: [1373] d_loss: 0.56435251, g_loss: 0.93049753\n",
      "Step: [1374] d_loss: 0.51700908, g_loss: 0.94221407\n",
      "Step: [1375] d_loss: 0.40346450, g_loss: 0.98787957\n",
      "Step: [1376] d_loss: 0.45140702, g_loss: 0.95262676\n",
      "Step: [1377] d_loss: 0.47321877, g_loss: 0.94955742\n",
      "Step: [1378] d_loss: 0.48068038, g_loss: 0.93521029\n",
      "Step: [1379] d_loss: 0.57866770, g_loss: 0.89162797\n",
      "Step: [1380] d_loss: 0.44464436, g_loss: 0.98320591\n",
      "Step: [1381] d_loss: 0.45900980, g_loss: 0.98871762\n",
      "Step: [1382] d_loss: 0.45321229, g_loss: 1.00293887\n",
      "Step: [1383] d_loss: 0.53047639, g_loss: 0.93188465\n",
      "Step: [1384] d_loss: 0.51065665, g_loss: 0.98757786\n",
      "Step: [1385] d_loss: 0.43898708, g_loss: 0.97299820\n",
      "Step: [1386] d_loss: 0.44063023, g_loss: 0.96348459\n",
      "Step: [1387] d_loss: 0.46782753, g_loss: 0.96455109\n",
      "Step: [1388] d_loss: 0.41675112, g_loss: 0.98615575\n",
      "Step: [1389] d_loss: 0.43460381, g_loss: 1.00880933\n",
      "Step: [1390] d_loss: 0.45671877, g_loss: 0.96586102\n",
      "Step: [1391] d_loss: 0.51089680, g_loss: 0.96783572\n",
      "Step: [1392] d_loss: 0.47115180, g_loss: 0.99181634\n",
      "Step: [1393] d_loss: 0.47387296, g_loss: 0.94902992\n",
      "Step: [1394] d_loss: 0.45350525, g_loss: 1.03801501\n",
      "Step: [1395] d_loss: 0.40445998, g_loss: 0.94506007\n",
      "Step: [1396] d_loss: 0.40778211, g_loss: 0.96734011\n",
      "Step: [1397] d_loss: 0.47953033, g_loss: 0.92086083\n",
      "Step: [1398] d_loss: 0.38433221, g_loss: 0.96739125\n",
      "Step: [1399] d_loss: 0.45898283, g_loss: 1.02615988\n",
      "Step: [1400] d_loss: 0.42871204, g_loss: 0.96397477\n",
      "Step: [1401] d_loss: 0.39547896, g_loss: 1.01446974\n",
      "Step: [1402] d_loss: 0.37484556, g_loss: 1.04162169\n",
      "Step: [1403] d_loss: 0.44895497, g_loss: 0.96264029\n",
      "Step: [1404] d_loss: 0.49764448, g_loss: 0.91871107\n",
      "Step: [1405] d_loss: 0.43253034, g_loss: 0.93916917\n",
      "Step: [1406] d_loss: 0.44665417, g_loss: 0.98984826\n",
      "Step: [1407] d_loss: 0.42144996, g_loss: 0.99978065\n",
      "Step: [1408] d_loss: 0.38784170, g_loss: 1.02983129\n",
      "Step: [1409] d_loss: 0.50101477, g_loss: 0.92354870\n",
      "Step: [1410] d_loss: 0.46614385, g_loss: 0.94001496\n",
      "Step: [1411] d_loss: 0.51685959, g_loss: 0.92034185\n",
      "Step: [1412] d_loss: 0.52326643, g_loss: 0.89650422\n",
      "Step: [1413] d_loss: 0.42417437, g_loss: 1.00589991\n",
      "Step: [1414] d_loss: 0.41303107, g_loss: 1.03628147\n",
      "Step: [1415] d_loss: 0.46030322, g_loss: 0.91353530\n",
      "Step: [1416] d_loss: 0.38064566, g_loss: 1.03727388\n",
      "Step: [1417] d_loss: 0.42734429, g_loss: 0.98825562\n",
      "Step: [1418] d_loss: 0.41989103, g_loss: 0.96439332\n",
      "Step: [1419] d_loss: 0.38651130, g_loss: 0.95951331\n",
      "Step: [1420] d_loss: 0.44015083, g_loss: 0.95692205\n",
      "Step: [1421] d_loss: 0.46416587, g_loss: 0.95817816\n",
      "Step: [1422] d_loss: 0.45382193, g_loss: 0.95047170\n",
      "Step: [1423] d_loss: 0.42267150, g_loss: 1.00277948\n",
      "Step: [1424] d_loss: 0.47571313, g_loss: 0.95388913\n",
      "Step: [1425] d_loss: 0.40772551, g_loss: 0.96369457\n",
      "Step: [1426] d_loss: 0.48078856, g_loss: 1.00897181\n",
      "Step: [1427] d_loss: 0.39762485, g_loss: 1.01220047\n",
      "Step: [1428] d_loss: 0.39372832, g_loss: 0.98257256\n",
      "Step: [1429] d_loss: 0.41105568, g_loss: 1.00746763\n",
      "Step: [1430] d_loss: 0.36652920, g_loss: 1.02618682\n",
      "Step: [1431] d_loss: 0.37533414, g_loss: 1.00988925\n",
      "Step: [1432] d_loss: 0.42876545, g_loss: 1.06056368\n",
      "Step: [1433] d_loss: 0.36013243, g_loss: 1.00842941\n",
      "Step: [1434] d_loss: 0.42155492, g_loss: 0.98353577\n",
      "Step: [1435] d_loss: 0.52200186, g_loss: 0.92743361\n",
      "Step: [1436] d_loss: 0.42218900, g_loss: 0.97076041\n",
      "Step: [1437] d_loss: 0.42865977, g_loss: 1.00585711\n",
      "Step: [1438] d_loss: 0.38030937, g_loss: 1.01938653\n",
      "Step: [1439] d_loss: 0.41166919, g_loss: 1.00754082\n",
      "Step: [1440] d_loss: 0.42439678, g_loss: 0.95154136\n",
      "Step: [1441] d_loss: 0.44193718, g_loss: 0.98537755\n",
      "Step: [1442] d_loss: 0.44092882, g_loss: 0.97477156\n",
      "Step: [1443] d_loss: 0.52625048, g_loss: 0.94903505\n",
      "Step: [1444] d_loss: 0.43113598, g_loss: 1.03213882\n",
      "Step: [1445] d_loss: 0.39102626, g_loss: 0.99593878\n",
      "Step: [1446] d_loss: 0.39040747, g_loss: 1.01561987\n",
      "Step: [1447] d_loss: 0.47865406, g_loss: 0.95621109\n",
      "Step: [1448] d_loss: 0.47164270, g_loss: 0.95296550\n",
      "Step: [1449] d_loss: 0.40116435, g_loss: 1.02654254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1450] d_loss: 0.45544216, g_loss: 1.00017166\n",
      "Step: [1451] d_loss: 0.39853141, g_loss: 1.01489091\n",
      "Step: [1452] d_loss: 0.40506607, g_loss: 1.01278973\n",
      "Step: [1453] d_loss: 0.47470999, g_loss: 0.94976223\n",
      "Step: [1454] d_loss: 0.50539601, g_loss: 0.98660207\n",
      "Step: [1455] d_loss: 0.42734900, g_loss: 0.96107602\n",
      "Step: [1456] d_loss: 0.42021137, g_loss: 0.96406192\n",
      "Step: [1457] d_loss: 0.38753790, g_loss: 1.01631224\n",
      "Step: [1458] d_loss: 0.40303397, g_loss: 0.97414112\n",
      "Step: [1459] d_loss: 0.46397007, g_loss: 0.98390889\n",
      "Step: [1460] d_loss: 0.47284266, g_loss: 0.96336639\n",
      "Step: [1461] d_loss: 0.38667521, g_loss: 1.02477872\n",
      "Step: [1462] d_loss: 0.39637563, g_loss: 1.00254440\n",
      "Step: [1463] d_loss: 0.41592276, g_loss: 0.98818052\n",
      "Step: [1464] d_loss: 0.37803915, g_loss: 1.00397885\n",
      "Step: [1465] d_loss: 0.37366232, g_loss: 1.00384545\n",
      "Step: [1466] d_loss: 0.46519211, g_loss: 0.98484188\n",
      "Step: [1467] d_loss: 0.47143629, g_loss: 0.98286301\n",
      "Step: [1468] d_loss: 0.36689526, g_loss: 1.01889908\n",
      "Step: [1469] d_loss: 0.46253574, g_loss: 0.99315435\n",
      "Step: [1470] d_loss: 0.37561986, g_loss: 1.01253319\n",
      "Step: [1471] d_loss: 0.46132386, g_loss: 0.98191172\n",
      "Step: [1472] d_loss: 0.36737937, g_loss: 0.98918575\n",
      "Step: [1473] d_loss: 0.38084850, g_loss: 0.99983966\n",
      "Step: [1474] d_loss: 0.41966584, g_loss: 0.98960209\n",
      "Step: [1475] d_loss: 0.43078309, g_loss: 1.00174403\n",
      "Step: [1476] d_loss: 0.38251862, g_loss: 1.01192880\n",
      "Step: [1477] d_loss: 0.38799143, g_loss: 1.00825429\n",
      "Step: [1478] d_loss: 0.38509545, g_loss: 1.01064026\n",
      "Step: [1479] d_loss: 0.47460699, g_loss: 0.98157603\n",
      "Step: [1480] d_loss: 0.44038355, g_loss: 0.99307209\n",
      "Step: [1481] d_loss: 0.39130968, g_loss: 1.01609492\n",
      "Step: [1482] d_loss: 0.43308184, g_loss: 1.01202118\n",
      "Step: [1483] d_loss: 0.38301399, g_loss: 1.00525582\n",
      "Step: [1484] d_loss: 0.44715363, g_loss: 0.99783850\n",
      "Step: [1485] d_loss: 0.39080018, g_loss: 0.99161154\n",
      "Step: [1486] d_loss: 0.39604601, g_loss: 1.01735640\n",
      "Step: [1487] d_loss: 0.43722421, g_loss: 0.97225678\n",
      "Step: [1488] d_loss: 0.46121874, g_loss: 0.97383142\n",
      "Step: [1489] d_loss: 0.45161659, g_loss: 0.95850605\n",
      "Step: [1490] d_loss: 0.37509078, g_loss: 1.00302935\n",
      "Step: [1491] d_loss: 0.44062564, g_loss: 0.99012291\n",
      "Step: [1492] d_loss: 0.38872287, g_loss: 1.02268887\n",
      "Step: [1493] d_loss: 0.39239797, g_loss: 0.98236883\n",
      "Step: [1494] d_loss: 0.38295615, g_loss: 1.01837909\n",
      "Step: [1495] d_loss: 0.42797807, g_loss: 0.98559183\n",
      "Step: [1496] d_loss: 0.38162890, g_loss: 1.01803231\n",
      "Step: [1497] d_loss: 0.39333141, g_loss: 1.01635301\n",
      "Step: [1498] d_loss: 0.36318159, g_loss: 1.04940403\n",
      "Step: [1499] d_loss: 0.42415476, g_loss: 0.96772915\n",
      "Step: [1500] d_loss: 0.39603919, g_loss: 0.99675572\n",
      "Step: [1501] d_loss: 0.37431407, g_loss: 1.01929748\n",
      "Step: [1502] d_loss: 0.39742470, g_loss: 0.98994982\n",
      "Step: [1503] d_loss: 0.43267080, g_loss: 0.98257947\n",
      "Step: [1504] d_loss: 0.44200507, g_loss: 0.97346908\n",
      "Step: [1505] d_loss: 0.44899476, g_loss: 0.97526580\n",
      "Step: [1506] d_loss: 0.40401143, g_loss: 0.97917628\n",
      "Step: [1507] d_loss: 0.39688957, g_loss: 1.01702929\n",
      "Step: [1508] d_loss: 0.42682186, g_loss: 0.98567837\n",
      "Step: [1509] d_loss: 0.44855335, g_loss: 1.01007807\n",
      "Step: [1510] d_loss: 0.37326923, g_loss: 1.01368666\n",
      "Step: [1511] d_loss: 0.37739840, g_loss: 1.05595994\n",
      "Step: [1512] d_loss: 0.36706698, g_loss: 1.00246036\n",
      "Step: [1513] d_loss: 0.44607908, g_loss: 0.96410608\n",
      "Step: [1514] d_loss: 0.40990674, g_loss: 0.98057097\n",
      "Step: [1515] d_loss: 0.50464803, g_loss: 0.95104641\n",
      "Step: [1516] d_loss: 0.38958153, g_loss: 1.00608718\n",
      "Step: [1517] d_loss: 0.40558037, g_loss: 1.00268114\n",
      "Step: [1518] d_loss: 0.38615283, g_loss: 1.01693583\n",
      "Step: [1519] d_loss: 0.36304954, g_loss: 0.97172785\n",
      "Step: [1520] d_loss: 0.39713484, g_loss: 1.01745808\n",
      "Step: [1521] d_loss: 0.43824813, g_loss: 0.93160844\n",
      "Step: [1522] d_loss: 0.41180655, g_loss: 1.00348938\n",
      "Step: [1523] d_loss: 0.50538069, g_loss: 0.93763638\n",
      "Step: [1524] d_loss: 0.41380227, g_loss: 0.96758211\n",
      "Step: [1525] d_loss: 0.51394421, g_loss: 0.95059067\n",
      "Step: [1526] d_loss: 0.43774971, g_loss: 0.98035890\n",
      "Step: [1527] d_loss: 0.43033981, g_loss: 0.96370059\n",
      "Step: [1528] d_loss: 0.36519104, g_loss: 1.01536512\n",
      "Step: [1529] d_loss: 0.43441615, g_loss: 0.99343455\n",
      "Step: [1530] d_loss: 0.36846864, g_loss: 1.01689947\n",
      "Step: [1531] d_loss: 0.40845636, g_loss: 0.99357694\n",
      "Step: [1532] d_loss: 0.38714445, g_loss: 1.00672626\n",
      "Step: [1533] d_loss: 0.41612208, g_loss: 0.98289609\n",
      "Step: [1534] d_loss: 0.47047231, g_loss: 0.97731340\n",
      "Step: [1535] d_loss: 0.42100018, g_loss: 1.01253569\n",
      "Step: [1536] d_loss: 0.38509563, g_loss: 0.97884446\n",
      "Step: [1537] d_loss: 0.44232315, g_loss: 0.99254894\n",
      "Step: [1538] d_loss: 0.43486038, g_loss: 0.98855346\n",
      "Step: [1539] d_loss: 0.46663412, g_loss: 0.98998404\n",
      "Step: [1540] d_loss: 0.43999016, g_loss: 1.01214147\n",
      "Step: [1541] d_loss: 0.38326335, g_loss: 1.03602171\n",
      "Step: [1542] d_loss: 0.38456485, g_loss: 1.07977104\n",
      "Step: [1543] d_loss: 0.36252069, g_loss: 1.05159831\n",
      "Step: [1544] d_loss: 0.44072708, g_loss: 1.02264512\n",
      "Step: [1545] d_loss: 0.34200856, g_loss: 1.01620758\n",
      "Step: [1546] d_loss: 0.43302229, g_loss: 1.01145625\n",
      "Step: [1547] d_loss: 0.43261969, g_loss: 1.01502264\n",
      "Step: [1548] d_loss: 0.46323559, g_loss: 1.03492868\n",
      "Step: [1549] d_loss: 0.39239305, g_loss: 1.04061460\n",
      "Step: [1550] d_loss: 0.44440687, g_loss: 0.99921161\n",
      "Step: [1551] d_loss: 0.36948943, g_loss: 1.03757083\n",
      "Step: [1552] d_loss: 0.40780914, g_loss: 1.02059388\n",
      "Step: [1553] d_loss: 0.44092119, g_loss: 0.99450082\n",
      "Step: [1554] d_loss: 0.40398368, g_loss: 1.02096522\n",
      "Step: [1555] d_loss: 0.38269544, g_loss: 1.04376817\n",
      "Step: [1556] d_loss: 0.37892717, g_loss: 1.03250730\n",
      "Step: [1557] d_loss: 0.38119465, g_loss: 1.04539227\n",
      "Step: [1558] d_loss: 0.37944666, g_loss: 1.01595473\n",
      "Step: [1559] d_loss: 0.38885713, g_loss: 0.99295896\n",
      "Step: [1560] d_loss: 0.39539400, g_loss: 1.03619361\n",
      "Step: [1561] d_loss: 0.40755716, g_loss: 1.00467646\n",
      "Step: [1562] d_loss: 0.42658606, g_loss: 0.98592526\n",
      "Step: [1563] d_loss: 0.39004725, g_loss: 1.00835860\n",
      "Step: [1564] d_loss: 0.41465804, g_loss: 1.01903760\n",
      "Step: [1565] d_loss: 0.44934464, g_loss: 1.02982998\n",
      "Step: [1566] d_loss: 0.44148570, g_loss: 1.01960433\n",
      "Step: [1567] d_loss: 0.48789895, g_loss: 0.97699261\n",
      "Step: [1568] d_loss: 0.36902648, g_loss: 1.04524481\n",
      "Step: [1569] d_loss: 0.37801516, g_loss: 1.02121699\n",
      "Step: [1570] d_loss: 0.42093986, g_loss: 0.99653977\n",
      "Step: [1571] d_loss: 0.43892112, g_loss: 1.01063538\n",
      "Step: [1572] d_loss: 0.46318662, g_loss: 0.99776638\n",
      "Step: [1573] d_loss: 0.39666688, g_loss: 1.02293050\n",
      "Step: [1574] d_loss: 0.43853810, g_loss: 1.00604963\n",
      "Step: [1575] d_loss: 0.44096503, g_loss: 1.03378594\n",
      "Step: [1576] d_loss: 0.37698290, g_loss: 1.03327656\n",
      "Step: [1577] d_loss: 0.36829352, g_loss: 1.01901209\n",
      "Step: [1578] d_loss: 0.45451134, g_loss: 1.01483214\n",
      "Step: [1579] d_loss: 0.37539995, g_loss: 1.04173994\n",
      "Step: [1580] d_loss: 0.37976873, g_loss: 1.02143061\n",
      "Step: [1581] d_loss: 0.47651699, g_loss: 0.99986762\n",
      "Step: [1582] d_loss: 0.44814160, g_loss: 0.98208755\n",
      "Step: [1583] d_loss: 0.41625261, g_loss: 1.02887249\n",
      "Step: [1584] d_loss: 0.40867755, g_loss: 1.01382184\n",
      "Step: [1585] d_loss: 0.50016809, g_loss: 1.00132060\n",
      "Step: [1586] d_loss: 0.37937981, g_loss: 1.03723419\n",
      "Step: [1587] d_loss: 0.42213526, g_loss: 1.06921363\n",
      "Step: [1588] d_loss: 0.39606094, g_loss: 1.05051756\n",
      "Step: [1589] d_loss: 0.41231716, g_loss: 1.05099690\n",
      "Step: [1590] d_loss: 0.36956111, g_loss: 1.04530752\n",
      "Step: [1591] d_loss: 0.41362882, g_loss: 1.05996656\n",
      "Step: [1592] d_loss: 0.38981605, g_loss: 1.07619166\n",
      "Step: [1593] d_loss: 0.36810130, g_loss: 1.07248163\n",
      "Step: [1594] d_loss: 0.39350855, g_loss: 1.08858645\n",
      "Step: [1595] d_loss: 0.42495149, g_loss: 1.05106175\n",
      "Step: [1596] d_loss: 0.39134863, g_loss: 1.08616519\n",
      "Step: [1597] d_loss: 0.38866809, g_loss: 1.07307529\n",
      "Step: [1598] d_loss: 0.37250811, g_loss: 1.06947815\n",
      "Step: [1599] d_loss: 0.36422727, g_loss: 1.04990423\n",
      "Step: [1600] d_loss: 0.38586998, g_loss: 1.06359935\n",
      "Step: [1601] d_loss: 0.40695968, g_loss: 1.06937313\n",
      "Step: [1602] d_loss: 0.37895823, g_loss: 1.07497370\n",
      "Step: [1603] d_loss: 0.40946427, g_loss: 1.05559337\n",
      "Step: [1604] d_loss: 0.45158529, g_loss: 1.00905192\n",
      "Step: [1605] d_loss: 0.39187783, g_loss: 1.05240440\n",
      "Step: [1606] d_loss: 0.39948994, g_loss: 1.08946371\n",
      "Step: [1607] d_loss: 0.39983180, g_loss: 1.09593618\n",
      "Step: [1608] d_loss: 0.38670906, g_loss: 1.11666739\n",
      "Step: [1609] d_loss: 0.41003773, g_loss: 1.07492518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1610] d_loss: 0.38516265, g_loss: 1.09259915\n",
      "Step: [1611] d_loss: 0.37055489, g_loss: 1.10301757\n",
      "Step: [1612] d_loss: 0.38978398, g_loss: 1.05222380\n",
      "Step: [1613] d_loss: 0.42439383, g_loss: 1.06730080\n",
      "Step: [1614] d_loss: 0.39834830, g_loss: 1.07465267\n",
      "Step: [1615] d_loss: 0.38123760, g_loss: 1.07549584\n",
      "Step: [1616] d_loss: 0.40381137, g_loss: 1.06528425\n",
      "Step: [1617] d_loss: 0.41629893, g_loss: 1.05949247\n",
      "Step: [1618] d_loss: 0.40308738, g_loss: 1.05779076\n",
      "Step: [1619] d_loss: 0.44363898, g_loss: 1.05065477\n",
      "Step: [1620] d_loss: 0.41637778, g_loss: 1.06004620\n",
      "Step: [1621] d_loss: 0.44410285, g_loss: 1.06622183\n",
      "Step: [1622] d_loss: 0.49932754, g_loss: 1.04424369\n",
      "Step: [1623] d_loss: 0.41667023, g_loss: 1.03521609\n",
      "Step: [1624] d_loss: 0.46910828, g_loss: 1.06459332\n",
      "Step: [1625] d_loss: 0.41597867, g_loss: 1.03511167\n",
      "Step: [1626] d_loss: 0.40686414, g_loss: 1.11398697\n",
      "Step: [1627] d_loss: 0.46661314, g_loss: 1.05085623\n",
      "Step: [1628] d_loss: 0.43564528, g_loss: 1.10471952\n",
      "Step: [1629] d_loss: 0.42497113, g_loss: 1.07547951\n",
      "Step: [1630] d_loss: 0.39512187, g_loss: 1.05650020\n",
      "Step: [1631] d_loss: 0.43849358, g_loss: 1.06414819\n",
      "Step: [1632] d_loss: 0.40786600, g_loss: 1.14806402\n",
      "Step: [1633] d_loss: 0.43453655, g_loss: 1.03334212\n",
      "Step: [1634] d_loss: 0.39645329, g_loss: 1.03306246\n",
      "Step: [1635] d_loss: 0.40566993, g_loss: 1.07262337\n",
      "Step: [1636] d_loss: 0.49276015, g_loss: 0.98778772\n",
      "Step: [1637] d_loss: 0.61062074, g_loss: 1.13124359\n",
      "Step: [1638] d_loss: 0.49914098, g_loss: 1.17665279\n",
      "Step: [1639] d_loss: 0.49974531, g_loss: 0.97386980\n",
      "Step: [1640] d_loss: 0.47615561, g_loss: 1.38849926\n",
      "Step: [1641] d_loss: 0.58884013, g_loss: 0.95860523\n",
      "Step: [1642] d_loss: 0.63384187, g_loss: 0.94753391\n",
      "Step: [1643] d_loss: 0.55893904, g_loss: 0.93402773\n",
      "Step: [1644] d_loss: 0.50992739, g_loss: 0.96789575\n",
      "Step: [1645] d_loss: 0.58647698, g_loss: 1.14505959\n",
      "Step: [1646] d_loss: 0.69546354, g_loss: 1.00505626\n",
      "Step: [1647] d_loss: 0.57142103, g_loss: 0.83016348\n",
      "Step: [1648] d_loss: 0.77571142, g_loss: 0.79567081\n",
      "Step: [1649] d_loss: 0.75763309, g_loss: 0.77045989\n",
      "Step: [1650] d_loss: 0.81721312, g_loss: 0.86875165\n",
      "Step: [1651] d_loss: 0.74192888, g_loss: 0.96040618\n",
      "Step: [1652] d_loss: 0.75356823, g_loss: 0.87245089\n",
      "Step: [1653] d_loss: 0.75876671, g_loss: 0.91000909\n",
      "Step: [1654] d_loss: 0.79505616, g_loss: 0.88292897\n",
      "Step: [1655] d_loss: 0.90375370, g_loss: 0.96329004\n",
      "Step: [1656] d_loss: 0.77735263, g_loss: 0.94375044\n",
      "Step: [1657] d_loss: 0.77582830, g_loss: 1.05594671\n",
      "Step: [1658] d_loss: 0.87254566, g_loss: 0.84481978\n",
      "Step: [1659] d_loss: 0.81120574, g_loss: 0.95154953\n",
      "Step: [1660] d_loss: 0.90963399, g_loss: 0.87017328\n",
      "Step: [1661] d_loss: 0.95754802, g_loss: 0.92881387\n",
      "Step: [1662] d_loss: 0.85424525, g_loss: 1.05464220\n",
      "Step: [1663] d_loss: 0.80334496, g_loss: 0.94223768\n",
      "Step: [1664] d_loss: 0.81714743, g_loss: 0.89535546\n",
      "Step: [1665] d_loss: 0.81071687, g_loss: 0.96618229\n",
      "Step: [1666] d_loss: 0.89986187, g_loss: 0.98785049\n",
      "Step: [1667] d_loss: 0.85254699, g_loss: 1.07321167\n",
      "Step: [1668] d_loss: 0.84114558, g_loss: 0.89993125\n",
      "Step: [1669] d_loss: 0.79763538, g_loss: 0.91003054\n",
      "Step: [1670] d_loss: 0.83992052, g_loss: 0.88949668\n",
      "Step: [1671] d_loss: 0.85644168, g_loss: 0.87757671\n",
      "Step: [1672] d_loss: 0.92368227, g_loss: 0.86873662\n",
      "Step: [1673] d_loss: 0.84175014, g_loss: 1.01768160\n",
      "Step: [1674] d_loss: 0.74638689, g_loss: 1.10185957\n",
      "Step: [1675] d_loss: 0.80553001, g_loss: 0.98485976\n",
      "Step: [1676] d_loss: 0.71836609, g_loss: 0.83977270\n",
      "Step: [1677] d_loss: 0.79061133, g_loss: 0.87246567\n",
      "Step: [1678] d_loss: 0.74426472, g_loss: 0.94429272\n",
      "Step: [1679] d_loss: 0.69018441, g_loss: 1.05229914\n",
      "Step: [1680] d_loss: 0.78937310, g_loss: 1.06388772\n",
      "Step: [1681] d_loss: 0.74658465, g_loss: 0.89470005\n",
      "Step: [1682] d_loss: 0.81486994, g_loss: 0.90712821\n",
      "Step: [1683] d_loss: 0.82825673, g_loss: 0.97721481\n",
      "Step: [1684] d_loss: 0.75797707, g_loss: 1.01405966\n",
      "Step: [1685] d_loss: 0.83804792, g_loss: 0.89519262\n",
      "Step: [1686] d_loss: 0.79689282, g_loss: 0.96880275\n",
      "Step: [1687] d_loss: 0.78445971, g_loss: 0.96544206\n",
      "Step: [1688] d_loss: 0.84502757, g_loss: 0.90750223\n",
      "Step: [1689] d_loss: 0.72494233, g_loss: 0.95056361\n",
      "Step: [1690] d_loss: 0.70528090, g_loss: 0.90414715\n",
      "Step: [1691] d_loss: 0.64121503, g_loss: 0.90489674\n",
      "Step: [1692] d_loss: 0.82340503, g_loss: 0.92828315\n",
      "Step: [1693] d_loss: 0.77939641, g_loss: 0.93670213\n",
      "Step: [1694] d_loss: 0.72892320, g_loss: 0.93928289\n",
      "Step: [1695] d_loss: 0.67369497, g_loss: 0.90853417\n",
      "Step: [1696] d_loss: 0.74451500, g_loss: 0.89137447\n",
      "Step: [1697] d_loss: 0.74290454, g_loss: 0.95925421\n",
      "Step: [1698] d_loss: 0.71834850, g_loss: 0.99033731\n",
      "Step: [1699] d_loss: 0.64236474, g_loss: 0.87978059\n",
      "Step: [1700] d_loss: 0.64095527, g_loss: 0.96470100\n",
      "Step: [1701] d_loss: 0.62865537, g_loss: 1.00639117\n",
      "Step: [1702] d_loss: 0.62132484, g_loss: 0.92567813\n",
      "Step: [1703] d_loss: 0.66432536, g_loss: 0.95166004\n",
      "Step: [1704] d_loss: 0.66160119, g_loss: 0.97319156\n",
      "Step: [1705] d_loss: 0.63715172, g_loss: 0.98122984\n",
      "Step: [1706] d_loss: 0.66356659, g_loss: 0.97710001\n",
      "Step: [1707] d_loss: 0.71791482, g_loss: 1.04213083\n",
      "Step: [1708] d_loss: 0.76355946, g_loss: 0.92030072\n",
      "Step: [1709] d_loss: 0.67921036, g_loss: 0.86751437\n",
      "Step: [1710] d_loss: 0.70776266, g_loss: 0.86972445\n",
      "Step: [1711] d_loss: 0.67610359, g_loss: 0.98152703\n",
      "Step: [1712] d_loss: 0.74293238, g_loss: 0.94505185\n",
      "Step: [1713] d_loss: 0.70021391, g_loss: 1.05143964\n",
      "Step: [1714] d_loss: 0.69142348, g_loss: 0.97539717\n",
      "Step: [1715] d_loss: 0.70085162, g_loss: 1.00851929\n",
      "Step: [1716] d_loss: 0.69338912, g_loss: 1.00151896\n",
      "Step: [1717] d_loss: 0.70441300, g_loss: 1.10996366\n",
      "Step: [1718] d_loss: 0.71959174, g_loss: 0.94879645\n",
      "Step: [1719] d_loss: 0.70105755, g_loss: 0.90142947\n",
      "Step: [1720] d_loss: 0.67944235, g_loss: 0.97937775\n",
      "Step: [1721] d_loss: 0.69638795, g_loss: 0.92756802\n",
      "Step: [1722] d_loss: 0.76019382, g_loss: 0.93391883\n",
      "Step: [1723] d_loss: 0.71559197, g_loss: 1.00436938\n",
      "Step: [1724] d_loss: 0.81790102, g_loss: 0.92756999\n",
      "Step: [1725] d_loss: 0.68046576, g_loss: 0.87612820\n",
      "Step: [1726] d_loss: 0.65933800, g_loss: 0.86336905\n",
      "Step: [1727] d_loss: 0.69341004, g_loss: 0.88937253\n",
      "Step: [1728] d_loss: 0.68883353, g_loss: 0.89315057\n",
      "Step: [1729] d_loss: 0.63283104, g_loss: 0.84785491\n",
      "Step: [1730] d_loss: 0.65460879, g_loss: 0.95830274\n",
      "Step: [1731] d_loss: 0.66138333, g_loss: 0.90478909\n",
      "Step: [1732] d_loss: 0.72973776, g_loss: 0.80306011\n",
      "Step: [1733] d_loss: 0.73703933, g_loss: 0.81144756\n",
      "Step: [1734] d_loss: 0.81833357, g_loss: 0.86748320\n",
      "Step: [1735] d_loss: 0.77704126, g_loss: 0.96317226\n",
      "Step: [1736] d_loss: 0.66287369, g_loss: 1.02515042\n",
      "Step: [1737] d_loss: 0.82004321, g_loss: 0.90595728\n",
      "Step: [1738] d_loss: 0.70665139, g_loss: 0.90134293\n",
      "Step: [1739] d_loss: 0.87669390, g_loss: 0.79563034\n",
      "Step: [1740] d_loss: 0.76563174, g_loss: 0.89441633\n",
      "Step: [1741] d_loss: 0.64464444, g_loss: 0.83783734\n",
      "Step: [1742] d_loss: 0.76051384, g_loss: 0.87284696\n",
      "Step: [1743] d_loss: 0.60944462, g_loss: 0.81708342\n",
      "Step: [1744] d_loss: 0.73897451, g_loss: 0.82675695\n",
      "Step: [1745] d_loss: 0.79918998, g_loss: 0.83442086\n",
      "Step: [1746] d_loss: 0.80826390, g_loss: 0.92529535\n",
      "Step: [1747] d_loss: 0.77405614, g_loss: 0.91260004\n",
      "Step: [1748] d_loss: 0.64530635, g_loss: 0.89555955\n",
      "Step: [1749] d_loss: 0.74780494, g_loss: 0.85819352\n",
      "Step: [1750] d_loss: 0.80007899, g_loss: 0.89705944\n",
      "Step: [1751] d_loss: 0.71750444, g_loss: 0.95958900\n",
      "Step: [1752] d_loss: 0.77500665, g_loss: 0.92155230\n",
      "Step: [1753] d_loss: 0.63411576, g_loss: 0.80632788\n",
      "Step: [1754] d_loss: 0.73236781, g_loss: 0.86072934\n",
      "Step: [1755] d_loss: 0.72018546, g_loss: 0.89071369\n",
      "Step: [1756] d_loss: 0.75219876, g_loss: 0.94075984\n",
      "Step: [1757] d_loss: 0.68884283, g_loss: 0.88397491\n",
      "Step: [1758] d_loss: 0.67008036, g_loss: 0.88613057\n",
      "Step: [1759] d_loss: 0.60241741, g_loss: 0.86247784\n",
      "Step: [1760] d_loss: 0.77023590, g_loss: 0.86184973\n",
      "Step: [1761] d_loss: 0.79303449, g_loss: 0.93340248\n",
      "Step: [1762] d_loss: 0.77441663, g_loss: 0.98617047\n",
      "Step: [1763] d_loss: 0.74704987, g_loss: 0.87731194\n",
      "Step: [1764] d_loss: 0.66523880, g_loss: 0.95364547\n",
      "Step: [1765] d_loss: 0.68687463, g_loss: 0.98945117\n",
      "Step: [1766] d_loss: 0.84430820, g_loss: 0.90603399\n",
      "Step: [1767] d_loss: 0.76500577, g_loss: 0.86068815\n",
      "Step: [1768] d_loss: 0.68136203, g_loss: 0.85834163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1769] d_loss: 0.75190538, g_loss: 0.93592632\n",
      "Step: [1770] d_loss: 0.77015334, g_loss: 0.94579393\n",
      "Step: [1771] d_loss: 0.69850361, g_loss: 0.88100970\n",
      "Step: [1772] d_loss: 0.62352401, g_loss: 0.87458181\n",
      "Step: [1773] d_loss: 0.69086856, g_loss: 0.86564642\n",
      "Step: [1774] d_loss: 0.74975187, g_loss: 0.94187498\n",
      "Step: [1775] d_loss: 0.77999282, g_loss: 0.92138445\n",
      "Step: [1776] d_loss: 0.71112764, g_loss: 1.00126100\n",
      "Step: [1777] d_loss: 0.70226455, g_loss: 0.89930129\n",
      "Step: [1778] d_loss: 0.76498014, g_loss: 0.95902056\n",
      "Step: [1779] d_loss: 0.75352550, g_loss: 0.97262961\n",
      "Step: [1780] d_loss: 0.69632500, g_loss: 0.91324371\n",
      "Step: [1781] d_loss: 0.83448064, g_loss: 0.88902146\n",
      "Step: [1782] d_loss: 0.69998878, g_loss: 0.98710942\n",
      "Step: [1783] d_loss: 0.77283216, g_loss: 0.88260531\n",
      "Step: [1784] d_loss: 0.77591473, g_loss: 0.84560549\n",
      "Step: [1785] d_loss: 0.72529715, g_loss: 0.89811349\n",
      "Step: [1786] d_loss: 0.81016886, g_loss: 0.93757081\n",
      "Step: [1787] d_loss: 0.73487389, g_loss: 0.96795881\n",
      "Step: [1788] d_loss: 0.66367090, g_loss: 0.93030971\n",
      "Step: [1789] d_loss: 0.73976070, g_loss: 0.84354007\n",
      "Step: [1790] d_loss: 0.78855610, g_loss: 0.83527839\n",
      "Step: [1791] d_loss: 0.67807513, g_loss: 0.92717594\n",
      "Step: [1792] d_loss: 0.77372539, g_loss: 0.90135407\n",
      "Step: [1793] d_loss: 0.69264787, g_loss: 0.97805989\n",
      "Step: [1794] d_loss: 0.80424935, g_loss: 1.00806153\n",
      "Step: [1795] d_loss: 0.73980051, g_loss: 1.00612319\n",
      "Step: [1796] d_loss: 0.65648168, g_loss: 0.96293533\n",
      "Step: [1797] d_loss: 0.71526355, g_loss: 0.90102577\n",
      "Step: [1798] d_loss: 0.66124290, g_loss: 0.96325254\n",
      "Step: [1799] d_loss: 0.68206555, g_loss: 0.90616977\n",
      "Step: [1800] d_loss: 0.61560506, g_loss: 0.92651725\n",
      "Step: [1801] d_loss: 0.71008325, g_loss: 0.95726359\n",
      "Step: [1802] d_loss: 0.64203817, g_loss: 0.95211864\n",
      "Step: [1803] d_loss: 0.72076613, g_loss: 1.03825116\n",
      "Step: [1804] d_loss: 0.68682539, g_loss: 0.90282178\n",
      "Step: [1805] d_loss: 0.71632147, g_loss: 0.95898223\n",
      "Step: [1806] d_loss: 0.72644287, g_loss: 0.96005672\n",
      "Step: [1807] d_loss: 0.74920815, g_loss: 0.91220391\n",
      "Step: [1808] d_loss: 0.75077075, g_loss: 0.91000903\n",
      "Step: [1809] d_loss: 0.70204991, g_loss: 0.94582838\n",
      "Step: [1810] d_loss: 0.74841517, g_loss: 0.96641701\n",
      "Step: [1811] d_loss: 0.65618902, g_loss: 0.89126062\n",
      "Step: [1812] d_loss: 0.70733953, g_loss: 0.96308851\n",
      "Step: [1813] d_loss: 0.74683762, g_loss: 0.97014922\n",
      "Step: [1814] d_loss: 0.68150187, g_loss: 1.00732458\n",
      "Step: [1815] d_loss: 0.77996582, g_loss: 0.96106011\n",
      "Step: [1816] d_loss: 0.76521236, g_loss: 0.90978229\n",
      "Step: [1817] d_loss: 0.74387056, g_loss: 0.94142318\n",
      "Step: [1818] d_loss: 0.78643215, g_loss: 1.02019954\n",
      "Step: [1819] d_loss: 0.64132863, g_loss: 0.95631754\n",
      "Step: [1820] d_loss: 0.68334359, g_loss: 0.87823552\n",
      "Step: [1821] d_loss: 0.67979515, g_loss: 0.91485769\n",
      "Step: [1822] d_loss: 0.75749439, g_loss: 0.91913325\n",
      "Step: [1823] d_loss: 0.71902990, g_loss: 0.92014468\n",
      "Step: [1824] d_loss: 0.81481344, g_loss: 0.93415660\n",
      "Step: [1825] d_loss: 0.64891708, g_loss: 1.02040708\n",
      "Step: [1826] d_loss: 0.71804518, g_loss: 0.95993108\n",
      "Step: [1827] d_loss: 0.66457224, g_loss: 0.89661264\n",
      "Step: [1828] d_loss: 0.73039418, g_loss: 0.91695857\n",
      "Step: [1829] d_loss: 0.69953346, g_loss: 0.93734598\n",
      "Step: [1830] d_loss: 0.73310208, g_loss: 0.91245139\n",
      "Step: [1831] d_loss: 0.66108131, g_loss: 0.92247361\n",
      "Step: [1832] d_loss: 0.79559618, g_loss: 0.87833935\n",
      "Step: [1833] d_loss: 0.74553335, g_loss: 0.90863186\n",
      "Step: [1834] d_loss: 0.74977475, g_loss: 0.86330122\n",
      "Step: [1835] d_loss: 0.83012128, g_loss: 0.91248554\n",
      "Step: [1836] d_loss: 0.70061487, g_loss: 0.93182558\n",
      "Step: [1837] d_loss: 0.69882274, g_loss: 0.97270805\n",
      "Step: [1838] d_loss: 0.65829504, g_loss: 0.92430633\n",
      "Step: [1839] d_loss: 0.74464488, g_loss: 0.98192197\n",
      "Step: [1840] d_loss: 0.68747956, g_loss: 0.88366908\n",
      "Step: [1841] d_loss: 0.67434740, g_loss: 0.88853788\n",
      "Step: [1842] d_loss: 0.64950478, g_loss: 0.92658335\n",
      "Step: [1843] d_loss: 0.59532815, g_loss: 0.91641748\n",
      "Step: [1844] d_loss: 0.69829315, g_loss: 0.99536878\n",
      "Step: [1845] d_loss: 0.69674057, g_loss: 1.00348485\n",
      "Step: [1846] d_loss: 0.75068682, g_loss: 0.91178614\n",
      "Step: [1847] d_loss: 0.70606703, g_loss: 0.86751235\n",
      "Step: [1848] d_loss: 0.80416518, g_loss: 0.86377275\n",
      "Step: [1849] d_loss: 0.65080154, g_loss: 0.92445779\n",
      "Step: [1850] d_loss: 0.80398327, g_loss: 0.90978777\n",
      "Step: [1851] d_loss: 0.67046529, g_loss: 0.91066778\n",
      "Step: [1852] d_loss: 0.73564082, g_loss: 0.93016595\n",
      "Step: [1853] d_loss: 0.84494776, g_loss: 0.86732882\n",
      "Step: [1854] d_loss: 0.79771495, g_loss: 0.95485675\n",
      "Step: [1855] d_loss: 0.69198048, g_loss: 0.96922237\n",
      "Step: [1856] d_loss: 0.75084323, g_loss: 0.89805335\n",
      "Step: [1857] d_loss: 0.69794798, g_loss: 0.84503156\n",
      "Step: [1858] d_loss: 0.89201593, g_loss: 0.83196706\n",
      "Step: [1859] d_loss: 0.81668508, g_loss: 0.89185989\n",
      "Step: [1860] d_loss: 0.66089958, g_loss: 0.96545845\n",
      "Step: [1861] d_loss: 0.67996895, g_loss: 0.96096909\n",
      "Step: [1862] d_loss: 0.74066883, g_loss: 0.87912881\n",
      "Step: [1863] d_loss: 0.79959625, g_loss: 0.93235916\n",
      "Step: [1864] d_loss: 0.78442878, g_loss: 0.95514369\n",
      "Step: [1865] d_loss: 0.72255749, g_loss: 1.02042961\n",
      "Step: [1866] d_loss: 0.70275182, g_loss: 0.92556626\n",
      "Step: [1867] d_loss: 0.74656934, g_loss: 0.98429161\n",
      "Step: [1868] d_loss: 0.66004771, g_loss: 0.99859625\n",
      "Step: [1869] d_loss: 0.68704420, g_loss: 0.96472448\n",
      "Step: [1870] d_loss: 0.75594282, g_loss: 0.95625412\n",
      "Step: [1871] d_loss: 0.78538519, g_loss: 0.99463564\n",
      "Step: [1872] d_loss: 0.73059821, g_loss: 0.93226665\n",
      "Step: [1873] d_loss: 0.69439203, g_loss: 0.93936408\n",
      "Step: [1874] d_loss: 0.63989675, g_loss: 0.96379566\n",
      "Step: [1875] d_loss: 0.69245696, g_loss: 0.92037022\n",
      "Step: [1876] d_loss: 0.68294686, g_loss: 0.87505859\n",
      "Step: [1877] d_loss: 0.69639057, g_loss: 0.87378430\n",
      "Step: [1878] d_loss: 0.70613551, g_loss: 0.94662827\n",
      "Step: [1879] d_loss: 0.65243793, g_loss: 0.87850457\n",
      "Step: [1880] d_loss: 0.75978857, g_loss: 0.87394720\n",
      "Step: [1881] d_loss: 0.72425741, g_loss: 0.91655117\n",
      "Step: [1882] d_loss: 0.79029262, g_loss: 0.89413023\n",
      "Step: [1883] d_loss: 0.67236584, g_loss: 0.98066992\n",
      "Step: [1884] d_loss: 0.71481401, g_loss: 0.98792940\n",
      "Step: [1885] d_loss: 0.75642103, g_loss: 0.93700075\n",
      "Step: [1886] d_loss: 0.76513129, g_loss: 0.91560835\n",
      "Step: [1887] d_loss: 0.68990648, g_loss: 0.85637140\n",
      "Step: [1888] d_loss: 0.70872581, g_loss: 0.83887619\n",
      "Step: [1889] d_loss: 0.74868882, g_loss: 0.84543955\n",
      "Step: [1890] d_loss: 0.76694608, g_loss: 0.82394028\n",
      "Step: [1891] d_loss: 0.73344254, g_loss: 0.90584618\n",
      "Step: [1892] d_loss: 0.70789361, g_loss: 0.97536385\n",
      "Step: [1893] d_loss: 0.78182876, g_loss: 0.98472089\n",
      "Step: [1894] d_loss: 0.72814971, g_loss: 0.90981644\n",
      "Step: [1895] d_loss: 0.69382727, g_loss: 0.87331438\n",
      "Step: [1896] d_loss: 0.89925140, g_loss: 0.82344800\n",
      "Step: [1897] d_loss: 0.69536644, g_loss: 0.90541404\n",
      "Step: [1898] d_loss: 0.70117033, g_loss: 0.93712968\n",
      "Step: [1899] d_loss: 0.75083339, g_loss: 0.94132400\n",
      "Step: [1900] d_loss: 0.82421875, g_loss: 0.97124296\n",
      "Step: [1901] d_loss: 0.74804384, g_loss: 0.95759577\n",
      "Step: [1902] d_loss: 0.70498228, g_loss: 0.96226835\n",
      "Step: [1903] d_loss: 0.69840932, g_loss: 0.85313934\n",
      "Step: [1904] d_loss: 0.74080861, g_loss: 0.91578573\n",
      "Step: [1905] d_loss: 0.79333383, g_loss: 0.87233227\n",
      "Step: [1906] d_loss: 0.71106595, g_loss: 0.94379151\n",
      "Step: [1907] d_loss: 0.73777419, g_loss: 0.94316733\n",
      "Step: [1908] d_loss: 0.82402980, g_loss: 0.94919759\n",
      "Step: [1909] d_loss: 0.80476922, g_loss: 0.89103925\n",
      "Step: [1910] d_loss: 0.69855863, g_loss: 0.92953038\n",
      "Step: [1911] d_loss: 0.77730018, g_loss: 0.97278869\n",
      "Step: [1912] d_loss: 0.77226484, g_loss: 0.99305290\n",
      "Step: [1913] d_loss: 0.75549728, g_loss: 0.95073473\n",
      "Step: [1914] d_loss: 0.77782369, g_loss: 0.92430961\n",
      "Step: [1915] d_loss: 0.71193111, g_loss: 0.95612288\n",
      "Step: [1916] d_loss: 0.78748459, g_loss: 0.93753314\n",
      "Step: [1917] d_loss: 0.76241481, g_loss: 0.89109671\n",
      "Step: [1918] d_loss: 0.72115600, g_loss: 1.00389171\n",
      "Step: [1919] d_loss: 0.70644820, g_loss: 0.95375603\n",
      "Step: [1920] d_loss: 0.72367424, g_loss: 0.91604012\n",
      "Step: [1921] d_loss: 0.66823161, g_loss: 0.99283284\n",
      "Step: [1922] d_loss: 0.71117902, g_loss: 0.96039474\n",
      "Step: [1923] d_loss: 0.73199606, g_loss: 0.89610076\n",
      "Step: [1924] d_loss: 0.77047992, g_loss: 0.88152063\n",
      "Step: [1925] d_loss: 0.72168356, g_loss: 0.89180523\n",
      "Step: [1926] d_loss: 0.74454582, g_loss: 0.90876389\n",
      "Step: [1927] d_loss: 0.72134340, g_loss: 0.91235751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1928] d_loss: 0.76104766, g_loss: 0.93191898\n",
      "Step: [1929] d_loss: 0.71395284, g_loss: 0.95334846\n",
      "Step: [1930] d_loss: 0.78836787, g_loss: 0.95221901\n",
      "Step: [1931] d_loss: 0.76649362, g_loss: 0.94689828\n",
      "Step: [1932] d_loss: 0.71223551, g_loss: 0.94196832\n",
      "Step: [1933] d_loss: 0.78612769, g_loss: 0.93631232\n",
      "Step: [1934] d_loss: 0.66463500, g_loss: 0.92389196\n",
      "Step: [1935] d_loss: 0.68307477, g_loss: 0.84286529\n",
      "Step: [1936] d_loss: 0.71452749, g_loss: 0.81939352\n",
      "Step: [1937] d_loss: 0.83913779, g_loss: 0.90053177\n",
      "Step: [1938] d_loss: 0.78261238, g_loss: 0.90519613\n",
      "Step: [1939] d_loss: 0.64601684, g_loss: 0.94677860\n",
      "Step: [1940] d_loss: 0.68901569, g_loss: 0.88393581\n",
      "Step: [1941] d_loss: 0.67191696, g_loss: 0.96526277\n",
      "Step: [1942] d_loss: 0.66656870, g_loss: 0.84337580\n",
      "Step: [1943] d_loss: 0.73704171, g_loss: 0.91437775\n",
      "Step: [1944] d_loss: 0.79884177, g_loss: 0.85950166\n",
      "Step: [1945] d_loss: 0.77899420, g_loss: 0.90651816\n",
      "Step: [1946] d_loss: 0.70478004, g_loss: 0.90794027\n",
      "Step: [1947] d_loss: 0.71041387, g_loss: 0.92303592\n",
      "Step: [1948] d_loss: 0.70023954, g_loss: 0.92303717\n",
      "Step: [1949] d_loss: 0.77375853, g_loss: 0.84490573\n",
      "Step: [1950] d_loss: 0.72181678, g_loss: 0.96245480\n",
      "Step: [1951] d_loss: 0.73079157, g_loss: 0.89180136\n",
      "Step: [1952] d_loss: 0.75187832, g_loss: 0.91836649\n",
      "Step: [1953] d_loss: 0.70357174, g_loss: 0.92633080\n",
      "Step: [1954] d_loss: 0.73028147, g_loss: 0.94232470\n",
      "Step: [1955] d_loss: 0.70553446, g_loss: 0.92499048\n",
      "Step: [1956] d_loss: 0.75101745, g_loss: 0.96711099\n",
      "Step: [1957] d_loss: 0.70166343, g_loss: 0.87091827\n",
      "Step: [1958] d_loss: 0.64293867, g_loss: 0.90755451\n",
      "Step: [1959] d_loss: 0.73970330, g_loss: 0.89003301\n",
      "Step: [1960] d_loss: 0.73814273, g_loss: 0.86245573\n",
      "Step: [1961] d_loss: 0.85105902, g_loss: 0.79008222\n",
      "Step: [1962] d_loss: 0.81542391, g_loss: 0.82667100\n",
      "Step: [1963] d_loss: 0.69503868, g_loss: 0.87886637\n",
      "Step: [1964] d_loss: 0.85522497, g_loss: 0.92266220\n",
      "Step: [1965] d_loss: 0.64433092, g_loss: 0.84854430\n",
      "Step: [1966] d_loss: 0.86379582, g_loss: 0.91851115\n",
      "Step: [1967] d_loss: 0.69286978, g_loss: 0.90761787\n",
      "Step: [1968] d_loss: 0.71315753, g_loss: 0.92530417\n",
      "Step: [1969] d_loss: 0.73462081, g_loss: 0.96949542\n",
      "Step: [1970] d_loss: 0.74585944, g_loss: 0.93395537\n",
      "Step: [1971] d_loss: 0.77249378, g_loss: 0.99478668\n",
      "Step: [1972] d_loss: 0.75497037, g_loss: 0.92099494\n",
      "Step: [1973] d_loss: 0.75409138, g_loss: 0.90540946\n",
      "Step: [1974] d_loss: 0.69154459, g_loss: 0.97985780\n",
      "Step: [1975] d_loss: 0.64476454, g_loss: 0.93027025\n",
      "Step: [1976] d_loss: 0.66498619, g_loss: 0.90313238\n",
      "Step: [1977] d_loss: 0.70792758, g_loss: 0.93606669\n",
      "Step: [1978] d_loss: 0.70699191, g_loss: 0.92631125\n",
      "Step: [1979] d_loss: 0.71912313, g_loss: 0.96035087\n",
      "Step: [1980] d_loss: 0.73389840, g_loss: 0.93760812\n",
      "Step: [1981] d_loss: 0.76491582, g_loss: 0.93425572\n",
      "Step: [1982] d_loss: 0.66998643, g_loss: 0.94028038\n",
      "Step: [1983] d_loss: 0.75935733, g_loss: 0.94149774\n",
      "Step: [1984] d_loss: 0.71020460, g_loss: 0.95508391\n",
      "Step: [1985] d_loss: 0.66781336, g_loss: 0.90205055\n",
      "Step: [1986] d_loss: 0.77481645, g_loss: 0.88414383\n",
      "Step: [1987] d_loss: 0.67461383, g_loss: 0.85975063\n",
      "Step: [1988] d_loss: 0.64051825, g_loss: 0.94577694\n",
      "Step: [1989] d_loss: 0.80985266, g_loss: 0.87435788\n",
      "Step: [1990] d_loss: 0.64691645, g_loss: 0.96207416\n",
      "Step: [1991] d_loss: 0.63601333, g_loss: 0.96006739\n",
      "Step: [1992] d_loss: 0.68314040, g_loss: 0.87795019\n",
      "Step: [1993] d_loss: 0.60876691, g_loss: 0.95149899\n",
      "Step: [1994] d_loss: 0.69832611, g_loss: 0.95550776\n",
      "Step: [1995] d_loss: 0.72867799, g_loss: 0.92763054\n",
      "Step: [1996] d_loss: 0.74948621, g_loss: 0.94149590\n",
      "Step: [1997] d_loss: 0.73637372, g_loss: 0.91110939\n",
      "Step: [1998] d_loss: 0.72107661, g_loss: 0.96935636\n",
      "Step: [1999] d_loss: 0.72936577, g_loss: 0.99528855\n",
      "Step: [2000] d_loss: 0.76564837, g_loss: 0.96073967\n",
      "Step: [2001] d_loss: 0.73473030, g_loss: 0.97732729\n",
      "Step: [2002] d_loss: 0.73060423, g_loss: 0.93869227\n",
      "Step: [2003] d_loss: 0.74048048, g_loss: 1.03010321\n",
      "Step: [2004] d_loss: 0.67465752, g_loss: 0.90743828\n",
      "Step: [2005] d_loss: 0.69493568, g_loss: 0.91501439\n",
      "Step: [2006] d_loss: 0.71876860, g_loss: 0.87994200\n",
      "Step: [2007] d_loss: 0.72523451, g_loss: 0.96087235\n",
      "Step: [2008] d_loss: 0.68991172, g_loss: 0.96993029\n",
      "Step: [2009] d_loss: 0.66272503, g_loss: 0.93740082\n",
      "Step: [2010] d_loss: 0.77907997, g_loss: 0.91618949\n",
      "Step: [2011] d_loss: 0.76153094, g_loss: 0.95430148\n",
      "Step: [2012] d_loss: 0.73488891, g_loss: 0.94803411\n",
      "Step: [2013] d_loss: 0.73036754, g_loss: 0.86837548\n",
      "Step: [2014] d_loss: 0.76264805, g_loss: 0.88787550\n",
      "Step: [2015] d_loss: 0.66682738, g_loss: 0.94684428\n",
      "Step: [2016] d_loss: 0.73061723, g_loss: 0.95601785\n",
      "Step: [2017] d_loss: 0.73069412, g_loss: 0.96811002\n",
      "Step: [2018] d_loss: 0.70673060, g_loss: 1.02003634\n",
      "Step: [2019] d_loss: 0.70468330, g_loss: 0.85881698\n",
      "Step: [2020] d_loss: 0.66889936, g_loss: 0.87958276\n",
      "Step: [2021] d_loss: 0.80423677, g_loss: 0.85242945\n",
      "Step: [2022] d_loss: 0.64489514, g_loss: 0.91357613\n",
      "Step: [2023] d_loss: 0.77791607, g_loss: 0.83652437\n",
      "Step: [2024] d_loss: 0.70982921, g_loss: 0.93262166\n",
      "Step: [2025] d_loss: 0.75020146, g_loss: 0.91078198\n",
      "Step: [2026] d_loss: 0.70545787, g_loss: 1.01079023\n",
      "Step: [2027] d_loss: 0.67819554, g_loss: 0.92988110\n",
      "Step: [2028] d_loss: 0.67218554, g_loss: 0.97788358\n",
      "Step: [2029] d_loss: 0.74319714, g_loss: 0.96515673\n",
      "Step: [2030] d_loss: 0.70303202, g_loss: 0.88884020\n",
      "Step: [2031] d_loss: 0.74470013, g_loss: 0.88467032\n",
      "Step: [2032] d_loss: 0.75983721, g_loss: 0.89963573\n",
      "Step: [2033] d_loss: 0.68350679, g_loss: 0.96059811\n",
      "Step: [2034] d_loss: 0.70970136, g_loss: 0.92088771\n",
      "Step: [2035] d_loss: 0.73846078, g_loss: 0.88029057\n",
      "Step: [2036] d_loss: 0.68929762, g_loss: 0.85112768\n",
      "Step: [2037] d_loss: 0.75229162, g_loss: 0.87844974\n",
      "Step: [2038] d_loss: 0.67652130, g_loss: 0.97647190\n",
      "Step: [2039] d_loss: 0.74721003, g_loss: 0.92479116\n",
      "Step: [2040] d_loss: 0.77443886, g_loss: 0.96125185\n",
      "Step: [2041] d_loss: 0.73703682, g_loss: 0.96236843\n",
      "Step: [2042] d_loss: 0.76416183, g_loss: 0.87281084\n",
      "Step: [2043] d_loss: 0.72865546, g_loss: 0.87193239\n",
      "Step: [2044] d_loss: 0.76627833, g_loss: 0.85703027\n",
      "Step: [2045] d_loss: 0.64182109, g_loss: 0.89207339\n",
      "Step: [2046] d_loss: 0.80479592, g_loss: 0.83241886\n",
      "Step: [2047] d_loss: 0.67234218, g_loss: 0.93600845\n",
      "Step: [2048] d_loss: 0.63544923, g_loss: 0.89954257\n",
      "Step: [2049] d_loss: 0.70287544, g_loss: 0.89799857\n",
      "Step: [2050] d_loss: 0.87934256, g_loss: 0.85503602\n",
      "Step: [2051] d_loss: 0.69584113, g_loss: 0.95334941\n",
      "Step: [2052] d_loss: 0.68059897, g_loss: 0.91166407\n",
      "Step: [2053] d_loss: 0.67057788, g_loss: 0.98395234\n",
      "Step: [2054] d_loss: 0.69398707, g_loss: 0.95514679\n",
      "Step: [2055] d_loss: 0.72532648, g_loss: 0.88806391\n",
      "Step: [2056] d_loss: 0.70292342, g_loss: 0.84966016\n",
      "Step: [2057] d_loss: 0.71458280, g_loss: 0.88708234\n",
      "Step: [2058] d_loss: 0.80975151, g_loss: 0.84521437\n",
      "Step: [2059] d_loss: 0.73889524, g_loss: 0.88169116\n",
      "Step: [2060] d_loss: 0.76650989, g_loss: 0.90035695\n",
      "Step: [2061] d_loss: 0.80313313, g_loss: 0.92883813\n",
      "Step: [2062] d_loss: 0.85656774, g_loss: 0.90521097\n",
      "Step: [2063] d_loss: 0.66269243, g_loss: 0.98906648\n",
      "Step: [2064] d_loss: 0.70746517, g_loss: 0.95027262\n",
      "Step: [2065] d_loss: 0.70603019, g_loss: 0.93686175\n",
      "Step: [2066] d_loss: 0.79431117, g_loss: 0.92647016\n",
      "Step: [2067] d_loss: 0.70399320, g_loss: 0.84576666\n",
      "Step: [2068] d_loss: 0.69305027, g_loss: 0.85107374\n",
      "Step: [2069] d_loss: 0.82821387, g_loss: 0.86440390\n",
      "Step: [2070] d_loss: 0.66129071, g_loss: 0.92238021\n",
      "Step: [2071] d_loss: 0.78796762, g_loss: 0.96201158\n",
      "Step: [2072] d_loss: 0.77600861, g_loss: 0.94763774\n",
      "Step: [2073] d_loss: 0.77962679, g_loss: 0.95497257\n",
      "Step: [2074] d_loss: 0.84050226, g_loss: 0.91675717\n",
      "Step: [2075] d_loss: 0.77332580, g_loss: 0.91078693\n",
      "Step: [2076] d_loss: 0.73761737, g_loss: 0.92916346\n",
      "Step: [2077] d_loss: 0.75533271, g_loss: 0.93330431\n",
      "Step: [2078] d_loss: 0.79022175, g_loss: 0.95539463\n",
      "Step: [2079] d_loss: 0.75702626, g_loss: 0.90177888\n",
      "Step: [2080] d_loss: 0.73330325, g_loss: 0.91868311\n",
      "Step: [2081] d_loss: 0.70740098, g_loss: 0.96482193\n",
      "Step: [2082] d_loss: 0.69640726, g_loss: 0.96430826\n",
      "Step: [2083] d_loss: 0.73077226, g_loss: 0.96992439\n",
      "Step: [2084] d_loss: 0.70040077, g_loss: 0.89448160\n",
      "Step: [2085] d_loss: 0.68057191, g_loss: 0.90565115\n",
      "Step: [2086] d_loss: 0.78177321, g_loss: 0.84441596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2087] d_loss: 0.85154831, g_loss: 0.84163737\n",
      "Step: [2088] d_loss: 0.82345885, g_loss: 0.91510141\n",
      "Step: [2089] d_loss: 0.87463188, g_loss: 0.92492306\n",
      "Step: [2090] d_loss: 0.75123888, g_loss: 0.93285394\n",
      "Step: [2091] d_loss: 0.72542053, g_loss: 0.95075786\n",
      "Step: [2092] d_loss: 0.81545991, g_loss: 0.95884931\n",
      "Step: [2093] d_loss: 0.71923465, g_loss: 0.95845056\n",
      "Step: [2094] d_loss: 0.74560559, g_loss: 0.96040434\n",
      "Step: [2095] d_loss: 0.77196765, g_loss: 0.99458408\n",
      "Step: [2096] d_loss: 0.76569432, g_loss: 0.98392248\n",
      "Step: [2097] d_loss: 0.71602821, g_loss: 0.95970160\n",
      "Step: [2098] d_loss: 0.82898265, g_loss: 0.90136224\n",
      "Step: [2099] d_loss: 0.77525568, g_loss: 0.95435774\n",
      "Step: [2100] d_loss: 0.69805437, g_loss: 0.97366953\n",
      "Step: [2101] d_loss: 0.72942102, g_loss: 0.95436114\n",
      "Step: [2102] d_loss: 0.77207685, g_loss: 0.97935045\n",
      "Step: [2103] d_loss: 0.65749192, g_loss: 0.97072470\n",
      "Step: [2104] d_loss: 0.71423554, g_loss: 0.94000530\n",
      "Step: [2105] d_loss: 0.74186039, g_loss: 0.95709807\n",
      "Step: [2106] d_loss: 0.68283254, g_loss: 1.02004349\n",
      "Step: [2107] d_loss: 0.80974859, g_loss: 0.94085824\n",
      "Step: [2108] d_loss: 0.85116822, g_loss: 0.88324022\n",
      "Step: [2109] d_loss: 0.81282383, g_loss: 0.84134144\n",
      "Step: [2110] d_loss: 0.76449877, g_loss: 0.92360950\n",
      "Step: [2111] d_loss: 0.72192055, g_loss: 0.94942951\n",
      "Step: [2112] d_loss: 0.67042333, g_loss: 0.91560936\n",
      "Step: [2113] d_loss: 0.72675318, g_loss: 0.93215358\n",
      "Step: [2114] d_loss: 0.73374659, g_loss: 0.92019325\n",
      "Step: [2115] d_loss: 0.70015943, g_loss: 0.87028855\n",
      "Step: [2116] d_loss: 0.82287580, g_loss: 0.86506081\n",
      "Step: [2117] d_loss: 0.80608994, g_loss: 0.85776252\n",
      "Step: [2118] d_loss: 0.81168032, g_loss: 0.87452018\n",
      "Step: [2119] d_loss: 0.68746597, g_loss: 0.93357849\n",
      "Step: [2120] d_loss: 0.73800170, g_loss: 0.91405177\n",
      "Step: [2121] d_loss: 0.79016113, g_loss: 0.91914260\n",
      "Step: [2122] d_loss: 0.72208029, g_loss: 0.95272714\n",
      "Step: [2123] d_loss: 0.73260885, g_loss: 0.88231397\n",
      "Step: [2124] d_loss: 0.64918184, g_loss: 0.91385996\n",
      "Step: [2125] d_loss: 0.68232483, g_loss: 0.88138491\n",
      "Step: [2126] d_loss: 0.76711601, g_loss: 0.91560531\n",
      "Step: [2127] d_loss: 0.70086312, g_loss: 0.90404516\n",
      "Step: [2128] d_loss: 0.81010830, g_loss: 0.87054086\n",
      "Step: [2129] d_loss: 0.71895576, g_loss: 0.89130616\n",
      "Step: [2130] d_loss: 0.73918986, g_loss: 0.92908692\n",
      "Step: [2131] d_loss: 0.76710218, g_loss: 0.90850157\n",
      "Step: [2132] d_loss: 0.68296880, g_loss: 0.90756118\n",
      "Step: [2133] d_loss: 0.73197877, g_loss: 0.90695292\n",
      "Step: [2134] d_loss: 0.72954506, g_loss: 0.88212466\n",
      "Step: [2135] d_loss: 0.74501526, g_loss: 0.94841564\n",
      "Step: [2136] d_loss: 0.68717319, g_loss: 0.88994509\n",
      "Step: [2137] d_loss: 0.67656398, g_loss: 0.88885516\n",
      "Step: [2138] d_loss: 0.72303414, g_loss: 0.90911472\n",
      "Step: [2139] d_loss: 0.77553219, g_loss: 0.90404236\n",
      "Step: [2140] d_loss: 0.74426341, g_loss: 0.88848394\n",
      "Step: [2141] d_loss: 0.75103801, g_loss: 0.90501642\n",
      "Step: [2142] d_loss: 0.72492337, g_loss: 1.00231171\n",
      "Step: [2143] d_loss: 0.77198714, g_loss: 0.99131870\n",
      "Step: [2144] d_loss: 0.80240726, g_loss: 0.97898978\n",
      "Step: [2145] d_loss: 0.77722692, g_loss: 0.93782991\n",
      "Step: [2146] d_loss: 0.78017610, g_loss: 0.90574038\n",
      "Step: [2147] d_loss: 0.76281500, g_loss: 0.97437036\n",
      "Step: [2148] d_loss: 0.76249707, g_loss: 0.89392239\n",
      "Step: [2149] d_loss: 0.78667235, g_loss: 0.89404452\n",
      "Step: [2150] d_loss: 0.72551513, g_loss: 0.88765299\n",
      "Step: [2151] d_loss: 0.71056354, g_loss: 0.91108137\n",
      "Step: [2152] d_loss: 0.78537625, g_loss: 0.89047009\n",
      "Step: [2153] d_loss: 0.75042897, g_loss: 0.90982729\n",
      "Step: [2154] d_loss: 0.93335801, g_loss: 0.91076410\n",
      "Step: [2155] d_loss: 0.74324191, g_loss: 0.91500115\n",
      "Step: [2156] d_loss: 0.82245642, g_loss: 0.91999626\n",
      "Step: [2157] d_loss: 0.71067798, g_loss: 0.99425519\n",
      "Step: [2158] d_loss: 0.66221857, g_loss: 0.91464436\n",
      "Step: [2159] d_loss: 0.71336806, g_loss: 0.93675840\n",
      "Step: [2160] d_loss: 0.81738746, g_loss: 0.92445707\n",
      "Step: [2161] d_loss: 0.88178563, g_loss: 0.86566985\n",
      "Step: [2162] d_loss: 0.74797499, g_loss: 0.95761609\n",
      "Step: [2163] d_loss: 0.73500443, g_loss: 0.91449678\n",
      "Step: [2164] d_loss: 0.82202548, g_loss: 0.89345157\n",
      "Step: [2165] d_loss: 0.70189184, g_loss: 0.91913933\n",
      "Step: [2166] d_loss: 0.65507662, g_loss: 0.85415864\n",
      "Step: [2167] d_loss: 0.73442078, g_loss: 0.86184549\n",
      "Step: [2168] d_loss: 0.76722169, g_loss: 0.83511901\n",
      "Step: [2169] d_loss: 0.80395222, g_loss: 0.85573626\n",
      "Step: [2170] d_loss: 0.71661717, g_loss: 0.87155288\n",
      "Step: [2171] d_loss: 0.74744642, g_loss: 0.92363280\n",
      "Step: [2172] d_loss: 0.72801131, g_loss: 0.92125332\n",
      "Step: [2173] d_loss: 0.79091257, g_loss: 0.88681579\n",
      "Step: [2174] d_loss: 0.67084688, g_loss: 0.97032052\n",
      "Step: [2175] d_loss: 0.72113496, g_loss: 0.98254919\n",
      "Step: [2176] d_loss: 0.78862375, g_loss: 0.92868537\n",
      "Step: [2177] d_loss: 0.69066852, g_loss: 0.97171021\n",
      "Step: [2178] d_loss: 0.72305930, g_loss: 0.91732502\n",
      "Step: [2179] d_loss: 0.70253164, g_loss: 0.90190446\n",
      "Step: [2180] d_loss: 0.81426108, g_loss: 0.84796625\n",
      "Step: [2181] d_loss: 0.78479379, g_loss: 0.86964661\n",
      "Step: [2182] d_loss: 0.82078350, g_loss: 0.90725541\n",
      "Step: [2183] d_loss: 0.78886801, g_loss: 0.93009996\n",
      "Step: [2184] d_loss: 0.76753652, g_loss: 0.94159085\n",
      "Step: [2185] d_loss: 0.82752860, g_loss: 0.90941364\n",
      "Step: [2186] d_loss: 0.68754780, g_loss: 0.92829162\n",
      "Step: [2187] d_loss: 0.69231260, g_loss: 0.92311651\n",
      "Step: [2188] d_loss: 0.75127751, g_loss: 0.92986023\n",
      "Step: [2189] d_loss: 0.69625312, g_loss: 0.90353519\n",
      "Step: [2190] d_loss: 0.63068062, g_loss: 0.91998231\n",
      "Step: [2191] d_loss: 0.74307817, g_loss: 0.92686450\n",
      "Step: [2192] d_loss: 0.73046583, g_loss: 0.89351898\n",
      "Step: [2193] d_loss: 0.83382839, g_loss: 0.85597068\n",
      "Step: [2194] d_loss: 0.76617593, g_loss: 0.90147686\n",
      "Step: [2195] d_loss: 0.77759057, g_loss: 0.91452390\n",
      "Step: [2196] d_loss: 0.65635401, g_loss: 0.86894929\n",
      "Step: [2197] d_loss: 0.70229656, g_loss: 0.90099692\n",
      "Step: [2198] d_loss: 0.78164452, g_loss: 0.90330869\n",
      "Step: [2199] d_loss: 0.71138674, g_loss: 0.90112692\n",
      "Step: [2200] d_loss: 0.69975507, g_loss: 0.93054473\n",
      "Step: [2201] d_loss: 0.84771842, g_loss: 0.87088251\n",
      "Step: [2202] d_loss: 0.80518186, g_loss: 0.93949622\n",
      "Step: [2203] d_loss: 0.74897557, g_loss: 0.92798245\n",
      "Step: [2204] d_loss: 0.67424113, g_loss: 0.89517385\n",
      "Step: [2205] d_loss: 0.72380662, g_loss: 0.94352382\n",
      "Step: [2206] d_loss: 0.77508473, g_loss: 0.92212844\n",
      "Step: [2207] d_loss: 0.75832802, g_loss: 0.88286406\n",
      "Step: [2208] d_loss: 0.80631870, g_loss: 0.92856598\n",
      "Step: [2209] d_loss: 0.67945081, g_loss: 0.98175925\n",
      "Step: [2210] d_loss: 0.65315962, g_loss: 0.87916213\n",
      "Step: [2211] d_loss: 0.74154121, g_loss: 0.89631045\n",
      "Step: [2212] d_loss: 0.77199513, g_loss: 0.94206613\n",
      "Step: [2213] d_loss: 0.80858976, g_loss: 0.95766091\n",
      "Step: [2214] d_loss: 0.79232401, g_loss: 0.96424788\n",
      "Step: [2215] d_loss: 0.74040461, g_loss: 0.89155096\n",
      "Step: [2216] d_loss: 0.77708972, g_loss: 0.90166581\n",
      "Step: [2217] d_loss: 0.74320304, g_loss: 0.92819375\n",
      "Step: [2218] d_loss: 0.73708695, g_loss: 0.94662142\n",
      "Step: [2219] d_loss: 0.70705146, g_loss: 0.86725491\n",
      "Step: [2220] d_loss: 0.74659330, g_loss: 0.91330945\n",
      "Step: [2221] d_loss: 0.67298889, g_loss: 0.93846262\n",
      "Step: [2222] d_loss: 0.69868678, g_loss: 0.88618833\n",
      "Step: [2223] d_loss: 0.82115102, g_loss: 0.92636007\n",
      "Step: [2224] d_loss: 0.76671481, g_loss: 0.94695240\n",
      "Step: [2225] d_loss: 0.68833572, g_loss: 0.91050363\n",
      "Step: [2226] d_loss: 0.65602887, g_loss: 0.91863263\n",
      "Step: [2227] d_loss: 0.74529099, g_loss: 0.86786550\n",
      "Step: [2228] d_loss: 0.85130763, g_loss: 0.91795385\n",
      "Step: [2229] d_loss: 0.72584224, g_loss: 0.91983706\n",
      "Step: [2230] d_loss: 0.74828780, g_loss: 0.90592974\n",
      "Step: [2231] d_loss: 0.67505383, g_loss: 0.92397892\n",
      "Step: [2232] d_loss: 0.80236584, g_loss: 0.88764101\n",
      "Step: [2233] d_loss: 0.78714842, g_loss: 0.91265428\n",
      "Step: [2234] d_loss: 0.77823722, g_loss: 0.92411649\n",
      "Step: [2235] d_loss: 0.72571510, g_loss: 0.94140440\n",
      "Step: [2236] d_loss: 0.75112277, g_loss: 0.92345881\n",
      "Step: [2237] d_loss: 0.71845114, g_loss: 0.86953038\n",
      "Step: [2238] d_loss: 0.71586561, g_loss: 0.91078842\n",
      "Step: [2239] d_loss: 0.81573319, g_loss: 0.87986612\n",
      "Step: [2240] d_loss: 0.74559915, g_loss: 0.89457524\n",
      "Step: [2241] d_loss: 0.75171393, g_loss: 0.91952360\n",
      "Step: [2242] d_loss: 0.74514496, g_loss: 0.98953068\n",
      "Step: [2243] d_loss: 0.78206599, g_loss: 0.97591180\n",
      "Step: [2244] d_loss: 0.74826729, g_loss: 0.93009961\n",
      "Step: [2245] d_loss: 0.74756712, g_loss: 0.92456734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2246] d_loss: 0.70073080, g_loss: 0.92529392\n",
      "Step: [2247] d_loss: 0.74441302, g_loss: 0.91448283\n",
      "Step: [2248] d_loss: 0.69637752, g_loss: 0.84274191\n",
      "Step: [2249] d_loss: 0.78532749, g_loss: 0.83267897\n",
      "Step: [2250] d_loss: 0.78153652, g_loss: 0.87196368\n",
      "Step: [2251] d_loss: 0.68101579, g_loss: 0.97092819\n",
      "Step: [2252] d_loss: 0.80254489, g_loss: 0.93378317\n",
      "Step: [2253] d_loss: 0.82015055, g_loss: 0.91793931\n",
      "Step: [2254] d_loss: 0.78884536, g_loss: 0.88288146\n",
      "Step: [2255] d_loss: 0.77522814, g_loss: 0.87214667\n",
      "Step: [2256] d_loss: 0.70118594, g_loss: 0.87328660\n",
      "Step: [2257] d_loss: 0.67431879, g_loss: 0.93259752\n",
      "Step: [2258] d_loss: 0.68828660, g_loss: 0.89513814\n",
      "Step: [2259] d_loss: 0.75699234, g_loss: 0.90367055\n",
      "Step: [2260] d_loss: 0.77514142, g_loss: 0.88686466\n",
      "Step: [2261] d_loss: 0.79662287, g_loss: 0.91431016\n",
      "Step: [2262] d_loss: 0.67755556, g_loss: 0.95214909\n",
      "Step: [2263] d_loss: 0.71515232, g_loss: 0.97167152\n",
      "Step: [2264] d_loss: 0.82113475, g_loss: 0.94575244\n",
      "Step: [2265] d_loss: 0.76734191, g_loss: 0.94823217\n",
      "Step: [2266] d_loss: 0.72374600, g_loss: 0.91996157\n",
      "Step: [2267] d_loss: 0.69852924, g_loss: 0.92045653\n",
      "Step: [2268] d_loss: 0.69951576, g_loss: 0.97876871\n",
      "Step: [2269] d_loss: 0.68363059, g_loss: 0.95481431\n",
      "Step: [2270] d_loss: 0.77202493, g_loss: 0.99409741\n",
      "Step: [2271] d_loss: 0.75264925, g_loss: 0.91764617\n",
      "Step: [2272] d_loss: 0.70219642, g_loss: 0.93294042\n",
      "Step: [2273] d_loss: 0.80152750, g_loss: 0.92342687\n",
      "Step: [2274] d_loss: 0.73522741, g_loss: 0.90595329\n",
      "Step: [2275] d_loss: 0.73276210, g_loss: 0.97929287\n",
      "Step: [2276] d_loss: 0.74410897, g_loss: 0.97393489\n",
      "Step: [2277] d_loss: 0.75033629, g_loss: 0.89930224\n",
      "Step: [2278] d_loss: 0.65996951, g_loss: 0.90055996\n",
      "Step: [2279] d_loss: 0.65549332, g_loss: 0.91964042\n",
      "Step: [2280] d_loss: 0.74659699, g_loss: 0.94104236\n",
      "Step: [2281] d_loss: 0.71750849, g_loss: 0.89809591\n",
      "Step: [2282] d_loss: 0.74678051, g_loss: 0.89540911\n",
      "Step: [2283] d_loss: 0.81438231, g_loss: 0.83792752\n",
      "Step: [2284] d_loss: 0.77931678, g_loss: 0.88565898\n",
      "Step: [2285] d_loss: 0.73254424, g_loss: 0.92027402\n",
      "Step: [2286] d_loss: 0.86633641, g_loss: 0.94668400\n",
      "Step: [2287] d_loss: 0.81180108, g_loss: 0.89562482\n",
      "Step: [2288] d_loss: 0.80055285, g_loss: 0.93866134\n",
      "Step: [2289] d_loss: 0.76873833, g_loss: 0.98948020\n",
      "Step: [2290] d_loss: 0.82110673, g_loss: 0.93330306\n",
      "Step: [2291] d_loss: 0.74705172, g_loss: 0.89436108\n",
      "Step: [2292] d_loss: 0.76660013, g_loss: 0.89941967\n",
      "Step: [2293] d_loss: 0.75551450, g_loss: 0.90619862\n",
      "Step: [2294] d_loss: 0.71445954, g_loss: 0.89946580\n",
      "Step: [2295] d_loss: 0.74982166, g_loss: 0.92277026\n",
      "Step: [2296] d_loss: 0.74095428, g_loss: 0.92227662\n",
      "Step: [2297] d_loss: 0.74365550, g_loss: 0.93125087\n",
      "Step: [2298] d_loss: 0.70094246, g_loss: 0.92047548\n",
      "Step: [2299] d_loss: 0.78867900, g_loss: 0.90397769\n",
      "Step: [2300] d_loss: 0.64735651, g_loss: 0.88247681\n",
      "Step: [2301] d_loss: 0.68330622, g_loss: 0.88003868\n",
      "Step: [2302] d_loss: 0.72429776, g_loss: 0.94732016\n",
      "Step: [2303] d_loss: 0.70420891, g_loss: 0.92805928\n",
      "Step: [2304] d_loss: 0.72962999, g_loss: 0.92322719\n",
      "Step: [2305] d_loss: 0.78821796, g_loss: 0.96652055\n",
      "Step: [2306] d_loss: 0.80652630, g_loss: 0.92793715\n",
      "Step: [2307] d_loss: 0.69065958, g_loss: 0.96424419\n",
      "Step: [2308] d_loss: 0.79416770, g_loss: 0.90050977\n",
      "Step: [2309] d_loss: 0.72738832, g_loss: 0.89298743\n",
      "Step: [2310] d_loss: 0.71882993, g_loss: 0.84205717\n",
      "Step: [2311] d_loss: 0.67295700, g_loss: 0.90709358\n",
      "Step: [2312] d_loss: 0.74248624, g_loss: 0.99100673\n",
      "Step: [2313] d_loss: 0.68568420, g_loss: 0.90855259\n",
      "Step: [2314] d_loss: 0.69402850, g_loss: 0.92572653\n",
      "Step: [2315] d_loss: 0.75864220, g_loss: 0.90971589\n",
      "Step: [2316] d_loss: 0.73894620, g_loss: 0.93839663\n",
      "Step: [2317] d_loss: 0.69791937, g_loss: 0.94003898\n",
      "Step: [2318] d_loss: 0.73099172, g_loss: 0.93379092\n",
      "Step: [2319] d_loss: 0.70211917, g_loss: 0.95757937\n",
      "Step: [2320] d_loss: 0.69677508, g_loss: 0.88988030\n",
      "Step: [2321] d_loss: 0.68462044, g_loss: 0.93006617\n",
      "Step: [2322] d_loss: 0.78909957, g_loss: 0.92797756\n",
      "Step: [2323] d_loss: 0.69692314, g_loss: 0.96952534\n",
      "Step: [2324] d_loss: 0.80175471, g_loss: 0.99975806\n",
      "Step: [2325] d_loss: 0.70512062, g_loss: 0.98156202\n",
      "Step: [2326] d_loss: 0.77024537, g_loss: 0.88423282\n",
      "Step: [2327] d_loss: 0.79356748, g_loss: 0.88652992\n",
      "Step: [2328] d_loss: 0.66096300, g_loss: 0.87406778\n",
      "Step: [2329] d_loss: 0.81062728, g_loss: 0.86266291\n",
      "Step: [2330] d_loss: 0.81075424, g_loss: 0.87494320\n",
      "Step: [2331] d_loss: 0.73489153, g_loss: 0.88304144\n",
      "Step: [2332] d_loss: 0.85175759, g_loss: 0.88559932\n",
      "Step: [2333] d_loss: 0.82582086, g_loss: 0.89128369\n",
      "Step: [2334] d_loss: 0.69860137, g_loss: 0.90676844\n",
      "Step: [2335] d_loss: 0.69874012, g_loss: 0.90191865\n",
      "Step: [2336] d_loss: 0.68732774, g_loss: 0.90662044\n",
      "Step: [2337] d_loss: 0.76660258, g_loss: 0.91092622\n",
      "Step: [2338] d_loss: 0.79135466, g_loss: 0.88076752\n",
      "Step: [2339] d_loss: 0.69454867, g_loss: 0.93382967\n",
      "Step: [2340] d_loss: 0.66359615, g_loss: 0.93158329\n",
      "Step: [2341] d_loss: 0.65702093, g_loss: 0.93877918\n",
      "Step: [2342] d_loss: 0.70420277, g_loss: 0.92375147\n",
      "Step: [2343] d_loss: 0.80837131, g_loss: 0.84970868\n",
      "Step: [2344] d_loss: 0.67241114, g_loss: 0.93406349\n",
      "Step: [2345] d_loss: 0.73383850, g_loss: 0.93939424\n",
      "Step: [2346] d_loss: 0.68508190, g_loss: 0.93795300\n",
      "Step: [2347] d_loss: 0.74890596, g_loss: 0.91193736\n",
      "Step: [2348] d_loss: 0.73910958, g_loss: 0.91368461\n",
      "Step: [2349] d_loss: 0.73739433, g_loss: 0.90375233\n",
      "Step: [2350] d_loss: 0.81578726, g_loss: 0.88612533\n",
      "Step: [2351] d_loss: 0.75818723, g_loss: 0.88339901\n",
      "Step: [2352] d_loss: 0.77177811, g_loss: 0.94328070\n",
      "Step: [2353] d_loss: 0.80845326, g_loss: 0.93677777\n",
      "Step: [2354] d_loss: 0.72563982, g_loss: 1.02288091\n",
      "Step: [2355] d_loss: 0.73458433, g_loss: 0.97942847\n",
      "Step: [2356] d_loss: 0.71834546, g_loss: 0.94413340\n",
      "Step: [2357] d_loss: 0.74228650, g_loss: 0.97736263\n",
      "Step: [2358] d_loss: 0.70856023, g_loss: 0.96712017\n",
      "Step: [2359] d_loss: 0.75989443, g_loss: 1.01768124\n",
      "Step: [2360] d_loss: 0.71354681, g_loss: 0.97258633\n",
      "Step: [2361] d_loss: 0.82168931, g_loss: 0.94718778\n",
      "Step: [2362] d_loss: 0.74969482, g_loss: 0.97785622\n",
      "Step: [2363] d_loss: 0.74751854, g_loss: 1.01334441\n",
      "Step: [2364] d_loss: 0.77112967, g_loss: 0.95451385\n",
      "Step: [2365] d_loss: 0.81620228, g_loss: 0.96568221\n",
      "Step: [2366] d_loss: 0.76478815, g_loss: 0.95053589\n",
      "Step: [2367] d_loss: 0.77562952, g_loss: 0.93918222\n",
      "Step: [2368] d_loss: 0.69086027, g_loss: 0.94645566\n",
      "Step: [2369] d_loss: 0.71277493, g_loss: 0.96488708\n",
      "Step: [2370] d_loss: 0.75741303, g_loss: 0.95413435\n",
      "Step: [2371] d_loss: 0.76920700, g_loss: 0.91790563\n",
      "Step: [2372] d_loss: 0.74004072, g_loss: 0.92233562\n",
      "Step: [2373] d_loss: 0.70949650, g_loss: 0.87637788\n",
      "Step: [2374] d_loss: 0.63747752, g_loss: 0.97479367\n",
      "Step: [2375] d_loss: 0.77776176, g_loss: 0.96610677\n",
      "Step: [2376] d_loss: 0.75997341, g_loss: 0.97717816\n",
      "Step: [2377] d_loss: 0.86823958, g_loss: 0.90661728\n",
      "Step: [2378] d_loss: 0.71869051, g_loss: 0.97986686\n",
      "Step: [2379] d_loss: 0.76860136, g_loss: 0.88663882\n",
      "Step: [2380] d_loss: 0.71030134, g_loss: 0.93147391\n",
      "Step: [2381] d_loss: 0.76219660, g_loss: 0.85450619\n",
      "Step: [2382] d_loss: 0.71254998, g_loss: 0.94310379\n",
      "Step: [2383] d_loss: 0.75709397, g_loss: 0.86626053\n",
      "Step: [2384] d_loss: 0.81589794, g_loss: 0.87304819\n",
      "Step: [2385] d_loss: 0.71371293, g_loss: 0.95768410\n",
      "Step: [2386] d_loss: 0.74936402, g_loss: 0.92786956\n",
      "Step: [2387] d_loss: 0.71041948, g_loss: 0.94584161\n",
      "Step: [2388] d_loss: 0.75559258, g_loss: 0.94934011\n",
      "Step: [2389] d_loss: 0.66949069, g_loss: 0.92040098\n",
      "Step: [2390] d_loss: 0.67969453, g_loss: 0.89332104\n",
      "Step: [2391] d_loss: 0.73597193, g_loss: 0.93237436\n",
      "Step: [2392] d_loss: 0.72010833, g_loss: 0.92431539\n",
      "Step: [2393] d_loss: 0.75906515, g_loss: 0.92255682\n",
      "Step: [2394] d_loss: 0.72694188, g_loss: 0.91878611\n",
      "Step: [2395] d_loss: 0.76427591, g_loss: 0.93507385\n",
      "Step: [2396] d_loss: 0.78289807, g_loss: 0.89596927\n",
      "Step: [2397] d_loss: 0.76033592, g_loss: 0.93213433\n",
      "Step: [2398] d_loss: 0.85698181, g_loss: 0.91113967\n",
      "Step: [2399] d_loss: 0.74838984, g_loss: 0.94766366\n",
      "Step: [2400] d_loss: 0.72073483, g_loss: 0.95139605\n",
      "Step: [2401] d_loss: 0.72219414, g_loss: 0.95718992\n",
      "Step: [2402] d_loss: 0.77366590, g_loss: 0.91355503\n",
      "Step: [2403] d_loss: 0.65389717, g_loss: 0.98320973\n",
      "Step: [2404] d_loss: 0.75208485, g_loss: 0.92234534\n",
      "Step: [2405] d_loss: 0.66572416, g_loss: 0.96692932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2406] d_loss: 0.82731420, g_loss: 0.89288843\n",
      "Step: [2407] d_loss: 0.67520362, g_loss: 0.97395515\n",
      "Step: [2408] d_loss: 0.69764978, g_loss: 0.92074049\n",
      "Step: [2409] d_loss: 0.81740689, g_loss: 0.92120492\n",
      "Step: [2410] d_loss: 0.72173113, g_loss: 0.97657156\n",
      "Step: [2411] d_loss: 0.74586600, g_loss: 0.95424724\n",
      "Step: [2412] d_loss: 0.70103133, g_loss: 0.95249355\n",
      "Step: [2413] d_loss: 0.70230097, g_loss: 0.94546962\n",
      "Step: [2414] d_loss: 0.73322356, g_loss: 0.95803082\n",
      "Step: [2415] d_loss: 0.71895587, g_loss: 0.96725655\n",
      "Step: [2416] d_loss: 0.77563328, g_loss: 1.00935233\n",
      "Step: [2417] d_loss: 0.71393406, g_loss: 0.89816231\n",
      "Step: [2418] d_loss: 0.76069903, g_loss: 0.94089454\n",
      "Step: [2419] d_loss: 0.77154326, g_loss: 0.98899001\n",
      "Step: [2420] d_loss: 0.76120740, g_loss: 0.91431278\n",
      "Step: [2421] d_loss: 0.74433780, g_loss: 0.92627764\n",
      "Step: [2422] d_loss: 0.74206358, g_loss: 0.91022831\n",
      "Step: [2423] d_loss: 0.73745668, g_loss: 0.89991069\n",
      "Step: [2424] d_loss: 0.70194662, g_loss: 0.91271931\n",
      "Step: [2425] d_loss: 0.73475111, g_loss: 0.93033904\n",
      "Step: [2426] d_loss: 0.72910404, g_loss: 0.88193738\n",
      "Step: [2427] d_loss: 0.77524209, g_loss: 0.81253940\n",
      "Step: [2428] d_loss: 0.83949763, g_loss: 0.81928736\n",
      "Step: [2429] d_loss: 0.72849488, g_loss: 0.86279118\n",
      "Step: [2430] d_loss: 0.73560935, g_loss: 0.90352470\n",
      "Step: [2431] d_loss: 0.76100361, g_loss: 0.92615318\n",
      "Step: [2432] d_loss: 0.70845932, g_loss: 0.96721554\n",
      "Step: [2433] d_loss: 0.72788382, g_loss: 0.93254960\n",
      "Step: [2434] d_loss: 0.77270478, g_loss: 0.92063427\n",
      "Step: [2435] d_loss: 0.66572142, g_loss: 0.91345495\n",
      "Step: [2436] d_loss: 0.69542754, g_loss: 0.96824235\n",
      "Step: [2437] d_loss: 0.81151056, g_loss: 0.92306602\n",
      "Step: [2438] d_loss: 0.75039041, g_loss: 0.96674240\n",
      "Step: [2439] d_loss: 0.80260366, g_loss: 0.96478963\n",
      "Step: [2440] d_loss: 0.75168806, g_loss: 0.94373912\n",
      "Step: [2441] d_loss: 0.77006525, g_loss: 0.95127892\n",
      "Step: [2442] d_loss: 0.67161041, g_loss: 0.90784955\n",
      "Step: [2443] d_loss: 0.69812977, g_loss: 0.91776741\n",
      "Step: [2444] d_loss: 0.72355199, g_loss: 0.89790571\n",
      "Step: [2445] d_loss: 0.72287601, g_loss: 0.94307685\n",
      "Step: [2446] d_loss: 0.70334417, g_loss: 0.97504771\n",
      "Step: [2447] d_loss: 0.75482744, g_loss: 0.92158759\n",
      "Step: [2448] d_loss: 0.76502377, g_loss: 0.94321245\n",
      "Step: [2449] d_loss: 0.67936909, g_loss: 0.93561220\n",
      "Step: [2450] d_loss: 0.69346100, g_loss: 0.92906445\n",
      "Step: [2451] d_loss: 0.75781971, g_loss: 0.86690629\n",
      "Step: [2452] d_loss: 0.73873013, g_loss: 0.93640238\n",
      "Step: [2453] d_loss: 0.71216893, g_loss: 0.98290527\n",
      "Step: [2454] d_loss: 0.74656051, g_loss: 0.91861534\n",
      "Step: [2455] d_loss: 0.70934695, g_loss: 0.96528369\n",
      "Step: [2456] d_loss: 0.69262606, g_loss: 0.94437551\n",
      "Step: [2457] d_loss: 0.74468082, g_loss: 0.94551808\n",
      "Step: [2458] d_loss: 0.74105990, g_loss: 0.92919254\n",
      "Step: [2459] d_loss: 0.75350136, g_loss: 0.97022104\n",
      "Step: [2460] d_loss: 0.82812786, g_loss: 0.98022270\n",
      "Step: [2461] d_loss: 0.68118954, g_loss: 0.96069956\n",
      "Step: [2462] d_loss: 0.73590857, g_loss: 0.92406249\n",
      "Step: [2463] d_loss: 0.76851118, g_loss: 0.90037936\n",
      "Step: [2464] d_loss: 0.74584335, g_loss: 0.91202372\n",
      "Step: [2465] d_loss: 0.74726450, g_loss: 0.93295610\n",
      "Step: [2466] d_loss: 0.75964963, g_loss: 0.96951830\n",
      "Step: [2467] d_loss: 0.77402669, g_loss: 0.90832853\n",
      "Step: [2468] d_loss: 0.77448916, g_loss: 0.88274109\n",
      "Step: [2469] d_loss: 0.80977237, g_loss: 0.84853339\n",
      "Step: [2470] d_loss: 0.68991357, g_loss: 0.88254637\n",
      "Step: [2471] d_loss: 0.78729820, g_loss: 0.84223980\n",
      "Step: [2472] d_loss: 0.79740119, g_loss: 0.82971066\n",
      "Step: [2473] d_loss: 0.73909730, g_loss: 0.88573754\n",
      "Step: [2474] d_loss: 0.74273473, g_loss: 0.92078662\n",
      "Step: [2475] d_loss: 0.77310407, g_loss: 0.91454804\n",
      "Step: [2476] d_loss: 0.76237345, g_loss: 0.96047413\n",
      "Step: [2477] d_loss: 0.75755000, g_loss: 0.92175949\n",
      "Step: [2478] d_loss: 0.78214979, g_loss: 0.94887352\n",
      "Step: [2479] d_loss: 0.75362766, g_loss: 1.00592279\n",
      "Step: [2480] d_loss: 0.73996186, g_loss: 0.93803722\n",
      "Step: [2481] d_loss: 0.75629079, g_loss: 0.91719759\n",
      "Step: [2482] d_loss: 0.80886102, g_loss: 0.87802148\n",
      "Step: [2483] d_loss: 0.71159160, g_loss: 0.96722209\n",
      "Step: [2484] d_loss: 0.79926020, g_loss: 0.95342952\n",
      "Step: [2485] d_loss: 0.72528672, g_loss: 0.95369625\n",
      "Step: [2486] d_loss: 0.70846212, g_loss: 0.94666052\n",
      "Step: [2487] d_loss: 0.77443671, g_loss: 0.93313336\n",
      "Step: [2488] d_loss: 0.72076952, g_loss: 0.91436976\n",
      "Step: [2489] d_loss: 0.69795454, g_loss: 0.86456764\n",
      "Step: [2490] d_loss: 0.71167386, g_loss: 0.91775477\n",
      "Step: [2491] d_loss: 0.66664875, g_loss: 0.91320503\n",
      "Step: [2492] d_loss: 0.85592759, g_loss: 0.89951271\n",
      "Step: [2493] d_loss: 0.84780163, g_loss: 0.90917063\n",
      "Step: [2494] d_loss: 0.80850732, g_loss: 0.89654756\n",
      "Step: [2495] d_loss: 0.70797032, g_loss: 0.97724020\n",
      "Step: [2496] d_loss: 0.67406774, g_loss: 0.93451375\n",
      "Step: [2497] d_loss: 0.74536610, g_loss: 0.98063362\n",
      "Step: [2498] d_loss: 0.66301048, g_loss: 0.90699857\n",
      "Step: [2499] d_loss: 0.75033963, g_loss: 0.89985549\n",
      "Step: [2500] d_loss: 0.74356169, g_loss: 0.94637668\n",
      "Step: [2501] d_loss: 0.75417995, g_loss: 0.90634412\n",
      "Step: [2502] d_loss: 0.67463529, g_loss: 0.94090927\n",
      "Step: [2503] d_loss: 0.72505325, g_loss: 0.97262818\n",
      "Step: [2504] d_loss: 0.76525038, g_loss: 0.93620348\n",
      "Step: [2505] d_loss: 0.67471397, g_loss: 0.94067568\n",
      "Step: [2506] d_loss: 0.67795855, g_loss: 0.94497186\n",
      "Step: [2507] d_loss: 0.61249650, g_loss: 1.01031816\n",
      "Step: [2508] d_loss: 0.76481396, g_loss: 0.92628896\n",
      "Step: [2509] d_loss: 0.67847097, g_loss: 0.91576391\n",
      "Step: [2510] d_loss: 0.70156801, g_loss: 0.89720821\n",
      "Step: [2511] d_loss: 0.72764534, g_loss: 0.95123261\n",
      "Step: [2512] d_loss: 0.63896155, g_loss: 0.97912735\n",
      "Step: [2513] d_loss: 0.66289878, g_loss: 0.90858692\n",
      "Step: [2514] d_loss: 0.72795910, g_loss: 0.98200238\n",
      "Step: [2515] d_loss: 0.72645116, g_loss: 0.88354522\n",
      "Step: [2516] d_loss: 0.71308470, g_loss: 0.89591873\n",
      "Step: [2517] d_loss: 0.75138438, g_loss: 0.95765311\n",
      "Step: [2518] d_loss: 0.67387843, g_loss: 0.88302839\n",
      "Step: [2519] d_loss: 0.67899424, g_loss: 0.91485190\n",
      "Step: [2520] d_loss: 0.72384542, g_loss: 0.88701582\n",
      "Step: [2521] d_loss: 0.68693858, g_loss: 0.97739297\n",
      "Step: [2522] d_loss: 0.72971517, g_loss: 0.91253501\n",
      "Step: [2523] d_loss: 0.74087441, g_loss: 0.90395117\n",
      "Step: [2524] d_loss: 0.70296514, g_loss: 0.94716948\n",
      "Step: [2525] d_loss: 0.71134984, g_loss: 0.87530482\n",
      "Step: [2526] d_loss: 0.65202385, g_loss: 0.89560944\n",
      "Step: [2527] d_loss: 0.73266429, g_loss: 0.85772198\n",
      "Step: [2528] d_loss: 0.65486360, g_loss: 0.90266550\n",
      "Step: [2529] d_loss: 0.73310822, g_loss: 0.86975771\n",
      "Step: [2530] d_loss: 0.81223905, g_loss: 0.88858104\n",
      "Step: [2531] d_loss: 0.87133682, g_loss: 0.82017285\n",
      "Step: [2532] d_loss: 0.64299893, g_loss: 0.88025898\n",
      "Step: [2533] d_loss: 0.76016742, g_loss: 0.92352819\n",
      "Step: [2534] d_loss: 0.78017861, g_loss: 0.97079390\n",
      "Step: [2535] d_loss: 0.82089520, g_loss: 0.93242240\n",
      "Step: [2536] d_loss: 0.74608916, g_loss: 0.94719982\n",
      "Step: [2537] d_loss: 0.81675386, g_loss: 0.93699336\n",
      "Step: [2538] d_loss: 0.77435941, g_loss: 0.96422958\n",
      "Step: [2539] d_loss: 0.78477871, g_loss: 0.85996962\n",
      "Step: [2540] d_loss: 0.75102520, g_loss: 0.91086084\n",
      "Step: [2541] d_loss: 0.65188223, g_loss: 0.92015064\n",
      "Step: [2542] d_loss: 0.88665438, g_loss: 0.87348944\n",
      "Step: [2543] d_loss: 0.81289947, g_loss: 0.94942945\n",
      "Step: [2544] d_loss: 0.69798410, g_loss: 0.91533661\n",
      "Step: [2545] d_loss: 0.76573610, g_loss: 0.89933270\n",
      "Step: [2546] d_loss: 0.76669222, g_loss: 0.87247777\n",
      "Step: [2547] d_loss: 0.84297585, g_loss: 0.84416813\n",
      "Step: [2548] d_loss: 0.72154582, g_loss: 0.92181003\n",
      "Step: [2549] d_loss: 0.71347779, g_loss: 0.94993812\n",
      "Step: [2550] d_loss: 0.74532795, g_loss: 1.00211513\n",
      "Step: [2551] d_loss: 0.74510115, g_loss: 0.98135680\n",
      "Step: [2552] d_loss: 0.80409974, g_loss: 0.98455393\n",
      "Step: [2553] d_loss: 0.75779754, g_loss: 0.95497847\n",
      "Step: [2554] d_loss: 0.79952502, g_loss: 0.94879377\n",
      "Step: [2555] d_loss: 0.71119368, g_loss: 0.97549188\n",
      "Step: [2556] d_loss: 0.72821832, g_loss: 0.92831630\n",
      "Step: [2557] d_loss: 0.65915996, g_loss: 0.91950828\n",
      "Step: [2558] d_loss: 0.83967209, g_loss: 0.90940201\n",
      "Step: [2559] d_loss: 0.80384880, g_loss: 0.87964433\n",
      "Step: [2560] d_loss: 0.77502143, g_loss: 0.89873034\n",
      "Step: [2561] d_loss: 0.80530852, g_loss: 0.88038015\n",
      "Step: [2562] d_loss: 0.68013024, g_loss: 0.91150934\n",
      "Step: [2563] d_loss: 0.60792381, g_loss: 0.93645555\n",
      "Step: [2564] d_loss: 0.77027160, g_loss: 0.90889329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2565] d_loss: 0.78068072, g_loss: 0.93694562\n",
      "Step: [2566] d_loss: 0.77063560, g_loss: 0.93976879\n",
      "Step: [2567] d_loss: 0.70524901, g_loss: 0.92344040\n",
      "Step: [2568] d_loss: 0.81970716, g_loss: 0.87378597\n",
      "Step: [2569] d_loss: 0.72361034, g_loss: 0.94939590\n",
      "Step: [2570] d_loss: 0.72825080, g_loss: 0.90731937\n",
      "Step: [2571] d_loss: 0.74661893, g_loss: 0.91805756\n",
      "Step: [2572] d_loss: 0.70840454, g_loss: 0.93817657\n",
      "Step: [2573] d_loss: 0.79037750, g_loss: 0.93806875\n",
      "Step: [2574] d_loss: 0.80865234, g_loss: 0.96548939\n",
      "Step: [2575] d_loss: 0.74834573, g_loss: 0.99116111\n",
      "Step: [2576] d_loss: 0.75064784, g_loss: 0.92763585\n",
      "Step: [2577] d_loss: 0.75011718, g_loss: 0.95611691\n",
      "Step: [2578] d_loss: 0.77817988, g_loss: 0.94741911\n",
      "Step: [2579] d_loss: 0.83256555, g_loss: 0.92979759\n",
      "Step: [2580] d_loss: 0.81468576, g_loss: 0.97412568\n",
      "Step: [2581] d_loss: 0.79676771, g_loss: 0.96013844\n",
      "Step: [2582] d_loss: 0.75066990, g_loss: 0.94227004\n",
      "Step: [2583] d_loss: 0.76104718, g_loss: 0.93350828\n",
      "Step: [2584] d_loss: 0.75831211, g_loss: 0.87651974\n",
      "Step: [2585] d_loss: 0.71257848, g_loss: 0.90671176\n",
      "Step: [2586] d_loss: 0.77013248, g_loss: 0.89181960\n",
      "Step: [2587] d_loss: 0.77273029, g_loss: 0.90872765\n",
      "Step: [2588] d_loss: 0.76618421, g_loss: 0.97435176\n",
      "Step: [2589] d_loss: 0.73526925, g_loss: 0.91464490\n",
      "Step: [2590] d_loss: 0.74539924, g_loss: 0.93192548\n",
      "Step: [2591] d_loss: 0.79610664, g_loss: 0.91663909\n",
      "Step: [2592] d_loss: 0.70927972, g_loss: 0.92920548\n",
      "Step: [2593] d_loss: 0.79137403, g_loss: 0.92984831\n",
      "Step: [2594] d_loss: 0.76067096, g_loss: 0.96412039\n",
      "Step: [2595] d_loss: 0.74416107, g_loss: 0.93975258\n",
      "Step: [2596] d_loss: 0.81152451, g_loss: 0.92274082\n",
      "Step: [2597] d_loss: 0.68212885, g_loss: 0.94041961\n",
      "Step: [2598] d_loss: 0.81386989, g_loss: 0.93768549\n",
      "Step: [2599] d_loss: 0.74132246, g_loss: 0.94928294\n",
      "Step: [2600] d_loss: 0.71216822, g_loss: 0.89346254\n",
      "Step: [2601] d_loss: 0.78185821, g_loss: 0.93327343\n",
      "Step: [2602] d_loss: 0.78725594, g_loss: 0.98006296\n",
      "Step: [2603] d_loss: 0.74669635, g_loss: 0.93156677\n",
      "Step: [2604] d_loss: 0.71726203, g_loss: 0.90009737\n",
      "Step: [2605] d_loss: 0.70649266, g_loss: 0.90392375\n",
      "Step: [2606] d_loss: 0.67419440, g_loss: 0.89716363\n",
      "Step: [2607] d_loss: 0.79895616, g_loss: 0.87170291\n",
      "Step: [2608] d_loss: 0.78103364, g_loss: 0.88643646\n",
      "Step: [2609] d_loss: 0.69623125, g_loss: 0.91062880\n",
      "Step: [2610] d_loss: 0.83437687, g_loss: 0.88498735\n",
      "Step: [2611] d_loss: 0.75913334, g_loss: 0.90107042\n",
      "Step: [2612] d_loss: 0.66141838, g_loss: 0.89981323\n",
      "Step: [2613] d_loss: 0.63532346, g_loss: 0.86635137\n",
      "Step: [2614] d_loss: 0.70780677, g_loss: 0.90919000\n",
      "Step: [2615] d_loss: 0.72760326, g_loss: 0.87034053\n",
      "Step: [2616] d_loss: 0.82027161, g_loss: 0.86489087\n",
      "Step: [2617] d_loss: 0.97756559, g_loss: 0.92223239\n",
      "Step: [2618] d_loss: 0.73862988, g_loss: 0.95381802\n",
      "Step: [2619] d_loss: 0.77804881, g_loss: 0.97689527\n",
      "Step: [2620] d_loss: 0.84678429, g_loss: 0.91630870\n",
      "Step: [2621] d_loss: 0.74277055, g_loss: 0.95445549\n",
      "Step: [2622] d_loss: 0.78771389, g_loss: 0.93265218\n",
      "Step: [2623] d_loss: 0.80785626, g_loss: 0.86997646\n",
      "Step: [2624] d_loss: 0.73797458, g_loss: 0.86760736\n",
      "Step: [2625] d_loss: 0.79495060, g_loss: 0.91864306\n",
      "Step: [2626] d_loss: 0.75881839, g_loss: 0.92959952\n",
      "Step: [2627] d_loss: 0.75357974, g_loss: 0.89000261\n",
      "Step: [2628] d_loss: 0.79495347, g_loss: 0.84277123\n",
      "Step: [2629] d_loss: 0.72547030, g_loss: 0.90625191\n",
      "Step: [2630] d_loss: 0.73454696, g_loss: 0.96432650\n",
      "Step: [2631] d_loss: 0.79741019, g_loss: 0.91877794\n",
      "Step: [2632] d_loss: 0.82890499, g_loss: 0.93069857\n",
      "Step: [2633] d_loss: 0.76320326, g_loss: 0.90071648\n",
      "Step: [2634] d_loss: 0.81054115, g_loss: 0.92088753\n",
      "Step: [2635] d_loss: 0.68483526, g_loss: 0.92266905\n",
      "Step: [2636] d_loss: 0.76745909, g_loss: 0.91175723\n",
      "Step: [2637] d_loss: 0.67054743, g_loss: 0.95195276\n",
      "Step: [2638] d_loss: 0.79130620, g_loss: 0.97138667\n",
      "Step: [2639] d_loss: 0.78690434, g_loss: 0.95555514\n",
      "Step: [2640] d_loss: 0.78302491, g_loss: 0.92648798\n",
      "Step: [2641] d_loss: 0.78363574, g_loss: 0.94182462\n",
      "Step: [2642] d_loss: 0.79521364, g_loss: 0.90817678\n",
      "Step: [2643] d_loss: 0.82917178, g_loss: 0.91374737\n",
      "Step: [2644] d_loss: 0.69745821, g_loss: 0.89552850\n",
      "Step: [2645] d_loss: 0.71862012, g_loss: 0.94062972\n",
      "Step: [2646] d_loss: 0.75340962, g_loss: 0.89359266\n",
      "Step: [2647] d_loss: 0.73687786, g_loss: 0.92657459\n",
      "Step: [2648] d_loss: 0.77902514, g_loss: 0.94762498\n",
      "Step: [2649] d_loss: 0.73512524, g_loss: 0.97302669\n",
      "Step: [2650] d_loss: 0.77388126, g_loss: 0.96422595\n",
      "Step: [2651] d_loss: 0.67719698, g_loss: 0.94115657\n",
      "Step: [2652] d_loss: 0.74973667, g_loss: 0.89803243\n",
      "Step: [2653] d_loss: 0.75956452, g_loss: 0.90436029\n",
      "Step: [2654] d_loss: 0.68613815, g_loss: 0.87650406\n",
      "Step: [2655] d_loss: 0.82220089, g_loss: 0.85336608\n",
      "Step: [2656] d_loss: 0.76456249, g_loss: 0.89036137\n",
      "Step: [2657] d_loss: 0.81954622, g_loss: 0.83209836\n",
      "Step: [2658] d_loss: 0.75112599, g_loss: 0.92767769\n",
      "Step: [2659] d_loss: 0.69722223, g_loss: 0.93604171\n",
      "Step: [2660] d_loss: 0.76178616, g_loss: 0.92155141\n",
      "Step: [2661] d_loss: 0.74956799, g_loss: 0.94353598\n",
      "Step: [2662] d_loss: 0.74336094, g_loss: 0.99056035\n",
      "Step: [2663] d_loss: 0.77649248, g_loss: 0.92995304\n",
      "Step: [2664] d_loss: 0.71431589, g_loss: 0.90798485\n",
      "Step: [2665] d_loss: 0.67785877, g_loss: 0.90482777\n",
      "Step: [2666] d_loss: 0.67495412, g_loss: 0.87314254\n",
      "Step: [2667] d_loss: 0.83616996, g_loss: 0.84698677\n",
      "Step: [2668] d_loss: 0.75083590, g_loss: 0.91324359\n",
      "Step: [2669] d_loss: 0.67639601, g_loss: 0.98357499\n",
      "Step: [2670] d_loss: 0.69502866, g_loss: 0.97112656\n",
      "Step: [2671] d_loss: 0.70140910, g_loss: 0.86535043\n",
      "Step: [2672] d_loss: 0.87121832, g_loss: 0.88469923\n",
      "Step: [2673] d_loss: 0.74750984, g_loss: 0.93655157\n",
      "Step: [2674] d_loss: 0.81196576, g_loss: 0.87865674\n",
      "Step: [2675] d_loss: 0.79746205, g_loss: 0.93421572\n",
      "Step: [2676] d_loss: 0.80061597, g_loss: 0.91754901\n",
      "Step: [2677] d_loss: 0.71889412, g_loss: 0.95183986\n",
      "Step: [2678] d_loss: 0.73786658, g_loss: 0.92891866\n",
      "Step: [2679] d_loss: 0.73874664, g_loss: 0.93715119\n",
      "Step: [2680] d_loss: 0.70663542, g_loss: 0.95640171\n",
      "Step: [2681] d_loss: 0.78227019, g_loss: 0.99903989\n",
      "Step: [2682] d_loss: 0.72028941, g_loss: 0.97159326\n",
      "Step: [2683] d_loss: 0.72766620, g_loss: 0.88217849\n",
      "Step: [2684] d_loss: 0.70627260, g_loss: 0.89899021\n",
      "Step: [2685] d_loss: 0.79342765, g_loss: 0.87660217\n",
      "Step: [2686] d_loss: 0.66431582, g_loss: 0.91998547\n",
      "Step: [2687] d_loss: 0.75960642, g_loss: 0.95536673\n",
      "Step: [2688] d_loss: 0.76757449, g_loss: 0.95455557\n",
      "Step: [2689] d_loss: 0.73274618, g_loss: 0.95988685\n",
      "Step: [2690] d_loss: 0.73277533, g_loss: 0.91477191\n",
      "Step: [2691] d_loss: 0.62754899, g_loss: 0.88099813\n",
      "Step: [2692] d_loss: 0.71911711, g_loss: 0.89239872\n",
      "Step: [2693] d_loss: 0.73024207, g_loss: 0.90529829\n",
      "Step: [2694] d_loss: 0.75559205, g_loss: 0.89370942\n",
      "Step: [2695] d_loss: 0.85879177, g_loss: 0.87694877\n",
      "Step: [2696] d_loss: 0.66817880, g_loss: 0.94625270\n",
      "Step: [2697] d_loss: 0.73046869, g_loss: 0.93494159\n",
      "Step: [2698] d_loss: 0.79418403, g_loss: 0.93845391\n",
      "Step: [2699] d_loss: 0.70660967, g_loss: 0.89692491\n",
      "Step: [2700] d_loss: 0.74889368, g_loss: 0.91141725\n",
      "Step: [2701] d_loss: 0.77901691, g_loss: 0.90763891\n",
      "Step: [2702] d_loss: 0.67813933, g_loss: 0.98593760\n",
      "Step: [2703] d_loss: 0.79503316, g_loss: 0.94599617\n",
      "Step: [2704] d_loss: 0.76805174, g_loss: 0.91759938\n",
      "Step: [2705] d_loss: 0.74932283, g_loss: 0.90348035\n",
      "Step: [2706] d_loss: 0.81738126, g_loss: 0.87735415\n",
      "Step: [2707] d_loss: 0.73063982, g_loss: 0.89804637\n",
      "Step: [2708] d_loss: 0.80264980, g_loss: 0.91064179\n",
      "Step: [2709] d_loss: 0.79673886, g_loss: 0.89044338\n",
      "Step: [2710] d_loss: 0.79862916, g_loss: 0.94087762\n",
      "Step: [2711] d_loss: 0.77946931, g_loss: 0.92791063\n",
      "Step: [2712] d_loss: 0.70416802, g_loss: 0.93697244\n",
      "Step: [2713] d_loss: 0.76110494, g_loss: 0.97893608\n",
      "Step: [2714] d_loss: 0.75823992, g_loss: 0.90280175\n",
      "Step: [2715] d_loss: 0.72659868, g_loss: 0.94447231\n",
      "Step: [2716] d_loss: 0.82803184, g_loss: 0.94628888\n",
      "Step: [2717] d_loss: 0.74392283, g_loss: 0.91718400\n",
      "Step: [2718] d_loss: 0.69361234, g_loss: 0.89383912\n",
      "Step: [2719] d_loss: 0.72434807, g_loss: 0.91525453\n",
      "Step: [2720] d_loss: 0.77952230, g_loss: 0.89798427\n",
      "Step: [2721] d_loss: 0.76616961, g_loss: 0.92267966\n",
      "Step: [2722] d_loss: 0.77333736, g_loss: 0.89261335\n",
      "Step: [2723] d_loss: 0.69500571, g_loss: 0.96794403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2724] d_loss: 0.71416622, g_loss: 1.05475879\n",
      "Step: [2725] d_loss: 0.68606424, g_loss: 0.94242716\n",
      "Step: [2726] d_loss: 0.75141168, g_loss: 0.95869422\n",
      "Step: [2727] d_loss: 0.70957917, g_loss: 0.91267955\n",
      "Step: [2728] d_loss: 0.81674069, g_loss: 0.91570699\n",
      "Step: [2729] d_loss: 0.65534252, g_loss: 0.96327484\n",
      "Step: [2730] d_loss: 0.69178641, g_loss: 0.91466188\n",
      "Step: [2731] d_loss: 0.81302267, g_loss: 0.87108082\n",
      "Step: [2732] d_loss: 0.77586746, g_loss: 0.94873893\n",
      "Step: [2733] d_loss: 0.79460251, g_loss: 0.94067991\n",
      "Step: [2734] d_loss: 0.79321331, g_loss: 0.94559258\n",
      "Step: [2735] d_loss: 0.81526047, g_loss: 0.98946393\n",
      "Step: [2736] d_loss: 0.76072860, g_loss: 0.91114151\n",
      "Step: [2737] d_loss: 0.68414646, g_loss: 0.89272976\n",
      "Step: [2738] d_loss: 0.74009132, g_loss: 0.92535669\n",
      "Step: [2739] d_loss: 0.67741275, g_loss: 0.93360519\n",
      "Step: [2740] d_loss: 0.69071007, g_loss: 0.94502193\n",
      "Step: [2741] d_loss: 0.78730839, g_loss: 0.92089194\n",
      "Step: [2742] d_loss: 0.65725636, g_loss: 0.90396804\n",
      "Step: [2743] d_loss: 0.64866871, g_loss: 0.92726260\n",
      "Step: [2744] d_loss: 0.81936139, g_loss: 0.92850655\n",
      "Step: [2745] d_loss: 0.75342971, g_loss: 0.96274894\n",
      "Step: [2746] d_loss: 0.71473050, g_loss: 0.93583882\n",
      "Step: [2747] d_loss: 0.79134715, g_loss: 0.92331660\n",
      "Step: [2748] d_loss: 0.67954803, g_loss: 0.95569074\n",
      "Step: [2749] d_loss: 0.70479560, g_loss: 0.97638232\n",
      "Step: [2750] d_loss: 0.77378851, g_loss: 0.92900503\n",
      "Step: [2751] d_loss: 0.69852924, g_loss: 0.90468538\n",
      "Step: [2752] d_loss: 0.75065404, g_loss: 0.92579728\n",
      "Step: [2753] d_loss: 0.71295249, g_loss: 0.94239640\n",
      "Step: [2754] d_loss: 0.80404699, g_loss: 0.88853526\n",
      "Step: [2755] d_loss: 0.76971519, g_loss: 0.92153966\n",
      "Step: [2756] d_loss: 0.81694800, g_loss: 0.85907954\n",
      "Step: [2757] d_loss: 0.72069079, g_loss: 0.91747028\n",
      "Step: [2758] d_loss: 0.78476882, g_loss: 0.92716360\n",
      "Step: [2759] d_loss: 0.73118061, g_loss: 0.91022503\n",
      "Step: [2760] d_loss: 0.73409653, g_loss: 0.93628633\n",
      "Step: [2761] d_loss: 0.76910126, g_loss: 0.90666711\n",
      "Step: [2762] d_loss: 0.77968568, g_loss: 0.89973682\n",
      "Step: [2763] d_loss: 0.79569626, g_loss: 0.88769746\n",
      "Step: [2764] d_loss: 0.81725001, g_loss: 0.90395391\n",
      "Step: [2765] d_loss: 0.75555676, g_loss: 0.92115527\n",
      "Step: [2766] d_loss: 0.73928684, g_loss: 0.92438143\n",
      "Step: [2767] d_loss: 0.75715971, g_loss: 0.91448826\n",
      "Step: [2768] d_loss: 0.81117582, g_loss: 0.89825994\n",
      "Step: [2769] d_loss: 0.78504759, g_loss: 0.91414577\n",
      "Step: [2770] d_loss: 0.72964841, g_loss: 0.98172385\n",
      "Step: [2771] d_loss: 0.75088978, g_loss: 0.99663818\n",
      "Step: [2772] d_loss: 0.70490754, g_loss: 0.94899338\n",
      "Step: [2773] d_loss: 0.71544731, g_loss: 0.92414123\n",
      "Step: [2774] d_loss: 0.71178168, g_loss: 0.91942674\n",
      "Step: [2775] d_loss: 0.79263139, g_loss: 0.83688688\n",
      "Step: [2776] d_loss: 0.75755626, g_loss: 0.91325188\n",
      "Step: [2777] d_loss: 0.85083032, g_loss: 0.84465218\n",
      "Step: [2778] d_loss: 0.77067012, g_loss: 0.90470457\n",
      "Step: [2779] d_loss: 0.82667863, g_loss: 0.90772706\n",
      "Step: [2780] d_loss: 0.70790923, g_loss: 0.94800603\n",
      "Step: [2781] d_loss: 0.74136126, g_loss: 0.93140537\n",
      "Step: [2782] d_loss: 0.80300957, g_loss: 0.92418367\n",
      "Step: [2783] d_loss: 0.66919929, g_loss: 0.91071147\n",
      "Step: [2784] d_loss: 0.69364566, g_loss: 0.88570994\n",
      "Step: [2785] d_loss: 0.76702070, g_loss: 0.87625283\n",
      "Step: [2786] d_loss: 0.72721511, g_loss: 0.96548307\n",
      "Step: [2787] d_loss: 0.72336090, g_loss: 0.93234122\n",
      "Step: [2788] d_loss: 0.70140481, g_loss: 0.93266237\n",
      "Step: [2789] d_loss: 0.70230430, g_loss: 0.98507965\n",
      "Step: [2790] d_loss: 0.73854977, g_loss: 0.97015095\n",
      "Step: [2791] d_loss: 0.74301565, g_loss: 0.93513596\n",
      "Step: [2792] d_loss: 0.73710001, g_loss: 0.91250890\n",
      "Step: [2793] d_loss: 0.71895325, g_loss: 0.96400130\n",
      "Step: [2794] d_loss: 0.81801969, g_loss: 0.89761811\n",
      "Step: [2795] d_loss: 0.77859801, g_loss: 0.91763341\n",
      "Step: [2796] d_loss: 0.75575602, g_loss: 0.94460201\n",
      "Step: [2797] d_loss: 0.65287006, g_loss: 0.91357595\n",
      "Step: [2798] d_loss: 0.71442538, g_loss: 0.88374865\n",
      "Step: [2799] d_loss: 0.68759096, g_loss: 0.95108104\n",
      "Step: [2800] d_loss: 0.74050021, g_loss: 0.94726318\n",
      "Step: [2801] d_loss: 0.86295933, g_loss: 0.89290535\n",
      "Step: [2802] d_loss: 0.76428235, g_loss: 0.95269537\n",
      "Step: [2803] d_loss: 0.84029281, g_loss: 0.95394957\n",
      "Step: [2804] d_loss: 0.85539365, g_loss: 0.95298344\n",
      "Step: [2805] d_loss: 0.78060067, g_loss: 0.96455103\n",
      "Step: [2806] d_loss: 0.67903912, g_loss: 0.98031926\n",
      "Step: [2807] d_loss: 0.84214425, g_loss: 0.95491624\n",
      "Step: [2808] d_loss: 0.80824298, g_loss: 0.96657968\n",
      "Step: [2809] d_loss: 0.70851886, g_loss: 0.98181474\n",
      "Step: [2810] d_loss: 0.76386255, g_loss: 0.94832903\n",
      "Step: [2811] d_loss: 0.68533838, g_loss: 0.90451419\n",
      "Step: [2812] d_loss: 0.78424430, g_loss: 0.89992023\n",
      "Step: [2813] d_loss: 0.78395432, g_loss: 0.88168061\n",
      "Step: [2814] d_loss: 0.73950893, g_loss: 0.91457778\n",
      "Step: [2815] d_loss: 0.72701550, g_loss: 0.92986339\n",
      "Step: [2816] d_loss: 0.67958188, g_loss: 0.95784879\n",
      "Step: [2817] d_loss: 0.80103916, g_loss: 0.93327141\n",
      "Step: [2818] d_loss: 0.77427828, g_loss: 0.94382495\n",
      "Step: [2819] d_loss: 0.76467288, g_loss: 0.91575444\n",
      "Step: [2820] d_loss: 0.66195917, g_loss: 0.93619782\n",
      "Step: [2821] d_loss: 0.72557902, g_loss: 0.94174987\n",
      "Step: [2822] d_loss: 0.75772083, g_loss: 0.87389904\n",
      "Step: [2823] d_loss: 0.70510322, g_loss: 0.89194971\n",
      "Step: [2824] d_loss: 0.80740517, g_loss: 0.88934249\n",
      "Step: [2825] d_loss: 0.69102526, g_loss: 0.93876326\n",
      "Step: [2826] d_loss: 0.71101987, g_loss: 0.96792102\n",
      "Step: [2827] d_loss: 0.75299674, g_loss: 0.91367233\n",
      "Step: [2828] d_loss: 0.77322972, g_loss: 0.88083774\n",
      "Step: [2829] d_loss: 0.78611809, g_loss: 0.85083365\n",
      "Step: [2830] d_loss: 0.81881732, g_loss: 0.83691245\n",
      "Step: [2831] d_loss: 0.77868873, g_loss: 0.89273059\n",
      "Step: [2832] d_loss: 0.79136014, g_loss: 0.88039821\n",
      "Step: [2833] d_loss: 0.75535661, g_loss: 0.91605264\n",
      "Step: [2834] d_loss: 0.74056083, g_loss: 0.93989325\n",
      "Step: [2835] d_loss: 0.70341170, g_loss: 0.95229834\n",
      "Step: [2836] d_loss: 0.83353573, g_loss: 0.90583211\n",
      "Step: [2837] d_loss: 0.69789344, g_loss: 0.98291039\n",
      "Step: [2838] d_loss: 0.72491223, g_loss: 0.97631556\n",
      "Step: [2839] d_loss: 0.67834854, g_loss: 0.93505204\n",
      "Step: [2840] d_loss: 0.77906120, g_loss: 0.93821460\n",
      "Step: [2841] d_loss: 0.79472643, g_loss: 0.89475572\n",
      "Step: [2842] d_loss: 0.71810037, g_loss: 0.87673712\n",
      "Step: [2843] d_loss: 0.75808877, g_loss: 0.87200403\n",
      "Step: [2844] d_loss: 0.82299441, g_loss: 0.88526374\n",
      "Step: [2845] d_loss: 0.76215833, g_loss: 0.89523745\n",
      "Step: [2846] d_loss: 0.85289615, g_loss: 0.92000711\n",
      "Step: [2847] d_loss: 0.70153737, g_loss: 0.92042863\n",
      "Step: [2848] d_loss: 0.65856165, g_loss: 0.97044432\n",
      "Step: [2849] d_loss: 0.81131953, g_loss: 0.96099025\n",
      "Step: [2850] d_loss: 0.71716797, g_loss: 0.93452299\n",
      "Step: [2851] d_loss: 0.71450692, g_loss: 0.97973263\n",
      "Step: [2852] d_loss: 0.68437350, g_loss: 0.92557335\n",
      "Step: [2853] d_loss: 0.80450583, g_loss: 0.88245678\n",
      "Step: [2854] d_loss: 0.71290004, g_loss: 0.92554694\n",
      "Step: [2855] d_loss: 0.63910663, g_loss: 0.97536546\n",
      "Step: [2856] d_loss: 0.70566314, g_loss: 0.91757703\n",
      "Step: [2857] d_loss: 0.73594069, g_loss: 0.96191484\n",
      "Step: [2858] d_loss: 0.68562025, g_loss: 0.92730457\n",
      "Step: [2859] d_loss: 0.71912110, g_loss: 0.95126295\n",
      "Step: [2860] d_loss: 0.75148028, g_loss: 0.92923701\n",
      "Step: [2861] d_loss: 0.68778121, g_loss: 0.92931986\n",
      "Step: [2862] d_loss: 0.74835908, g_loss: 0.89286453\n",
      "Step: [2863] d_loss: 0.77733386, g_loss: 0.92886680\n",
      "Step: [2864] d_loss: 0.71178180, g_loss: 0.94742566\n",
      "Step: [2865] d_loss: 0.81070471, g_loss: 0.92021888\n",
      "Step: [2866] d_loss: 0.74911952, g_loss: 0.96252829\n",
      "Step: [2867] d_loss: 0.70915294, g_loss: 0.86999345\n",
      "Step: [2868] d_loss: 0.69122213, g_loss: 0.88926899\n",
      "Step: [2869] d_loss: 0.67097700, g_loss: 0.87286782\n",
      "Step: [2870] d_loss: 0.86634707, g_loss: 0.87445962\n",
      "Step: [2871] d_loss: 0.76874036, g_loss: 0.89828050\n",
      "Step: [2872] d_loss: 0.74357861, g_loss: 0.90103483\n",
      "Step: [2873] d_loss: 0.70602739, g_loss: 0.96941042\n",
      "Step: [2874] d_loss: 0.79756671, g_loss: 0.93567884\n",
      "Step: [2875] d_loss: 0.70530635, g_loss: 0.93493849\n",
      "Step: [2876] d_loss: 0.71833467, g_loss: 0.90113980\n",
      "Step: [2877] d_loss: 0.75709569, g_loss: 0.90049392\n",
      "Step: [2878] d_loss: 0.74344695, g_loss: 0.88881606\n",
      "Step: [2879] d_loss: 0.76484042, g_loss: 0.87948352\n",
      "Step: [2880] d_loss: 0.77187639, g_loss: 0.92942548\n",
      "Step: [2881] d_loss: 0.69767255, g_loss: 0.88919127\n",
      "Step: [2882] d_loss: 0.79493427, g_loss: 0.89413524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [2883] d_loss: 0.76082116, g_loss: 0.91214126\n",
      "Step: [2884] d_loss: 0.73784828, g_loss: 0.91852754\n",
      "Step: [2885] d_loss: 0.73399270, g_loss: 0.93443203\n",
      "Step: [2886] d_loss: 0.77217519, g_loss: 0.91782951\n",
      "Step: [2887] d_loss: 0.72657061, g_loss: 0.91412222\n",
      "Step: [2888] d_loss: 0.66821969, g_loss: 0.98920047\n",
      "Step: [2889] d_loss: 0.81664085, g_loss: 0.94939476\n",
      "Step: [2890] d_loss: 0.72778910, g_loss: 0.93509340\n",
      "Step: [2891] d_loss: 0.75018340, g_loss: 0.90714747\n",
      "Step: [2892] d_loss: 0.65637630, g_loss: 0.90782154\n",
      "Step: [2893] d_loss: 0.77222657, g_loss: 0.92597550\n",
      "Step: [2894] d_loss: 0.69755673, g_loss: 0.93569762\n",
      "Step: [2895] d_loss: 0.70349032, g_loss: 0.89168352\n",
      "Step: [2896] d_loss: 0.74851465, g_loss: 0.95483321\n",
      "Step: [2897] d_loss: 0.76527184, g_loss: 0.95816839\n",
      "Step: [2898] d_loss: 0.76907003, g_loss: 0.93806529\n",
      "Step: [2899] d_loss: 0.74542230, g_loss: 0.90567952\n",
      "Step: [2900] d_loss: 0.80573696, g_loss: 0.93760484\n",
      "Step: [2901] d_loss: 0.81319642, g_loss: 0.89374107\n",
      "Step: [2902] d_loss: 0.72373766, g_loss: 0.94529277\n",
      "Step: [2903] d_loss: 0.73161334, g_loss: 0.91154814\n",
      "Step: [2904] d_loss: 0.80479854, g_loss: 0.86295295\n",
      "Step: [2905] d_loss: 0.72600961, g_loss: 0.93389255\n",
      "Step: [2906] d_loss: 0.74027437, g_loss: 0.89767116\n",
      "Step: [2907] d_loss: 0.67436904, g_loss: 0.91242909\n",
      "Step: [2908] d_loss: 0.81918412, g_loss: 0.87477934\n",
      "Step: [2909] d_loss: 0.71603209, g_loss: 0.93490094\n",
      "Step: [2910] d_loss: 0.77680433, g_loss: 0.91580516\n",
      "Step: [2911] d_loss: 0.72483438, g_loss: 0.97534674\n",
      "Step: [2912] d_loss: 0.64022273, g_loss: 0.95333469\n",
      "Step: [2913] d_loss: 0.68686295, g_loss: 0.94176394\n",
      "Step: [2914] d_loss: 0.67682785, g_loss: 0.92553699\n",
      "Step: [2915] d_loss: 0.68512738, g_loss: 0.97743338\n",
      "Step: [2916] d_loss: 0.84084022, g_loss: 0.99033922\n",
      "Step: [2917] d_loss: 0.78412318, g_loss: 0.92195714\n",
      "Step: [2918] d_loss: 0.71187741, g_loss: 0.92806447\n",
      "Step: [2919] d_loss: 0.79364711, g_loss: 0.93704188\n",
      "Step: [2920] d_loss: 0.72016531, g_loss: 0.90428931\n",
      "Step: [2921] d_loss: 0.73964912, g_loss: 0.91104496\n",
      "Step: [2922] d_loss: 0.71573704, g_loss: 0.91991758\n",
      "Step: [2923] d_loss: 0.68874609, g_loss: 0.92576343\n",
      "Step: [2924] d_loss: 0.76482999, g_loss: 0.95770550\n",
      "Step: [2925] d_loss: 0.81826937, g_loss: 0.92260104\n",
      "Step: [2926] d_loss: 0.79459167, g_loss: 0.91324723\n",
      "Step: [2927] d_loss: 0.83349144, g_loss: 0.92614537\n",
      "Step: [2928] d_loss: 0.82590044, g_loss: 0.94463599\n",
      "Step: [2929] d_loss: 0.70063537, g_loss: 0.88330781\n",
      "Step: [2930] d_loss: 0.71419156, g_loss: 0.90935737\n",
      "Step: [2931] d_loss: 0.86338454, g_loss: 0.80754709\n",
      "Step: [2932] d_loss: 0.76936436, g_loss: 0.91228771\n",
      "Step: [2933] d_loss: 0.80992007, g_loss: 0.87679523\n",
      "Step: [2934] d_loss: 0.69223499, g_loss: 0.95322627\n",
      "Step: [2935] d_loss: 0.71579897, g_loss: 0.88340503\n",
      "Step: [2936] d_loss: 0.81004864, g_loss: 0.86946863\n",
      "Step: [2937] d_loss: 0.66934836, g_loss: 0.88751358\n",
      "Step: [2938] d_loss: 0.75753820, g_loss: 0.90745717\n",
      "Step: [2939] d_loss: 0.73795909, g_loss: 0.94903654\n",
      "Step: [2940] d_loss: 0.74391419, g_loss: 0.88548374\n",
      "Step: [2941] d_loss: 0.77715772, g_loss: 0.88515645\n",
      "Step: [2942] d_loss: 0.80972493, g_loss: 0.90600270\n",
      "Step: [2943] d_loss: 0.78367829, g_loss: 0.93248355\n",
      "Step: [2944] d_loss: 0.80024034, g_loss: 0.97975844\n",
      "Step: [2945] d_loss: 0.71195626, g_loss: 0.91625696\n",
      "Step: [2946] d_loss: 0.68656880, g_loss: 0.88626206\n",
      "Step: [2947] d_loss: 0.80364054, g_loss: 0.91330069\n",
      "Step: [2948] d_loss: 0.81824684, g_loss: 0.87968475\n",
      "Step: [2949] d_loss: 0.81769276, g_loss: 0.91797030\n",
      "Step: [2950] d_loss: 0.74627393, g_loss: 0.91021407\n",
      "Step: [2951] d_loss: 0.80423617, g_loss: 0.89355409\n",
      "Step: [2952] d_loss: 0.73232001, g_loss: 0.94070286\n",
      "Step: [2953] d_loss: 0.77020258, g_loss: 0.95159644\n",
      "Step: [2954] d_loss: 0.76796073, g_loss: 0.90490723\n",
      "Step: [2955] d_loss: 0.82854873, g_loss: 0.90410632\n",
      "Step: [2956] d_loss: 0.81395030, g_loss: 0.89099163\n",
      "Step: [2957] d_loss: 0.77879822, g_loss: 0.97391516\n",
      "Step: [2958] d_loss: 0.69978106, g_loss: 0.93929869\n",
      "Step: [2959] d_loss: 0.77701426, g_loss: 0.91688836\n",
      "Step: [2960] d_loss: 0.73384631, g_loss: 0.93194884\n",
      "Step: [2961] d_loss: 0.79837412, g_loss: 0.93838680\n",
      "Step: [2962] d_loss: 0.78946763, g_loss: 0.95489824\n",
      "Step: [2963] d_loss: 0.74656957, g_loss: 0.89378506\n",
      "Step: [2964] d_loss: 0.71471226, g_loss: 0.93461323\n",
      "Step: [2965] d_loss: 0.67413956, g_loss: 0.89611340\n",
      "Step: [2966] d_loss: 0.71326292, g_loss: 0.88795167\n",
      "Step: [2967] d_loss: 0.75874513, g_loss: 0.92775714\n",
      "Step: [2968] d_loss: 0.72029865, g_loss: 0.92807651\n",
      "Step: [2969] d_loss: 0.85343987, g_loss: 0.95141023\n",
      "Step: [2970] d_loss: 0.77517468, g_loss: 0.96421188\n",
      "Step: [2971] d_loss: 0.80561280, g_loss: 0.94098949\n",
      "Step: [2972] d_loss: 0.74646139, g_loss: 0.88296616\n",
      "Step: [2973] d_loss: 0.74823821, g_loss: 0.87467003\n",
      "Step: [2974] d_loss: 0.78868949, g_loss: 0.86718094\n",
      "Step: [2975] d_loss: 0.75567883, g_loss: 0.92330205\n",
      "Step: [2976] d_loss: 0.79095209, g_loss: 0.88283670\n",
      "Step: [2977] d_loss: 0.73208886, g_loss: 0.89096284\n",
      "Step: [2978] d_loss: 0.69634300, g_loss: 0.90395701\n",
      "Step: [2979] d_loss: 0.69506449, g_loss: 0.91101676\n",
      "Step: [2980] d_loss: 0.74308771, g_loss: 0.91496181\n",
      "Step: [2981] d_loss: 0.77600008, g_loss: 0.89652294\n",
      "Step: [2982] d_loss: 0.65879226, g_loss: 0.91925550\n",
      "Step: [2983] d_loss: 0.77622348, g_loss: 0.87439150\n",
      "Step: [2984] d_loss: 0.70598716, g_loss: 0.91977817\n",
      "Step: [2985] d_loss: 0.79181868, g_loss: 0.84069645\n",
      "Step: [2986] d_loss: 0.65617603, g_loss: 0.96485054\n",
      "Step: [2987] d_loss: 0.75848413, g_loss: 0.92571700\n",
      "Step: [2988] d_loss: 0.70495641, g_loss: 0.98815000\n",
      "Step: [2989] d_loss: 0.78561604, g_loss: 0.94580495\n",
      "Step: [2990] d_loss: 0.69513977, g_loss: 0.97563654\n",
      "Step: [2991] d_loss: 0.76983291, g_loss: 0.97890216\n",
      "Step: [2992] d_loss: 0.77167600, g_loss: 0.95344067\n",
      "Step: [2993] d_loss: 0.74729663, g_loss: 0.95418000\n",
      "Step: [2994] d_loss: 0.70842952, g_loss: 0.94123322\n",
      "Step: [2995] d_loss: 0.75312483, g_loss: 0.89412397\n",
      "Step: [2996] d_loss: 0.76444542, g_loss: 0.92439044\n",
      "Step: [2997] d_loss: 0.74705225, g_loss: 0.96774888\n",
      "Step: [2998] d_loss: 0.78371322, g_loss: 0.97251338\n",
      "Step: [2999] d_loss: 0.73393023, g_loss: 0.90895665\n",
      "Step: [3000] d_loss: 0.86987740, g_loss: 0.87611371\n",
      "Step: [3001] d_loss: 0.75168300, g_loss: 0.93196201\n",
      "Step: [3002] d_loss: 0.69985694, g_loss: 0.98777831\n",
      "Step: [3003] d_loss: 0.73540288, g_loss: 0.89629894\n",
      "Step: [3004] d_loss: 0.73263717, g_loss: 0.89687800\n",
      "Step: [3005] d_loss: 0.78958935, g_loss: 0.88673633\n",
      "Step: [3006] d_loss: 0.69881451, g_loss: 0.95748955\n",
      "Step: [3007] d_loss: 0.79141307, g_loss: 0.86776358\n",
      "Step: [3008] d_loss: 0.81188732, g_loss: 0.88875854\n",
      "Step: [3009] d_loss: 0.72970223, g_loss: 0.92753750\n",
      "Step: [3010] d_loss: 0.79748148, g_loss: 0.92191076\n",
      "Step: [3011] d_loss: 0.73770380, g_loss: 0.93138790\n",
      "Step: [3012] d_loss: 0.78478664, g_loss: 0.91303891\n",
      "Step: [3013] d_loss: 0.67884755, g_loss: 0.86617672\n",
      "Step: [3014] d_loss: 0.69182146, g_loss: 0.86974269\n",
      "Step: [3015] d_loss: 0.66887528, g_loss: 0.86125517\n",
      "Step: [3016] d_loss: 0.81769782, g_loss: 0.85641479\n",
      "Step: [3017] d_loss: 0.78791934, g_loss: 0.88209361\n",
      "Step: [3018] d_loss: 0.77071333, g_loss: 0.87681389\n",
      "Step: [3019] d_loss: 0.77493548, g_loss: 0.91671360\n",
      "Step: [3020] d_loss: 0.68481636, g_loss: 0.90682548\n",
      "Step: [3021] d_loss: 0.75104040, g_loss: 0.91996336\n",
      "Step: [3022] d_loss: 0.77611887, g_loss: 0.92411608\n",
      "Step: [3023] d_loss: 0.85577679, g_loss: 0.96209359\n",
      "Step: [3024] d_loss: 0.71088445, g_loss: 0.87945521\n",
      "Step: [3025] d_loss: 0.84119374, g_loss: 0.89260387\n",
      "Step: [3026] d_loss: 0.85061735, g_loss: 0.91893101\n",
      "Step: [3027] d_loss: 0.69830418, g_loss: 0.93984002\n",
      "Step: [3028] d_loss: 0.75253791, g_loss: 0.90414160\n",
      "Step: [3029] d_loss: 0.73655558, g_loss: 0.98485398\n",
      "Step: [3030] d_loss: 0.80770278, g_loss: 0.94802976\n",
      "Step: [3031] d_loss: 0.70803869, g_loss: 0.92596298\n",
      "Step: [3032] d_loss: 0.72270262, g_loss: 0.88001442\n",
      "Step: [3033] d_loss: 0.75055009, g_loss: 0.85389483\n",
      "Step: [3034] d_loss: 0.66028702, g_loss: 0.93320423\n",
      "Step: [3035] d_loss: 0.82396990, g_loss: 0.87276894\n",
      "Step: [3036] d_loss: 0.80347997, g_loss: 0.91584015\n",
      "Step: [3037] d_loss: 0.75213146, g_loss: 0.92531627\n",
      "Step: [3038] d_loss: 0.81351721, g_loss: 0.95508742\n",
      "Step: [3039] d_loss: 0.63146681, g_loss: 0.91934204\n",
      "Step: [3040] d_loss: 0.71108752, g_loss: 0.96280354\n",
      "Step: [3041] d_loss: 0.69625205, g_loss: 0.93920749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3042] d_loss: 0.86307091, g_loss: 0.89534086\n",
      "Step: [3043] d_loss: 0.76697516, g_loss: 0.86837065\n",
      "Step: [3044] d_loss: 0.73932731, g_loss: 0.94411087\n",
      "Step: [3045] d_loss: 0.75629044, g_loss: 0.93256754\n",
      "Step: [3046] d_loss: 0.78725207, g_loss: 0.96305531\n",
      "Step: [3047] d_loss: 0.74639904, g_loss: 0.98695636\n",
      "Step: [3048] d_loss: 0.76251781, g_loss: 0.89929336\n",
      "Step: [3049] d_loss: 0.70361990, g_loss: 0.88266653\n",
      "Step: [3050] d_loss: 0.75653142, g_loss: 0.89874661\n",
      "Step: [3051] d_loss: 0.74633741, g_loss: 0.85915112\n",
      "Step: [3052] d_loss: 0.70966917, g_loss: 0.90602553\n",
      "Step: [3053] d_loss: 0.72926414, g_loss: 0.88638479\n",
      "Step: [3054] d_loss: 0.81823027, g_loss: 0.91640812\n",
      "Step: [3055] d_loss: 0.71135682, g_loss: 0.97637457\n",
      "Step: [3056] d_loss: 0.79434395, g_loss: 0.90732419\n",
      "Step: [3057] d_loss: 0.85497457, g_loss: 0.86792195\n",
      "Step: [3058] d_loss: 0.74795407, g_loss: 0.91029686\n",
      "Step: [3059] d_loss: 0.76679093, g_loss: 0.87116331\n",
      "Step: [3060] d_loss: 0.69627929, g_loss: 0.91195846\n",
      "Step: [3061] d_loss: 0.76744121, g_loss: 0.93595445\n",
      "Step: [3062] d_loss: 0.74151182, g_loss: 0.98358500\n",
      "Step: [3063] d_loss: 0.73531044, g_loss: 0.95816857\n",
      "Step: [3064] d_loss: 0.70908004, g_loss: 0.91407585\n",
      "Step: [3065] d_loss: 0.84503615, g_loss: 0.88409883\n",
      "Step: [3066] d_loss: 0.71305251, g_loss: 0.92268354\n",
      "Step: [3067] d_loss: 0.81973594, g_loss: 0.91942114\n",
      "Step: [3068] d_loss: 0.79314142, g_loss: 0.99123228\n",
      "Step: [3069] d_loss: 0.69555885, g_loss: 0.90578717\n",
      "Step: [3070] d_loss: 0.69139403, g_loss: 0.91137135\n",
      "Step: [3071] d_loss: 0.64266807, g_loss: 0.91464949\n",
      "Step: [3072] d_loss: 0.76225251, g_loss: 0.88893259\n",
      "Step: [3073] d_loss: 0.70640582, g_loss: 0.88069004\n",
      "Step: [3074] d_loss: 0.68771285, g_loss: 0.96931326\n",
      "Step: [3075] d_loss: 0.69070882, g_loss: 0.91153568\n",
      "Step: [3076] d_loss: 0.76835299, g_loss: 0.89780593\n",
      "Step: [3077] d_loss: 0.72742444, g_loss: 0.92938197\n",
      "Step: [3078] d_loss: 0.69250143, g_loss: 0.96350795\n",
      "Step: [3079] d_loss: 0.72804636, g_loss: 0.94728136\n",
      "Step: [3080] d_loss: 0.77337593, g_loss: 0.91857243\n",
      "Step: [3081] d_loss: 0.81368315, g_loss: 0.94780320\n",
      "Step: [3082] d_loss: 0.79397994, g_loss: 0.92530060\n",
      "Step: [3083] d_loss: 0.76021332, g_loss: 0.91776609\n",
      "Step: [3084] d_loss: 0.83332652, g_loss: 0.83443785\n",
      "Step: [3085] d_loss: 0.82183921, g_loss: 0.90981901\n",
      "Step: [3086] d_loss: 0.72317016, g_loss: 0.93665141\n",
      "Step: [3087] d_loss: 0.74744093, g_loss: 0.96039057\n",
      "Step: [3088] d_loss: 0.70195436, g_loss: 0.99080586\n",
      "Step: [3089] d_loss: 0.73195350, g_loss: 0.92264724\n",
      "Step: [3090] d_loss: 0.77708644, g_loss: 0.97460598\n",
      "Step: [3091] d_loss: 0.72347689, g_loss: 0.96429735\n",
      "Step: [3092] d_loss: 0.73799759, g_loss: 0.88089907\n",
      "Step: [3093] d_loss: 0.75727457, g_loss: 0.86705941\n",
      "Step: [3094] d_loss: 0.77822316, g_loss: 0.92258692\n",
      "Step: [3095] d_loss: 0.76286995, g_loss: 0.87028122\n",
      "Step: [3096] d_loss: 0.80587256, g_loss: 0.88749987\n",
      "Step: [3097] d_loss: 0.75562644, g_loss: 0.96702462\n",
      "Step: [3098] d_loss: 0.71393013, g_loss: 0.96597606\n",
      "Step: [3099] d_loss: 0.78133434, g_loss: 0.90861201\n",
      "Step: [3100] d_loss: 0.78253883, g_loss: 0.94858992\n",
      "Step: [3101] d_loss: 0.74784213, g_loss: 0.93558246\n",
      "Step: [3102] d_loss: 0.80611104, g_loss: 0.94955462\n",
      "Step: [3103] d_loss: 0.72848481, g_loss: 0.94226766\n",
      "Step: [3104] d_loss: 0.79796010, g_loss: 0.86449635\n",
      "Step: [3105] d_loss: 0.75520235, g_loss: 0.94570345\n",
      "Step: [3106] d_loss: 0.73195791, g_loss: 0.95186198\n",
      "Step: [3107] d_loss: 0.71099317, g_loss: 0.89005148\n",
      "Step: [3108] d_loss: 0.68533486, g_loss: 0.93750906\n",
      "Step: [3109] d_loss: 0.70906585, g_loss: 0.96317148\n",
      "Step: [3110] d_loss: 0.73687142, g_loss: 0.92942023\n",
      "Step: [3111] d_loss: 0.72292322, g_loss: 0.97609633\n",
      "Step: [3112] d_loss: 0.73855972, g_loss: 0.92475951\n",
      "Step: [3113] d_loss: 0.70125580, g_loss: 0.91423810\n",
      "Step: [3114] d_loss: 0.72905463, g_loss: 0.88311309\n",
      "Step: [3115] d_loss: 0.78520471, g_loss: 0.89376587\n",
      "Step: [3116] d_loss: 0.78783029, g_loss: 0.91437757\n",
      "Step: [3117] d_loss: 0.75236380, g_loss: 0.98631412\n",
      "Step: [3118] d_loss: 0.77014494, g_loss: 0.93000329\n",
      "Step: [3119] d_loss: 0.67393160, g_loss: 0.95650381\n",
      "Step: [3120] d_loss: 0.79425597, g_loss: 0.93547678\n",
      "Step: [3121] d_loss: 0.78378236, g_loss: 0.90696323\n",
      "Step: [3122] d_loss: 0.81720698, g_loss: 0.87855542\n",
      "Step: [3123] d_loss: 0.73870921, g_loss: 0.93969733\n",
      "Step: [3124] d_loss: 0.82047969, g_loss: 0.92262393\n",
      "Step: [3125] d_loss: 0.83048093, g_loss: 0.92665100\n",
      "Step: [3126] d_loss: 0.73850697, g_loss: 0.87243205\n",
      "Step: [3127] d_loss: 0.72365141, g_loss: 0.95462972\n",
      "Step: [3128] d_loss: 0.71663707, g_loss: 0.96543986\n",
      "Step: [3129] d_loss: 0.84940118, g_loss: 0.87125611\n",
      "Step: [3130] d_loss: 0.76977056, g_loss: 0.91115916\n",
      "Step: [3131] d_loss: 0.86983359, g_loss: 0.85034591\n",
      "Step: [3132] d_loss: 0.72367626, g_loss: 0.94900221\n",
      "Step: [3133] d_loss: 0.78967661, g_loss: 0.88078547\n",
      "Step: [3134] d_loss: 0.87029225, g_loss: 0.91697371\n",
      "Step: [3135] d_loss: 0.74339098, g_loss: 0.94514376\n",
      "Step: [3136] d_loss: 0.78676260, g_loss: 0.89317590\n",
      "Step: [3137] d_loss: 0.72700965, g_loss: 0.92928851\n",
      "Step: [3138] d_loss: 0.74293250, g_loss: 0.96311277\n",
      "Step: [3139] d_loss: 0.78208840, g_loss: 0.89838767\n",
      "Step: [3140] d_loss: 0.81709194, g_loss: 0.91620731\n",
      "Step: [3141] d_loss: 0.69890845, g_loss: 0.97335804\n",
      "Step: [3142] d_loss: 0.80936390, g_loss: 0.93026459\n",
      "Step: [3143] d_loss: 0.72748631, g_loss: 0.98909128\n",
      "Step: [3144] d_loss: 0.72251880, g_loss: 0.93983912\n",
      "Step: [3145] d_loss: 0.66374677, g_loss: 0.88965678\n",
      "Step: [3146] d_loss: 0.77210563, g_loss: 0.90855742\n",
      "Step: [3147] d_loss: 0.66671300, g_loss: 0.97216135\n",
      "Step: [3148] d_loss: 0.79421294, g_loss: 0.92805547\n",
      "Step: [3149] d_loss: 0.66991878, g_loss: 0.98999631\n",
      "Step: [3150] d_loss: 0.71604645, g_loss: 0.89890152\n",
      "Step: [3151] d_loss: 0.75688684, g_loss: 0.91750669\n",
      "Step: [3152] d_loss: 0.71963477, g_loss: 0.92706382\n",
      "Step: [3153] d_loss: 0.68871808, g_loss: 0.86824131\n",
      "Step: [3154] d_loss: 0.75103080, g_loss: 0.89905614\n",
      "Step: [3155] d_loss: 0.70224136, g_loss: 0.89786601\n",
      "Step: [3156] d_loss: 0.82695085, g_loss: 0.87473702\n",
      "Step: [3157] d_loss: 0.83307397, g_loss: 0.88237149\n",
      "Step: [3158] d_loss: 0.73803818, g_loss: 0.93679976\n",
      "Step: [3159] d_loss: 0.79386699, g_loss: 0.93416059\n",
      "Step: [3160] d_loss: 0.67328060, g_loss: 0.92765689\n",
      "Step: [3161] d_loss: 0.81012529, g_loss: 0.90018189\n",
      "Step: [3162] d_loss: 0.68836266, g_loss: 0.98248482\n",
      "Step: [3163] d_loss: 0.70772773, g_loss: 0.85421419\n",
      "Step: [3164] d_loss: 0.75656140, g_loss: 0.93642229\n",
      "Step: [3165] d_loss: 0.74982697, g_loss: 0.88622212\n",
      "Step: [3166] d_loss: 0.75168192, g_loss: 0.95021689\n",
      "Step: [3167] d_loss: 0.84902632, g_loss: 0.91613096\n",
      "Step: [3168] d_loss: 0.76412678, g_loss: 0.95638722\n",
      "Step: [3169] d_loss: 0.75845498, g_loss: 0.95483255\n",
      "Step: [3170] d_loss: 0.79387659, g_loss: 0.96068966\n",
      "Step: [3171] d_loss: 0.79700559, g_loss: 0.95962399\n",
      "Step: [3172] d_loss: 0.74134368, g_loss: 0.93932569\n",
      "Step: [3173] d_loss: 0.77200711, g_loss: 0.93231499\n",
      "Step: [3174] d_loss: 0.75755167, g_loss: 0.94955987\n",
      "Step: [3175] d_loss: 0.72502494, g_loss: 0.93389469\n",
      "Step: [3176] d_loss: 0.73155260, g_loss: 0.97553682\n",
      "Step: [3177] d_loss: 0.72459185, g_loss: 0.96109456\n",
      "Step: [3178] d_loss: 0.74265438, g_loss: 0.94226247\n",
      "Step: [3179] d_loss: 0.86886561, g_loss: 0.92815459\n",
      "Step: [3180] d_loss: 0.71920592, g_loss: 0.94326949\n",
      "Step: [3181] d_loss: 0.68719763, g_loss: 0.89900154\n",
      "Step: [3182] d_loss: 0.68017995, g_loss: 0.93795878\n",
      "Step: [3183] d_loss: 0.75680977, g_loss: 0.84636974\n",
      "Step: [3184] d_loss: 0.77767414, g_loss: 0.89232206\n",
      "Step: [3185] d_loss: 0.73640835, g_loss: 0.91598457\n",
      "Step: [3186] d_loss: 0.77276689, g_loss: 0.91382641\n",
      "Step: [3187] d_loss: 0.74198353, g_loss: 0.93013048\n",
      "Step: [3188] d_loss: 0.75290138, g_loss: 0.92339653\n",
      "Step: [3189] d_loss: 0.74957836, g_loss: 0.91986567\n",
      "Step: [3190] d_loss: 0.77494848, g_loss: 0.87149000\n",
      "Step: [3191] d_loss: 0.67996365, g_loss: 0.90958691\n",
      "Step: [3192] d_loss: 0.80676013, g_loss: 0.90592062\n",
      "Step: [3193] d_loss: 0.71795553, g_loss: 0.89140666\n",
      "Step: [3194] d_loss: 0.77479678, g_loss: 0.92208517\n",
      "Step: [3195] d_loss: 0.76656526, g_loss: 0.94849420\n",
      "Step: [3196] d_loss: 0.87170488, g_loss: 0.86343920\n",
      "Step: [3197] d_loss: 0.78025943, g_loss: 0.93146384\n",
      "Step: [3198] d_loss: 0.74883199, g_loss: 0.93927020\n",
      "Step: [3199] d_loss: 0.76577032, g_loss: 0.96834493\n",
      "Step: [3200] d_loss: 0.83044642, g_loss: 0.90467423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3201] d_loss: 0.73156691, g_loss: 0.88995761\n",
      "Step: [3202] d_loss: 0.67370075, g_loss: 0.92300808\n",
      "Step: [3203] d_loss: 0.75307858, g_loss: 0.89537883\n",
      "Step: [3204] d_loss: 0.73673290, g_loss: 0.86280626\n",
      "Step: [3205] d_loss: 0.71197838, g_loss: 0.87769783\n",
      "Step: [3206] d_loss: 0.80917430, g_loss: 0.88980877\n",
      "Step: [3207] d_loss: 0.75697082, g_loss: 0.90736258\n",
      "Step: [3208] d_loss: 0.70678586, g_loss: 0.93453383\n",
      "Step: [3209] d_loss: 0.76820260, g_loss: 0.92885518\n",
      "Step: [3210] d_loss: 0.74484664, g_loss: 0.94767451\n",
      "Step: [3211] d_loss: 0.80230236, g_loss: 0.93706512\n",
      "Step: [3212] d_loss: 0.74171501, g_loss: 0.93169093\n",
      "Step: [3213] d_loss: 0.81949830, g_loss: 0.94267631\n",
      "Step: [3214] d_loss: 0.72372007, g_loss: 0.95558816\n",
      "Step: [3215] d_loss: 0.69778544, g_loss: 0.90953779\n",
      "Step: [3216] d_loss: 0.74593866, g_loss: 0.91791517\n",
      "Step: [3217] d_loss: 0.78709340, g_loss: 0.86821997\n",
      "Step: [3218] d_loss: 0.77279675, g_loss: 0.89628726\n",
      "Step: [3219] d_loss: 0.74618387, g_loss: 0.89438468\n",
      "Step: [3220] d_loss: 0.69286859, g_loss: 0.90012580\n",
      "Step: [3221] d_loss: 0.76309586, g_loss: 0.90894198\n",
      "Step: [3222] d_loss: 0.84828573, g_loss: 0.89747584\n",
      "Step: [3223] d_loss: 0.74935782, g_loss: 0.89836383\n",
      "Step: [3224] d_loss: 0.68144941, g_loss: 0.97905290\n",
      "Step: [3225] d_loss: 0.79423118, g_loss: 0.98612177\n",
      "Step: [3226] d_loss: 0.79234213, g_loss: 0.89400446\n",
      "Step: [3227] d_loss: 0.76880687, g_loss: 0.89429581\n",
      "Step: [3228] d_loss: 0.73974484, g_loss: 0.88723683\n",
      "Step: [3229] d_loss: 0.69714838, g_loss: 0.94391882\n",
      "Step: [3230] d_loss: 0.74086100, g_loss: 0.91981387\n",
      "Step: [3231] d_loss: 0.72962898, g_loss: 0.90753704\n",
      "Step: [3232] d_loss: 0.80664831, g_loss: 0.87441802\n",
      "Step: [3233] d_loss: 0.71026057, g_loss: 0.91101325\n",
      "Step: [3234] d_loss: 0.73866141, g_loss: 0.89453220\n",
      "Step: [3235] d_loss: 0.75139016, g_loss: 0.91985595\n",
      "Step: [3236] d_loss: 0.76601809, g_loss: 0.95472836\n",
      "Step: [3237] d_loss: 0.81934929, g_loss: 0.86898702\n",
      "Step: [3238] d_loss: 0.69230247, g_loss: 0.90476817\n",
      "Step: [3239] d_loss: 0.74536461, g_loss: 0.94092983\n",
      "Step: [3240] d_loss: 0.77171779, g_loss: 0.94213164\n",
      "Step: [3241] d_loss: 0.84271294, g_loss: 0.88679940\n",
      "Step: [3242] d_loss: 0.74550068, g_loss: 0.91102415\n",
      "Step: [3243] d_loss: 0.81796986, g_loss: 0.87230402\n",
      "Step: [3244] d_loss: 0.73642439, g_loss: 0.86800051\n",
      "Step: [3245] d_loss: 0.76492125, g_loss: 0.97449547\n",
      "Step: [3246] d_loss: 0.70798683, g_loss: 0.92807823\n",
      "Step: [3247] d_loss: 0.73862988, g_loss: 0.90434229\n",
      "Step: [3248] d_loss: 0.70491135, g_loss: 0.88009858\n",
      "Step: [3249] d_loss: 0.78692043, g_loss: 0.92585671\n",
      "Step: [3250] d_loss: 0.76250094, g_loss: 0.92468077\n",
      "Step: [3251] d_loss: 0.75527620, g_loss: 0.88322288\n",
      "Step: [3252] d_loss: 0.70549494, g_loss: 0.92813724\n",
      "Step: [3253] d_loss: 0.70055348, g_loss: 0.90352792\n",
      "Step: [3254] d_loss: 0.71595967, g_loss: 0.90314806\n",
      "Step: [3255] d_loss: 0.72998434, g_loss: 0.92075264\n",
      "Step: [3256] d_loss: 0.68157476, g_loss: 0.94000047\n",
      "Step: [3257] d_loss: 0.78051299, g_loss: 0.95080012\n",
      "Step: [3258] d_loss: 0.70193726, g_loss: 0.92505103\n",
      "Step: [3259] d_loss: 0.76604080, g_loss: 0.89864439\n",
      "Step: [3260] d_loss: 0.71309608, g_loss: 0.92075241\n",
      "Step: [3261] d_loss: 0.78518063, g_loss: 0.89985472\n",
      "Step: [3262] d_loss: 0.75665081, g_loss: 0.90350324\n",
      "Step: [3263] d_loss: 0.79931146, g_loss: 0.95225179\n",
      "Step: [3264] d_loss: 0.75515586, g_loss: 0.88253468\n",
      "Step: [3265] d_loss: 0.83286905, g_loss: 0.87648070\n",
      "Step: [3266] d_loss: 0.64362705, g_loss: 0.96721303\n",
      "Step: [3267] d_loss: 0.66627437, g_loss: 0.91203415\n",
      "Step: [3268] d_loss: 0.77687287, g_loss: 0.88149691\n",
      "Step: [3269] d_loss: 0.76271307, g_loss: 0.86688179\n",
      "Step: [3270] d_loss: 0.78738528, g_loss: 0.83323038\n",
      "Step: [3271] d_loss: 0.64789510, g_loss: 0.85747784\n",
      "Step: [3272] d_loss: 0.69194973, g_loss: 0.88655949\n",
      "Step: [3273] d_loss: 0.74857545, g_loss: 0.83999389\n",
      "Step: [3274] d_loss: 0.70083648, g_loss: 0.91341680\n",
      "Step: [3275] d_loss: 0.76818699, g_loss: 0.84782034\n",
      "Step: [3276] d_loss: 0.82219547, g_loss: 0.86276615\n",
      "Step: [3277] d_loss: 0.73883533, g_loss: 0.93284446\n",
      "Step: [3278] d_loss: 0.71895498, g_loss: 0.95435488\n",
      "Step: [3279] d_loss: 0.72931099, g_loss: 0.92255902\n",
      "Step: [3280] d_loss: 0.73000205, g_loss: 0.90627766\n",
      "Step: [3281] d_loss: 0.65317959, g_loss: 0.84458411\n",
      "Step: [3282] d_loss: 0.78221780, g_loss: 0.88295746\n",
      "Step: [3283] d_loss: 0.88002056, g_loss: 0.84538811\n",
      "Step: [3284] d_loss: 0.75406784, g_loss: 0.87532645\n",
      "Step: [3285] d_loss: 0.74743330, g_loss: 0.91384017\n",
      "Step: [3286] d_loss: 0.85549593, g_loss: 0.90164709\n",
      "Step: [3287] d_loss: 0.78096002, g_loss: 0.97266519\n",
      "Step: [3288] d_loss: 0.73840004, g_loss: 0.99242580\n",
      "Step: [3289] d_loss: 0.71012759, g_loss: 0.91782528\n",
      "Step: [3290] d_loss: 0.72661388, g_loss: 0.85142142\n",
      "Step: [3291] d_loss: 0.70368695, g_loss: 0.90646684\n",
      "Step: [3292] d_loss: 0.75616813, g_loss: 0.91464061\n",
      "Step: [3293] d_loss: 0.76168525, g_loss: 0.91918552\n",
      "Step: [3294] d_loss: 0.70288414, g_loss: 0.89028001\n",
      "Step: [3295] d_loss: 0.75622416, g_loss: 0.92845958\n",
      "Step: [3296] d_loss: 0.70409620, g_loss: 0.88387114\n",
      "Step: [3297] d_loss: 0.73511416, g_loss: 0.93920207\n",
      "Step: [3298] d_loss: 0.78467196, g_loss: 0.89832288\n",
      "Step: [3299] d_loss: 0.83156282, g_loss: 0.88944930\n",
      "Step: [3300] d_loss: 0.69066793, g_loss: 0.90461010\n",
      "Step: [3301] d_loss: 0.78968257, g_loss: 0.92236388\n",
      "Step: [3302] d_loss: 0.75279379, g_loss: 0.88516766\n",
      "Step: [3303] d_loss: 0.76068711, g_loss: 0.87596202\n",
      "Step: [3304] d_loss: 0.67384517, g_loss: 0.92007142\n",
      "Step: [3305] d_loss: 0.73424876, g_loss: 0.90675098\n",
      "Step: [3306] d_loss: 0.67085803, g_loss: 0.92143244\n",
      "Step: [3307] d_loss: 0.74921823, g_loss: 0.92717695\n",
      "Step: [3308] d_loss: 0.70545614, g_loss: 0.90822136\n",
      "Step: [3309] d_loss: 0.76949900, g_loss: 0.86439693\n",
      "Step: [3310] d_loss: 0.75481802, g_loss: 0.87772447\n",
      "Step: [3311] d_loss: 0.80403441, g_loss: 0.89722157\n",
      "Step: [3312] d_loss: 0.72735226, g_loss: 0.91729140\n",
      "Step: [3313] d_loss: 0.68144053, g_loss: 0.95640230\n",
      "Step: [3314] d_loss: 0.68679255, g_loss: 0.96145988\n",
      "Step: [3315] d_loss: 0.76923209, g_loss: 0.91473716\n",
      "Step: [3316] d_loss: 0.77027231, g_loss: 0.90054333\n",
      "Step: [3317] d_loss: 0.72643399, g_loss: 0.94059563\n",
      "Step: [3318] d_loss: 0.74366742, g_loss: 0.95002168\n",
      "Step: [3319] d_loss: 0.80709624, g_loss: 0.94569993\n",
      "Step: [3320] d_loss: 0.69555491, g_loss: 0.94810009\n",
      "Step: [3321] d_loss: 0.69315457, g_loss: 0.90302145\n",
      "Step: [3322] d_loss: 0.71746475, g_loss: 0.93509960\n",
      "Step: [3323] d_loss: 0.74721980, g_loss: 0.93828118\n",
      "Step: [3324] d_loss: 0.69641709, g_loss: 0.96280992\n",
      "Step: [3325] d_loss: 0.66828495, g_loss: 0.88035160\n",
      "Step: [3326] d_loss: 0.71830249, g_loss: 0.85338181\n",
      "Step: [3327] d_loss: 0.77493250, g_loss: 0.84828520\n",
      "Step: [3328] d_loss: 0.81193352, g_loss: 0.80526012\n",
      "Step: [3329] d_loss: 0.71747100, g_loss: 0.85006112\n",
      "Step: [3330] d_loss: 0.67065543, g_loss: 0.89063066\n",
      "Step: [3331] d_loss: 0.72034055, g_loss: 0.91946107\n",
      "Step: [3332] d_loss: 0.80113411, g_loss: 0.94132483\n",
      "Step: [3333] d_loss: 0.71774691, g_loss: 0.95595801\n",
      "Step: [3334] d_loss: 0.70515585, g_loss: 0.92829013\n",
      "Step: [3335] d_loss: 0.72967166, g_loss: 0.91321683\n",
      "Step: [3336] d_loss: 0.65949368, g_loss: 0.93706447\n",
      "Step: [3337] d_loss: 0.80467027, g_loss: 0.83883923\n",
      "Step: [3338] d_loss: 0.69075900, g_loss: 0.90319341\n",
      "Step: [3339] d_loss: 0.72420359, g_loss: 0.92976838\n",
      "Step: [3340] d_loss: 0.72402835, g_loss: 0.93286741\n",
      "Step: [3341] d_loss: 0.75461078, g_loss: 0.93441445\n",
      "Step: [3342] d_loss: 0.87167525, g_loss: 0.91498315\n",
      "Step: [3343] d_loss: 0.64659667, g_loss: 0.97877240\n",
      "Step: [3344] d_loss: 0.75731057, g_loss: 0.96268439\n",
      "Step: [3345] d_loss: 0.75358814, g_loss: 0.95784873\n",
      "Step: [3346] d_loss: 0.77264619, g_loss: 0.91220802\n",
      "Step: [3347] d_loss: 0.70802212, g_loss: 0.94654393\n",
      "Step: [3348] d_loss: 0.71740741, g_loss: 0.96991229\n",
      "Step: [3349] d_loss: 0.78164965, g_loss: 0.95076758\n",
      "Step: [3350] d_loss: 0.70905721, g_loss: 0.96818274\n",
      "Step: [3351] d_loss: 0.67595667, g_loss: 0.92987376\n",
      "Step: [3352] d_loss: 0.71805972, g_loss: 0.92971790\n",
      "Step: [3353] d_loss: 0.68866038, g_loss: 0.98605418\n",
      "Step: [3354] d_loss: 0.81358325, g_loss: 0.95809382\n",
      "Step: [3355] d_loss: 0.71809685, g_loss: 0.99322444\n",
      "Step: [3356] d_loss: 0.76195514, g_loss: 0.92583621\n",
      "Step: [3357] d_loss: 0.75898260, g_loss: 0.91978228\n",
      "Step: [3358] d_loss: 0.74398434, g_loss: 0.93940789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3359] d_loss: 0.77913815, g_loss: 1.02065372\n",
      "Step: [3360] d_loss: 0.71439755, g_loss: 0.96952969\n",
      "Step: [3361] d_loss: 0.78400242, g_loss: 0.90425742\n",
      "Step: [3362] d_loss: 0.78801638, g_loss: 0.86333412\n",
      "Step: [3363] d_loss: 0.73698586, g_loss: 0.89426851\n",
      "Step: [3364] d_loss: 0.70314419, g_loss: 0.98240030\n",
      "Step: [3365] d_loss: 0.74010539, g_loss: 0.95783091\n",
      "Step: [3366] d_loss: 0.84408158, g_loss: 0.87832016\n",
      "Step: [3367] d_loss: 0.75182253, g_loss: 0.96543831\n",
      "Step: [3368] d_loss: 0.77165008, g_loss: 1.01088357\n",
      "Step: [3369] d_loss: 0.77803612, g_loss: 0.93668848\n",
      "Step: [3370] d_loss: 0.73043841, g_loss: 0.93581665\n",
      "Step: [3371] d_loss: 0.75012946, g_loss: 0.92006069\n",
      "Step: [3372] d_loss: 0.68657732, g_loss: 0.92417341\n",
      "Step: [3373] d_loss: 0.72447395, g_loss: 0.90648234\n",
      "Step: [3374] d_loss: 0.66303378, g_loss: 0.86475444\n",
      "Step: [3375] d_loss: 0.72243822, g_loss: 0.91081089\n",
      "Step: [3376] d_loss: 0.69531465, g_loss: 0.93180931\n",
      "Step: [3377] d_loss: 0.69397968, g_loss: 0.92286450\n",
      "Step: [3378] d_loss: 0.79463172, g_loss: 0.89084810\n",
      "Step: [3379] d_loss: 0.69372797, g_loss: 0.94526696\n",
      "Step: [3380] d_loss: 0.74856734, g_loss: 0.91521078\n",
      "Step: [3381] d_loss: 0.73754096, g_loss: 0.90509176\n",
      "Step: [3382] d_loss: 0.68909818, g_loss: 0.88746387\n",
      "Step: [3383] d_loss: 0.76896977, g_loss: 0.89645267\n",
      "Step: [3384] d_loss: 0.68599260, g_loss: 0.91976643\n",
      "Step: [3385] d_loss: 0.72096223, g_loss: 0.86796051\n",
      "Step: [3386] d_loss: 0.72930819, g_loss: 0.92443317\n",
      "Step: [3387] d_loss: 0.68263185, g_loss: 0.93487936\n",
      "Step: [3388] d_loss: 0.78168428, g_loss: 0.93685412\n",
      "Step: [3389] d_loss: 0.68464535, g_loss: 0.92754084\n",
      "Step: [3390] d_loss: 0.72447991, g_loss: 0.91368306\n",
      "Step: [3391] d_loss: 0.69261730, g_loss: 0.88762552\n",
      "Step: [3392] d_loss: 0.69115990, g_loss: 0.88157868\n",
      "Step: [3393] d_loss: 0.80567741, g_loss: 0.84286553\n",
      "Step: [3394] d_loss: 0.74292016, g_loss: 0.89620370\n",
      "Step: [3395] d_loss: 0.74805671, g_loss: 0.90218425\n",
      "Step: [3396] d_loss: 0.71989894, g_loss: 0.90648150\n",
      "Step: [3397] d_loss: 0.77308273, g_loss: 0.93767345\n",
      "Step: [3398] d_loss: 0.76900184, g_loss: 0.88757432\n",
      "Step: [3399] d_loss: 0.76015955, g_loss: 0.92820078\n",
      "Step: [3400] d_loss: 0.77732545, g_loss: 0.95343971\n",
      "Step: [3401] d_loss: 0.70348072, g_loss: 0.87470633\n",
      "Step: [3402] d_loss: 0.72269380, g_loss: 0.87552255\n",
      "Step: [3403] d_loss: 0.74220002, g_loss: 0.91364753\n",
      "Step: [3404] d_loss: 0.72405303, g_loss: 0.90062416\n",
      "Step: [3405] d_loss: 0.80298632, g_loss: 0.87996835\n",
      "Step: [3406] d_loss: 0.83495694, g_loss: 0.86391854\n",
      "Step: [3407] d_loss: 0.74216723, g_loss: 0.92536891\n",
      "Step: [3408] d_loss: 0.79694086, g_loss: 0.84785998\n",
      "Step: [3409] d_loss: 0.82446319, g_loss: 0.84970164\n",
      "Step: [3410] d_loss: 0.67615485, g_loss: 0.94096452\n",
      "Step: [3411] d_loss: 0.78170043, g_loss: 0.86193269\n",
      "Step: [3412] d_loss: 0.70172566, g_loss: 0.94261819\n",
      "Step: [3413] d_loss: 0.69132221, g_loss: 0.87959552\n",
      "Step: [3414] d_loss: 0.75981700, g_loss: 0.87588090\n",
      "Step: [3415] d_loss: 0.75810790, g_loss: 0.89021379\n",
      "Step: [3416] d_loss: 0.86216748, g_loss: 0.83857524\n",
      "Step: [3417] d_loss: 0.77302957, g_loss: 0.93864977\n",
      "Step: [3418] d_loss: 0.68021423, g_loss: 0.89044201\n",
      "Step: [3419] d_loss: 0.81402564, g_loss: 0.86722243\n",
      "Step: [3420] d_loss: 0.72623295, g_loss: 0.86094469\n",
      "Step: [3421] d_loss: 0.84570283, g_loss: 0.85273647\n",
      "Step: [3422] d_loss: 0.74752533, g_loss: 0.91966367\n",
      "Step: [3423] d_loss: 0.73227370, g_loss: 0.95513433\n",
      "Step: [3424] d_loss: 0.85722554, g_loss: 0.93830347\n",
      "Step: [3425] d_loss: 0.72742790, g_loss: 0.90570319\n",
      "Step: [3426] d_loss: 0.81651777, g_loss: 0.88567448\n",
      "Step: [3427] d_loss: 0.75386369, g_loss: 0.87181145\n",
      "Step: [3428] d_loss: 0.73171741, g_loss: 0.95809901\n",
      "Step: [3429] d_loss: 0.74363965, g_loss: 0.93764454\n",
      "Step: [3430] d_loss: 0.72714031, g_loss: 0.94352853\n",
      "Step: [3431] d_loss: 0.76217693, g_loss: 0.99076724\n",
      "Step: [3432] d_loss: 0.72540212, g_loss: 0.92880529\n",
      "Step: [3433] d_loss: 0.77268195, g_loss: 0.90553153\n",
      "Step: [3434] d_loss: 0.72593391, g_loss: 0.88392788\n",
      "Step: [3435] d_loss: 0.71457183, g_loss: 0.91158158\n",
      "Step: [3436] d_loss: 0.67831331, g_loss: 0.92308605\n",
      "Step: [3437] d_loss: 0.66287649, g_loss: 0.88527298\n",
      "Step: [3438] d_loss: 0.70318681, g_loss: 0.87713057\n",
      "Step: [3439] d_loss: 0.79292572, g_loss: 0.85743988\n",
      "Step: [3440] d_loss: 0.78592205, g_loss: 0.84069139\n",
      "Step: [3441] d_loss: 0.70757240, g_loss: 0.84001827\n",
      "Step: [3442] d_loss: 0.80227250, g_loss: 0.89492482\n",
      "Step: [3443] d_loss: 0.68307549, g_loss: 0.91973245\n",
      "Step: [3444] d_loss: 0.74164516, g_loss: 0.90261072\n",
      "Step: [3445] d_loss: 0.75662172, g_loss: 0.92077428\n",
      "Step: [3446] d_loss: 0.66092116, g_loss: 0.92132121\n",
      "Step: [3447] d_loss: 0.68611020, g_loss: 0.97465551\n",
      "Step: [3448] d_loss: 0.82396334, g_loss: 0.89633340\n",
      "Step: [3449] d_loss: 0.72483635, g_loss: 0.91800421\n",
      "Step: [3450] d_loss: 0.71132147, g_loss: 0.88659370\n",
      "Step: [3451] d_loss: 0.81008136, g_loss: 0.88010293\n",
      "Step: [3452] d_loss: 0.75223285, g_loss: 0.88216406\n",
      "Step: [3453] d_loss: 0.70770037, g_loss: 0.93855184\n",
      "Step: [3454] d_loss: 0.83983225, g_loss: 0.89564353\n",
      "Step: [3455] d_loss: 0.67795128, g_loss: 0.98492467\n",
      "Step: [3456] d_loss: 0.74185371, g_loss: 0.96918142\n",
      "Step: [3457] d_loss: 0.75239849, g_loss: 0.83347410\n",
      "Step: [3458] d_loss: 0.68360364, g_loss: 0.90502435\n",
      "Step: [3459] d_loss: 0.77824080, g_loss: 0.90452856\n",
      "Step: [3460] d_loss: 0.75898647, g_loss: 0.92435843\n",
      "Step: [3461] d_loss: 0.72857553, g_loss: 0.92579865\n",
      "Step: [3462] d_loss: 0.71193844, g_loss: 0.89179409\n",
      "Step: [3463] d_loss: 0.81453699, g_loss: 0.93390495\n",
      "Step: [3464] d_loss: 0.68090856, g_loss: 0.91522878\n",
      "Step: [3465] d_loss: 0.73283076, g_loss: 0.88820207\n",
      "Step: [3466] d_loss: 0.76436740, g_loss: 0.91658545\n",
      "Step: [3467] d_loss: 0.69611347, g_loss: 0.90967607\n",
      "Step: [3468] d_loss: 0.72420394, g_loss: 0.92875093\n",
      "Step: [3469] d_loss: 0.74056816, g_loss: 0.90967345\n",
      "Step: [3470] d_loss: 0.73886204, g_loss: 0.93484235\n",
      "Step: [3471] d_loss: 0.68694931, g_loss: 0.90664124\n",
      "Step: [3472] d_loss: 0.73637950, g_loss: 0.94241118\n",
      "Step: [3473] d_loss: 0.72389662, g_loss: 0.91108203\n",
      "Step: [3474] d_loss: 0.69963539, g_loss: 0.92906153\n",
      "Step: [3475] d_loss: 0.76375836, g_loss: 0.87668157\n",
      "Step: [3476] d_loss: 0.68896204, g_loss: 0.92239708\n",
      "Step: [3477] d_loss: 0.71083397, g_loss: 0.94130802\n",
      "Step: [3478] d_loss: 0.68919480, g_loss: 0.86871099\n",
      "Step: [3479] d_loss: 0.73148751, g_loss: 0.88982439\n",
      "Step: [3480] d_loss: 0.83397198, g_loss: 0.81378305\n",
      "Step: [3481] d_loss: 0.77460092, g_loss: 0.81933564\n",
      "Step: [3482] d_loss: 0.82318056, g_loss: 0.78643042\n",
      "Step: [3483] d_loss: 0.80853140, g_loss: 0.82734346\n",
      "Step: [3484] d_loss: 0.69953465, g_loss: 0.92108917\n",
      "Step: [3485] d_loss: 0.75143081, g_loss: 0.86880082\n",
      "Step: [3486] d_loss: 0.76823401, g_loss: 0.88602066\n",
      "Step: [3487] d_loss: 0.72384363, g_loss: 0.97118515\n",
      "Step: [3488] d_loss: 0.78767300, g_loss: 0.91718966\n",
      "Step: [3489] d_loss: 0.83590931, g_loss: 0.93055868\n",
      "Step: [3490] d_loss: 0.75997120, g_loss: 0.95458376\n",
      "Step: [3491] d_loss: 0.73990124, g_loss: 0.90786582\n",
      "Step: [3492] d_loss: 0.76533711, g_loss: 0.91885614\n",
      "Step: [3493] d_loss: 0.86659944, g_loss: 0.87087762\n",
      "Step: [3494] d_loss: 0.78385842, g_loss: 0.95141524\n",
      "Step: [3495] d_loss: 0.76994008, g_loss: 0.91626841\n",
      "Step: [3496] d_loss: 0.74028289, g_loss: 0.89368629\n",
      "Step: [3497] d_loss: 0.68223566, g_loss: 0.90203404\n",
      "Step: [3498] d_loss: 0.73802811, g_loss: 0.89843988\n",
      "Step: [3499] d_loss: 0.77003914, g_loss: 0.89001715\n",
      "Step: [3500] d_loss: 0.77994752, g_loss: 0.90353125\n",
      "Step: [3501] d_loss: 0.69055033, g_loss: 0.94177341\n",
      "Step: [3502] d_loss: 0.69692010, g_loss: 0.92922634\n",
      "Step: [3503] d_loss: 0.67973328, g_loss: 0.94290823\n",
      "Step: [3504] d_loss: 0.67797142, g_loss: 0.96630615\n",
      "Step: [3505] d_loss: 0.81254464, g_loss: 0.92505831\n",
      "Step: [3506] d_loss: 0.68155354, g_loss: 0.93493783\n",
      "Step: [3507] d_loss: 0.74457300, g_loss: 0.95319498\n",
      "Step: [3508] d_loss: 0.67370909, g_loss: 0.88346100\n",
      "Step: [3509] d_loss: 0.80881393, g_loss: 0.91593766\n",
      "Step: [3510] d_loss: 0.79982078, g_loss: 0.84011149\n",
      "Step: [3511] d_loss: 0.82456893, g_loss: 0.87756336\n",
      "Step: [3512] d_loss: 0.72663611, g_loss: 0.87460989\n",
      "Step: [3513] d_loss: 0.69344252, g_loss: 0.94658101\n",
      "Step: [3514] d_loss: 0.84755278, g_loss: 0.85655487\n",
      "Step: [3515] d_loss: 0.76341951, g_loss: 0.88175976\n",
      "Step: [3516] d_loss: 0.71810448, g_loss: 0.91429263\n",
      "Step: [3517] d_loss: 0.65710354, g_loss: 0.95764840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3518] d_loss: 0.71118051, g_loss: 0.98031545\n",
      "Step: [3519] d_loss: 0.79794109, g_loss: 0.92631310\n",
      "Step: [3520] d_loss: 0.71347368, g_loss: 0.90524489\n",
      "Step: [3521] d_loss: 0.72468376, g_loss: 0.92441428\n",
      "Step: [3522] d_loss: 0.70886415, g_loss: 0.90367407\n",
      "Step: [3523] d_loss: 0.70795232, g_loss: 0.92775071\n",
      "Step: [3524] d_loss: 0.70064688, g_loss: 0.94313288\n",
      "Step: [3525] d_loss: 0.69510448, g_loss: 0.98276699\n",
      "Step: [3526] d_loss: 0.77864909, g_loss: 0.96460032\n",
      "Step: [3527] d_loss: 0.77750719, g_loss: 0.87924504\n",
      "Step: [3528] d_loss: 0.71734715, g_loss: 0.92201161\n",
      "Step: [3529] d_loss: 0.71288460, g_loss: 0.90836668\n",
      "Step: [3530] d_loss: 0.69328821, g_loss: 0.92633182\n",
      "Step: [3531] d_loss: 0.68939507, g_loss: 0.89443481\n",
      "Step: [3532] d_loss: 0.74271107, g_loss: 0.92882407\n",
      "Step: [3533] d_loss: 0.73012775, g_loss: 0.86700380\n",
      "Step: [3534] d_loss: 0.76620698, g_loss: 0.92378736\n",
      "Step: [3535] d_loss: 0.86028606, g_loss: 0.89906085\n",
      "Step: [3536] d_loss: 0.91254485, g_loss: 0.89613539\n",
      "Step: [3537] d_loss: 0.71247143, g_loss: 0.88662028\n",
      "Step: [3538] d_loss: 0.81970251, g_loss: 0.90604711\n",
      "Step: [3539] d_loss: 0.67174691, g_loss: 0.92640632\n",
      "Step: [3540] d_loss: 0.79254395, g_loss: 0.88430947\n",
      "Step: [3541] d_loss: 0.81018811, g_loss: 0.91879165\n",
      "Step: [3542] d_loss: 0.67952913, g_loss: 0.96132326\n",
      "Step: [3543] d_loss: 0.76170796, g_loss: 0.93076664\n",
      "Step: [3544] d_loss: 0.76458585, g_loss: 0.97065061\n",
      "Step: [3545] d_loss: 0.70167565, g_loss: 0.92631465\n",
      "Step: [3546] d_loss: 0.74726373, g_loss: 0.92359334\n",
      "Step: [3547] d_loss: 0.82954198, g_loss: 0.88489008\n",
      "Step: [3548] d_loss: 0.77652693, g_loss: 0.90809226\n",
      "Step: [3549] d_loss: 0.73917896, g_loss: 0.95471352\n",
      "Step: [3550] d_loss: 0.76044297, g_loss: 0.93680745\n",
      "Step: [3551] d_loss: 0.76350367, g_loss: 0.93692726\n",
      "Step: [3552] d_loss: 0.69723606, g_loss: 0.94039708\n",
      "Step: [3553] d_loss: 0.80102754, g_loss: 0.94399321\n",
      "Step: [3554] d_loss: 0.68284225, g_loss: 0.90771210\n",
      "Step: [3555] d_loss: 0.67637831, g_loss: 0.91120642\n",
      "Step: [3556] d_loss: 0.74557316, g_loss: 0.88544303\n",
      "Step: [3557] d_loss: 0.72906238, g_loss: 0.90074188\n",
      "Step: [3558] d_loss: 0.72908813, g_loss: 0.90976310\n",
      "Step: [3559] d_loss: 0.69896895, g_loss: 0.86668932\n",
      "Step: [3560] d_loss: 0.72902548, g_loss: 0.92946988\n",
      "Step: [3561] d_loss: 0.75861931, g_loss: 0.90472454\n",
      "Step: [3562] d_loss: 0.73883533, g_loss: 0.94000727\n",
      "Step: [3563] d_loss: 0.71123850, g_loss: 0.89317083\n",
      "Step: [3564] d_loss: 0.71764725, g_loss: 0.93935877\n",
      "Step: [3565] d_loss: 0.67455846, g_loss: 0.88253909\n",
      "Step: [3566] d_loss: 0.69262457, g_loss: 0.93593740\n",
      "Step: [3567] d_loss: 0.68673581, g_loss: 0.90051860\n",
      "Step: [3568] d_loss: 0.68638259, g_loss: 0.95843130\n",
      "Step: [3569] d_loss: 0.76287651, g_loss: 0.93806523\n",
      "Step: [3570] d_loss: 0.76611620, g_loss: 0.94306606\n",
      "Step: [3571] d_loss: 0.76101857, g_loss: 0.86741304\n",
      "Step: [3572] d_loss: 0.79892141, g_loss: 0.92042106\n",
      "Step: [3573] d_loss: 0.66519850, g_loss: 0.89560145\n",
      "Step: [3574] d_loss: 0.75966197, g_loss: 0.89330322\n",
      "Step: [3575] d_loss: 0.72193891, g_loss: 0.91932195\n",
      "Step: [3576] d_loss: 0.72195059, g_loss: 0.95606399\n",
      "Step: [3577] d_loss: 0.74298799, g_loss: 0.94362229\n",
      "Step: [3578] d_loss: 0.65493721, g_loss: 0.90984285\n",
      "Step: [3579] d_loss: 0.75442559, g_loss: 0.88130414\n",
      "Step: [3580] d_loss: 0.74239790, g_loss: 0.90174437\n",
      "Step: [3581] d_loss: 0.72858495, g_loss: 0.89247835\n",
      "Step: [3582] d_loss: 0.69870412, g_loss: 0.87090009\n",
      "Step: [3583] d_loss: 0.63668889, g_loss: 0.86549139\n",
      "Step: [3584] d_loss: 0.76442826, g_loss: 0.86754113\n",
      "Step: [3585] d_loss: 0.74192953, g_loss: 0.84994280\n",
      "Step: [3586] d_loss: 0.69032091, g_loss: 0.91823876\n",
      "Step: [3587] d_loss: 0.73036551, g_loss: 0.90334982\n",
      "Step: [3588] d_loss: 0.83315188, g_loss: 0.89371926\n",
      "Step: [3589] d_loss: 0.74775177, g_loss: 0.92267823\n",
      "Step: [3590] d_loss: 0.75508827, g_loss: 0.90702862\n",
      "Step: [3591] d_loss: 0.75311518, g_loss: 0.94946998\n",
      "Step: [3592] d_loss: 0.69925624, g_loss: 0.87426412\n",
      "Step: [3593] d_loss: 0.76415110, g_loss: 0.92052931\n",
      "Step: [3594] d_loss: 0.72547907, g_loss: 0.90883338\n",
      "Step: [3595] d_loss: 0.69639659, g_loss: 0.88493490\n",
      "Step: [3596] d_loss: 0.67066413, g_loss: 0.87669367\n",
      "Step: [3597] d_loss: 0.73329103, g_loss: 0.92753005\n",
      "Step: [3598] d_loss: 0.64574069, g_loss: 0.99379551\n",
      "Step: [3599] d_loss: 0.80699950, g_loss: 0.90664184\n",
      "Step: [3600] d_loss: 0.72904694, g_loss: 0.93376875\n",
      "Step: [3601] d_loss: 0.71363789, g_loss: 0.93502587\n",
      "Step: [3602] d_loss: 0.69215316, g_loss: 0.90826988\n",
      "Step: [3603] d_loss: 0.67636579, g_loss: 0.89373398\n",
      "Step: [3604] d_loss: 0.70959938, g_loss: 0.94231975\n",
      "Step: [3605] d_loss: 0.66448140, g_loss: 0.94474286\n",
      "Step: [3606] d_loss: 0.78231055, g_loss: 0.91462874\n",
      "Step: [3607] d_loss: 0.69143391, g_loss: 0.93887568\n",
      "Step: [3608] d_loss: 0.67623997, g_loss: 0.91209602\n",
      "Step: [3609] d_loss: 0.70380181, g_loss: 0.91737753\n",
      "Step: [3610] d_loss: 0.83690637, g_loss: 0.95828325\n",
      "Step: [3611] d_loss: 0.70055944, g_loss: 0.94067723\n",
      "Step: [3612] d_loss: 0.73493332, g_loss: 0.97145200\n",
      "Step: [3613] d_loss: 0.64396930, g_loss: 0.95018214\n",
      "Step: [3614] d_loss: 0.71677762, g_loss: 0.91803068\n",
      "Step: [3615] d_loss: 0.69376200, g_loss: 0.86597455\n",
      "Step: [3616] d_loss: 0.66466308, g_loss: 0.95349556\n",
      "Step: [3617] d_loss: 0.75952238, g_loss: 0.95927626\n",
      "Step: [3618] d_loss: 0.69989324, g_loss: 0.92458665\n",
      "Step: [3619] d_loss: 0.72970003, g_loss: 0.92325032\n",
      "Step: [3620] d_loss: 0.72584295, g_loss: 0.95056623\n",
      "Step: [3621] d_loss: 0.69494992, g_loss: 0.89509457\n",
      "Step: [3622] d_loss: 0.80973029, g_loss: 0.92545182\n",
      "Step: [3623] d_loss: 0.69386959, g_loss: 0.91842759\n",
      "Step: [3624] d_loss: 0.80022383, g_loss: 0.92774612\n",
      "Step: [3625] d_loss: 0.71846843, g_loss: 0.90257066\n",
      "Step: [3626] d_loss: 0.74376708, g_loss: 0.86906505\n",
      "Step: [3627] d_loss: 0.72741145, g_loss: 0.91338825\n",
      "Step: [3628] d_loss: 0.70434922, g_loss: 0.94113082\n",
      "Step: [3629] d_loss: 0.68683106, g_loss: 0.92253727\n",
      "Step: [3630] d_loss: 0.86553895, g_loss: 0.88265455\n",
      "Step: [3631] d_loss: 0.75135696, g_loss: 0.92880487\n",
      "Step: [3632] d_loss: 0.76434696, g_loss: 0.88231373\n",
      "Step: [3633] d_loss: 0.66856688, g_loss: 0.86201119\n",
      "Step: [3634] d_loss: 0.73650324, g_loss: 0.90043533\n",
      "Step: [3635] d_loss: 0.77956945, g_loss: 0.89849776\n",
      "Step: [3636] d_loss: 0.74100882, g_loss: 0.91291702\n",
      "Step: [3637] d_loss: 0.66350734, g_loss: 0.93312246\n",
      "Step: [3638] d_loss: 0.70089996, g_loss: 0.93385386\n",
      "Step: [3639] d_loss: 0.72919136, g_loss: 0.91171265\n",
      "Step: [3640] d_loss: 0.78732902, g_loss: 0.92707604\n",
      "Step: [3641] d_loss: 0.79157692, g_loss: 0.93908161\n",
      "Step: [3642] d_loss: 0.73683447, g_loss: 0.93474489\n",
      "Step: [3643] d_loss: 0.76524979, g_loss: 0.93698728\n",
      "Step: [3644] d_loss: 0.70811719, g_loss: 0.92586374\n",
      "Step: [3645] d_loss: 0.70468783, g_loss: 0.87308598\n",
      "Step: [3646] d_loss: 0.80880654, g_loss: 0.89497429\n",
      "Step: [3647] d_loss: 0.75193566, g_loss: 0.91196311\n",
      "Step: [3648] d_loss: 0.79075462, g_loss: 0.93869996\n",
      "Step: [3649] d_loss: 0.76107347, g_loss: 0.95418137\n",
      "Step: [3650] d_loss: 0.76179498, g_loss: 0.90264779\n",
      "Step: [3651] d_loss: 0.68039393, g_loss: 0.92890280\n",
      "Step: [3652] d_loss: 0.67900819, g_loss: 0.91568887\n",
      "Step: [3653] d_loss: 0.72731459, g_loss: 0.89683092\n",
      "Step: [3654] d_loss: 0.76762140, g_loss: 0.87764585\n",
      "Step: [3655] d_loss: 0.69189209, g_loss: 0.94870520\n",
      "Step: [3656] d_loss: 0.78231740, g_loss: 0.97871351\n",
      "Step: [3657] d_loss: 0.75082618, g_loss: 0.94313353\n",
      "Step: [3658] d_loss: 0.86704445, g_loss: 0.96685290\n",
      "Step: [3659] d_loss: 0.74719071, g_loss: 0.92204946\n",
      "Step: [3660] d_loss: 0.67690021, g_loss: 0.86978406\n",
      "Step: [3661] d_loss: 0.75545377, g_loss: 0.90334988\n",
      "Step: [3662] d_loss: 0.79613233, g_loss: 0.94364959\n",
      "Step: [3663] d_loss: 0.72918272, g_loss: 0.90821373\n",
      "Step: [3664] d_loss: 0.89555788, g_loss: 0.85587311\n",
      "Step: [3665] d_loss: 0.77285373, g_loss: 0.96474946\n",
      "Step: [3666] d_loss: 0.69601238, g_loss: 0.88699764\n",
      "Step: [3667] d_loss: 0.69214857, g_loss: 0.95140690\n",
      "Step: [3668] d_loss: 0.71040517, g_loss: 0.96069509\n",
      "Step: [3669] d_loss: 0.83358175, g_loss: 0.95414037\n",
      "Step: [3670] d_loss: 0.79155773, g_loss: 0.91811866\n",
      "Step: [3671] d_loss: 0.80626744, g_loss: 0.90907848\n",
      "Step: [3672] d_loss: 0.77715415, g_loss: 0.91893220\n",
      "Step: [3673] d_loss: 0.71752310, g_loss: 0.91969168\n",
      "Step: [3674] d_loss: 0.70313364, g_loss: 0.95203185\n",
      "Step: [3675] d_loss: 0.66818804, g_loss: 0.93840790\n",
      "Step: [3676] d_loss: 0.73580569, g_loss: 0.98957688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3677] d_loss: 0.80831856, g_loss: 0.95601106\n",
      "Step: [3678] d_loss: 0.71423501, g_loss: 0.95657086\n",
      "Step: [3679] d_loss: 0.69837737, g_loss: 0.94377196\n",
      "Step: [3680] d_loss: 0.80614465, g_loss: 0.95531678\n",
      "Step: [3681] d_loss: 0.78800738, g_loss: 0.96127224\n",
      "Step: [3682] d_loss: 0.78847337, g_loss: 0.93574619\n",
      "Step: [3683] d_loss: 0.69554263, g_loss: 0.93353093\n",
      "Step: [3684] d_loss: 0.64955753, g_loss: 0.90758783\n",
      "Step: [3685] d_loss: 0.70266092, g_loss: 0.91372585\n",
      "Step: [3686] d_loss: 0.64984810, g_loss: 0.92607415\n",
      "Step: [3687] d_loss: 0.71715611, g_loss: 0.91813111\n",
      "Step: [3688] d_loss: 0.71164095, g_loss: 0.91385609\n",
      "Step: [3689] d_loss: 0.71929264, g_loss: 0.95123583\n",
      "Step: [3690] d_loss: 0.75518578, g_loss: 0.93905604\n",
      "Step: [3691] d_loss: 0.69597530, g_loss: 0.88116586\n",
      "Step: [3692] d_loss: 0.67456490, g_loss: 0.92290735\n",
      "Step: [3693] d_loss: 0.77621508, g_loss: 0.87426782\n",
      "Step: [3694] d_loss: 0.73999125, g_loss: 0.89163554\n",
      "Step: [3695] d_loss: 0.83616608, g_loss: 0.90810680\n",
      "Step: [3696] d_loss: 0.78267825, g_loss: 0.89463168\n",
      "Step: [3697] d_loss: 0.78512198, g_loss: 0.91085923\n",
      "Step: [3698] d_loss: 0.75489211, g_loss: 0.89992231\n",
      "Step: [3699] d_loss: 0.69543558, g_loss: 0.95036936\n",
      "Step: [3700] d_loss: 0.83272934, g_loss: 0.92656779\n",
      "Step: [3701] d_loss: 0.69815153, g_loss: 0.92514706\n",
      "Step: [3702] d_loss: 0.73209041, g_loss: 0.92388928\n",
      "Step: [3703] d_loss: 0.76017988, g_loss: 0.95378125\n",
      "Step: [3704] d_loss: 0.74368531, g_loss: 0.96291530\n",
      "Step: [3705] d_loss: 0.76393628, g_loss: 0.87558895\n",
      "Step: [3706] d_loss: 0.77500463, g_loss: 0.92401445\n",
      "Step: [3707] d_loss: 0.71966040, g_loss: 0.93222880\n",
      "Step: [3708] d_loss: 0.83593452, g_loss: 0.86679530\n",
      "Step: [3709] d_loss: 0.70318580, g_loss: 0.90960729\n",
      "Step: [3710] d_loss: 0.70195359, g_loss: 0.96145749\n",
      "Step: [3711] d_loss: 0.72419310, g_loss: 0.90858567\n",
      "Step: [3712] d_loss: 0.73168361, g_loss: 0.96112120\n",
      "Step: [3713] d_loss: 0.67591816, g_loss: 0.92959791\n",
      "Step: [3714] d_loss: 0.70567536, g_loss: 0.92276001\n",
      "Step: [3715] d_loss: 0.78059232, g_loss: 0.91640365\n",
      "Step: [3716] d_loss: 0.81301636, g_loss: 0.92484450\n",
      "Step: [3717] d_loss: 0.71639436, g_loss: 0.96249700\n",
      "Step: [3718] d_loss: 0.81253892, g_loss: 0.93746322\n",
      "Step: [3719] d_loss: 0.74973333, g_loss: 0.90329510\n",
      "Step: [3720] d_loss: 0.88423640, g_loss: 0.89689356\n",
      "Step: [3721] d_loss: 0.78334641, g_loss: 0.91746736\n",
      "Step: [3722] d_loss: 0.78651595, g_loss: 0.92980367\n",
      "Step: [3723] d_loss: 0.83303779, g_loss: 0.95716256\n",
      "Step: [3724] d_loss: 0.73025370, g_loss: 0.93786657\n",
      "Step: [3725] d_loss: 0.74259859, g_loss: 0.88558573\n",
      "Step: [3726] d_loss: 0.79836565, g_loss: 0.88344973\n",
      "Step: [3727] d_loss: 0.84379131, g_loss: 0.91203994\n",
      "Step: [3728] d_loss: 0.70566583, g_loss: 0.87871075\n",
      "Step: [3729] d_loss: 0.70126212, g_loss: 0.88741368\n",
      "Step: [3730] d_loss: 0.85193741, g_loss: 0.87411153\n",
      "Step: [3731] d_loss: 0.70996112, g_loss: 0.92272717\n",
      "Step: [3732] d_loss: 0.73292357, g_loss: 0.88346535\n",
      "Step: [3733] d_loss: 0.78121495, g_loss: 0.89418805\n",
      "Step: [3734] d_loss: 0.74979937, g_loss: 0.93320614\n",
      "Step: [3735] d_loss: 0.73128313, g_loss: 0.90896660\n",
      "Step: [3736] d_loss: 0.75851774, g_loss: 0.95268643\n",
      "Step: [3737] d_loss: 0.72707695, g_loss: 0.87592733\n",
      "Step: [3738] d_loss: 0.74727237, g_loss: 0.92224908\n",
      "Step: [3739] d_loss: 0.75783169, g_loss: 0.93255371\n",
      "Step: [3740] d_loss: 0.87735218, g_loss: 0.88890940\n",
      "Step: [3741] d_loss: 0.73063898, g_loss: 0.90336621\n",
      "Step: [3742] d_loss: 0.75783610, g_loss: 0.92575520\n",
      "Step: [3743] d_loss: 0.76176709, g_loss: 0.93146133\n",
      "Step: [3744] d_loss: 0.82714611, g_loss: 0.88550359\n",
      "Step: [3745] d_loss: 0.75671959, g_loss: 0.92258239\n",
      "Step: [3746] d_loss: 0.76195067, g_loss: 1.00353909\n",
      "Step: [3747] d_loss: 0.72058010, g_loss: 0.93017864\n",
      "Step: [3748] d_loss: 0.73386753, g_loss: 0.87091762\n",
      "Step: [3749] d_loss: 0.81853211, g_loss: 0.90524131\n",
      "Step: [3750] d_loss: 0.77872866, g_loss: 0.88694298\n",
      "Step: [3751] d_loss: 0.77069795, g_loss: 0.90462953\n",
      "Step: [3752] d_loss: 0.73729509, g_loss: 0.89231139\n",
      "Step: [3753] d_loss: 0.73133105, g_loss: 0.86185956\n",
      "Step: [3754] d_loss: 0.75956082, g_loss: 0.89512980\n",
      "Step: [3755] d_loss: 0.68496621, g_loss: 0.90939885\n",
      "Step: [3756] d_loss: 0.67721605, g_loss: 0.94377983\n",
      "Step: [3757] d_loss: 0.68132859, g_loss: 0.92411995\n",
      "Step: [3758] d_loss: 0.76941836, g_loss: 0.89852113\n",
      "Step: [3759] d_loss: 0.74491400, g_loss: 0.94595343\n",
      "Step: [3760] d_loss: 0.64953840, g_loss: 0.94480646\n",
      "Step: [3761] d_loss: 0.74206239, g_loss: 0.92836183\n",
      "Step: [3762] d_loss: 0.83686221, g_loss: 0.86595416\n",
      "Step: [3763] d_loss: 0.69643414, g_loss: 0.95126116\n",
      "Step: [3764] d_loss: 0.89850992, g_loss: 0.87220156\n",
      "Step: [3765] d_loss: 0.73283070, g_loss: 0.94184923\n",
      "Step: [3766] d_loss: 0.78235686, g_loss: 0.92707008\n",
      "Step: [3767] d_loss: 0.80329508, g_loss: 0.97781855\n",
      "Step: [3768] d_loss: 0.76306617, g_loss: 1.01339674\n",
      "Step: [3769] d_loss: 0.75304860, g_loss: 0.94187397\n",
      "Step: [3770] d_loss: 0.73215038, g_loss: 0.93221593\n",
      "Step: [3771] d_loss: 0.78984213, g_loss: 0.93711382\n",
      "Step: [3772] d_loss: 0.66798550, g_loss: 0.92821473\n",
      "Step: [3773] d_loss: 0.73911858, g_loss: 0.91172361\n",
      "Step: [3774] d_loss: 0.70879078, g_loss: 0.91580433\n",
      "Step: [3775] d_loss: 0.78930956, g_loss: 0.86639661\n",
      "Step: [3776] d_loss: 0.70603830, g_loss: 0.95246315\n",
      "Step: [3777] d_loss: 0.71127230, g_loss: 0.94380742\n",
      "Step: [3778] d_loss: 0.70514703, g_loss: 1.01257539\n",
      "Step: [3779] d_loss: 0.73420191, g_loss: 0.95659101\n",
      "Step: [3780] d_loss: 0.73268044, g_loss: 0.92716777\n",
      "Step: [3781] d_loss: 0.76060998, g_loss: 0.87969244\n",
      "Step: [3782] d_loss: 0.76071215, g_loss: 0.89683563\n",
      "Step: [3783] d_loss: 0.70481980, g_loss: 0.94018751\n",
      "Step: [3784] d_loss: 0.72327334, g_loss: 0.91962630\n",
      "Step: [3785] d_loss: 0.73723441, g_loss: 0.94655478\n",
      "Step: [3786] d_loss: 0.75575501, g_loss: 0.95256037\n",
      "Step: [3787] d_loss: 0.77952081, g_loss: 0.93396097\n",
      "Step: [3788] d_loss: 0.73521972, g_loss: 0.92672855\n",
      "Step: [3789] d_loss: 0.74962974, g_loss: 0.93695754\n",
      "Step: [3790] d_loss: 0.69606012, g_loss: 0.87903869\n",
      "Step: [3791] d_loss: 0.79695946, g_loss: 0.90190846\n",
      "Step: [3792] d_loss: 0.75876611, g_loss: 0.89584643\n",
      "Step: [3793] d_loss: 0.74492836, g_loss: 0.95530617\n",
      "Step: [3794] d_loss: 0.68741655, g_loss: 0.90880525\n",
      "Step: [3795] d_loss: 0.75180721, g_loss: 0.92541337\n",
      "Step: [3796] d_loss: 0.69783723, g_loss: 0.95420462\n",
      "Step: [3797] d_loss: 0.80846000, g_loss: 0.92395133\n",
      "Step: [3798] d_loss: 0.67479062, g_loss: 0.89830470\n",
      "Step: [3799] d_loss: 0.69615757, g_loss: 0.88732880\n",
      "Step: [3800] d_loss: 0.80395806, g_loss: 0.88567793\n",
      "Step: [3801] d_loss: 0.71393454, g_loss: 0.90200806\n",
      "Step: [3802] d_loss: 0.69664675, g_loss: 0.95638937\n",
      "Step: [3803] d_loss: 0.74917030, g_loss: 0.94174802\n",
      "Step: [3804] d_loss: 0.72151786, g_loss: 0.86979628\n",
      "Step: [3805] d_loss: 0.71372473, g_loss: 0.87557119\n",
      "Step: [3806] d_loss: 0.83892757, g_loss: 0.94954985\n",
      "Step: [3807] d_loss: 0.73039132, g_loss: 0.98184699\n",
      "Step: [3808] d_loss: 0.69967926, g_loss: 0.94929957\n",
      "Step: [3809] d_loss: 0.78464085, g_loss: 0.93695354\n",
      "Step: [3810] d_loss: 0.72107899, g_loss: 0.92261648\n",
      "Step: [3811] d_loss: 0.78083766, g_loss: 0.86331522\n",
      "Step: [3812] d_loss: 0.76704657, g_loss: 0.94645107\n",
      "Step: [3813] d_loss: 0.70864445, g_loss: 0.94649255\n",
      "Step: [3814] d_loss: 0.76443827, g_loss: 0.90518910\n",
      "Step: [3815] d_loss: 0.83482993, g_loss: 0.89064473\n",
      "Step: [3816] d_loss: 0.76162511, g_loss: 0.97025794\n",
      "Step: [3817] d_loss: 0.74176669, g_loss: 0.85401255\n",
      "Step: [3818] d_loss: 0.75288069, g_loss: 0.86847466\n",
      "Step: [3819] d_loss: 0.76493639, g_loss: 0.89849561\n",
      "Step: [3820] d_loss: 0.83692425, g_loss: 0.86310536\n",
      "Step: [3821] d_loss: 0.87997073, g_loss: 0.83726639\n",
      "Step: [3822] d_loss: 0.68725395, g_loss: 0.87382591\n",
      "Step: [3823] d_loss: 0.80538964, g_loss: 0.88567781\n",
      "Step: [3824] d_loss: 0.68165332, g_loss: 0.89434814\n",
      "Step: [3825] d_loss: 0.74540675, g_loss: 0.87868434\n",
      "Step: [3826] d_loss: 0.73113739, g_loss: 0.94348472\n",
      "Step: [3827] d_loss: 0.79380071, g_loss: 0.90768045\n",
      "Step: [3828] d_loss: 0.70624328, g_loss: 0.94248259\n",
      "Step: [3829] d_loss: 0.72217983, g_loss: 0.99728286\n",
      "Step: [3830] d_loss: 0.81884211, g_loss: 0.92842746\n",
      "Step: [3831] d_loss: 0.77579808, g_loss: 0.94626254\n",
      "Step: [3832] d_loss: 0.74587655, g_loss: 0.91261137\n",
      "Step: [3833] d_loss: 0.71844411, g_loss: 0.92612535\n",
      "Step: [3834] d_loss: 0.71700698, g_loss: 0.90167570\n",
      "Step: [3835] d_loss: 0.73745477, g_loss: 0.90787566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3836] d_loss: 0.73265511, g_loss: 0.94842583\n",
      "Step: [3837] d_loss: 0.72971618, g_loss: 0.93621868\n",
      "Step: [3838] d_loss: 0.68961585, g_loss: 0.90481550\n",
      "Step: [3839] d_loss: 0.72763407, g_loss: 0.91188473\n",
      "Step: [3840] d_loss: 0.72444153, g_loss: 0.88871247\n",
      "Step: [3841] d_loss: 0.76391476, g_loss: 0.90282190\n",
      "Step: [3842] d_loss: 0.76735860, g_loss: 0.94929439\n",
      "Step: [3843] d_loss: 0.72815269, g_loss: 0.98393613\n",
      "Step: [3844] d_loss: 0.74361593, g_loss: 0.94343692\n",
      "Step: [3845] d_loss: 0.73611486, g_loss: 0.92175126\n",
      "Step: [3846] d_loss: 0.68167156, g_loss: 0.97892040\n",
      "Step: [3847] d_loss: 0.69390315, g_loss: 0.91539443\n",
      "Step: [3848] d_loss: 0.62664926, g_loss: 0.92949831\n",
      "Step: [3849] d_loss: 0.71760595, g_loss: 0.86666244\n",
      "Step: [3850] d_loss: 0.78366703, g_loss: 0.83109611\n",
      "Step: [3851] d_loss: 0.71979755, g_loss: 0.89857346\n",
      "Step: [3852] d_loss: 0.70216084, g_loss: 0.91101116\n",
      "Step: [3853] d_loss: 0.72104198, g_loss: 0.90984786\n",
      "Step: [3854] d_loss: 0.80500060, g_loss: 0.96513939\n",
      "Step: [3855] d_loss: 0.68408865, g_loss: 0.95282876\n",
      "Step: [3856] d_loss: 0.76622379, g_loss: 0.98528564\n",
      "Step: [3857] d_loss: 0.72128755, g_loss: 0.94282073\n",
      "Step: [3858] d_loss: 0.75698870, g_loss: 0.88851690\n",
      "Step: [3859] d_loss: 0.64778221, g_loss: 0.87179488\n",
      "Step: [3860] d_loss: 0.70354301, g_loss: 0.86838979\n",
      "Step: [3861] d_loss: 0.72194785, g_loss: 0.85266340\n",
      "Step: [3862] d_loss: 0.70146191, g_loss: 0.85640174\n",
      "Step: [3863] d_loss: 0.70303011, g_loss: 0.87154537\n",
      "Step: [3864] d_loss: 0.72749716, g_loss: 0.89488173\n",
      "Step: [3865] d_loss: 0.92027223, g_loss: 0.88780975\n",
      "Step: [3866] d_loss: 0.68325508, g_loss: 0.94196302\n",
      "Step: [3867] d_loss: 0.77410376, g_loss: 0.90390992\n",
      "Step: [3868] d_loss: 0.70545024, g_loss: 0.96116221\n",
      "Step: [3869] d_loss: 0.77940691, g_loss: 0.92300510\n",
      "Step: [3870] d_loss: 0.82368785, g_loss: 0.94840693\n",
      "Step: [3871] d_loss: 0.72390151, g_loss: 0.90895039\n",
      "Step: [3872] d_loss: 0.65302902, g_loss: 0.89073986\n",
      "Step: [3873] d_loss: 0.78043991, g_loss: 0.86985099\n",
      "Step: [3874] d_loss: 0.75914979, g_loss: 0.90567243\n",
      "Step: [3875] d_loss: 0.78370506, g_loss: 0.86374629\n",
      "Step: [3876] d_loss: 0.65684927, g_loss: 0.88035178\n",
      "Step: [3877] d_loss: 0.69343841, g_loss: 0.93406051\n",
      "Step: [3878] d_loss: 0.68480909, g_loss: 0.94041020\n",
      "Step: [3879] d_loss: 0.82285738, g_loss: 0.91374254\n",
      "Step: [3880] d_loss: 0.72014284, g_loss: 0.95607471\n",
      "Step: [3881] d_loss: 0.80813175, g_loss: 0.95874101\n",
      "Step: [3882] d_loss: 0.93029350, g_loss: 0.92269236\n",
      "Step: [3883] d_loss: 0.74160999, g_loss: 0.94829637\n",
      "Step: [3884] d_loss: 0.63795036, g_loss: 0.96622157\n",
      "Step: [3885] d_loss: 0.77791315, g_loss: 0.84313434\n",
      "Step: [3886] d_loss: 0.66783524, g_loss: 0.84711939\n",
      "Step: [3887] d_loss: 0.74841183, g_loss: 0.85152310\n",
      "Step: [3888] d_loss: 0.67858207, g_loss: 0.92538649\n",
      "Step: [3889] d_loss: 0.67305416, g_loss: 0.90983570\n",
      "Step: [3890] d_loss: 0.78940636, g_loss: 0.88295990\n",
      "Step: [3891] d_loss: 0.67069072, g_loss: 0.93200999\n",
      "Step: [3892] d_loss: 0.73157823, g_loss: 0.91354936\n",
      "Step: [3893] d_loss: 0.70143914, g_loss: 0.90894794\n",
      "Step: [3894] d_loss: 0.72416610, g_loss: 0.90219384\n",
      "Step: [3895] d_loss: 0.78947103, g_loss: 0.89677596\n",
      "Step: [3896] d_loss: 0.70611864, g_loss: 0.91501147\n",
      "Step: [3897] d_loss: 0.66780144, g_loss: 0.92851341\n",
      "Step: [3898] d_loss: 0.70616722, g_loss: 0.90996170\n",
      "Step: [3899] d_loss: 0.76743668, g_loss: 0.86183590\n",
      "Step: [3900] d_loss: 0.72956556, g_loss: 0.85181868\n",
      "Step: [3901] d_loss: 0.80787784, g_loss: 0.87668550\n",
      "Step: [3902] d_loss: 0.79329342, g_loss: 0.88664347\n",
      "Step: [3903] d_loss: 0.73503685, g_loss: 0.88853955\n",
      "Step: [3904] d_loss: 0.73357427, g_loss: 0.90153325\n",
      "Step: [3905] d_loss: 0.70704484, g_loss: 0.94022840\n",
      "Step: [3906] d_loss: 0.62176287, g_loss: 0.92441654\n",
      "Step: [3907] d_loss: 0.74688381, g_loss: 0.90354443\n",
      "Step: [3908] d_loss: 0.73952913, g_loss: 0.97615314\n",
      "Step: [3909] d_loss: 0.73087883, g_loss: 0.94080997\n",
      "Step: [3910] d_loss: 0.71802503, g_loss: 0.97154987\n",
      "Step: [3911] d_loss: 0.84215879, g_loss: 0.90737998\n",
      "Step: [3912] d_loss: 0.64557546, g_loss: 0.87900323\n",
      "Step: [3913] d_loss: 0.78108853, g_loss: 0.86877465\n",
      "Step: [3914] d_loss: 0.69498074, g_loss: 0.95628339\n",
      "Step: [3915] d_loss: 0.67243254, g_loss: 0.85791761\n",
      "Step: [3916] d_loss: 0.77764589, g_loss: 0.83954972\n",
      "Step: [3917] d_loss: 0.79944611, g_loss: 0.87660903\n",
      "Step: [3918] d_loss: 0.68834114, g_loss: 0.94245064\n",
      "Step: [3919] d_loss: 0.76298809, g_loss: 0.96138990\n",
      "Step: [3920] d_loss: 0.70817584, g_loss: 0.91441059\n",
      "Step: [3921] d_loss: 0.76203179, g_loss: 0.98494297\n",
      "Step: [3922] d_loss: 0.81947762, g_loss: 0.93772697\n",
      "Step: [3923] d_loss: 0.75622481, g_loss: 0.94274378\n",
      "Step: [3924] d_loss: 0.73056418, g_loss: 0.93055660\n",
      "Step: [3925] d_loss: 0.76433361, g_loss: 0.88242120\n",
      "Step: [3926] d_loss: 0.69372165, g_loss: 0.87951881\n",
      "Step: [3927] d_loss: 0.63573265, g_loss: 0.89678657\n",
      "Step: [3928] d_loss: 0.71917653, g_loss: 0.91447365\n",
      "Step: [3929] d_loss: 0.72459650, g_loss: 0.93322128\n",
      "Step: [3930] d_loss: 0.78929478, g_loss: 0.87856048\n",
      "Step: [3931] d_loss: 0.80533957, g_loss: 0.89255714\n",
      "Step: [3932] d_loss: 0.71844548, g_loss: 0.87764126\n",
      "Step: [3933] d_loss: 0.68789285, g_loss: 0.89387101\n",
      "Step: [3934] d_loss: 0.91271293, g_loss: 0.80943161\n",
      "Step: [3935] d_loss: 0.68819028, g_loss: 0.84997803\n",
      "Step: [3936] d_loss: 0.76828086, g_loss: 0.82795322\n",
      "Step: [3937] d_loss: 0.72396249, g_loss: 0.92067575\n",
      "Step: [3938] d_loss: 0.72328883, g_loss: 0.90238160\n",
      "Step: [3939] d_loss: 0.72287405, g_loss: 0.88329065\n",
      "Step: [3940] d_loss: 0.81156385, g_loss: 0.82529223\n",
      "Step: [3941] d_loss: 0.60526490, g_loss: 0.89251977\n",
      "Step: [3942] d_loss: 0.63314766, g_loss: 0.90484297\n",
      "Step: [3943] d_loss: 0.86430436, g_loss: 0.86945432\n",
      "Step: [3944] d_loss: 0.86426508, g_loss: 0.91957855\n",
      "Step: [3945] d_loss: 0.79760349, g_loss: 0.92879415\n",
      "Step: [3946] d_loss: 0.77530855, g_loss: 0.91502470\n",
      "Step: [3947] d_loss: 0.77400523, g_loss: 0.91329724\n",
      "Step: [3948] d_loss: 0.74213481, g_loss: 0.94245791\n",
      "Step: [3949] d_loss: 0.79717726, g_loss: 0.96129382\n",
      "Step: [3950] d_loss: 0.76406199, g_loss: 0.97010171\n",
      "Step: [3951] d_loss: 0.85647148, g_loss: 0.90614724\n",
      "Step: [3952] d_loss: 0.76726425, g_loss: 0.90362990\n",
      "Step: [3953] d_loss: 0.77719218, g_loss: 0.91114771\n",
      "Step: [3954] d_loss: 0.72470254, g_loss: 0.94505996\n",
      "Step: [3955] d_loss: 0.82489675, g_loss: 0.93433654\n",
      "Step: [3956] d_loss: 0.65622842, g_loss: 0.93375975\n",
      "Step: [3957] d_loss: 0.70959353, g_loss: 0.87408793\n",
      "Step: [3958] d_loss: 0.66280740, g_loss: 0.88436967\n",
      "Step: [3959] d_loss: 0.69689661, g_loss: 0.92863995\n",
      "Step: [3960] d_loss: 0.78426933, g_loss: 0.86934006\n",
      "Step: [3961] d_loss: 0.73251528, g_loss: 0.89579535\n",
      "Step: [3962] d_loss: 0.75364482, g_loss: 0.95875752\n",
      "Step: [3963] d_loss: 0.72212726, g_loss: 0.92659807\n",
      "Step: [3964] d_loss: 0.83127087, g_loss: 0.90047729\n",
      "Step: [3965] d_loss: 0.74477404, g_loss: 0.92158270\n",
      "Step: [3966] d_loss: 0.79684860, g_loss: 0.92224169\n",
      "Step: [3967] d_loss: 0.74809122, g_loss: 0.89070272\n",
      "Step: [3968] d_loss: 0.79859525, g_loss: 0.89938021\n",
      "Step: [3969] d_loss: 0.78776377, g_loss: 0.89315039\n",
      "Step: [3970] d_loss: 0.78597581, g_loss: 0.91025436\n",
      "Step: [3971] d_loss: 0.72725737, g_loss: 0.88059962\n",
      "Step: [3972] d_loss: 0.76768839, g_loss: 0.89604211\n",
      "Step: [3973] d_loss: 0.69892043, g_loss: 0.95295203\n",
      "Step: [3974] d_loss: 0.76531434, g_loss: 0.95441097\n",
      "Step: [3975] d_loss: 0.83470798, g_loss: 0.93575728\n",
      "Step: [3976] d_loss: 0.81849939, g_loss: 0.88394618\n",
      "Step: [3977] d_loss: 0.75746590, g_loss: 0.94902128\n",
      "Step: [3978] d_loss: 0.69115537, g_loss: 0.94428694\n",
      "Step: [3979] d_loss: 0.71531951, g_loss: 0.92493188\n",
      "Step: [3980] d_loss: 0.75930715, g_loss: 0.92541713\n",
      "Step: [3981] d_loss: 0.79263890, g_loss: 0.96138525\n",
      "Step: [3982] d_loss: 0.76126397, g_loss: 0.90993971\n",
      "Step: [3983] d_loss: 0.71342897, g_loss: 0.94979894\n",
      "Step: [3984] d_loss: 0.67628896, g_loss: 0.87590337\n",
      "Step: [3985] d_loss: 0.73291922, g_loss: 0.88347971\n",
      "Step: [3986] d_loss: 0.75938964, g_loss: 0.86526465\n",
      "Step: [3987] d_loss: 0.74473405, g_loss: 0.90700841\n",
      "Step: [3988] d_loss: 0.72770894, g_loss: 0.92757267\n",
      "Step: [3989] d_loss: 0.69790649, g_loss: 0.92951787\n",
      "Step: [3990] d_loss: 0.71284240, g_loss: 0.91557419\n",
      "Step: [3991] d_loss: 0.72092026, g_loss: 0.92498279\n",
      "Step: [3992] d_loss: 0.78873062, g_loss: 0.90495324\n",
      "Step: [3993] d_loss: 0.74497700, g_loss: 0.92259353\n",
      "Step: [3994] d_loss: 0.77946275, g_loss: 0.95329523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [3995] d_loss: 0.84305471, g_loss: 0.95041561\n",
      "Step: [3996] d_loss: 0.78541070, g_loss: 0.97891068\n",
      "Step: [3997] d_loss: 0.79028767, g_loss: 0.97212893\n",
      "Step: [3998] d_loss: 0.69447124, g_loss: 0.91340625\n",
      "Step: [3999] d_loss: 0.88057035, g_loss: 0.92172188\n",
      "Step: [4000] d_loss: 0.69052064, g_loss: 0.89472663\n",
      "Step: [4001] d_loss: 0.71312046, g_loss: 0.89272612\n",
      "Step: [4002] d_loss: 0.75010777, g_loss: 0.90806800\n",
      "Step: [4003] d_loss: 0.73548692, g_loss: 0.94049770\n",
      "Step: [4004] d_loss: 0.73587769, g_loss: 0.89182931\n",
      "Step: [4005] d_loss: 0.72845882, g_loss: 0.90126204\n",
      "Step: [4006] d_loss: 0.77586913, g_loss: 0.93167096\n",
      "Step: [4007] d_loss: 0.71336108, g_loss: 0.89512146\n",
      "Step: [4008] d_loss: 0.67264408, g_loss: 0.92820799\n",
      "Step: [4009] d_loss: 0.75665522, g_loss: 0.96009952\n",
      "Step: [4010] d_loss: 0.79813641, g_loss: 0.91982287\n",
      "Step: [4011] d_loss: 0.78911537, g_loss: 0.94385362\n",
      "Step: [4012] d_loss: 0.74096900, g_loss: 0.89666206\n",
      "Step: [4013] d_loss: 0.73377973, g_loss: 0.95268953\n",
      "Step: [4014] d_loss: 0.82001811, g_loss: 0.93724173\n",
      "Step: [4015] d_loss: 0.65874285, g_loss: 0.92048210\n",
      "Step: [4016] d_loss: 0.61672431, g_loss: 0.91219562\n",
      "Step: [4017] d_loss: 0.76526749, g_loss: 0.87285817\n",
      "Step: [4018] d_loss: 0.74987626, g_loss: 0.86877543\n",
      "Step: [4019] d_loss: 0.73158818, g_loss: 0.88935655\n",
      "Step: [4020] d_loss: 0.70195514, g_loss: 0.89439118\n",
      "Step: [4021] d_loss: 0.73204881, g_loss: 0.94210964\n",
      "Step: [4022] d_loss: 0.72986251, g_loss: 0.92217720\n",
      "Step: [4023] d_loss: 0.71581483, g_loss: 0.88950789\n",
      "Step: [4024] d_loss: 0.74283171, g_loss: 0.93777132\n",
      "Step: [4025] d_loss: 0.78113210, g_loss: 0.87275267\n",
      "Step: [4026] d_loss: 0.73879015, g_loss: 0.85519469\n",
      "Step: [4027] d_loss: 0.78070450, g_loss: 0.84288001\n",
      "Step: [4028] d_loss: 0.75759149, g_loss: 0.92468083\n",
      "Step: [4029] d_loss: 0.71154594, g_loss: 0.93877214\n",
      "Step: [4030] d_loss: 0.68810558, g_loss: 0.90638638\n",
      "Step: [4031] d_loss: 0.66343391, g_loss: 0.90592557\n",
      "Step: [4032] d_loss: 0.69337326, g_loss: 0.90739465\n",
      "Step: [4033] d_loss: 0.68690097, g_loss: 0.91646940\n",
      "Step: [4034] d_loss: 0.76492703, g_loss: 0.90787077\n",
      "Step: [4035] d_loss: 0.77212501, g_loss: 0.91653967\n",
      "Step: [4036] d_loss: 0.76388437, g_loss: 0.92320633\n",
      "Step: [4037] d_loss: 0.73970830, g_loss: 0.90973836\n",
      "Step: [4038] d_loss: 0.73659658, g_loss: 0.90931302\n",
      "Step: [4039] d_loss: 0.72633332, g_loss: 0.91325837\n",
      "Step: [4040] d_loss: 0.73609012, g_loss: 0.89814466\n",
      "Step: [4041] d_loss: 0.65119469, g_loss: 0.93731576\n",
      "Step: [4042] d_loss: 0.82879478, g_loss: 0.88020647\n",
      "Step: [4043] d_loss: 0.71674013, g_loss: 0.91039354\n",
      "Step: [4044] d_loss: 0.79457027, g_loss: 0.92127997\n",
      "Step: [4045] d_loss: 0.65236467, g_loss: 0.94409907\n",
      "Step: [4046] d_loss: 0.74936050, g_loss: 0.93135262\n",
      "Step: [4047] d_loss: 0.80729806, g_loss: 0.89136970\n",
      "Step: [4048] d_loss: 0.74777442, g_loss: 0.90461040\n",
      "Step: [4049] d_loss: 0.74502951, g_loss: 0.88093519\n",
      "Step: [4050] d_loss: 0.71146190, g_loss: 0.89012706\n",
      "Step: [4051] d_loss: 0.77056187, g_loss: 0.88387561\n",
      "Step: [4052] d_loss: 0.81393784, g_loss: 0.86416245\n",
      "Step: [4053] d_loss: 0.82215065, g_loss: 0.86809742\n",
      "Step: [4054] d_loss: 0.71674347, g_loss: 0.90103018\n",
      "Step: [4055] d_loss: 0.79002470, g_loss: 0.87796474\n",
      "Step: [4056] d_loss: 0.79295152, g_loss: 0.90940940\n",
      "Step: [4057] d_loss: 0.72915393, g_loss: 0.90313792\n",
      "Step: [4058] d_loss: 0.75942141, g_loss: 0.93247318\n",
      "Step: [4059] d_loss: 0.81253386, g_loss: 0.94937438\n",
      "Step: [4060] d_loss: 0.72491968, g_loss: 0.94650483\n",
      "Step: [4061] d_loss: 0.73210931, g_loss: 0.95761967\n",
      "Step: [4062] d_loss: 0.78418320, g_loss: 0.90633017\n",
      "Step: [4063] d_loss: 0.71469140, g_loss: 0.93250191\n",
      "Step: [4064] d_loss: 0.72124052, g_loss: 0.90086281\n",
      "Step: [4065] d_loss: 0.67827171, g_loss: 0.96375805\n",
      "Step: [4066] d_loss: 0.72928369, g_loss: 0.94125217\n",
      "Step: [4067] d_loss: 0.69003242, g_loss: 0.97256660\n",
      "Step: [4068] d_loss: 0.72381997, g_loss: 0.96070415\n",
      "Step: [4069] d_loss: 0.76250726, g_loss: 0.92403960\n",
      "Step: [4070] d_loss: 0.79965699, g_loss: 0.91849744\n",
      "Step: [4071] d_loss: 0.69019276, g_loss: 0.95621771\n",
      "Step: [4072] d_loss: 0.68992233, g_loss: 0.91362506\n",
      "Step: [4073] d_loss: 0.78932720, g_loss: 0.90065902\n",
      "Step: [4074] d_loss: 0.74862891, g_loss: 0.90690893\n",
      "Step: [4075] d_loss: 0.70284116, g_loss: 0.91733146\n",
      "Step: [4076] d_loss: 0.84593308, g_loss: 0.87606668\n",
      "Step: [4077] d_loss: 0.69580168, g_loss: 0.97975636\n",
      "Step: [4078] d_loss: 0.78255653, g_loss: 0.95848739\n",
      "Step: [4079] d_loss: 0.71352631, g_loss: 0.85424596\n",
      "Step: [4080] d_loss: 0.80295533, g_loss: 0.90369797\n",
      "Step: [4081] d_loss: 0.74176741, g_loss: 0.94109571\n",
      "Step: [4082] d_loss: 0.66948926, g_loss: 0.94162464\n",
      "Step: [4083] d_loss: 0.79021198, g_loss: 0.90477878\n",
      "Step: [4084] d_loss: 0.74642539, g_loss: 0.85681856\n",
      "Step: [4085] d_loss: 0.70346224, g_loss: 0.84552121\n",
      "Step: [4086] d_loss: 0.80775124, g_loss: 0.87837368\n",
      "Step: [4087] d_loss: 0.75500643, g_loss: 0.85519165\n",
      "Step: [4088] d_loss: 0.79017788, g_loss: 0.93576652\n",
      "Step: [4089] d_loss: 0.71976972, g_loss: 0.91911024\n",
      "Step: [4090] d_loss: 0.78985679, g_loss: 0.92548978\n",
      "Step: [4091] d_loss: 0.73317409, g_loss: 0.97706252\n",
      "Step: [4092] d_loss: 0.76264995, g_loss: 0.95676482\n",
      "Step: [4093] d_loss: 0.72997236, g_loss: 0.93058717\n",
      "Step: [4094] d_loss: 0.75062299, g_loss: 0.89662027\n",
      "Step: [4095] d_loss: 0.70368409, g_loss: 0.95260894\n",
      "Step: [4096] d_loss: 0.75678313, g_loss: 0.86967111\n",
      "Step: [4097] d_loss: 0.66979915, g_loss: 0.91228527\n",
      "Step: [4098] d_loss: 0.73704559, g_loss: 0.94217759\n",
      "Step: [4099] d_loss: 0.75430971, g_loss: 0.94572693\n",
      "Step: [4100] d_loss: 0.74869168, g_loss: 0.99244088\n",
      "Step: [4101] d_loss: 0.71561658, g_loss: 0.90070397\n",
      "Step: [4102] d_loss: 0.71378726, g_loss: 0.85470015\n",
      "Step: [4103] d_loss: 0.70855188, g_loss: 0.87471354\n",
      "Step: [4104] d_loss: 0.84196460, g_loss: 0.83305466\n",
      "Step: [4105] d_loss: 0.74107748, g_loss: 0.87503350\n",
      "Step: [4106] d_loss: 0.75610304, g_loss: 0.90137213\n",
      "Step: [4107] d_loss: 0.73623359, g_loss: 0.90098786\n",
      "Step: [4108] d_loss: 0.70683026, g_loss: 0.94340968\n",
      "Step: [4109] d_loss: 0.70658046, g_loss: 0.96573526\n",
      "Step: [4110] d_loss: 0.66668814, g_loss: 0.91047746\n",
      "Step: [4111] d_loss: 0.77092630, g_loss: 0.93904549\n",
      "Step: [4112] d_loss: 0.68821448, g_loss: 0.94904619\n",
      "Step: [4113] d_loss: 0.69951928, g_loss: 0.88405162\n",
      "Step: [4114] d_loss: 0.66546363, g_loss: 0.95076030\n",
      "Step: [4115] d_loss: 0.68532825, g_loss: 0.91592938\n",
      "Step: [4116] d_loss: 0.68634892, g_loss: 0.92940432\n",
      "Step: [4117] d_loss: 0.84557760, g_loss: 0.90060288\n",
      "Step: [4118] d_loss: 0.79364848, g_loss: 0.91021276\n",
      "Step: [4119] d_loss: 0.73604327, g_loss: 0.92544675\n",
      "Step: [4120] d_loss: 0.67788690, g_loss: 0.92080158\n",
      "Step: [4121] d_loss: 0.72452015, g_loss: 0.89598572\n",
      "Step: [4122] d_loss: 0.61803931, g_loss: 0.88563359\n",
      "Step: [4123] d_loss: 0.73484331, g_loss: 0.87809467\n",
      "Step: [4124] d_loss: 0.65034086, g_loss: 0.92059988\n",
      "Step: [4125] d_loss: 0.71013033, g_loss: 0.89563596\n",
      "Step: [4126] d_loss: 0.73509765, g_loss: 0.91438824\n",
      "Step: [4127] d_loss: 0.63781029, g_loss: 0.91664773\n",
      "Step: [4128] d_loss: 0.80024415, g_loss: 0.92257744\n",
      "Step: [4129] d_loss: 0.68001997, g_loss: 0.90662414\n",
      "Step: [4130] d_loss: 0.75761247, g_loss: 0.92847729\n",
      "Step: [4131] d_loss: 0.71838737, g_loss: 0.93281049\n",
      "Step: [4132] d_loss: 0.77029026, g_loss: 0.87922996\n",
      "Step: [4133] d_loss: 0.70933414, g_loss: 0.89174098\n",
      "Step: [4134] d_loss: 0.74176198, g_loss: 0.94011462\n",
      "Step: [4135] d_loss: 0.72265697, g_loss: 0.92785472\n",
      "Step: [4136] d_loss: 0.70740408, g_loss: 0.92623860\n",
      "Step: [4137] d_loss: 0.71723682, g_loss: 0.93490732\n",
      "Step: [4138] d_loss: 0.70357543, g_loss: 0.94905049\n",
      "Step: [4139] d_loss: 0.68603212, g_loss: 0.98148608\n",
      "Step: [4140] d_loss: 0.76568609, g_loss: 0.96212602\n",
      "Step: [4141] d_loss: 0.74959332, g_loss: 0.98489159\n",
      "Step: [4142] d_loss: 0.77759022, g_loss: 0.91348159\n",
      "Step: [4143] d_loss: 0.64496601, g_loss: 0.88703960\n",
      "Step: [4144] d_loss: 0.65038258, g_loss: 0.87306434\n",
      "Step: [4145] d_loss: 0.60596675, g_loss: 0.85566598\n",
      "Step: [4146] d_loss: 0.76075792, g_loss: 0.82772940\n",
      "Step: [4147] d_loss: 0.72706234, g_loss: 0.84252876\n",
      "Step: [4148] d_loss: 0.74837214, g_loss: 0.87348253\n",
      "Step: [4149] d_loss: 0.69064331, g_loss: 0.87474650\n",
      "Step: [4150] d_loss: 0.68493378, g_loss: 0.90070069\n",
      "Step: [4151] d_loss: 0.84957546, g_loss: 0.86182636\n",
      "Step: [4152] d_loss: 0.75039375, g_loss: 0.93411267\n",
      "Step: [4153] d_loss: 0.79533410, g_loss: 0.90501148\n",
      "Step: [4154] d_loss: 0.68959481, g_loss: 0.91563570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4155] d_loss: 0.70540673, g_loss: 0.85257918\n",
      "Step: [4156] d_loss: 0.68936068, g_loss: 0.90329581\n",
      "Step: [4157] d_loss: 0.69587088, g_loss: 0.91678566\n",
      "Step: [4158] d_loss: 0.69314915, g_loss: 0.89732462\n",
      "Step: [4159] d_loss: 0.78666872, g_loss: 0.87308216\n",
      "Step: [4160] d_loss: 0.63637847, g_loss: 0.88310313\n",
      "Step: [4161] d_loss: 0.71318126, g_loss: 0.90938085\n",
      "Step: [4162] d_loss: 0.72764939, g_loss: 0.93367177\n",
      "Step: [4163] d_loss: 0.71314192, g_loss: 0.91403174\n",
      "Step: [4164] d_loss: 0.76114172, g_loss: 0.91485339\n",
      "Step: [4165] d_loss: 0.78163046, g_loss: 0.88084334\n",
      "Step: [4166] d_loss: 0.71468073, g_loss: 0.93181932\n",
      "Step: [4167] d_loss: 0.67403829, g_loss: 0.87774909\n",
      "Step: [4168] d_loss: 0.75887436, g_loss: 0.84687072\n",
      "Step: [4169] d_loss: 0.67113483, g_loss: 0.90371084\n",
      "Step: [4170] d_loss: 0.73154306, g_loss: 0.88851339\n",
      "Step: [4171] d_loss: 0.74522752, g_loss: 0.92591804\n",
      "Step: [4172] d_loss: 0.73538393, g_loss: 0.91205508\n",
      "Step: [4173] d_loss: 0.77133620, g_loss: 0.89781076\n",
      "Step: [4174] d_loss: 0.70201981, g_loss: 0.92589027\n",
      "Step: [4175] d_loss: 0.67263764, g_loss: 0.92882407\n",
      "Step: [4176] d_loss: 0.79649532, g_loss: 0.92769909\n",
      "Step: [4177] d_loss: 0.73566681, g_loss: 0.85356402\n",
      "Step: [4178] d_loss: 0.71001494, g_loss: 0.92287976\n",
      "Step: [4179] d_loss: 0.71113724, g_loss: 0.88241363\n",
      "Step: [4180] d_loss: 0.73467880, g_loss: 0.93248397\n",
      "Step: [4181] d_loss: 0.73289728, g_loss: 0.90324998\n",
      "Step: [4182] d_loss: 0.72343642, g_loss: 0.90452814\n",
      "Step: [4183] d_loss: 0.67701566, g_loss: 0.93359005\n",
      "Step: [4184] d_loss: 0.70228887, g_loss: 0.87277967\n",
      "Step: [4185] d_loss: 0.71588099, g_loss: 0.85394919\n",
      "Step: [4186] d_loss: 0.78202176, g_loss: 0.88508809\n",
      "Step: [4187] d_loss: 0.75419790, g_loss: 0.87044215\n",
      "Step: [4188] d_loss: 0.73541409, g_loss: 0.88851881\n",
      "Step: [4189] d_loss: 0.70708162, g_loss: 0.94907922\n",
      "Step: [4190] d_loss: 0.73448986, g_loss: 0.94214547\n",
      "Step: [4191] d_loss: 0.76861382, g_loss: 0.94374311\n",
      "Step: [4192] d_loss: 0.70136207, g_loss: 0.89357179\n",
      "Step: [4193] d_loss: 0.71254057, g_loss: 0.93314773\n",
      "Step: [4194] d_loss: 0.73291278, g_loss: 0.89081508\n",
      "Step: [4195] d_loss: 0.75210261, g_loss: 0.86079514\n",
      "Step: [4196] d_loss: 0.85967422, g_loss: 0.83463091\n",
      "Step: [4197] d_loss: 0.76459098, g_loss: 0.89247108\n",
      "Step: [4198] d_loss: 0.80688137, g_loss: 0.90933645\n",
      "Step: [4199] d_loss: 0.73400193, g_loss: 0.95428246\n",
      "Step: [4200] d_loss: 0.67179888, g_loss: 0.95209461\n",
      "Step: [4201] d_loss: 0.80444837, g_loss: 0.85357255\n",
      "Step: [4202] d_loss: 0.74336004, g_loss: 0.91156703\n",
      "Step: [4203] d_loss: 0.72913992, g_loss: 0.91627800\n",
      "Step: [4204] d_loss: 0.73609924, g_loss: 0.89862967\n",
      "Step: [4205] d_loss: 0.77255785, g_loss: 0.87062109\n",
      "Step: [4206] d_loss: 0.71430117, g_loss: 0.91872752\n",
      "Step: [4207] d_loss: 0.75542939, g_loss: 0.94076133\n",
      "Step: [4208] d_loss: 0.75203753, g_loss: 0.96528262\n",
      "Step: [4209] d_loss: 0.75891882, g_loss: 0.95795482\n",
      "Step: [4210] d_loss: 0.78029603, g_loss: 0.95198411\n",
      "Step: [4211] d_loss: 0.70311368, g_loss: 0.86608404\n",
      "Step: [4212] d_loss: 0.70562971, g_loss: 0.92175895\n",
      "Step: [4213] d_loss: 0.62766171, g_loss: 0.87733227\n",
      "Step: [4214] d_loss: 0.65994954, g_loss: 0.88262445\n",
      "Step: [4215] d_loss: 0.68917394, g_loss: 0.85885268\n",
      "Step: [4216] d_loss: 0.68184859, g_loss: 0.90070969\n",
      "Step: [4217] d_loss: 0.80054241, g_loss: 0.83299029\n",
      "Step: [4218] d_loss: 0.76553398, g_loss: 0.90033793\n",
      "Step: [4219] d_loss: 0.73290461, g_loss: 0.90827972\n",
      "Step: [4220] d_loss: 0.71342677, g_loss: 0.96063042\n",
      "Step: [4221] d_loss: 0.74400467, g_loss: 0.88996667\n",
      "Step: [4222] d_loss: 0.69986397, g_loss: 0.96913838\n",
      "Step: [4223] d_loss: 0.76034820, g_loss: 0.86609209\n",
      "Step: [4224] d_loss: 0.75184709, g_loss: 0.91286778\n",
      "Step: [4225] d_loss: 0.71505868, g_loss: 0.94937742\n",
      "Step: [4226] d_loss: 0.81607950, g_loss: 0.90631884\n",
      "Step: [4227] d_loss: 0.75785577, g_loss: 0.92275023\n",
      "Step: [4228] d_loss: 0.75171357, g_loss: 0.87081671\n",
      "Step: [4229] d_loss: 0.79184574, g_loss: 0.88847178\n",
      "Step: [4230] d_loss: 0.67742735, g_loss: 0.91074669\n",
      "Step: [4231] d_loss: 0.64835995, g_loss: 0.91439509\n",
      "Step: [4232] d_loss: 0.67446744, g_loss: 0.94027030\n",
      "Step: [4233] d_loss: 0.73250788, g_loss: 0.92224735\n",
      "Step: [4234] d_loss: 0.71866500, g_loss: 0.92431366\n",
      "Step: [4235] d_loss: 0.69431567, g_loss: 0.90493119\n",
      "Step: [4236] d_loss: 0.81678963, g_loss: 0.90885991\n",
      "Step: [4237] d_loss: 0.68914986, g_loss: 0.90665692\n",
      "Step: [4238] d_loss: 0.59465796, g_loss: 0.88912177\n",
      "Step: [4239] d_loss: 0.74932224, g_loss: 0.87459290\n",
      "Step: [4240] d_loss: 0.72045732, g_loss: 0.87548995\n",
      "Step: [4241] d_loss: 0.81874865, g_loss: 0.83447182\n",
      "Step: [4242] d_loss: 0.70057821, g_loss: 0.84691983\n",
      "Step: [4243] d_loss: 0.62846941, g_loss: 0.94691777\n",
      "Step: [4244] d_loss: 0.69323313, g_loss: 0.92485404\n",
      "Step: [4245] d_loss: 0.68070811, g_loss: 0.91928369\n",
      "Step: [4246] d_loss: 0.77511698, g_loss: 0.95931286\n",
      "Step: [4247] d_loss: 0.75893706, g_loss: 0.96166003\n",
      "Step: [4248] d_loss: 0.72122586, g_loss: 0.89512068\n",
      "Step: [4249] d_loss: 0.70494336, g_loss: 0.89779961\n",
      "Step: [4250] d_loss: 0.73843372, g_loss: 0.87346178\n",
      "Step: [4251] d_loss: 0.79832202, g_loss: 0.86855960\n",
      "Step: [4252] d_loss: 0.68721348, g_loss: 0.86793852\n",
      "Step: [4253] d_loss: 0.73554301, g_loss: 0.92487603\n",
      "Step: [4254] d_loss: 0.69308585, g_loss: 0.92316192\n",
      "Step: [4255] d_loss: 0.71685791, g_loss: 0.92191470\n",
      "Step: [4256] d_loss: 0.67155552, g_loss: 0.97473973\n",
      "Step: [4257] d_loss: 0.73052460, g_loss: 0.93590367\n",
      "Step: [4258] d_loss: 0.68292081, g_loss: 0.92055207\n",
      "Step: [4259] d_loss: 0.71766287, g_loss: 0.93576908\n",
      "Step: [4260] d_loss: 0.71150047, g_loss: 0.96176523\n",
      "Step: [4261] d_loss: 0.76507187, g_loss: 0.93618584\n",
      "Step: [4262] d_loss: 0.61360705, g_loss: 0.97138691\n",
      "Step: [4263] d_loss: 0.68287748, g_loss: 0.95356911\n",
      "Step: [4264] d_loss: 0.63021696, g_loss: 0.92854965\n",
      "Step: [4265] d_loss: 0.69148880, g_loss: 0.98354489\n",
      "Step: [4266] d_loss: 0.72458923, g_loss: 0.90020192\n",
      "Step: [4267] d_loss: 0.74872196, g_loss: 0.91670334\n",
      "Step: [4268] d_loss: 0.71958131, g_loss: 0.96346283\n",
      "Step: [4269] d_loss: 0.66941935, g_loss: 0.92044258\n",
      "Step: [4270] d_loss: 0.73664510, g_loss: 0.94640547\n",
      "Step: [4271] d_loss: 0.65974206, g_loss: 0.89979011\n",
      "Step: [4272] d_loss: 0.75148338, g_loss: 0.92098552\n",
      "Step: [4273] d_loss: 0.69739687, g_loss: 0.92130727\n",
      "Step: [4274] d_loss: 0.64311397, g_loss: 0.90935403\n",
      "Step: [4275] d_loss: 0.65231586, g_loss: 0.92509878\n",
      "Step: [4276] d_loss: 0.71053940, g_loss: 0.88452864\n",
      "Step: [4277] d_loss: 0.69950753, g_loss: 0.92286628\n",
      "Step: [4278] d_loss: 0.70579094, g_loss: 0.89503765\n",
      "Step: [4279] d_loss: 0.79227734, g_loss: 0.94620365\n",
      "Step: [4280] d_loss: 0.75385332, g_loss: 0.97410524\n",
      "Step: [4281] d_loss: 0.69347107, g_loss: 0.92907619\n",
      "Step: [4282] d_loss: 0.76104468, g_loss: 0.93762010\n",
      "Step: [4283] d_loss: 0.77965993, g_loss: 0.94000220\n",
      "Step: [4284] d_loss: 0.68919182, g_loss: 0.92271686\n",
      "Step: [4285] d_loss: 0.78819710, g_loss: 0.88717026\n",
      "Step: [4286] d_loss: 0.71930552, g_loss: 0.89889473\n",
      "Step: [4287] d_loss: 0.71105790, g_loss: 0.92428899\n",
      "Step: [4288] d_loss: 0.66622734, g_loss: 0.93342620\n",
      "Step: [4289] d_loss: 0.67379069, g_loss: 0.90850937\n",
      "Step: [4290] d_loss: 0.64287651, g_loss: 0.94295263\n",
      "Step: [4291] d_loss: 0.82041079, g_loss: 0.88458133\n",
      "Step: [4292] d_loss: 0.77391279, g_loss: 0.88366729\n",
      "Step: [4293] d_loss: 0.80151284, g_loss: 0.92160583\n",
      "Step: [4294] d_loss: 0.74493265, g_loss: 0.94487268\n",
      "Step: [4295] d_loss: 0.74790847, g_loss: 0.96850008\n",
      "Step: [4296] d_loss: 0.66694880, g_loss: 0.92587900\n",
      "Step: [4297] d_loss: 0.70533216, g_loss: 0.91302699\n",
      "Step: [4298] d_loss: 0.72666895, g_loss: 0.86591202\n",
      "Step: [4299] d_loss: 0.69403458, g_loss: 0.93181968\n",
      "Step: [4300] d_loss: 0.71832001, g_loss: 0.93892145\n",
      "Step: [4301] d_loss: 0.72414047, g_loss: 0.95394045\n",
      "Step: [4302] d_loss: 0.71420574, g_loss: 0.98407960\n",
      "Step: [4303] d_loss: 0.72258580, g_loss: 0.97776097\n",
      "Step: [4304] d_loss: 0.75543094, g_loss: 0.94661397\n",
      "Step: [4305] d_loss: 0.73601121, g_loss: 0.94275880\n",
      "Step: [4306] d_loss: 0.67547524, g_loss: 0.94161177\n",
      "Step: [4307] d_loss: 0.74133372, g_loss: 0.91693687\n",
      "Step: [4308] d_loss: 0.75178790, g_loss: 0.93425280\n",
      "Step: [4309] d_loss: 0.69176209, g_loss: 0.92480195\n",
      "Step: [4310] d_loss: 0.70132613, g_loss: 0.90664732\n",
      "Step: [4311] d_loss: 0.71190071, g_loss: 0.81493890\n",
      "Step: [4312] d_loss: 0.76650703, g_loss: 0.88738251\n",
      "Step: [4313] d_loss: 0.74457902, g_loss: 0.90198767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4314] d_loss: 0.69912422, g_loss: 0.90287095\n",
      "Step: [4315] d_loss: 0.78600371, g_loss: 0.90362203\n",
      "Step: [4316] d_loss: 0.77007598, g_loss: 0.86768115\n",
      "Step: [4317] d_loss: 0.66916376, g_loss: 0.88060993\n",
      "Step: [4318] d_loss: 0.67480940, g_loss: 0.89102584\n",
      "Step: [4319] d_loss: 0.77721989, g_loss: 0.85238606\n",
      "Step: [4320] d_loss: 0.72619051, g_loss: 0.88827640\n",
      "Step: [4321] d_loss: 0.80870664, g_loss: 0.88773328\n",
      "Step: [4322] d_loss: 0.83911401, g_loss: 0.89979565\n",
      "Step: [4323] d_loss: 0.71608412, g_loss: 0.92286134\n",
      "Step: [4324] d_loss: 0.73803002, g_loss: 0.95543069\n",
      "Step: [4325] d_loss: 0.79384959, g_loss: 0.90438014\n",
      "Step: [4326] d_loss: 0.66727251, g_loss: 0.94772559\n",
      "Step: [4327] d_loss: 0.72448599, g_loss: 0.90279698\n",
      "Step: [4328] d_loss: 0.70605254, g_loss: 0.93571603\n",
      "Step: [4329] d_loss: 0.79235840, g_loss: 0.92400664\n",
      "Step: [4330] d_loss: 0.70108235, g_loss: 0.90109402\n",
      "Step: [4331] d_loss: 0.74866056, g_loss: 0.95734239\n",
      "Step: [4332] d_loss: 0.73636305, g_loss: 0.91885477\n",
      "Step: [4333] d_loss: 0.74328065, g_loss: 0.91150320\n",
      "Step: [4334] d_loss: 0.70410293, g_loss: 0.93316162\n",
      "Step: [4335] d_loss: 0.66020888, g_loss: 0.90937996\n",
      "Step: [4336] d_loss: 0.70024478, g_loss: 0.94405764\n",
      "Step: [4337] d_loss: 0.75084537, g_loss: 0.91261464\n",
      "Step: [4338] d_loss: 0.77931380, g_loss: 0.86379349\n",
      "Step: [4339] d_loss: 0.82149595, g_loss: 0.89711118\n",
      "Step: [4340] d_loss: 0.78283846, g_loss: 0.86712503\n",
      "Step: [4341] d_loss: 0.69148254, g_loss: 0.93501616\n",
      "Step: [4342] d_loss: 0.84541643, g_loss: 0.89589900\n",
      "Step: [4343] d_loss: 0.71188682, g_loss: 0.92211729\n",
      "Step: [4344] d_loss: 0.74628949, g_loss: 0.88146228\n",
      "Step: [4345] d_loss: 0.75455129, g_loss: 0.92950779\n",
      "Step: [4346] d_loss: 0.71755415, g_loss: 0.92763293\n",
      "Step: [4347] d_loss: 0.73089755, g_loss: 0.90800387\n",
      "Step: [4348] d_loss: 0.69512343, g_loss: 0.97805536\n",
      "Step: [4349] d_loss: 0.67687529, g_loss: 0.92769390\n",
      "Step: [4350] d_loss: 0.70894295, g_loss: 0.87645906\n",
      "Step: [4351] d_loss: 0.73976260, g_loss: 0.91775542\n",
      "Step: [4352] d_loss: 0.69564325, g_loss: 0.91807204\n",
      "Step: [4353] d_loss: 0.77313036, g_loss: 0.89842492\n",
      "Step: [4354] d_loss: 0.76315343, g_loss: 0.98071969\n",
      "Step: [4355] d_loss: 0.66587400, g_loss: 0.99018812\n",
      "Step: [4356] d_loss: 0.77363312, g_loss: 0.91302472\n",
      "Step: [4357] d_loss: 0.78977805, g_loss: 0.95544595\n",
      "Step: [4358] d_loss: 0.66596186, g_loss: 0.91107535\n",
      "Step: [4359] d_loss: 0.83016503, g_loss: 0.88823229\n",
      "Step: [4360] d_loss: 0.71360916, g_loss: 0.90570754\n",
      "Step: [4361] d_loss: 0.70038319, g_loss: 0.93054694\n",
      "Step: [4362] d_loss: 0.69718003, g_loss: 0.87742102\n",
      "Step: [4363] d_loss: 0.74073720, g_loss: 0.92420042\n",
      "Step: [4364] d_loss: 0.78710538, g_loss: 0.92550641\n",
      "Step: [4365] d_loss: 0.68077850, g_loss: 0.94566065\n",
      "Step: [4366] d_loss: 0.71995020, g_loss: 0.93363321\n",
      "Step: [4367] d_loss: 0.67901528, g_loss: 0.96736991\n",
      "Step: [4368] d_loss: 0.70328957, g_loss: 0.93190420\n",
      "Step: [4369] d_loss: 0.75603503, g_loss: 1.00215852\n",
      "Step: [4370] d_loss: 0.74142587, g_loss: 0.87977314\n",
      "Step: [4371] d_loss: 0.75122368, g_loss: 0.90185201\n",
      "Step: [4372] d_loss: 0.67559671, g_loss: 0.90217048\n",
      "Step: [4373] d_loss: 0.66447568, g_loss: 0.94169420\n",
      "Step: [4374] d_loss: 0.74113309, g_loss: 0.90028125\n",
      "Step: [4375] d_loss: 0.65202278, g_loss: 0.97754741\n",
      "Step: [4376] d_loss: 0.73018450, g_loss: 0.99948615\n",
      "Step: [4377] d_loss: 0.71821105, g_loss: 0.95096195\n",
      "Step: [4378] d_loss: 0.69345611, g_loss: 0.92214143\n",
      "Step: [4379] d_loss: 0.72483045, g_loss: 0.95011264\n",
      "Step: [4380] d_loss: 0.68592215, g_loss: 0.93047595\n",
      "Step: [4381] d_loss: 0.74074674, g_loss: 0.89232510\n",
      "Step: [4382] d_loss: 0.71068245, g_loss: 0.91007888\n",
      "Step: [4383] d_loss: 0.71198094, g_loss: 0.92265630\n",
      "Step: [4384] d_loss: 0.68959481, g_loss: 0.98435539\n",
      "Step: [4385] d_loss: 0.74106139, g_loss: 0.97763896\n",
      "Step: [4386] d_loss: 0.67210817, g_loss: 0.95877141\n",
      "Step: [4387] d_loss: 0.78582704, g_loss: 0.87091285\n",
      "Step: [4388] d_loss: 0.73168403, g_loss: 0.85496014\n",
      "Step: [4389] d_loss: 0.76037419, g_loss: 0.92250466\n",
      "Step: [4390] d_loss: 0.70450336, g_loss: 0.93069768\n",
      "Step: [4391] d_loss: 0.74220699, g_loss: 0.93273413\n",
      "Step: [4392] d_loss: 0.69200182, g_loss: 0.92352974\n",
      "Step: [4393] d_loss: 0.75743276, g_loss: 0.89982665\n",
      "Step: [4394] d_loss: 0.78216189, g_loss: 0.90628874\n",
      "Step: [4395] d_loss: 0.79440659, g_loss: 0.93994224\n",
      "Step: [4396] d_loss: 0.67646390, g_loss: 0.94824350\n",
      "Step: [4397] d_loss: 0.75624675, g_loss: 0.95692903\n",
      "Step: [4398] d_loss: 0.71853340, g_loss: 0.91233683\n",
      "Step: [4399] d_loss: 0.75824523, g_loss: 0.90407467\n",
      "Step: [4400] d_loss: 0.74831259, g_loss: 0.92333210\n",
      "Step: [4401] d_loss: 0.76829916, g_loss: 0.96129501\n",
      "Step: [4402] d_loss: 0.63843840, g_loss: 0.97475690\n",
      "Step: [4403] d_loss: 0.69063938, g_loss: 0.90098083\n",
      "Step: [4404] d_loss: 0.77910829, g_loss: 0.89315629\n",
      "Step: [4405] d_loss: 0.73848504, g_loss: 0.93596476\n",
      "Step: [4406] d_loss: 0.73791188, g_loss: 0.88269538\n",
      "Step: [4407] d_loss: 0.85516036, g_loss: 0.88006359\n",
      "Step: [4408] d_loss: 0.85869849, g_loss: 0.86171573\n",
      "Step: [4409] d_loss: 0.73525316, g_loss: 0.92195243\n",
      "Step: [4410] d_loss: 0.60912198, g_loss: 0.88045913\n",
      "Step: [4411] d_loss: 0.81230372, g_loss: 0.91357380\n",
      "Step: [4412] d_loss: 0.70566046, g_loss: 0.90520203\n",
      "Step: [4413] d_loss: 0.72582984, g_loss: 0.85941374\n",
      "Step: [4414] d_loss: 0.73238802, g_loss: 0.92847693\n",
      "Step: [4415] d_loss: 0.74275243, g_loss: 0.89121443\n",
      "Step: [4416] d_loss: 0.70375270, g_loss: 0.88033819\n",
      "Step: [4417] d_loss: 0.65514439, g_loss: 0.92821032\n",
      "Step: [4418] d_loss: 0.71167743, g_loss: 0.96047610\n",
      "Step: [4419] d_loss: 0.80017310, g_loss: 0.90097767\n",
      "Step: [4420] d_loss: 0.67952341, g_loss: 0.91775060\n",
      "Step: [4421] d_loss: 0.72974545, g_loss: 0.94496208\n",
      "Step: [4422] d_loss: 0.77267832, g_loss: 0.90272677\n",
      "Step: [4423] d_loss: 0.83892339, g_loss: 0.83578432\n",
      "Step: [4424] d_loss: 0.69403082, g_loss: 0.89912730\n",
      "Step: [4425] d_loss: 0.84039015, g_loss: 0.88324261\n",
      "Step: [4426] d_loss: 0.75322568, g_loss: 0.95789433\n",
      "Step: [4427] d_loss: 0.74333763, g_loss: 0.95494735\n",
      "Step: [4428] d_loss: 0.72882801, g_loss: 0.91395324\n",
      "Step: [4429] d_loss: 0.76241565, g_loss: 0.93023837\n",
      "Step: [4430] d_loss: 0.71948808, g_loss: 0.95536852\n",
      "Step: [4431] d_loss: 0.70873255, g_loss: 0.89194113\n",
      "Step: [4432] d_loss: 0.68965369, g_loss: 0.84583706\n",
      "Step: [4433] d_loss: 0.72058839, g_loss: 0.87026238\n",
      "Step: [4434] d_loss: 0.74878705, g_loss: 0.84616983\n",
      "Step: [4435] d_loss: 0.78247440, g_loss: 0.87895799\n",
      "Step: [4436] d_loss: 0.79131359, g_loss: 0.90770274\n",
      "Step: [4437] d_loss: 0.79027325, g_loss: 0.88302082\n",
      "Step: [4438] d_loss: 0.67078030, g_loss: 0.93900830\n",
      "Step: [4439] d_loss: 0.76724207, g_loss: 0.95488250\n",
      "Step: [4440] d_loss: 0.81608850, g_loss: 0.90346700\n",
      "Step: [4441] d_loss: 0.76351273, g_loss: 0.98409438\n",
      "Step: [4442] d_loss: 0.68579918, g_loss: 1.00947428\n",
      "Step: [4443] d_loss: 0.79911029, g_loss: 0.91265750\n",
      "Step: [4444] d_loss: 0.65936166, g_loss: 0.96106499\n",
      "Step: [4445] d_loss: 0.81369716, g_loss: 0.98519063\n",
      "Step: [4446] d_loss: 0.78565347, g_loss: 0.92199016\n",
      "Step: [4447] d_loss: 0.72632980, g_loss: 0.90499854\n",
      "Step: [4448] d_loss: 0.63634014, g_loss: 0.88057089\n",
      "Step: [4449] d_loss: 0.69053352, g_loss: 0.86209184\n",
      "Step: [4450] d_loss: 0.74525017, g_loss: 0.86013514\n",
      "Step: [4451] d_loss: 0.65265322, g_loss: 0.91569066\n",
      "Step: [4452] d_loss: 0.66047901, g_loss: 0.91191131\n",
      "Step: [4453] d_loss: 0.73368818, g_loss: 0.90002507\n",
      "Step: [4454] d_loss: 0.65818346, g_loss: 0.99476361\n",
      "Step: [4455] d_loss: 0.71174192, g_loss: 0.92090112\n",
      "Step: [4456] d_loss: 0.73472929, g_loss: 0.89576370\n",
      "Step: [4457] d_loss: 0.75206035, g_loss: 0.92319405\n",
      "Step: [4458] d_loss: 0.80454540, g_loss: 0.90308172\n",
      "Step: [4459] d_loss: 0.81453800, g_loss: 0.91308683\n",
      "Step: [4460] d_loss: 0.62833822, g_loss: 0.88975072\n",
      "Step: [4461] d_loss: 0.66887152, g_loss: 0.91321069\n",
      "Step: [4462] d_loss: 0.75787127, g_loss: 0.94393909\n",
      "Step: [4463] d_loss: 0.81398439, g_loss: 0.92435217\n",
      "Step: [4464] d_loss: 0.76211965, g_loss: 0.89183599\n",
      "Step: [4465] d_loss: 0.71794391, g_loss: 0.91632932\n",
      "Step: [4466] d_loss: 0.73448604, g_loss: 0.94560605\n",
      "Step: [4467] d_loss: 0.72723025, g_loss: 0.97812903\n",
      "Step: [4468] d_loss: 0.69244266, g_loss: 0.89520019\n",
      "Step: [4469] d_loss: 0.78288215, g_loss: 0.92510444\n",
      "Step: [4470] d_loss: 0.65882474, g_loss: 0.95638788\n",
      "Step: [4471] d_loss: 0.71013421, g_loss: 0.92485726\n",
      "Step: [4472] d_loss: 0.72072959, g_loss: 0.91729128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4473] d_loss: 0.62221634, g_loss: 0.95490903\n",
      "Step: [4474] d_loss: 0.61973315, g_loss: 0.91062814\n",
      "Step: [4475] d_loss: 0.80063510, g_loss: 0.86836457\n",
      "Step: [4476] d_loss: 0.74606550, g_loss: 0.88039976\n",
      "Step: [4477] d_loss: 0.65021485, g_loss: 0.90875155\n",
      "Step: [4478] d_loss: 0.69937140, g_loss: 0.95673418\n",
      "Step: [4479] d_loss: 0.67574972, g_loss: 0.93506187\n",
      "Step: [4480] d_loss: 0.68335706, g_loss: 0.97676343\n",
      "Step: [4481] d_loss: 0.72026753, g_loss: 0.95727938\n",
      "Step: [4482] d_loss: 0.75061250, g_loss: 0.91705847\n",
      "Step: [4483] d_loss: 0.82184368, g_loss: 0.93813747\n",
      "Step: [4484] d_loss: 0.75893849, g_loss: 0.95311850\n",
      "Step: [4485] d_loss: 0.72672409, g_loss: 0.94582355\n",
      "Step: [4486] d_loss: 0.79113126, g_loss: 0.88774288\n",
      "Step: [4487] d_loss: 0.71620131, g_loss: 0.97585356\n",
      "Step: [4488] d_loss: 0.71220493, g_loss: 0.93376720\n",
      "Step: [4489] d_loss: 0.74891853, g_loss: 0.93247920\n",
      "Step: [4490] d_loss: 0.67196596, g_loss: 0.91633970\n",
      "Step: [4491] d_loss: 0.76092494, g_loss: 0.90307862\n",
      "Step: [4492] d_loss: 0.74926007, g_loss: 0.90254921\n",
      "Step: [4493] d_loss: 0.71434498, g_loss: 0.90677571\n",
      "Step: [4494] d_loss: 0.75400740, g_loss: 0.85136276\n",
      "Step: [4495] d_loss: 0.69869095, g_loss: 0.96707863\n",
      "Step: [4496] d_loss: 0.74584216, g_loss: 0.92301250\n",
      "Step: [4497] d_loss: 0.69635755, g_loss: 0.96126592\n",
      "Step: [4498] d_loss: 0.85742652, g_loss: 0.93599772\n",
      "Step: [4499] d_loss: 0.72731864, g_loss: 0.93932754\n",
      "Step: [4500] d_loss: 0.71701747, g_loss: 0.90930724\n",
      "Step: [4501] d_loss: 0.76463151, g_loss: 0.93615353\n",
      "Step: [4502] d_loss: 0.76948661, g_loss: 0.95395750\n",
      "Step: [4503] d_loss: 0.70756388, g_loss: 0.92370129\n",
      "Step: [4504] d_loss: 0.64244354, g_loss: 0.91396755\n",
      "Step: [4505] d_loss: 0.76372218, g_loss: 0.90066922\n",
      "Step: [4506] d_loss: 0.72600609, g_loss: 0.93684953\n",
      "Step: [4507] d_loss: 0.83092785, g_loss: 0.84331477\n",
      "Step: [4508] d_loss: 0.73650742, g_loss: 0.84680849\n",
      "Step: [4509] d_loss: 0.78624606, g_loss: 0.94982374\n",
      "Step: [4510] d_loss: 0.64712191, g_loss: 0.86314958\n",
      "Step: [4511] d_loss: 0.85263264, g_loss: 0.87196887\n",
      "Step: [4512] d_loss: 0.76032227, g_loss: 0.99689531\n",
      "Step: [4513] d_loss: 0.65198410, g_loss: 0.97349471\n",
      "Step: [4514] d_loss: 0.73791498, g_loss: 0.91701943\n",
      "Step: [4515] d_loss: 0.66299975, g_loss: 0.94901174\n",
      "Step: [4516] d_loss: 0.71466333, g_loss: 0.91953963\n",
      "Step: [4517] d_loss: 0.67472297, g_loss: 0.88387084\n",
      "Step: [4518] d_loss: 0.65609086, g_loss: 0.95529580\n",
      "Step: [4519] d_loss: 0.74667066, g_loss: 0.89966774\n",
      "Step: [4520] d_loss: 0.66597813, g_loss: 0.92338729\n",
      "Step: [4521] d_loss: 0.79160219, g_loss: 0.84822446\n",
      "Step: [4522] d_loss: 0.62326115, g_loss: 0.95212364\n",
      "Step: [4523] d_loss: 0.73107016, g_loss: 0.90550405\n",
      "Step: [4524] d_loss: 0.72285920, g_loss: 0.90070701\n",
      "Step: [4525] d_loss: 0.73361361, g_loss: 0.92603493\n",
      "Step: [4526] d_loss: 0.67025328, g_loss: 0.92554170\n",
      "Step: [4527] d_loss: 0.77957064, g_loss: 0.97850430\n",
      "Step: [4528] d_loss: 0.72817880, g_loss: 0.92903596\n",
      "Step: [4529] d_loss: 0.67906225, g_loss: 0.92612195\n",
      "Step: [4530] d_loss: 0.72123033, g_loss: 0.90261090\n",
      "Step: [4531] d_loss: 0.66481781, g_loss: 0.85608572\n",
      "Step: [4532] d_loss: 0.82666647, g_loss: 0.90338558\n",
      "Step: [4533] d_loss: 0.79434913, g_loss: 0.88249373\n",
      "Step: [4534] d_loss: 0.81733149, g_loss: 0.88740647\n",
      "Step: [4535] d_loss: 0.73931760, g_loss: 0.93438816\n",
      "Step: [4536] d_loss: 0.81729937, g_loss: 0.93539149\n",
      "Step: [4537] d_loss: 0.77435625, g_loss: 0.97483540\n",
      "Step: [4538] d_loss: 0.73203731, g_loss: 0.95661604\n",
      "Step: [4539] d_loss: 0.84834927, g_loss: 0.91962361\n",
      "Step: [4540] d_loss: 0.73810107, g_loss: 0.91768599\n",
      "Step: [4541] d_loss: 0.73530251, g_loss: 0.96695095\n",
      "Step: [4542] d_loss: 0.71458572, g_loss: 0.97516143\n",
      "Step: [4543] d_loss: 0.66094583, g_loss: 0.92110842\n",
      "Step: [4544] d_loss: 0.80524909, g_loss: 0.95618540\n",
      "Step: [4545] d_loss: 0.71762538, g_loss: 0.93260431\n",
      "Step: [4546] d_loss: 0.77812850, g_loss: 0.87093461\n",
      "Step: [4547] d_loss: 0.78186870, g_loss: 0.88710463\n",
      "Step: [4548] d_loss: 0.67680347, g_loss: 0.90349811\n",
      "Step: [4549] d_loss: 0.78118050, g_loss: 0.93089330\n",
      "Step: [4550] d_loss: 0.77586114, g_loss: 0.91960585\n",
      "Step: [4551] d_loss: 0.78637779, g_loss: 0.93835127\n",
      "Step: [4552] d_loss: 0.74450797, g_loss: 0.88171369\n",
      "Step: [4553] d_loss: 0.62193668, g_loss: 0.85142827\n",
      "Step: [4554] d_loss: 0.66710788, g_loss: 0.88027287\n",
      "Step: [4555] d_loss: 0.79233146, g_loss: 0.85524350\n",
      "Step: [4556] d_loss: 0.79132557, g_loss: 0.89594859\n",
      "Step: [4557] d_loss: 0.74667913, g_loss: 0.96710336\n",
      "Step: [4558] d_loss: 0.75701833, g_loss: 0.95330095\n",
      "Step: [4559] d_loss: 0.70621598, g_loss: 0.95717263\n",
      "Step: [4560] d_loss: 0.77039713, g_loss: 0.90876174\n",
      "Step: [4561] d_loss: 0.66278267, g_loss: 0.94503534\n",
      "Step: [4562] d_loss: 0.68141836, g_loss: 0.94050378\n",
      "Step: [4563] d_loss: 0.66580957, g_loss: 0.94523686\n",
      "Step: [4564] d_loss: 0.70591742, g_loss: 0.94490224\n",
      "Step: [4565] d_loss: 0.76025409, g_loss: 0.91193026\n",
      "Step: [4566] d_loss: 0.69365090, g_loss: 0.92986709\n",
      "Step: [4567] d_loss: 0.71317244, g_loss: 0.93280625\n",
      "Step: [4568] d_loss: 0.65534389, g_loss: 0.91151804\n",
      "Step: [4569] d_loss: 0.77997124, g_loss: 0.88831848\n",
      "Step: [4570] d_loss: 0.76579231, g_loss: 0.86160880\n",
      "Step: [4571] d_loss: 0.73424858, g_loss: 0.83531791\n",
      "Step: [4572] d_loss: 0.71274000, g_loss: 0.92139977\n",
      "Step: [4573] d_loss: 0.68382144, g_loss: 0.97821212\n",
      "Step: [4574] d_loss: 0.64177632, g_loss: 0.98079610\n",
      "Step: [4575] d_loss: 0.66830415, g_loss: 0.93977302\n",
      "Step: [4576] d_loss: 0.74633855, g_loss: 0.88722420\n",
      "Step: [4577] d_loss: 0.75654364, g_loss: 0.90664977\n",
      "Step: [4578] d_loss: 0.68873382, g_loss: 0.94926625\n",
      "Step: [4579] d_loss: 0.70168662, g_loss: 0.93592453\n",
      "Step: [4580] d_loss: 0.72963369, g_loss: 0.99444377\n",
      "Step: [4581] d_loss: 0.82863718, g_loss: 0.98379779\n",
      "Step: [4582] d_loss: 0.72980988, g_loss: 0.92981583\n",
      "Step: [4583] d_loss: 0.82391578, g_loss: 0.85274172\n",
      "Step: [4584] d_loss: 0.67677170, g_loss: 0.89351130\n",
      "Step: [4585] d_loss: 0.73480010, g_loss: 0.99943447\n",
      "Step: [4586] d_loss: 0.71743673, g_loss: 0.91913110\n",
      "Step: [4587] d_loss: 0.68274629, g_loss: 0.94785309\n",
      "Step: [4588] d_loss: 0.66258842, g_loss: 0.95453697\n",
      "Step: [4589] d_loss: 0.74657923, g_loss: 0.93042421\n",
      "Step: [4590] d_loss: 0.74399740, g_loss: 0.92301285\n",
      "Step: [4591] d_loss: 0.81481761, g_loss: 0.89857763\n",
      "Step: [4592] d_loss: 0.68429977, g_loss: 0.94675738\n",
      "Step: [4593] d_loss: 0.67381740, g_loss: 0.96546191\n",
      "Step: [4594] d_loss: 0.69511861, g_loss: 0.90116560\n",
      "Step: [4595] d_loss: 0.82889462, g_loss: 0.87278020\n",
      "Step: [4596] d_loss: 0.70787275, g_loss: 0.92020464\n",
      "Step: [4597] d_loss: 0.70523214, g_loss: 0.90533137\n",
      "Step: [4598] d_loss: 0.73905003, g_loss: 0.93480265\n",
      "Step: [4599] d_loss: 0.75637937, g_loss: 0.86311084\n",
      "Step: [4600] d_loss: 0.73702949, g_loss: 0.93694943\n",
      "Step: [4601] d_loss: 0.69089425, g_loss: 0.94403243\n",
      "Step: [4602] d_loss: 0.71508121, g_loss: 0.94953239\n",
      "Step: [4603] d_loss: 0.74139827, g_loss: 0.97405809\n",
      "Step: [4604] d_loss: 0.80427474, g_loss: 0.91433287\n",
      "Step: [4605] d_loss: 0.78154612, g_loss: 0.98539376\n",
      "Step: [4606] d_loss: 0.74825746, g_loss: 0.87301964\n",
      "Step: [4607] d_loss: 0.77071112, g_loss: 0.90745991\n",
      "Step: [4608] d_loss: 0.70839787, g_loss: 0.91300970\n",
      "Step: [4609] d_loss: 0.70436198, g_loss: 0.91269499\n",
      "Step: [4610] d_loss: 0.72773838, g_loss: 0.92801076\n",
      "Step: [4611] d_loss: 0.74342793, g_loss: 0.90581912\n",
      "Step: [4612] d_loss: 0.71562976, g_loss: 0.94234955\n",
      "Step: [4613] d_loss: 0.67514592, g_loss: 0.93266147\n",
      "Step: [4614] d_loss: 0.67993045, g_loss: 0.89554250\n",
      "Step: [4615] d_loss: 0.74824035, g_loss: 0.85645330\n",
      "Step: [4616] d_loss: 0.68330795, g_loss: 0.95327735\n",
      "Step: [4617] d_loss: 0.75142121, g_loss: 0.92612785\n",
      "Step: [4618] d_loss: 0.67594862, g_loss: 0.91143274\n",
      "Step: [4619] d_loss: 0.71102965, g_loss: 0.95622683\n",
      "Step: [4620] d_loss: 0.79473943, g_loss: 0.86812794\n",
      "Step: [4621] d_loss: 0.77466172, g_loss: 0.90751648\n",
      "Step: [4622] d_loss: 0.68941212, g_loss: 0.95502663\n",
      "Step: [4623] d_loss: 0.68982202, g_loss: 0.91345477\n",
      "Step: [4624] d_loss: 0.79188353, g_loss: 0.87995243\n",
      "Step: [4625] d_loss: 0.70183617, g_loss: 0.90230411\n",
      "Step: [4626] d_loss: 0.70794868, g_loss: 0.90988648\n",
      "Step: [4627] d_loss: 0.77773374, g_loss: 0.88057947\n",
      "Step: [4628] d_loss: 0.82341683, g_loss: 0.88598055\n",
      "Step: [4629] d_loss: 0.73378259, g_loss: 0.93584979\n",
      "Step: [4630] d_loss: 0.69202584, g_loss: 0.90279794\n",
      "Step: [4631] d_loss: 0.68950027, g_loss: 0.89643258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4632] d_loss: 0.67753643, g_loss: 0.84492874\n",
      "Step: [4633] d_loss: 0.67759240, g_loss: 0.91079706\n",
      "Step: [4634] d_loss: 0.70723182, g_loss: 0.89393705\n",
      "Step: [4635] d_loss: 0.73302019, g_loss: 0.89285284\n",
      "Step: [4636] d_loss: 0.75487256, g_loss: 0.94748223\n",
      "Step: [4637] d_loss: 0.75844711, g_loss: 0.85720730\n",
      "Step: [4638] d_loss: 0.69321758, g_loss: 0.90504557\n",
      "Step: [4639] d_loss: 0.70667642, g_loss: 0.94345492\n",
      "Step: [4640] d_loss: 0.78320885, g_loss: 0.95162565\n",
      "Step: [4641] d_loss: 0.77316332, g_loss: 0.88234073\n",
      "Step: [4642] d_loss: 0.78261435, g_loss: 0.91574830\n",
      "Step: [4643] d_loss: 0.72954011, g_loss: 0.92088526\n",
      "Step: [4644] d_loss: 0.74770445, g_loss: 0.96669525\n",
      "Step: [4645] d_loss: 0.71460646, g_loss: 0.96280986\n",
      "Step: [4646] d_loss: 0.71089530, g_loss: 0.95323747\n",
      "Step: [4647] d_loss: 0.74031281, g_loss: 0.92218858\n",
      "Step: [4648] d_loss: 0.72402561, g_loss: 0.96091837\n",
      "Step: [4649] d_loss: 0.74568504, g_loss: 0.93970376\n",
      "Step: [4650] d_loss: 0.77997899, g_loss: 0.91980016\n",
      "Step: [4651] d_loss: 0.83874112, g_loss: 0.92542160\n",
      "Step: [4652] d_loss: 0.76819450, g_loss: 0.94976187\n",
      "Step: [4653] d_loss: 0.69944245, g_loss: 0.88705647\n",
      "Step: [4654] d_loss: 0.68723828, g_loss: 0.86804140\n",
      "Step: [4655] d_loss: 0.77021855, g_loss: 0.91872913\n",
      "Step: [4656] d_loss: 0.84652120, g_loss: 0.89882356\n",
      "Step: [4657] d_loss: 0.70745617, g_loss: 0.92865455\n",
      "Step: [4658] d_loss: 0.77839315, g_loss: 0.92105061\n",
      "Step: [4659] d_loss: 0.76407796, g_loss: 0.99517149\n",
      "Step: [4660] d_loss: 0.71451372, g_loss: 0.96175134\n",
      "Step: [4661] d_loss: 0.70572597, g_loss: 0.98690754\n",
      "Step: [4662] d_loss: 0.73280036, g_loss: 0.96344751\n",
      "Step: [4663] d_loss: 0.77919346, g_loss: 0.93451405\n",
      "Step: [4664] d_loss: 0.71857232, g_loss: 0.88857889\n",
      "Step: [4665] d_loss: 0.80758840, g_loss: 0.91304821\n",
      "Step: [4666] d_loss: 0.64287460, g_loss: 0.92322737\n",
      "Step: [4667] d_loss: 0.76595777, g_loss: 0.92973483\n",
      "Step: [4668] d_loss: 0.73286664, g_loss: 0.85850239\n",
      "Step: [4669] d_loss: 0.72022420, g_loss: 0.90568471\n",
      "Step: [4670] d_loss: 0.78244811, g_loss: 0.85847306\n",
      "Step: [4671] d_loss: 0.60689855, g_loss: 0.92151409\n",
      "Step: [4672] d_loss: 0.72465897, g_loss: 0.90221113\n",
      "Step: [4673] d_loss: 0.72755659, g_loss: 0.92030591\n",
      "Step: [4674] d_loss: 0.76704973, g_loss: 0.93908393\n",
      "Step: [4675] d_loss: 0.73219573, g_loss: 0.98796356\n",
      "Step: [4676] d_loss: 0.67912632, g_loss: 0.96469885\n",
      "Step: [4677] d_loss: 0.72665977, g_loss: 0.91474861\n",
      "Step: [4678] d_loss: 0.67778993, g_loss: 0.84045160\n",
      "Step: [4679] d_loss: 0.79035759, g_loss: 0.87707949\n",
      "Step: [4680] d_loss: 0.70471400, g_loss: 0.88671529\n",
      "Step: [4681] d_loss: 0.81322747, g_loss: 0.81981778\n",
      "Step: [4682] d_loss: 0.78270888, g_loss: 0.89998162\n",
      "Step: [4683] d_loss: 0.77370292, g_loss: 0.84732896\n",
      "Step: [4684] d_loss: 0.77475685, g_loss: 0.89810652\n",
      "Step: [4685] d_loss: 0.74081773, g_loss: 0.94559443\n",
      "Step: [4686] d_loss: 0.77131188, g_loss: 0.94179446\n",
      "Step: [4687] d_loss: 0.67593259, g_loss: 0.97721249\n",
      "Step: [4688] d_loss: 0.69179571, g_loss: 0.95625144\n",
      "Step: [4689] d_loss: 0.65233356, g_loss: 0.86551279\n",
      "Step: [4690] d_loss: 0.72542471, g_loss: 0.89728177\n",
      "Step: [4691] d_loss: 0.76720566, g_loss: 0.87298620\n",
      "Step: [4692] d_loss: 0.72806555, g_loss: 0.89975148\n",
      "Step: [4693] d_loss: 0.75204891, g_loss: 0.91263628\n",
      "Step: [4694] d_loss: 0.81772625, g_loss: 0.86638677\n",
      "Step: [4695] d_loss: 0.72723556, g_loss: 0.92337638\n",
      "Step: [4696] d_loss: 0.74642664, g_loss: 0.92435056\n",
      "Step: [4697] d_loss: 0.73010921, g_loss: 0.88729405\n",
      "Step: [4698] d_loss: 0.66793841, g_loss: 0.95124340\n",
      "Step: [4699] d_loss: 0.71732253, g_loss: 0.90869111\n",
      "Step: [4700] d_loss: 0.74623996, g_loss: 0.88628006\n",
      "Step: [4701] d_loss: 0.78381813, g_loss: 0.88566160\n",
      "Step: [4702] d_loss: 0.63786083, g_loss: 0.92238164\n",
      "Step: [4703] d_loss: 0.80940974, g_loss: 0.86817801\n",
      "Step: [4704] d_loss: 0.62172657, g_loss: 0.89617962\n",
      "Step: [4705] d_loss: 0.70672315, g_loss: 0.91460550\n",
      "Step: [4706] d_loss: 0.73877770, g_loss: 0.92861414\n",
      "Step: [4707] d_loss: 0.76412827, g_loss: 0.95936245\n",
      "Step: [4708] d_loss: 0.70605713, g_loss: 0.88184333\n",
      "Step: [4709] d_loss: 0.77925366, g_loss: 0.90619045\n",
      "Step: [4710] d_loss: 0.66867244, g_loss: 0.91223574\n",
      "Step: [4711] d_loss: 0.64399505, g_loss: 0.96359938\n",
      "Step: [4712] d_loss: 0.72635818, g_loss: 0.96832836\n",
      "Step: [4713] d_loss: 0.69928706, g_loss: 0.94713712\n",
      "Step: [4714] d_loss: 0.74348283, g_loss: 0.90270716\n",
      "Step: [4715] d_loss: 0.64384830, g_loss: 0.85521656\n",
      "Step: [4716] d_loss: 0.74091905, g_loss: 0.79842418\n",
      "Step: [4717] d_loss: 0.63859874, g_loss: 0.93274176\n",
      "Step: [4718] d_loss: 0.62514687, g_loss: 0.91615415\n",
      "Step: [4719] d_loss: 0.76126200, g_loss: 0.89394861\n",
      "Step: [4720] d_loss: 0.81057167, g_loss: 0.90309417\n",
      "Step: [4721] d_loss: 0.75556660, g_loss: 0.89430666\n",
      "Step: [4722] d_loss: 0.74273109, g_loss: 0.93073153\n",
      "Step: [4723] d_loss: 0.78367537, g_loss: 0.98195124\n",
      "Step: [4724] d_loss: 0.73586243, g_loss: 0.88683319\n",
      "Step: [4725] d_loss: 0.67488605, g_loss: 0.92724550\n",
      "Step: [4726] d_loss: 0.68751627, g_loss: 0.94791245\n",
      "Step: [4727] d_loss: 0.77465576, g_loss: 0.89082617\n",
      "Step: [4728] d_loss: 0.67360079, g_loss: 0.90295976\n",
      "Step: [4729] d_loss: 0.71323889, g_loss: 0.90810698\n",
      "Step: [4730] d_loss: 0.71010590, g_loss: 0.90378398\n",
      "Step: [4731] d_loss: 0.75377047, g_loss: 0.88685000\n",
      "Step: [4732] d_loss: 0.63351041, g_loss: 0.94810909\n",
      "Step: [4733] d_loss: 0.69192815, g_loss: 0.90666771\n",
      "Step: [4734] d_loss: 0.78213656, g_loss: 0.85468769\n",
      "Step: [4735] d_loss: 0.65993631, g_loss: 0.92074525\n",
      "Step: [4736] d_loss: 0.71189654, g_loss: 0.92221749\n",
      "Step: [4737] d_loss: 0.85633838, g_loss: 0.90431368\n",
      "Step: [4738] d_loss: 0.67851549, g_loss: 0.93507499\n",
      "Step: [4739] d_loss: 0.65478939, g_loss: 0.91405588\n",
      "Step: [4740] d_loss: 0.69420004, g_loss: 0.97589022\n",
      "Step: [4741] d_loss: 0.75406468, g_loss: 0.93437892\n",
      "Step: [4742] d_loss: 0.64693260, g_loss: 0.94793189\n",
      "Step: [4743] d_loss: 0.75915974, g_loss: 0.93583858\n",
      "Step: [4744] d_loss: 0.73134863, g_loss: 0.94159847\n",
      "Step: [4745] d_loss: 0.67521960, g_loss: 0.91397536\n",
      "Step: [4746] d_loss: 0.71548945, g_loss: 0.96766090\n",
      "Step: [4747] d_loss: 0.72907048, g_loss: 0.89865065\n",
      "Step: [4748] d_loss: 0.70764166, g_loss: 0.92586589\n",
      "Step: [4749] d_loss: 0.79223895, g_loss: 0.91860694\n",
      "Step: [4750] d_loss: 0.71110523, g_loss: 0.98751718\n",
      "Step: [4751] d_loss: 0.72050667, g_loss: 0.96575844\n",
      "Step: [4752] d_loss: 0.69526970, g_loss: 0.93422329\n",
      "Step: [4753] d_loss: 0.78128874, g_loss: 0.91215181\n",
      "Step: [4754] d_loss: 0.82856899, g_loss: 0.90805662\n",
      "Step: [4755] d_loss: 0.67996997, g_loss: 0.96059424\n",
      "Step: [4756] d_loss: 0.74825180, g_loss: 0.95194983\n",
      "Step: [4757] d_loss: 0.66266257, g_loss: 0.95888716\n",
      "Step: [4758] d_loss: 0.69909966, g_loss: 0.94093192\n",
      "Step: [4759] d_loss: 0.70755649, g_loss: 0.94597805\n",
      "Step: [4760] d_loss: 0.75557917, g_loss: 0.85928643\n",
      "Step: [4761] d_loss: 0.78328723, g_loss: 0.88535118\n",
      "Step: [4762] d_loss: 0.66646111, g_loss: 0.87963551\n",
      "Step: [4763] d_loss: 0.71427381, g_loss: 0.91060388\n",
      "Step: [4764] d_loss: 0.75158262, g_loss: 0.97580749\n",
      "Step: [4765] d_loss: 0.70697737, g_loss: 1.00872147\n",
      "Step: [4766] d_loss: 0.69255704, g_loss: 0.92267299\n",
      "Step: [4767] d_loss: 0.67239153, g_loss: 0.97979575\n",
      "Step: [4768] d_loss: 0.74214363, g_loss: 0.95643049\n",
      "Step: [4769] d_loss: 0.75464225, g_loss: 0.87196678\n",
      "Step: [4770] d_loss: 0.83023679, g_loss: 0.83534271\n",
      "Step: [4771] d_loss: 0.70365661, g_loss: 0.92100805\n",
      "Step: [4772] d_loss: 0.60721302, g_loss: 0.92145532\n",
      "Step: [4773] d_loss: 0.72333980, g_loss: 0.93039608\n",
      "Step: [4774] d_loss: 0.77655762, g_loss: 0.95241582\n",
      "Step: [4775] d_loss: 0.66951948, g_loss: 0.94485116\n",
      "Step: [4776] d_loss: 0.72098279, g_loss: 0.88459450\n",
      "Step: [4777] d_loss: 0.70079309, g_loss: 0.92445219\n",
      "Step: [4778] d_loss: 0.69723707, g_loss: 0.91471392\n",
      "Step: [4779] d_loss: 0.77086508, g_loss: 0.92411911\n",
      "Step: [4780] d_loss: 0.78186661, g_loss: 0.87854594\n",
      "Step: [4781] d_loss: 0.76645792, g_loss: 0.85190845\n",
      "Step: [4782] d_loss: 0.66908497, g_loss: 0.92381310\n",
      "Step: [4783] d_loss: 0.70600742, g_loss: 0.91552490\n",
      "Step: [4784] d_loss: 0.79470474, g_loss: 0.93492848\n",
      "Step: [4785] d_loss: 0.68439108, g_loss: 0.90731442\n",
      "Step: [4786] d_loss: 0.67784917, g_loss: 0.90070397\n",
      "Step: [4787] d_loss: 0.76905197, g_loss: 0.86987394\n",
      "Step: [4788] d_loss: 0.70127380, g_loss: 0.87550646\n",
      "Step: [4789] d_loss: 0.83147508, g_loss: 0.84049767\n",
      "Step: [4790] d_loss: 0.69568533, g_loss: 0.86059749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4791] d_loss: 0.77144986, g_loss: 0.87335509\n",
      "Step: [4792] d_loss: 0.75567919, g_loss: 0.89578152\n",
      "Step: [4793] d_loss: 0.72443324, g_loss: 0.91948384\n",
      "Step: [4794] d_loss: 0.78122360, g_loss: 0.95850688\n",
      "Step: [4795] d_loss: 0.81006336, g_loss: 0.91093445\n",
      "Step: [4796] d_loss: 0.67947263, g_loss: 0.90336883\n",
      "Step: [4797] d_loss: 0.78563106, g_loss: 0.90430027\n",
      "Step: [4798] d_loss: 0.78048348, g_loss: 0.91630602\n",
      "Step: [4799] d_loss: 0.66841686, g_loss: 0.93342304\n",
      "Step: [4800] d_loss: 0.78540891, g_loss: 0.91795021\n",
      "Step: [4801] d_loss: 0.77977246, g_loss: 0.90680885\n",
      "Step: [4802] d_loss: 0.77429670, g_loss: 0.92446834\n",
      "Step: [4803] d_loss: 0.74561387, g_loss: 0.89171666\n",
      "Step: [4804] d_loss: 0.66187435, g_loss: 0.88782984\n",
      "Step: [4805] d_loss: 0.72022295, g_loss: 0.86827517\n",
      "Step: [4806] d_loss: 0.77832270, g_loss: 0.90442431\n",
      "Step: [4807] d_loss: 0.66523051, g_loss: 0.86683649\n",
      "Step: [4808] d_loss: 0.67751032, g_loss: 0.88650370\n",
      "Step: [4809] d_loss: 0.69387740, g_loss: 0.90199214\n",
      "Step: [4810] d_loss: 0.72856534, g_loss: 0.87777644\n",
      "Step: [4811] d_loss: 0.66044891, g_loss: 0.96689427\n",
      "Step: [4812] d_loss: 0.72122759, g_loss: 0.86403602\n",
      "Step: [4813] d_loss: 0.62941355, g_loss: 0.86960012\n",
      "Step: [4814] d_loss: 0.76853800, g_loss: 0.88492411\n",
      "Step: [4815] d_loss: 0.72840267, g_loss: 0.92270881\n",
      "Step: [4816] d_loss: 0.63753045, g_loss: 0.91044188\n",
      "Step: [4817] d_loss: 0.71550888, g_loss: 0.90679735\n",
      "Step: [4818] d_loss: 0.61873841, g_loss: 0.97952217\n",
      "Step: [4819] d_loss: 0.72365785, g_loss: 0.89307898\n",
      "Step: [4820] d_loss: 0.82330155, g_loss: 0.90857995\n",
      "Step: [4821] d_loss: 0.76338482, g_loss: 0.86723793\n",
      "Step: [4822] d_loss: 0.77301311, g_loss: 0.94049406\n",
      "Step: [4823] d_loss: 0.75970590, g_loss: 0.94611013\n",
      "Step: [4824] d_loss: 0.78623432, g_loss: 0.86441422\n",
      "Step: [4825] d_loss: 0.72100765, g_loss: 0.89146185\n",
      "Step: [4826] d_loss: 0.71129817, g_loss: 0.94062972\n",
      "Step: [4827] d_loss: 0.74031311, g_loss: 0.92506242\n",
      "Step: [4828] d_loss: 0.84082979, g_loss: 0.88607192\n",
      "Step: [4829] d_loss: 0.76596576, g_loss: 0.92556483\n",
      "Step: [4830] d_loss: 0.71402949, g_loss: 0.95733875\n",
      "Step: [4831] d_loss: 0.69610292, g_loss: 0.89628178\n",
      "Step: [4832] d_loss: 0.72821873, g_loss: 0.88608694\n",
      "Step: [4833] d_loss: 0.83054250, g_loss: 0.83360702\n",
      "Step: [4834] d_loss: 0.72603303, g_loss: 0.87290442\n",
      "Step: [4835] d_loss: 0.69659829, g_loss: 0.93543983\n",
      "Step: [4836] d_loss: 0.72095823, g_loss: 0.87779164\n",
      "Step: [4837] d_loss: 0.83435774, g_loss: 0.86101544\n",
      "Step: [4838] d_loss: 0.70686197, g_loss: 0.90511000\n",
      "Step: [4839] d_loss: 0.72021174, g_loss: 0.94325835\n",
      "Step: [4840] d_loss: 0.74926370, g_loss: 0.91998208\n",
      "Step: [4841] d_loss: 0.75408268, g_loss: 0.90185201\n",
      "Step: [4842] d_loss: 0.67185915, g_loss: 0.89137924\n",
      "Step: [4843] d_loss: 0.69910735, g_loss: 0.94886929\n",
      "Step: [4844] d_loss: 0.68785840, g_loss: 0.95642483\n",
      "Step: [4845] d_loss: 0.71440029, g_loss: 0.91100693\n",
      "Step: [4846] d_loss: 0.73477441, g_loss: 0.92218041\n",
      "Step: [4847] d_loss: 0.70796639, g_loss: 0.88541597\n",
      "Step: [4848] d_loss: 0.70664108, g_loss: 0.89893740\n",
      "Step: [4849] d_loss: 0.71187139, g_loss: 0.90416932\n",
      "Step: [4850] d_loss: 0.73309124, g_loss: 0.86071080\n",
      "Step: [4851] d_loss: 0.75647777, g_loss: 0.96247816\n",
      "Step: [4852] d_loss: 0.73275220, g_loss: 0.86525166\n",
      "Step: [4853] d_loss: 0.72567362, g_loss: 0.94961452\n",
      "Step: [4854] d_loss: 0.72698402, g_loss: 0.94795454\n",
      "Step: [4855] d_loss: 0.72812486, g_loss: 0.94688451\n",
      "Step: [4856] d_loss: 0.70845509, g_loss: 0.98576021\n",
      "Step: [4857] d_loss: 0.69512159, g_loss: 0.93604523\n",
      "Step: [4858] d_loss: 0.71572196, g_loss: 0.86190838\n",
      "Step: [4859] d_loss: 0.67205822, g_loss: 0.84854668\n",
      "Step: [4860] d_loss: 0.65846258, g_loss: 0.92575914\n",
      "Step: [4861] d_loss: 0.77046770, g_loss: 0.84133476\n",
      "Step: [4862] d_loss: 0.77056861, g_loss: 0.85726655\n",
      "Step: [4863] d_loss: 0.72230065, g_loss: 0.88466555\n",
      "Step: [4864] d_loss: 0.67264414, g_loss: 0.96157587\n",
      "Step: [4865] d_loss: 0.70496160, g_loss: 0.94550818\n",
      "Step: [4866] d_loss: 0.68776482, g_loss: 0.97491062\n",
      "Step: [4867] d_loss: 0.70429891, g_loss: 0.91721791\n",
      "Step: [4868] d_loss: 0.69187403, g_loss: 0.89825779\n",
      "Step: [4869] d_loss: 0.70999563, g_loss: 0.93025720\n",
      "Step: [4870] d_loss: 0.72635645, g_loss: 0.91989744\n",
      "Step: [4871] d_loss: 0.63750744, g_loss: 0.86335254\n",
      "Step: [4872] d_loss: 0.68203425, g_loss: 0.87556243\n",
      "Step: [4873] d_loss: 0.65175265, g_loss: 0.88501161\n",
      "Step: [4874] d_loss: 0.69420892, g_loss: 0.95226836\n",
      "Step: [4875] d_loss: 0.77014291, g_loss: 0.90459812\n",
      "Step: [4876] d_loss: 0.66829878, g_loss: 0.93154085\n",
      "Step: [4877] d_loss: 0.66875100, g_loss: 0.97800100\n",
      "Step: [4878] d_loss: 0.77174705, g_loss: 0.98966801\n",
      "Step: [4879] d_loss: 0.76063454, g_loss: 0.88488543\n",
      "Step: [4880] d_loss: 0.71304393, g_loss: 0.89769453\n",
      "Step: [4881] d_loss: 0.73538011, g_loss: 0.87653011\n",
      "Step: [4882] d_loss: 0.81270909, g_loss: 0.89988619\n",
      "Step: [4883] d_loss: 0.82700187, g_loss: 0.83868915\n",
      "Step: [4884] d_loss: 0.71961188, g_loss: 0.93961710\n",
      "Step: [4885] d_loss: 0.66154319, g_loss: 0.93369913\n",
      "Step: [4886] d_loss: 0.71574730, g_loss: 0.89011562\n",
      "Step: [4887] d_loss: 0.74637300, g_loss: 0.91096693\n",
      "Step: [4888] d_loss: 0.67670888, g_loss: 0.86403751\n",
      "Step: [4889] d_loss: 0.71998471, g_loss: 0.85418606\n",
      "Step: [4890] d_loss: 0.64912748, g_loss: 0.93050057\n",
      "Step: [4891] d_loss: 0.77210021, g_loss: 0.89530206\n",
      "Step: [4892] d_loss: 0.83682489, g_loss: 0.90680653\n",
      "Step: [4893] d_loss: 0.69445419, g_loss: 0.94559538\n",
      "Step: [4894] d_loss: 0.73419631, g_loss: 0.94630015\n",
      "Step: [4895] d_loss: 0.72333938, g_loss: 0.92856634\n",
      "Step: [4896] d_loss: 0.73612547, g_loss: 0.91176987\n",
      "Step: [4897] d_loss: 0.77547091, g_loss: 0.89726716\n",
      "Step: [4898] d_loss: 0.66056889, g_loss: 0.95230848\n",
      "Step: [4899] d_loss: 0.72357529, g_loss: 0.90633768\n",
      "Step: [4900] d_loss: 0.65109503, g_loss: 0.84677845\n",
      "Step: [4901] d_loss: 0.72162420, g_loss: 0.90253818\n",
      "Step: [4902] d_loss: 0.65670562, g_loss: 0.98579061\n",
      "Step: [4903] d_loss: 0.75707620, g_loss: 0.87456983\n",
      "Step: [4904] d_loss: 0.70067316, g_loss: 0.95912296\n",
      "Step: [4905] d_loss: 0.69928616, g_loss: 0.93319750\n",
      "Step: [4906] d_loss: 0.69927186, g_loss: 0.88567775\n",
      "Step: [4907] d_loss: 0.73022050, g_loss: 0.92659485\n",
      "Step: [4908] d_loss: 0.73071706, g_loss: 0.93432742\n",
      "Step: [4909] d_loss: 0.76808202, g_loss: 0.88230062\n",
      "Step: [4910] d_loss: 0.69516343, g_loss: 1.00924242\n",
      "Step: [4911] d_loss: 0.63817114, g_loss: 0.94013214\n",
      "Step: [4912] d_loss: 0.65009212, g_loss: 0.90087682\n",
      "Step: [4913] d_loss: 0.66359597, g_loss: 0.92839670\n",
      "Step: [4914] d_loss: 0.70766640, g_loss: 0.88264394\n",
      "Step: [4915] d_loss: 0.69456148, g_loss: 0.86228114\n",
      "Step: [4916] d_loss: 0.82138181, g_loss: 0.88293356\n",
      "Step: [4917] d_loss: 0.63969702, g_loss: 0.87778246\n",
      "Step: [4918] d_loss: 0.65648490, g_loss: 0.91459382\n",
      "Step: [4919] d_loss: 0.71941966, g_loss: 0.90287369\n",
      "Step: [4920] d_loss: 0.70252049, g_loss: 0.95616716\n",
      "Step: [4921] d_loss: 0.74224514, g_loss: 0.94909346\n",
      "Step: [4922] d_loss: 0.77426654, g_loss: 0.93216002\n",
      "Step: [4923] d_loss: 0.74719542, g_loss: 0.92435539\n",
      "Step: [4924] d_loss: 0.68477863, g_loss: 0.94463575\n",
      "Step: [4925] d_loss: 0.76167434, g_loss: 0.92872727\n",
      "Step: [4926] d_loss: 0.77937460, g_loss: 0.92314595\n",
      "Step: [4927] d_loss: 0.72844684, g_loss: 0.94991553\n",
      "Step: [4928] d_loss: 0.65898371, g_loss: 0.91339344\n",
      "Step: [4929] d_loss: 0.70183182, g_loss: 0.97113329\n",
      "Step: [4930] d_loss: 0.71825874, g_loss: 0.95785493\n",
      "Step: [4931] d_loss: 0.81707293, g_loss: 0.88195693\n",
      "Step: [4932] d_loss: 0.74924523, g_loss: 0.92140979\n",
      "Step: [4933] d_loss: 0.81827980, g_loss: 0.89278543\n",
      "Step: [4934] d_loss: 0.73368001, g_loss: 0.94647366\n",
      "Step: [4935] d_loss: 0.81591672, g_loss: 0.87204838\n",
      "Step: [4936] d_loss: 0.75094050, g_loss: 0.82474488\n",
      "Step: [4937] d_loss: 0.61213547, g_loss: 0.90178561\n",
      "Step: [4938] d_loss: 0.76320130, g_loss: 0.82549977\n",
      "Step: [4939] d_loss: 0.76726353, g_loss: 0.85222542\n",
      "Step: [4940] d_loss: 0.68079311, g_loss: 0.90224540\n",
      "Step: [4941] d_loss: 0.69891924, g_loss: 0.91257799\n",
      "Step: [4942] d_loss: 0.66373801, g_loss: 0.83021784\n",
      "Step: [4943] d_loss: 0.76704890, g_loss: 0.84482664\n",
      "Step: [4944] d_loss: 0.65801287, g_loss: 0.94570243\n",
      "Step: [4945] d_loss: 0.65830415, g_loss: 0.92318833\n",
      "Step: [4946] d_loss: 0.74003428, g_loss: 0.90778410\n",
      "Step: [4947] d_loss: 0.68814331, g_loss: 0.88132179\n",
      "Step: [4948] d_loss: 0.74807179, g_loss: 0.87943304\n",
      "Step: [4949] d_loss: 0.70616686, g_loss: 0.90802079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [4950] d_loss: 0.69388139, g_loss: 0.92832214\n",
      "Step: [4951] d_loss: 0.70397258, g_loss: 0.91725785\n",
      "Step: [4952] d_loss: 0.82391709, g_loss: 0.86191487\n",
      "Step: [4953] d_loss: 0.90790009, g_loss: 0.96486968\n",
      "Step: [4954] d_loss: 0.69019872, g_loss: 0.95557809\n",
      "Step: [4955] d_loss: 0.75322050, g_loss: 0.93091726\n",
      "Step: [4956] d_loss: 0.64321041, g_loss: 0.90803081\n",
      "Step: [4957] d_loss: 0.66802931, g_loss: 0.95460230\n",
      "Step: [4958] d_loss: 0.74960905, g_loss: 0.94969428\n",
      "Step: [4959] d_loss: 0.75519764, g_loss: 0.96799845\n",
      "Step: [4960] d_loss: 0.78149694, g_loss: 0.93094027\n",
      "Step: [4961] d_loss: 0.73914576, g_loss: 0.94917989\n",
      "Step: [4962] d_loss: 0.66333401, g_loss: 0.91875601\n",
      "Step: [4963] d_loss: 0.69013077, g_loss: 0.91729629\n",
      "Step: [4964] d_loss: 0.77332187, g_loss: 0.87683457\n",
      "Step: [4965] d_loss: 0.66653711, g_loss: 0.85121638\n",
      "Step: [4966] d_loss: 0.82751453, g_loss: 0.82722795\n",
      "Step: [4967] d_loss: 0.78167772, g_loss: 0.82217753\n",
      "Step: [4968] d_loss: 0.75482970, g_loss: 0.90068793\n",
      "Step: [4969] d_loss: 0.79747963, g_loss: 0.87866676\n",
      "Step: [4970] d_loss: 0.76628071, g_loss: 0.90230244\n",
      "Step: [4971] d_loss: 0.76174432, g_loss: 0.85150909\n",
      "Step: [4972] d_loss: 0.77612573, g_loss: 0.86706352\n",
      "Step: [4973] d_loss: 0.67594451, g_loss: 0.89680058\n",
      "Step: [4974] d_loss: 0.82615227, g_loss: 0.86164278\n",
      "Step: [4975] d_loss: 0.67215484, g_loss: 0.98300391\n",
      "Step: [4976] d_loss: 0.80736780, g_loss: 0.87965870\n",
      "Step: [4977] d_loss: 0.71464533, g_loss: 0.87713957\n",
      "Step: [4978] d_loss: 0.80814964, g_loss: 0.93318254\n",
      "Step: [4979] d_loss: 0.78310388, g_loss: 0.87237883\n",
      "Step: [4980] d_loss: 0.68286359, g_loss: 0.94949472\n",
      "Step: [4981] d_loss: 0.67793846, g_loss: 0.90025461\n",
      "Step: [4982] d_loss: 0.66877037, g_loss: 0.93552107\n",
      "Step: [4983] d_loss: 0.69834882, g_loss: 0.88119715\n",
      "Step: [4984] d_loss: 0.79178810, g_loss: 0.85439658\n",
      "Step: [4985] d_loss: 0.69132245, g_loss: 0.90497929\n",
      "Step: [4986] d_loss: 0.68391377, g_loss: 0.90946144\n",
      "Step: [4987] d_loss: 0.77699918, g_loss: 0.91999149\n",
      "Step: [4988] d_loss: 0.71348238, g_loss: 1.01933300\n",
      "Step: [4989] d_loss: 0.77168888, g_loss: 0.92294431\n",
      "Step: [4990] d_loss: 0.72282004, g_loss: 0.92211127\n",
      "Step: [4991] d_loss: 0.69614637, g_loss: 0.92064649\n",
      "Step: [4992] d_loss: 0.73021883, g_loss: 0.90825838\n",
      "Step: [4993] d_loss: 0.71444458, g_loss: 0.94940555\n",
      "Step: [4994] d_loss: 0.73866862, g_loss: 0.94036454\n",
      "Step: [4995] d_loss: 0.72802824, g_loss: 0.93617839\n",
      "Step: [4996] d_loss: 0.77088368, g_loss: 0.97911787\n",
      "Step: [4997] d_loss: 0.66631454, g_loss: 0.94711077\n",
      "Step: [4998] d_loss: 0.75040632, g_loss: 0.91456145\n",
      "Step: [4999] d_loss: 0.64531881, g_loss: 0.94877946\n",
      "Step: [5000] d_loss: 0.71951348, g_loss: 0.91178709\n",
      "Step: [5001] d_loss: 0.68972498, g_loss: 0.90463686\n",
      "Step: [5002] d_loss: 0.65183073, g_loss: 0.89111859\n",
      "Step: [5003] d_loss: 0.69110972, g_loss: 0.91338682\n",
      "Step: [5004] d_loss: 0.70024663, g_loss: 0.89612216\n",
      "Step: [5005] d_loss: 0.74294722, g_loss: 0.96801639\n",
      "Step: [5006] d_loss: 0.75664479, g_loss: 0.91028094\n",
      "Step: [5007] d_loss: 0.83740884, g_loss: 0.88267851\n",
      "Step: [5008] d_loss: 0.69086748, g_loss: 0.93767017\n",
      "Step: [5009] d_loss: 0.69859266, g_loss: 0.98471785\n",
      "Step: [5010] d_loss: 0.71774429, g_loss: 0.94448864\n",
      "Step: [5011] d_loss: 0.74694282, g_loss: 0.85447907\n",
      "Step: [5012] d_loss: 0.66928077, g_loss: 0.88429683\n",
      "Step: [5013] d_loss: 0.69047254, g_loss: 0.89006263\n",
      "Step: [5014] d_loss: 0.66390550, g_loss: 0.94286710\n",
      "Step: [5015] d_loss: 0.67147249, g_loss: 0.88832146\n",
      "Step: [5016] d_loss: 0.73800725, g_loss: 0.91242993\n",
      "Step: [5017] d_loss: 0.77297074, g_loss: 0.90149909\n",
      "Step: [5018] d_loss: 0.72378767, g_loss: 0.91390818\n",
      "Step: [5019] d_loss: 0.60216928, g_loss: 0.87493479\n",
      "Step: [5020] d_loss: 0.62542945, g_loss: 0.90676922\n",
      "Step: [5021] d_loss: 0.67311662, g_loss: 0.91043746\n",
      "Step: [5022] d_loss: 0.63597530, g_loss: 0.93460113\n",
      "Step: [5023] d_loss: 0.74482191, g_loss: 0.97404605\n",
      "Step: [5024] d_loss: 0.69571227, g_loss: 0.95419633\n",
      "Step: [5025] d_loss: 0.73642558, g_loss: 0.98374450\n",
      "Step: [5026] d_loss: 0.65195757, g_loss: 0.90527874\n",
      "Step: [5027] d_loss: 0.88725436, g_loss: 0.83717537\n",
      "Step: [5028] d_loss: 0.71875018, g_loss: 0.92297882\n",
      "Step: [5029] d_loss: 0.71124583, g_loss: 0.93231899\n",
      "Step: [5030] d_loss: 0.68042892, g_loss: 0.98913008\n",
      "Step: [5031] d_loss: 0.77662462, g_loss: 0.88476068\n",
      "Step: [5032] d_loss: 0.68696058, g_loss: 0.90889317\n",
      "Step: [5033] d_loss: 0.66166168, g_loss: 0.90134466\n",
      "Step: [5034] d_loss: 0.75790352, g_loss: 0.87027216\n",
      "Step: [5035] d_loss: 0.71686816, g_loss: 0.93151599\n",
      "Step: [5036] d_loss: 0.62879223, g_loss: 0.94741666\n",
      "Step: [5037] d_loss: 0.67096227, g_loss: 0.87376088\n",
      "Step: [5038] d_loss: 0.73700291, g_loss: 0.87930566\n",
      "Step: [5039] d_loss: 0.75794727, g_loss: 0.80418426\n",
      "Step: [5040] d_loss: 0.77476513, g_loss: 0.82640737\n",
      "Step: [5041] d_loss: 0.72857541, g_loss: 0.86121351\n",
      "Step: [5042] d_loss: 0.75840747, g_loss: 0.88321263\n",
      "Step: [5043] d_loss: 0.71439278, g_loss: 0.92047644\n",
      "Step: [5044] d_loss: 0.66049135, g_loss: 0.92971826\n",
      "Step: [5045] d_loss: 0.66679537, g_loss: 0.88405097\n",
      "Step: [5046] d_loss: 0.71051335, g_loss: 0.94695973\n",
      "Step: [5047] d_loss: 0.63996494, g_loss: 0.89456165\n",
      "Step: [5048] d_loss: 0.67736197, g_loss: 0.92785919\n",
      "Step: [5049] d_loss: 0.70092350, g_loss: 0.94438249\n",
      "Step: [5050] d_loss: 0.72318429, g_loss: 0.89578456\n",
      "Step: [5051] d_loss: 0.68040776, g_loss: 0.88586676\n",
      "Step: [5052] d_loss: 0.64284068, g_loss: 0.89911270\n",
      "Step: [5053] d_loss: 0.80940515, g_loss: 0.95385194\n",
      "Step: [5054] d_loss: 0.64365667, g_loss: 0.89263308\n",
      "Step: [5055] d_loss: 0.66691655, g_loss: 0.92655808\n",
      "Step: [5056] d_loss: 0.65569532, g_loss: 0.92040974\n",
      "Step: [5057] d_loss: 0.69605839, g_loss: 0.93547165\n",
      "Step: [5058] d_loss: 0.75825095, g_loss: 0.87454700\n",
      "Step: [5059] d_loss: 0.66885662, g_loss: 0.86316842\n",
      "Step: [5060] d_loss: 0.77713537, g_loss: 0.87697625\n",
      "Step: [5061] d_loss: 0.63301724, g_loss: 0.89767402\n",
      "Step: [5062] d_loss: 0.70763230, g_loss: 0.88510156\n",
      "Step: [5063] d_loss: 0.84242308, g_loss: 0.92116117\n",
      "Step: [5064] d_loss: 0.82229096, g_loss: 0.91493016\n",
      "Step: [5065] d_loss: 0.67127216, g_loss: 0.93798012\n",
      "Step: [5066] d_loss: 0.70045018, g_loss: 0.90368545\n",
      "Step: [5067] d_loss: 0.76611078, g_loss: 0.90371472\n",
      "Step: [5068] d_loss: 0.63609785, g_loss: 0.96641695\n",
      "Step: [5069] d_loss: 0.76898873, g_loss: 0.93241560\n",
      "Step: [5070] d_loss: 0.65495986, g_loss: 0.93073976\n",
      "Step: [5071] d_loss: 0.64560926, g_loss: 0.90870678\n",
      "Step: [5072] d_loss: 0.71405977, g_loss: 0.90122902\n",
      "Step: [5073] d_loss: 0.76548225, g_loss: 0.89321154\n",
      "Step: [5074] d_loss: 0.68218940, g_loss: 0.91007209\n",
      "Step: [5075] d_loss: 0.71618330, g_loss: 0.90075445\n",
      "Step: [5076] d_loss: 0.67298424, g_loss: 0.89937592\n",
      "Step: [5077] d_loss: 0.72454214, g_loss: 0.86123067\n",
      "Step: [5078] d_loss: 0.64337248, g_loss: 0.96660131\n",
      "Step: [5079] d_loss: 0.69602984, g_loss: 0.89694279\n",
      "Step: [5080] d_loss: 0.65112728, g_loss: 0.97522444\n",
      "Step: [5081] d_loss: 0.74711972, g_loss: 0.85327423\n",
      "Step: [5082] d_loss: 0.64540011, g_loss: 0.94969618\n",
      "Step: [5083] d_loss: 0.70181847, g_loss: 0.89766026\n",
      "Step: [5084] d_loss: 0.71817279, g_loss: 0.89842945\n",
      "Step: [5085] d_loss: 0.77688044, g_loss: 0.89130795\n",
      "Step: [5086] d_loss: 0.71443850, g_loss: 0.90178126\n",
      "Step: [5087] d_loss: 0.72778428, g_loss: 0.93955278\n",
      "Step: [5088] d_loss: 0.76029605, g_loss: 0.90788871\n",
      "Step: [5089] d_loss: 0.71084505, g_loss: 0.85697293\n",
      "Step: [5090] d_loss: 0.74658918, g_loss: 0.87659580\n",
      "Step: [5091] d_loss: 0.64700150, g_loss: 0.88710314\n",
      "Step: [5092] d_loss: 0.68108857, g_loss: 0.88362408\n",
      "Step: [5093] d_loss: 0.79016930, g_loss: 0.88758063\n",
      "Step: [5094] d_loss: 0.70476341, g_loss: 0.90265524\n",
      "Step: [5095] d_loss: 0.80381536, g_loss: 0.87976706\n",
      "Step: [5096] d_loss: 0.79087228, g_loss: 0.92929000\n",
      "Step: [5097] d_loss: 0.67930269, g_loss: 0.99140322\n",
      "Step: [5098] d_loss: 0.74582428, g_loss: 0.91926676\n",
      "Step: [5099] d_loss: 0.70913368, g_loss: 0.89613259\n",
      "Step: [5100] d_loss: 0.64212340, g_loss: 0.87975323\n",
      "Step: [5101] d_loss: 0.64575607, g_loss: 0.88300610\n",
      "Step: [5102] d_loss: 0.74538612, g_loss: 0.87652433\n",
      "Step: [5103] d_loss: 0.65044463, g_loss: 0.96520299\n",
      "Step: [5104] d_loss: 0.67865491, g_loss: 0.84595138\n",
      "Step: [5105] d_loss: 0.75896877, g_loss: 0.91023046\n",
      "Step: [5106] d_loss: 0.68088561, g_loss: 0.91714150\n",
      "Step: [5107] d_loss: 0.73821688, g_loss: 0.88997698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5108] d_loss: 0.61781818, g_loss: 1.00363600\n",
      "Step: [5109] d_loss: 0.67328042, g_loss: 0.94116145\n",
      "Step: [5110] d_loss: 0.71214247, g_loss: 0.89611530\n",
      "Step: [5111] d_loss: 0.74015069, g_loss: 0.92095166\n",
      "Step: [5112] d_loss: 0.68460381, g_loss: 1.01620066\n",
      "Step: [5113] d_loss: 0.69845158, g_loss: 0.94786358\n",
      "Step: [5114] d_loss: 0.76275855, g_loss: 0.91564208\n",
      "Step: [5115] d_loss: 0.66190189, g_loss: 0.96550292\n",
      "Step: [5116] d_loss: 0.78790879, g_loss: 0.95907104\n",
      "Step: [5117] d_loss: 0.72039419, g_loss: 0.98262966\n",
      "Step: [5118] d_loss: 0.72128093, g_loss: 0.92021286\n",
      "Step: [5119] d_loss: 0.67335933, g_loss: 0.95085335\n",
      "Step: [5120] d_loss: 0.77601635, g_loss: 0.85901839\n",
      "Step: [5121] d_loss: 0.65744305, g_loss: 0.87301743\n",
      "Step: [5122] d_loss: 0.66234893, g_loss: 0.89356118\n",
      "Step: [5123] d_loss: 0.75685090, g_loss: 0.84888393\n",
      "Step: [5124] d_loss: 0.82619405, g_loss: 0.92242128\n",
      "Step: [5125] d_loss: 0.69903362, g_loss: 0.93352276\n",
      "Step: [5126] d_loss: 0.69214886, g_loss: 0.92266798\n",
      "Step: [5127] d_loss: 0.77635068, g_loss: 0.97357047\n",
      "Step: [5128] d_loss: 0.68966138, g_loss: 0.94370592\n",
      "Step: [5129] d_loss: 0.67725676, g_loss: 0.97253406\n",
      "Step: [5130] d_loss: 0.69173795, g_loss: 0.93742096\n",
      "Step: [5131] d_loss: 0.67810553, g_loss: 0.86061025\n",
      "Step: [5132] d_loss: 0.68978548, g_loss: 0.86924595\n",
      "Step: [5133] d_loss: 0.75955951, g_loss: 0.86321372\n",
      "Step: [5134] d_loss: 0.62794495, g_loss: 0.93291670\n",
      "Step: [5135] d_loss: 0.69223112, g_loss: 0.92563158\n",
      "Step: [5136] d_loss: 0.68953902, g_loss: 0.95590413\n",
      "Step: [5137] d_loss: 0.64487964, g_loss: 0.91346204\n",
      "Step: [5138] d_loss: 0.72494400, g_loss: 0.90134573\n",
      "Step: [5139] d_loss: 0.68528771, g_loss: 0.93129504\n",
      "Step: [5140] d_loss: 0.67129314, g_loss: 0.91032094\n",
      "Step: [5141] d_loss: 0.73326707, g_loss: 0.87973261\n",
      "Step: [5142] d_loss: 0.71476245, g_loss: 0.90787327\n",
      "Step: [5143] d_loss: 0.66227496, g_loss: 0.91782254\n",
      "Step: [5144] d_loss: 0.81457609, g_loss: 0.94548136\n",
      "Step: [5145] d_loss: 0.71232229, g_loss: 0.87733310\n",
      "Step: [5146] d_loss: 0.74302536, g_loss: 0.96117514\n",
      "Step: [5147] d_loss: 0.68702787, g_loss: 0.95842063\n",
      "Step: [5148] d_loss: 0.64877909, g_loss: 0.89259589\n",
      "Step: [5149] d_loss: 0.69448817, g_loss: 0.91388804\n",
      "Step: [5150] d_loss: 0.65439212, g_loss: 0.87971997\n",
      "Step: [5151] d_loss: 0.64811313, g_loss: 0.87260211\n",
      "Step: [5152] d_loss: 0.58351713, g_loss: 0.83312404\n",
      "Step: [5153] d_loss: 0.79195374, g_loss: 0.82459438\n",
      "Step: [5154] d_loss: 0.72040558, g_loss: 0.87522542\n",
      "Step: [5155] d_loss: 0.63605940, g_loss: 0.90037769\n",
      "Step: [5156] d_loss: 0.62744123, g_loss: 0.95576262\n",
      "Step: [5157] d_loss: 0.65224022, g_loss: 0.89809364\n",
      "Step: [5158] d_loss: 0.66571474, g_loss: 0.88022614\n",
      "Step: [5159] d_loss: 0.66644025, g_loss: 0.91057551\n",
      "Step: [5160] d_loss: 0.75418019, g_loss: 0.87210304\n",
      "Step: [5161] d_loss: 0.65391701, g_loss: 0.92728931\n",
      "Step: [5162] d_loss: 0.62517130, g_loss: 0.95472145\n",
      "Step: [5163] d_loss: 0.67417473, g_loss: 0.91075438\n",
      "Step: [5164] d_loss: 0.79391372, g_loss: 0.91419226\n",
      "Step: [5165] d_loss: 0.67322332, g_loss: 0.93916678\n",
      "Step: [5166] d_loss: 0.65078765, g_loss: 0.93111902\n",
      "Step: [5167] d_loss: 0.81203729, g_loss: 0.90883881\n",
      "Step: [5168] d_loss: 0.64690965, g_loss: 0.88671374\n",
      "Step: [5169] d_loss: 0.73876983, g_loss: 0.91022861\n",
      "Step: [5170] d_loss: 0.70847809, g_loss: 0.93893290\n",
      "Step: [5171] d_loss: 0.77943730, g_loss: 0.86855507\n",
      "Step: [5172] d_loss: 0.65616769, g_loss: 0.92346525\n",
      "Step: [5173] d_loss: 0.70915186, g_loss: 0.91217273\n",
      "Step: [5174] d_loss: 0.76122659, g_loss: 0.88901454\n",
      "Step: [5175] d_loss: 0.81535882, g_loss: 0.89982861\n",
      "Step: [5176] d_loss: 0.70561838, g_loss: 0.83768868\n",
      "Step: [5177] d_loss: 0.72373825, g_loss: 0.89710492\n",
      "Step: [5178] d_loss: 0.63911247, g_loss: 0.86814547\n",
      "Step: [5179] d_loss: 0.68407512, g_loss: 0.87564075\n",
      "Step: [5180] d_loss: 0.66651946, g_loss: 0.84015530\n",
      "Step: [5181] d_loss: 0.61200327, g_loss: 0.96977043\n",
      "Step: [5182] d_loss: 0.74116069, g_loss: 0.90541679\n",
      "Step: [5183] d_loss: 0.77580523, g_loss: 0.90058315\n",
      "Step: [5184] d_loss: 0.72018933, g_loss: 0.91110545\n",
      "Step: [5185] d_loss: 0.73136544, g_loss: 0.87146688\n",
      "Step: [5186] d_loss: 0.69729239, g_loss: 0.91435045\n",
      "Step: [5187] d_loss: 0.67232096, g_loss: 0.92819118\n",
      "Step: [5188] d_loss: 0.70844871, g_loss: 0.86897779\n",
      "Step: [5189] d_loss: 0.73802769, g_loss: 0.95213699\n",
      "Step: [5190] d_loss: 0.69182497, g_loss: 0.97302169\n",
      "Step: [5191] d_loss: 0.65088809, g_loss: 1.00778723\n",
      "Step: [5192] d_loss: 0.69233537, g_loss: 0.90900403\n",
      "Step: [5193] d_loss: 0.68977219, g_loss: 0.89510244\n",
      "Step: [5194] d_loss: 0.80807507, g_loss: 0.85321307\n",
      "Step: [5195] d_loss: 0.71901274, g_loss: 0.90069509\n",
      "Step: [5196] d_loss: 0.87022048, g_loss: 0.81775451\n",
      "Step: [5197] d_loss: 0.77050078, g_loss: 0.90026695\n",
      "Step: [5198] d_loss: 0.72509187, g_loss: 0.89995116\n",
      "Step: [5199] d_loss: 0.69628090, g_loss: 0.93187606\n",
      "Step: [5200] d_loss: 0.64020211, g_loss: 0.94881171\n",
      "Step: [5201] d_loss: 0.68061453, g_loss: 0.89364564\n",
      "Step: [5202] d_loss: 0.73727793, g_loss: 0.92943716\n",
      "Step: [5203] d_loss: 0.72009426, g_loss: 0.89178151\n",
      "Step: [5204] d_loss: 0.69834465, g_loss: 0.94147992\n",
      "Step: [5205] d_loss: 0.69346541, g_loss: 0.94967085\n",
      "Step: [5206] d_loss: 0.70594656, g_loss: 0.93543822\n",
      "Step: [5207] d_loss: 0.69831759, g_loss: 0.86330652\n",
      "Step: [5208] d_loss: 0.79153782, g_loss: 0.86526942\n",
      "Step: [5209] d_loss: 0.71270996, g_loss: 0.92464638\n",
      "Step: [5210] d_loss: 0.65391415, g_loss: 0.88277459\n",
      "Step: [5211] d_loss: 0.72265661, g_loss: 0.87148535\n",
      "Step: [5212] d_loss: 0.77986985, g_loss: 0.87244332\n",
      "Step: [5213] d_loss: 0.77242208, g_loss: 0.87461150\n",
      "Step: [5214] d_loss: 0.70092666, g_loss: 0.90728283\n",
      "Step: [5215] d_loss: 0.63633794, g_loss: 0.86658555\n",
      "Step: [5216] d_loss: 0.70527923, g_loss: 0.83073640\n",
      "Step: [5217] d_loss: 0.75743324, g_loss: 0.90548933\n",
      "Step: [5218] d_loss: 0.72820163, g_loss: 0.90027732\n",
      "Step: [5219] d_loss: 0.75125408, g_loss: 0.84327000\n",
      "Step: [5220] d_loss: 0.74525189, g_loss: 0.89842123\n",
      "Step: [5221] d_loss: 0.65737629, g_loss: 0.94403809\n",
      "Step: [5222] d_loss: 0.69897681, g_loss: 0.93657070\n",
      "Step: [5223] d_loss: 0.69667608, g_loss: 0.93176830\n",
      "Step: [5224] d_loss: 0.68111712, g_loss: 0.88779145\n",
      "Step: [5225] d_loss: 0.69631392, g_loss: 0.86728132\n",
      "Step: [5226] d_loss: 0.78386039, g_loss: 0.88429296\n",
      "Step: [5227] d_loss: 0.77382815, g_loss: 0.92115682\n",
      "Step: [5228] d_loss: 0.75267971, g_loss: 0.88019514\n",
      "Step: [5229] d_loss: 0.74753135, g_loss: 0.93110597\n",
      "Step: [5230] d_loss: 0.80126524, g_loss: 0.90067500\n",
      "Step: [5231] d_loss: 0.79345250, g_loss: 0.95632744\n",
      "Step: [5232] d_loss: 0.75461161, g_loss: 0.94274604\n",
      "Step: [5233] d_loss: 0.73486108, g_loss: 0.86944383\n",
      "Step: [5234] d_loss: 0.70214331, g_loss: 0.89518535\n",
      "Step: [5235] d_loss: 0.70490754, g_loss: 0.83866906\n",
      "Step: [5236] d_loss: 0.77310884, g_loss: 0.89911896\n",
      "Step: [5237] d_loss: 0.67859834, g_loss: 0.92732680\n",
      "Step: [5238] d_loss: 0.67163068, g_loss: 0.90042734\n",
      "Step: [5239] d_loss: 0.67629880, g_loss: 0.87023056\n",
      "Step: [5240] d_loss: 0.76180100, g_loss: 0.89108098\n",
      "Step: [5241] d_loss: 0.72531581, g_loss: 0.87461525\n",
      "Step: [5242] d_loss: 0.78849465, g_loss: 0.85617077\n",
      "Step: [5243] d_loss: 0.65893614, g_loss: 0.95742851\n",
      "Step: [5244] d_loss: 0.69196576, g_loss: 0.95033598\n",
      "Step: [5245] d_loss: 0.75700766, g_loss: 0.96475726\n",
      "Step: [5246] d_loss: 0.70018548, g_loss: 0.90794140\n",
      "Step: [5247] d_loss: 0.73657781, g_loss: 0.96553093\n",
      "Step: [5248] d_loss: 0.78593487, g_loss: 0.96504384\n",
      "Step: [5249] d_loss: 0.76515764, g_loss: 0.97561854\n",
      "Step: [5250] d_loss: 0.67537922, g_loss: 0.90544879\n",
      "Step: [5251] d_loss: 0.65386140, g_loss: 0.85975295\n",
      "Step: [5252] d_loss: 0.67137212, g_loss: 0.89783615\n",
      "Step: [5253] d_loss: 0.65378869, g_loss: 0.97624946\n",
      "Step: [5254] d_loss: 0.71390843, g_loss: 0.89249015\n",
      "Step: [5255] d_loss: 0.69800723, g_loss: 0.89593303\n",
      "Step: [5256] d_loss: 0.66792661, g_loss: 0.90684932\n",
      "Step: [5257] d_loss: 0.72206903, g_loss: 0.84294993\n",
      "Step: [5258] d_loss: 0.77135187, g_loss: 0.84433407\n",
      "Step: [5259] d_loss: 0.81377071, g_loss: 0.90802425\n",
      "Step: [5260] d_loss: 0.60248983, g_loss: 0.96662539\n",
      "Step: [5261] d_loss: 0.66241777, g_loss: 0.94082761\n",
      "Step: [5262] d_loss: 0.65460736, g_loss: 0.89956766\n",
      "Step: [5263] d_loss: 0.77641290, g_loss: 0.92166108\n",
      "Step: [5264] d_loss: 0.74675590, g_loss: 0.89644301\n",
      "Step: [5265] d_loss: 0.69197220, g_loss: 0.91763324\n",
      "Step: [5266] d_loss: 0.75548762, g_loss: 0.89420998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5267] d_loss: 0.64435351, g_loss: 0.87795103\n",
      "Step: [5268] d_loss: 0.78365469, g_loss: 0.90065444\n",
      "Step: [5269] d_loss: 0.74956846, g_loss: 0.91974896\n",
      "Step: [5270] d_loss: 0.86951995, g_loss: 0.88622469\n",
      "Step: [5271] d_loss: 0.68280661, g_loss: 0.99341059\n",
      "Step: [5272] d_loss: 0.68136275, g_loss: 0.93058753\n",
      "Step: [5273] d_loss: 0.67559338, g_loss: 0.94021797\n",
      "Step: [5274] d_loss: 0.72544867, g_loss: 0.85007674\n",
      "Step: [5275] d_loss: 0.67131525, g_loss: 0.89484787\n",
      "Step: [5276] d_loss: 0.67593914, g_loss: 0.86615306\n",
      "Step: [5277] d_loss: 0.64133906, g_loss: 0.91427821\n",
      "Step: [5278] d_loss: 0.81386381, g_loss: 0.81361955\n",
      "Step: [5279] d_loss: 0.69128883, g_loss: 0.89594382\n",
      "Step: [5280] d_loss: 0.68607199, g_loss: 0.92616469\n",
      "Step: [5281] d_loss: 0.70521718, g_loss: 0.94075286\n",
      "Step: [5282] d_loss: 0.77960849, g_loss: 0.94177794\n",
      "Step: [5283] d_loss: 0.77379727, g_loss: 0.98102260\n",
      "Step: [5284] d_loss: 0.67967659, g_loss: 0.91294146\n",
      "Step: [5285] d_loss: 0.70396990, g_loss: 0.93818706\n",
      "Step: [5286] d_loss: 0.73541951, g_loss: 0.88778448\n",
      "Step: [5287] d_loss: 0.71656871, g_loss: 0.89669693\n",
      "Step: [5288] d_loss: 0.71066529, g_loss: 0.87429738\n",
      "Step: [5289] d_loss: 0.70120150, g_loss: 0.90493381\n",
      "Step: [5290] d_loss: 0.83523571, g_loss: 0.81275952\n",
      "Step: [5291] d_loss: 0.76025558, g_loss: 0.85390663\n",
      "Step: [5292] d_loss: 0.64943671, g_loss: 0.92945462\n",
      "Step: [5293] d_loss: 0.75865078, g_loss: 0.94532919\n",
      "Step: [5294] d_loss: 0.71169043, g_loss: 0.91221398\n",
      "Step: [5295] d_loss: 0.71180946, g_loss: 0.86956441\n",
      "Step: [5296] d_loss: 0.70041895, g_loss: 0.92687869\n",
      "Step: [5297] d_loss: 0.69303691, g_loss: 0.94184995\n",
      "Step: [5298] d_loss: 0.69804114, g_loss: 0.88841391\n",
      "Step: [5299] d_loss: 0.68682128, g_loss: 0.91259289\n",
      "Step: [5300] d_loss: 0.75110626, g_loss: 0.86011219\n",
      "Step: [5301] d_loss: 0.67174315, g_loss: 0.89450991\n",
      "Step: [5302] d_loss: 0.66271758, g_loss: 0.91191232\n",
      "Step: [5303] d_loss: 0.65076858, g_loss: 0.91577065\n",
      "Step: [5304] d_loss: 0.70217532, g_loss: 0.93911034\n",
      "Step: [5305] d_loss: 0.71164298, g_loss: 0.94615054\n",
      "Step: [5306] d_loss: 0.71494943, g_loss: 0.88210404\n",
      "Step: [5307] d_loss: 0.75213706, g_loss: 0.86456233\n",
      "Step: [5308] d_loss: 0.73978257, g_loss: 0.85894269\n",
      "Step: [5309] d_loss: 0.70375901, g_loss: 0.94199169\n",
      "Step: [5310] d_loss: 0.68379915, g_loss: 0.87281144\n",
      "Step: [5311] d_loss: 0.69931775, g_loss: 0.95773143\n",
      "Step: [5312] d_loss: 0.74723500, g_loss: 0.93989348\n",
      "Step: [5313] d_loss: 0.71886373, g_loss: 0.93430674\n",
      "Step: [5314] d_loss: 0.73398614, g_loss: 0.94838983\n",
      "Step: [5315] d_loss: 0.70483017, g_loss: 0.95939535\n",
      "Step: [5316] d_loss: 0.68399310, g_loss: 0.90217954\n",
      "Step: [5317] d_loss: 0.74129981, g_loss: 0.91916287\n",
      "Step: [5318] d_loss: 0.69851506, g_loss: 0.91599560\n",
      "Step: [5319] d_loss: 0.67595243, g_loss: 0.95657754\n",
      "Step: [5320] d_loss: 0.66576022, g_loss: 0.87476742\n",
      "Step: [5321] d_loss: 0.64406973, g_loss: 0.90657288\n",
      "Step: [5322] d_loss: 0.65543860, g_loss: 0.91404331\n",
      "Step: [5323] d_loss: 0.60378551, g_loss: 0.95437968\n",
      "Step: [5324] d_loss: 0.69730711, g_loss: 0.93280655\n",
      "Step: [5325] d_loss: 0.67518175, g_loss: 0.90038413\n",
      "Step: [5326] d_loss: 0.66519457, g_loss: 0.89091808\n",
      "Step: [5327] d_loss: 0.71953511, g_loss: 0.86950511\n",
      "Step: [5328] d_loss: 0.67736906, g_loss: 0.86360252\n",
      "Step: [5329] d_loss: 0.69097817, g_loss: 0.91011131\n",
      "Step: [5330] d_loss: 0.69338703, g_loss: 0.91771668\n",
      "Step: [5331] d_loss: 0.70443243, g_loss: 0.88459122\n",
      "Step: [5332] d_loss: 0.74503964, g_loss: 0.91183287\n",
      "Step: [5333] d_loss: 0.72741890, g_loss: 0.95960349\n",
      "Step: [5334] d_loss: 0.70273185, g_loss: 0.99416113\n",
      "Step: [5335] d_loss: 0.70361292, g_loss: 0.87488145\n",
      "Step: [5336] d_loss: 0.75910848, g_loss: 0.91781676\n",
      "Step: [5337] d_loss: 0.75313938, g_loss: 0.86603910\n",
      "Step: [5338] d_loss: 0.65453285, g_loss: 0.88955021\n",
      "Step: [5339] d_loss: 0.74315953, g_loss: 0.88015622\n",
      "Step: [5340] d_loss: 0.75205559, g_loss: 0.91893721\n",
      "Step: [5341] d_loss: 0.71621209, g_loss: 0.94445312\n",
      "Step: [5342] d_loss: 0.69600123, g_loss: 0.89985007\n",
      "Step: [5343] d_loss: 0.70514321, g_loss: 0.89504808\n",
      "Step: [5344] d_loss: 0.69982296, g_loss: 0.92533374\n",
      "Step: [5345] d_loss: 0.65768123, g_loss: 0.92436236\n",
      "Step: [5346] d_loss: 0.68330055, g_loss: 0.95994037\n",
      "Step: [5347] d_loss: 0.65569633, g_loss: 0.91387200\n",
      "Step: [5348] d_loss: 0.82828736, g_loss: 0.89495772\n",
      "Step: [5349] d_loss: 0.67314839, g_loss: 0.91697842\n",
      "Step: [5350] d_loss: 0.73028791, g_loss: 0.86256868\n",
      "Step: [5351] d_loss: 0.64185035, g_loss: 0.86392033\n",
      "Step: [5352] d_loss: 0.69767910, g_loss: 0.89366394\n",
      "Step: [5353] d_loss: 0.78590816, g_loss: 0.85193276\n",
      "Step: [5354] d_loss: 0.62584782, g_loss: 0.95879239\n",
      "Step: [5355] d_loss: 0.63809085, g_loss: 0.94294804\n",
      "Step: [5356] d_loss: 0.63422906, g_loss: 0.94219244\n",
      "Step: [5357] d_loss: 0.74976945, g_loss: 0.94693321\n",
      "Step: [5358] d_loss: 0.73853314, g_loss: 1.00352383\n",
      "Step: [5359] d_loss: 0.77127767, g_loss: 0.91857964\n",
      "Step: [5360] d_loss: 0.66128570, g_loss: 0.84415907\n",
      "Step: [5361] d_loss: 0.66505593, g_loss: 0.91616088\n",
      "Step: [5362] d_loss: 0.76445615, g_loss: 0.89047623\n",
      "Step: [5363] d_loss: 0.67786014, g_loss: 0.93704152\n",
      "Step: [5364] d_loss: 0.78581601, g_loss: 0.87780237\n",
      "Step: [5365] d_loss: 0.65079105, g_loss: 0.98385370\n",
      "Step: [5366] d_loss: 0.73940969, g_loss: 0.93274933\n",
      "Step: [5367] d_loss: 0.74006522, g_loss: 0.92883176\n",
      "Step: [5368] d_loss: 0.78786880, g_loss: 0.90208566\n",
      "Step: [5369] d_loss: 0.69615287, g_loss: 0.88393545\n",
      "Step: [5370] d_loss: 0.65189928, g_loss: 0.86671865\n",
      "Step: [5371] d_loss: 0.80228788, g_loss: 0.86999220\n",
      "Step: [5372] d_loss: 0.71289730, g_loss: 0.91340393\n",
      "Step: [5373] d_loss: 0.78837717, g_loss: 0.95191997\n",
      "Step: [5374] d_loss: 0.72861761, g_loss: 0.90795612\n",
      "Step: [5375] d_loss: 0.78459734, g_loss: 0.84790421\n",
      "Step: [5376] d_loss: 0.67816287, g_loss: 0.88774431\n",
      "Step: [5377] d_loss: 0.73265046, g_loss: 0.83538657\n",
      "Step: [5378] d_loss: 0.80640638, g_loss: 0.87024957\n",
      "Step: [5379] d_loss: 0.80484968, g_loss: 0.86194128\n",
      "Step: [5380] d_loss: 0.62678272, g_loss: 0.96408796\n",
      "Step: [5381] d_loss: 0.63241655, g_loss: 0.94691622\n",
      "Step: [5382] d_loss: 0.64291388, g_loss: 0.88508171\n",
      "Step: [5383] d_loss: 0.67742455, g_loss: 0.88075840\n",
      "Step: [5384] d_loss: 0.68731058, g_loss: 0.94478101\n",
      "Step: [5385] d_loss: 0.73643351, g_loss: 0.90291762\n",
      "Step: [5386] d_loss: 0.65673530, g_loss: 0.97364807\n",
      "Step: [5387] d_loss: 0.68154645, g_loss: 0.87530279\n",
      "Step: [5388] d_loss: 0.64702302, g_loss: 0.88047791\n",
      "Step: [5389] d_loss: 0.66804147, g_loss: 0.87715262\n",
      "Step: [5390] d_loss: 0.67242312, g_loss: 0.89203626\n",
      "Step: [5391] d_loss: 0.69487762, g_loss: 0.85649753\n",
      "Step: [5392] d_loss: 0.70966440, g_loss: 0.88422084\n",
      "Step: [5393] d_loss: 0.63915306, g_loss: 0.91802186\n",
      "Step: [5394] d_loss: 0.69150436, g_loss: 0.88515854\n",
      "Step: [5395] d_loss: 0.69568998, g_loss: 0.90868467\n",
      "Step: [5396] d_loss: 0.72558099, g_loss: 0.87239087\n",
      "Step: [5397] d_loss: 0.71060473, g_loss: 0.92556405\n",
      "Step: [5398] d_loss: 0.63858753, g_loss: 0.94641286\n",
      "Step: [5399] d_loss: 0.80791271, g_loss: 0.91743684\n",
      "Step: [5400] d_loss: 0.71742970, g_loss: 0.95139396\n",
      "Step: [5401] d_loss: 0.68935621, g_loss: 0.97916102\n",
      "Step: [5402] d_loss: 0.71156818, g_loss: 0.94429654\n",
      "Step: [5403] d_loss: 0.75614220, g_loss: 0.87063187\n",
      "Step: [5404] d_loss: 0.71722978, g_loss: 0.89694446\n",
      "Step: [5405] d_loss: 0.69179279, g_loss: 0.89398563\n",
      "Step: [5406] d_loss: 0.63926178, g_loss: 0.95534039\n",
      "Step: [5407] d_loss: 0.72488070, g_loss: 0.96718603\n",
      "Step: [5408] d_loss: 0.67495370, g_loss: 0.94636333\n",
      "Step: [5409] d_loss: 0.69003856, g_loss: 0.91007996\n",
      "Step: [5410] d_loss: 0.69431835, g_loss: 0.97886485\n",
      "Step: [5411] d_loss: 0.72171116, g_loss: 0.84426415\n",
      "Step: [5412] d_loss: 0.68304998, g_loss: 0.86887956\n",
      "Step: [5413] d_loss: 0.68851995, g_loss: 0.93565100\n",
      "Step: [5414] d_loss: 0.82491845, g_loss: 0.87159920\n",
      "Step: [5415] d_loss: 0.77063543, g_loss: 0.86101788\n",
      "Step: [5416] d_loss: 0.80286998, g_loss: 0.85252059\n",
      "Step: [5417] d_loss: 0.68292636, g_loss: 0.97625464\n",
      "Step: [5418] d_loss: 0.66574442, g_loss: 0.88768005\n",
      "Step: [5419] d_loss: 0.67929441, g_loss: 0.91172290\n",
      "Step: [5420] d_loss: 0.76429391, g_loss: 0.86931252\n",
      "Step: [5421] d_loss: 0.78867984, g_loss: 0.94867468\n",
      "Step: [5422] d_loss: 0.72397840, g_loss: 0.91977954\n",
      "Step: [5423] d_loss: 0.76422328, g_loss: 0.87923521\n",
      "Step: [5424] d_loss: 0.74921054, g_loss: 0.91613722\n",
      "Step: [5425] d_loss: 0.68072659, g_loss: 0.87844640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5426] d_loss: 0.62062955, g_loss: 0.92732066\n",
      "Step: [5427] d_loss: 0.73386467, g_loss: 0.90998977\n",
      "Step: [5428] d_loss: 0.82163626, g_loss: 0.84902966\n",
      "Step: [5429] d_loss: 0.70247716, g_loss: 0.97778547\n",
      "Step: [5430] d_loss: 0.75767720, g_loss: 0.91527009\n",
      "Step: [5431] d_loss: 0.77184951, g_loss: 0.90507489\n",
      "Step: [5432] d_loss: 0.68215388, g_loss: 0.88882166\n",
      "Step: [5433] d_loss: 0.82773972, g_loss: 0.86317325\n",
      "Step: [5434] d_loss: 0.73639119, g_loss: 0.86814010\n",
      "Step: [5435] d_loss: 0.67215055, g_loss: 0.95927787\n",
      "Step: [5436] d_loss: 0.73303735, g_loss: 0.86586571\n",
      "Step: [5437] d_loss: 0.77336961, g_loss: 0.92655724\n",
      "Step: [5438] d_loss: 0.70546830, g_loss: 0.89842671\n",
      "Step: [5439] d_loss: 0.66319388, g_loss: 0.90893221\n",
      "Step: [5440] d_loss: 0.78954315, g_loss: 0.85349524\n",
      "Step: [5441] d_loss: 0.77690792, g_loss: 0.85610002\n",
      "Step: [5442] d_loss: 0.78900325, g_loss: 0.91655254\n",
      "Step: [5443] d_loss: 0.73975015, g_loss: 0.92092454\n",
      "Step: [5444] d_loss: 0.61956602, g_loss: 0.90402204\n",
      "Step: [5445] d_loss: 0.69533575, g_loss: 0.92644864\n",
      "Step: [5446] d_loss: 0.65417129, g_loss: 0.87988156\n",
      "Step: [5447] d_loss: 0.70999533, g_loss: 0.92031395\n",
      "Step: [5448] d_loss: 0.72965288, g_loss: 0.89388335\n",
      "Step: [5449] d_loss: 0.69911563, g_loss: 0.85524207\n",
      "Step: [5450] d_loss: 0.67458266, g_loss: 0.93278825\n",
      "Step: [5451] d_loss: 0.72368139, g_loss: 0.93228018\n",
      "Step: [5452] d_loss: 0.69398993, g_loss: 0.97011828\n",
      "Step: [5453] d_loss: 0.74249738, g_loss: 0.92251575\n",
      "Step: [5454] d_loss: 0.71760100, g_loss: 0.90236545\n",
      "Step: [5455] d_loss: 0.79061991, g_loss: 0.91111708\n",
      "Step: [5456] d_loss: 0.64327496, g_loss: 0.86937213\n",
      "Step: [5457] d_loss: 0.61801809, g_loss: 0.91167176\n",
      "Step: [5458] d_loss: 0.64472431, g_loss: 0.88293105\n",
      "Step: [5459] d_loss: 0.73177743, g_loss: 0.89870143\n",
      "Step: [5460] d_loss: 0.66701049, g_loss: 0.88855988\n",
      "Step: [5461] d_loss: 0.78983301, g_loss: 0.90300071\n",
      "Step: [5462] d_loss: 0.79300052, g_loss: 0.92226124\n",
      "Step: [5463] d_loss: 0.65923160, g_loss: 0.86976838\n",
      "Step: [5464] d_loss: 0.75235003, g_loss: 0.82951170\n",
      "Step: [5465] d_loss: 0.79375160, g_loss: 0.83321869\n",
      "Step: [5466] d_loss: 0.77706122, g_loss: 0.79237872\n",
      "Step: [5467] d_loss: 0.68869746, g_loss: 0.88872457\n",
      "Step: [5468] d_loss: 0.73486018, g_loss: 0.87792039\n",
      "Step: [5469] d_loss: 0.74166501, g_loss: 0.84546232\n",
      "Step: [5470] d_loss: 0.71121591, g_loss: 0.88938934\n",
      "Step: [5471] d_loss: 0.75918770, g_loss: 0.96550792\n",
      "Step: [5472] d_loss: 0.76363295, g_loss: 0.93573821\n",
      "Step: [5473] d_loss: 0.75800622, g_loss: 0.90536803\n",
      "Step: [5474] d_loss: 0.70724756, g_loss: 0.94199353\n",
      "Step: [5475] d_loss: 0.68236423, g_loss: 0.87615961\n",
      "Step: [5476] d_loss: 0.73234814, g_loss: 0.87548262\n",
      "Step: [5477] d_loss: 0.80082941, g_loss: 0.87050360\n",
      "Step: [5478] d_loss: 0.66043884, g_loss: 0.93749726\n",
      "Step: [5479] d_loss: 0.68238020, g_loss: 0.94474918\n",
      "Step: [5480] d_loss: 0.66735804, g_loss: 0.88723063\n",
      "Step: [5481] d_loss: 0.63317156, g_loss: 0.87164676\n",
      "Step: [5482] d_loss: 0.74724019, g_loss: 0.96961361\n",
      "Step: [5483] d_loss: 0.68982464, g_loss: 0.99374866\n",
      "Step: [5484] d_loss: 0.64509213, g_loss: 0.91531724\n",
      "Step: [5485] d_loss: 0.72119540, g_loss: 0.93971848\n",
      "Step: [5486] d_loss: 0.64650697, g_loss: 0.93544632\n",
      "Step: [5487] d_loss: 0.71411920, g_loss: 0.93561977\n",
      "Step: [5488] d_loss: 0.77756542, g_loss: 0.85389555\n",
      "Step: [5489] d_loss: 0.77546805, g_loss: 0.87234014\n",
      "Step: [5490] d_loss: 0.70807809, g_loss: 0.90546614\n",
      "Step: [5491] d_loss: 0.78728628, g_loss: 0.86826956\n",
      "Step: [5492] d_loss: 0.70988232, g_loss: 0.90608346\n",
      "Step: [5493] d_loss: 0.75463462, g_loss: 0.91283971\n",
      "Step: [5494] d_loss: 0.79108191, g_loss: 0.93370861\n",
      "Step: [5495] d_loss: 0.64006042, g_loss: 0.89067519\n",
      "Step: [5496] d_loss: 0.68892038, g_loss: 0.88983387\n",
      "Step: [5497] d_loss: 0.63723755, g_loss: 1.01697779\n",
      "Step: [5498] d_loss: 0.70789540, g_loss: 0.92010742\n",
      "Step: [5499] d_loss: 0.79925382, g_loss: 0.90783381\n",
      "Step: [5500] d_loss: 0.76138788, g_loss: 0.91886246\n",
      "Step: [5501] d_loss: 0.73904669, g_loss: 0.87499678\n",
      "Step: [5502] d_loss: 0.73225975, g_loss: 0.89756441\n",
      "Step: [5503] d_loss: 0.71839106, g_loss: 0.94878030\n",
      "Step: [5504] d_loss: 0.65597963, g_loss: 0.92269957\n",
      "Step: [5505] d_loss: 0.76422977, g_loss: 0.89530849\n",
      "Step: [5506] d_loss: 0.67150581, g_loss: 0.90327686\n",
      "Step: [5507] d_loss: 0.68773937, g_loss: 0.94437319\n",
      "Step: [5508] d_loss: 0.73892570, g_loss: 0.88165689\n",
      "Step: [5509] d_loss: 0.86348516, g_loss: 0.80290931\n",
      "Step: [5510] d_loss: 0.76571590, g_loss: 0.90358222\n",
      "Step: [5511] d_loss: 0.74645936, g_loss: 0.94600546\n",
      "Step: [5512] d_loss: 0.73386610, g_loss: 0.95254779\n",
      "Step: [5513] d_loss: 0.65789902, g_loss: 0.95285428\n",
      "Step: [5514] d_loss: 0.65089619, g_loss: 0.90333021\n",
      "Step: [5515] d_loss: 0.68047887, g_loss: 0.88788742\n",
      "Step: [5516] d_loss: 0.68845659, g_loss: 0.87878788\n",
      "Step: [5517] d_loss: 0.71934015, g_loss: 0.87625575\n",
      "Step: [5518] d_loss: 0.68353063, g_loss: 0.93769169\n",
      "Step: [5519] d_loss: 0.68611127, g_loss: 0.91323686\n",
      "Step: [5520] d_loss: 0.68938923, g_loss: 0.97271401\n",
      "Step: [5521] d_loss: 0.75423735, g_loss: 0.87257040\n",
      "Step: [5522] d_loss: 0.70481384, g_loss: 0.92822146\n",
      "Step: [5523] d_loss: 0.70869178, g_loss: 0.90024531\n",
      "Step: [5524] d_loss: 0.78889304, g_loss: 0.90432411\n",
      "Step: [5525] d_loss: 0.77528977, g_loss: 0.94072449\n",
      "Step: [5526] d_loss: 0.65066510, g_loss: 0.88810754\n",
      "Step: [5527] d_loss: 0.76624590, g_loss: 0.91930801\n",
      "Step: [5528] d_loss: 0.75005674, g_loss: 0.93795693\n",
      "Step: [5529] d_loss: 0.61894476, g_loss: 0.86285454\n",
      "Step: [5530] d_loss: 0.74198478, g_loss: 0.88766438\n",
      "Step: [5531] d_loss: 0.76737595, g_loss: 0.86523926\n",
      "Step: [5532] d_loss: 0.69356048, g_loss: 0.90188342\n",
      "Step: [5533] d_loss: 0.76803768, g_loss: 0.88134122\n",
      "Step: [5534] d_loss: 0.64158231, g_loss: 0.97389519\n",
      "Step: [5535] d_loss: 0.66259497, g_loss: 0.94010955\n",
      "Step: [5536] d_loss: 0.76067549, g_loss: 0.88734871\n",
      "Step: [5537] d_loss: 0.69707084, g_loss: 0.90421367\n",
      "Step: [5538] d_loss: 0.65973926, g_loss: 0.88609654\n",
      "Step: [5539] d_loss: 0.88080513, g_loss: 0.79890728\n",
      "Step: [5540] d_loss: 0.68882406, g_loss: 0.87619603\n",
      "Step: [5541] d_loss: 0.72683036, g_loss: 0.89174497\n",
      "Step: [5542] d_loss: 0.69572359, g_loss: 0.86560017\n",
      "Step: [5543] d_loss: 0.67647958, g_loss: 0.87811995\n",
      "Step: [5544] d_loss: 0.68456233, g_loss: 0.94686282\n",
      "Step: [5545] d_loss: 0.67051393, g_loss: 0.88365179\n",
      "Step: [5546] d_loss: 0.71571219, g_loss: 0.87697715\n",
      "Step: [5547] d_loss: 0.60950297, g_loss: 0.86800218\n",
      "Step: [5548] d_loss: 0.71980083, g_loss: 0.82942885\n",
      "Step: [5549] d_loss: 0.78665555, g_loss: 0.87455612\n",
      "Step: [5550] d_loss: 0.70449793, g_loss: 0.88073987\n",
      "Step: [5551] d_loss: 0.71853405, g_loss: 0.89824569\n",
      "Step: [5552] d_loss: 0.66781658, g_loss: 0.91208106\n",
      "Step: [5553] d_loss: 0.69728273, g_loss: 0.94817746\n",
      "Step: [5554] d_loss: 0.70686424, g_loss: 0.98425364\n",
      "Step: [5555] d_loss: 0.66780484, g_loss: 0.91446245\n",
      "Step: [5556] d_loss: 0.73501581, g_loss: 0.92576754\n",
      "Step: [5557] d_loss: 0.70170754, g_loss: 0.88078064\n",
      "Step: [5558] d_loss: 0.74335474, g_loss: 0.87930268\n",
      "Step: [5559] d_loss: 0.77902138, g_loss: 0.94459701\n",
      "Step: [5560] d_loss: 0.72825527, g_loss: 0.92941010\n",
      "Step: [5561] d_loss: 0.69654626, g_loss: 0.95439714\n",
      "Step: [5562] d_loss: 0.76116705, g_loss: 0.97740889\n",
      "Step: [5563] d_loss: 0.72700816, g_loss: 0.93910301\n",
      "Step: [5564] d_loss: 0.81220216, g_loss: 0.87360632\n",
      "Step: [5565] d_loss: 0.70074463, g_loss: 0.94038057\n",
      "Step: [5566] d_loss: 0.75881147, g_loss: 0.92579639\n",
      "Step: [5567] d_loss: 0.69984126, g_loss: 0.93782771\n",
      "Step: [5568] d_loss: 0.67206186, g_loss: 0.89041704\n",
      "Step: [5569] d_loss: 0.76476860, g_loss: 0.82731843\n",
      "Step: [5570] d_loss: 0.85280687, g_loss: 0.81579906\n",
      "Step: [5571] d_loss: 0.77603483, g_loss: 0.84222484\n",
      "Step: [5572] d_loss: 0.80703640, g_loss: 0.83686322\n",
      "Step: [5573] d_loss: 0.72878283, g_loss: 0.85315311\n",
      "Step: [5574] d_loss: 0.66136771, g_loss: 0.87181097\n",
      "Step: [5575] d_loss: 0.64400536, g_loss: 0.92484504\n",
      "Step: [5576] d_loss: 0.71080935, g_loss: 0.88907576\n",
      "Step: [5577] d_loss: 0.73525560, g_loss: 0.89117700\n",
      "Step: [5578] d_loss: 0.82996911, g_loss: 0.87547201\n",
      "Step: [5579] d_loss: 0.78792727, g_loss: 0.89089149\n",
      "Step: [5580] d_loss: 0.69846165, g_loss: 0.92319798\n",
      "Step: [5581] d_loss: 0.74292260, g_loss: 0.91176850\n",
      "Step: [5582] d_loss: 0.65058649, g_loss: 0.90051389\n",
      "Step: [5583] d_loss: 0.67481011, g_loss: 0.93788975\n",
      "Step: [5584] d_loss: 0.74233419, g_loss: 0.89072484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5585] d_loss: 0.73335314, g_loss: 0.89390892\n",
      "Step: [5586] d_loss: 0.70402569, g_loss: 0.93097305\n",
      "Step: [5587] d_loss: 0.78082925, g_loss: 0.94712144\n",
      "Step: [5588] d_loss: 0.78993893, g_loss: 1.00129223\n",
      "Step: [5589] d_loss: 0.68123698, g_loss: 0.91646284\n",
      "Step: [5590] d_loss: 0.68638539, g_loss: 0.96543467\n",
      "Step: [5591] d_loss: 0.86959928, g_loss: 0.98667848\n",
      "Step: [5592] d_loss: 0.82237405, g_loss: 0.82199824\n",
      "Step: [5593] d_loss: 0.69334137, g_loss: 0.91652256\n",
      "Step: [5594] d_loss: 0.79702520, g_loss: 0.93714416\n",
      "Step: [5595] d_loss: 0.65028137, g_loss: 0.93197304\n",
      "Step: [5596] d_loss: 0.70031548, g_loss: 0.94632506\n",
      "Step: [5597] d_loss: 0.68396324, g_loss: 0.92932630\n",
      "Step: [5598] d_loss: 0.68886721, g_loss: 0.92199361\n",
      "Step: [5599] d_loss: 0.65883714, g_loss: 0.95463502\n",
      "Step: [5600] d_loss: 0.63188285, g_loss: 0.90263307\n",
      "Step: [5601] d_loss: 0.82620895, g_loss: 0.86332428\n",
      "Step: [5602] d_loss: 0.76320398, g_loss: 0.91133159\n",
      "Step: [5603] d_loss: 0.75214458, g_loss: 0.95050126\n",
      "Step: [5604] d_loss: 0.65713620, g_loss: 0.91807848\n",
      "Step: [5605] d_loss: 0.70743150, g_loss: 0.92777371\n",
      "Step: [5606] d_loss: 0.76237988, g_loss: 0.87997395\n",
      "Step: [5607] d_loss: 0.72346604, g_loss: 0.92620420\n",
      "Step: [5608] d_loss: 0.71611458, g_loss: 0.97804606\n",
      "Step: [5609] d_loss: 0.62215757, g_loss: 0.93552786\n",
      "Step: [5610] d_loss: 0.73642772, g_loss: 0.94714278\n",
      "Step: [5611] d_loss: 0.77007014, g_loss: 0.94078588\n",
      "Step: [5612] d_loss: 0.65672970, g_loss: 0.90230107\n",
      "Step: [5613] d_loss: 0.65716648, g_loss: 0.91296458\n",
      "Step: [5614] d_loss: 0.78344047, g_loss: 0.92185879\n",
      "Step: [5615] d_loss: 0.65217435, g_loss: 0.90064991\n",
      "Step: [5616] d_loss: 0.66796696, g_loss: 0.91362804\n",
      "Step: [5617] d_loss: 0.72543335, g_loss: 0.96124893\n",
      "Step: [5618] d_loss: 0.66781622, g_loss: 0.95430654\n",
      "Step: [5619] d_loss: 0.75469011, g_loss: 1.00785363\n",
      "Step: [5620] d_loss: 0.78155577, g_loss: 0.97118014\n",
      "Step: [5621] d_loss: 0.70212930, g_loss: 0.85209453\n",
      "Step: [5622] d_loss: 0.80193156, g_loss: 0.91719216\n",
      "Step: [5623] d_loss: 0.71298033, g_loss: 0.92456537\n",
      "Step: [5624] d_loss: 0.79773426, g_loss: 0.88385653\n",
      "Step: [5625] d_loss: 0.72665924, g_loss: 0.91605163\n",
      "Step: [5626] d_loss: 0.60300666, g_loss: 0.89772660\n",
      "Step: [5627] d_loss: 0.77951121, g_loss: 0.87636387\n",
      "Step: [5628] d_loss: 0.60900879, g_loss: 0.92084414\n",
      "Step: [5629] d_loss: 0.71019018, g_loss: 0.92967427\n",
      "Step: [5630] d_loss: 0.80722213, g_loss: 0.86693722\n",
      "Step: [5631] d_loss: 0.79345721, g_loss: 0.90879774\n",
      "Step: [5632] d_loss: 0.80062652, g_loss: 0.90489668\n",
      "Step: [5633] d_loss: 0.80743456, g_loss: 0.85823590\n",
      "Step: [5634] d_loss: 0.74051464, g_loss: 0.92167324\n",
      "Step: [5635] d_loss: 0.69322437, g_loss: 0.98714119\n",
      "Step: [5636] d_loss: 0.79253191, g_loss: 0.97714013\n",
      "Step: [5637] d_loss: 0.72610182, g_loss: 0.88137710\n",
      "Step: [5638] d_loss: 0.76625186, g_loss: 0.89439601\n",
      "Step: [5639] d_loss: 0.67720580, g_loss: 0.86225855\n",
      "Step: [5640] d_loss: 0.74155581, g_loss: 0.87586558\n",
      "Step: [5641] d_loss: 0.64018935, g_loss: 0.96201116\n",
      "Step: [5642] d_loss: 0.78316844, g_loss: 0.94317251\n",
      "Step: [5643] d_loss: 0.67376792, g_loss: 1.00465679\n",
      "Step: [5644] d_loss: 0.71710670, g_loss: 0.99571019\n",
      "Step: [5645] d_loss: 0.72069412, g_loss: 1.00949252\n",
      "Step: [5646] d_loss: 0.74967360, g_loss: 0.95046645\n",
      "Step: [5647] d_loss: 0.67828113, g_loss: 0.87310368\n",
      "Step: [5648] d_loss: 0.76897287, g_loss: 0.83131558\n",
      "Step: [5649] d_loss: 0.72108787, g_loss: 0.87664735\n",
      "Step: [5650] d_loss: 0.72019678, g_loss: 0.88815564\n",
      "Step: [5651] d_loss: 0.79833549, g_loss: 0.90949416\n",
      "Step: [5652] d_loss: 0.73401070, g_loss: 0.85236961\n",
      "Step: [5653] d_loss: 0.69868946, g_loss: 0.90981495\n",
      "Step: [5654] d_loss: 0.64446372, g_loss: 0.88138282\n",
      "Step: [5655] d_loss: 0.79346752, g_loss: 0.85372937\n",
      "Step: [5656] d_loss: 0.71253675, g_loss: 0.96553063\n",
      "Step: [5657] d_loss: 0.69513381, g_loss: 0.94675267\n",
      "Step: [5658] d_loss: 0.67441827, g_loss: 0.95825368\n",
      "Step: [5659] d_loss: 0.86808187, g_loss: 0.87065327\n",
      "Step: [5660] d_loss: 0.74415141, g_loss: 0.92686039\n",
      "Step: [5661] d_loss: 0.75329429, g_loss: 0.95401752\n",
      "Step: [5662] d_loss: 0.66298825, g_loss: 0.91081220\n",
      "Step: [5663] d_loss: 0.72176498, g_loss: 0.89604563\n",
      "Step: [5664] d_loss: 0.70541143, g_loss: 0.82923621\n",
      "Step: [5665] d_loss: 0.63792652, g_loss: 0.95152175\n",
      "Step: [5666] d_loss: 0.74245226, g_loss: 0.90763503\n",
      "Step: [5667] d_loss: 0.74652058, g_loss: 0.88160932\n",
      "Step: [5668] d_loss: 0.68073523, g_loss: 0.92866135\n",
      "Step: [5669] d_loss: 0.70967096, g_loss: 0.85751861\n",
      "Step: [5670] d_loss: 0.66161507, g_loss: 0.88164729\n",
      "Step: [5671] d_loss: 0.79221153, g_loss: 0.82907385\n",
      "Step: [5672] d_loss: 0.69335765, g_loss: 0.89496869\n",
      "Step: [5673] d_loss: 0.74359715, g_loss: 0.92046070\n",
      "Step: [5674] d_loss: 0.73107231, g_loss: 0.96547759\n",
      "Step: [5675] d_loss: 0.66751331, g_loss: 0.91225028\n",
      "Step: [5676] d_loss: 0.66846192, g_loss: 0.96944708\n",
      "Step: [5677] d_loss: 0.74867570, g_loss: 0.95543557\n",
      "Step: [5678] d_loss: 0.71969068, g_loss: 0.93254340\n",
      "Step: [5679] d_loss: 0.76320881, g_loss: 0.91263109\n",
      "Step: [5680] d_loss: 0.75441051, g_loss: 0.92390794\n",
      "Step: [5681] d_loss: 0.69899839, g_loss: 0.89157134\n",
      "Step: [5682] d_loss: 0.68321252, g_loss: 0.92815840\n",
      "Step: [5683] d_loss: 0.68725038, g_loss: 0.85026646\n",
      "Step: [5684] d_loss: 0.65891325, g_loss: 0.91470432\n",
      "Step: [5685] d_loss: 0.67981046, g_loss: 0.89849782\n",
      "Step: [5686] d_loss: 0.73101854, g_loss: 0.89315826\n",
      "Step: [5687] d_loss: 0.68272930, g_loss: 0.91976243\n",
      "Step: [5688] d_loss: 0.72757608, g_loss: 0.93553865\n",
      "Step: [5689] d_loss: 0.82117075, g_loss: 0.90312719\n",
      "Step: [5690] d_loss: 0.70043010, g_loss: 0.88390291\n",
      "Step: [5691] d_loss: 0.72843695, g_loss: 0.89890647\n",
      "Step: [5692] d_loss: 0.74626088, g_loss: 0.90545774\n",
      "Step: [5693] d_loss: 0.64325494, g_loss: 0.91408068\n",
      "Step: [5694] d_loss: 0.73832208, g_loss: 0.93153745\n",
      "Step: [5695] d_loss: 0.72674024, g_loss: 0.97794819\n",
      "Step: [5696] d_loss: 0.72795290, g_loss: 0.89601749\n",
      "Step: [5697] d_loss: 0.69862109, g_loss: 0.92715496\n",
      "Step: [5698] d_loss: 0.70236003, g_loss: 0.95008188\n",
      "Step: [5699] d_loss: 0.71341401, g_loss: 0.91117698\n",
      "Step: [5700] d_loss: 0.66660887, g_loss: 0.92596263\n",
      "Step: [5701] d_loss: 0.73013389, g_loss: 0.92132849\n",
      "Step: [5702] d_loss: 0.68066448, g_loss: 0.90792608\n",
      "Step: [5703] d_loss: 0.70659816, g_loss: 0.91184807\n",
      "Step: [5704] d_loss: 0.65422916, g_loss: 0.94423789\n",
      "Step: [5705] d_loss: 0.69674695, g_loss: 0.94324833\n",
      "Step: [5706] d_loss: 0.80916929, g_loss: 0.91861075\n",
      "Step: [5707] d_loss: 0.69689310, g_loss: 0.89361632\n",
      "Step: [5708] d_loss: 0.76697576, g_loss: 0.95034897\n",
      "Step: [5709] d_loss: 0.78824353, g_loss: 0.86902761\n",
      "Step: [5710] d_loss: 0.72353888, g_loss: 0.92427111\n",
      "Step: [5711] d_loss: 0.75783980, g_loss: 0.92129791\n",
      "Step: [5712] d_loss: 0.71705896, g_loss: 0.95228183\n",
      "Step: [5713] d_loss: 0.67653060, g_loss: 0.90848005\n",
      "Step: [5714] d_loss: 0.73636478, g_loss: 0.93210256\n",
      "Step: [5715] d_loss: 0.62469429, g_loss: 0.84357673\n",
      "Step: [5716] d_loss: 0.69612730, g_loss: 0.88704216\n",
      "Step: [5717] d_loss: 0.79241788, g_loss: 0.90425444\n",
      "Step: [5718] d_loss: 0.67107213, g_loss: 0.96133739\n",
      "Step: [5719] d_loss: 0.75918531, g_loss: 0.89208692\n",
      "Step: [5720] d_loss: 0.69738185, g_loss: 0.92694277\n",
      "Step: [5721] d_loss: 0.76770645, g_loss: 0.89858079\n",
      "Step: [5722] d_loss: 0.69377005, g_loss: 0.91276878\n",
      "Step: [5723] d_loss: 0.71051973, g_loss: 0.90515065\n",
      "Step: [5724] d_loss: 0.76850581, g_loss: 0.91122139\n",
      "Step: [5725] d_loss: 0.67486995, g_loss: 0.89717430\n",
      "Step: [5726] d_loss: 0.63975513, g_loss: 0.91201961\n",
      "Step: [5727] d_loss: 0.72201699, g_loss: 0.85683304\n",
      "Step: [5728] d_loss: 0.83335268, g_loss: 0.81048524\n",
      "Step: [5729] d_loss: 0.64880478, g_loss: 0.91766357\n",
      "Step: [5730] d_loss: 0.82076269, g_loss: 0.86734563\n",
      "Step: [5731] d_loss: 0.66250145, g_loss: 0.93891340\n",
      "Step: [5732] d_loss: 0.75449216, g_loss: 0.92575270\n",
      "Step: [5733] d_loss: 0.66140050, g_loss: 0.89854872\n",
      "Step: [5734] d_loss: 0.66156918, g_loss: 0.89318126\n",
      "Step: [5735] d_loss: 0.76558203, g_loss: 0.90926480\n",
      "Step: [5736] d_loss: 0.76131284, g_loss: 0.86419076\n",
      "Step: [5737] d_loss: 0.67171496, g_loss: 0.92085344\n",
      "Step: [5738] d_loss: 0.72967511, g_loss: 0.95695579\n",
      "Step: [5739] d_loss: 0.60363364, g_loss: 0.89841217\n",
      "Step: [5740] d_loss: 0.72679079, g_loss: 0.89336693\n",
      "Step: [5741] d_loss: 0.69263536, g_loss: 0.89984542\n",
      "Step: [5742] d_loss: 0.68513221, g_loss: 0.92368680\n",
      "Step: [5743] d_loss: 0.67175239, g_loss: 0.92930764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5744] d_loss: 0.74339437, g_loss: 0.88653541\n",
      "Step: [5745] d_loss: 0.73872191, g_loss: 0.96176159\n",
      "Step: [5746] d_loss: 0.83082032, g_loss: 0.89169085\n",
      "Step: [5747] d_loss: 0.73774499, g_loss: 0.91398799\n",
      "Step: [5748] d_loss: 0.72161412, g_loss: 0.94051075\n",
      "Step: [5749] d_loss: 0.82843709, g_loss: 0.92319834\n",
      "Step: [5750] d_loss: 0.66182429, g_loss: 0.92389840\n",
      "Step: [5751] d_loss: 0.66818804, g_loss: 0.93274152\n",
      "Step: [5752] d_loss: 0.66671073, g_loss: 0.96762019\n",
      "Step: [5753] d_loss: 0.78590870, g_loss: 0.95235556\n",
      "Step: [5754] d_loss: 0.71090174, g_loss: 0.92741048\n",
      "Step: [5755] d_loss: 0.76427376, g_loss: 0.93253857\n",
      "Step: [5756] d_loss: 0.70128834, g_loss: 0.98373818\n",
      "Step: [5757] d_loss: 0.66803515, g_loss: 0.90908432\n",
      "Step: [5758] d_loss: 0.71933031, g_loss: 0.91850823\n",
      "Step: [5759] d_loss: 0.73574609, g_loss: 0.96253484\n",
      "Step: [5760] d_loss: 0.72086024, g_loss: 0.90732205\n",
      "Step: [5761] d_loss: 0.67352670, g_loss: 0.91095281\n",
      "Step: [5762] d_loss: 0.73459518, g_loss: 0.94478083\n",
      "Step: [5763] d_loss: 0.81914592, g_loss: 0.90474224\n",
      "Step: [5764] d_loss: 0.73658007, g_loss: 0.95367920\n",
      "Step: [5765] d_loss: 0.79861951, g_loss: 0.92579412\n",
      "Step: [5766] d_loss: 0.72043550, g_loss: 0.90069032\n",
      "Step: [5767] d_loss: 0.66413331, g_loss: 0.89115810\n",
      "Step: [5768] d_loss: 0.63282394, g_loss: 0.91841817\n",
      "Step: [5769] d_loss: 0.65142411, g_loss: 0.95709258\n",
      "Step: [5770] d_loss: 0.70904958, g_loss: 0.93860799\n",
      "Step: [5771] d_loss: 0.70975095, g_loss: 0.90864801\n",
      "Step: [5772] d_loss: 0.73785126, g_loss: 0.90467137\n",
      "Step: [5773] d_loss: 0.67125201, g_loss: 0.90188867\n",
      "Step: [5774] d_loss: 0.74881834, g_loss: 0.94475973\n",
      "Step: [5775] d_loss: 0.63232636, g_loss: 0.89249146\n",
      "Step: [5776] d_loss: 0.64215684, g_loss: 0.97360998\n",
      "Step: [5777] d_loss: 0.80115008, g_loss: 0.87934798\n",
      "Step: [5778] d_loss: 0.71450037, g_loss: 0.90960419\n",
      "Step: [5779] d_loss: 0.71476954, g_loss: 0.91407299\n",
      "Step: [5780] d_loss: 0.68762541, g_loss: 0.91871160\n",
      "Step: [5781] d_loss: 0.72687644, g_loss: 0.84840095\n",
      "Step: [5782] d_loss: 0.76233631, g_loss: 0.90150493\n",
      "Step: [5783] d_loss: 0.66977274, g_loss: 0.96720850\n",
      "Step: [5784] d_loss: 0.78122205, g_loss: 0.97033858\n",
      "Step: [5785] d_loss: 0.72175664, g_loss: 0.99848980\n",
      "Step: [5786] d_loss: 0.79430687, g_loss: 0.97944307\n",
      "Step: [5787] d_loss: 0.83948499, g_loss: 1.08114672\n",
      "Step: [5788] d_loss: 0.72034121, g_loss: 0.89725679\n",
      "Step: [5789] d_loss: 0.71392357, g_loss: 0.91234976\n",
      "Step: [5790] d_loss: 0.62068832, g_loss: 0.93479681\n",
      "Step: [5791] d_loss: 0.73831314, g_loss: 0.87190902\n",
      "Step: [5792] d_loss: 0.74314189, g_loss: 0.83570963\n",
      "Step: [5793] d_loss: 0.70262915, g_loss: 0.89584488\n",
      "Step: [5794] d_loss: 0.68381268, g_loss: 0.89894789\n",
      "Step: [5795] d_loss: 0.81865138, g_loss: 0.89393324\n",
      "Step: [5796] d_loss: 0.69016808, g_loss: 0.87737870\n",
      "Step: [5797] d_loss: 0.61427701, g_loss: 0.92028582\n",
      "Step: [5798] d_loss: 0.65644705, g_loss: 0.87733424\n",
      "Step: [5799] d_loss: 0.72274572, g_loss: 0.84874797\n",
      "Step: [5800] d_loss: 0.73476237, g_loss: 0.88131392\n",
      "Step: [5801] d_loss: 0.76406723, g_loss: 0.88373739\n",
      "Step: [5802] d_loss: 0.66726780, g_loss: 0.91283947\n",
      "Step: [5803] d_loss: 0.69363499, g_loss: 0.87889349\n",
      "Step: [5804] d_loss: 0.74591827, g_loss: 0.92573476\n",
      "Step: [5805] d_loss: 0.82781947, g_loss: 0.88520312\n",
      "Step: [5806] d_loss: 0.72259396, g_loss: 0.97598153\n",
      "Step: [5807] d_loss: 0.71682662, g_loss: 0.90277207\n",
      "Step: [5808] d_loss: 0.69741231, g_loss: 0.89362597\n",
      "Step: [5809] d_loss: 0.67487866, g_loss: 0.87283528\n",
      "Step: [5810] d_loss: 0.69924790, g_loss: 0.94589663\n",
      "Step: [5811] d_loss: 0.70086694, g_loss: 0.88203895\n",
      "Step: [5812] d_loss: 0.67770600, g_loss: 0.95086056\n",
      "Step: [5813] d_loss: 0.76154321, g_loss: 0.90162164\n",
      "Step: [5814] d_loss: 0.68124223, g_loss: 0.91706026\n",
      "Step: [5815] d_loss: 0.66402972, g_loss: 0.87250054\n",
      "Step: [5816] d_loss: 0.72583073, g_loss: 0.89105809\n",
      "Step: [5817] d_loss: 0.71867919, g_loss: 0.88561833\n",
      "Step: [5818] d_loss: 0.71907508, g_loss: 0.95354885\n",
      "Step: [5819] d_loss: 0.67855674, g_loss: 0.94756538\n",
      "Step: [5820] d_loss: 0.69185942, g_loss: 0.91594255\n",
      "Step: [5821] d_loss: 0.67565835, g_loss: 0.94704473\n",
      "Step: [5822] d_loss: 0.71036839, g_loss: 0.93664044\n",
      "Step: [5823] d_loss: 0.69804049, g_loss: 0.97232801\n",
      "Step: [5824] d_loss: 0.77896482, g_loss: 0.98331177\n",
      "Step: [5825] d_loss: 0.71713066, g_loss: 0.91542721\n",
      "Step: [5826] d_loss: 0.64469004, g_loss: 0.92696470\n",
      "Step: [5827] d_loss: 0.71637857, g_loss: 0.98669827\n",
      "Step: [5828] d_loss: 0.68882459, g_loss: 0.93283200\n",
      "Step: [5829] d_loss: 0.76788741, g_loss: 0.82547063\n",
      "Step: [5830] d_loss: 0.67266649, g_loss: 0.92573404\n",
      "Step: [5831] d_loss: 0.69990396, g_loss: 0.92674178\n",
      "Step: [5832] d_loss: 0.70404953, g_loss: 0.90462011\n",
      "Step: [5833] d_loss: 0.64730489, g_loss: 0.88850820\n",
      "Step: [5834] d_loss: 0.67699862, g_loss: 0.88990724\n",
      "Step: [5835] d_loss: 0.70997703, g_loss: 0.91622663\n",
      "Step: [5836] d_loss: 0.65496075, g_loss: 0.90237463\n",
      "Step: [5837] d_loss: 0.72112548, g_loss: 0.90024722\n",
      "Step: [5838] d_loss: 0.64825332, g_loss: 0.89900887\n",
      "Step: [5839] d_loss: 0.67518759, g_loss: 0.95039171\n",
      "Step: [5840] d_loss: 0.68582690, g_loss: 0.95333934\n",
      "Step: [5841] d_loss: 0.70392740, g_loss: 0.92113447\n",
      "Step: [5842] d_loss: 0.70176369, g_loss: 0.98349285\n",
      "Step: [5843] d_loss: 0.65916085, g_loss: 0.98687565\n",
      "Step: [5844] d_loss: 0.75539798, g_loss: 0.90449536\n",
      "Step: [5845] d_loss: 0.69791240, g_loss: 0.88165969\n",
      "Step: [5846] d_loss: 0.66724753, g_loss: 0.89562035\n",
      "Step: [5847] d_loss: 0.70721149, g_loss: 0.81549227\n",
      "Step: [5848] d_loss: 0.75735444, g_loss: 0.84825122\n",
      "Step: [5849] d_loss: 0.74444997, g_loss: 0.93862063\n",
      "Step: [5850] d_loss: 0.69565076, g_loss: 0.94207597\n",
      "Step: [5851] d_loss: 0.63786274, g_loss: 0.93840390\n",
      "Step: [5852] d_loss: 0.70427096, g_loss: 0.89835709\n",
      "Step: [5853] d_loss: 0.70516038, g_loss: 0.94652152\n",
      "Step: [5854] d_loss: 0.64543551, g_loss: 0.90657419\n",
      "Step: [5855] d_loss: 0.75651985, g_loss: 0.89508551\n",
      "Step: [5856] d_loss: 0.81656420, g_loss: 0.87741792\n",
      "Step: [5857] d_loss: 0.70263368, g_loss: 0.91701233\n",
      "Step: [5858] d_loss: 0.67803252, g_loss: 0.90983289\n",
      "Step: [5859] d_loss: 0.67036474, g_loss: 0.88084698\n",
      "Step: [5860] d_loss: 0.72603709, g_loss: 0.84277403\n",
      "Step: [5861] d_loss: 0.75614089, g_loss: 0.91360885\n",
      "Step: [5862] d_loss: 0.71353871, g_loss: 0.91682887\n",
      "Step: [5863] d_loss: 0.78630602, g_loss: 0.90251994\n",
      "Step: [5864] d_loss: 0.63959670, g_loss: 0.88509285\n",
      "Step: [5865] d_loss: 0.74805123, g_loss: 0.85332394\n",
      "Step: [5866] d_loss: 0.66190171, g_loss: 0.86440718\n",
      "Step: [5867] d_loss: 0.67541355, g_loss: 0.90540028\n",
      "Step: [5868] d_loss: 0.69848305, g_loss: 0.92930174\n",
      "Step: [5869] d_loss: 0.69879335, g_loss: 0.92858219\n",
      "Step: [5870] d_loss: 0.64276552, g_loss: 0.87728143\n",
      "Step: [5871] d_loss: 0.73237687, g_loss: 0.88036001\n",
      "Step: [5872] d_loss: 0.67778379, g_loss: 0.89308161\n",
      "Step: [5873] d_loss: 0.78028691, g_loss: 0.89614218\n",
      "Step: [5874] d_loss: 0.68167120, g_loss: 0.96393287\n",
      "Step: [5875] d_loss: 0.67906976, g_loss: 0.95329130\n",
      "Step: [5876] d_loss: 0.72658873, g_loss: 0.85794860\n",
      "Step: [5877] d_loss: 0.66655838, g_loss: 0.92276537\n",
      "Step: [5878] d_loss: 0.67085266, g_loss: 0.86678648\n",
      "Step: [5879] d_loss: 0.66324598, g_loss: 0.91408157\n",
      "Step: [5880] d_loss: 0.62661827, g_loss: 0.93261474\n",
      "Step: [5881] d_loss: 0.63574165, g_loss: 0.99975759\n",
      "Step: [5882] d_loss: 0.67272162, g_loss: 0.95000756\n",
      "Step: [5883] d_loss: 0.72208941, g_loss: 0.90259302\n",
      "Step: [5884] d_loss: 0.63039762, g_loss: 0.91370708\n",
      "Step: [5885] d_loss: 0.67238420, g_loss: 0.89746356\n",
      "Step: [5886] d_loss: 0.73485255, g_loss: 0.87800920\n",
      "Step: [5887] d_loss: 0.69439065, g_loss: 0.87619305\n",
      "Step: [5888] d_loss: 0.69014037, g_loss: 0.90600389\n",
      "Step: [5889] d_loss: 0.65440983, g_loss: 0.96931440\n",
      "Step: [5890] d_loss: 0.56687665, g_loss: 0.95984077\n",
      "Step: [5891] d_loss: 0.79107738, g_loss: 0.90356213\n",
      "Step: [5892] d_loss: 0.68845832, g_loss: 0.95218998\n",
      "Step: [5893] d_loss: 0.69393122, g_loss: 0.89682633\n",
      "Step: [5894] d_loss: 0.67938077, g_loss: 0.88808161\n",
      "Step: [5895] d_loss: 0.69415587, g_loss: 0.87517869\n",
      "Step: [5896] d_loss: 0.72443122, g_loss: 0.90208358\n",
      "Step: [5897] d_loss: 0.77540779, g_loss: 0.92905086\n",
      "Step: [5898] d_loss: 0.66923779, g_loss: 0.88708889\n",
      "Step: [5899] d_loss: 0.73495311, g_loss: 0.84066427\n",
      "Step: [5900] d_loss: 0.67296058, g_loss: 0.83519548\n",
      "Step: [5901] d_loss: 0.64434350, g_loss: 0.86521453\n",
      "Step: [5902] d_loss: 0.77892631, g_loss: 0.86592269\n",
      "Step: [5903] d_loss: 0.74786919, g_loss: 0.88459504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [5904] d_loss: 0.65445393, g_loss: 0.95218873\n",
      "Step: [5905] d_loss: 0.74600160, g_loss: 0.85066628\n",
      "Step: [5906] d_loss: 0.73592281, g_loss: 0.93025446\n",
      "Step: [5907] d_loss: 0.79711652, g_loss: 0.87063938\n",
      "Step: [5908] d_loss: 0.75547570, g_loss: 0.90161693\n",
      "Step: [5909] d_loss: 0.63561666, g_loss: 0.91892481\n",
      "Step: [5910] d_loss: 0.68241507, g_loss: 0.93961447\n",
      "Step: [5911] d_loss: 0.72997171, g_loss: 0.89404911\n",
      "Step: [5912] d_loss: 0.65236926, g_loss: 0.92904431\n",
      "Step: [5913] d_loss: 0.66824418, g_loss: 0.92212659\n",
      "Step: [5914] d_loss: 0.63888931, g_loss: 0.94057935\n",
      "Step: [5915] d_loss: 0.63401592, g_loss: 0.92658132\n",
      "Step: [5916] d_loss: 0.74660778, g_loss: 0.88201892\n",
      "Step: [5917] d_loss: 0.74404502, g_loss: 0.94818068\n",
      "Step: [5918] d_loss: 0.67099166, g_loss: 0.99898636\n",
      "Step: [5919] d_loss: 0.71599197, g_loss: 0.99632573\n",
      "Step: [5920] d_loss: 0.65042937, g_loss: 0.93511456\n",
      "Step: [5921] d_loss: 0.78414589, g_loss: 0.88675874\n",
      "Step: [5922] d_loss: 0.66831881, g_loss: 0.86513549\n",
      "Step: [5923] d_loss: 0.68551105, g_loss: 0.88944209\n",
      "Step: [5924] d_loss: 0.68382609, g_loss: 0.86485505\n",
      "Step: [5925] d_loss: 0.73159474, g_loss: 0.84505719\n",
      "Step: [5926] d_loss: 0.77978939, g_loss: 0.81706488\n",
      "Step: [5927] d_loss: 0.73633862, g_loss: 0.88924348\n",
      "Step: [5928] d_loss: 0.68489730, g_loss: 0.88491827\n",
      "Step: [5929] d_loss: 0.71075743, g_loss: 0.94576937\n",
      "Step: [5930] d_loss: 0.68089712, g_loss: 0.86413199\n",
      "Step: [5931] d_loss: 0.65714145, g_loss: 0.85057712\n",
      "Step: [5932] d_loss: 0.63896596, g_loss: 0.88356763\n",
      "Step: [5933] d_loss: 0.69144148, g_loss: 0.91483432\n",
      "Step: [5934] d_loss: 0.78081524, g_loss: 0.86645395\n",
      "Step: [5935] d_loss: 0.75041604, g_loss: 0.89970857\n",
      "Step: [5936] d_loss: 0.65966046, g_loss: 0.89048100\n",
      "Step: [5937] d_loss: 0.61674893, g_loss: 0.94342357\n",
      "Step: [5938] d_loss: 0.75838304, g_loss: 0.92000395\n",
      "Step: [5939] d_loss: 0.66396630, g_loss: 0.94319248\n",
      "Step: [5940] d_loss: 0.63091648, g_loss: 0.88063800\n",
      "Step: [5941] d_loss: 0.69307762, g_loss: 0.88451415\n",
      "Step: [5942] d_loss: 0.60858005, g_loss: 0.91766405\n",
      "Step: [5943] d_loss: 0.68600190, g_loss: 0.88349766\n",
      "Step: [5944] d_loss: 0.61276263, g_loss: 0.95640415\n",
      "Step: [5945] d_loss: 0.64970851, g_loss: 0.96384895\n",
      "Step: [5946] d_loss: 0.68282139, g_loss: 0.92596412\n",
      "Step: [5947] d_loss: 0.69409817, g_loss: 0.91841161\n",
      "Step: [5948] d_loss: 0.73210150, g_loss: 0.84716803\n",
      "Step: [5949] d_loss: 0.70531094, g_loss: 0.89604092\n",
      "Step: [5950] d_loss: 0.59563953, g_loss: 0.94105542\n",
      "Step: [5951] d_loss: 0.67854583, g_loss: 0.85322231\n",
      "Step: [5952] d_loss: 0.74234998, g_loss: 0.83917147\n",
      "Step: [5953] d_loss: 0.70287889, g_loss: 0.90357506\n",
      "Step: [5954] d_loss: 0.80487782, g_loss: 0.87757993\n",
      "Step: [5955] d_loss: 0.67147136, g_loss: 0.87711293\n",
      "Step: [5956] d_loss: 0.65821624, g_loss: 0.86574841\n",
      "Step: [5957] d_loss: 0.61810178, g_loss: 0.88778967\n",
      "Step: [5958] d_loss: 0.73356152, g_loss: 0.85332096\n",
      "Step: [5959] d_loss: 0.65800279, g_loss: 0.91977668\n",
      "Step: [5960] d_loss: 0.60631531, g_loss: 0.92987955\n",
      "Step: [5961] d_loss: 0.72924209, g_loss: 0.86623979\n",
      "Step: [5962] d_loss: 0.65359098, g_loss: 0.91368872\n",
      "Step: [5963] d_loss: 0.68124104, g_loss: 0.93807071\n",
      "Step: [5964] d_loss: 0.64113277, g_loss: 0.92200524\n",
      "Step: [5965] d_loss: 0.61995548, g_loss: 0.84781241\n",
      "Step: [5966] d_loss: 0.67232382, g_loss: 0.89389694\n",
      "Step: [5967] d_loss: 0.65975887, g_loss: 0.86162144\n",
      "Step: [5968] d_loss: 0.74948651, g_loss: 0.87542820\n",
      "Step: [5969] d_loss: 0.71005470, g_loss: 0.89187354\n",
      "Step: [5970] d_loss: 0.69802457, g_loss: 0.89827710\n",
      "Step: [5971] d_loss: 0.68951172, g_loss: 0.85740858\n",
      "Step: [5972] d_loss: 0.63513267, g_loss: 0.95234591\n",
      "Step: [5973] d_loss: 0.74674886, g_loss: 0.80728012\n",
      "Step: [5974] d_loss: 0.73349386, g_loss: 0.87469745\n",
      "Step: [5975] d_loss: 0.65440249, g_loss: 0.91201264\n",
      "Step: [5976] d_loss: 0.64840496, g_loss: 0.90928882\n",
      "Step: [5977] d_loss: 0.61840743, g_loss: 0.93106759\n",
      "Step: [5978] d_loss: 0.69120920, g_loss: 0.99751443\n",
      "Step: [5979] d_loss: 0.74522513, g_loss: 0.93053019\n",
      "Step: [5980] d_loss: 0.65785325, g_loss: 0.93785399\n",
      "Step: [5981] d_loss: 0.76188087, g_loss: 0.88965958\n",
      "Step: [5982] d_loss: 0.70421588, g_loss: 0.96305293\n",
      "Step: [5983] d_loss: 0.65313095, g_loss: 0.95755273\n",
      "Step: [5984] d_loss: 0.70554388, g_loss: 0.91588044\n",
      "Step: [5985] d_loss: 0.71488982, g_loss: 0.84105366\n",
      "Step: [5986] d_loss: 0.56440443, g_loss: 0.91258001\n",
      "Step: [5987] d_loss: 0.68021381, g_loss: 0.91912305\n",
      "Step: [5988] d_loss: 0.66234761, g_loss: 0.87183011\n",
      "Step: [5989] d_loss: 0.68803900, g_loss: 0.93955445\n",
      "Step: [5990] d_loss: 0.67577982, g_loss: 0.88416857\n",
      "Step: [5991] d_loss: 0.73884660, g_loss: 0.90825391\n",
      "Step: [5992] d_loss: 0.67523628, g_loss: 0.94664359\n",
      "Step: [5993] d_loss: 0.77566308, g_loss: 0.94908345\n",
      "Step: [5994] d_loss: 0.74478233, g_loss: 1.00811768\n",
      "Step: [5995] d_loss: 0.68051082, g_loss: 0.98809004\n",
      "Step: [5996] d_loss: 0.74245834, g_loss: 0.86684769\n",
      "Step: [5997] d_loss: 0.74676651, g_loss: 0.85316688\n",
      "Step: [5998] d_loss: 0.61699724, g_loss: 0.84993923\n",
      "Step: [5999] d_loss: 0.64779782, g_loss: 0.91484153\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "start_batch_id = 0\n",
    "\n",
    "\n",
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.reshape([batch_size,28,28,1]).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "\n",
    "    # update D network\n",
    "    _ , D_loss = sess.run([d_optim, d_loss], feed_dict={inputs: batch_images, z: batch_z})\n",
    "    # update G network\n",
    "    _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "    _, G_loss = sess.run([g_optim,g_loss], feed_dict={z: batch_z})\n",
    "\n",
    "        \n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "\n",
    "        samples = sess.run(fake_images, feed_dict={z: sample_z})\n",
    "        # put the \"batch_size\" images into one big canvas\n",
    "        row = col = int(np.sqrt(batch_size))\n",
    "        img = np.zeros( [row*28, col*28] )\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                img[i*28:(i+1)*28,j*28:(j+1)*28] = samples[i*col+j, :, :, :].squeeze()\n",
    "        #save the result      \n",
    "        scipy.misc.imsave('{}.jpg'.format(step_ind),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
