{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code is implementation of \"Improved Techniques for Training GANs\", but I didn't find a better implementation to reproduce the result of the generator. I'll be back to refine this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the network structure borrow from https://github.com/LDOUBLEV/semi-supervised-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "mnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn(net,scope,is_training):\n",
    "    return tf.contrib.layers.batch_norm(net, decay=0.9,updates_collections=None, epsilon=1e-5,scale=True,\n",
    "                                            is_training=is_training, scope=scope)\n",
    "\n",
    "def conv(net, wscope, bscope, output_depth=64, receptive_field=[5,5], stride=[2,2], padding='SAME', isDiscriminator=False):\n",
    "    shape = net.get_shape()\n",
    "    weights = tf.get_variable(wscope, receptive_field+[shape[-1], output_depth],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    if isDiscriminator:\n",
    "        tf.add_to_collection('regularizer', tf.contrib.layers.l2_regularizer(scale=0.0001)(weights)) # weight regularizer\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.nn.conv2d(net, weights, strides=[1]+stride+[1], padding=padding)\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def linear(net,wscope,bscope,output_depth,isDiscriminator=False):\n",
    "    shape = net.get_shape()       \n",
    "    weights = tf.get_variable(wscope, [shape[-1], output_depth], tf.float32,tf.random_normal_initializer(stddev=0.02))\n",
    "    if isDiscriminator:\n",
    "        tf.add_to_collection('regularizer', tf.contrib.layers.l2_regularizer(scale=0.0001)(weights)) # weight regularizer\n",
    "    biases = tf.get_variable(bscope, [output_depth], initializer=tf.constant_initializer(0.0))\n",
    "    out_logit = tf.matmul(net, weights) + biases\n",
    "    return out_logit\n",
    "\n",
    "def deconv(net,wscope,bscope,output_shape,receptive_field=[5,5],stride=[2,2]):\n",
    "    weights = tf.get_variable(wscope, receptive_field+[output_shape[-1], net.get_shape()[-1]],\n",
    "                                       initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "    #tf.add_to_collection('regularizer', tf.contrib.layers.l2_regularizer(scale=0.0001)(weights)) # weight regularizer\n",
    "    net = tf.nn.conv2d_transpose(net, weights, output_shape=output_shape, strides=[1]+stride+[1], padding='SAME')\n",
    "    biases = tf.get_variable(bscope, [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "    net = tf.reshape(tf.nn.bias_add(net, biases), net.get_shape())\n",
    "    return net\n",
    "\n",
    "def get_shape(net):\n",
    "    return net.get_shape().as_list()\n",
    "\n",
    "def display(img_samples=None, batch_size=None, img_h=None, img_w=None, image_dims=None, step_ind=None):\n",
    "    \n",
    "    row = col = int(np.sqrt(batch_size))\n",
    "    img = np.zeros( [row*img_h, col*img_w] )\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            img[i*img_w:(i+1)*img_w, j*img_h:(j+1)*img_h] = img_samples[i*col+j, :].reshape(image_dims[:2])\n",
    "    #save the result      \n",
    "    scipy.misc.imsave('multi_{}.jpg'.format(step_ind),img)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: add gaussian noise to each layer\n",
    "def discriminator(inputs, batch_size, is_training=True, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        outsli = []\n",
    "        \n",
    "        '''1st: Conv -> lrelu'''\n",
    "        net = conv(inputs,'d_wconv1','d_bconv1',64, [5,5], [2,2], 'SAME',True) # 28x28 -> 14x14 \n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        outsli.append(net)\n",
    "        \n",
    "        '''2nd: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv2','d_bconv2',64*2, [3,3], [2,2], 'SAME', True) # 14x14 -> 7x7\n",
    "        net = bn(net, scope = 'd_bn2',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        outsli.append(net)\n",
    "        \n",
    "        '''3th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv3','d_bconv3',64*4, [3,3], [1,1], 'VALID', True) # 7x7 -> 5x5\n",
    "        net = bn(net, scope = 'd_bn3',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        outsli.append(net)\n",
    "        \n",
    "        '''4th: Conv -> bn -> lrelu'''\n",
    "        net = conv(net,'d_wconv4','d_bconv4',64*4, [3,3], [2,2], 'VALID', True) # 5x5 -> 2x2\n",
    "        net = bn(net, scope = 'd_bn4',is_training=is_training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "        outsli.append(net)\n",
    "        \n",
    "        net = tf.reshape(net, [batch_size, -1]) # batch_size x (2x2x64x4)\n",
    "\n",
    "        '''5th: linear '''\n",
    "        out_logit = linear(net,\"d_wlinear5\",\"d_blinear5\",11, True) # attention\n",
    "        '''6th: sigmoid'''\n",
    "        # out = tf.nn.sigmoid(out_logit)  \n",
    "        out = outsli\n",
    "\n",
    "        return out, out_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator( z, batch_size, output_height=28, output_width=28,output_depth=1, is_training=True, reuse=False):\n",
    "    \n",
    "    height0,width0 = output_height,output_width # 28,28\n",
    "    height2,width2 = math.ceil(float(height0)/2),math.ceil(float(width0)/2) # 14,14\n",
    "    height4,width4 = math.ceil(float(height2)/2),math.ceil(float(width2)/2) # 7,7\n",
    "    height8,width8 = math.ceil(float(height4)/2),math.ceil(float(width4)/2) # 4,4\n",
    "    height16,width16 = math.ceil(float(height8)/2),math.ceil(float(width8)/2) # 2,2\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        \n",
    "        '''1st: linear -> reshape -> bn -> relu '''\n",
    "        net = linear(z, \"g_wlinear1\", \"g_blinear1\", 64*height16*width16)\n",
    "        net = tf.reshape(net,[batch_size, height16, width16, 64]) # -1,2,2,64\n",
    "        net = bn(net,scope='g_bn1',is_training=is_training)\n",
    "        net = tf.nn.relu(net)  \n",
    "    \n",
    "        '''2nd: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height8, width8, 64*4] # -1,4,4,64*4\n",
    "        net = deconv(net,'g_wdeconv2','g_bdeconv2',output_shape,[5,5],[2,2])\n",
    "        net = bn(net,scope='g_bn2',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''3th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height4, width4, 64*2] # -1,7,7,64*2\n",
    "        net = deconv(net,'g_wdeconv3','g_bdeconv3',output_shape,[5,5],[2,2])\n",
    "        net = bn(net,scope='g_bn3',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "       \n",
    "        '''4th: deconv -> bn -> relu '''\n",
    "        output_shape = [batch_size, height2, width2, 64*2] # -1,14,14,64\n",
    "        net = deconv(net,'g_wdeconv4','g_bdeconv4',output_shape,[5,5])\n",
    "        net = bn(net,scope='g_bn4',is_training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        '''5th: deconv -> tanh '''\n",
    "        output_shape = [batch_size, height0, width0, 1*1] # -1,28,28,1\n",
    "        net = deconv(net,'g_wdeconv5','g_bdeconv5',output_shape,[5,5])\n",
    "        out = tf.nn.sigmoid(net)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "image_dims = [28, 28, 1]\n",
    "batch_size = 64\n",
    "z_dim = 128\n",
    "y_dim = 10\n",
    "num_class = y_dim + 1\n",
    "learning_rate = 0.0002 #0.001\n",
    "beta1 = 0.5\n",
    "output_height,output_width,output_depth = [28,28,1]\n",
    "\n",
    "\"\"\" Graph Input \"\"\"\n",
    "# images\n",
    "inputs_supervised = tf.placeholder(tf.float32, [batch_size] + image_dims, name='supervised_images')\n",
    "inputs_unsupervised = tf.placeholder(tf.float32, [batch_size] + image_dims, name='unsupervised_images')\n",
    "\n",
    "#labels\n",
    "y = tf.placeholder(tf.float32, [batch_size,y_dim], name='y_label')\n",
    "# noises\n",
    "z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z_noise')\n",
    "\n",
    "flag = tf.placeholder(tf.float32, shape=[], name='flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different author has different implementation, you should choice one style of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below loss function structure borrowed from  \n",
    "https://github.com/LDOUBLEV/semi-supervised-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-19ab26b49293>:31: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "'''define the network'''\n",
    "# output of D for real images\n",
    "layerlist_real, d_logits_real = discriminator(inputs_supervised, batch_size, is_training=True, reuse=False)\n",
    "\n",
    "# output of D for fake images\n",
    "g_img = generator(z, batch_size, is_training=True, reuse=False)\n",
    "layerlist_fake, d_logits_fake = discriminator(g_img, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "'''weight normalization for discriminator'''\n",
    "d_regular = tf.add_n(tf.get_collection('regularizer', 'discriminator'), 'loss')  # D regular loss\n",
    "\n",
    "    \n",
    "\n",
    "'''caculate the unsupervised loss'''\n",
    "unsupervised_label_real = tf.concat([tf.ones_like(y), tf.zeros(shape=(batch_size, 1))], axis=1)\n",
    "unsupervised_label_fake = tf.concat([tf.zeros_like(y), tf.ones(shape=(batch_size, 1))], axis=1)\n",
    "# logits_r, logits_f = tf.nn.softmax(d_logits_real), tf.nn.softmax(d_logits_fake)\n",
    "\n",
    "d_unloss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=unsupervised_label_real*0.9, logits=d_logits_real))\n",
    "d_unloss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=unsupervised_label_fake*0.9, logits=d_logits_fake))\n",
    "\n",
    "'''feature match'''\n",
    "feature_match = tf.constant(0, dtype=tf.float32)\n",
    "tmp = [tf.reduce_mean(tf.multiply(r-f, r-f))  for r,f in zip(layerlist_real, layerlist_fake)]\n",
    "for i in tmp: feature_match += i\n",
    "\n",
    "'''caculate the supervised loss'''\n",
    "supervised_label = tf.concat([y,tf.zeros(shape=(batch_size, 1))], axis=1)\n",
    "supervised_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=supervised_label, logits=d_logits_real))\n",
    "supervised_loss_fake = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=unsupervised_label_fake*0.9, logits=d_logits_fake))  # same as d_loss_f\n",
    "\n",
    "d_loss_1, d_loss_2 = d_unloss_real - d_unloss_fake, supervised_loss_real\n",
    "\n",
    "''' loss of discriminator'''\n",
    "supervised_loss = supervised_loss_real + supervised_loss_fake\n",
    "unsupervised_loss = d_unloss_real + d_unloss_fake\n",
    "\n",
    "d_loss = unsupervised_loss + supervised_loss_real*flag + d_regular \n",
    "\n",
    "''' loss of generator '''\n",
    "g_loss = d_unloss_fake + 0.5*feature_match\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below loss function structure borrowed from    \n",
    "https://github.com/bruno-31/ImprovedGAN-Tensorflow/blob/master/train_cifar.py and  \n",
    "https://github.com/openai/improved-gan/blob/master/mnist_svhn_cifar10/train_mnist_feature_matching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "'''define the network'''\n",
    "# output of D for real images\n",
    "layerlist_real, d_logits_real = discriminator(inputs_supervised, batch_size, is_training=True, reuse=False)\n",
    "\n",
    "# output of D for fake images\n",
    "g_img = generator(z, batch_size, is_training=True, reuse=False)\n",
    "layerlist_fake, d_logits_fake = discriminator(g_img, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "layerlist_real_unsupervised, d_logits_unsupervised = discriminator(inputs_unsupervised, batch_size, is_training=True, reuse=True)\n",
    "\n",
    "\n",
    "'''caculate the unsupervised loss'''\n",
    "unsupervised_label_real = tf.concat([tf.ones_like(y), tf.zeros(shape=(batch_size, 1))], axis=1)\n",
    "unsupervised_label_fake = tf.concat([tf.zeros_like(y), tf.ones(shape=(batch_size, 1))], axis=1)\n",
    "# logits_r, logits_f = tf.nn.softmax(d_logits_real), tf.nn.softmax(d_logits_fake)\n",
    "\n",
    "d_unloss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=unsupervised_label_real*0.9, logits=d_logits_real))\n",
    "d_unloss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=unsupervised_label_fake*0.9, logits=d_logits_fake))\n",
    "\n",
    "'''feature match'''\n",
    "feature_match = tf.constant(0, dtype=tf.float32)\n",
    "tmp = [tf.reduce_mean(tf.multiply(r-f, r-f))  for r,f in zip(layerlist_real, layerlist_fake)]\n",
    "for i in tmp: feature_match += i\n",
    "\n",
    "'''caculate the supervised loss'''\n",
    "supervised_label = tf.concat([y,tf.zeros(shape=(batch_size, 1))], axis=1)\n",
    "supervised_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=supervised_label, logits=d_logits_real))\n",
    "supervised_loss_fake = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=unsupervised_label_fake*0.9, logits=d_logits_fake))  # same as d_loss_f\n",
    "\n",
    "\n",
    "''' loss of discriminator'''\n",
    "\n",
    "l_unl = tf.reduce_logsumexp(d_logits_unsupervised, axis=1)\n",
    "l_gen = tf.reduce_logsumexp(d_logits_fake, axis=1)\n",
    "supervised_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=supervised_label, logits=d_logits_real))\n",
    "unsupervised_loss = - 0.5 * tf.reduce_mean(l_unl) \\\n",
    "                   + 0.5 * tf.reduce_mean(tf.nn.softplus(l_unl)) \\\n",
    "                   + 0.5 * tf.reduce_mean(tf.nn.softplus(l_gen))\n",
    "\n",
    "d_loss = unsupervised_loss + supervised_loss\n",
    "\n",
    "''' loss of generator '''\n",
    "g_loss = feature_match\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the generator parameters and discriminator parameters into two list, then define how to train the two subnetwork and get the fake image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "# optimizers\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "\"\"\"\" Testing \"\"\"\n",
    "# for test\n",
    "_, test_logits = discriminator(inputs_supervised, batch_size, is_training=False, reuse=True)\n",
    "test_labels_pred = tf.nn.softmax(test_logits)\n",
    "temp = tf.reshape(test_labels_pred[:, -1], shape=[batch_size, 1])\n",
    "for i in range(10):\n",
    "    temp = tf.concat([temp, tf.reshape(test_labels_pred[:, -1],shape=[batch_size, 1])], axis=1)\n",
    "test_labels_pred -= temp\n",
    "prediction = tf.nn.in_top_k(test_labels_pred, tf.argmax(supervised_label, axis=1), 1)\n",
    "\n",
    "\n",
    "fake_images = generator(z, batch_size, is_training=False, reuse=True)\n",
    "\n",
    "# graph inputs for visualize training results\n",
    "sample_z = np.random.uniform(-1, 1, size=(batch_size , z_dim)).astype(np.float32)\n",
    "\n",
    "test_labels_onehot = np.zeros([batch_size, y_dim],dtype = np.float32)\n",
    "test_labels_onehot[np.arange(batch_size), np.arange(batch_size)%int(y_dim)] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [0] d_loss: 2.74950695, g_loss: 1.49454856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: [1] d_loss: 1.96254301, g_loss: 1.35227847\n",
      "Step: [2] d_loss: 1.73136282, g_loss: 1.32268691\n",
      "Step: [3] d_loss: 1.49738908, g_loss: 1.29308164\n",
      "Step: [4] d_loss: 1.35347605, g_loss: 1.26671267\n",
      "Step: [5] d_loss: 1.21242261, g_loss: 1.26091623\n",
      "Step: [6] d_loss: 1.08511353, g_loss: 1.24323249\n",
      "Step: [7] d_loss: 1.11811829, g_loss: 1.22308862\n",
      "Step: [8] d_loss: 1.05652559, g_loss: 1.21816635\n",
      "Step: [9] d_loss: 1.00498426, g_loss: 1.18879640\n",
      "Step: [10] d_loss: 0.99508077, g_loss: 1.17240310\n",
      "Step: [11] d_loss: 0.95415258, g_loss: 1.17435229\n",
      "Step: [12] d_loss: 0.92159289, g_loss: 1.14264071\n",
      "Step: [13] d_loss: 0.94192803, g_loss: 1.14142275\n",
      "Step: [14] d_loss: 0.82619208, g_loss: 1.13413107\n",
      "Step: [15] d_loss: 0.80396771, g_loss: 1.13649464\n",
      "Step: [16] d_loss: 0.78793514, g_loss: 1.12904787\n",
      "Step: [17] d_loss: 0.83301014, g_loss: 1.10376573\n",
      "Step: [18] d_loss: 0.82380354, g_loss: 1.11703765\n",
      "Step: [19] d_loss: 0.80326885, g_loss: 1.11349821\n",
      "Step: [20] d_loss: 0.80454636, g_loss: 1.07388473\n",
      "Step: [21] d_loss: 0.77918899, g_loss: 1.10251415\n",
      "Step: [22] d_loss: 0.73614317, g_loss: 1.07543063\n",
      "Step: [23] d_loss: 0.67762446, g_loss: 1.08830535\n",
      "Step: [24] d_loss: 0.67979068, g_loss: 1.07040083\n",
      "Step: [25] d_loss: 0.73068339, g_loss: 1.05932701\n",
      "Step: [26] d_loss: 0.81492513, g_loss: 1.07948697\n",
      "Step: [27] d_loss: 0.78752249, g_loss: 1.04892933\n",
      "Step: [28] d_loss: 0.66832078, g_loss: 1.07230854\n",
      "Step: [29] d_loss: 0.77015108, g_loss: 1.04801571\n",
      "Step: [30] d_loss: 0.66818273, g_loss: 1.07693493\n",
      "Step: [31] d_loss: 0.70718640, g_loss: 1.07759953\n",
      "Step: [32] d_loss: 0.64129776, g_loss: 1.05630565\n",
      "Step: [33] d_loss: 0.70563161, g_loss: 1.05481613\n",
      "Step: [34] d_loss: 0.58582133, g_loss: 1.04937410\n",
      "Step: [35] d_loss: 0.62134451, g_loss: 1.06100988\n",
      "Step: [36] d_loss: 0.64089555, g_loss: 1.04504597\n",
      "Step: [37] d_loss: 0.71804917, g_loss: 1.06512499\n",
      "Step: [38] d_loss: 0.69222653, g_loss: 1.04329014\n",
      "Step: [39] d_loss: 0.58079129, g_loss: 1.04510581\n",
      "Step: [40] d_loss: 0.61898726, g_loss: 1.05905306\n",
      "Step: [41] d_loss: 0.64656460, g_loss: 1.06558263\n",
      "Step: [42] d_loss: 0.60650563, g_loss: 1.06652272\n",
      "Step: [43] d_loss: 0.54619074, g_loss: 1.07409835\n",
      "Step: [44] d_loss: 0.58916247, g_loss: 1.07124877\n",
      "Step: [45] d_loss: 0.60630774, g_loss: 1.07696521\n",
      "Step: [46] d_loss: 0.60939342, g_loss: 1.06517780\n",
      "Step: [47] d_loss: 0.65420860, g_loss: 1.06379819\n",
      "Step: [48] d_loss: 0.59419268, g_loss: 1.07820964\n",
      "Step: [49] d_loss: 0.59641176, g_loss: 1.06442940\n",
      "Step: [50] d_loss: 0.55307794, g_loss: 1.05471969\n",
      "Step: [51] d_loss: 0.63304120, g_loss: 1.07728636\n",
      "Step: [52] d_loss: 0.57160944, g_loss: 1.07531548\n",
      "Step: [53] d_loss: 0.57974780, g_loss: 1.08132398\n",
      "Step: [54] d_loss: 0.58184975, g_loss: 1.02070689\n",
      "Step: [55] d_loss: 0.59707439, g_loss: 1.04773211\n",
      "Step: [56] d_loss: 0.60704547, g_loss: 1.05216289\n",
      "Step: [57] d_loss: 0.60290170, g_loss: 1.07286847\n",
      "Step: [58] d_loss: 0.62463146, g_loss: 1.05847287\n",
      "Step: [59] d_loss: 0.58949840, g_loss: 1.07541502\n",
      "Step: [60] d_loss: 0.56678218, g_loss: 1.04976797\n",
      "Step: [61] d_loss: 0.82435095, g_loss: 1.48718286\n",
      "Step: [62] d_loss: 0.65272653, g_loss: 1.12981486\n",
      "Step: [63] d_loss: 0.60162544, g_loss: 1.10487199\n",
      "Step: [64] d_loss: 0.56395137, g_loss: 1.08652818\n",
      "Step: [65] d_loss: 0.64921737, g_loss: 1.08841562\n",
      "Step: [66] d_loss: 0.60420758, g_loss: 1.07560492\n",
      "Step: [67] d_loss: 0.62590343, g_loss: 1.10315585\n",
      "Step: [68] d_loss: 0.52120733, g_loss: 1.06988883\n",
      "Step: [69] d_loss: 0.56032759, g_loss: 1.12776804\n",
      "Step: [70] d_loss: 0.54744124, g_loss: 1.09758580\n",
      "Step: [71] d_loss: 0.55663288, g_loss: 1.08644724\n",
      "Step: [72] d_loss: 0.55904394, g_loss: 1.06708646\n",
      "Step: [73] d_loss: 0.57608402, g_loss: 1.10237384\n",
      "Step: [74] d_loss: 0.59865350, g_loss: 1.09032178\n",
      "Step: [75] d_loss: 0.55715799, g_loss: 1.10379028\n",
      "Step: [76] d_loss: 0.57133126, g_loss: 1.10157979\n",
      "Step: [77] d_loss: 0.55747080, g_loss: 1.12122536\n",
      "Step: [78] d_loss: 0.53107560, g_loss: 1.08966637\n",
      "Step: [79] d_loss: 0.56822628, g_loss: 1.08246815\n",
      "Step: [80] d_loss: 0.54094040, g_loss: 1.11213267\n",
      "Step: [81] d_loss: 0.50632077, g_loss: 1.11407351\n",
      "Step: [82] d_loss: 0.51613081, g_loss: 1.07809961\n",
      "Step: [83] d_loss: 0.57103980, g_loss: 1.08896983\n",
      "Step: [84] d_loss: 0.52998638, g_loss: 1.08578813\n",
      "Step: [85] d_loss: 0.51319629, g_loss: 1.09691739\n",
      "Step: [86] d_loss: 0.53758663, g_loss: 1.09326458\n",
      "Step: [87] d_loss: 0.56034410, g_loss: 1.09177089\n",
      "Step: [88] d_loss: 0.52642834, g_loss: 1.10813403\n",
      "Step: [89] d_loss: 0.61439794, g_loss: 1.08715594\n",
      "Step: [90] d_loss: 0.51390254, g_loss: 1.08583963\n",
      "Step: [91] d_loss: 0.52197462, g_loss: 1.09509587\n",
      "Step: [92] d_loss: 0.55355167, g_loss: 1.08075726\n",
      "Step: [93] d_loss: 0.50657487, g_loss: 1.06960464\n",
      "Step: [94] d_loss: 0.50375354, g_loss: 1.10766816\n",
      "Step: [95] d_loss: 0.52189016, g_loss: 1.08796203\n",
      "Step: [96] d_loss: 0.50314522, g_loss: 1.08421659\n",
      "Step: [97] d_loss: 0.51757729, g_loss: 1.06965423\n",
      "Step: [98] d_loss: 0.56814140, g_loss: 1.07535124\n",
      "Step: [99] d_loss: 0.54250127, g_loss: 1.07826340\n",
      "Step: [100] d_loss: 0.54310858, g_loss: 1.07753062\n",
      "Step: [101] d_loss: 0.55656826, g_loss: 1.06442869\n",
      "Step: [102] d_loss: 0.52872181, g_loss: 1.08834100\n",
      "Step: [103] d_loss: 0.51805246, g_loss: 1.07926059\n",
      "Step: [104] d_loss: 0.50739294, g_loss: 1.09696758\n",
      "Step: [105] d_loss: 0.57878721, g_loss: 1.07162881\n",
      "Step: [106] d_loss: 0.55141157, g_loss: 1.09126174\n",
      "Step: [107] d_loss: 0.55153084, g_loss: 1.07098997\n",
      "Step: [108] d_loss: 0.53712893, g_loss: 1.07452583\n",
      "Step: [109] d_loss: 0.56582612, g_loss: 1.06576276\n",
      "Step: [110] d_loss: 0.48906493, g_loss: 1.02767587\n",
      "Step: [111] d_loss: 0.96020705, g_loss: 1.19562995\n",
      "Step: [112] d_loss: 0.81175184, g_loss: 1.15298414\n",
      "Step: [113] d_loss: 0.62858588, g_loss: 1.08335543\n",
      "Step: [114] d_loss: 0.55466366, g_loss: 1.16014194\n",
      "Step: [115] d_loss: 0.57885349, g_loss: 1.16781366\n",
      "Step: [116] d_loss: 0.52755892, g_loss: 1.13519263\n",
      "Step: [117] d_loss: 0.52384508, g_loss: 1.12133646\n",
      "Step: [118] d_loss: 0.61409032, g_loss: 1.12198436\n",
      "Step: [119] d_loss: 0.54406130, g_loss: 1.13134599\n",
      "Step: [120] d_loss: 0.55397773, g_loss: 1.12150025\n",
      "Step: [121] d_loss: 0.54701787, g_loss: 1.11439323\n",
      "Step: [122] d_loss: 0.54552484, g_loss: 1.11258602\n",
      "Step: [123] d_loss: 0.51691556, g_loss: 1.10034418\n",
      "Step: [124] d_loss: 0.48474303, g_loss: 1.11655617\n",
      "Step: [125] d_loss: 0.54400277, g_loss: 1.12299645\n",
      "Step: [126] d_loss: 0.50896502, g_loss: 1.10278440\n",
      "Step: [127] d_loss: 0.53273004, g_loss: 1.09227800\n",
      "Step: [128] d_loss: 0.56055140, g_loss: 1.11895680\n",
      "Step: [129] d_loss: 0.54367709, g_loss: 1.06458390\n",
      "Step: [130] d_loss: 0.52187830, g_loss: 1.10035312\n",
      "Step: [131] d_loss: 0.52668774, g_loss: 1.09312665\n",
      "Step: [132] d_loss: 0.51327366, g_loss: 1.09802294\n",
      "Step: [133] d_loss: 0.52648586, g_loss: 1.09488857\n",
      "Step: [134] d_loss: 0.50572276, g_loss: 1.12729728\n",
      "Step: [135] d_loss: 0.52111000, g_loss: 1.10956502\n",
      "Step: [136] d_loss: 0.47365078, g_loss: 1.10990810\n",
      "Step: [137] d_loss: 0.50322878, g_loss: 1.13971031\n",
      "Step: [138] d_loss: 0.52311993, g_loss: 1.11654294\n",
      "Step: [139] d_loss: 0.55597478, g_loss: 1.11122334\n",
      "Step: [140] d_loss: 0.50034690, g_loss: 1.15451121\n",
      "Step: [141] d_loss: 0.53853315, g_loss: 1.11909020\n",
      "Step: [142] d_loss: 0.51785666, g_loss: 1.11425829\n",
      "Step: [143] d_loss: 0.51699042, g_loss: 1.13927853\n",
      "Step: [144] d_loss: 0.48732758, g_loss: 1.14038718\n",
      "Step: [145] d_loss: 0.55816215, g_loss: 1.11176252\n",
      "Step: [146] d_loss: 0.60213661, g_loss: 1.10780931\n",
      "Step: [147] d_loss: 0.60531926, g_loss: 1.09897089\n",
      "Step: [148] d_loss: 0.52611184, g_loss: 1.11453223\n",
      "Step: [149] d_loss: 0.52380645, g_loss: 1.13054681\n",
      "Step: [150] d_loss: 0.48911455, g_loss: 1.09628916\n",
      "Step: [151] d_loss: 0.49066117, g_loss: 1.10691750\n",
      "Step: [152] d_loss: 0.51451939, g_loss: 1.12216198\n",
      "Step: [153] d_loss: 0.54292262, g_loss: 1.08054805\n",
      "Step: [154] d_loss: 0.54592156, g_loss: 1.06930745\n",
      "Step: [155] d_loss: 0.56359547, g_loss: 1.07340848\n",
      "Step: [156] d_loss: 0.50759518, g_loss: 1.08588469\n",
      "Step: [157] d_loss: 0.48555997, g_loss: 1.09243822\n",
      "Step: [158] d_loss: 0.50349545, g_loss: 1.07553279\n",
      "Step: [159] d_loss: 0.49341917, g_loss: 1.09988427\n",
      "Step: [160] d_loss: 0.47883371, g_loss: 1.07338691\n",
      "Step: [161] d_loss: 0.51732439, g_loss: 1.09066832\n",
      "Step: [162] d_loss: 0.52913761, g_loss: 1.07794809\n",
      "Step: [163] d_loss: 0.49555731, g_loss: 1.07727969\n",
      "Step: [164] d_loss: 0.47345704, g_loss: 1.07537746\n",
      "Step: [165] d_loss: 0.53091532, g_loss: 1.08968520\n",
      "Step: [166] d_loss: 0.51770365, g_loss: 1.06963229\n",
      "Step: [167] d_loss: 0.48709214, g_loss: 1.06949651\n",
      "Step: [168] d_loss: 0.51410532, g_loss: 1.05544722\n",
      "Step: [169] d_loss: 0.48313099, g_loss: 1.06976795\n",
      "Step: [170] d_loss: 0.49784067, g_loss: 1.07593346\n",
      "Step: [171] d_loss: 0.49134174, g_loss: 1.07104325\n",
      "Step: [172] d_loss: 0.54768604, g_loss: 1.04540730\n",
      "Step: [173] d_loss: 0.51227772, g_loss: 1.05792630\n",
      "Step: [174] d_loss: 0.56926471, g_loss: 1.05788183\n",
      "Step: [175] d_loss: 0.53914475, g_loss: 1.10666430\n",
      "Step: [176] d_loss: 0.51834857, g_loss: 1.06773496\n",
      "Step: [177] d_loss: 0.51203239, g_loss: 1.08134663\n",
      "Step: [178] d_loss: 0.50300276, g_loss: 1.09361279\n",
      "Step: [179] d_loss: 0.52140349, g_loss: 1.07441831\n",
      "Step: [180] d_loss: 0.53674406, g_loss: 1.09008205\n",
      "Step: [181] d_loss: 0.49907848, g_loss: 1.05070913\n",
      "Step: [182] d_loss: 0.53946632, g_loss: 1.08583677\n",
      "Step: [183] d_loss: 0.51549149, g_loss: 1.10070229\n",
      "Step: [184] d_loss: 0.48630291, g_loss: 1.06808591\n",
      "Step: [185] d_loss: 0.48978376, g_loss: 1.09364712\n",
      "Step: [186] d_loss: 0.51999879, g_loss: 1.08769989\n",
      "Step: [187] d_loss: 0.49448663, g_loss: 1.05928624\n",
      "Step: [188] d_loss: 0.50800174, g_loss: 1.09445643\n",
      "Step: [189] d_loss: 0.47841340, g_loss: 1.10414815\n",
      "Step: [190] d_loss: 0.49375260, g_loss: 1.06864846\n",
      "Step: [191] d_loss: 0.51275498, g_loss: 1.08333862\n",
      "Step: [192] d_loss: 0.52264971, g_loss: 1.06428242\n",
      "Step: [193] d_loss: 0.51232141, g_loss: 1.05967951\n",
      "Step: [194] d_loss: 0.49475962, g_loss: 1.07150638\n",
      "Step: [195] d_loss: 0.50471264, g_loss: 1.12104750\n",
      "Step: [196] d_loss: 0.50377011, g_loss: 1.09266293\n",
      "Step: [197] d_loss: 0.48833731, g_loss: 1.08790207\n",
      "Step: [198] d_loss: 0.51087612, g_loss: 1.06816697\n",
      "Step: [199] d_loss: 0.51059294, g_loss: 1.07090247\n",
      "Step: [200] d_loss: 0.49275595, g_loss: 1.08699775\n",
      "Step: [201] d_loss: 0.50068319, g_loss: 1.09262013\n",
      "Step: [202] d_loss: 0.49631554, g_loss: 1.09220862\n",
      "Step: [203] d_loss: 0.54380608, g_loss: 1.06578815\n",
      "Step: [204] d_loss: 0.54379517, g_loss: 1.07825828\n",
      "Step: [205] d_loss: 0.48935825, g_loss: 1.08056080\n",
      "Step: [206] d_loss: 0.53877211, g_loss: 1.08256853\n",
      "Step: [207] d_loss: 0.50871497, g_loss: 1.07106376\n",
      "Step: [208] d_loss: 0.49643692, g_loss: 1.10133469\n",
      "Step: [209] d_loss: 0.48328385, g_loss: 1.10024095\n",
      "Step: [210] d_loss: 0.48413354, g_loss: 1.11052024\n",
      "Step: [211] d_loss: 0.47108710, g_loss: 1.08521938\n",
      "Step: [212] d_loss: 0.48817545, g_loss: 1.11988914\n",
      "Step: [213] d_loss: 0.51279265, g_loss: 1.11449170\n",
      "Step: [214] d_loss: 0.52049571, g_loss: 1.08776259\n",
      "Step: [215] d_loss: 0.48528033, g_loss: 1.08235288\n",
      "Step: [216] d_loss: 0.49484697, g_loss: 1.08722830\n",
      "Step: [217] d_loss: 0.49748141, g_loss: 1.10330307\n",
      "Step: [218] d_loss: 0.48946857, g_loss: 1.07619548\n",
      "Step: [219] d_loss: 0.49598491, g_loss: 1.08905876\n",
      "Step: [220] d_loss: 0.49421009, g_loss: 1.07145727\n",
      "Step: [221] d_loss: 0.51846886, g_loss: 1.09457254\n",
      "Step: [222] d_loss: 0.50393766, g_loss: 1.09555626\n",
      "Step: [223] d_loss: 0.48443756, g_loss: 1.06981587\n",
      "Step: [224] d_loss: 0.48374805, g_loss: 1.06972682\n",
      "Step: [225] d_loss: 0.47275773, g_loss: 1.09974957\n",
      "Step: [226] d_loss: 0.48060116, g_loss: 1.06205845\n",
      "Step: [227] d_loss: 0.55102515, g_loss: 1.10943604\n",
      "Step: [228] d_loss: 0.50092554, g_loss: 1.09272528\n",
      "Step: [229] d_loss: 0.49381030, g_loss: 1.07493329\n",
      "Step: [230] d_loss: 0.50208485, g_loss: 1.06216180\n",
      "Step: [231] d_loss: 0.49763089, g_loss: 1.08276725\n",
      "Step: [232] d_loss: 0.49235940, g_loss: 1.03664184\n",
      "Step: [233] d_loss: 0.54119557, g_loss: 1.06097353\n",
      "Step: [234] d_loss: 0.48698425, g_loss: 1.04828930\n",
      "Step: [235] d_loss: 0.47872847, g_loss: 1.06167984\n",
      "Step: [236] d_loss: 0.50195467, g_loss: 1.04732716\n",
      "Step: [237] d_loss: 0.48302883, g_loss: 1.07837808\n",
      "Step: [238] d_loss: 0.48085803, g_loss: 1.08073270\n",
      "Step: [239] d_loss: 0.49911481, g_loss: 1.06589007\n",
      "Step: [240] d_loss: 0.47843879, g_loss: 1.08066869\n",
      "Step: [241] d_loss: 0.49037758, g_loss: 1.07087433\n",
      "Step: [242] d_loss: 0.50643265, g_loss: 1.06838596\n",
      "Step: [243] d_loss: 0.47611177, g_loss: 1.09458244\n",
      "Step: [244] d_loss: 0.49525616, g_loss: 1.07660627\n",
      "Step: [245] d_loss: 0.50430715, g_loss: 1.09675074\n",
      "Step: [246] d_loss: 0.49052644, g_loss: 1.08049905\n",
      "Step: [247] d_loss: 0.55385745, g_loss: 1.08556080\n",
      "Step: [248] d_loss: 0.47797692, g_loss: 1.10325646\n",
      "Step: [249] d_loss: 0.47540423, g_loss: 1.10436201\n",
      "Step: [250] d_loss: 0.46689433, g_loss: 1.09230268\n",
      "Step: [251] d_loss: 0.53101063, g_loss: 1.06153226\n",
      "Step: [252] d_loss: 0.49346849, g_loss: 1.11570084\n",
      "Step: [253] d_loss: 0.55463159, g_loss: 1.10049188\n",
      "Step: [254] d_loss: 0.49134260, g_loss: 1.10769701\n",
      "Step: [255] d_loss: 0.48628259, g_loss: 1.13270271\n",
      "Step: [256] d_loss: 0.50417203, g_loss: 1.10577869\n",
      "Step: [257] d_loss: 0.47344029, g_loss: 1.05980265\n",
      "Step: [258] d_loss: 0.49412560, g_loss: 1.09361064\n",
      "Step: [259] d_loss: 0.56002426, g_loss: 1.08872521\n",
      "Step: [260] d_loss: 0.51440459, g_loss: 1.11996579\n",
      "Step: [261] d_loss: 0.50810319, g_loss: 1.07240629\n",
      "Step: [262] d_loss: 0.47734335, g_loss: 1.10651994\n",
      "Step: [263] d_loss: 0.47030264, g_loss: 1.08679020\n",
      "Step: [264] d_loss: 0.48205373, g_loss: 1.10132599\n",
      "Step: [265] d_loss: 0.48106319, g_loss: 1.08196640\n",
      "Step: [266] d_loss: 0.49306470, g_loss: 1.07222509\n",
      "Step: [267] d_loss: 0.51367629, g_loss: 1.10056126\n",
      "Step: [268] d_loss: 0.47387344, g_loss: 1.10488212\n",
      "Step: [269] d_loss: 0.48132005, g_loss: 1.08327818\n",
      "Step: [270] d_loss: 0.49542272, g_loss: 1.05415666\n",
      "Step: [271] d_loss: 0.47127160, g_loss: 1.12855971\n",
      "Step: [272] d_loss: 0.50573874, g_loss: 1.10275733\n",
      "Step: [273] d_loss: 0.49494091, g_loss: 1.07464921\n",
      "Step: [274] d_loss: 0.47501493, g_loss: 1.10855186\n",
      "Step: [275] d_loss: 0.46984231, g_loss: 1.08496463\n",
      "Step: [276] d_loss: 0.48395488, g_loss: 1.09402239\n",
      "Step: [277] d_loss: 0.46615881, g_loss: 1.08619654\n",
      "Step: [278] d_loss: 0.47571236, g_loss: 1.09295332\n",
      "Step: [279] d_loss: 0.46669561, g_loss: 1.11175323\n",
      "Step: [280] d_loss: 0.47236374, g_loss: 1.09632540\n",
      "Step: [281] d_loss: 0.46955976, g_loss: 1.04772878\n",
      "Step: [282] d_loss: 0.49711016, g_loss: 1.07400346\n",
      "Step: [283] d_loss: 0.50060391, g_loss: 1.06599069\n",
      "Step: [284] d_loss: 0.47699568, g_loss: 1.09491074\n",
      "Step: [285] d_loss: 0.48669592, g_loss: 1.10597003\n",
      "Step: [286] d_loss: 0.47085130, g_loss: 1.08334887\n",
      "Step: [287] d_loss: 0.50385368, g_loss: 1.08035183\n",
      "Step: [288] d_loss: 0.49626690, g_loss: 1.05558348\n",
      "Step: [289] d_loss: 0.49822176, g_loss: 1.06460238\n",
      "Step: [290] d_loss: 0.47298929, g_loss: 1.07648802\n",
      "Step: [291] d_loss: 0.51058882, g_loss: 1.06394732\n",
      "Step: [292] d_loss: 0.46531612, g_loss: 1.07538819\n",
      "Step: [293] d_loss: 0.48388609, g_loss: 1.02710426\n",
      "Step: [294] d_loss: 0.48400992, g_loss: 1.08384812\n",
      "Step: [295] d_loss: 0.49171931, g_loss: 1.06786239\n",
      "Step: [296] d_loss: 0.47993323, g_loss: 1.06744289\n",
      "Step: [297] d_loss: 0.51349998, g_loss: 1.07303929\n",
      "Step: [298] d_loss: 0.49872711, g_loss: 1.05898404\n",
      "Step: [299] d_loss: 0.58335960, g_loss: 1.07498741\n",
      "Step: [300] d_loss: 0.56125867, g_loss: 1.07596135\n",
      "Step: [301] d_loss: 0.54181111, g_loss: 1.11423337\n",
      "Step: [302] d_loss: 0.49371439, g_loss: 1.10657680\n",
      "Step: [303] d_loss: 0.48851803, g_loss: 1.08542120\n",
      "Step: [304] d_loss: 0.47704127, g_loss: 1.08560181\n",
      "Step: [305] d_loss: 0.49412924, g_loss: 1.07573318\n",
      "Step: [306] d_loss: 0.51582718, g_loss: 1.07543993\n",
      "Step: [307] d_loss: 0.46932474, g_loss: 1.07996762\n",
      "Step: [308] d_loss: 0.50771856, g_loss: 1.05326331\n",
      "Step: [309] d_loss: 0.50599527, g_loss: 1.07135582\n",
      "Step: [310] d_loss: 0.48344645, g_loss: 1.06865108\n",
      "Step: [311] d_loss: 0.51906979, g_loss: 1.08218050\n",
      "Step: [312] d_loss: 0.50630403, g_loss: 1.04571569\n",
      "Step: [313] d_loss: 0.50595105, g_loss: 1.07879698\n",
      "Step: [314] d_loss: 0.48408180, g_loss: 1.08420277\n",
      "Step: [315] d_loss: 0.50413412, g_loss: 1.10617292\n",
      "Step: [316] d_loss: 0.51873136, g_loss: 1.08510137\n",
      "Step: [317] d_loss: 0.48793837, g_loss: 1.13250351\n",
      "Step: [318] d_loss: 0.47704014, g_loss: 1.12395644\n",
      "Step: [319] d_loss: 0.48451281, g_loss: 1.11193478\n",
      "Step: [320] d_loss: 0.47680813, g_loss: 1.12419295\n",
      "Step: [321] d_loss: 0.47757217, g_loss: 1.10471427\n",
      "Step: [322] d_loss: 0.47436988, g_loss: 1.11695158\n",
      "Step: [323] d_loss: 0.46785384, g_loss: 1.11533141\n",
      "Step: [324] d_loss: 0.47340530, g_loss: 1.13417983\n",
      "Step: [325] d_loss: 0.46723720, g_loss: 1.12675226\n",
      "Step: [326] d_loss: 0.49752522, g_loss: 1.08667445\n",
      "Step: [327] d_loss: 0.51016557, g_loss: 1.11932576\n",
      "Step: [328] d_loss: 0.48227429, g_loss: 1.11396575\n",
      "Step: [329] d_loss: 0.51242185, g_loss: 1.09807873\n",
      "Step: [330] d_loss: 0.48009107, g_loss: 1.08519280\n",
      "Step: [331] d_loss: 0.47210464, g_loss: 1.07235086\n",
      "Step: [332] d_loss: 0.48113960, g_loss: 1.06384003\n",
      "Step: [333] d_loss: 0.49111906, g_loss: 1.08516538\n",
      "Step: [334] d_loss: 0.47005579, g_loss: 1.09726560\n",
      "Step: [335] d_loss: 0.47485191, g_loss: 1.07579052\n",
      "Step: [336] d_loss: 0.46608356, g_loss: 1.10048723\n",
      "Step: [337] d_loss: 0.47359681, g_loss: 1.07315505\n",
      "Step: [338] d_loss: 0.46753314, g_loss: 1.07876444\n",
      "Step: [339] d_loss: 0.47432080, g_loss: 1.06580603\n",
      "Step: [340] d_loss: 0.47144371, g_loss: 1.09764087\n",
      "Step: [341] d_loss: 0.47701851, g_loss: 1.07749927\n",
      "Step: [342] d_loss: 0.48311284, g_loss: 1.09812152\n",
      "Step: [343] d_loss: 0.47218016, g_loss: 1.08760905\n",
      "Step: [344] d_loss: 0.46549720, g_loss: 1.07335556\n",
      "Step: [345] d_loss: 0.46971485, g_loss: 1.07156551\n",
      "Step: [346] d_loss: 0.49165046, g_loss: 1.06214035\n",
      "Step: [347] d_loss: 0.46825284, g_loss: 1.09399831\n",
      "Step: [348] d_loss: 0.49364719, g_loss: 1.07646823\n",
      "Step: [349] d_loss: 0.48330057, g_loss: 1.11784899\n",
      "Step: [350] d_loss: 0.49103349, g_loss: 1.11030543\n",
      "Step: [351] d_loss: 0.48484012, g_loss: 1.13255298\n",
      "Step: [352] d_loss: 0.47227386, g_loss: 1.10793197\n",
      "Step: [353] d_loss: 0.48899120, g_loss: 1.12900853\n",
      "Step: [354] d_loss: 0.48243088, g_loss: 1.12038207\n",
      "Step: [355] d_loss: 0.46988517, g_loss: 1.10305130\n",
      "Step: [356] d_loss: 0.48076808, g_loss: 1.09891236\n",
      "Step: [357] d_loss: 0.52055544, g_loss: 1.08859015\n",
      "Step: [358] d_loss: 0.49329320, g_loss: 1.10751021\n",
      "Step: [359] d_loss: 0.48032087, g_loss: 1.08669937\n",
      "Step: [360] d_loss: 0.47305706, g_loss: 1.11304867\n",
      "Step: [361] d_loss: 0.47124326, g_loss: 1.06295228\n",
      "Step: [362] d_loss: 0.47539499, g_loss: 1.08530176\n",
      "Step: [363] d_loss: 0.47411969, g_loss: 1.09815133\n",
      "Step: [364] d_loss: 0.48335958, g_loss: 1.08503258\n",
      "Step: [365] d_loss: 0.47278297, g_loss: 1.06906796\n",
      "Step: [366] d_loss: 0.48003814, g_loss: 1.07952428\n",
      "Step: [367] d_loss: 0.47131228, g_loss: 1.06178975\n",
      "Step: [368] d_loss: 0.47636592, g_loss: 1.06473339\n",
      "Step: [369] d_loss: 0.47273779, g_loss: 1.08552742\n",
      "Step: [370] d_loss: 0.46351531, g_loss: 1.08471215\n",
      "Step: [371] d_loss: 0.50204098, g_loss: 1.09373248\n",
      "Step: [372] d_loss: 0.47107041, g_loss: 1.12792051\n",
      "Step: [373] d_loss: 0.46173111, g_loss: 1.07846999\n",
      "Step: [374] d_loss: 0.57636923, g_loss: 1.11966443\n",
      "Step: [375] d_loss: 0.50634444, g_loss: 1.12874222\n",
      "Step: [376] d_loss: 0.48562175, g_loss: 1.08245373\n",
      "Step: [377] d_loss: 0.46865675, g_loss: 1.09098434\n",
      "Step: [378] d_loss: 0.49565360, g_loss: 1.11848152\n",
      "Step: [379] d_loss: 0.48435920, g_loss: 1.07755864\n",
      "Step: [380] d_loss: 0.47496164, g_loss: 1.11432087\n",
      "Step: [381] d_loss: 0.47562078, g_loss: 1.09447289\n",
      "Step: [382] d_loss: 0.47570622, g_loss: 1.10190523\n",
      "Step: [383] d_loss: 0.47108319, g_loss: 1.09326661\n",
      "Step: [384] d_loss: 0.47296003, g_loss: 1.09950233\n",
      "Step: [385] d_loss: 0.47460914, g_loss: 1.08558846\n",
      "Step: [386] d_loss: 0.51777512, g_loss: 1.11555982\n",
      "Step: [387] d_loss: 0.48335564, g_loss: 1.09037793\n",
      "Step: [388] d_loss: 0.49405444, g_loss: 1.07331824\n",
      "Step: [389] d_loss: 0.49944350, g_loss: 1.08555031\n",
      "Step: [390] d_loss: 0.48523730, g_loss: 1.06677985\n",
      "Step: [391] d_loss: 0.48071250, g_loss: 1.08547056\n",
      "Step: [392] d_loss: 0.48817453, g_loss: 1.09812069\n",
      "Step: [393] d_loss: 0.51096064, g_loss: 1.05679262\n",
      "Step: [394] d_loss: 0.47007132, g_loss: 1.08948159\n",
      "Step: [395] d_loss: 0.47444767, g_loss: 1.06657171\n",
      "Step: [396] d_loss: 0.47196978, g_loss: 1.09477723\n",
      "Step: [397] d_loss: 0.47444826, g_loss: 1.06115866\n",
      "Step: [398] d_loss: 0.47617191, g_loss: 1.08825886\n",
      "Step: [399] d_loss: 0.46257648, g_loss: 1.09732568\n",
      "Step: [400] d_loss: 0.46932808, g_loss: 1.08403659\n",
      "Step: [401] d_loss: 0.46539810, g_loss: 1.08789814\n",
      "Step: [402] d_loss: 0.46012568, g_loss: 1.10317743\n",
      "Step: [403] d_loss: 0.46943805, g_loss: 1.07773840\n",
      "Step: [404] d_loss: 0.48120910, g_loss: 1.04716182\n",
      "Step: [405] d_loss: 0.47878918, g_loss: 1.06031811\n",
      "Step: [406] d_loss: 0.47492805, g_loss: 1.06867850\n",
      "Step: [407] d_loss: 0.46680960, g_loss: 1.12422347\n",
      "Step: [408] d_loss: 0.53357613, g_loss: 1.10331595\n",
      "Step: [409] d_loss: 0.47333914, g_loss: 1.09024727\n",
      "Step: [410] d_loss: 0.47001672, g_loss: 1.07903302\n",
      "Step: [411] d_loss: 0.46280068, g_loss: 1.08086622\n",
      "Step: [412] d_loss: 0.46981257, g_loss: 1.06078231\n",
      "Step: [413] d_loss: 0.48069453, g_loss: 1.07881665\n",
      "Step: [414] d_loss: 0.48416254, g_loss: 1.04789948\n",
      "Step: [415] d_loss: 0.47548735, g_loss: 1.07667279\n",
      "Step: [416] d_loss: 0.47483066, g_loss: 1.04386699\n",
      "Step: [417] d_loss: 0.47234011, g_loss: 1.10844195\n",
      "Step: [418] d_loss: 0.48390076, g_loss: 1.11875606\n",
      "Step: [419] d_loss: 0.47354352, g_loss: 1.10583150\n",
      "Step: [420] d_loss: 0.46561152, g_loss: 1.09053421\n",
      "Step: [421] d_loss: 0.47913480, g_loss: 1.10941875\n",
      "Step: [422] d_loss: 0.47175032, g_loss: 1.07763517\n",
      "Step: [423] d_loss: 0.47555092, g_loss: 1.11343014\n",
      "Step: [424] d_loss: 0.47482288, g_loss: 1.08549142\n",
      "Step: [425] d_loss: 0.47414538, g_loss: 1.11371768\n",
      "Step: [426] d_loss: 0.46496314, g_loss: 1.11173964\n",
      "Step: [427] d_loss: 0.46775198, g_loss: 1.10199273\n",
      "Step: [428] d_loss: 0.48064405, g_loss: 1.09450138\n",
      "Step: [429] d_loss: 0.46451709, g_loss: 1.11672509\n",
      "Step: [430] d_loss: 0.47733665, g_loss: 1.11333752\n",
      "Step: [431] d_loss: 0.46006423, g_loss: 1.09295440\n",
      "Step: [432] d_loss: 0.47427499, g_loss: 1.12149787\n",
      "Step: [433] d_loss: 0.46660179, g_loss: 1.08416426\n",
      "Step: [434] d_loss: 0.52420819, g_loss: 1.12285686\n",
      "Step: [435] d_loss: 0.46285364, g_loss: 1.11591291\n",
      "Step: [436] d_loss: 0.47777981, g_loss: 1.11861098\n",
      "Step: [437] d_loss: 0.47213975, g_loss: 1.12477183\n",
      "Step: [438] d_loss: 0.46799475, g_loss: 1.09893119\n",
      "Step: [439] d_loss: 0.46721959, g_loss: 1.11951065\n",
      "Step: [440] d_loss: 0.46818382, g_loss: 1.08371294\n",
      "Step: [441] d_loss: 0.46223587, g_loss: 1.05719280\n",
      "Step: [442] d_loss: 0.47377765, g_loss: 1.07745981\n",
      "Step: [443] d_loss: 0.47539526, g_loss: 1.07422006\n",
      "Step: [444] d_loss: 0.46173123, g_loss: 1.10498464\n",
      "Step: [445] d_loss: 0.48101342, g_loss: 1.09505475\n",
      "Step: [446] d_loss: 0.48700634, g_loss: 1.09840631\n",
      "Step: [447] d_loss: 0.46430403, g_loss: 1.11132932\n",
      "Step: [448] d_loss: 0.48183024, g_loss: 1.08656621\n",
      "Step: [449] d_loss: 0.48476887, g_loss: 1.08852911\n",
      "Step: [450] d_loss: 0.46714306, g_loss: 1.09151363\n",
      "Step: [451] d_loss: 0.45583072, g_loss: 1.09771144\n",
      "Step: [452] d_loss: 0.47678077, g_loss: 1.11121762\n",
      "Step: [453] d_loss: 0.46513912, g_loss: 1.10097563\n",
      "Step: [454] d_loss: 0.46060258, g_loss: 1.09763563\n",
      "Step: [455] d_loss: 0.47140956, g_loss: 1.08796740\n",
      "Step: [456] d_loss: 0.45937157, g_loss: 1.08472466\n",
      "Step: [457] d_loss: 0.46963054, g_loss: 1.09926522\n",
      "Step: [458] d_loss: 0.47856921, g_loss: 1.09410536\n",
      "Step: [459] d_loss: 0.47788355, g_loss: 1.09575236\n",
      "Step: [460] d_loss: 0.47785223, g_loss: 1.09123194\n",
      "Step: [461] d_loss: 0.47693345, g_loss: 1.12596011\n",
      "Step: [462] d_loss: 0.49343526, g_loss: 1.11506033\n",
      "Step: [463] d_loss: 0.48652443, g_loss: 1.10020626\n",
      "Step: [464] d_loss: 0.46751970, g_loss: 1.12594438\n",
      "Step: [465] d_loss: 0.45311317, g_loss: 1.15225995\n",
      "Step: [466] d_loss: 0.47454602, g_loss: 1.11564159\n",
      "Step: [467] d_loss: 0.46354640, g_loss: 1.15773261\n",
      "Step: [468] d_loss: 0.46222147, g_loss: 1.11582136\n",
      "Step: [469] d_loss: 0.48415142, g_loss: 1.14109683\n",
      "Step: [470] d_loss: 0.49384844, g_loss: 1.12312341\n",
      "Step: [471] d_loss: 0.46433669, g_loss: 1.12492752\n",
      "Step: [472] d_loss: 0.52012914, g_loss: 1.14100993\n",
      "Step: [473] d_loss: 0.46841916, g_loss: 1.10196579\n",
      "Step: [474] d_loss: 0.51300466, g_loss: 1.13889205\n",
      "Step: [475] d_loss: 0.48444453, g_loss: 1.13176191\n",
      "Step: [476] d_loss: 0.46786579, g_loss: 1.09371412\n",
      "Step: [477] d_loss: 0.46316779, g_loss: 1.11237156\n",
      "Step: [478] d_loss: 0.47706443, g_loss: 1.09846461\n",
      "Step: [479] d_loss: 0.46757019, g_loss: 1.09640610\n",
      "Step: [480] d_loss: 0.47887138, g_loss: 1.08540571\n",
      "Step: [481] d_loss: 0.47043511, g_loss: 1.09084904\n",
      "Step: [482] d_loss: 0.47444776, g_loss: 1.08042181\n",
      "Step: [483] d_loss: 0.46956185, g_loss: 1.07800758\n",
      "Step: [484] d_loss: 0.47330880, g_loss: 1.09032214\n",
      "Step: [485] d_loss: 0.47080886, g_loss: 1.10472846\n",
      "Step: [486] d_loss: 0.48478478, g_loss: 1.11735570\n",
      "Step: [487] d_loss: 0.46937367, g_loss: 1.11510181\n",
      "Step: [488] d_loss: 0.46272120, g_loss: 1.09280849\n",
      "Step: [489] d_loss: 0.46214098, g_loss: 1.09811425\n",
      "Step: [490] d_loss: 0.46228275, g_loss: 1.09504986\n",
      "Step: [491] d_loss: 0.47937629, g_loss: 1.08708787\n",
      "Step: [492] d_loss: 0.46425620, g_loss: 1.09319031\n",
      "Step: [493] d_loss: 0.47731930, g_loss: 1.11991656\n",
      "Step: [494] d_loss: 0.48455513, g_loss: 1.10998738\n",
      "Step: [495] d_loss: 0.46652183, g_loss: 1.13448358\n",
      "Step: [496] d_loss: 0.46797463, g_loss: 1.11951303\n",
      "Step: [497] d_loss: 0.46711445, g_loss: 1.12768102\n",
      "Step: [498] d_loss: 0.48003656, g_loss: 1.12616742\n",
      "Step: [499] d_loss: 0.49656677, g_loss: 1.12536049\n",
      "Step: [500] d_loss: 0.46400774, g_loss: 1.13584054\n",
      "Step: [501] d_loss: 0.46642315, g_loss: 1.10715067\n",
      "Step: [502] d_loss: 0.46868277, g_loss: 1.10331821\n",
      "Step: [503] d_loss: 0.47441113, g_loss: 1.11811137\n",
      "Step: [504] d_loss: 0.47824168, g_loss: 1.09856546\n",
      "Step: [505] d_loss: 0.47349787, g_loss: 1.12380314\n",
      "Step: [506] d_loss: 0.48981008, g_loss: 1.14513791\n",
      "Step: [507] d_loss: 0.46965659, g_loss: 1.09379494\n",
      "Step: [508] d_loss: 0.47765300, g_loss: 1.11596489\n",
      "Step: [509] d_loss: 0.46821931, g_loss: 1.15309703\n",
      "Step: [510] d_loss: 0.46170825, g_loss: 1.09276211\n",
      "Step: [511] d_loss: 0.47481126, g_loss: 1.11956656\n",
      "Step: [512] d_loss: 0.46364978, g_loss: 1.13000977\n",
      "Step: [513] d_loss: 0.46197575, g_loss: 1.11131775\n",
      "Step: [514] d_loss: 0.50102448, g_loss: 1.03342712\n",
      "Step: [515] d_loss: 0.54499966, g_loss: 1.10977459\n",
      "Step: [516] d_loss: 0.48285943, g_loss: 1.09613252\n",
      "Step: [517] d_loss: 0.48207596, g_loss: 1.09263444\n",
      "Step: [518] d_loss: 0.50305814, g_loss: 1.08056378\n",
      "Step: [519] d_loss: 0.47255808, g_loss: 1.11802828\n",
      "Step: [520] d_loss: 0.46487325, g_loss: 1.08904982\n",
      "Step: [521] d_loss: 0.46771052, g_loss: 1.08724773\n",
      "Step: [522] d_loss: 0.46414191, g_loss: 1.07672524\n",
      "Step: [523] d_loss: 0.47491384, g_loss: 1.06336224\n",
      "Step: [524] d_loss: 0.49128529, g_loss: 1.04630792\n",
      "Step: [525] d_loss: 0.47408208, g_loss: 1.07920134\n",
      "Step: [526] d_loss: 0.46107730, g_loss: 1.09450018\n",
      "Step: [527] d_loss: 0.46782061, g_loss: 1.10734963\n",
      "Step: [528] d_loss: 0.45418525, g_loss: 1.11118507\n",
      "Step: [529] d_loss: 0.47941393, g_loss: 1.07770789\n",
      "Step: [530] d_loss: 0.46550721, g_loss: 1.11382222\n",
      "Step: [531] d_loss: 0.45789692, g_loss: 1.11861074\n",
      "Step: [532] d_loss: 0.46068111, g_loss: 1.10703182\n",
      "Step: [533] d_loss: 0.46516162, g_loss: 1.11291409\n",
      "Step: [534] d_loss: 0.47561952, g_loss: 1.10913002\n",
      "Step: [535] d_loss: 0.46076119, g_loss: 1.11328220\n",
      "Step: [536] d_loss: 0.46694097, g_loss: 1.13924551\n",
      "Step: [537] d_loss: 0.46224767, g_loss: 1.12295628\n",
      "Step: [538] d_loss: 0.46443504, g_loss: 1.10321665\n",
      "Step: [539] d_loss: 0.47427177, g_loss: 1.12861621\n",
      "Step: [540] d_loss: 0.45674482, g_loss: 1.12935400\n",
      "Step: [541] d_loss: 0.46522838, g_loss: 1.11850095\n",
      "Step: [542] d_loss: 0.46570095, g_loss: 1.11703062\n",
      "Step: [543] d_loss: 0.50757176, g_loss: 1.11864913\n",
      "Step: [544] d_loss: 0.47999522, g_loss: 1.09481120\n",
      "Step: [545] d_loss: 0.47618055, g_loss: 1.08312976\n",
      "Step: [546] d_loss: 0.46565166, g_loss: 1.07820642\n",
      "Step: [547] d_loss: 0.46163601, g_loss: 1.06476247\n",
      "Step: [548] d_loss: 0.48338544, g_loss: 1.06604314\n",
      "Step: [549] d_loss: 0.47116947, g_loss: 1.07826900\n",
      "Step: [550] d_loss: 0.46323705, g_loss: 1.05322528\n",
      "Step: [551] d_loss: 0.46665457, g_loss: 1.05778098\n",
      "Step: [552] d_loss: 0.47009447, g_loss: 1.09530890\n",
      "Step: [553] d_loss: 0.46956122, g_loss: 1.08284461\n",
      "Step: [554] d_loss: 0.47437236, g_loss: 1.07646120\n",
      "Step: [555] d_loss: 0.45824078, g_loss: 1.08416700\n",
      "Step: [556] d_loss: 0.46653157, g_loss: 1.08279884\n",
      "Step: [557] d_loss: 0.49637747, g_loss: 1.09068704\n",
      "Step: [558] d_loss: 0.46375746, g_loss: 1.09359992\n",
      "Step: [559] d_loss: 0.48442602, g_loss: 1.09882998\n",
      "Step: [560] d_loss: 0.47177303, g_loss: 1.09163058\n",
      "Step: [561] d_loss: 0.47502565, g_loss: 1.07480752\n",
      "Step: [562] d_loss: 0.46582147, g_loss: 1.10890079\n",
      "Step: [563] d_loss: 0.47598517, g_loss: 1.13391697\n",
      "Step: [564] d_loss: 0.50232452, g_loss: 1.09592485\n",
      "Step: [565] d_loss: 0.45286161, g_loss: 1.09098244\n",
      "Step: [566] d_loss: 0.48247084, g_loss: 1.07488394\n",
      "Step: [567] d_loss: 0.47071323, g_loss: 1.10539293\n",
      "Step: [568] d_loss: 0.51127762, g_loss: 1.08829296\n",
      "Step: [569] d_loss: 0.49043620, g_loss: 1.09956896\n",
      "Step: [570] d_loss: 0.47943220, g_loss: 1.08969820\n",
      "Step: [571] d_loss: 0.47304800, g_loss: 1.11084795\n",
      "Step: [572] d_loss: 0.47247070, g_loss: 1.09115231\n",
      "Step: [573] d_loss: 0.46458316, g_loss: 1.11494792\n",
      "Step: [574] d_loss: 0.46418774, g_loss: 1.10907054\n",
      "Step: [575] d_loss: 0.45884919, g_loss: 1.09667540\n",
      "Step: [576] d_loss: 0.49342418, g_loss: 1.09540546\n",
      "Step: [577] d_loss: 0.47114590, g_loss: 1.10008228\n",
      "Step: [578] d_loss: 0.46866158, g_loss: 1.10730684\n",
      "Step: [579] d_loss: 0.47205186, g_loss: 1.07577324\n",
      "Step: [580] d_loss: 0.46481645, g_loss: 1.11664009\n",
      "Step: [581] d_loss: 0.46319792, g_loss: 1.10147047\n",
      "Step: [582] d_loss: 0.45282254, g_loss: 1.09176183\n",
      "Step: [583] d_loss: 0.46877018, g_loss: 1.11605084\n",
      "Step: [584] d_loss: 0.46641490, g_loss: 1.11853635\n",
      "Step: [585] d_loss: 0.55534375, g_loss: 1.13103557\n",
      "Step: [586] d_loss: 0.46504101, g_loss: 1.10084689\n",
      "Step: [587] d_loss: 0.45558599, g_loss: 1.11303496\n",
      "Step: [588] d_loss: 0.46195483, g_loss: 1.11586237\n",
      "Step: [589] d_loss: 0.45685080, g_loss: 1.10517371\n",
      "Step: [590] d_loss: 0.46606237, g_loss: 1.14853191\n",
      "Step: [591] d_loss: 0.50609905, g_loss: 1.11597598\n",
      "Step: [592] d_loss: 0.47776747, g_loss: 1.11386657\n",
      "Step: [593] d_loss: 0.47472799, g_loss: 1.15826035\n",
      "Step: [594] d_loss: 0.46139282, g_loss: 1.15459371\n",
      "Step: [595] d_loss: 0.45541120, g_loss: 1.13113296\n",
      "Step: [596] d_loss: 0.46697187, g_loss: 1.13389170\n",
      "Step: [597] d_loss: 0.46352559, g_loss: 1.13392437\n",
      "Step: [598] d_loss: 0.46878213, g_loss: 1.12242842\n",
      "Step: [599] d_loss: 0.46483278, g_loss: 1.08739960\n",
      "Step: [600] d_loss: 0.50641745, g_loss: 1.07286167\n",
      "Step: [601] d_loss: 0.46878842, g_loss: 1.12143981\n",
      "Step: [602] d_loss: 0.46647102, g_loss: 1.11793077\n",
      "Step: [603] d_loss: 0.47582424, g_loss: 1.10253441\n",
      "Step: [604] d_loss: 0.46093163, g_loss: 1.10624027\n",
      "Step: [605] d_loss: 0.45819855, g_loss: 1.13602698\n",
      "Step: [606] d_loss: 0.46235335, g_loss: 1.13095176\n",
      "Step: [607] d_loss: 0.45540264, g_loss: 1.08760941\n",
      "Step: [608] d_loss: 0.45253763, g_loss: 1.09064412\n",
      "Step: [609] d_loss: 0.45260197, g_loss: 1.12611282\n",
      "Step: [610] d_loss: 0.46190244, g_loss: 1.11008096\n",
      "Step: [611] d_loss: 0.46217060, g_loss: 1.08560884\n",
      "Step: [612] d_loss: 0.46670926, g_loss: 1.10710585\n",
      "Step: [613] d_loss: 0.45951054, g_loss: 1.10219252\n",
      "Step: [614] d_loss: 0.46101981, g_loss: 1.11903822\n",
      "Step: [615] d_loss: 0.46394733, g_loss: 1.11106336\n",
      "Step: [616] d_loss: 0.46711716, g_loss: 1.07797170\n",
      "Step: [617] d_loss: 0.46184713, g_loss: 1.09233093\n",
      "Step: [618] d_loss: 0.47780776, g_loss: 1.10487187\n",
      "Step: [619] d_loss: 0.46167547, g_loss: 1.11248827\n",
      "Step: [620] d_loss: 0.45999610, g_loss: 1.08574140\n",
      "Step: [621] d_loss: 0.46028736, g_loss: 1.07793748\n",
      "Step: [622] d_loss: 0.46137273, g_loss: 1.08875883\n",
      "Step: [623] d_loss: 0.45622274, g_loss: 1.07745457\n",
      "Step: [624] d_loss: 0.45972344, g_loss: 1.08848453\n",
      "Step: [625] d_loss: 0.46380901, g_loss: 1.04096735\n",
      "Step: [626] d_loss: 0.47614571, g_loss: 1.03100133\n",
      "Step: [627] d_loss: 0.50303692, g_loss: 1.09177780\n",
      "Step: [628] d_loss: 0.46735495, g_loss: 1.10892892\n",
      "Step: [629] d_loss: 0.46900228, g_loss: 1.07211053\n",
      "Step: [630] d_loss: 0.45354986, g_loss: 1.05505073\n",
      "Step: [631] d_loss: 0.46517578, g_loss: 1.10796392\n",
      "Step: [632] d_loss: 0.46064612, g_loss: 1.06944847\n",
      "Step: [633] d_loss: 0.46415702, g_loss: 1.10241997\n",
      "Step: [634] d_loss: 0.45849425, g_loss: 1.08192301\n",
      "Step: [635] d_loss: 0.45667458, g_loss: 1.08985412\n",
      "Step: [636] d_loss: 0.45452544, g_loss: 1.08319688\n",
      "Step: [637] d_loss: 0.47767240, g_loss: 1.10801280\n",
      "Step: [638] d_loss: 0.47927654, g_loss: 1.10942411\n",
      "Step: [639] d_loss: 0.46525350, g_loss: 1.12399530\n",
      "Step: [640] d_loss: 0.46096784, g_loss: 1.10431457\n",
      "Step: [641] d_loss: 0.46565059, g_loss: 1.08241248\n",
      "Step: [642] d_loss: 0.45607030, g_loss: 1.10671365\n",
      "Step: [643] d_loss: 0.48261660, g_loss: 1.07808685\n",
      "Step: [644] d_loss: 0.47935456, g_loss: 1.11269128\n",
      "Step: [645] d_loss: 0.46069610, g_loss: 1.13473952\n",
      "Step: [646] d_loss: 0.46284252, g_loss: 1.09758079\n",
      "Step: [647] d_loss: 0.46186844, g_loss: 1.09256160\n",
      "Step: [648] d_loss: 0.46330976, g_loss: 1.11905193\n",
      "Step: [649] d_loss: 0.45736039, g_loss: 1.09746242\n",
      "Step: [650] d_loss: 0.48865387, g_loss: 1.12598562\n",
      "Step: [651] d_loss: 0.46540272, g_loss: 1.12793601\n",
      "Step: [652] d_loss: 0.46737811, g_loss: 1.12580073\n",
      "Step: [653] d_loss: 0.46045834, g_loss: 1.08836186\n",
      "Step: [654] d_loss: 0.46052912, g_loss: 1.11523545\n",
      "Step: [655] d_loss: 0.46677876, g_loss: 1.08968103\n",
      "Step: [656] d_loss: 0.46292785, g_loss: 1.08231175\n",
      "Step: [657] d_loss: 0.46714413, g_loss: 1.07691264\n",
      "Step: [658] d_loss: 0.45819050, g_loss: 1.09939039\n",
      "Step: [659] d_loss: 0.45945728, g_loss: 1.06416953\n",
      "Step: [660] d_loss: 0.45894858, g_loss: 1.10523903\n",
      "Step: [661] d_loss: 0.45866805, g_loss: 1.07277596\n",
      "Step: [662] d_loss: 0.46304244, g_loss: 1.09560597\n",
      "Step: [663] d_loss: 0.46952990, g_loss: 1.08446574\n",
      "Step: [664] d_loss: 0.47096288, g_loss: 1.08061528\n",
      "Step: [665] d_loss: 0.46222311, g_loss: 1.07126963\n",
      "Step: [666] d_loss: 0.46055257, g_loss: 1.09248734\n",
      "Step: [667] d_loss: 0.45768481, g_loss: 1.04733825\n",
      "Step: [668] d_loss: 0.46160650, g_loss: 1.08849299\n",
      "Step: [669] d_loss: 0.46710035, g_loss: 1.11028636\n",
      "Step: [670] d_loss: 0.44777596, g_loss: 1.07072830\n",
      "Step: [671] d_loss: 0.46522391, g_loss: 1.09635663\n",
      "Step: [672] d_loss: 0.45430404, g_loss: 1.07920611\n",
      "Step: [673] d_loss: 0.46049085, g_loss: 1.11657119\n",
      "Step: [674] d_loss: 0.46019235, g_loss: 1.08951306\n",
      "Step: [675] d_loss: 0.46216518, g_loss: 1.12548220\n",
      "Step: [676] d_loss: 0.53995097, g_loss: 0.90338963\n",
      "Step: [677] d_loss: 4.00026989, g_loss: 1.22007298\n",
      "Step: [678] d_loss: 1.02934146, g_loss: 1.12200427\n",
      "Step: [679] d_loss: 0.64604884, g_loss: 1.18140626\n",
      "Step: [680] d_loss: 0.60405093, g_loss: 1.21473467\n",
      "Step: [681] d_loss: 0.63484251, g_loss: 1.18350840\n",
      "Step: [682] d_loss: 0.59947407, g_loss: 1.16282463\n",
      "Step: [683] d_loss: 0.59790820, g_loss: 1.15764689\n",
      "Step: [684] d_loss: 0.58643204, g_loss: 1.13844180\n",
      "Step: [685] d_loss: 0.59136981, g_loss: 1.09619784\n",
      "Step: [686] d_loss: 0.60784602, g_loss: 1.11911070\n",
      "Step: [687] d_loss: 0.58181691, g_loss: 1.06996357\n",
      "Step: [688] d_loss: 0.53492093, g_loss: 1.06495392\n",
      "Step: [689] d_loss: 0.55378938, g_loss: 1.10495830\n",
      "Step: [690] d_loss: 0.51047069, g_loss: 1.10303950\n",
      "Step: [691] d_loss: 0.49936852, g_loss: 1.11580646\n",
      "Step: [692] d_loss: 0.48882520, g_loss: 1.12157989\n",
      "Step: [693] d_loss: 0.49775469, g_loss: 1.12283504\n",
      "Step: [694] d_loss: 0.54125100, g_loss: 1.09493947\n",
      "Step: [695] d_loss: 0.52019835, g_loss: 1.11610508\n",
      "Step: [696] d_loss: 0.48146647, g_loss: 1.12094939\n",
      "Step: [697] d_loss: 0.50912702, g_loss: 1.11687052\n",
      "Step: [698] d_loss: 0.50727493, g_loss: 1.10134888\n",
      "Step: [699] d_loss: 0.48855829, g_loss: 1.12345743\n",
      "Step: [700] d_loss: 0.47911906, g_loss: 1.10890484\n",
      "Step: [701] d_loss: 0.48644173, g_loss: 1.14905894\n",
      "Step: [702] d_loss: 0.49548823, g_loss: 1.15095973\n",
      "Step: [703] d_loss: 0.54372865, g_loss: 1.11193562\n",
      "Step: [704] d_loss: 0.47914308, g_loss: 1.13576305\n",
      "Step: [705] d_loss: 0.51225191, g_loss: 1.09844744\n",
      "Step: [706] d_loss: 0.54173011, g_loss: 1.10416663\n",
      "Step: [707] d_loss: 0.51382303, g_loss: 1.12301576\n",
      "Step: [708] d_loss: 0.49014860, g_loss: 1.12063229\n",
      "Step: [709] d_loss: 0.47845596, g_loss: 1.10213029\n",
      "Step: [710] d_loss: 0.48536116, g_loss: 1.14131606\n",
      "Step: [711] d_loss: 0.48050922, g_loss: 1.12570512\n",
      "Step: [712] d_loss: 0.49069166, g_loss: 1.14008689\n",
      "Step: [713] d_loss: 0.49034253, g_loss: 1.11838150\n",
      "Step: [714] d_loss: 0.48954514, g_loss: 1.12957907\n",
      "Step: [715] d_loss: 0.49731293, g_loss: 1.14086795\n",
      "Step: [716] d_loss: 0.49554721, g_loss: 1.09294546\n",
      "Step: [717] d_loss: 0.51916987, g_loss: 1.09871542\n",
      "Step: [718] d_loss: 0.47888562, g_loss: 1.13595772\n",
      "Step: [719] d_loss: 0.47621405, g_loss: 1.12579572\n",
      "Step: [720] d_loss: 0.50212294, g_loss: 1.15366149\n",
      "Step: [721] d_loss: 0.48039842, g_loss: 1.11435688\n",
      "Step: [722] d_loss: 0.49271116, g_loss: 1.15149224\n",
      "Step: [723] d_loss: 0.49917048, g_loss: 1.13723493\n",
      "Step: [724] d_loss: 0.49430868, g_loss: 1.15604055\n",
      "Step: [725] d_loss: 0.48528090, g_loss: 1.12904620\n",
      "Step: [726] d_loss: 0.49523699, g_loss: 1.10694659\n",
      "Step: [727] d_loss: 0.47730508, g_loss: 1.11406732\n",
      "Step: [728] d_loss: 0.50122833, g_loss: 1.15396547\n",
      "Step: [729] d_loss: 0.49128994, g_loss: 1.12620425\n",
      "Step: [730] d_loss: 0.47603095, g_loss: 1.12108421\n",
      "Step: [731] d_loss: 0.47583309, g_loss: 1.13393307\n",
      "Step: [732] d_loss: 0.49826166, g_loss: 1.11981022\n",
      "Step: [733] d_loss: 0.47683793, g_loss: 1.13293672\n",
      "Step: [734] d_loss: 0.49548301, g_loss: 1.10054219\n",
      "Step: [735] d_loss: 0.47113565, g_loss: 1.10329282\n",
      "Step: [736] d_loss: 0.48178145, g_loss: 1.11909235\n",
      "Step: [737] d_loss: 0.47911662, g_loss: 1.11414981\n",
      "Step: [738] d_loss: 0.48955277, g_loss: 1.11032236\n",
      "Step: [739] d_loss: 0.48202127, g_loss: 1.11577427\n",
      "Step: [740] d_loss: 0.48527241, g_loss: 1.13190770\n",
      "Step: [741] d_loss: 0.47539550, g_loss: 1.11870766\n",
      "Step: [742] d_loss: 0.46994740, g_loss: 1.13208330\n",
      "Step: [743] d_loss: 0.47834426, g_loss: 1.12448835\n",
      "Step: [744] d_loss: 0.49295980, g_loss: 1.09647810\n",
      "Step: [745] d_loss: 0.49032274, g_loss: 1.11538863\n",
      "Step: [746] d_loss: 0.47974172, g_loss: 1.12542069\n",
      "Step: [747] d_loss: 0.48564747, g_loss: 1.14054704\n",
      "Step: [748] d_loss: 0.48624370, g_loss: 1.10755932\n",
      "Step: [749] d_loss: 0.52433616, g_loss: 1.11010504\n",
      "Step: [750] d_loss: 0.48152331, g_loss: 1.15523326\n",
      "Step: [751] d_loss: 0.48712358, g_loss: 1.12835002\n",
      "Step: [752] d_loss: 0.48613188, g_loss: 1.11487651\n",
      "Step: [753] d_loss: 0.46327788, g_loss: 1.12553132\n",
      "Step: [754] d_loss: 0.47950706, g_loss: 1.11508024\n",
      "Step: [755] d_loss: 0.46811524, g_loss: 1.12303996\n",
      "Step: [756] d_loss: 0.46171558, g_loss: 1.14291847\n",
      "Step: [757] d_loss: 0.46389019, g_loss: 1.11222315\n",
      "Step: [758] d_loss: 0.51592898, g_loss: 1.12935662\n",
      "Step: [759] d_loss: 0.46794638, g_loss: 1.16290510\n",
      "Step: [760] d_loss: 0.46889052, g_loss: 1.07635236\n",
      "Step: [761] d_loss: 0.49839211, g_loss: 1.11700451\n",
      "Step: [762] d_loss: 0.47203416, g_loss: 1.18621409\n",
      "Step: [763] d_loss: 0.49177271, g_loss: 1.12416232\n",
      "Step: [764] d_loss: 0.48609239, g_loss: 1.10837793\n",
      "Step: [765] d_loss: 0.48166662, g_loss: 1.13563108\n",
      "Step: [766] d_loss: 0.47626659, g_loss: 1.11600769\n",
      "Step: [767] d_loss: 0.47003162, g_loss: 1.12150109\n",
      "Step: [768] d_loss: 0.47592682, g_loss: 1.14363253\n",
      "Step: [769] d_loss: 0.50519621, g_loss: 1.13816822\n",
      "Step: [770] d_loss: 0.46755329, g_loss: 1.13365149\n",
      "Step: [771] d_loss: 0.47307265, g_loss: 1.10694695\n",
      "Step: [772] d_loss: 0.45968232, g_loss: 1.12170923\n",
      "Step: [773] d_loss: 0.48786059, g_loss: 1.12407243\n",
      "Step: [774] d_loss: 0.47429389, g_loss: 1.12620878\n",
      "Step: [775] d_loss: 0.48566553, g_loss: 1.12427855\n",
      "Step: [776] d_loss: 0.48432368, g_loss: 1.11332440\n",
      "Step: [777] d_loss: 0.48121595, g_loss: 1.14559293\n",
      "Step: [778] d_loss: 0.49382564, g_loss: 1.13326740\n",
      "Step: [779] d_loss: 0.45536235, g_loss: 1.13334668\n",
      "Step: [780] d_loss: 0.47417474, g_loss: 1.11368752\n",
      "Step: [781] d_loss: 0.49283186, g_loss: 1.11536837\n",
      "Step: [782] d_loss: 0.45908460, g_loss: 1.11623800\n",
      "Step: [783] d_loss: 0.46582741, g_loss: 1.09870696\n",
      "Step: [784] d_loss: 0.47173065, g_loss: 1.12457025\n",
      "Step: [785] d_loss: 0.48421487, g_loss: 1.09637022\n",
      "Step: [786] d_loss: 0.46862984, g_loss: 1.09834599\n",
      "Step: [787] d_loss: 0.49520281, g_loss: 1.11202812\n",
      "Step: [788] d_loss: 0.48777345, g_loss: 1.09468997\n",
      "Step: [789] d_loss: 0.48699844, g_loss: 1.07894695\n",
      "Step: [790] d_loss: 0.49010479, g_loss: 1.08145225\n",
      "Step: [791] d_loss: 0.49177065, g_loss: 1.08256006\n",
      "Step: [792] d_loss: 0.48196313, g_loss: 1.10486662\n",
      "Step: [793] d_loss: 0.47123027, g_loss: 1.10929346\n",
      "Step: [794] d_loss: 0.48494661, g_loss: 1.14695692\n",
      "Step: [795] d_loss: 0.46167096, g_loss: 1.10902679\n",
      "Step: [796] d_loss: 0.46201888, g_loss: 1.12067544\n",
      "Step: [797] d_loss: 0.47185767, g_loss: 1.13103104\n",
      "Step: [798] d_loss: 0.47268173, g_loss: 1.12943625\n",
      "Step: [799] d_loss: 0.45349264, g_loss: 1.09365809\n",
      "Step: [800] d_loss: 0.48577932, g_loss: 1.13645816\n",
      "Step: [801] d_loss: 0.45785266, g_loss: 1.13672018\n",
      "Step: [802] d_loss: 0.46582484, g_loss: 1.09676099\n",
      "Step: [803] d_loss: 0.46063232, g_loss: 1.09931433\n",
      "Step: [804] d_loss: 0.47127283, g_loss: 1.07181752\n",
      "Step: [805] d_loss: 0.47892070, g_loss: 1.09283471\n",
      "Step: [806] d_loss: 0.47747999, g_loss: 1.09330547\n",
      "Step: [807] d_loss: 0.47402918, g_loss: 1.11822236\n",
      "Step: [808] d_loss: 0.46884334, g_loss: 1.08432019\n",
      "Step: [809] d_loss: 0.47272307, g_loss: 1.07699108\n",
      "Step: [810] d_loss: 0.45702109, g_loss: 1.11586428\n",
      "Step: [811] d_loss: 0.46479183, g_loss: 1.11115098\n",
      "Step: [812] d_loss: 0.47325763, g_loss: 1.09882593\n",
      "Step: [813] d_loss: 0.47296074, g_loss: 1.09885037\n",
      "Step: [814] d_loss: 0.46750838, g_loss: 1.11912477\n",
      "Step: [815] d_loss: 0.45527822, g_loss: 1.11974061\n",
      "Step: [816] d_loss: 0.45755523, g_loss: 1.07357562\n",
      "Step: [817] d_loss: 0.46702549, g_loss: 1.12985754\n",
      "Step: [818] d_loss: 0.45684066, g_loss: 1.11543202\n",
      "Step: [819] d_loss: 0.47943121, g_loss: 1.13485920\n",
      "Step: [820] d_loss: 0.46049309, g_loss: 1.09743571\n",
      "Step: [821] d_loss: 0.46989223, g_loss: 1.11745918\n",
      "Step: [822] d_loss: 0.47227499, g_loss: 1.10910571\n",
      "Step: [823] d_loss: 0.46353760, g_loss: 1.11417425\n",
      "Step: [824] d_loss: 0.47008172, g_loss: 1.11286843\n",
      "Step: [825] d_loss: 0.46606022, g_loss: 1.10545003\n",
      "Step: [826] d_loss: 0.46445042, g_loss: 1.09222412\n",
      "Step: [827] d_loss: 0.48194891, g_loss: 1.11992788\n",
      "Step: [828] d_loss: 0.45696712, g_loss: 1.09804022\n",
      "Step: [829] d_loss: 0.47279435, g_loss: 1.13393760\n",
      "Step: [830] d_loss: 0.47091120, g_loss: 1.10868788\n",
      "Step: [831] d_loss: 0.46333909, g_loss: 1.14141941\n",
      "Step: [832] d_loss: 0.46958977, g_loss: 1.10426795\n",
      "Step: [833] d_loss: 0.45829672, g_loss: 1.10658288\n",
      "Step: [834] d_loss: 0.47194701, g_loss: 1.14544463\n",
      "Step: [835] d_loss: 0.47529626, g_loss: 1.12097311\n",
      "Step: [836] d_loss: 0.46766683, g_loss: 1.14127493\n",
      "Step: [837] d_loss: 0.46672863, g_loss: 1.11798906\n",
      "Step: [838] d_loss: 0.45955724, g_loss: 1.13368869\n",
      "Step: [839] d_loss: 0.46170178, g_loss: 1.10705042\n",
      "Step: [840] d_loss: 0.47481602, g_loss: 1.13157535\n",
      "Step: [841] d_loss: 0.46917254, g_loss: 1.10665250\n",
      "Step: [842] d_loss: 0.46306527, g_loss: 1.12809658\n",
      "Step: [843] d_loss: 0.46019879, g_loss: 1.13527179\n",
      "Step: [844] d_loss: 0.48401472, g_loss: 1.12301517\n",
      "Step: [845] d_loss: 0.46659854, g_loss: 1.12411487\n",
      "Step: [846] d_loss: 0.46858251, g_loss: 1.11678493\n",
      "Step: [847] d_loss: 0.47552937, g_loss: 1.12870896\n",
      "Step: [848] d_loss: 0.46971908, g_loss: 1.14720273\n",
      "Step: [849] d_loss: 0.45611635, g_loss: 1.13915765\n",
      "Step: [850] d_loss: 0.48269126, g_loss: 1.09531164\n",
      "Step: [851] d_loss: 0.47646382, g_loss: 1.07334459\n",
      "Step: [852] d_loss: 0.46360433, g_loss: 1.11459160\n",
      "Step: [853] d_loss: 0.46406871, g_loss: 1.12739384\n",
      "Step: [854] d_loss: 0.47103685, g_loss: 1.11387527\n",
      "Step: [855] d_loss: 0.47069743, g_loss: 1.11789942\n",
      "Step: [856] d_loss: 0.47360569, g_loss: 1.08541191\n",
      "Step: [857] d_loss: 0.46636239, g_loss: 1.11189735\n",
      "Step: [858] d_loss: 0.47172269, g_loss: 1.08417106\n",
      "Step: [859] d_loss: 0.45933646, g_loss: 1.10868573\n",
      "Step: [860] d_loss: 0.46900013, g_loss: 1.09659457\n",
      "Step: [861] d_loss: 0.46302259, g_loss: 1.13187754\n",
      "Step: [862] d_loss: 0.48649919, g_loss: 1.08026290\n",
      "Step: [863] d_loss: 0.47395754, g_loss: 1.09263670\n",
      "Step: [864] d_loss: 0.46310997, g_loss: 1.11194265\n",
      "Step: [865] d_loss: 0.48322380, g_loss: 1.11532581\n",
      "Step: [866] d_loss: 0.45163316, g_loss: 1.06961548\n",
      "Step: [867] d_loss: 0.45122698, g_loss: 1.11487818\n",
      "Step: [868] d_loss: 0.47614923, g_loss: 1.11145318\n",
      "Step: [869] d_loss: 0.48327288, g_loss: 1.08343554\n",
      "Step: [870] d_loss: 0.45809701, g_loss: 1.09332681\n",
      "Step: [871] d_loss: 0.48758191, g_loss: 1.07491899\n",
      "Step: [872] d_loss: 0.46996868, g_loss: 1.09883678\n",
      "Step: [873] d_loss: 0.46580920, g_loss: 1.11188388\n",
      "Step: [874] d_loss: 0.46987024, g_loss: 1.11342466\n",
      "Step: [875] d_loss: 0.46566382, g_loss: 1.10094833\n",
      "Step: [876] d_loss: 0.44953269, g_loss: 1.07892537\n",
      "Step: [877] d_loss: 0.45913896, g_loss: 1.09842622\n",
      "Step: [878] d_loss: 0.46365422, g_loss: 1.07860291\n",
      "Step: [879] d_loss: 0.46706033, g_loss: 1.08083177\n",
      "Step: [880] d_loss: 0.46161404, g_loss: 1.06826103\n",
      "Step: [881] d_loss: 0.45899028, g_loss: 1.11459029\n",
      "Step: [882] d_loss: 0.47065422, g_loss: 1.10054529\n",
      "Step: [883] d_loss: 0.45763987, g_loss: 1.07991326\n",
      "Step: [884] d_loss: 0.45865822, g_loss: 1.06300426\n",
      "Step: [885] d_loss: 0.45872608, g_loss: 1.11242950\n",
      "Step: [886] d_loss: 0.46819773, g_loss: 1.08972538\n",
      "Step: [887] d_loss: 0.46169412, g_loss: 1.04747915\n",
      "Step: [888] d_loss: 0.46369487, g_loss: 1.08187675\n",
      "Step: [889] d_loss: 0.46178895, g_loss: 1.09058940\n",
      "Step: [890] d_loss: 0.46515477, g_loss: 1.10225368\n",
      "Step: [891] d_loss: 0.46898192, g_loss: 1.09178889\n",
      "Step: [892] d_loss: 0.46351594, g_loss: 1.10919821\n",
      "Step: [893] d_loss: 0.45828369, g_loss: 1.10777819\n",
      "Step: [894] d_loss: 0.46891785, g_loss: 1.08907962\n",
      "Step: [895] d_loss: 0.45872587, g_loss: 1.11899519\n",
      "Step: [896] d_loss: 0.45643049, g_loss: 1.09381878\n",
      "Step: [897] d_loss: 0.45519039, g_loss: 1.08690155\n",
      "Step: [898] d_loss: 0.46308100, g_loss: 1.09732783\n",
      "Step: [899] d_loss: 0.46211523, g_loss: 1.06004345\n",
      "Step: [900] d_loss: 0.47141302, g_loss: 1.10244370\n",
      "Step: [901] d_loss: 0.47978103, g_loss: 1.10835993\n",
      "Step: [902] d_loss: 0.45636520, g_loss: 1.13653016\n",
      "Step: [903] d_loss: 0.44739139, g_loss: 1.11852634\n",
      "Step: [904] d_loss: 0.46753982, g_loss: 1.06939912\n",
      "Step: [905] d_loss: 0.45586854, g_loss: 1.09710169\n",
      "Step: [906] d_loss: 0.46457365, g_loss: 1.05856824\n",
      "Step: [907] d_loss: 0.46174294, g_loss: 1.10096943\n",
      "Step: [908] d_loss: 0.45693076, g_loss: 1.08418274\n",
      "Step: [909] d_loss: 0.46778932, g_loss: 1.06402564\n",
      "Step: [910] d_loss: 0.45656672, g_loss: 1.09709489\n",
      "Step: [911] d_loss: 0.45563850, g_loss: 1.07845962\n",
      "Step: [912] d_loss: 0.46961203, g_loss: 1.06318665\n",
      "Step: [913] d_loss: 0.45601940, g_loss: 1.08346248\n",
      "Step: [914] d_loss: 0.45711726, g_loss: 1.06425786\n",
      "Step: [915] d_loss: 0.50697404, g_loss: 1.11122549\n",
      "Step: [916] d_loss: 0.46802777, g_loss: 1.10079455\n",
      "Step: [917] d_loss: 0.45765030, g_loss: 1.05750322\n",
      "Step: [918] d_loss: 0.45451000, g_loss: 1.10333765\n",
      "Step: [919] d_loss: 0.45948693, g_loss: 1.09632218\n",
      "Step: [920] d_loss: 0.47186017, g_loss: 1.08829832\n",
      "Step: [921] d_loss: 0.47246289, g_loss: 1.07455432\n",
      "Step: [922] d_loss: 0.45991594, g_loss: 1.12771273\n",
      "Step: [923] d_loss: 0.48557979, g_loss: 1.06512952\n",
      "Step: [924] d_loss: 0.46892929, g_loss: 1.10544479\n",
      "Step: [925] d_loss: 0.75988460, g_loss: 0.83829391\n",
      "Step: [926] d_loss: 1.11799359, g_loss: 1.25866258\n",
      "Step: [927] d_loss: 0.60238731, g_loss: 1.24750113\n",
      "Step: [928] d_loss: 0.57165855, g_loss: 1.21666992\n",
      "Step: [929] d_loss: 0.58196497, g_loss: 1.20012128\n",
      "Step: [930] d_loss: 0.57907593, g_loss: 1.18537927\n",
      "Step: [931] d_loss: 0.52012879, g_loss: 1.16696084\n",
      "Step: [932] d_loss: 0.51196194, g_loss: 1.17723548\n",
      "Step: [933] d_loss: 0.49020910, g_loss: 1.15589511\n",
      "Step: [934] d_loss: 0.53589350, g_loss: 1.22670150\n",
      "Step: [935] d_loss: 0.51126039, g_loss: 1.15741611\n",
      "Step: [936] d_loss: 0.51018733, g_loss: 1.15299678\n",
      "Step: [937] d_loss: 0.48696804, g_loss: 1.11228585\n",
      "Step: [938] d_loss: 0.47744614, g_loss: 1.17098284\n",
      "Step: [939] d_loss: 0.49061155, g_loss: 1.10879505\n",
      "Step: [940] d_loss: 0.49758592, g_loss: 1.15254903\n",
      "Step: [941] d_loss: 0.48941684, g_loss: 1.14268267\n",
      "Step: [942] d_loss: 0.46904063, g_loss: 1.15096402\n",
      "Step: [943] d_loss: 0.48577324, g_loss: 1.12237084\n",
      "Step: [944] d_loss: 0.49334529, g_loss: 1.12496257\n",
      "Step: [945] d_loss: 0.51846713, g_loss: 1.10749197\n",
      "Step: [946] d_loss: 0.49930570, g_loss: 1.14674330\n",
      "Step: [947] d_loss: 0.50519437, g_loss: 1.13022339\n",
      "Step: [948] d_loss: 0.49227574, g_loss: 1.17133296\n",
      "Step: [949] d_loss: 0.49206132, g_loss: 1.12761712\n",
      "Step: [950] d_loss: 0.49424541, g_loss: 1.13092363\n",
      "Step: [951] d_loss: 0.48930877, g_loss: 1.15710199\n",
      "Step: [952] d_loss: 0.47532353, g_loss: 1.15414572\n",
      "Step: [953] d_loss: 0.49592713, g_loss: 1.17104816\n",
      "Step: [954] d_loss: 0.47792810, g_loss: 1.18037546\n",
      "Step: [955] d_loss: 0.47510025, g_loss: 1.16662395\n",
      "Step: [956] d_loss: 0.49055079, g_loss: 1.14432859\n",
      "Step: [957] d_loss: 0.50516337, g_loss: 1.18251908\n",
      "Step: [958] d_loss: 0.48792148, g_loss: 1.14651275\n",
      "Step: [959] d_loss: 0.47051555, g_loss: 1.16685069\n",
      "Step: [960] d_loss: 0.47818181, g_loss: 1.18731546\n",
      "Step: [961] d_loss: 0.46254098, g_loss: 1.17475998\n",
      "Step: [962] d_loss: 0.48263484, g_loss: 1.14157200\n",
      "Step: [963] d_loss: 0.47925004, g_loss: 1.15910649\n",
      "Step: [964] d_loss: 0.48134121, g_loss: 1.13448584\n",
      "Step: [965] d_loss: 0.49455455, g_loss: 1.13583374\n",
      "Step: [966] d_loss: 0.47795826, g_loss: 1.14737284\n",
      "Step: [967] d_loss: 0.48197493, g_loss: 1.12311172\n",
      "Step: [968] d_loss: 0.48801830, g_loss: 1.10859382\n",
      "Step: [969] d_loss: 0.47187689, g_loss: 1.12983727\n",
      "Step: [970] d_loss: 0.51251596, g_loss: 1.14456463\n",
      "Step: [971] d_loss: 0.47034609, g_loss: 1.17462194\n",
      "Step: [972] d_loss: 0.49369535, g_loss: 1.21669102\n",
      "Step: [973] d_loss: 0.47959310, g_loss: 1.14718032\n",
      "Step: [974] d_loss: 0.48463395, g_loss: 1.14737165\n",
      "Step: [975] d_loss: 0.47871578, g_loss: 1.11856472\n",
      "Step: [976] d_loss: 0.50884807, g_loss: 1.16866004\n",
      "Step: [977] d_loss: 0.47960651, g_loss: 1.17078280\n",
      "Step: [978] d_loss: 0.48082659, g_loss: 1.12818313\n",
      "Step: [979] d_loss: 0.48719683, g_loss: 1.18039882\n",
      "Step: [980] d_loss: 0.47337663, g_loss: 1.12861001\n",
      "Step: [981] d_loss: 0.47317523, g_loss: 1.13394237\n",
      "Step: [982] d_loss: 0.47761485, g_loss: 1.12437320\n",
      "Step: [983] d_loss: 0.50013125, g_loss: 1.16356146\n",
      "Step: [984] d_loss: 0.48756999, g_loss: 1.15631747\n",
      "Step: [985] d_loss: 0.46988231, g_loss: 1.13130808\n",
      "Step: [986] d_loss: 0.48006544, g_loss: 1.16321862\n",
      "Step: [987] d_loss: 0.47063619, g_loss: 1.14924240\n",
      "Step: [988] d_loss: 0.47713605, g_loss: 1.12116909\n",
      "Step: [989] d_loss: 0.47720170, g_loss: 1.14751387\n",
      "Step: [990] d_loss: 0.46415177, g_loss: 1.14984441\n",
      "Step: [991] d_loss: 0.47837442, g_loss: 1.13439023\n",
      "Step: [992] d_loss: 0.46803325, g_loss: 1.15437555\n",
      "Step: [993] d_loss: 0.46371064, g_loss: 1.14937937\n",
      "Step: [994] d_loss: 0.45579708, g_loss: 1.16046822\n",
      "Step: [995] d_loss: 0.47175878, g_loss: 1.13476861\n",
      "Step: [996] d_loss: 0.46643019, g_loss: 1.15674019\n",
      "Step: [997] d_loss: 0.46812624, g_loss: 1.11681890\n",
      "Step: [998] d_loss: 0.46502846, g_loss: 1.12122774\n",
      "Step: [999] d_loss: 0.49359256, g_loss: 1.13845193\n",
      "Step: [1000] d_loss: 0.46149951, g_loss: 1.14205110\n",
      "Step: [1001] d_loss: 0.49051434, g_loss: 1.15791988\n",
      "Step: [1002] d_loss: 0.48437670, g_loss: 1.11169076\n",
      "Step: [1003] d_loss: 0.49109906, g_loss: 1.09435582\n",
      "Step: [1004] d_loss: 0.46705759, g_loss: 1.11462438\n",
      "Step: [1005] d_loss: 0.47324425, g_loss: 1.09611380\n",
      "Step: [1006] d_loss: 0.45778829, g_loss: 1.12790298\n",
      "Step: [1007] d_loss: 0.46104342, g_loss: 1.12012243\n",
      "Step: [1008] d_loss: 0.49386498, g_loss: 1.16226792\n",
      "Step: [1009] d_loss: 0.47435942, g_loss: 1.11508572\n",
      "Step: [1010] d_loss: 0.48036265, g_loss: 1.14057243\n",
      "Step: [1011] d_loss: 0.46240133, g_loss: 1.16305542\n",
      "Step: [1012] d_loss: 0.47379434, g_loss: 1.12663496\n",
      "Step: [1013] d_loss: 0.45796692, g_loss: 1.11820590\n",
      "Step: [1014] d_loss: 0.47429895, g_loss: 1.09068573\n",
      "Step: [1015] d_loss: 0.51450831, g_loss: 1.10283315\n",
      "Step: [1016] d_loss: 0.46771768, g_loss: 1.12181306\n",
      "Step: [1017] d_loss: 0.48515597, g_loss: 1.11627984\n",
      "Step: [1018] d_loss: 0.45709842, g_loss: 1.10302031\n",
      "Step: [1019] d_loss: 0.46626422, g_loss: 1.09812856\n",
      "Step: [1020] d_loss: 0.45882156, g_loss: 1.10356092\n",
      "Step: [1021] d_loss: 0.47686413, g_loss: 1.12169886\n",
      "Step: [1022] d_loss: 0.47573459, g_loss: 1.07476223\n",
      "Step: [1023] d_loss: 0.46434921, g_loss: 1.11387801\n",
      "Step: [1024] d_loss: 0.46348804, g_loss: 1.10284591\n",
      "Step: [1025] d_loss: 0.46970469, g_loss: 1.12372518\n",
      "Step: [1026] d_loss: 0.45978427, g_loss: 1.12228119\n",
      "Step: [1027] d_loss: 0.47282711, g_loss: 1.11676824\n",
      "Step: [1028] d_loss: 0.47184762, g_loss: 1.09979439\n",
      "Step: [1029] d_loss: 0.47329128, g_loss: 1.11253953\n",
      "Step: [1030] d_loss: 0.46217388, g_loss: 1.10728765\n",
      "Step: [1031] d_loss: 0.46906707, g_loss: 1.11499119\n",
      "Step: [1032] d_loss: 0.47625285, g_loss: 1.11350012\n",
      "Step: [1033] d_loss: 0.47353885, g_loss: 1.14381111\n",
      "Step: [1034] d_loss: 0.47284710, g_loss: 1.11310828\n",
      "Step: [1035] d_loss: 0.46396077, g_loss: 1.14460039\n",
      "Step: [1036] d_loss: 0.46697757, g_loss: 1.14687717\n",
      "Step: [1037] d_loss: 0.46608829, g_loss: 1.11448562\n",
      "Step: [1038] d_loss: 0.46813783, g_loss: 1.12496257\n",
      "Step: [1039] d_loss: 0.46612722, g_loss: 1.12534451\n",
      "Step: [1040] d_loss: 0.46705291, g_loss: 1.12831688\n",
      "Step: [1041] d_loss: 0.46597287, g_loss: 1.12721574\n",
      "Step: [1042] d_loss: 0.49336436, g_loss: 1.11486578\n",
      "Step: [1043] d_loss: 0.48295057, g_loss: 1.10268676\n",
      "Step: [1044] d_loss: 0.46454638, g_loss: 1.11633790\n",
      "Step: [1045] d_loss: 0.46903124, g_loss: 1.09927976\n",
      "Step: [1046] d_loss: 0.47042671, g_loss: 1.11569834\n",
      "Step: [1047] d_loss: 0.45696923, g_loss: 1.10361075\n",
      "Step: [1048] d_loss: 0.46026218, g_loss: 1.11756146\n",
      "Step: [1049] d_loss: 0.45846552, g_loss: 1.13760185\n",
      "Step: [1050] d_loss: 0.45915142, g_loss: 1.09659791\n",
      "Step: [1051] d_loss: 0.47623035, g_loss: 1.11387491\n",
      "Step: [1052] d_loss: 0.45976046, g_loss: 1.09992182\n",
      "Step: [1053] d_loss: 0.47053909, g_loss: 1.10432184\n",
      "Step: [1054] d_loss: 0.45732915, g_loss: 1.14108109\n",
      "Step: [1055] d_loss: 0.45619950, g_loss: 1.13889861\n",
      "Step: [1056] d_loss: 0.46163228, g_loss: 1.12908685\n",
      "Step: [1057] d_loss: 0.46270066, g_loss: 1.09770882\n",
      "Step: [1058] d_loss: 0.45743227, g_loss: 1.11421359\n",
      "Step: [1059] d_loss: 0.45405674, g_loss: 1.13162470\n",
      "Step: [1060] d_loss: 0.45761585, g_loss: 1.10352695\n",
      "Step: [1061] d_loss: 0.47356328, g_loss: 1.13101614\n",
      "Step: [1062] d_loss: 0.46395817, g_loss: 1.12089396\n",
      "Step: [1063] d_loss: 0.46013370, g_loss: 1.10721600\n",
      "Step: [1064] d_loss: 0.47825217, g_loss: 1.11517203\n",
      "Step: [1065] d_loss: 0.53529584, g_loss: 1.07371831\n",
      "Step: [1066] d_loss: 0.45456469, g_loss: 1.11419559\n",
      "Step: [1067] d_loss: 0.46024367, g_loss: 1.11074531\n",
      "Step: [1068] d_loss: 0.48234728, g_loss: 1.06555593\n",
      "Step: [1069] d_loss: 0.46295172, g_loss: 1.08934271\n",
      "Step: [1070] d_loss: 0.45619580, g_loss: 1.13621712\n",
      "Step: [1071] d_loss: 0.45367506, g_loss: 1.15430522\n",
      "Step: [1072] d_loss: 0.46585992, g_loss: 1.15546143\n",
      "Step: [1073] d_loss: 0.45699206, g_loss: 1.10880244\n",
      "Step: [1074] d_loss: 0.47253111, g_loss: 1.10452485\n",
      "Step: [1075] d_loss: 0.47205952, g_loss: 1.10290039\n",
      "Step: [1076] d_loss: 0.47468704, g_loss: 1.12934709\n",
      "Step: [1077] d_loss: 0.46684951, g_loss: 1.10014069\n",
      "Step: [1078] d_loss: 0.46134162, g_loss: 1.10251856\n",
      "Step: [1079] d_loss: 0.46353790, g_loss: 1.15423572\n",
      "Step: [1080] d_loss: 0.46420988, g_loss: 1.12443459\n",
      "Step: [1081] d_loss: 0.46018815, g_loss: 1.12247515\n",
      "Step: [1082] d_loss: 0.45438057, g_loss: 1.14920497\n",
      "Step: [1083] d_loss: 0.46054739, g_loss: 1.13346243\n",
      "Step: [1084] d_loss: 0.46135312, g_loss: 1.13907278\n",
      "Step: [1085] d_loss: 0.46932206, g_loss: 1.12780643\n",
      "Step: [1086] d_loss: 0.46849808, g_loss: 1.10656786\n",
      "Step: [1087] d_loss: 0.46876496, g_loss: 1.10005915\n",
      "Step: [1088] d_loss: 0.46605337, g_loss: 1.12179530\n",
      "Step: [1089] d_loss: 0.45115593, g_loss: 1.11409354\n",
      "Step: [1090] d_loss: 0.47910190, g_loss: 1.09326673\n",
      "Step: [1091] d_loss: 0.46497858, g_loss: 1.13373184\n",
      "Step: [1092] d_loss: 0.46711773, g_loss: 1.09181499\n",
      "Step: [1093] d_loss: 0.46155864, g_loss: 1.10568821\n",
      "Step: [1094] d_loss: 0.46153277, g_loss: 1.09759009\n",
      "Step: [1095] d_loss: 0.46728221, g_loss: 1.09096944\n",
      "Step: [1096] d_loss: 0.46052682, g_loss: 1.08233035\n",
      "Step: [1097] d_loss: 0.45547396, g_loss: 1.11121142\n",
      "Step: [1098] d_loss: 0.46662077, g_loss: 1.12079406\n",
      "Step: [1099] d_loss: 0.45283142, g_loss: 1.12104034\n",
      "Step: [1100] d_loss: 0.47478464, g_loss: 1.09781754\n",
      "Step: [1101] d_loss: 0.45996100, g_loss: 1.11339402\n",
      "Step: [1102] d_loss: 0.45423275, g_loss: 1.11747873\n",
      "Step: [1103] d_loss: 0.45396259, g_loss: 1.11623979\n",
      "Step: [1104] d_loss: 0.48178044, g_loss: 1.10756969\n",
      "Step: [1105] d_loss: 0.45878053, g_loss: 1.12460649\n",
      "Step: [1106] d_loss: 0.46172643, g_loss: 1.07601309\n",
      "Step: [1107] d_loss: 0.47740221, g_loss: 1.11925268\n",
      "Step: [1108] d_loss: 0.46583492, g_loss: 1.11123466\n",
      "Step: [1109] d_loss: 0.46476755, g_loss: 1.12596631\n",
      "Step: [1110] d_loss: 0.46159494, g_loss: 1.11863279\n",
      "Step: [1111] d_loss: 0.45898920, g_loss: 1.09116769\n",
      "Step: [1112] d_loss: 0.45362046, g_loss: 1.08891058\n",
      "Step: [1113] d_loss: 0.46935487, g_loss: 1.09204650\n",
      "Step: [1114] d_loss: 0.47292727, g_loss: 1.11743546\n",
      "Step: [1115] d_loss: 0.46788958, g_loss: 1.12296140\n",
      "Step: [1116] d_loss: 0.47381964, g_loss: 1.09271002\n",
      "Step: [1117] d_loss: 0.47269112, g_loss: 1.10362053\n",
      "Step: [1118] d_loss: 0.46961471, g_loss: 1.17431927\n",
      "Step: [1119] d_loss: 0.45917213, g_loss: 1.12720621\n",
      "Step: [1120] d_loss: 0.45951310, g_loss: 1.14603150\n",
      "Step: [1121] d_loss: 0.46585950, g_loss: 1.13941824\n",
      "Step: [1122] d_loss: 0.46652651, g_loss: 1.12932682\n",
      "Step: [1123] d_loss: 0.45099768, g_loss: 1.11390507\n",
      "Step: [1124] d_loss: 0.46136799, g_loss: 1.11769187\n",
      "Step: [1125] d_loss: 0.46325541, g_loss: 1.07882512\n",
      "Step: [1126] d_loss: 0.45551196, g_loss: 1.11742568\n",
      "Step: [1127] d_loss: 0.45838892, g_loss: 1.09885681\n",
      "Step: [1128] d_loss: 0.46122995, g_loss: 1.14894009\n",
      "Step: [1129] d_loss: 0.49108976, g_loss: 1.13024390\n",
      "Step: [1130] d_loss: 0.47127262, g_loss: 1.10701251\n",
      "Step: [1131] d_loss: 0.46106803, g_loss: 1.13079143\n",
      "Step: [1132] d_loss: 0.46229130, g_loss: 1.12331188\n",
      "Step: [1133] d_loss: 0.45841366, g_loss: 1.10293734\n",
      "Step: [1134] d_loss: 0.45464408, g_loss: 1.11438298\n",
      "Step: [1135] d_loss: 0.45702183, g_loss: 1.11103451\n",
      "Step: [1136] d_loss: 0.46800020, g_loss: 1.10548472\n",
      "Step: [1137] d_loss: 0.45890018, g_loss: 1.12310970\n",
      "Step: [1138] d_loss: 0.45267355, g_loss: 1.14258599\n",
      "Step: [1139] d_loss: 0.45174009, g_loss: 1.12766004\n",
      "Step: [1140] d_loss: 0.45560506, g_loss: 1.12669933\n",
      "Step: [1141] d_loss: 0.47332346, g_loss: 1.10849845\n",
      "Step: [1142] d_loss: 0.46632379, g_loss: 1.10350680\n",
      "Step: [1143] d_loss: 0.46235311, g_loss: 1.08393133\n",
      "Step: [1144] d_loss: 0.46511546, g_loss: 1.08032584\n",
      "Step: [1145] d_loss: 0.46045306, g_loss: 1.12269318\n",
      "Step: [1146] d_loss: 0.48439550, g_loss: 1.09694850\n",
      "Step: [1147] d_loss: 0.46882826, g_loss: 1.09656775\n",
      "Step: [1148] d_loss: 0.45459649, g_loss: 1.14798212\n",
      "Step: [1149] d_loss: 0.45960543, g_loss: 1.12436891\n",
      "Step: [1150] d_loss: 0.46096018, g_loss: 1.09075534\n",
      "Step: [1151] d_loss: 0.45869324, g_loss: 1.09555304\n",
      "Step: [1152] d_loss: 0.47695822, g_loss: 1.08829200\n",
      "Step: [1153] d_loss: 0.45775440, g_loss: 1.15093386\n",
      "Step: [1154] d_loss: 0.49625608, g_loss: 1.10209394\n",
      "Step: [1155] d_loss: 0.47235736, g_loss: 1.08871078\n",
      "Step: [1156] d_loss: 0.45764634, g_loss: 1.09379494\n",
      "Step: [1157] d_loss: 0.46290436, g_loss: 1.12542617\n",
      "Step: [1158] d_loss: 0.47265953, g_loss: 1.10976219\n",
      "Step: [1159] d_loss: 0.45297176, g_loss: 1.10649335\n",
      "Step: [1160] d_loss: 0.47772765, g_loss: 1.10721231\n",
      "Step: [1161] d_loss: 0.46004131, g_loss: 1.12697005\n",
      "Step: [1162] d_loss: 0.48137322, g_loss: 1.10859179\n",
      "Step: [1163] d_loss: 0.46761140, g_loss: 1.09329796\n",
      "Step: [1164] d_loss: 0.49792555, g_loss: 1.11925232\n",
      "Step: [1165] d_loss: 0.47042170, g_loss: 1.16203797\n",
      "Step: [1166] d_loss: 0.47291103, g_loss: 1.13795733\n",
      "Step: [1167] d_loss: 0.48452103, g_loss: 1.09266186\n",
      "Step: [1168] d_loss: 0.47032869, g_loss: 1.15140855\n",
      "Step: [1169] d_loss: 0.51096940, g_loss: 1.14562714\n",
      "Step: [1170] d_loss: 0.48427019, g_loss: 1.16847503\n",
      "Step: [1171] d_loss: 0.48100531, g_loss: 1.09529400\n",
      "Step: [1172] d_loss: 0.46553344, g_loss: 1.13213468\n",
      "Step: [1173] d_loss: 0.47397798, g_loss: 1.09478235\n",
      "Step: [1174] d_loss: 0.47412458, g_loss: 1.13580859\n",
      "Step: [1175] d_loss: 0.46974722, g_loss: 1.12367105\n",
      "Step: [1176] d_loss: 0.46136487, g_loss: 1.08016849\n",
      "Step: [1177] d_loss: 0.48678970, g_loss: 1.12366223\n",
      "Step: [1178] d_loss: 0.48170376, g_loss: 1.13530171\n",
      "Step: [1179] d_loss: 0.45808968, g_loss: 1.10779870\n",
      "Step: [1180] d_loss: 0.46159968, g_loss: 1.14910543\n",
      "Step: [1181] d_loss: 0.46862331, g_loss: 1.09946036\n",
      "Step: [1182] d_loss: 0.48759866, g_loss: 1.13065732\n",
      "Step: [1183] d_loss: 0.46449086, g_loss: 1.13113165\n",
      "Step: [1184] d_loss: 0.46958181, g_loss: 1.09738684\n",
      "Step: [1185] d_loss: 0.46922916, g_loss: 1.14762688\n",
      "Step: [1186] d_loss: 0.46079481, g_loss: 1.09790635\n",
      "Step: [1187] d_loss: 0.46485627, g_loss: 1.12375021\n",
      "Step: [1188] d_loss: 0.46307629, g_loss: 1.10536468\n",
      "Step: [1189] d_loss: 0.48158836, g_loss: 1.05983531\n",
      "Step: [1190] d_loss: 0.46322095, g_loss: 1.11959922\n",
      "Step: [1191] d_loss: 0.45330685, g_loss: 1.13934660\n",
      "Step: [1192] d_loss: 0.47451293, g_loss: 1.12424827\n",
      "Step: [1193] d_loss: 0.45847341, g_loss: 1.10321331\n",
      "Step: [1194] d_loss: 0.46327105, g_loss: 1.15696490\n",
      "Step: [1195] d_loss: 0.46360296, g_loss: 1.12160254\n",
      "Step: [1196] d_loss: 0.46577170, g_loss: 1.11974907\n",
      "Step: [1197] d_loss: 0.46121070, g_loss: 1.15216339\n",
      "Step: [1198] d_loss: 0.45595923, g_loss: 1.09100437\n",
      "Step: [1199] d_loss: 0.45383084, g_loss: 1.12678409\n",
      "Step: [1200] d_loss: 0.45868558, g_loss: 1.09683418\n",
      "Step: [1201] d_loss: 0.46451005, g_loss: 1.13807487\n",
      "Step: [1202] d_loss: 0.45747143, g_loss: 1.06926668\n",
      "Step: [1203] d_loss: 0.46403241, g_loss: 1.07222593\n",
      "Step: [1204] d_loss: 0.46361977, g_loss: 1.10373878\n",
      "Step: [1205] d_loss: 0.45850682, g_loss: 1.11755168\n",
      "Step: [1206] d_loss: 0.46163288, g_loss: 1.11911058\n",
      "Step: [1207] d_loss: 0.47478211, g_loss: 1.10829484\n",
      "Step: [1208] d_loss: 0.47995114, g_loss: 1.12558889\n",
      "Step: [1209] d_loss: 0.45353779, g_loss: 1.12161040\n",
      "Step: [1210] d_loss: 0.46666449, g_loss: 1.09259212\n",
      "Step: [1211] d_loss: 0.45509663, g_loss: 1.08870316\n",
      "Step: [1212] d_loss: 0.44741321, g_loss: 1.11902988\n",
      "Step: [1213] d_loss: 0.45761600, g_loss: 1.13038111\n",
      "Step: [1214] d_loss: 0.46498263, g_loss: 1.13286579\n",
      "Step: [1215] d_loss: 0.46114153, g_loss: 1.13443470\n",
      "Step: [1216] d_loss: 0.46317917, g_loss: 1.09901059\n",
      "Step: [1217] d_loss: 0.46633258, g_loss: 1.09249830\n",
      "Step: [1218] d_loss: 0.46373111, g_loss: 1.16105390\n",
      "Step: [1219] d_loss: 0.45442313, g_loss: 1.13518953\n",
      "Step: [1220] d_loss: 0.45495424, g_loss: 1.13183475\n",
      "Step: [1221] d_loss: 0.45736137, g_loss: 1.12076640\n",
      "Step: [1222] d_loss: 0.47173217, g_loss: 1.12293267\n",
      "Step: [1223] d_loss: 0.45455518, g_loss: 1.09672117\n",
      "Step: [1224] d_loss: 0.47659662, g_loss: 1.14074743\n",
      "Step: [1225] d_loss: 0.45296347, g_loss: 1.06885576\n",
      "Step: [1226] d_loss: 0.46561936, g_loss: 1.11365402\n",
      "Step: [1227] d_loss: 0.45327497, g_loss: 1.12414205\n",
      "Step: [1228] d_loss: 0.46135008, g_loss: 1.11342466\n",
      "Step: [1229] d_loss: 0.47163469, g_loss: 1.11315858\n",
      "Step: [1230] d_loss: 0.45540684, g_loss: 1.12192893\n",
      "Step: [1231] d_loss: 0.46002793, g_loss: 1.08717561\n",
      "Step: [1232] d_loss: 0.46555117, g_loss: 1.06169224\n",
      "Step: [1233] d_loss: 0.46049652, g_loss: 1.11858606\n",
      "Step: [1234] d_loss: 0.46207702, g_loss: 1.11645889\n",
      "Step: [1235] d_loss: 0.45710543, g_loss: 1.09807515\n",
      "Step: [1236] d_loss: 0.46260768, g_loss: 1.11574829\n",
      "Step: [1237] d_loss: 0.46658733, g_loss: 1.13367796\n",
      "Step: [1238] d_loss: 0.48317677, g_loss: 1.10277045\n",
      "Step: [1239] d_loss: 0.47963965, g_loss: 1.15570104\n",
      "Step: [1240] d_loss: 0.45818752, g_loss: 1.13386130\n",
      "Step: [1241] d_loss: 0.45379782, g_loss: 1.17127669\n",
      "Step: [1242] d_loss: 0.46539298, g_loss: 1.15896153\n",
      "Step: [1243] d_loss: 0.45799315, g_loss: 1.12373912\n",
      "Step: [1244] d_loss: 0.46645457, g_loss: 1.13354063\n",
      "Step: [1245] d_loss: 0.46117762, g_loss: 1.12865353\n",
      "Step: [1246] d_loss: 0.47293556, g_loss: 1.08383048\n",
      "Step: [1247] d_loss: 0.47194421, g_loss: 1.12979281\n",
      "Step: [1248] d_loss: 0.45593405, g_loss: 1.13119221\n",
      "Step: [1249] d_loss: 0.46533999, g_loss: 1.13227403\n",
      "Step: [1250] d_loss: 0.46262780, g_loss: 1.11687267\n",
      "Step: [1251] d_loss: 0.45810062, g_loss: 1.13175404\n",
      "Step: [1252] d_loss: 0.45719784, g_loss: 1.13494158\n",
      "Step: [1253] d_loss: 0.46045005, g_loss: 1.12699068\n",
      "Step: [1254] d_loss: 0.45401749, g_loss: 1.14301229\n",
      "Step: [1255] d_loss: 0.45714816, g_loss: 1.11082602\n",
      "Step: [1256] d_loss: 0.45435867, g_loss: 1.11681843\n",
      "Step: [1257] d_loss: 0.45426697, g_loss: 1.15428805\n",
      "Step: [1258] d_loss: 0.46397632, g_loss: 1.11984372\n",
      "Step: [1259] d_loss: 0.45253029, g_loss: 1.14648688\n",
      "Step: [1260] d_loss: 0.46875629, g_loss: 1.13917017\n",
      "Step: [1261] d_loss: 0.45866972, g_loss: 1.11131656\n",
      "Step: [1262] d_loss: 0.46213427, g_loss: 1.10990238\n",
      "Step: [1263] d_loss: 0.50788647, g_loss: 1.11907887\n",
      "Step: [1264] d_loss: 0.47107103, g_loss: 1.13865077\n",
      "Step: [1265] d_loss: 0.48645872, g_loss: 1.12737143\n",
      "Step: [1266] d_loss: 0.48173404, g_loss: 1.14697647\n",
      "Step: [1267] d_loss: 0.46056116, g_loss: 1.10154486\n",
      "Step: [1268] d_loss: 0.46101767, g_loss: 1.10621703\n",
      "Step: [1269] d_loss: 0.46793240, g_loss: 1.13610971\n",
      "Step: [1270] d_loss: 0.45823315, g_loss: 1.09259534\n",
      "Step: [1271] d_loss: 0.47615620, g_loss: 1.11922204\n",
      "Step: [1272] d_loss: 0.46388355, g_loss: 1.13568437\n",
      "Step: [1273] d_loss: 0.45908618, g_loss: 1.14603257\n",
      "Step: [1274] d_loss: 0.45344317, g_loss: 1.11363876\n",
      "Step: [1275] d_loss: 0.45270061, g_loss: 1.12541747\n",
      "Step: [1276] d_loss: 0.45225936, g_loss: 1.11313260\n",
      "Step: [1277] d_loss: 0.45178488, g_loss: 1.13640773\n",
      "Step: [1278] d_loss: 0.46158773, g_loss: 1.11663806\n",
      "Step: [1279] d_loss: 0.44824594, g_loss: 1.14189899\n",
      "Step: [1280] d_loss: 0.45516849, g_loss: 1.09299350\n",
      "Step: [1281] d_loss: 0.45505419, g_loss: 1.09817541\n",
      "Step: [1282] d_loss: 0.45819351, g_loss: 1.09406710\n",
      "Step: [1283] d_loss: 0.46034652, g_loss: 1.08260798\n",
      "Step: [1284] d_loss: 0.45761693, g_loss: 1.12102008\n",
      "Step: [1285] d_loss: 0.45578650, g_loss: 1.14821696\n",
      "Step: [1286] d_loss: 0.44966790, g_loss: 1.16278565\n",
      "Step: [1287] d_loss: 0.44285402, g_loss: 1.09647787\n",
      "Step: [1288] d_loss: 0.45506465, g_loss: 1.15545678\n",
      "Step: [1289] d_loss: 0.44582281, g_loss: 1.10217762\n",
      "Step: [1290] d_loss: 0.45514828, g_loss: 1.13414061\n",
      "Step: [1291] d_loss: 0.45098719, g_loss: 1.11516535\n",
      "Step: [1292] d_loss: 0.44423196, g_loss: 1.13410783\n",
      "Step: [1293] d_loss: 0.45323616, g_loss: 1.12998295\n",
      "Step: [1294] d_loss: 0.45158076, g_loss: 1.12919784\n",
      "Step: [1295] d_loss: 0.45146042, g_loss: 1.15188777\n",
      "Step: [1296] d_loss: 0.45997050, g_loss: 1.12826622\n",
      "Step: [1297] d_loss: 0.44552362, g_loss: 1.11996531\n",
      "Step: [1298] d_loss: 0.45834357, g_loss: 1.14327192\n",
      "Step: [1299] d_loss: 0.44339347, g_loss: 1.17350960\n",
      "Step: [1300] d_loss: 0.45895445, g_loss: 1.10877407\n",
      "Step: [1301] d_loss: 0.46800810, g_loss: 1.13169026\n",
      "Step: [1302] d_loss: 0.46784598, g_loss: 1.16577029\n",
      "Step: [1303] d_loss: 0.46682575, g_loss: 1.16124475\n",
      "Step: [1304] d_loss: 0.48584542, g_loss: 1.14133644\n",
      "Step: [1305] d_loss: 0.90503407, g_loss: 0.87354738\n",
      "Step: [1306] d_loss: 0.94484472, g_loss: 1.32292986\n",
      "Step: [1307] d_loss: 0.76038003, g_loss: 1.27925718\n",
      "Step: [1308] d_loss: 0.80395991, g_loss: 1.32736254\n",
      "Step: [1309] d_loss: 0.59136170, g_loss: 1.28904617\n",
      "Step: [1310] d_loss: 0.54883736, g_loss: 1.29917312\n",
      "Step: [1311] d_loss: 0.54195148, g_loss: 1.29088163\n",
      "Step: [1312] d_loss: 0.59142017, g_loss: 1.29215467\n",
      "Step: [1313] d_loss: 0.55403376, g_loss: 1.21600473\n",
      "Step: [1314] d_loss: 0.49895829, g_loss: 1.24791551\n",
      "Step: [1315] d_loss: 0.53622884, g_loss: 1.21688473\n",
      "Step: [1316] d_loss: 0.49513355, g_loss: 1.21528232\n",
      "Step: [1317] d_loss: 0.52690059, g_loss: 1.21950078\n",
      "Step: [1318] d_loss: 0.52857715, g_loss: 1.23370802\n",
      "Step: [1319] d_loss: 0.50439352, g_loss: 1.20306158\n",
      "Step: [1320] d_loss: 0.51059937, g_loss: 1.22731149\n",
      "Step: [1321] d_loss: 0.50923085, g_loss: 1.14422476\n",
      "Step: [1322] d_loss: 0.57909864, g_loss: 1.19477093\n",
      "Step: [1323] d_loss: 0.51151347, g_loss: 1.19127083\n",
      "Step: [1324] d_loss: 0.54892814, g_loss: 1.16336131\n",
      "Step: [1325] d_loss: 0.50586939, g_loss: 1.16431999\n",
      "Step: [1326] d_loss: 0.50071520, g_loss: 1.12148154\n",
      "Step: [1327] d_loss: 0.49350223, g_loss: 1.16772962\n",
      "Step: [1328] d_loss: 0.47735232, g_loss: 1.13932455\n",
      "Step: [1329] d_loss: 0.47567078, g_loss: 1.13028359\n",
      "Step: [1330] d_loss: 0.49014926, g_loss: 1.14463329\n",
      "Step: [1331] d_loss: 0.47579598, g_loss: 1.10680234\n",
      "Step: [1332] d_loss: 0.47221765, g_loss: 1.14735878\n",
      "Step: [1333] d_loss: 0.48754606, g_loss: 1.16072679\n",
      "Step: [1334] d_loss: 0.51244879, g_loss: 1.11681402\n",
      "Step: [1335] d_loss: 0.49190950, g_loss: 1.10328817\n",
      "Step: [1336] d_loss: 0.49210656, g_loss: 1.13588130\n",
      "Step: [1337] d_loss: 0.49032390, g_loss: 1.11388743\n",
      "Step: [1338] d_loss: 0.47833499, g_loss: 1.10047054\n",
      "Step: [1339] d_loss: 0.56688499, g_loss: 1.12898397\n",
      "Step: [1340] d_loss: 0.50149834, g_loss: 1.17558241\n",
      "Step: [1341] d_loss: 0.48280948, g_loss: 1.14878476\n",
      "Step: [1342] d_loss: 0.50775689, g_loss: 1.14720058\n",
      "Step: [1343] d_loss: 0.47242841, g_loss: 1.11053920\n",
      "Step: [1344] d_loss: 0.46286130, g_loss: 1.12701631\n",
      "Step: [1345] d_loss: 0.48037034, g_loss: 1.11925280\n",
      "Step: [1346] d_loss: 0.47939003, g_loss: 1.07871556\n",
      "Step: [1347] d_loss: 0.46768177, g_loss: 1.10457242\n",
      "Step: [1348] d_loss: 0.48230880, g_loss: 1.14012146\n",
      "Step: [1349] d_loss: 0.49189377, g_loss: 1.11524451\n",
      "Step: [1350] d_loss: 0.53856808, g_loss: 1.11067033\n",
      "Step: [1351] d_loss: 0.47030056, g_loss: 1.14556634\n",
      "Step: [1352] d_loss: 0.46684849, g_loss: 1.15206218\n",
      "Step: [1353] d_loss: 0.48702320, g_loss: 1.12797892\n",
      "Step: [1354] d_loss: 0.48404533, g_loss: 1.13886654\n",
      "Step: [1355] d_loss: 0.47481886, g_loss: 1.10677135\n",
      "Step: [1356] d_loss: 0.47102121, g_loss: 1.11837769\n",
      "Step: [1357] d_loss: 0.48059896, g_loss: 1.10928655\n",
      "Step: [1358] d_loss: 0.49014044, g_loss: 1.11379886\n",
      "Step: [1359] d_loss: 0.48639575, g_loss: 1.13442397\n",
      "Step: [1360] d_loss: 0.47703809, g_loss: 1.12802434\n",
      "Step: [1361] d_loss: 0.48056018, g_loss: 1.10142744\n",
      "Step: [1362] d_loss: 0.47347271, g_loss: 1.11437941\n",
      "Step: [1363] d_loss: 0.47588912, g_loss: 1.09140706\n",
      "Step: [1364] d_loss: 0.47906882, g_loss: 1.11719060\n",
      "Step: [1365] d_loss: 0.47496647, g_loss: 1.11508024\n",
      "Step: [1366] d_loss: 0.47546065, g_loss: 1.09198511\n",
      "Step: [1367] d_loss: 0.48841500, g_loss: 1.09208310\n",
      "Step: [1368] d_loss: 0.47066098, g_loss: 1.10570824\n",
      "Step: [1369] d_loss: 0.48136687, g_loss: 1.09742105\n",
      "Step: [1370] d_loss: 0.46185499, g_loss: 1.11442232\n",
      "Step: [1371] d_loss: 0.47723466, g_loss: 1.13084078\n",
      "Step: [1372] d_loss: 0.46916464, g_loss: 1.09306562\n",
      "Step: [1373] d_loss: 0.47481331, g_loss: 1.10838258\n",
      "Step: [1374] d_loss: 0.46028444, g_loss: 1.10985076\n",
      "Step: [1375] d_loss: 0.51575035, g_loss: 1.10562503\n",
      "Step: [1376] d_loss: 0.48690042, g_loss: 1.13381279\n",
      "Step: [1377] d_loss: 0.47722355, g_loss: 1.12611532\n",
      "Step: [1378] d_loss: 0.47067976, g_loss: 1.12631226\n",
      "Step: [1379] d_loss: 0.48585898, g_loss: 1.12803948\n",
      "Step: [1380] d_loss: 0.48325366, g_loss: 1.10792315\n",
      "Step: [1381] d_loss: 0.47453219, g_loss: 1.10045183\n",
      "Step: [1382] d_loss: 0.50104386, g_loss: 1.12842214\n",
      "Step: [1383] d_loss: 0.47214311, g_loss: 1.14589632\n",
      "Step: [1384] d_loss: 0.48099378, g_loss: 1.08683240\n",
      "Step: [1385] d_loss: 0.46667033, g_loss: 1.14579546\n",
      "Step: [1386] d_loss: 0.46869758, g_loss: 1.12584138\n",
      "Step: [1387] d_loss: 0.47811142, g_loss: 1.10356069\n",
      "Step: [1388] d_loss: 0.48360384, g_loss: 1.11233652\n",
      "Step: [1389] d_loss: 0.51422125, g_loss: 1.11886752\n",
      "Step: [1390] d_loss: 0.47910309, g_loss: 1.10565853\n",
      "Step: [1391] d_loss: 0.47409618, g_loss: 1.11532378\n",
      "Step: [1392] d_loss: 0.47567946, g_loss: 1.13623011\n",
      "Step: [1393] d_loss: 0.46856648, g_loss: 1.09538043\n",
      "Step: [1394] d_loss: 0.47515982, g_loss: 1.14083838\n",
      "Step: [1395] d_loss: 0.46908429, g_loss: 1.13027656\n",
      "Step: [1396] d_loss: 0.46790805, g_loss: 1.10644400\n",
      "Step: [1397] d_loss: 0.48334149, g_loss: 1.14365017\n",
      "Step: [1398] d_loss: 0.46933314, g_loss: 1.14298987\n",
      "Step: [1399] d_loss: 0.47136790, g_loss: 1.13027167\n",
      "Step: [1400] d_loss: 0.45852077, g_loss: 1.08672273\n",
      "Step: [1401] d_loss: 0.47716972, g_loss: 1.16103363\n",
      "Step: [1402] d_loss: 0.46155357, g_loss: 1.16408932\n",
      "Step: [1403] d_loss: 0.46496665, g_loss: 1.13762295\n",
      "Step: [1404] d_loss: 0.47125489, g_loss: 1.12627542\n",
      "Step: [1405] d_loss: 0.47146279, g_loss: 1.15541577\n",
      "Step: [1406] d_loss: 0.48015264, g_loss: 1.16590106\n",
      "Step: [1407] d_loss: 0.47189689, g_loss: 1.15019250\n",
      "Step: [1408] d_loss: 0.47420529, g_loss: 1.13834071\n",
      "Step: [1409] d_loss: 0.46664745, g_loss: 1.14300096\n",
      "Step: [1410] d_loss: 0.45354712, g_loss: 1.10502934\n",
      "Step: [1411] d_loss: 0.49510849, g_loss: 1.10337770\n",
      "Step: [1412] d_loss: 0.48526677, g_loss: 1.13944817\n",
      "Step: [1413] d_loss: 0.46894318, g_loss: 1.13632858\n",
      "Step: [1414] d_loss: 0.47741362, g_loss: 1.17287374\n",
      "Step: [1415] d_loss: 0.47063535, g_loss: 1.14719880\n",
      "Step: [1416] d_loss: 0.45626473, g_loss: 1.14260185\n",
      "Step: [1417] d_loss: 0.46978772, g_loss: 1.15431142\n",
      "Step: [1418] d_loss: 0.45952782, g_loss: 1.11390615\n",
      "Step: [1419] d_loss: 0.47454035, g_loss: 1.13509917\n",
      "Step: [1420] d_loss: 0.46195731, g_loss: 1.14774644\n",
      "Step: [1421] d_loss: 0.46648201, g_loss: 1.14674389\n",
      "Step: [1422] d_loss: 0.47200638, g_loss: 1.13985300\n",
      "Step: [1423] d_loss: 0.46875042, g_loss: 1.13621008\n",
      "Step: [1424] d_loss: 0.48937988, g_loss: 1.13298273\n",
      "Step: [1425] d_loss: 0.46241525, g_loss: 1.14071250\n",
      "Step: [1426] d_loss: 0.46010607, g_loss: 1.12433624\n",
      "Step: [1427] d_loss: 0.46620843, g_loss: 1.11922085\n",
      "Step: [1428] d_loss: 0.47800407, g_loss: 1.14358711\n",
      "Step: [1429] d_loss: 0.46720320, g_loss: 1.10747182\n",
      "Step: [1430] d_loss: 0.48270407, g_loss: 1.08527720\n",
      "Step: [1431] d_loss: 0.47920477, g_loss: 1.10494328\n",
      "Step: [1432] d_loss: 0.46597281, g_loss: 1.11218488\n",
      "Step: [1433] d_loss: 0.47329515, g_loss: 1.11397231\n",
      "Step: [1434] d_loss: 0.49966547, g_loss: 1.09653819\n",
      "Step: [1435] d_loss: 0.46741372, g_loss: 1.12481415\n",
      "Step: [1436] d_loss: 0.46663970, g_loss: 1.13867617\n",
      "Step: [1437] d_loss: 0.47891536, g_loss: 1.11718810\n",
      "Step: [1438] d_loss: 0.48266053, g_loss: 1.11902916\n",
      "Step: [1439] d_loss: 0.46310323, g_loss: 1.11795998\n",
      "Step: [1440] d_loss: 0.46790215, g_loss: 1.12371886\n",
      "Step: [1441] d_loss: 0.47675478, g_loss: 1.12897766\n",
      "Step: [1442] d_loss: 0.46571097, g_loss: 1.11207807\n",
      "Step: [1443] d_loss: 0.46574718, g_loss: 1.10394335\n",
      "Step: [1444] d_loss: 0.45740917, g_loss: 1.11463654\n",
      "Step: [1445] d_loss: 0.45326465, g_loss: 1.13119745\n",
      "Step: [1446] d_loss: 0.46525383, g_loss: 1.12921321\n",
      "Step: [1447] d_loss: 0.46621394, g_loss: 1.12656629\n",
      "Step: [1448] d_loss: 0.46839505, g_loss: 1.17988670\n",
      "Step: [1449] d_loss: 0.47229272, g_loss: 1.11659884\n",
      "Step: [1450] d_loss: 0.45384884, g_loss: 1.12476063\n",
      "Step: [1451] d_loss: 0.48151055, g_loss: 1.11561120\n",
      "Step: [1452] d_loss: 0.46649453, g_loss: 1.15061378\n",
      "Step: [1453] d_loss: 0.45638809, g_loss: 1.14367962\n",
      "Step: [1454] d_loss: 0.46194687, g_loss: 1.11997283\n",
      "Step: [1455] d_loss: 0.46274385, g_loss: 1.12997234\n",
      "Step: [1456] d_loss: 0.46486753, g_loss: 1.10319042\n",
      "Step: [1457] d_loss: 0.47045368, g_loss: 1.10232103\n",
      "Step: [1458] d_loss: 0.49639025, g_loss: 1.13377583\n",
      "Step: [1459] d_loss: 0.48023018, g_loss: 1.11095405\n",
      "Step: [1460] d_loss: 0.46234134, g_loss: 1.08774042\n",
      "Step: [1461] d_loss: 0.46231467, g_loss: 1.12598121\n",
      "Step: [1462] d_loss: 0.45826954, g_loss: 1.14589465\n",
      "Step: [1463] d_loss: 0.46292159, g_loss: 1.12440336\n",
      "Step: [1464] d_loss: 0.45590112, g_loss: 1.10332131\n",
      "Step: [1465] d_loss: 0.46573466, g_loss: 1.11640394\n",
      "Step: [1466] d_loss: 0.46689609, g_loss: 1.11524594\n",
      "Step: [1467] d_loss: 0.47283903, g_loss: 1.10926712\n",
      "Step: [1468] d_loss: 0.49482465, g_loss: 1.11291754\n",
      "Step: [1469] d_loss: 0.46117008, g_loss: 1.11062872\n",
      "Step: [1470] d_loss: 0.47514385, g_loss: 1.14416146\n",
      "Step: [1471] d_loss: 0.45675364, g_loss: 1.12075067\n",
      "Step: [1472] d_loss: 0.46862370, g_loss: 1.12462270\n",
      "Step: [1473] d_loss: 0.48322853, g_loss: 1.09007072\n",
      "Step: [1474] d_loss: 0.47353840, g_loss: 1.11144483\n",
      "Step: [1475] d_loss: 0.48072970, g_loss: 1.14317060\n",
      "Step: [1476] d_loss: 0.47674116, g_loss: 1.12718379\n",
      "Step: [1477] d_loss: 0.46414429, g_loss: 1.15934515\n",
      "Step: [1478] d_loss: 0.47474712, g_loss: 1.10422313\n",
      "Step: [1479] d_loss: 0.45529860, g_loss: 1.09396982\n",
      "Step: [1480] d_loss: 0.46381557, g_loss: 1.09779251\n",
      "Step: [1481] d_loss: 0.45424190, g_loss: 1.11562109\n",
      "Step: [1482] d_loss: 0.47518909, g_loss: 1.14305770\n",
      "Step: [1483] d_loss: 0.46305430, g_loss: 1.09384167\n",
      "Step: [1484] d_loss: 0.46119851, g_loss: 1.17676175\n",
      "Step: [1485] d_loss: 0.44831079, g_loss: 1.14439356\n",
      "Step: [1486] d_loss: 0.45630455, g_loss: 1.12892258\n",
      "Step: [1487] d_loss: 0.45540377, g_loss: 1.10795701\n",
      "Step: [1488] d_loss: 0.46609330, g_loss: 1.10552716\n",
      "Step: [1489] d_loss: 0.45577723, g_loss: 1.08982742\n",
      "Step: [1490] d_loss: 0.46828800, g_loss: 1.12880576\n",
      "Step: [1491] d_loss: 0.46096873, g_loss: 1.07310772\n",
      "Step: [1492] d_loss: 0.45156634, g_loss: 1.10685503\n",
      "Step: [1493] d_loss: 0.45118356, g_loss: 1.10487866\n",
      "Step: [1494] d_loss: 0.46566528, g_loss: 1.11509776\n",
      "Step: [1495] d_loss: 0.45702264, g_loss: 1.08758092\n",
      "Step: [1496] d_loss: 0.45386112, g_loss: 1.11139572\n",
      "Step: [1497] d_loss: 0.45834506, g_loss: 1.10870206\n",
      "Step: [1498] d_loss: 0.48339650, g_loss: 1.12626398\n",
      "Step: [1499] d_loss: 0.46945673, g_loss: 1.09602606\n",
      "Step: [1500] d_loss: 0.46779040, g_loss: 1.13644469\n",
      "Step: [1501] d_loss: 0.45640007, g_loss: 1.11549056\n",
      "Step: [1502] d_loss: 0.45200282, g_loss: 1.11138642\n",
      "Step: [1503] d_loss: 0.46500915, g_loss: 1.09834611\n",
      "Step: [1504] d_loss: 0.47588935, g_loss: 1.12150156\n",
      "Step: [1505] d_loss: 0.46533096, g_loss: 1.12455213\n",
      "Step: [1506] d_loss: 0.48109248, g_loss: 1.08610272\n",
      "Step: [1507] d_loss: 0.46307275, g_loss: 1.12112415\n",
      "Step: [1508] d_loss: 0.45986336, g_loss: 1.11802423\n",
      "Step: [1509] d_loss: 0.44762230, g_loss: 1.09804416\n",
      "Step: [1510] d_loss: 0.45783901, g_loss: 1.14082301\n",
      "Step: [1511] d_loss: 0.47210979, g_loss: 1.10807812\n",
      "Step: [1512] d_loss: 0.45991728, g_loss: 1.11274123\n",
      "Step: [1513] d_loss: 0.47555950, g_loss: 1.11905801\n",
      "Step: [1514] d_loss: 0.46158829, g_loss: 1.09088278\n",
      "Step: [1515] d_loss: 0.45841068, g_loss: 1.11205471\n",
      "Step: [1516] d_loss: 0.47015819, g_loss: 1.12146819\n",
      "Step: [1517] d_loss: 0.45134932, g_loss: 1.13056016\n",
      "Step: [1518] d_loss: 0.47786671, g_loss: 1.14669263\n",
      "Step: [1519] d_loss: 0.45286894, g_loss: 1.11092603\n",
      "Step: [1520] d_loss: 0.45323968, g_loss: 1.08954954\n",
      "Step: [1521] d_loss: 0.45874789, g_loss: 1.11955440\n",
      "Step: [1522] d_loss: 0.45471388, g_loss: 1.11865258\n",
      "Step: [1523] d_loss: 0.46523243, g_loss: 1.08868825\n",
      "Step: [1524] d_loss: 0.46099544, g_loss: 1.10789490\n",
      "Step: [1525] d_loss: 0.45536292, g_loss: 1.11725652\n",
      "Step: [1526] d_loss: 0.45298079, g_loss: 1.06852221\n",
      "Step: [1527] d_loss: 0.47633195, g_loss: 1.09902668\n",
      "Step: [1528] d_loss: 0.46755034, g_loss: 1.09627831\n",
      "Step: [1529] d_loss: 0.45938048, g_loss: 1.09295690\n",
      "Step: [1530] d_loss: 0.46157146, g_loss: 1.12969565\n",
      "Step: [1531] d_loss: 0.47120208, g_loss: 1.10581875\n",
      "Step: [1532] d_loss: 0.47673020, g_loss: 1.10222721\n",
      "Step: [1533] d_loss: 0.45575213, g_loss: 1.10036135\n",
      "Step: [1534] d_loss: 0.46039474, g_loss: 1.09863853\n",
      "Step: [1535] d_loss: 0.47771063, g_loss: 1.12168324\n",
      "Step: [1536] d_loss: 0.48370877, g_loss: 1.10582197\n",
      "Step: [1537] d_loss: 0.46793899, g_loss: 1.11047637\n",
      "Step: [1538] d_loss: 0.45911160, g_loss: 1.12940049\n",
      "Step: [1539] d_loss: 0.47348261, g_loss: 1.11540961\n",
      "Step: [1540] d_loss: 0.45012376, g_loss: 1.10585117\n",
      "Step: [1541] d_loss: 0.46648392, g_loss: 1.12166572\n",
      "Step: [1542] d_loss: 0.46169344, g_loss: 1.12275493\n",
      "Step: [1543] d_loss: 0.46946320, g_loss: 1.12954509\n",
      "Step: [1544] d_loss: 0.45821780, g_loss: 1.09862828\n",
      "Step: [1545] d_loss: 0.46167994, g_loss: 1.07229292\n",
      "Step: [1546] d_loss: 0.46180075, g_loss: 1.09968412\n",
      "Step: [1547] d_loss: 0.47821754, g_loss: 1.08757210\n",
      "Step: [1548] d_loss: 0.45094815, g_loss: 1.09677374\n",
      "Step: [1549] d_loss: 0.45621011, g_loss: 1.10865152\n",
      "Step: [1550] d_loss: 0.46199739, g_loss: 1.09912133\n",
      "Step: [1551] d_loss: 0.46178001, g_loss: 1.10761130\n",
      "Step: [1552] d_loss: 0.47475058, g_loss: 1.07714355\n",
      "Step: [1553] d_loss: 0.47521269, g_loss: 1.12101007\n",
      "Step: [1554] d_loss: 0.46715581, g_loss: 1.09119236\n",
      "Step: [1555] d_loss: 0.46914721, g_loss: 1.06890273\n",
      "Step: [1556] d_loss: 0.46383971, g_loss: 1.10852122\n",
      "Step: [1557] d_loss: 0.45745540, g_loss: 1.11706257\n",
      "Step: [1558] d_loss: 0.45429760, g_loss: 1.09544683\n",
      "Step: [1559] d_loss: 0.45930722, g_loss: 1.10133719\n",
      "Step: [1560] d_loss: 0.46394509, g_loss: 1.13476396\n",
      "Step: [1561] d_loss: 0.46901685, g_loss: 1.10220075\n",
      "Step: [1562] d_loss: 0.47083300, g_loss: 1.12058830\n",
      "Step: [1563] d_loss: 0.48947105, g_loss: 1.09748101\n",
      "Step: [1564] d_loss: 0.46174949, g_loss: 1.10818875\n",
      "Step: [1565] d_loss: 0.45656341, g_loss: 1.09930122\n",
      "Step: [1566] d_loss: 0.45651150, g_loss: 1.07891011\n",
      "Step: [1567] d_loss: 0.46088603, g_loss: 1.12048411\n",
      "Step: [1568] d_loss: 0.46388823, g_loss: 1.09757459\n",
      "Step: [1569] d_loss: 0.46662456, g_loss: 1.10621965\n",
      "Step: [1570] d_loss: 0.46032944, g_loss: 1.10381162\n",
      "Step: [1571] d_loss: 0.44995713, g_loss: 1.12365627\n",
      "Step: [1572] d_loss: 0.47446093, g_loss: 1.08295631\n",
      "Step: [1573] d_loss: 0.45370856, g_loss: 1.09305823\n",
      "Step: [1574] d_loss: 0.46136832, g_loss: 1.11450469\n",
      "Step: [1575] d_loss: 0.46104994, g_loss: 1.07284868\n",
      "Step: [1576] d_loss: 0.46361172, g_loss: 1.13203084\n",
      "Step: [1577] d_loss: 0.46017322, g_loss: 1.12370169\n",
      "Step: [1578] d_loss: 0.47367969, g_loss: 1.09877741\n",
      "Step: [1579] d_loss: 0.45870638, g_loss: 1.11174595\n",
      "Step: [1580] d_loss: 0.45164135, g_loss: 1.06540501\n",
      "Step: [1581] d_loss: 0.45526990, g_loss: 1.10281634\n",
      "Step: [1582] d_loss: 0.45395988, g_loss: 1.09393620\n",
      "Step: [1583] d_loss: 0.45163894, g_loss: 1.09289277\n",
      "Step: [1584] d_loss: 0.45123270, g_loss: 1.08465004\n",
      "Step: [1585] d_loss: 0.45883834, g_loss: 1.07210839\n",
      "Step: [1586] d_loss: 0.45463365, g_loss: 1.09417105\n",
      "Step: [1587] d_loss: 0.45201719, g_loss: 1.10524356\n",
      "Step: [1588] d_loss: 0.44812763, g_loss: 1.09430051\n",
      "Step: [1589] d_loss: 0.45434114, g_loss: 1.13021696\n",
      "Step: [1590] d_loss: 0.45563060, g_loss: 1.09906685\n",
      "Step: [1591] d_loss: 0.44663486, g_loss: 1.08828533\n",
      "Step: [1592] d_loss: 0.46423203, g_loss: 1.10370648\n",
      "Step: [1593] d_loss: 0.46140584, g_loss: 1.07343507\n",
      "Step: [1594] d_loss: 0.45750701, g_loss: 1.11910260\n",
      "Step: [1595] d_loss: 0.45763949, g_loss: 1.10682940\n",
      "Step: [1596] d_loss: 0.45194691, g_loss: 1.10866344\n",
      "Step: [1597] d_loss: 0.45865780, g_loss: 1.12786615\n",
      "Step: [1598] d_loss: 0.45906037, g_loss: 1.08592796\n",
      "Step: [1599] d_loss: 0.45835021, g_loss: 1.12269926\n",
      "Step: [1600] d_loss: 0.45835841, g_loss: 1.11016548\n",
      "Step: [1601] d_loss: 0.45185766, g_loss: 1.11179686\n",
      "Step: [1602] d_loss: 0.45405218, g_loss: 1.09444642\n",
      "Step: [1603] d_loss: 0.45546341, g_loss: 1.14456952\n",
      "Step: [1604] d_loss: 0.46018136, g_loss: 1.10245526\n",
      "Step: [1605] d_loss: 0.47573429, g_loss: 1.14191782\n",
      "Step: [1606] d_loss: 0.45990184, g_loss: 1.14521742\n",
      "Step: [1607] d_loss: 0.45317426, g_loss: 1.16290295\n",
      "Step: [1608] d_loss: 0.45875534, g_loss: 1.13439548\n",
      "Step: [1609] d_loss: 0.44979522, g_loss: 1.10177314\n",
      "Step: [1610] d_loss: 0.45714739, g_loss: 1.10598278\n",
      "Step: [1611] d_loss: 0.45607090, g_loss: 1.13606727\n",
      "Step: [1612] d_loss: 0.49863526, g_loss: 1.13076556\n",
      "Step: [1613] d_loss: 0.46560284, g_loss: 1.09709191\n",
      "Step: [1614] d_loss: 0.47955218, g_loss: 1.07400310\n",
      "Step: [1615] d_loss: 0.45211536, g_loss: 1.08563006\n",
      "Step: [1616] d_loss: 0.46040866, g_loss: 1.07721853\n",
      "Step: [1617] d_loss: 0.45127565, g_loss: 1.09302008\n",
      "Step: [1618] d_loss: 0.46845284, g_loss: 1.08371317\n",
      "Step: [1619] d_loss: 0.46568534, g_loss: 1.10813141\n",
      "Step: [1620] d_loss: 0.46410236, g_loss: 1.09242821\n",
      "Step: [1621] d_loss: 0.46170574, g_loss: 1.10322893\n",
      "Step: [1622] d_loss: 0.47689626, g_loss: 1.13624847\n",
      "Step: [1623] d_loss: 0.45984426, g_loss: 1.12644577\n",
      "Step: [1624] d_loss: 0.46369517, g_loss: 1.07619905\n",
      "Step: [1625] d_loss: 0.45261896, g_loss: 1.11090326\n",
      "Step: [1626] d_loss: 0.46264613, g_loss: 1.07971227\n",
      "Step: [1627] d_loss: 0.45530328, g_loss: 1.09544861\n",
      "Step: [1628] d_loss: 0.45921245, g_loss: 1.11483598\n",
      "Step: [1629] d_loss: 0.45750540, g_loss: 1.08431447\n",
      "Step: [1630] d_loss: 0.44456506, g_loss: 1.10037410\n",
      "Step: [1631] d_loss: 0.46621180, g_loss: 1.12612379\n",
      "Step: [1632] d_loss: 0.46516827, g_loss: 1.12051868\n",
      "Step: [1633] d_loss: 0.47539312, g_loss: 1.07411790\n",
      "Step: [1634] d_loss: 0.45972949, g_loss: 1.15328598\n",
      "Step: [1635] d_loss: 0.45538580, g_loss: 1.12761986\n",
      "Step: [1636] d_loss: 0.45686999, g_loss: 1.08703279\n",
      "Step: [1637] d_loss: 0.45578492, g_loss: 1.14886904\n",
      "Step: [1638] d_loss: 0.46684754, g_loss: 1.14468706\n",
      "Step: [1639] d_loss: 0.45356214, g_loss: 1.09268248\n",
      "Step: [1640] d_loss: 0.45760170, g_loss: 1.06997025\n",
      "Step: [1641] d_loss: 0.45828578, g_loss: 1.11558604\n",
      "Step: [1642] d_loss: 0.45313531, g_loss: 1.07750928\n",
      "Step: [1643] d_loss: 0.45914799, g_loss: 1.10260558\n",
      "Step: [1644] d_loss: 0.46340320, g_loss: 1.08556032\n",
      "Step: [1645] d_loss: 0.47006238, g_loss: 1.10463238\n",
      "Step: [1646] d_loss: 0.44901505, g_loss: 1.12485325\n",
      "Step: [1647] d_loss: 0.46336964, g_loss: 1.09064031\n",
      "Step: [1648] d_loss: 0.46730429, g_loss: 1.10847950\n",
      "Step: [1649] d_loss: 0.48032767, g_loss: 1.07586503\n",
      "Step: [1650] d_loss: 0.47640187, g_loss: 1.09788835\n",
      "Step: [1651] d_loss: 0.45699692, g_loss: 1.04075348\n",
      "Step: [1652] d_loss: 0.45548141, g_loss: 1.09174657\n",
      "Step: [1653] d_loss: 0.45091239, g_loss: 1.10746503\n",
      "Step: [1654] d_loss: 0.45567295, g_loss: 1.10448372\n",
      "Step: [1655] d_loss: 0.44929689, g_loss: 1.08812642\n",
      "Step: [1656] d_loss: 0.46588379, g_loss: 1.10378718\n",
      "Step: [1657] d_loss: 0.44767329, g_loss: 1.12202311\n",
      "Step: [1658] d_loss: 0.44861603, g_loss: 1.07743788\n",
      "Step: [1659] d_loss: 0.45082316, g_loss: 1.06257010\n",
      "Step: [1660] d_loss: 0.45324519, g_loss: 1.10239565\n",
      "Step: [1661] d_loss: 0.44437733, g_loss: 1.11280298\n",
      "Step: [1662] d_loss: 0.44770336, g_loss: 1.11889315\n",
      "Step: [1663] d_loss: 0.45281062, g_loss: 1.09980369\n",
      "Step: [1664] d_loss: 0.61353576, g_loss: 0.96264833\n",
      "Step: [1665] d_loss: 0.71635693, g_loss: 1.24137354\n",
      "Step: [1666] d_loss: 0.64435196, g_loss: 1.22529864\n",
      "Step: [1667] d_loss: 0.60389310, g_loss: 1.24959195\n",
      "Step: [1668] d_loss: 0.56102014, g_loss: 1.23334539\n",
      "Step: [1669] d_loss: 0.52890259, g_loss: 1.17990458\n",
      "Step: [1670] d_loss: 0.50853997, g_loss: 1.20168209\n",
      "Step: [1671] d_loss: 0.48653650, g_loss: 1.14417827\n",
      "Step: [1672] d_loss: 0.48664060, g_loss: 1.18115747\n",
      "Step: [1673] d_loss: 0.50313151, g_loss: 1.17970884\n",
      "Step: [1674] d_loss: 0.47928333, g_loss: 1.16055524\n",
      "Step: [1675] d_loss: 0.46943840, g_loss: 1.12467420\n",
      "Step: [1676] d_loss: 0.47155944, g_loss: 1.20178759\n",
      "Step: [1677] d_loss: 0.48102406, g_loss: 1.15571880\n",
      "Step: [1678] d_loss: 0.47623685, g_loss: 1.13285458\n",
      "Step: [1679] d_loss: 0.49896008, g_loss: 1.12244356\n",
      "Step: [1680] d_loss: 0.47698465, g_loss: 1.19120741\n",
      "Step: [1681] d_loss: 0.47191185, g_loss: 1.18755162\n",
      "Step: [1682] d_loss: 0.47850662, g_loss: 1.13574672\n",
      "Step: [1683] d_loss: 0.47522211, g_loss: 1.16871107\n",
      "Step: [1684] d_loss: 0.50309193, g_loss: 1.15794086\n",
      "Step: [1685] d_loss: 0.46596333, g_loss: 1.08572400\n",
      "Step: [1686] d_loss: 0.47048527, g_loss: 1.16415679\n",
      "Step: [1687] d_loss: 0.45341775, g_loss: 1.11910605\n",
      "Step: [1688] d_loss: 0.46102384, g_loss: 1.18211114\n",
      "Step: [1689] d_loss: 0.47613138, g_loss: 1.13752031\n",
      "Step: [1690] d_loss: 0.48632252, g_loss: 1.15814555\n",
      "Step: [1691] d_loss: 0.48128501, g_loss: 1.13982117\n",
      "Step: [1692] d_loss: 0.46697578, g_loss: 1.16355705\n",
      "Step: [1693] d_loss: 0.47052470, g_loss: 1.17799807\n",
      "Step: [1694] d_loss: 0.46336555, g_loss: 1.15019155\n",
      "Step: [1695] d_loss: 0.50309646, g_loss: 1.16737354\n",
      "Step: [1696] d_loss: 0.47099617, g_loss: 1.16796911\n",
      "Step: [1697] d_loss: 0.49047506, g_loss: 1.12591374\n",
      "Step: [1698] d_loss: 0.47111833, g_loss: 1.17108989\n",
      "Step: [1699] d_loss: 0.46106619, g_loss: 1.12789202\n",
      "Step: [1700] d_loss: 0.46544862, g_loss: 1.15159047\n",
      "Step: [1701] d_loss: 0.46017611, g_loss: 1.15811813\n",
      "Step: [1702] d_loss: 0.45786995, g_loss: 1.15335429\n",
      "Step: [1703] d_loss: 0.46219131, g_loss: 1.09854507\n",
      "Step: [1704] d_loss: 0.48105511, g_loss: 1.16855669\n",
      "Step: [1705] d_loss: 0.46633884, g_loss: 1.15822649\n",
      "Step: [1706] d_loss: 0.46250910, g_loss: 1.13832855\n",
      "Step: [1707] d_loss: 0.46620756, g_loss: 1.15734053\n",
      "Step: [1708] d_loss: 0.45115086, g_loss: 1.18430698\n",
      "Step: [1709] d_loss: 0.46254563, g_loss: 1.14731538\n",
      "Step: [1710] d_loss: 0.46781015, g_loss: 1.14848602\n",
      "Step: [1711] d_loss: 0.46181500, g_loss: 1.13737702\n",
      "Step: [1712] d_loss: 0.46279883, g_loss: 1.14261270\n",
      "Step: [1713] d_loss: 0.46022505, g_loss: 1.14208615\n",
      "Step: [1714] d_loss: 0.46992734, g_loss: 1.12573135\n",
      "Step: [1715] d_loss: 0.45048094, g_loss: 1.08644903\n",
      "Step: [1716] d_loss: 0.46069741, g_loss: 1.11084211\n",
      "Step: [1717] d_loss: 0.45860586, g_loss: 1.08551717\n",
      "Step: [1718] d_loss: 0.45318604, g_loss: 1.14120698\n",
      "Step: [1719] d_loss: 0.46365404, g_loss: 1.15806770\n",
      "Step: [1720] d_loss: 0.46488592, g_loss: 1.11796999\n",
      "Step: [1721] d_loss: 0.45547858, g_loss: 1.13061261\n",
      "Step: [1722] d_loss: 0.45480171, g_loss: 1.16648555\n",
      "Step: [1723] d_loss: 0.45145455, g_loss: 1.15810788\n",
      "Step: [1724] d_loss: 0.45300254, g_loss: 1.15479672\n",
      "Step: [1725] d_loss: 0.47067717, g_loss: 1.12043750\n",
      "Step: [1726] d_loss: 0.46329328, g_loss: 1.11098063\n",
      "Step: [1727] d_loss: 0.45485964, g_loss: 1.10625827\n",
      "Step: [1728] d_loss: 0.45861977, g_loss: 1.10928357\n",
      "Step: [1729] d_loss: 0.48355705, g_loss: 1.16454268\n",
      "Step: [1730] d_loss: 0.45492709, g_loss: 1.13777208\n",
      "Step: [1731] d_loss: 0.46711540, g_loss: 1.11961222\n",
      "Step: [1732] d_loss: 0.46201563, g_loss: 1.09532988\n",
      "Step: [1733] d_loss: 0.45799679, g_loss: 1.14869976\n",
      "Step: [1734] d_loss: 0.48199129, g_loss: 1.16209352\n",
      "Step: [1735] d_loss: 0.46167424, g_loss: 1.14804935\n",
      "Step: [1736] d_loss: 0.45558625, g_loss: 1.13454008\n",
      "Step: [1737] d_loss: 0.45923263, g_loss: 1.14782536\n",
      "Step: [1738] d_loss: 0.45524979, g_loss: 1.12690806\n",
      "Step: [1739] d_loss: 0.46564004, g_loss: 1.16538525\n",
      "Step: [1740] d_loss: 0.46564791, g_loss: 1.13502252\n",
      "Step: [1741] d_loss: 0.45271999, g_loss: 1.16885304\n",
      "Step: [1742] d_loss: 0.44987944, g_loss: 1.14908898\n",
      "Step: [1743] d_loss: 0.46475923, g_loss: 1.10197079\n",
      "Step: [1744] d_loss: 0.45813772, g_loss: 1.14394963\n",
      "Step: [1745] d_loss: 0.46355343, g_loss: 1.13522029\n",
      "Step: [1746] d_loss: 0.45496792, g_loss: 1.16647351\n",
      "Step: [1747] d_loss: 0.46364319, g_loss: 1.13729250\n",
      "Step: [1748] d_loss: 0.45905176, g_loss: 1.12108898\n",
      "Step: [1749] d_loss: 0.45191431, g_loss: 1.14579296\n",
      "Step: [1750] d_loss: 0.46057987, g_loss: 1.12573802\n",
      "Step: [1751] d_loss: 0.45343032, g_loss: 1.13366961\n",
      "Step: [1752] d_loss: 0.45640334, g_loss: 1.13486969\n",
      "Step: [1753] d_loss: 0.45531693, g_loss: 1.08483875\n",
      "Step: [1754] d_loss: 0.45778650, g_loss: 1.13193011\n",
      "Step: [1755] d_loss: 0.46549246, g_loss: 1.10468864\n",
      "Step: [1756] d_loss: 0.46307373, g_loss: 1.14847541\n",
      "Step: [1757] d_loss: 0.45950904, g_loss: 1.09752202\n",
      "Step: [1758] d_loss: 0.45735818, g_loss: 1.12500036\n",
      "Step: [1759] d_loss: 0.45390326, g_loss: 1.09674430\n",
      "Step: [1760] d_loss: 0.44883543, g_loss: 1.11642015\n",
      "Step: [1761] d_loss: 0.46170527, g_loss: 1.10066640\n",
      "Step: [1762] d_loss: 0.45471963, g_loss: 1.07688558\n",
      "Step: [1763] d_loss: 0.46016049, g_loss: 1.10743999\n",
      "Step: [1764] d_loss: 0.45529109, g_loss: 1.18462682\n",
      "Step: [1765] d_loss: 0.45077026, g_loss: 1.13369346\n",
      "Step: [1766] d_loss: 0.45954207, g_loss: 1.14736986\n",
      "Step: [1767] d_loss: 0.45781675, g_loss: 1.16117179\n",
      "Step: [1768] d_loss: 0.45628673, g_loss: 1.11656141\n",
      "Step: [1769] d_loss: 0.46297932, g_loss: 1.09258652\n",
      "Step: [1770] d_loss: 0.45076963, g_loss: 1.14229596\n",
      "Step: [1771] d_loss: 0.45417383, g_loss: 1.15397394\n",
      "Step: [1772] d_loss: 0.44613600, g_loss: 1.17443538\n",
      "Step: [1773] d_loss: 0.46395162, g_loss: 1.10737908\n",
      "Step: [1774] d_loss: 0.45285696, g_loss: 1.11520648\n",
      "Step: [1775] d_loss: 0.45046443, g_loss: 1.09077871\n",
      "Step: [1776] d_loss: 0.44768241, g_loss: 1.12001956\n",
      "Step: [1777] d_loss: 0.45373985, g_loss: 1.08122313\n",
      "Step: [1778] d_loss: 0.45930120, g_loss: 1.13949382\n",
      "Step: [1779] d_loss: 0.45557469, g_loss: 1.14640987\n",
      "Step: [1780] d_loss: 0.46577147, g_loss: 1.10324931\n",
      "Step: [1781] d_loss: 0.45089290, g_loss: 1.14294422\n",
      "Step: [1782] d_loss: 0.45030662, g_loss: 1.11085200\n",
      "Step: [1783] d_loss: 0.44986349, g_loss: 1.12398171\n",
      "Step: [1784] d_loss: 0.45131040, g_loss: 1.09303010\n",
      "Step: [1785] d_loss: 0.45244128, g_loss: 1.12297976\n",
      "Step: [1786] d_loss: 0.45040309, g_loss: 1.13509822\n",
      "Step: [1787] d_loss: 0.44392112, g_loss: 1.12593019\n",
      "Step: [1788] d_loss: 0.45857158, g_loss: 1.13879776\n",
      "Step: [1789] d_loss: 0.44968572, g_loss: 1.13670087\n",
      "Step: [1790] d_loss: 0.44875744, g_loss: 1.16215289\n",
      "Step: [1791] d_loss: 0.45798805, g_loss: 1.13538492\n",
      "Step: [1792] d_loss: 0.44942299, g_loss: 1.14373469\n",
      "Step: [1793] d_loss: 0.45741713, g_loss: 1.12023592\n",
      "Step: [1794] d_loss: 0.44990912, g_loss: 1.10619235\n",
      "Step: [1795] d_loss: 0.45497981, g_loss: 1.12635803\n",
      "Step: [1796] d_loss: 0.45091164, g_loss: 1.14650893\n",
      "Step: [1797] d_loss: 0.45036960, g_loss: 1.08985579\n",
      "Step: [1798] d_loss: 0.44197738, g_loss: 1.12333417\n",
      "Step: [1799] d_loss: 0.45361507, g_loss: 1.12199891\n",
      "Step: [1800] d_loss: 0.44913703, g_loss: 1.12202454\n",
      "Step: [1801] d_loss: 0.45473498, g_loss: 1.13069582\n",
      "Step: [1802] d_loss: 0.45209590, g_loss: 1.14593601\n",
      "Step: [1803] d_loss: 0.45075476, g_loss: 1.14144361\n",
      "Step: [1804] d_loss: 0.45531425, g_loss: 1.11979890\n",
      "Step: [1805] d_loss: 0.44775814, g_loss: 1.14803708\n",
      "Step: [1806] d_loss: 0.45600230, g_loss: 1.13776767\n",
      "Step: [1807] d_loss: 0.45559889, g_loss: 1.14263582\n",
      "Step: [1808] d_loss: 0.44959968, g_loss: 1.12398148\n",
      "Step: [1809] d_loss: 0.45373359, g_loss: 1.13097143\n",
      "Step: [1810] d_loss: 0.45473978, g_loss: 1.13418293\n",
      "Step: [1811] d_loss: 0.44987684, g_loss: 1.10801446\n",
      "Step: [1812] d_loss: 0.45647895, g_loss: 1.16536915\n",
      "Step: [1813] d_loss: 0.44440153, g_loss: 1.15804756\n",
      "Step: [1814] d_loss: 0.45329157, g_loss: 1.13599646\n",
      "Step: [1815] d_loss: 0.45470613, g_loss: 1.13687897\n",
      "Step: [1816] d_loss: 0.45000157, g_loss: 1.14627218\n",
      "Step: [1817] d_loss: 0.44937804, g_loss: 1.13972652\n",
      "Step: [1818] d_loss: 0.44859368, g_loss: 1.14784586\n",
      "Step: [1819] d_loss: 0.45377216, g_loss: 1.12704813\n",
      "Step: [1820] d_loss: 0.46486720, g_loss: 1.15369856\n",
      "Step: [1821] d_loss: 0.45677125, g_loss: 1.10269880\n",
      "Step: [1822] d_loss: 0.45299625, g_loss: 1.13699913\n",
      "Step: [1823] d_loss: 0.44419801, g_loss: 1.14133704\n",
      "Step: [1824] d_loss: 0.45346364, g_loss: 1.15167320\n",
      "Step: [1825] d_loss: 0.44780692, g_loss: 1.10750687\n",
      "Step: [1826] d_loss: 0.44740835, g_loss: 1.15115011\n",
      "Step: [1827] d_loss: 0.45850283, g_loss: 1.12610042\n",
      "Step: [1828] d_loss: 0.45247227, g_loss: 1.14602768\n",
      "Step: [1829] d_loss: 0.44624114, g_loss: 1.14001226\n",
      "Step: [1830] d_loss: 0.44504544, g_loss: 1.12754369\n",
      "Step: [1831] d_loss: 0.43902037, g_loss: 1.14883375\n",
      "Step: [1832] d_loss: 0.44861898, g_loss: 1.12272525\n",
      "Step: [1833] d_loss: 0.45229134, g_loss: 1.12237382\n",
      "Step: [1834] d_loss: 0.44494307, g_loss: 1.14984667\n",
      "Step: [1835] d_loss: 0.44357026, g_loss: 1.15406227\n",
      "Step: [1836] d_loss: 0.44931155, g_loss: 1.10108030\n",
      "Step: [1837] d_loss: 0.44739196, g_loss: 1.13139641\n",
      "Step: [1838] d_loss: 0.44772467, g_loss: 1.11222541\n",
      "Step: [1839] d_loss: 0.44538805, g_loss: 1.14380765\n",
      "Step: [1840] d_loss: 0.45353568, g_loss: 1.10167849\n",
      "Step: [1841] d_loss: 0.44820264, g_loss: 1.11023176\n",
      "Step: [1842] d_loss: 0.45164782, g_loss: 1.13321245\n",
      "Step: [1843] d_loss: 0.45563400, g_loss: 1.12998533\n",
      "Step: [1844] d_loss: 0.46469158, g_loss: 1.14972627\n",
      "Step: [1845] d_loss: 0.45966297, g_loss: 1.10225809\n",
      "Step: [1846] d_loss: 0.45431203, g_loss: 1.15575671\n",
      "Step: [1847] d_loss: 0.45378670, g_loss: 1.10768080\n",
      "Step: [1848] d_loss: 0.45547590, g_loss: 1.13699389\n",
      "Step: [1849] d_loss: 0.45923066, g_loss: 1.11590886\n",
      "Step: [1850] d_loss: 0.45705634, g_loss: 1.10135984\n",
      "Step: [1851] d_loss: 0.45105147, g_loss: 1.08724689\n",
      "Step: [1852] d_loss: 0.45604867, g_loss: 1.13147128\n",
      "Step: [1853] d_loss: 0.45692784, g_loss: 1.11802793\n",
      "Step: [1854] d_loss: 0.45590386, g_loss: 1.12552702\n",
      "Step: [1855] d_loss: 0.45874220, g_loss: 1.08807158\n",
      "Step: [1856] d_loss: 0.45693088, g_loss: 1.12558115\n",
      "Step: [1857] d_loss: 0.45629197, g_loss: 1.09672117\n",
      "Step: [1858] d_loss: 0.45883006, g_loss: 1.10025930\n",
      "Step: [1859] d_loss: 0.45847586, g_loss: 1.11993992\n",
      "Step: [1860] d_loss: 0.45808938, g_loss: 1.12375808\n",
      "Step: [1861] d_loss: 0.45146671, g_loss: 1.08352399\n",
      "Step: [1862] d_loss: 0.45868620, g_loss: 1.12812614\n",
      "Step: [1863] d_loss: 0.45021701, g_loss: 1.10552561\n",
      "Step: [1864] d_loss: 0.45029914, g_loss: 1.12875545\n",
      "Step: [1865] d_loss: 0.44889760, g_loss: 1.13403583\n",
      "Step: [1866] d_loss: 0.45584980, g_loss: 1.11630392\n",
      "Step: [1867] d_loss: 0.45849189, g_loss: 1.08314574\n",
      "Step: [1868] d_loss: 0.45669910, g_loss: 1.08348000\n",
      "Step: [1869] d_loss: 0.45311436, g_loss: 1.12547326\n",
      "Step: [1870] d_loss: 0.44824675, g_loss: 1.10505354\n",
      "Step: [1871] d_loss: 0.45138863, g_loss: 1.11130428\n",
      "Step: [1872] d_loss: 0.45212793, g_loss: 1.12381732\n",
      "Step: [1873] d_loss: 0.45026919, g_loss: 1.12994325\n",
      "Step: [1874] d_loss: 0.44581968, g_loss: 1.12201929\n",
      "Step: [1875] d_loss: 0.46327364, g_loss: 1.07022274\n",
      "Step: [1876] d_loss: 0.45751995, g_loss: 1.12847638\n",
      "Step: [1877] d_loss: 0.45507640, g_loss: 1.13956952\n",
      "Step: [1878] d_loss: 0.45302057, g_loss: 1.10427821\n",
      "Step: [1879] d_loss: 0.45458463, g_loss: 1.11773288\n",
      "Step: [1880] d_loss: 0.45172757, g_loss: 1.14954793\n",
      "Step: [1881] d_loss: 0.44483382, g_loss: 1.13762295\n",
      "Step: [1882] d_loss: 0.44670278, g_loss: 1.15147161\n",
      "Step: [1883] d_loss: 0.44841167, g_loss: 1.11548424\n",
      "Step: [1884] d_loss: 0.45418519, g_loss: 1.11265242\n",
      "Step: [1885] d_loss: 0.45387250, g_loss: 1.10686541\n",
      "Step: [1886] d_loss: 0.44656831, g_loss: 1.10833192\n",
      "Step: [1887] d_loss: 0.46232399, g_loss: 1.09540236\n",
      "Step: [1888] d_loss: 0.46384758, g_loss: 1.11850441\n",
      "Step: [1889] d_loss: 0.45662871, g_loss: 1.11895585\n",
      "Step: [1890] d_loss: 0.45084420, g_loss: 1.12448621\n",
      "Step: [1891] d_loss: 0.49175784, g_loss: 1.11399293\n",
      "Step: [1892] d_loss: 0.46212620, g_loss: 1.14253318\n",
      "Step: [1893] d_loss: 0.45207688, g_loss: 1.14013791\n",
      "Step: [1894] d_loss: 0.45291820, g_loss: 1.11483037\n",
      "Step: [1895] d_loss: 0.46968395, g_loss: 1.14828467\n",
      "Step: [1896] d_loss: 0.46552846, g_loss: 1.11261058\n",
      "Step: [1897] d_loss: 0.48826683, g_loss: 1.15052462\n",
      "Step: [1898] d_loss: 0.47089887, g_loss: 1.10242033\n",
      "Step: [1899] d_loss: 0.46378759, g_loss: 1.12364423\n",
      "Step: [1900] d_loss: 0.45625648, g_loss: 1.13661432\n",
      "Step: [1901] d_loss: 0.46150476, g_loss: 1.10596001\n",
      "Step: [1902] d_loss: 0.45698151, g_loss: 1.13268888\n",
      "Step: [1903] d_loss: 0.45030203, g_loss: 1.12479270\n",
      "Step: [1904] d_loss: 0.45053545, g_loss: 1.13240194\n",
      "Step: [1905] d_loss: 0.45600107, g_loss: 1.09823930\n",
      "Step: [1906] d_loss: 0.46590367, g_loss: 1.08992004\n",
      "Step: [1907] d_loss: 0.46175870, g_loss: 1.09666455\n",
      "Step: [1908] d_loss: 0.46314010, g_loss: 1.14773107\n",
      "Step: [1909] d_loss: 0.46180886, g_loss: 1.17935526\n",
      "Step: [1910] d_loss: 0.45211396, g_loss: 1.10021424\n",
      "Step: [1911] d_loss: 0.46365225, g_loss: 1.11611080\n",
      "Step: [1912] d_loss: 0.44901276, g_loss: 1.11709821\n",
      "Step: [1913] d_loss: 0.45604265, g_loss: 1.14847696\n",
      "Step: [1914] d_loss: 0.44641376, g_loss: 1.12678123\n",
      "Step: [1915] d_loss: 0.45233572, g_loss: 1.09701514\n",
      "Step: [1916] d_loss: 0.44667980, g_loss: 1.19221461\n",
      "Step: [1917] d_loss: 0.52623308, g_loss: 1.11013758\n",
      "Step: [1918] d_loss: 0.48106462, g_loss: 1.15645027\n",
      "Step: [1919] d_loss: 0.48825002, g_loss: 1.14326096\n",
      "Step: [1920] d_loss: 0.46869054, g_loss: 1.11677063\n",
      "Step: [1921] d_loss: 0.44863307, g_loss: 1.07983029\n",
      "Step: [1922] d_loss: 0.46603116, g_loss: 1.14984429\n",
      "Step: [1923] d_loss: 0.46495143, g_loss: 1.12328815\n",
      "Step: [1924] d_loss: 0.45882154, g_loss: 1.09161401\n",
      "Step: [1925] d_loss: 0.45610920, g_loss: 1.07822752\n",
      "Step: [1926] d_loss: 0.45063984, g_loss: 1.05525291\n",
      "Step: [1927] d_loss: 0.45083654, g_loss: 1.08383298\n",
      "Step: [1928] d_loss: 0.44839513, g_loss: 1.13123512\n",
      "Step: [1929] d_loss: 0.44884303, g_loss: 1.09485555\n",
      "Step: [1930] d_loss: 0.44783616, g_loss: 1.10481286\n",
      "Step: [1931] d_loss: 0.45359075, g_loss: 1.08792150\n",
      "Step: [1932] d_loss: 0.45383289, g_loss: 1.10327697\n",
      "Step: [1933] d_loss: 0.46883449, g_loss: 1.08562720\n",
      "Step: [1934] d_loss: 0.45445931, g_loss: 1.12944448\n",
      "Step: [1935] d_loss: 0.45385933, g_loss: 1.07813096\n",
      "Step: [1936] d_loss: 0.45223486, g_loss: 1.10158682\n",
      "Step: [1937] d_loss: 0.45001721, g_loss: 1.09351850\n",
      "Step: [1938] d_loss: 0.45331818, g_loss: 1.11641800\n",
      "Step: [1939] d_loss: 0.44359878, g_loss: 1.10398960\n",
      "Step: [1940] d_loss: 0.45687634, g_loss: 1.11896908\n",
      "Step: [1941] d_loss: 0.45983425, g_loss: 1.13074529\n",
      "Step: [1942] d_loss: 0.44916683, g_loss: 1.10976708\n",
      "Step: [1943] d_loss: 0.45247811, g_loss: 1.12324846\n",
      "Step: [1944] d_loss: 0.44874048, g_loss: 1.09031832\n",
      "Step: [1945] d_loss: 0.46359926, g_loss: 1.12422740\n",
      "Step: [1946] d_loss: 0.46240279, g_loss: 1.11328542\n",
      "Step: [1947] d_loss: 0.44912413, g_loss: 1.10498285\n",
      "Step: [1948] d_loss: 0.45160139, g_loss: 1.11364865\n",
      "Step: [1949] d_loss: 0.45027378, g_loss: 1.10352242\n",
      "Step: [1950] d_loss: 0.44982255, g_loss: 1.05741560\n",
      "Step: [1951] d_loss: 0.45732465, g_loss: 1.09845638\n",
      "Step: [1952] d_loss: 0.44457510, g_loss: 1.13590145\n",
      "Step: [1953] d_loss: 0.45234716, g_loss: 1.07771564\n",
      "Step: [1954] d_loss: 0.45235965, g_loss: 1.13411486\n",
      "Step: [1955] d_loss: 0.46062192, g_loss: 1.14024711\n",
      "Step: [1956] d_loss: 0.45843899, g_loss: 1.09750283\n",
      "Step: [1957] d_loss: 0.46603584, g_loss: 1.14437354\n",
      "Step: [1958] d_loss: 0.45730591, g_loss: 1.18588984\n",
      "Step: [1959] d_loss: 0.44709480, g_loss: 1.10177350\n",
      "Step: [1960] d_loss: 0.45384371, g_loss: 1.14303648\n",
      "Step: [1961] d_loss: 0.45511806, g_loss: 1.11021411\n",
      "Step: [1962] d_loss: 0.45841527, g_loss: 1.12200511\n",
      "Step: [1963] d_loss: 0.46897274, g_loss: 1.14424503\n",
      "Step: [1964] d_loss: 0.45869067, g_loss: 1.13346815\n",
      "Step: [1965] d_loss: 0.45212334, g_loss: 1.09052026\n",
      "Step: [1966] d_loss: 0.45553616, g_loss: 1.09407032\n",
      "Step: [1967] d_loss: 0.45070025, g_loss: 1.12020254\n",
      "Step: [1968] d_loss: 0.47095349, g_loss: 1.09071481\n",
      "Step: [1969] d_loss: 0.45580581, g_loss: 1.11203492\n",
      "Step: [1970] d_loss: 0.45655715, g_loss: 1.13521945\n",
      "Step: [1971] d_loss: 0.44882977, g_loss: 1.07872391\n",
      "Step: [1972] d_loss: 0.44352818, g_loss: 1.09448397\n",
      "Step: [1973] d_loss: 0.44779658, g_loss: 1.10675049\n",
      "Step: [1974] d_loss: 0.45348150, g_loss: 1.11384964\n",
      "Step: [1975] d_loss: 0.45209229, g_loss: 1.14781213\n",
      "Step: [1976] d_loss: 0.45515063, g_loss: 1.12011039\n",
      "Step: [1977] d_loss: 0.44758007, g_loss: 1.13543963\n",
      "Step: [1978] d_loss: 0.47346550, g_loss: 1.10408199\n",
      "Step: [1979] d_loss: 0.45556086, g_loss: 1.13608360\n",
      "Step: [1980] d_loss: 0.44856277, g_loss: 1.12837899\n",
      "Step: [1981] d_loss: 0.44673079, g_loss: 1.11770737\n",
      "Step: [1982] d_loss: 0.45244741, g_loss: 1.10595703\n",
      "Step: [1983] d_loss: 0.44894129, g_loss: 1.10919559\n",
      "Step: [1984] d_loss: 0.45836240, g_loss: 1.09984016\n",
      "Step: [1985] d_loss: 0.45707092, g_loss: 1.13577771\n",
      "Step: [1986] d_loss: 0.45510861, g_loss: 1.09258938\n",
      "Step: [1987] d_loss: 0.44476417, g_loss: 1.12067807\n",
      "Step: [1988] d_loss: 0.44941789, g_loss: 1.08611274\n",
      "Step: [1989] d_loss: 0.44879961, g_loss: 1.14296269\n",
      "Step: [1990] d_loss: 0.44922236, g_loss: 1.09821749\n",
      "Step: [1991] d_loss: 0.44780362, g_loss: 1.07494545\n",
      "Step: [1992] d_loss: 0.45540807, g_loss: 1.11144865\n",
      "Step: [1993] d_loss: 0.45174795, g_loss: 1.11772692\n",
      "Step: [1994] d_loss: 0.44594139, g_loss: 1.09523880\n",
      "Step: [1995] d_loss: 0.44543228, g_loss: 1.12782466\n",
      "Step: [1996] d_loss: 0.46454865, g_loss: 1.10006249\n",
      "Step: [1997] d_loss: 0.44812548, g_loss: 1.08891225\n",
      "Step: [1998] d_loss: 0.45270997, g_loss: 1.10754299\n",
      "Step: [1999] d_loss: 0.44457373, g_loss: 1.08684146\n",
      "Step: [2000] d_loss: 0.44764507, g_loss: 1.13360584\n",
      "Step: [2001] d_loss: 0.44561663, g_loss: 1.10884547\n",
      "Step: [2002] d_loss: 0.46038887, g_loss: 1.11040127\n",
      "Step: [2003] d_loss: 0.49013221, g_loss: 1.09668398\n",
      "Step: [2004] d_loss: 0.44883427, g_loss: 1.09538102\n",
      "Step: [2005] d_loss: 0.46470439, g_loss: 1.12018394\n",
      "Step: [2006] d_loss: 0.44436425, g_loss: 1.07690513\n",
      "Step: [2007] d_loss: 0.44859356, g_loss: 1.07873535\n",
      "Step: [2008] d_loss: 0.44636711, g_loss: 1.11334097\n",
      "Step: [2009] d_loss: 0.44208467, g_loss: 1.12035751\n",
      "Step: [2010] d_loss: 0.45678800, g_loss: 1.09884679\n",
      "Step: [2011] d_loss: 0.45993328, g_loss: 1.08310294\n",
      "Step: [2012] d_loss: 0.45038465, g_loss: 1.08154416\n",
      "Step: [2013] d_loss: 0.44513589, g_loss: 1.15144885\n",
      "Step: [2014] d_loss: 0.44696751, g_loss: 1.10212207\n",
      "Step: [2015] d_loss: 0.45614916, g_loss: 1.12704372\n",
      "Step: [2016] d_loss: 0.44499224, g_loss: 1.09661567\n",
      "Step: [2017] d_loss: 0.44822919, g_loss: 1.11378551\n",
      "Step: [2018] d_loss: 0.44790700, g_loss: 1.11110330\n",
      "Step: [2019] d_loss: 0.46437600, g_loss: 1.12115467\n",
      "Step: [2020] d_loss: 0.45450780, g_loss: 1.10596263\n",
      "Step: [2021] d_loss: 0.44982836, g_loss: 1.12850428\n",
      "Step: [2022] d_loss: 0.44767228, g_loss: 1.12684643\n",
      "Step: [2023] d_loss: 0.44461465, g_loss: 1.13070643\n",
      "Step: [2024] d_loss: 0.44974858, g_loss: 1.10089445\n",
      "Step: [2025] d_loss: 0.44463128, g_loss: 1.09446132\n",
      "Step: [2026] d_loss: 0.44564584, g_loss: 1.11261046\n",
      "Step: [2027] d_loss: 0.44152677, g_loss: 1.09770346\n",
      "Step: [2028] d_loss: 0.44475830, g_loss: 1.13805199\n",
      "Step: [2029] d_loss: 0.45863366, g_loss: 1.14854336\n",
      "Step: [2030] d_loss: 0.45066243, g_loss: 1.09343457\n",
      "Step: [2031] d_loss: 0.45154449, g_loss: 1.10307324\n",
      "Step: [2032] d_loss: 0.45512471, g_loss: 1.14790058\n",
      "Step: [2033] d_loss: 0.44715199, g_loss: 1.11218762\n",
      "Step: [2034] d_loss: 0.45971951, g_loss: 1.10272050\n",
      "Step: [2035] d_loss: 0.44581488, g_loss: 1.14520633\n",
      "Step: [2036] d_loss: 0.45030767, g_loss: 1.14631510\n",
      "Step: [2037] d_loss: 0.45321286, g_loss: 1.13490880\n",
      "Step: [2038] d_loss: 0.46187174, g_loss: 1.13618767\n",
      "Step: [2039] d_loss: 0.45972157, g_loss: 1.13770509\n",
      "Step: [2040] d_loss: 0.44557902, g_loss: 1.15439737\n",
      "Step: [2041] d_loss: 0.45368424, g_loss: 1.10353041\n",
      "Step: [2042] d_loss: 0.44989261, g_loss: 1.09702682\n",
      "Step: [2043] d_loss: 0.46104684, g_loss: 1.08303952\n",
      "Step: [2044] d_loss: 0.44835979, g_loss: 1.09176099\n",
      "Step: [2045] d_loss: 0.45971161, g_loss: 1.10706937\n",
      "Step: [2046] d_loss: 0.45703712, g_loss: 1.10982871\n",
      "Step: [2047] d_loss: 0.44847876, g_loss: 1.09138989\n",
      "Step: [2048] d_loss: 0.46680456, g_loss: 1.11314487\n",
      "Step: [2049] d_loss: 0.45639670, g_loss: 1.06376112\n",
      "Step: [2050] d_loss: 0.44984573, g_loss: 1.05996633\n",
      "Step: [2051] d_loss: 0.44762817, g_loss: 1.15538919\n",
      "Step: [2052] d_loss: 0.44443518, g_loss: 1.09322572\n",
      "Step: [2053] d_loss: 0.44660857, g_loss: 1.13391745\n",
      "Step: [2054] d_loss: 0.44934899, g_loss: 1.12113845\n",
      "Step: [2055] d_loss: 0.44518936, g_loss: 1.12981570\n",
      "Step: [2056] d_loss: 0.43870285, g_loss: 1.10930753\n",
      "Step: [2057] d_loss: 0.44721070, g_loss: 1.08943403\n",
      "Step: [2058] d_loss: 0.44408217, g_loss: 1.09674537\n",
      "Step: [2059] d_loss: 0.44841290, g_loss: 1.12711263\n",
      "Step: [2060] d_loss: 0.44852641, g_loss: 1.11308062\n",
      "Step: [2061] d_loss: 0.44534981, g_loss: 1.10767722\n",
      "Step: [2062] d_loss: 0.44783312, g_loss: 1.12084377\n",
      "Step: [2063] d_loss: 0.44920537, g_loss: 1.12235653\n",
      "Step: [2064] d_loss: 0.44229525, g_loss: 1.11955190\n",
      "Step: [2065] d_loss: 0.44882539, g_loss: 1.10201395\n",
      "Step: [2066] d_loss: 0.44345540, g_loss: 1.15553832\n",
      "Step: [2067] d_loss: 0.44337869, g_loss: 1.14602220\n",
      "Step: [2068] d_loss: 0.44908288, g_loss: 1.12579381\n",
      "Step: [2069] d_loss: 0.44582918, g_loss: 1.11108267\n",
      "Step: [2070] d_loss: 0.45034319, g_loss: 1.13373435\n",
      "Step: [2071] d_loss: 0.44381827, g_loss: 1.12050080\n",
      "Step: [2072] d_loss: 0.45519897, g_loss: 1.13314056\n",
      "Step: [2073] d_loss: 0.44734740, g_loss: 1.11949289\n",
      "Step: [2074] d_loss: 0.45348957, g_loss: 1.11411965\n",
      "Step: [2075] d_loss: 0.44026759, g_loss: 1.10095465\n",
      "Step: [2076] d_loss: 0.45437446, g_loss: 1.15080702\n",
      "Step: [2077] d_loss: 0.44372308, g_loss: 1.14610231\n",
      "Step: [2078] d_loss: 0.45586231, g_loss: 1.10712278\n",
      "Step: [2079] d_loss: 0.44917360, g_loss: 1.10623741\n",
      "Step: [2080] d_loss: 0.44567978, g_loss: 1.10924888\n",
      "Step: [2081] d_loss: 0.45260131, g_loss: 1.09400499\n",
      "Step: [2082] d_loss: 0.45082897, g_loss: 1.09423673\n",
      "Step: [2083] d_loss: 0.46147233, g_loss: 1.14218080\n",
      "Step: [2084] d_loss: 0.45332250, g_loss: 1.12900162\n",
      "Step: [2085] d_loss: 0.44864446, g_loss: 1.10369718\n",
      "Step: [2086] d_loss: 0.45495346, g_loss: 1.10829616\n",
      "Step: [2087] d_loss: 0.44020319, g_loss: 1.11659849\n",
      "Step: [2088] d_loss: 0.45284870, g_loss: 1.09255660\n",
      "Step: [2089] d_loss: 0.44304228, g_loss: 1.13638413\n",
      "Step: [2090] d_loss: 0.44513571, g_loss: 1.08692861\n",
      "Step: [2091] d_loss: 0.44765741, g_loss: 1.11729944\n",
      "Step: [2092] d_loss: 0.44552830, g_loss: 1.12036324\n",
      "Step: [2093] d_loss: 0.45666477, g_loss: 1.06283927\n",
      "Step: [2094] d_loss: 0.45561513, g_loss: 1.13031507\n",
      "Step: [2095] d_loss: 0.45273060, g_loss: 1.11771429\n",
      "Step: [2096] d_loss: 0.45767373, g_loss: 1.10993242\n",
      "Step: [2097] d_loss: 0.44838059, g_loss: 1.12379134\n",
      "Step: [2098] d_loss: 0.45040032, g_loss: 1.13073611\n",
      "Step: [2099] d_loss: 0.44298798, g_loss: 1.11772728\n",
      "Step: [2100] d_loss: 0.45025203, g_loss: 1.12015367\n",
      "Step: [2101] d_loss: 0.45618680, g_loss: 1.11980617\n",
      "Step: [2102] d_loss: 0.44805118, g_loss: 1.09851789\n",
      "Step: [2103] d_loss: 0.44526541, g_loss: 1.12508166\n",
      "Step: [2104] d_loss: 0.44755960, g_loss: 1.10506094\n",
      "Step: [2105] d_loss: 0.43850657, g_loss: 1.13932216\n",
      "Step: [2106] d_loss: 0.44902211, g_loss: 1.09582567\n",
      "Step: [2107] d_loss: 0.44268519, g_loss: 1.15777457\n",
      "Step: [2108] d_loss: 0.44737798, g_loss: 1.10688198\n",
      "Step: [2109] d_loss: 0.45376363, g_loss: 1.11360013\n",
      "Step: [2110] d_loss: 0.46164110, g_loss: 1.11553609\n",
      "Step: [2111] d_loss: 0.45044252, g_loss: 1.11031890\n",
      "Step: [2112] d_loss: 0.48290101, g_loss: 1.10012710\n",
      "Step: [2113] d_loss: 0.45202106, g_loss: 1.11873329\n",
      "Step: [2114] d_loss: 0.44722465, g_loss: 1.09562218\n",
      "Step: [2115] d_loss: 0.44265771, g_loss: 1.09109640\n",
      "Step: [2116] d_loss: 0.44958791, g_loss: 1.09593344\n",
      "Step: [2117] d_loss: 0.44887751, g_loss: 1.06330037\n",
      "Step: [2118] d_loss: 0.44227317, g_loss: 1.11575758\n",
      "Step: [2119] d_loss: 0.44895327, g_loss: 1.08259559\n",
      "Step: [2120] d_loss: 0.44730633, g_loss: 1.08572817\n",
      "Step: [2121] d_loss: 0.44914225, g_loss: 1.11271501\n",
      "Step: [2122] d_loss: 0.44982448, g_loss: 1.08691287\n",
      "Step: [2123] d_loss: 0.44788307, g_loss: 1.09751225\n",
      "Step: [2124] d_loss: 0.44912347, g_loss: 1.11723554\n",
      "Step: [2125] d_loss: 0.44744635, g_loss: 1.11772680\n",
      "Step: [2126] d_loss: 0.45009017, g_loss: 1.04916501\n",
      "Step: [2127] d_loss: 0.43948695, g_loss: 1.12312102\n",
      "Step: [2128] d_loss: 0.45033208, g_loss: 1.10673451\n",
      "Step: [2129] d_loss: 0.44619972, g_loss: 1.05575848\n",
      "Step: [2130] d_loss: 0.45318371, g_loss: 1.11013448\n",
      "Step: [2131] d_loss: 0.45438293, g_loss: 1.07857597\n",
      "Step: [2132] d_loss: 0.45118487, g_loss: 1.08309484\n",
      "Step: [2133] d_loss: 0.44344226, g_loss: 1.10603189\n",
      "Step: [2134] d_loss: 0.44649470, g_loss: 1.11624587\n",
      "Step: [2135] d_loss: 0.44889432, g_loss: 1.13475728\n",
      "Step: [2136] d_loss: 0.44463158, g_loss: 1.14607537\n",
      "Step: [2137] d_loss: 0.44474128, g_loss: 1.11668634\n",
      "Step: [2138] d_loss: 0.43979836, g_loss: 1.10417211\n",
      "Step: [2139] d_loss: 0.44636053, g_loss: 1.12552726\n",
      "Step: [2140] d_loss: 0.44664592, g_loss: 1.11131012\n",
      "Step: [2141] d_loss: 0.44254795, g_loss: 1.10315442\n",
      "Step: [2142] d_loss: 0.43812129, g_loss: 1.10736811\n",
      "Step: [2143] d_loss: 0.44372529, g_loss: 1.10931432\n",
      "Step: [2144] d_loss: 0.44352999, g_loss: 1.11983812\n",
      "Step: [2145] d_loss: 0.44018164, g_loss: 1.10172915\n",
      "Step: [2146] d_loss: 0.44557315, g_loss: 1.10386777\n",
      "Step: [2147] d_loss: 0.44543836, g_loss: 1.14078331\n",
      "Step: [2148] d_loss: 0.46521357, g_loss: 1.14964044\n",
      "Step: [2149] d_loss: 0.45713621, g_loss: 1.12284958\n",
      "Step: [2150] d_loss: 0.45185900, g_loss: 1.14824724\n",
      "Step: [2151] d_loss: 0.44137990, g_loss: 1.10182345\n",
      "Step: [2152] d_loss: 0.44827017, g_loss: 1.11787498\n",
      "Step: [2153] d_loss: 0.44287792, g_loss: 1.11012793\n",
      "Step: [2154] d_loss: 0.44415236, g_loss: 1.09521437\n",
      "Step: [2155] d_loss: 0.44299740, g_loss: 1.14324081\n",
      "Step: [2156] d_loss: 0.44490826, g_loss: 1.09859610\n",
      "Step: [2157] d_loss: 0.44868177, g_loss: 1.12822616\n",
      "Step: [2158] d_loss: 0.44663963, g_loss: 1.13520420\n",
      "Step: [2159] d_loss: 0.44508538, g_loss: 1.14420271\n",
      "Step: [2160] d_loss: 0.44223148, g_loss: 1.12026024\n",
      "Step: [2161] d_loss: 0.44402483, g_loss: 1.14646089\n",
      "Step: [2162] d_loss: 0.44620946, g_loss: 1.08540297\n",
      "Step: [2163] d_loss: 0.45671189, g_loss: 1.12607014\n",
      "Step: [2164] d_loss: 0.44555506, g_loss: 1.12358117\n",
      "Step: [2165] d_loss: 0.44835022, g_loss: 1.12442076\n",
      "Step: [2166] d_loss: 0.44808894, g_loss: 1.09647655\n",
      "Step: [2167] d_loss: 0.44991365, g_loss: 1.16888797\n",
      "Step: [2168] d_loss: 0.44937730, g_loss: 1.18170655\n",
      "Step: [2169] d_loss: 0.45110470, g_loss: 1.09922421\n",
      "Step: [2170] d_loss: 0.44217342, g_loss: 1.16951954\n",
      "Step: [2171] d_loss: 0.44845620, g_loss: 1.13102531\n",
      "Step: [2172] d_loss: 0.44531164, g_loss: 1.08813930\n",
      "Step: [2173] d_loss: 0.44282943, g_loss: 1.12492645\n",
      "Step: [2174] d_loss: 0.44795105, g_loss: 1.12744498\n",
      "Step: [2175] d_loss: 0.44581467, g_loss: 1.13384926\n",
      "Step: [2176] d_loss: 0.44128388, g_loss: 1.10045779\n",
      "Step: [2177] d_loss: 0.44140384, g_loss: 1.12106788\n",
      "Step: [2178] d_loss: 0.44076094, g_loss: 1.11042249\n",
      "Step: [2179] d_loss: 0.44186562, g_loss: 1.12079573\n",
      "Step: [2180] d_loss: 0.45517883, g_loss: 1.09267128\n",
      "Step: [2181] d_loss: 0.44852352, g_loss: 1.07428932\n",
      "Step: [2182] d_loss: 0.45393550, g_loss: 1.12802935\n",
      "Step: [2183] d_loss: 0.45387167, g_loss: 1.14614701\n",
      "Step: [2184] d_loss: 0.45656848, g_loss: 1.11625707\n",
      "Step: [2185] d_loss: 0.44975004, g_loss: 1.11358523\n",
      "Step: [2186] d_loss: 0.46127224, g_loss: 1.12643754\n",
      "Step: [2187] d_loss: 0.44753838, g_loss: 1.11954248\n",
      "Step: [2188] d_loss: 0.44415095, g_loss: 1.09913993\n",
      "Step: [2189] d_loss: 0.44520494, g_loss: 1.14544940\n",
      "Step: [2190] d_loss: 0.44369856, g_loss: 1.10057878\n",
      "Step: [2191] d_loss: 0.44371617, g_loss: 1.11404347\n",
      "Step: [2192] d_loss: 0.44544584, g_loss: 1.09547091\n",
      "Step: [2193] d_loss: 0.45357639, g_loss: 1.08397627\n",
      "Step: [2194] d_loss: 0.44969115, g_loss: 1.08971322\n",
      "Step: [2195] d_loss: 0.47077182, g_loss: 1.13632202\n",
      "Step: [2196] d_loss: 0.45331585, g_loss: 1.11513543\n",
      "Step: [2197] d_loss: 0.44435954, g_loss: 1.11879301\n",
      "Step: [2198] d_loss: 0.45133954, g_loss: 1.07392931\n",
      "Step: [2199] d_loss: 0.45222604, g_loss: 1.10777557\n",
      "Step: [2200] d_loss: 0.45206660, g_loss: 1.07259226\n",
      "Step: [2201] d_loss: 0.44586095, g_loss: 1.11223328\n",
      "Step: [2202] d_loss: 0.45034823, g_loss: 1.05494010\n",
      "Step: [2203] d_loss: 0.45416349, g_loss: 1.09710848\n",
      "Step: [2204] d_loss: 0.44765753, g_loss: 1.10252571\n",
      "Step: [2205] d_loss: 0.44076848, g_loss: 1.08959389\n",
      "Step: [2206] d_loss: 0.46525085, g_loss: 1.08348477\n",
      "Step: [2207] d_loss: 0.45452067, g_loss: 1.14915228\n",
      "Step: [2208] d_loss: 0.44781020, g_loss: 1.08151317\n",
      "Step: [2209] d_loss: 0.44902113, g_loss: 1.08176911\n",
      "Step: [2210] d_loss: 0.44525662, g_loss: 1.10206234\n",
      "Step: [2211] d_loss: 0.44256172, g_loss: 1.05656636\n",
      "Step: [2212] d_loss: 0.44608131, g_loss: 1.08343279\n",
      "Step: [2213] d_loss: 0.44739407, g_loss: 1.03171265\n",
      "Step: [2214] d_loss: 0.44633231, g_loss: 1.06811178\n",
      "Step: [2215] d_loss: 0.45408309, g_loss: 1.11704934\n",
      "Step: [2216] d_loss: 0.44108105, g_loss: 1.08442783\n",
      "Step: [2217] d_loss: 0.44445130, g_loss: 1.10506654\n",
      "Step: [2218] d_loss: 0.44281077, g_loss: 1.09942198\n",
      "Step: [2219] d_loss: 0.44534299, g_loss: 1.08478081\n",
      "Step: [2220] d_loss: 0.45670247, g_loss: 1.06040490\n",
      "Step: [2221] d_loss: 0.44148603, g_loss: 1.09414101\n",
      "Step: [2222] d_loss: 0.44147056, g_loss: 1.12746608\n",
      "Step: [2223] d_loss: 0.44801661, g_loss: 1.08985555\n",
      "Step: [2224] d_loss: 0.44385540, g_loss: 1.10064125\n",
      "Step: [2225] d_loss: 0.44802770, g_loss: 1.10609400\n",
      "Step: [2226] d_loss: 0.45022163, g_loss: 1.08547568\n",
      "Step: [2227] d_loss: 0.44429758, g_loss: 1.12597907\n",
      "Step: [2228] d_loss: 0.45266581, g_loss: 1.08986259\n",
      "Step: [2229] d_loss: 0.44952458, g_loss: 1.09467864\n",
      "Step: [2230] d_loss: 0.44560999, g_loss: 1.16415274\n",
      "Step: [2231] d_loss: 0.44153973, g_loss: 1.11628425\n",
      "Step: [2232] d_loss: 0.45994785, g_loss: 1.09940553\n",
      "Step: [2233] d_loss: 0.44880167, g_loss: 1.08318174\n",
      "Step: [2234] d_loss: 0.44469815, g_loss: 1.07751203\n",
      "Step: [2235] d_loss: 0.45135731, g_loss: 1.10878432\n",
      "Step: [2236] d_loss: 0.44517493, g_loss: 1.09274471\n",
      "Step: [2237] d_loss: 0.44575840, g_loss: 1.09831107\n",
      "Step: [2238] d_loss: 0.44865820, g_loss: 1.13090122\n",
      "Step: [2239] d_loss: 0.44787252, g_loss: 1.13601863\n",
      "Step: [2240] d_loss: 0.44557726, g_loss: 1.11316085\n",
      "Step: [2241] d_loss: 0.44647595, g_loss: 1.11499572\n",
      "Step: [2242] d_loss: 0.45085815, g_loss: 1.08980596\n",
      "Step: [2243] d_loss: 0.45024443, g_loss: 1.06445754\n",
      "Step: [2244] d_loss: 0.44810677, g_loss: 1.11406982\n",
      "Step: [2245] d_loss: 0.45474464, g_loss: 1.09106147\n",
      "Step: [2246] d_loss: 0.45006463, g_loss: 1.17131341\n",
      "Step: [2247] d_loss: 0.45303488, g_loss: 1.13488376\n",
      "Step: [2248] d_loss: 0.45314783, g_loss: 1.10646093\n",
      "Step: [2249] d_loss: 0.44677889, g_loss: 1.14875329\n",
      "Step: [2250] d_loss: 0.44592288, g_loss: 1.14242148\n",
      "Step: [2251] d_loss: 0.44450021, g_loss: 1.10190988\n",
      "Step: [2252] d_loss: 0.44424132, g_loss: 1.12399030\n",
      "Step: [2253] d_loss: 0.44304761, g_loss: 1.12522042\n",
      "Step: [2254] d_loss: 0.44535524, g_loss: 1.15666568\n",
      "Step: [2255] d_loss: 0.44204849, g_loss: 1.13195372\n",
      "Step: [2256] d_loss: 0.44681156, g_loss: 1.15518332\n",
      "Step: [2257] d_loss: 0.44365934, g_loss: 1.08363855\n",
      "Step: [2258] d_loss: 0.46416143, g_loss: 1.08791590\n",
      "Step: [2259] d_loss: 0.45501462, g_loss: 1.11427212\n",
      "Step: [2260] d_loss: 0.45362246, g_loss: 1.09916556\n",
      "Step: [2261] d_loss: 0.45218050, g_loss: 1.13032615\n",
      "Step: [2262] d_loss: 0.44037351, g_loss: 1.14706850\n",
      "Step: [2263] d_loss: 0.44463786, g_loss: 1.11121166\n",
      "Step: [2264] d_loss: 0.44755307, g_loss: 1.10450852\n",
      "Step: [2265] d_loss: 0.44295573, g_loss: 1.10835826\n",
      "Step: [2266] d_loss: 0.44053662, g_loss: 1.10433877\n",
      "Step: [2267] d_loss: 0.43908757, g_loss: 1.08489621\n",
      "Step: [2268] d_loss: 0.43797946, g_loss: 1.09531999\n",
      "Step: [2269] d_loss: 0.44027415, g_loss: 1.06415176\n",
      "Step: [2270] d_loss: 0.44210467, g_loss: 1.13016880\n",
      "Step: [2271] d_loss: 0.44316921, g_loss: 1.11155295\n",
      "Step: [2272] d_loss: 0.45316377, g_loss: 1.06521940\n",
      "Step: [2273] d_loss: 0.44344372, g_loss: 1.10409009\n",
      "Step: [2274] d_loss: 0.44221181, g_loss: 1.08269298\n",
      "Step: [2275] d_loss: 0.44070229, g_loss: 1.03623986\n",
      "Step: [2276] d_loss: 0.44117135, g_loss: 1.05211508\n",
      "Step: [2277] d_loss: 0.43923226, g_loss: 1.10697377\n",
      "Step: [2278] d_loss: 0.44414750, g_loss: 1.07837355\n",
      "Step: [2279] d_loss: 0.44305104, g_loss: 1.06117809\n",
      "Step: [2280] d_loss: 0.45046476, g_loss: 1.14705253\n",
      "Step: [2281] d_loss: 0.45527479, g_loss: 1.11826921\n",
      "Step: [2282] d_loss: 0.45560071, g_loss: 1.07778978\n",
      "Step: [2283] d_loss: 0.44868547, g_loss: 1.06353009\n",
      "Step: [2284] d_loss: 0.44618368, g_loss: 1.10190606\n",
      "Step: [2285] d_loss: 0.44162971, g_loss: 1.09097195\n",
      "Step: [2286] d_loss: 0.44157276, g_loss: 1.11843908\n",
      "Step: [2287] d_loss: 0.43760327, g_loss: 1.11711836\n",
      "Step: [2288] d_loss: 0.44436389, g_loss: 1.11702847\n",
      "Step: [2289] d_loss: 0.44234410, g_loss: 1.05325890\n",
      "Step: [2290] d_loss: 0.43604955, g_loss: 1.08735919\n",
      "Step: [2291] d_loss: 0.44132861, g_loss: 1.06853211\n",
      "Step: [2292] d_loss: 0.44031048, g_loss: 1.04405665\n",
      "Step: [2293] d_loss: 0.44566888, g_loss: 1.09478927\n",
      "Step: [2294] d_loss: 0.44538319, g_loss: 1.11197722\n",
      "Step: [2295] d_loss: 0.43908724, g_loss: 1.09643602\n",
      "Step: [2296] d_loss: 0.44095033, g_loss: 1.12546217\n",
      "Step: [2297] d_loss: 0.44006249, g_loss: 1.12816024\n",
      "Step: [2298] d_loss: 0.43733770, g_loss: 1.11617875\n",
      "Step: [2299] d_loss: 0.43402529, g_loss: 1.10024595\n",
      "Step: [2300] d_loss: 0.44521308, g_loss: 1.10178101\n",
      "Step: [2301] d_loss: 0.44547594, g_loss: 1.08391643\n",
      "Step: [2302] d_loss: 0.44591546, g_loss: 1.10653770\n",
      "Step: [2303] d_loss: 0.44382414, g_loss: 1.11404622\n",
      "Step: [2304] d_loss: 0.45595008, g_loss: 1.12436044\n",
      "Step: [2305] d_loss: 0.45008293, g_loss: 1.10055721\n",
      "Step: [2306] d_loss: 0.48079631, g_loss: 1.15892935\n",
      "Step: [2307] d_loss: 0.45358923, g_loss: 1.12578940\n",
      "Step: [2308] d_loss: 0.45028013, g_loss: 1.14065886\n",
      "Step: [2309] d_loss: 0.44706035, g_loss: 1.14015234\n",
      "Step: [2310] d_loss: 0.44791386, g_loss: 1.10676622\n",
      "Step: [2311] d_loss: 0.44861367, g_loss: 1.12125218\n",
      "Step: [2312] d_loss: 0.44263449, g_loss: 1.15922213\n",
      "Step: [2313] d_loss: 0.43948999, g_loss: 1.15227425\n",
      "Step: [2314] d_loss: 0.44138500, g_loss: 1.14201236\n",
      "Step: [2315] d_loss: 0.44794288, g_loss: 1.12025332\n",
      "Step: [2316] d_loss: 0.44090629, g_loss: 1.08578110\n",
      "Step: [2317] d_loss: 0.44485345, g_loss: 1.16004002\n",
      "Step: [2318] d_loss: 0.43946621, g_loss: 1.15623677\n",
      "Step: [2319] d_loss: 0.43895596, g_loss: 1.12365961\n",
      "Step: [2320] d_loss: 0.44499066, g_loss: 1.13720155\n",
      "Step: [2321] d_loss: 0.44874653, g_loss: 1.14886653\n",
      "Step: [2322] d_loss: 0.45070386, g_loss: 1.17334712\n",
      "Step: [2323] d_loss: 0.44862059, g_loss: 1.13956606\n",
      "Step: [2324] d_loss: 0.45414358, g_loss: 1.17354119\n",
      "Step: [2325] d_loss: 0.45175108, g_loss: 1.16621912\n",
      "Step: [2326] d_loss: 0.44606221, g_loss: 1.12703681\n",
      "Step: [2327] d_loss: 0.44600523, g_loss: 1.16284978\n",
      "Step: [2328] d_loss: 0.44819748, g_loss: 1.10490096\n",
      "Step: [2329] d_loss: 0.44320318, g_loss: 1.16625738\n",
      "Step: [2330] d_loss: 0.44285509, g_loss: 1.12737298\n",
      "Step: [2331] d_loss: 0.44204795, g_loss: 1.13439822\n",
      "Step: [2332] d_loss: 0.44328028, g_loss: 1.14077568\n",
      "Step: [2333] d_loss: 0.44286218, g_loss: 1.11854339\n",
      "Step: [2334] d_loss: 0.46181610, g_loss: 1.12955856\n",
      "Step: [2335] d_loss: 0.44490701, g_loss: 1.16465449\n",
      "Step: [2336] d_loss: 0.45154586, g_loss: 1.13178837\n",
      "Step: [2337] d_loss: 0.44214025, g_loss: 1.10976911\n",
      "Step: [2338] d_loss: 0.44133654, g_loss: 1.15989733\n",
      "Step: [2339] d_loss: 0.44428584, g_loss: 1.13191342\n",
      "Step: [2340] d_loss: 0.44401112, g_loss: 1.15543437\n",
      "Step: [2341] d_loss: 0.43961284, g_loss: 1.12372243\n",
      "Step: [2342] d_loss: 0.48395428, g_loss: 1.10440636\n",
      "Step: [2343] d_loss: 0.45091411, g_loss: 1.10556328\n",
      "Step: [2344] d_loss: 0.45011672, g_loss: 1.13743174\n",
      "Step: [2345] d_loss: 0.44419506, g_loss: 1.13431132\n",
      "Step: [2346] d_loss: 0.44339728, g_loss: 1.09151912\n",
      "Step: [2347] d_loss: 0.44253400, g_loss: 1.10317993\n",
      "Step: [2348] d_loss: 0.44349113, g_loss: 1.09558892\n",
      "Step: [2349] d_loss: 0.44722614, g_loss: 1.14166594\n",
      "Step: [2350] d_loss: 0.44890079, g_loss: 1.12778616\n",
      "Step: [2351] d_loss: 0.44309652, g_loss: 1.12988842\n",
      "Step: [2352] d_loss: 0.44627762, g_loss: 1.08069515\n",
      "Step: [2353] d_loss: 0.44915518, g_loss: 1.12337232\n",
      "Step: [2354] d_loss: 0.44534364, g_loss: 1.07871330\n",
      "Step: [2355] d_loss: 0.44244930, g_loss: 1.08942711\n",
      "Step: [2356] d_loss: 0.46973252, g_loss: 1.13013053\n",
      "Step: [2357] d_loss: 0.44941473, g_loss: 1.12111628\n",
      "Step: [2358] d_loss: 0.45886934, g_loss: 1.12559438\n",
      "Step: [2359] d_loss: 0.46077293, g_loss: 1.10699344\n",
      "Step: [2360] d_loss: 0.44417423, g_loss: 1.10500479\n",
      "Step: [2361] d_loss: 0.44364637, g_loss: 1.11101305\n",
      "Step: [2362] d_loss: 0.44334328, g_loss: 1.09076285\n",
      "Step: [2363] d_loss: 0.44084990, g_loss: 1.14136064\n",
      "Step: [2364] d_loss: 0.44187230, g_loss: 1.11859143\n",
      "Step: [2365] d_loss: 0.43984023, g_loss: 1.09417498\n",
      "Step: [2366] d_loss: 0.43636304, g_loss: 1.09254420\n",
      "Step: [2367] d_loss: 0.44051138, g_loss: 1.10227704\n",
      "Step: [2368] d_loss: 0.43967545, g_loss: 1.10849679\n",
      "Step: [2369] d_loss: 0.44571069, g_loss: 1.08776283\n",
      "Step: [2370] d_loss: 0.44959146, g_loss: 1.09786987\n",
      "Step: [2371] d_loss: 0.44110611, g_loss: 1.07874930\n",
      "Step: [2372] d_loss: 0.43803218, g_loss: 1.12127781\n",
      "Step: [2373] d_loss: 0.44018739, g_loss: 1.11655939\n",
      "Step: [2374] d_loss: 0.44794178, g_loss: 1.08312941\n",
      "Step: [2375] d_loss: 0.44513649, g_loss: 1.11903620\n",
      "Step: [2376] d_loss: 0.43666562, g_loss: 1.08975220\n",
      "Step: [2377] d_loss: 0.43909183, g_loss: 1.09030497\n",
      "Step: [2378] d_loss: 0.43673536, g_loss: 1.10715878\n",
      "Step: [2379] d_loss: 0.44405469, g_loss: 1.09873593\n",
      "Step: [2380] d_loss: 0.44768313, g_loss: 1.09924698\n",
      "Step: [2381] d_loss: 0.44023603, g_loss: 1.11050534\n",
      "Step: [2382] d_loss: 0.44479454, g_loss: 1.10068655\n",
      "Step: [2383] d_loss: 0.43890986, g_loss: 1.07946932\n",
      "Step: [2384] d_loss: 0.44197536, g_loss: 1.13904703\n",
      "Step: [2385] d_loss: 0.44029549, g_loss: 1.12675941\n",
      "Step: [2386] d_loss: 0.43948826, g_loss: 1.09656286\n",
      "Step: [2387] d_loss: 0.44112000, g_loss: 1.08974969\n",
      "Step: [2388] d_loss: 0.44771704, g_loss: 1.11445773\n",
      "Step: [2389] d_loss: 0.44144183, g_loss: 1.07829654\n",
      "Step: [2390] d_loss: 0.44069260, g_loss: 1.14420485\n",
      "Step: [2391] d_loss: 0.43853649, g_loss: 1.10806501\n",
      "Step: [2392] d_loss: 0.44623455, g_loss: 1.12621880\n",
      "Step: [2393] d_loss: 0.44839907, g_loss: 1.11465251\n",
      "Step: [2394] d_loss: 0.44164523, g_loss: 1.10661304\n",
      "Step: [2395] d_loss: 0.43704244, g_loss: 1.10854840\n",
      "Step: [2396] d_loss: 0.43846652, g_loss: 1.12165272\n",
      "Step: [2397] d_loss: 0.43852124, g_loss: 1.10089993\n",
      "Step: [2398] d_loss: 0.44506121, g_loss: 1.14025986\n",
      "Step: [2399] d_loss: 0.44630262, g_loss: 1.10089338\n",
      "Step: [2400] d_loss: 0.45818201, g_loss: 1.12603605\n",
      "Step: [2401] d_loss: 0.45164782, g_loss: 1.06915677\n",
      "Step: [2402] d_loss: 0.44201007, g_loss: 1.10461068\n",
      "Step: [2403] d_loss: 0.44385818, g_loss: 1.12763655\n",
      "Step: [2404] d_loss: 0.44061431, g_loss: 1.12846839\n",
      "Step: [2405] d_loss: 0.43900427, g_loss: 1.12620807\n",
      "Step: [2406] d_loss: 0.44263801, g_loss: 1.13476408\n",
      "Step: [2407] d_loss: 0.44104677, g_loss: 1.13962054\n",
      "Step: [2408] d_loss: 0.44530278, g_loss: 1.07989800\n",
      "Step: [2409] d_loss: 0.44257525, g_loss: 1.14296937\n",
      "Step: [2410] d_loss: 0.44155240, g_loss: 1.11371219\n",
      "Step: [2411] d_loss: 0.44511086, g_loss: 1.12492144\n",
      "Step: [2412] d_loss: 0.45384520, g_loss: 1.08479512\n",
      "Step: [2413] d_loss: 0.45122534, g_loss: 1.08556414\n",
      "Step: [2414] d_loss: 0.45518848, g_loss: 1.10753548\n",
      "Step: [2415] d_loss: 0.44667646, g_loss: 1.13165641\n",
      "Step: [2416] d_loss: 0.44956982, g_loss: 1.09207976\n",
      "Step: [2417] d_loss: 0.45506331, g_loss: 1.08870888\n",
      "Step: [2418] d_loss: 0.44246808, g_loss: 1.13724327\n",
      "Step: [2419] d_loss: 0.43778619, g_loss: 1.09529459\n",
      "Step: [2420] d_loss: 0.44009045, g_loss: 1.09690082\n",
      "Step: [2421] d_loss: 0.45678639, g_loss: 1.11463046\n",
      "Step: [2422] d_loss: 0.45301184, g_loss: 1.12973022\n",
      "Step: [2423] d_loss: 0.43606356, g_loss: 1.11544120\n",
      "Step: [2424] d_loss: 0.45177427, g_loss: 1.11832440\n",
      "Step: [2425] d_loss: 0.45415455, g_loss: 1.13031304\n",
      "Step: [2426] d_loss: 0.44645357, g_loss: 1.14074504\n",
      "Step: [2427] d_loss: 0.46036479, g_loss: 1.10712063\n",
      "Step: [2428] d_loss: 0.45669085, g_loss: 1.13456416\n",
      "Step: [2429] d_loss: 0.44365937, g_loss: 1.11874020\n",
      "Step: [2430] d_loss: 0.44460544, g_loss: 1.12162960\n",
      "Step: [2431] d_loss: 0.45687699, g_loss: 1.12267864\n",
      "Step: [2432] d_loss: 0.44435874, g_loss: 1.10218751\n",
      "Step: [2433] d_loss: 0.44149134, g_loss: 1.11146498\n",
      "Step: [2434] d_loss: 0.44409585, g_loss: 1.13109040\n",
      "Step: [2435] d_loss: 0.44765958, g_loss: 1.07064342\n",
      "Step: [2436] d_loss: 0.44236624, g_loss: 1.11695516\n",
      "Step: [2437] d_loss: 0.44785437, g_loss: 1.08650827\n",
      "Step: [2438] d_loss: 0.45203045, g_loss: 1.13215184\n",
      "Step: [2439] d_loss: 0.44505668, g_loss: 1.08727789\n",
      "Step: [2440] d_loss: 0.46889731, g_loss: 1.14263701\n",
      "Step: [2441] d_loss: 0.44750246, g_loss: 1.15004957\n",
      "Step: [2442] d_loss: 0.43939042, g_loss: 1.10392630\n",
      "Step: [2443] d_loss: 0.44798994, g_loss: 1.15658498\n",
      "Step: [2444] d_loss: 0.44963577, g_loss: 1.11425662\n",
      "Step: [2445] d_loss: 0.44173723, g_loss: 1.13717937\n",
      "Step: [2446] d_loss: 0.44124705, g_loss: 1.15387511\n",
      "Step: [2447] d_loss: 0.44104430, g_loss: 1.12942541\n",
      "Step: [2448] d_loss: 0.44004038, g_loss: 1.11525118\n",
      "Step: [2449] d_loss: 0.44169995, g_loss: 1.11548901\n",
      "Step: [2450] d_loss: 0.44577229, g_loss: 1.10946465\n",
      "Step: [2451] d_loss: 0.44365701, g_loss: 1.12092328\n",
      "Step: [2452] d_loss: 0.44785762, g_loss: 1.12616587\n",
      "Step: [2453] d_loss: 0.43539086, g_loss: 1.11445308\n",
      "Step: [2454] d_loss: 0.43966436, g_loss: 1.10888684\n",
      "Step: [2455] d_loss: 0.44042867, g_loss: 1.12807047\n",
      "Step: [2456] d_loss: 0.45052731, g_loss: 1.13102770\n",
      "Step: [2457] d_loss: 0.44280741, g_loss: 1.11855662\n",
      "Step: [2458] d_loss: 0.45013696, g_loss: 1.13339925\n",
      "Step: [2459] d_loss: 0.43895870, g_loss: 1.12202668\n",
      "Step: [2460] d_loss: 0.45165491, g_loss: 1.15531456\n",
      "Step: [2461] d_loss: 0.43662938, g_loss: 1.14825058\n",
      "Step: [2462] d_loss: 0.44128799, g_loss: 1.12007928\n",
      "Step: [2463] d_loss: 0.45551994, g_loss: 1.16379428\n",
      "Step: [2464] d_loss: 0.44455472, g_loss: 1.15588129\n",
      "Step: [2465] d_loss: 0.44722542, g_loss: 1.12669861\n",
      "Step: [2466] d_loss: 0.45574552, g_loss: 1.17339492\n",
      "Step: [2467] d_loss: 0.44348750, g_loss: 1.15788746\n",
      "Step: [2468] d_loss: 0.45000368, g_loss: 1.12257576\n",
      "Step: [2469] d_loss: 0.44596294, g_loss: 1.16277289\n",
      "Step: [2470] d_loss: 0.43937480, g_loss: 1.12807608\n",
      "Step: [2471] d_loss: 0.44602081, g_loss: 1.12938595\n",
      "Step: [2472] d_loss: 0.44242367, g_loss: 1.11839175\n",
      "Step: [2473] d_loss: 0.44417506, g_loss: 1.11965919\n",
      "Step: [2474] d_loss: 0.43955764, g_loss: 1.15380144\n",
      "Step: [2475] d_loss: 0.43729895, g_loss: 1.14666605\n",
      "Step: [2476] d_loss: 0.43681246, g_loss: 1.11269593\n",
      "Step: [2477] d_loss: 0.43898335, g_loss: 1.14905393\n",
      "Step: [2478] d_loss: 0.44006920, g_loss: 1.09890974\n",
      "Step: [2479] d_loss: 0.44400388, g_loss: 1.13304853\n",
      "Step: [2480] d_loss: 0.43893990, g_loss: 1.11520505\n",
      "Step: [2481] d_loss: 0.44187498, g_loss: 1.12724507\n",
      "Step: [2482] d_loss: 0.46614683, g_loss: 1.11952639\n",
      "Step: [2483] d_loss: 0.45389494, g_loss: 1.18103576\n",
      "Step: [2484] d_loss: 0.44849461, g_loss: 1.09313869\n",
      "Step: [2485] d_loss: 0.45200771, g_loss: 1.10455668\n",
      "Step: [2486] d_loss: 0.44389340, g_loss: 1.13500500\n",
      "Step: [2487] d_loss: 0.44810370, g_loss: 1.07994854\n",
      "Step: [2488] d_loss: 0.44005311, g_loss: 1.13346112\n",
      "Step: [2489] d_loss: 0.44162619, g_loss: 1.12597179\n",
      "Step: [2490] d_loss: 0.44151491, g_loss: 1.09522688\n",
      "Step: [2491] d_loss: 0.44108242, g_loss: 1.12400043\n",
      "Step: [2492] d_loss: 0.44256404, g_loss: 1.10643911\n",
      "Step: [2493] d_loss: 0.43644071, g_loss: 1.11357188\n",
      "Step: [2494] d_loss: 0.44372264, g_loss: 1.11569011\n",
      "Step: [2495] d_loss: 0.43580031, g_loss: 1.09560812\n",
      "Step: [2496] d_loss: 0.44100931, g_loss: 1.09911847\n",
      "Step: [2497] d_loss: 0.44024730, g_loss: 1.11715972\n",
      "Step: [2498] d_loss: 0.44484144, g_loss: 1.10860205\n",
      "Step: [2499] d_loss: 0.43444014, g_loss: 1.12186849\n",
      "Step: [2500] d_loss: 0.44545919, g_loss: 1.12654042\n",
      "Step: [2501] d_loss: 0.44150880, g_loss: 1.13239658\n",
      "Step: [2502] d_loss: 0.43985441, g_loss: 1.10105884\n",
      "Step: [2503] d_loss: 0.44122440, g_loss: 1.14702833\n",
      "Step: [2504] d_loss: 0.43442106, g_loss: 1.13457716\n",
      "Step: [2505] d_loss: 0.43999732, g_loss: 1.12338841\n",
      "Step: [2506] d_loss: 0.43831733, g_loss: 1.10417080\n",
      "Step: [2507] d_loss: 0.43933564, g_loss: 1.09834957\n",
      "Step: [2508] d_loss: 0.43952572, g_loss: 1.11157382\n",
      "Step: [2509] d_loss: 0.44245213, g_loss: 1.06081593\n",
      "Step: [2510] d_loss: 0.44142908, g_loss: 1.11300516\n",
      "Step: [2511] d_loss: 0.43855762, g_loss: 1.10797846\n",
      "Step: [2512] d_loss: 0.44109744, g_loss: 1.08581221\n",
      "Step: [2513] d_loss: 0.43932140, g_loss: 1.09017909\n",
      "Step: [2514] d_loss: 0.44009465, g_loss: 1.10938668\n",
      "Step: [2515] d_loss: 0.44292921, g_loss: 1.05602884\n",
      "Step: [2516] d_loss: 0.43958220, g_loss: 1.07128584\n",
      "Step: [2517] d_loss: 0.43635601, g_loss: 1.09893417\n",
      "Step: [2518] d_loss: 0.45202211, g_loss: 1.09576404\n",
      "Step: [2519] d_loss: 0.44363502, g_loss: 1.11148131\n",
      "Step: [2520] d_loss: 0.44289511, g_loss: 1.07110643\n",
      "Step: [2521] d_loss: 0.44789147, g_loss: 1.12314034\n",
      "Step: [2522] d_loss: 0.43828288, g_loss: 1.07823527\n",
      "Step: [2523] d_loss: 0.43973166, g_loss: 1.07770538\n",
      "Step: [2524] d_loss: 0.44372278, g_loss: 1.05389059\n",
      "Step: [2525] d_loss: 0.45511159, g_loss: 1.14125097\n",
      "Step: [2526] d_loss: 0.47811869, g_loss: 1.10623419\n",
      "Step: [2527] d_loss: 0.46492410, g_loss: 1.14144385\n",
      "Step: [2528] d_loss: 0.44635236, g_loss: 1.13260877\n",
      "Step: [2529] d_loss: 0.44092572, g_loss: 1.13906503\n",
      "Step: [2530] d_loss: 0.43872559, g_loss: 1.12359416\n",
      "Step: [2531] d_loss: 0.43868977, g_loss: 1.06747127\n",
      "Step: [2532] d_loss: 0.43614933, g_loss: 1.08289158\n",
      "Step: [2533] d_loss: 0.44052565, g_loss: 1.14183259\n",
      "Step: [2534] d_loss: 0.43956336, g_loss: 1.10321116\n",
      "Step: [2535] d_loss: 0.43789595, g_loss: 1.13710260\n",
      "Step: [2536] d_loss: 0.43894053, g_loss: 1.11601329\n",
      "Step: [2537] d_loss: 0.43720084, g_loss: 1.11080635\n",
      "Step: [2538] d_loss: 0.43672544, g_loss: 1.12913334\n",
      "Step: [2539] d_loss: 0.44167960, g_loss: 1.08141530\n",
      "Step: [2540] d_loss: 0.44481722, g_loss: 1.08831787\n",
      "Step: [2541] d_loss: 0.44275823, g_loss: 1.11809206\n",
      "Step: [2542] d_loss: 0.44107303, g_loss: 1.11458659\n",
      "Step: [2543] d_loss: 0.43357489, g_loss: 1.15915000\n",
      "Step: [2544] d_loss: 0.43700752, g_loss: 1.09466887\n",
      "Step: [2545] d_loss: 0.43967134, g_loss: 1.08924758\n",
      "Step: [2546] d_loss: 0.44630477, g_loss: 1.06860816\n",
      "Step: [2547] d_loss: 0.44263792, g_loss: 1.08059657\n",
      "Step: [2548] d_loss: 0.44069722, g_loss: 1.08964813\n",
      "Step: [2549] d_loss: 0.44094211, g_loss: 1.14299524\n",
      "Step: [2550] d_loss: 0.44845498, g_loss: 1.05334044\n",
      "Step: [2551] d_loss: 0.45908725, g_loss: 1.11537874\n",
      "Step: [2552] d_loss: 0.44534388, g_loss: 1.11271322\n",
      "Step: [2553] d_loss: 0.44940341, g_loss: 1.13012171\n",
      "Step: [2554] d_loss: 0.44320148, g_loss: 1.10279191\n",
      "Step: [2555] d_loss: 0.44442186, g_loss: 1.12307405\n",
      "Step: [2556] d_loss: 0.45226124, g_loss: 1.09961140\n",
      "Step: [2557] d_loss: 0.44639596, g_loss: 1.11882937\n",
      "Step: [2558] d_loss: 0.45315519, g_loss: 1.11814296\n",
      "Step: [2559] d_loss: 0.44542006, g_loss: 1.12604642\n",
      "Step: [2560] d_loss: 0.44640887, g_loss: 1.10814071\n",
      "Step: [2561] d_loss: 0.44146848, g_loss: 1.12923622\n",
      "Step: [2562] d_loss: 0.43961456, g_loss: 1.10920382\n",
      "Step: [2563] d_loss: 0.43808103, g_loss: 1.11184001\n",
      "Step: [2564] d_loss: 0.44234481, g_loss: 1.11041749\n",
      "Step: [2565] d_loss: 0.43788314, g_loss: 1.11063361\n",
      "Step: [2566] d_loss: 0.43893558, g_loss: 1.12555242\n",
      "Step: [2567] d_loss: 0.44086495, g_loss: 1.12592387\n",
      "Step: [2568] d_loss: 0.46008152, g_loss: 1.14654744\n",
      "Step: [2569] d_loss: 0.44029951, g_loss: 1.11114264\n",
      "Step: [2570] d_loss: 0.45817119, g_loss: 1.10720098\n",
      "Step: [2571] d_loss: 0.45055306, g_loss: 1.09869218\n",
      "Step: [2572] d_loss: 0.44000199, g_loss: 1.13119233\n",
      "Step: [2573] d_loss: 0.45186865, g_loss: 1.08684528\n",
      "Step: [2574] d_loss: 0.44610122, g_loss: 1.09004033\n",
      "Step: [2575] d_loss: 0.44366610, g_loss: 1.08204734\n",
      "Step: [2576] d_loss: 0.44142130, g_loss: 1.09632039\n",
      "Step: [2577] d_loss: 0.43761817, g_loss: 1.12069571\n",
      "Step: [2578] d_loss: 0.44303772, g_loss: 1.09227610\n",
      "Step: [2579] d_loss: 0.43963766, g_loss: 1.08185613\n",
      "Step: [2580] d_loss: 0.44043860, g_loss: 1.12835932\n",
      "Step: [2581] d_loss: 0.43954206, g_loss: 1.07301474\n",
      "Step: [2582] d_loss: 0.43961340, g_loss: 1.09897101\n",
      "Step: [2583] d_loss: 0.43880293, g_loss: 1.10870206\n",
      "Step: [2584] d_loss: 0.43564662, g_loss: 1.11183417\n",
      "Step: [2585] d_loss: 0.44771421, g_loss: 1.11213505\n",
      "Step: [2586] d_loss: 0.43609756, g_loss: 1.13822126\n",
      "Step: [2587] d_loss: 0.44405016, g_loss: 1.07189882\n",
      "Step: [2588] d_loss: 0.43980759, g_loss: 1.10165393\n",
      "Step: [2589] d_loss: 0.44298452, g_loss: 1.12633514\n",
      "Step: [2590] d_loss: 0.44062519, g_loss: 1.13203311\n",
      "Step: [2591] d_loss: 0.43953875, g_loss: 1.14528775\n",
      "Step: [2592] d_loss: 0.44142628, g_loss: 1.10813475\n",
      "Step: [2593] d_loss: 0.43760017, g_loss: 1.14278972\n",
      "Step: [2594] d_loss: 0.43462104, g_loss: 1.12786937\n",
      "Step: [2595] d_loss: 0.44657290, g_loss: 1.10730338\n",
      "Step: [2596] d_loss: 0.43694365, g_loss: 1.12622631\n",
      "Step: [2597] d_loss: 0.44032383, g_loss: 1.11332726\n",
      "Step: [2598] d_loss: 0.44279504, g_loss: 1.10445082\n",
      "Step: [2599] d_loss: 0.43927005, g_loss: 1.10556614\n",
      "Step: [2600] d_loss: 0.43935192, g_loss: 1.10939229\n",
      "Step: [2601] d_loss: 0.44115642, g_loss: 1.12558568\n",
      "Step: [2602] d_loss: 0.43818966, g_loss: 1.05634224\n",
      "Step: [2603] d_loss: 0.43315202, g_loss: 1.08762372\n",
      "Step: [2604] d_loss: 0.43950462, g_loss: 1.13268518\n",
      "Step: [2605] d_loss: 0.43873495, g_loss: 1.15060592\n",
      "Step: [2606] d_loss: 0.43624690, g_loss: 1.10628021\n",
      "Step: [2607] d_loss: 0.43723401, g_loss: 1.11071801\n",
      "Step: [2608] d_loss: 0.44824782, g_loss: 1.11490309\n",
      "Step: [2609] d_loss: 0.44532898, g_loss: 1.12387931\n",
      "Step: [2610] d_loss: 0.44216335, g_loss: 1.12424862\n",
      "Step: [2611] d_loss: 0.43895021, g_loss: 1.13031328\n",
      "Step: [2612] d_loss: 0.44401577, g_loss: 1.12227952\n",
      "Step: [2613] d_loss: 0.44083124, g_loss: 1.14482474\n",
      "Step: [2614] d_loss: 0.44599175, g_loss: 1.14925885\n",
      "Step: [2615] d_loss: 0.44291839, g_loss: 1.14415872\n",
      "Step: [2616] d_loss: 0.44868895, g_loss: 1.09534156\n",
      "Step: [2617] d_loss: 0.44106132, g_loss: 1.14548254\n",
      "Step: [2618] d_loss: 0.44010022, g_loss: 1.13296533\n",
      "Step: [2619] d_loss: 0.43840003, g_loss: 1.13879585\n",
      "Step: [2620] d_loss: 0.44144043, g_loss: 1.09837699\n",
      "Step: [2621] d_loss: 0.44013414, g_loss: 1.10749435\n",
      "Step: [2622] d_loss: 0.43753588, g_loss: 1.10260904\n",
      "Step: [2623] d_loss: 0.43750373, g_loss: 1.14831293\n",
      "Step: [2624] d_loss: 0.44898498, g_loss: 1.12607777\n",
      "Step: [2625] d_loss: 0.44298238, g_loss: 1.11265397\n",
      "Step: [2626] d_loss: 0.44590336, g_loss: 1.10998976\n",
      "Step: [2627] d_loss: 0.44384009, g_loss: 1.12954319\n",
      "Step: [2628] d_loss: 0.43856525, g_loss: 1.13751078\n",
      "Step: [2629] d_loss: 0.43656570, g_loss: 1.16432536\n",
      "Step: [2630] d_loss: 0.43578207, g_loss: 1.12890673\n",
      "Step: [2631] d_loss: 0.43577403, g_loss: 1.15236616\n",
      "Step: [2632] d_loss: 0.43655208, g_loss: 1.12386024\n",
      "Step: [2633] d_loss: 0.44026899, g_loss: 1.14713418\n",
      "Step: [2634] d_loss: 0.44497022, g_loss: 1.08550680\n",
      "Step: [2635] d_loss: 0.43469521, g_loss: 1.13185227\n",
      "Step: [2636] d_loss: 0.43796119, g_loss: 1.12358046\n",
      "Step: [2637] d_loss: 0.43646991, g_loss: 1.13705218\n",
      "Step: [2638] d_loss: 0.44673502, g_loss: 1.13655102\n",
      "Step: [2639] d_loss: 0.45194858, g_loss: 1.09720314\n",
      "Step: [2640] d_loss: 0.44099918, g_loss: 1.13709021\n",
      "Step: [2641] d_loss: 0.44313598, g_loss: 1.16214371\n",
      "Step: [2642] d_loss: 0.45003894, g_loss: 1.12358177\n",
      "Step: [2643] d_loss: 0.44068038, g_loss: 1.13570428\n",
      "Step: [2644] d_loss: 0.44053826, g_loss: 1.12650859\n",
      "Step: [2645] d_loss: 0.43897337, g_loss: 1.12601924\n",
      "Step: [2646] d_loss: 0.45034400, g_loss: 1.09618366\n",
      "Step: [2647] d_loss: 0.43978703, g_loss: 1.13676286\n",
      "Step: [2648] d_loss: 0.43894461, g_loss: 1.09844756\n",
      "Step: [2649] d_loss: 0.44547370, g_loss: 1.06049657\n",
      "Step: [2650] d_loss: 0.44389442, g_loss: 1.12569141\n",
      "Step: [2651] d_loss: 0.43594840, g_loss: 1.09948802\n",
      "Step: [2652] d_loss: 0.43870360, g_loss: 1.12081730\n",
      "Step: [2653] d_loss: 0.44175389, g_loss: 1.11367512\n",
      "Step: [2654] d_loss: 0.44000992, g_loss: 1.09811509\n",
      "Step: [2655] d_loss: 0.44362968, g_loss: 1.13169348\n",
      "Step: [2656] d_loss: 0.44325879, g_loss: 1.11814272\n",
      "Step: [2657] d_loss: 0.43692160, g_loss: 1.10038221\n",
      "Step: [2658] d_loss: 0.43573561, g_loss: 1.10279608\n",
      "Step: [2659] d_loss: 0.44148284, g_loss: 1.06637144\n",
      "Step: [2660] d_loss: 0.43670964, g_loss: 1.09673369\n",
      "Step: [2661] d_loss: 0.44257089, g_loss: 1.09705734\n",
      "Step: [2662] d_loss: 0.43751109, g_loss: 1.10567451\n",
      "Step: [2663] d_loss: 0.43545219, g_loss: 1.11438501\n",
      "Step: [2664] d_loss: 0.43501422, g_loss: 1.10264564\n",
      "Step: [2665] d_loss: 0.43502501, g_loss: 1.12024367\n",
      "Step: [2666] d_loss: 0.43679565, g_loss: 1.08387947\n",
      "Step: [2667] d_loss: 0.43408248, g_loss: 1.10496831\n",
      "Step: [2668] d_loss: 0.43489134, g_loss: 1.09714520\n",
      "Step: [2669] d_loss: 0.43699187, g_loss: 1.09999144\n",
      "Step: [2670] d_loss: 0.44119486, g_loss: 1.08042777\n",
      "Step: [2671] d_loss: 0.44207317, g_loss: 1.08256066\n",
      "Step: [2672] d_loss: 0.43798643, g_loss: 1.07036579\n",
      "Step: [2673] d_loss: 0.43956518, g_loss: 1.07994175\n",
      "Step: [2674] d_loss: 0.44499743, g_loss: 1.09146821\n",
      "Step: [2675] d_loss: 0.43843278, g_loss: 1.10393262\n",
      "Step: [2676] d_loss: 0.43479732, g_loss: 1.11170864\n",
      "Step: [2677] d_loss: 0.43840480, g_loss: 1.10552859\n",
      "Step: [2678] d_loss: 0.44094813, g_loss: 1.10529697\n",
      "Step: [2679] d_loss: 0.43775463, g_loss: 1.04773605\n",
      "Step: [2680] d_loss: 0.44079050, g_loss: 1.08596444\n",
      "Step: [2681] d_loss: 0.43612236, g_loss: 1.07369602\n",
      "Step: [2682] d_loss: 0.43965667, g_loss: 1.10138226\n",
      "Step: [2683] d_loss: 0.44109330, g_loss: 1.06322050\n",
      "Step: [2684] d_loss: 0.43962091, g_loss: 1.10627544\n",
      "Step: [2685] d_loss: 0.43277156, g_loss: 1.09816587\n",
      "Step: [2686] d_loss: 0.43235132, g_loss: 1.11136317\n",
      "Step: [2687] d_loss: 0.43837970, g_loss: 1.10247958\n",
      "Step: [2688] d_loss: 0.43728483, g_loss: 1.09094214\n",
      "Step: [2689] d_loss: 0.43577701, g_loss: 1.10309148\n",
      "Step: [2690] d_loss: 0.44274786, g_loss: 1.11288738\n",
      "Step: [2691] d_loss: 0.43745565, g_loss: 1.10892415\n",
      "Step: [2692] d_loss: 0.44254631, g_loss: 1.10254288\n",
      "Step: [2693] d_loss: 0.43736327, g_loss: 1.12550247\n",
      "Step: [2694] d_loss: 0.44691846, g_loss: 1.13210106\n",
      "Step: [2695] d_loss: 0.44562674, g_loss: 1.09032989\n",
      "Step: [2696] d_loss: 0.44038010, g_loss: 1.11425507\n",
      "Step: [2697] d_loss: 0.45437172, g_loss: 1.13057864\n",
      "Step: [2698] d_loss: 0.43810827, g_loss: 1.10990810\n",
      "Step: [2699] d_loss: 0.43714300, g_loss: 1.08211422\n",
      "Step: [2700] d_loss: 0.43781310, g_loss: 1.06465101\n",
      "Step: [2701] d_loss: 0.43953723, g_loss: 1.11968160\n",
      "Step: [2702] d_loss: 0.43542090, g_loss: 1.08533967\n",
      "Step: [2703] d_loss: 0.43798783, g_loss: 1.10507762\n",
      "Step: [2704] d_loss: 0.43303928, g_loss: 1.11229539\n",
      "Step: [2705] d_loss: 0.44373125, g_loss: 1.06213140\n",
      "Step: [2706] d_loss: 0.43728927, g_loss: 1.09998488\n",
      "Step: [2707] d_loss: 0.43751675, g_loss: 1.08379173\n",
      "Step: [2708] d_loss: 0.43489611, g_loss: 1.10302603\n",
      "Step: [2709] d_loss: 0.44119954, g_loss: 1.10074854\n",
      "Step: [2710] d_loss: 0.43449831, g_loss: 1.11042941\n",
      "Step: [2711] d_loss: 0.43483078, g_loss: 1.09280503\n",
      "Step: [2712] d_loss: 0.44254330, g_loss: 1.09202576\n",
      "Step: [2713] d_loss: 0.43600938, g_loss: 1.04714632\n",
      "Step: [2714] d_loss: 0.44051957, g_loss: 1.07040274\n",
      "Step: [2715] d_loss: 0.43797511, g_loss: 1.12986076\n",
      "Step: [2716] d_loss: 0.44283372, g_loss: 1.05404568\n",
      "Step: [2717] d_loss: 0.43565920, g_loss: 1.12952602\n",
      "Step: [2718] d_loss: 0.44396040, g_loss: 1.12870824\n",
      "Step: [2719] d_loss: 0.43969348, g_loss: 1.10708773\n",
      "Step: [2720] d_loss: 0.44512883, g_loss: 1.09425652\n",
      "Step: [2721] d_loss: 0.43167529, g_loss: 1.11626709\n",
      "Step: [2722] d_loss: 0.43812314, g_loss: 1.13318813\n",
      "Step: [2723] d_loss: 0.43108299, g_loss: 1.12552285\n",
      "Step: [2724] d_loss: 0.43685719, g_loss: 1.08328092\n",
      "Step: [2725] d_loss: 0.43557519, g_loss: 1.08498979\n",
      "Step: [2726] d_loss: 0.43528798, g_loss: 1.11390376\n",
      "Step: [2727] d_loss: 0.43513614, g_loss: 1.11162686\n",
      "Step: [2728] d_loss: 0.43403280, g_loss: 1.10841548\n",
      "Step: [2729] d_loss: 0.43594298, g_loss: 1.11523759\n",
      "Step: [2730] d_loss: 0.43990508, g_loss: 1.08592582\n",
      "Step: [2731] d_loss: 0.43853781, g_loss: 1.12508035\n",
      "Step: [2732] d_loss: 0.43577674, g_loss: 1.06092668\n",
      "Step: [2733] d_loss: 0.44993308, g_loss: 1.10391557\n",
      "Step: [2734] d_loss: 0.43863946, g_loss: 1.13161337\n",
      "Step: [2735] d_loss: 0.43995774, g_loss: 1.09672320\n",
      "Step: [2736] d_loss: 0.43875164, g_loss: 1.12386715\n",
      "Step: [2737] d_loss: 0.45464593, g_loss: 1.12734175\n",
      "Step: [2738] d_loss: 0.44304138, g_loss: 1.08299756\n",
      "Step: [2739] d_loss: 0.44426170, g_loss: 1.13727725\n",
      "Step: [2740] d_loss: 0.44047189, g_loss: 1.09288907\n",
      "Step: [2741] d_loss: 0.43949535, g_loss: 1.12488699\n",
      "Step: [2742] d_loss: 0.43781728, g_loss: 1.07337070\n",
      "Step: [2743] d_loss: 0.44088444, g_loss: 1.12726951\n",
      "Step: [2744] d_loss: 0.43319935, g_loss: 1.13136721\n",
      "Step: [2745] d_loss: 0.43860677, g_loss: 1.11224771\n",
      "Step: [2746] d_loss: 0.43470684, g_loss: 1.09460294\n",
      "Step: [2747] d_loss: 0.43654546, g_loss: 1.05766892\n",
      "Step: [2748] d_loss: 0.44067422, g_loss: 1.10787141\n",
      "Step: [2749] d_loss: 0.44295534, g_loss: 1.09403324\n",
      "Step: [2750] d_loss: 0.43991458, g_loss: 1.07993579\n",
      "Step: [2751] d_loss: 0.43915349, g_loss: 1.14348972\n",
      "Step: [2752] d_loss: 0.43318301, g_loss: 1.13124835\n",
      "Step: [2753] d_loss: 0.43809262, g_loss: 1.07211125\n",
      "Step: [2754] d_loss: 0.43882379, g_loss: 1.12371206\n",
      "Step: [2755] d_loss: 0.44322398, g_loss: 1.11790776\n",
      "Step: [2756] d_loss: 0.43574426, g_loss: 1.12830567\n",
      "Step: [2757] d_loss: 0.43954068, g_loss: 1.13140535\n",
      "Step: [2758] d_loss: 0.43829072, g_loss: 1.16079342\n",
      "Step: [2759] d_loss: 0.43807036, g_loss: 1.16466546\n",
      "Step: [2760] d_loss: 0.43449467, g_loss: 1.09855926\n",
      "Step: [2761] d_loss: 0.43922099, g_loss: 1.11436450\n",
      "Step: [2762] d_loss: 0.43907398, g_loss: 1.12136900\n",
      "Step: [2763] d_loss: 0.43907833, g_loss: 1.10746849\n",
      "Step: [2764] d_loss: 0.43214738, g_loss: 1.11541188\n",
      "Step: [2765] d_loss: 0.43585455, g_loss: 1.10597956\n",
      "Step: [2766] d_loss: 0.43833479, g_loss: 1.10051560\n",
      "Step: [2767] d_loss: 0.43787244, g_loss: 1.10461712\n",
      "Step: [2768] d_loss: 0.44446486, g_loss: 1.13381302\n",
      "Step: [2769] d_loss: 0.43567300, g_loss: 1.09040427\n",
      "Step: [2770] d_loss: 0.43981484, g_loss: 1.12416792\n",
      "Step: [2771] d_loss: 0.43846315, g_loss: 1.08834171\n",
      "Step: [2772] d_loss: 0.43811843, g_loss: 1.13815463\n",
      "Step: [2773] d_loss: 0.43679258, g_loss: 1.11169577\n",
      "Step: [2774] d_loss: 0.43631214, g_loss: 1.09909773\n",
      "Step: [2775] d_loss: 0.43962398, g_loss: 1.13520324\n",
      "Step: [2776] d_loss: 0.44874570, g_loss: 1.12676811\n",
      "Step: [2777] d_loss: 0.43773642, g_loss: 1.10066450\n",
      "Step: [2778] d_loss: 0.43394071, g_loss: 1.14606726\n",
      "Step: [2779] d_loss: 0.43721977, g_loss: 1.12793159\n",
      "Step: [2780] d_loss: 0.43820974, g_loss: 1.10279584\n",
      "Step: [2781] d_loss: 0.43568185, g_loss: 1.10634100\n",
      "Step: [2782] d_loss: 0.43510002, g_loss: 1.11434138\n",
      "Step: [2783] d_loss: 0.43202907, g_loss: 1.12154031\n",
      "Step: [2784] d_loss: 0.44037706, g_loss: 1.14855373\n",
      "Step: [2785] d_loss: 0.43751350, g_loss: 1.11721325\n",
      "Step: [2786] d_loss: 0.43386406, g_loss: 1.13884747\n",
      "Step: [2787] d_loss: 0.43324465, g_loss: 1.12175286\n",
      "Step: [2788] d_loss: 0.43363631, g_loss: 1.11188793\n",
      "Step: [2789] d_loss: 0.43988767, g_loss: 1.14121068\n",
      "Step: [2790] d_loss: 0.43490052, g_loss: 1.13085079\n",
      "Step: [2791] d_loss: 0.43484107, g_loss: 1.11747050\n",
      "Step: [2792] d_loss: 0.43440178, g_loss: 1.10426700\n",
      "Step: [2793] d_loss: 0.43131101, g_loss: 1.12627149\n",
      "Step: [2794] d_loss: 0.44395757, g_loss: 1.12738431\n",
      "Step: [2795] d_loss: 0.43870294, g_loss: 1.17216384\n",
      "Step: [2796] d_loss: 0.43672231, g_loss: 1.11505187\n",
      "Step: [2797] d_loss: 0.43535787, g_loss: 1.10901153\n",
      "Step: [2798] d_loss: 0.43532199, g_loss: 1.11989701\n",
      "Step: [2799] d_loss: 0.43279889, g_loss: 1.11985207\n",
      "Step: [2800] d_loss: 0.44092304, g_loss: 1.12990451\n",
      "Step: [2801] d_loss: 0.44126862, g_loss: 1.10675251\n",
      "Step: [2802] d_loss: 0.44819772, g_loss: 1.11697495\n",
      "Step: [2803] d_loss: 0.44365644, g_loss: 1.13617218\n",
      "Step: [2804] d_loss: 0.43814167, g_loss: 1.11516356\n",
      "Step: [2805] d_loss: 0.43305010, g_loss: 1.13241029\n",
      "Step: [2806] d_loss: 0.43473053, g_loss: 1.15872097\n",
      "Step: [2807] d_loss: 0.43463334, g_loss: 1.12737834\n",
      "Step: [2808] d_loss: 0.43078253, g_loss: 1.13198936\n",
      "Step: [2809] d_loss: 0.44386774, g_loss: 1.11768866\n",
      "Step: [2810] d_loss: 0.43539247, g_loss: 1.12485433\n",
      "Step: [2811] d_loss: 0.44000211, g_loss: 1.14500487\n",
      "Step: [2812] d_loss: 0.44441065, g_loss: 1.13533974\n",
      "Step: [2813] d_loss: 0.44323063, g_loss: 1.08138835\n",
      "Step: [2814] d_loss: 0.44586477, g_loss: 1.12083864\n",
      "Step: [2815] d_loss: 0.44629884, g_loss: 1.08852470\n",
      "Step: [2816] d_loss: 0.43965405, g_loss: 1.14438319\n",
      "Step: [2817] d_loss: 0.43930387, g_loss: 1.13392222\n",
      "Step: [2818] d_loss: 0.43655095, g_loss: 1.11327624\n",
      "Step: [2819] d_loss: 0.43649220, g_loss: 1.08007717\n",
      "Step: [2820] d_loss: 0.43832844, g_loss: 1.09724081\n",
      "Step: [2821] d_loss: 0.43700579, g_loss: 1.11985278\n",
      "Step: [2822] d_loss: 0.43818536, g_loss: 1.11898804\n",
      "Step: [2823] d_loss: 0.43571037, g_loss: 1.09789538\n",
      "Step: [2824] d_loss: 0.43133381, g_loss: 1.12158406\n",
      "Step: [2825] d_loss: 0.43379542, g_loss: 1.10005915\n",
      "Step: [2826] d_loss: 0.43575403, g_loss: 1.10244024\n",
      "Step: [2827] d_loss: 0.44018173, g_loss: 1.16079712\n",
      "Step: [2828] d_loss: 0.43202022, g_loss: 1.10259140\n",
      "Step: [2829] d_loss: 0.43398112, g_loss: 1.11832762\n",
      "Step: [2830] d_loss: 0.43057612, g_loss: 1.11362672\n",
      "Step: [2831] d_loss: 0.43203789, g_loss: 1.08395600\n",
      "Step: [2832] d_loss: 0.43304300, g_loss: 1.12076008\n",
      "Step: [2833] d_loss: 0.43309000, g_loss: 1.10615957\n",
      "Step: [2834] d_loss: 0.43493760, g_loss: 1.10775352\n",
      "Step: [2835] d_loss: 0.44248924, g_loss: 1.12272465\n",
      "Step: [2836] d_loss: 0.43421605, g_loss: 1.10700071\n",
      "Step: [2837] d_loss: 0.43065822, g_loss: 1.07085216\n",
      "Step: [2838] d_loss: 0.43436649, g_loss: 1.09788597\n",
      "Step: [2839] d_loss: 0.43626624, g_loss: 1.09471142\n",
      "Step: [2840] d_loss: 0.43599731, g_loss: 1.06287360\n",
      "Step: [2841] d_loss: 0.43933347, g_loss: 1.07422197\n",
      "Step: [2842] d_loss: 0.44324151, g_loss: 1.09558368\n",
      "Step: [2843] d_loss: 0.43577915, g_loss: 1.04715323\n",
      "Step: [2844] d_loss: 0.43220562, g_loss: 1.08311021\n",
      "Step: [2845] d_loss: 0.44624421, g_loss: 1.11538398\n",
      "Step: [2846] d_loss: 0.44221959, g_loss: 1.11120403\n",
      "Step: [2847] d_loss: 0.43889165, g_loss: 1.12624109\n",
      "Step: [2848] d_loss: 0.43909863, g_loss: 1.07213640\n",
      "Step: [2849] d_loss: 0.43814391, g_loss: 1.09005070\n",
      "Step: [2850] d_loss: 0.43728843, g_loss: 1.11122417\n",
      "Step: [2851] d_loss: 0.43619925, g_loss: 1.09704936\n",
      "Step: [2852] d_loss: 0.43404540, g_loss: 1.13790250\n",
      "Step: [2853] d_loss: 0.44053829, g_loss: 1.11283338\n",
      "Step: [2854] d_loss: 0.44266459, g_loss: 1.06744230\n",
      "Step: [2855] d_loss: 0.44054097, g_loss: 1.09690595\n",
      "Step: [2856] d_loss: 0.44683346, g_loss: 1.12144113\n",
      "Step: [2857] d_loss: 0.44050157, g_loss: 1.10987794\n",
      "Step: [2858] d_loss: 0.43415904, g_loss: 1.12083375\n",
      "Step: [2859] d_loss: 0.44031018, g_loss: 1.06621540\n",
      "Step: [2860] d_loss: 0.43823776, g_loss: 1.09191120\n",
      "Step: [2861] d_loss: 0.43455341, g_loss: 1.10609269\n",
      "Step: [2862] d_loss: 0.43955484, g_loss: 1.06156445\n",
      "Step: [2863] d_loss: 0.43401638, g_loss: 1.11966622\n",
      "Step: [2864] d_loss: 0.43270075, g_loss: 1.11779201\n",
      "Step: [2865] d_loss: 0.43416190, g_loss: 1.10673034\n",
      "Step: [2866] d_loss: 0.43279609, g_loss: 1.11625099\n",
      "Step: [2867] d_loss: 0.43218470, g_loss: 1.07833970\n",
      "Step: [2868] d_loss: 0.43763137, g_loss: 1.07419622\n",
      "Step: [2869] d_loss: 0.44716892, g_loss: 1.10241854\n",
      "Step: [2870] d_loss: 0.43421528, g_loss: 1.07278824\n",
      "Step: [2871] d_loss: 0.43785360, g_loss: 1.10428059\n",
      "Step: [2872] d_loss: 0.43539655, g_loss: 1.09827781\n",
      "Step: [2873] d_loss: 0.44129223, g_loss: 1.12370658\n",
      "Step: [2874] d_loss: 0.43953800, g_loss: 1.06232965\n",
      "Step: [2875] d_loss: 0.44069999, g_loss: 1.14119971\n",
      "Step: [2876] d_loss: 0.43566632, g_loss: 1.11684668\n",
      "Step: [2877] d_loss: 0.43136233, g_loss: 1.10140634\n",
      "Step: [2878] d_loss: 0.43885183, g_loss: 1.10403645\n",
      "Step: [2879] d_loss: 0.43852213, g_loss: 1.10914266\n",
      "Step: [2880] d_loss: 0.44292808, g_loss: 1.10670280\n",
      "Step: [2881] d_loss: 0.43849987, g_loss: 1.08122778\n",
      "Step: [2882] d_loss: 0.44599232, g_loss: 1.10225320\n",
      "Step: [2883] d_loss: 0.43827581, g_loss: 1.06873274\n",
      "Step: [2884] d_loss: 0.43428987, g_loss: 1.10488904\n",
      "Step: [2885] d_loss: 0.43701801, g_loss: 1.12291992\n",
      "Step: [2886] d_loss: 0.43816629, g_loss: 1.08750176\n",
      "Step: [2887] d_loss: 0.43648830, g_loss: 1.13282406\n",
      "Step: [2888] d_loss: 0.44369468, g_loss: 1.10449934\n",
      "Step: [2889] d_loss: 0.44101697, g_loss: 1.12164736\n",
      "Step: [2890] d_loss: 0.43944666, g_loss: 1.09753036\n",
      "Step: [2891] d_loss: 0.44285327, g_loss: 1.12454462\n",
      "Step: [2892] d_loss: 0.43985984, g_loss: 1.08824921\n",
      "Step: [2893] d_loss: 0.43519258, g_loss: 1.14090765\n",
      "Step: [2894] d_loss: 0.44064078, g_loss: 1.13630843\n",
      "Step: [2895] d_loss: 0.43742296, g_loss: 1.11614311\n",
      "Step: [2896] d_loss: 0.43385050, g_loss: 1.14625454\n",
      "Step: [2897] d_loss: 0.43626359, g_loss: 1.10266399\n",
      "Step: [2898] d_loss: 0.43937898, g_loss: 1.13683808\n",
      "Step: [2899] d_loss: 0.45547462, g_loss: 1.10499406\n",
      "Step: [2900] d_loss: 0.45024091, g_loss: 1.13331175\n",
      "Step: [2901] d_loss: 0.43819034, g_loss: 1.10172975\n",
      "Step: [2902] d_loss: 0.43643287, g_loss: 1.09377730\n",
      "Step: [2903] d_loss: 0.43265706, g_loss: 1.10604846\n",
      "Step: [2904] d_loss: 0.43911824, g_loss: 1.10140383\n",
      "Step: [2905] d_loss: 0.43212971, g_loss: 1.07615900\n",
      "Step: [2906] d_loss: 0.43892819, g_loss: 1.12373817\n",
      "Step: [2907] d_loss: 0.43675560, g_loss: 1.13442302\n",
      "Step: [2908] d_loss: 0.43536684, g_loss: 1.10385883\n",
      "Step: [2909] d_loss: 0.43506277, g_loss: 1.12160063\n",
      "Step: [2910] d_loss: 0.43147439, g_loss: 1.08629870\n",
      "Step: [2911] d_loss: 0.43765149, g_loss: 1.12110043\n",
      "Step: [2912] d_loss: 0.43514258, g_loss: 1.11612809\n",
      "Step: [2913] d_loss: 0.43394339, g_loss: 1.10080624\n",
      "Step: [2914] d_loss: 0.43322438, g_loss: 1.15375876\n",
      "Step: [2915] d_loss: 0.42984077, g_loss: 1.12419021\n",
      "Step: [2916] d_loss: 0.43286225, g_loss: 1.14791799\n",
      "Step: [2917] d_loss: 0.43724179, g_loss: 1.15666926\n",
      "Step: [2918] d_loss: 0.43910354, g_loss: 1.12279797\n",
      "Step: [2919] d_loss: 0.44354469, g_loss: 1.14380014\n",
      "Step: [2920] d_loss: 0.43161210, g_loss: 1.10002303\n",
      "Step: [2921] d_loss: 0.43718883, g_loss: 1.13077188\n",
      "Step: [2922] d_loss: 0.43720868, g_loss: 1.16680300\n",
      "Step: [2923] d_loss: 0.43620014, g_loss: 1.15886903\n",
      "Step: [2924] d_loss: 0.43237126, g_loss: 1.13218927\n",
      "Step: [2925] d_loss: 0.43543315, g_loss: 1.08975148\n",
      "Step: [2926] d_loss: 0.43993604, g_loss: 1.14220893\n",
      "Step: [2927] d_loss: 0.44308507, g_loss: 1.11866629\n",
      "Step: [2928] d_loss: 0.44385689, g_loss: 1.12944126\n",
      "Step: [2929] d_loss: 0.43205288, g_loss: 1.15433311\n",
      "Step: [2930] d_loss: 0.43595290, g_loss: 1.10725367\n",
      "Step: [2931] d_loss: 0.43834913, g_loss: 1.14102829\n",
      "Step: [2932] d_loss: 0.43914378, g_loss: 1.11896253\n",
      "Step: [2933] d_loss: 0.43716824, g_loss: 1.15300930\n",
      "Step: [2934] d_loss: 0.43807909, g_loss: 1.14553213\n",
      "Step: [2935] d_loss: 0.43175381, g_loss: 1.08985615\n",
      "Step: [2936] d_loss: 0.43305260, g_loss: 1.14824378\n",
      "Step: [2937] d_loss: 0.44217041, g_loss: 1.14720750\n",
      "Step: [2938] d_loss: 0.44060576, g_loss: 1.10021782\n",
      "Step: [2939] d_loss: 0.43256480, g_loss: 1.06455863\n",
      "Step: [2940] d_loss: 0.43253127, g_loss: 1.10800922\n",
      "Step: [2941] d_loss: 0.43356779, g_loss: 1.08686042\n",
      "Step: [2942] d_loss: 0.44025961, g_loss: 1.15439057\n",
      "Step: [2943] d_loss: 0.44346496, g_loss: 1.06434846\n",
      "Step: [2944] d_loss: 0.43882167, g_loss: 1.11325324\n",
      "Step: [2945] d_loss: 0.43641093, g_loss: 1.08941412\n",
      "Step: [2946] d_loss: 0.43155947, g_loss: 1.07369959\n",
      "Step: [2947] d_loss: 0.43135267, g_loss: 1.11721385\n",
      "Step: [2948] d_loss: 0.43276730, g_loss: 1.10473871\n",
      "Step: [2949] d_loss: 0.44174173, g_loss: 1.12115109\n",
      "Step: [2950] d_loss: 0.43737939, g_loss: 1.07974064\n",
      "Step: [2951] d_loss: 0.43154326, g_loss: 1.10037518\n",
      "Step: [2952] d_loss: 0.43247086, g_loss: 1.08815706\n",
      "Step: [2953] d_loss: 0.43922979, g_loss: 1.10884416\n",
      "Step: [2954] d_loss: 0.43586898, g_loss: 1.10651934\n",
      "Step: [2955] d_loss: 0.43971846, g_loss: 1.12114751\n",
      "Step: [2956] d_loss: 0.43974540, g_loss: 1.12193537\n",
      "Step: [2957] d_loss: 0.43383744, g_loss: 1.13362479\n",
      "Step: [2958] d_loss: 0.43399373, g_loss: 1.10845280\n",
      "Step: [2959] d_loss: 0.43243670, g_loss: 1.12129438\n",
      "Step: [2960] d_loss: 0.43484789, g_loss: 1.14622402\n",
      "Step: [2961] d_loss: 0.43138042, g_loss: 1.12885594\n",
      "Step: [2962] d_loss: 0.43806198, g_loss: 1.12243938\n",
      "Step: [2963] d_loss: 0.43400151, g_loss: 1.11903882\n",
      "Step: [2964] d_loss: 0.43635395, g_loss: 1.11269069\n",
      "Step: [2965] d_loss: 0.43891901, g_loss: 1.13539922\n",
      "Step: [2966] d_loss: 0.43945006, g_loss: 1.12158537\n",
      "Step: [2967] d_loss: 0.43221068, g_loss: 1.08866680\n",
      "Step: [2968] d_loss: 0.43588978, g_loss: 1.14711106\n",
      "Step: [2969] d_loss: 0.43080199, g_loss: 1.11649537\n",
      "Step: [2970] d_loss: 0.43204480, g_loss: 1.09314775\n",
      "Step: [2971] d_loss: 0.44019091, g_loss: 1.13158309\n",
      "Step: [2972] d_loss: 0.43408337, g_loss: 1.11809039\n",
      "Step: [2973] d_loss: 0.43611911, g_loss: 1.12634742\n",
      "Step: [2974] d_loss: 0.44713113, g_loss: 1.09301102\n",
      "Step: [2975] d_loss: 0.44449630, g_loss: 1.11403286\n",
      "Step: [2976] d_loss: 0.43996140, g_loss: 1.16168773\n",
      "Step: [2977] d_loss: 0.44403872, g_loss: 1.08817053\n",
      "Step: [2978] d_loss: 0.43819517, g_loss: 1.10894918\n",
      "Step: [2979] d_loss: 0.43608719, g_loss: 1.12826073\n",
      "Step: [2980] d_loss: 0.43498749, g_loss: 1.10598969\n",
      "Step: [2981] d_loss: 0.43405676, g_loss: 1.12017739\n",
      "Step: [2982] d_loss: 0.43290260, g_loss: 1.13141572\n",
      "Step: [2983] d_loss: 0.43215114, g_loss: 1.12832916\n",
      "Step: [2984] d_loss: 0.43267524, g_loss: 1.08739793\n",
      "Step: [2985] d_loss: 0.43585065, g_loss: 1.10603821\n",
      "Step: [2986] d_loss: 0.43578309, g_loss: 1.12677574\n",
      "Step: [2987] d_loss: 0.43775105, g_loss: 1.11448550\n",
      "Step: [2988] d_loss: 0.43650442, g_loss: 1.09419715\n",
      "Step: [2989] d_loss: 0.43722054, g_loss: 1.11621535\n",
      "Step: [2990] d_loss: 0.43431389, g_loss: 1.12397933\n",
      "Step: [2991] d_loss: 0.43241221, g_loss: 1.11140990\n",
      "Step: [2992] d_loss: 0.43105790, g_loss: 1.10740662\n",
      "Step: [2993] d_loss: 0.43282643, g_loss: 1.07982945\n",
      "Step: [2994] d_loss: 0.43303916, g_loss: 1.09505510\n",
      "Step: [2995] d_loss: 0.43180442, g_loss: 1.10289788\n",
      "Step: [2996] d_loss: 0.43077090, g_loss: 1.10621607\n",
      "Step: [2997] d_loss: 0.43450704, g_loss: 1.09155810\n",
      "Step: [2998] d_loss: 0.43176222, g_loss: 1.10942626\n",
      "Step: [2999] d_loss: 0.43340534, g_loss: 1.05822945\n",
      "Step: [3000] d_loss: 0.43850476, g_loss: 1.09049106\n",
      "Step: [3001] d_loss: 0.44867411, g_loss: 1.12272680\n",
      "Step: [3002] d_loss: 0.43834421, g_loss: 1.16306365\n",
      "Step: [3003] d_loss: 0.43787631, g_loss: 1.10667181\n",
      "Step: [3004] d_loss: 0.44423369, g_loss: 1.13047743\n",
      "Step: [3005] d_loss: 0.43578386, g_loss: 1.10548091\n",
      "Step: [3006] d_loss: 0.43634629, g_loss: 1.10042226\n",
      "Step: [3007] d_loss: 0.43460986, g_loss: 1.09120262\n",
      "Step: [3008] d_loss: 0.43225750, g_loss: 1.15339792\n",
      "Step: [3009] d_loss: 0.43341640, g_loss: 1.12497413\n",
      "Step: [3010] d_loss: 0.44026369, g_loss: 1.09761155\n",
      "Step: [3011] d_loss: 0.43708181, g_loss: 1.07457244\n",
      "Step: [3012] d_loss: 0.44024229, g_loss: 1.12953413\n",
      "Step: [3013] d_loss: 0.43495727, g_loss: 1.09854007\n",
      "Step: [3014] d_loss: 0.43584743, g_loss: 1.12445939\n",
      "Step: [3015] d_loss: 0.43940273, g_loss: 1.13872004\n",
      "Step: [3016] d_loss: 0.43638352, g_loss: 1.10378301\n",
      "Step: [3017] d_loss: 0.44171941, g_loss: 1.11532092\n",
      "Step: [3018] d_loss: 0.43265933, g_loss: 1.11909306\n",
      "Step: [3019] d_loss: 0.43710798, g_loss: 1.12138402\n",
      "Step: [3020] d_loss: 0.43793115, g_loss: 1.14380050\n",
      "Step: [3021] d_loss: 0.43212160, g_loss: 1.14069581\n",
      "Step: [3022] d_loss: 0.43561411, g_loss: 1.12828124\n",
      "Step: [3023] d_loss: 0.43127364, g_loss: 1.11262798\n",
      "Step: [3024] d_loss: 0.43314564, g_loss: 1.10510004\n",
      "Step: [3025] d_loss: 0.44425815, g_loss: 1.11883533\n",
      "Step: [3026] d_loss: 0.43725124, g_loss: 1.11206865\n",
      "Step: [3027] d_loss: 0.43433362, g_loss: 1.07203686\n",
      "Step: [3028] d_loss: 0.44223735, g_loss: 1.10931671\n",
      "Step: [3029] d_loss: 0.44726479, g_loss: 1.12172961\n",
      "Step: [3030] d_loss: 0.43736279, g_loss: 1.10090268\n",
      "Step: [3031] d_loss: 0.43962938, g_loss: 1.10943770\n",
      "Step: [3032] d_loss: 0.43706673, g_loss: 1.10071552\n",
      "Step: [3033] d_loss: 0.44375420, g_loss: 1.15539753\n",
      "Step: [3034] d_loss: 0.43527913, g_loss: 1.14235699\n",
      "Step: [3035] d_loss: 0.44223619, g_loss: 1.15454578\n",
      "Step: [3036] d_loss: 0.43847093, g_loss: 1.13880098\n",
      "Step: [3037] d_loss: 0.43763557, g_loss: 1.11101723\n",
      "Step: [3038] d_loss: 0.43968707, g_loss: 1.16390717\n",
      "Step: [3039] d_loss: 0.43760633, g_loss: 1.12457824\n",
      "Step: [3040] d_loss: 0.43743247, g_loss: 1.12494743\n",
      "Step: [3041] d_loss: 0.43771765, g_loss: 1.16317511\n",
      "Step: [3042] d_loss: 0.43598142, g_loss: 1.15744877\n",
      "Step: [3043] d_loss: 0.43361166, g_loss: 1.12608397\n",
      "Step: [3044] d_loss: 0.43405595, g_loss: 1.12705255\n",
      "Step: [3045] d_loss: 0.43515116, g_loss: 1.14991856\n",
      "Step: [3046] d_loss: 0.43548399, g_loss: 1.13437092\n",
      "Step: [3047] d_loss: 0.43388018, g_loss: 1.17406774\n",
      "Step: [3048] d_loss: 0.43299735, g_loss: 1.12779236\n",
      "Step: [3049] d_loss: 0.43662676, g_loss: 1.14149594\n",
      "Step: [3050] d_loss: 0.43775582, g_loss: 1.18354273\n",
      "Step: [3051] d_loss: 0.43914258, g_loss: 1.11945081\n",
      "Step: [3052] d_loss: 0.44050223, g_loss: 1.16545129\n",
      "Step: [3053] d_loss: 0.43617743, g_loss: 1.18525422\n",
      "Step: [3054] d_loss: 0.43287531, g_loss: 1.14168096\n",
      "Step: [3055] d_loss: 0.43173736, g_loss: 1.12991858\n",
      "Step: [3056] d_loss: 0.43732944, g_loss: 1.14629817\n",
      "Step: [3057] d_loss: 0.44972879, g_loss: 1.13898826\n",
      "Step: [3058] d_loss: 0.43581140, g_loss: 1.15667081\n",
      "Step: [3059] d_loss: 0.43231145, g_loss: 1.15637708\n",
      "Step: [3060] d_loss: 0.43587643, g_loss: 1.13318503\n",
      "Step: [3061] d_loss: 0.43737102, g_loss: 1.15402365\n",
      "Step: [3062] d_loss: 0.46191916, g_loss: 1.12076986\n",
      "Step: [3063] d_loss: 0.44158220, g_loss: 1.15358794\n",
      "Step: [3064] d_loss: 0.43582082, g_loss: 1.15273607\n",
      "Step: [3065] d_loss: 0.43425530, g_loss: 1.12580585\n",
      "Step: [3066] d_loss: 0.43783671, g_loss: 1.12219739\n",
      "Step: [3067] d_loss: 0.43852553, g_loss: 1.14845240\n",
      "Step: [3068] d_loss: 0.43764129, g_loss: 1.13351500\n",
      "Step: [3069] d_loss: 0.43462425, g_loss: 1.12454498\n",
      "Step: [3070] d_loss: 0.43973073, g_loss: 1.13780200\n",
      "Step: [3071] d_loss: 0.43866193, g_loss: 1.11847699\n",
      "Step: [3072] d_loss: 0.43534926, g_loss: 1.11607480\n",
      "Step: [3073] d_loss: 0.43979800, g_loss: 1.15092194\n",
      "Step: [3074] d_loss: 0.47598290, g_loss: 1.11748910\n",
      "Step: [3075] d_loss: 0.47994396, g_loss: 1.10798502\n",
      "Step: [3076] d_loss: 0.44012761, g_loss: 1.12676573\n",
      "Step: [3077] d_loss: 0.43681747, g_loss: 1.12377918\n",
      "Step: [3078] d_loss: 0.43266600, g_loss: 1.10034132\n",
      "Step: [3079] d_loss: 0.43526328, g_loss: 1.15923119\n",
      "Step: [3080] d_loss: 0.43157437, g_loss: 1.09206641\n",
      "Step: [3081] d_loss: 0.43646145, g_loss: 1.06678283\n",
      "Step: [3082] d_loss: 0.43520683, g_loss: 1.13733935\n",
      "Step: [3083] d_loss: 0.43165568, g_loss: 1.13248158\n",
      "Step: [3084] d_loss: 0.43310294, g_loss: 1.09677541\n",
      "Step: [3085] d_loss: 0.43100792, g_loss: 1.17204309\n",
      "Step: [3086] d_loss: 0.43190131, g_loss: 1.10117590\n",
      "Step: [3087] d_loss: 0.43808046, g_loss: 1.08341897\n",
      "Step: [3088] d_loss: 0.43531775, g_loss: 1.10849798\n",
      "Step: [3089] d_loss: 0.43365651, g_loss: 1.10127044\n",
      "Step: [3090] d_loss: 0.43553868, g_loss: 1.10144496\n",
      "Step: [3091] d_loss: 0.43770784, g_loss: 1.13114178\n",
      "Step: [3092] d_loss: 0.43750298, g_loss: 1.09375405\n",
      "Step: [3093] d_loss: 0.43960711, g_loss: 1.12885535\n",
      "Step: [3094] d_loss: 0.43577409, g_loss: 1.07615483\n",
      "Step: [3095] d_loss: 0.43687236, g_loss: 1.07184029\n",
      "Step: [3096] d_loss: 0.43599617, g_loss: 1.08763969\n",
      "Step: [3097] d_loss: 0.43310502, g_loss: 1.06028199\n",
      "Step: [3098] d_loss: 0.44804463, g_loss: 1.07298911\n",
      "Step: [3099] d_loss: 0.43393120, g_loss: 1.08501446\n",
      "Step: [3100] d_loss: 0.43482015, g_loss: 1.09033394\n",
      "Step: [3101] d_loss: 0.43639874, g_loss: 1.07074630\n",
      "Step: [3102] d_loss: 0.43502346, g_loss: 1.09333503\n",
      "Step: [3103] d_loss: 0.43662471, g_loss: 1.08595753\n",
      "Step: [3104] d_loss: 0.43079627, g_loss: 1.08675134\n",
      "Step: [3105] d_loss: 0.42882714, g_loss: 1.09479260\n",
      "Step: [3106] d_loss: 0.43388680, g_loss: 1.08026767\n",
      "Step: [3107] d_loss: 0.43631428, g_loss: 1.07447290\n",
      "Step: [3108] d_loss: 0.43114591, g_loss: 1.09903145\n",
      "Step: [3109] d_loss: 0.43109700, g_loss: 1.10648990\n",
      "Step: [3110] d_loss: 0.43245384, g_loss: 1.07248247\n",
      "Step: [3111] d_loss: 0.42914355, g_loss: 1.06996477\n",
      "Step: [3112] d_loss: 0.42694435, g_loss: 1.05688369\n",
      "Step: [3113] d_loss: 0.43221518, g_loss: 1.08254623\n",
      "Step: [3114] d_loss: 0.43122622, g_loss: 1.09146130\n",
      "Step: [3115] d_loss: 0.43181834, g_loss: 1.09210515\n",
      "Step: [3116] d_loss: 0.43050140, g_loss: 1.08306038\n",
      "Step: [3117] d_loss: 0.43607894, g_loss: 1.05713820\n",
      "Step: [3118] d_loss: 0.43472555, g_loss: 1.07448101\n",
      "Step: [3119] d_loss: 0.43038324, g_loss: 1.07092869\n",
      "Step: [3120] d_loss: 0.43451560, g_loss: 1.11861730\n",
      "Step: [3121] d_loss: 0.44484410, g_loss: 1.08931947\n",
      "Step: [3122] d_loss: 0.43707076, g_loss: 1.11223125\n",
      "Step: [3123] d_loss: 0.43746370, g_loss: 1.08846247\n",
      "Step: [3124] d_loss: 0.43448752, g_loss: 1.07706213\n",
      "Step: [3125] d_loss: 0.43317047, g_loss: 1.07562292\n",
      "Step: [3126] d_loss: 0.43071875, g_loss: 1.08905077\n",
      "Step: [3127] d_loss: 0.43457252, g_loss: 1.09488285\n",
      "Step: [3128] d_loss: 0.44712374, g_loss: 1.10007989\n",
      "Step: [3129] d_loss: 0.43486685, g_loss: 1.09533787\n",
      "Step: [3130] d_loss: 0.43320492, g_loss: 1.10534060\n",
      "Step: [3131] d_loss: 0.43588305, g_loss: 1.08608353\n",
      "Step: [3132] d_loss: 0.43305779, g_loss: 1.09785485\n",
      "Step: [3133] d_loss: 0.42987385, g_loss: 1.11178017\n",
      "Step: [3134] d_loss: 0.43211746, g_loss: 1.15360057\n",
      "Step: [3135] d_loss: 0.43486574, g_loss: 1.09419763\n",
      "Step: [3136] d_loss: 0.42803198, g_loss: 1.08174765\n",
      "Step: [3137] d_loss: 0.42893675, g_loss: 1.12183428\n",
      "Step: [3138] d_loss: 0.42724925, g_loss: 1.08510017\n",
      "Step: [3139] d_loss: 0.43353665, g_loss: 1.10156560\n",
      "Step: [3140] d_loss: 0.42918584, g_loss: 1.12654281\n",
      "Step: [3141] d_loss: 0.43566963, g_loss: 1.09849072\n",
      "Step: [3142] d_loss: 0.43503273, g_loss: 1.12562156\n",
      "Step: [3143] d_loss: 0.42979264, g_loss: 1.08796668\n",
      "Step: [3144] d_loss: 0.43063021, g_loss: 1.11451149\n",
      "Step: [3145] d_loss: 0.43237779, g_loss: 1.10317385\n",
      "Step: [3146] d_loss: 0.43246111, g_loss: 1.14355886\n",
      "Step: [3147] d_loss: 0.43260077, g_loss: 1.08878434\n",
      "Step: [3148] d_loss: 0.43293625, g_loss: 1.09838188\n",
      "Step: [3149] d_loss: 0.43830505, g_loss: 1.08773923\n",
      "Step: [3150] d_loss: 0.43794417, g_loss: 1.06436241\n",
      "Step: [3151] d_loss: 0.43205047, g_loss: 1.09573364\n",
      "Step: [3152] d_loss: 0.43208960, g_loss: 1.12086129\n",
      "Step: [3153] d_loss: 0.44152147, g_loss: 1.14193952\n",
      "Step: [3154] d_loss: 0.43686423, g_loss: 1.09155500\n",
      "Step: [3155] d_loss: 0.43467358, g_loss: 1.12585545\n",
      "Step: [3156] d_loss: 0.43267629, g_loss: 1.11353457\n",
      "Step: [3157] d_loss: 0.43376783, g_loss: 1.07929492\n",
      "Step: [3158] d_loss: 0.44050962, g_loss: 1.12332582\n",
      "Step: [3159] d_loss: 0.43060577, g_loss: 1.14190340\n",
      "Step: [3160] d_loss: 0.43516353, g_loss: 1.09130442\n",
      "Step: [3161] d_loss: 0.43195632, g_loss: 1.09311402\n",
      "Step: [3162] d_loss: 0.43182203, g_loss: 1.15257955\n",
      "Step: [3163] d_loss: 0.43369877, g_loss: 1.12360168\n",
      "Step: [3164] d_loss: 0.42988658, g_loss: 1.12516510\n",
      "Step: [3165] d_loss: 0.43005899, g_loss: 1.11649179\n",
      "Step: [3166] d_loss: 0.43509123, g_loss: 1.12653136\n",
      "Step: [3167] d_loss: 0.43289584, g_loss: 1.09008241\n",
      "Step: [3168] d_loss: 0.43427444, g_loss: 1.13518846\n",
      "Step: [3169] d_loss: 0.43255243, g_loss: 1.06870151\n",
      "Step: [3170] d_loss: 0.42825240, g_loss: 1.13751376\n",
      "Step: [3171] d_loss: 0.43150297, g_loss: 1.09351003\n",
      "Step: [3172] d_loss: 0.43138647, g_loss: 1.11320066\n",
      "Step: [3173] d_loss: 0.43347335, g_loss: 1.08622885\n",
      "Step: [3174] d_loss: 0.43262926, g_loss: 1.10183334\n",
      "Step: [3175] d_loss: 0.43241578, g_loss: 1.12439001\n",
      "Step: [3176] d_loss: 0.44631302, g_loss: 1.10251915\n",
      "Step: [3177] d_loss: 0.43583164, g_loss: 1.12066329\n",
      "Step: [3178] d_loss: 0.43084446, g_loss: 1.16436589\n",
      "Step: [3179] d_loss: 0.43213153, g_loss: 1.08867288\n",
      "Step: [3180] d_loss: 0.43706772, g_loss: 1.07203019\n",
      "Step: [3181] d_loss: 0.43175346, g_loss: 1.07086205\n",
      "Step: [3182] d_loss: 0.43535605, g_loss: 1.08238697\n",
      "Step: [3183] d_loss: 0.43411434, g_loss: 1.12210321\n",
      "Step: [3184] d_loss: 0.44585949, g_loss: 1.09721673\n",
      "Step: [3185] d_loss: 0.44009179, g_loss: 1.08276737\n",
      "Step: [3186] d_loss: 0.44421256, g_loss: 1.08373618\n",
      "Step: [3187] d_loss: 0.43617657, g_loss: 1.12354612\n",
      "Step: [3188] d_loss: 0.43524855, g_loss: 1.09614003\n",
      "Step: [3189] d_loss: 0.43908039, g_loss: 1.11475360\n",
      "Step: [3190] d_loss: 0.43458921, g_loss: 1.08465075\n",
      "Step: [3191] d_loss: 0.44188538, g_loss: 1.07535660\n",
      "Step: [3192] d_loss: 0.43852046, g_loss: 1.13004339\n",
      "Step: [3193] d_loss: 0.43227834, g_loss: 1.11779416\n",
      "Step: [3194] d_loss: 0.43213421, g_loss: 1.06525874\n",
      "Step: [3195] d_loss: 0.43619156, g_loss: 1.12220752\n",
      "Step: [3196] d_loss: 0.43227452, g_loss: 1.12605381\n",
      "Step: [3197] d_loss: 0.43642288, g_loss: 1.11031258\n",
      "Step: [3198] d_loss: 0.44147247, g_loss: 1.14560008\n",
      "Step: [3199] d_loss: 0.43793222, g_loss: 1.09912586\n",
      "Step: [3200] d_loss: 0.43571004, g_loss: 1.10628104\n",
      "Step: [3201] d_loss: 0.43241057, g_loss: 1.11300349\n",
      "Step: [3202] d_loss: 0.43185154, g_loss: 1.09440076\n",
      "Step: [3203] d_loss: 0.43122992, g_loss: 1.12716758\n",
      "Step: [3204] d_loss: 0.43041104, g_loss: 1.13335621\n",
      "Step: [3205] d_loss: 0.43369478, g_loss: 1.09466529\n",
      "Step: [3206] d_loss: 0.43277705, g_loss: 1.09380746\n",
      "Step: [3207] d_loss: 0.43090558, g_loss: 1.08584929\n",
      "Step: [3208] d_loss: 0.42817700, g_loss: 1.11051893\n",
      "Step: [3209] d_loss: 0.43437254, g_loss: 1.08284760\n",
      "Step: [3210] d_loss: 0.42963442, g_loss: 1.09971297\n",
      "Step: [3211] d_loss: 0.43361604, g_loss: 1.10683513\n",
      "Step: [3212] d_loss: 0.43619397, g_loss: 1.10282135\n",
      "Step: [3213] d_loss: 0.43555927, g_loss: 1.09459639\n",
      "Step: [3214] d_loss: 0.43366277, g_loss: 1.11363149\n",
      "Step: [3215] d_loss: 0.43260336, g_loss: 1.05701804\n",
      "Step: [3216] d_loss: 0.44713700, g_loss: 1.12416291\n",
      "Step: [3217] d_loss: 0.44030100, g_loss: 1.12376797\n",
      "Step: [3218] d_loss: 0.43534362, g_loss: 1.11387956\n",
      "Step: [3219] d_loss: 0.42952529, g_loss: 1.09623647\n",
      "Step: [3220] d_loss: 0.43250471, g_loss: 1.11557603\n",
      "Step: [3221] d_loss: 0.43340099, g_loss: 1.09339714\n",
      "Step: [3222] d_loss: 0.44444361, g_loss: 1.06119132\n",
      "Step: [3223] d_loss: 0.43887472, g_loss: 1.13656461\n",
      "Step: [3224] d_loss: 0.43626276, g_loss: 1.10228980\n",
      "Step: [3225] d_loss: 0.43082678, g_loss: 1.16668785\n",
      "Step: [3226] d_loss: 0.43528432, g_loss: 1.08631325\n",
      "Step: [3227] d_loss: 0.43851024, g_loss: 1.13979387\n",
      "Step: [3228] d_loss: 0.43273550, g_loss: 1.11850286\n",
      "Step: [3229] d_loss: 0.43216592, g_loss: 1.08474302\n",
      "Step: [3230] d_loss: 0.43474799, g_loss: 1.05328023\n",
      "Step: [3231] d_loss: 0.42672834, g_loss: 1.06541944\n",
      "Step: [3232] d_loss: 0.43059063, g_loss: 1.10703456\n",
      "Step: [3233] d_loss: 0.43013158, g_loss: 1.10346353\n",
      "Step: [3234] d_loss: 0.42959839, g_loss: 1.05986118\n",
      "Step: [3235] d_loss: 0.42922127, g_loss: 1.08317506\n",
      "Step: [3236] d_loss: 0.43110681, g_loss: 1.08686984\n",
      "Step: [3237] d_loss: 0.42956930, g_loss: 1.05508995\n",
      "Step: [3238] d_loss: 0.42872465, g_loss: 1.07069862\n",
      "Step: [3239] d_loss: 0.43108749, g_loss: 1.06617010\n",
      "Step: [3240] d_loss: 0.43492365, g_loss: 1.04267859\n",
      "Step: [3241] d_loss: 0.43209720, g_loss: 1.10652137\n",
      "Step: [3242] d_loss: 0.43287227, g_loss: 1.11725438\n",
      "Step: [3243] d_loss: 0.43086097, g_loss: 1.09246624\n",
      "Step: [3244] d_loss: 0.43344468, g_loss: 1.07547069\n",
      "Step: [3245] d_loss: 0.43487307, g_loss: 1.07705569\n",
      "Step: [3246] d_loss: 0.43843710, g_loss: 1.08315241\n",
      "Step: [3247] d_loss: 0.43114749, g_loss: 1.10766077\n",
      "Step: [3248] d_loss: 0.43238851, g_loss: 1.06276023\n",
      "Step: [3249] d_loss: 0.43016821, g_loss: 1.07138801\n",
      "Step: [3250] d_loss: 0.42853516, g_loss: 1.06304693\n",
      "Step: [3251] d_loss: 0.43005475, g_loss: 1.07657325\n",
      "Step: [3252] d_loss: 0.42865142, g_loss: 1.06548095\n",
      "Step: [3253] d_loss: 0.43061388, g_loss: 1.13827133\n",
      "Step: [3254] d_loss: 0.42779565, g_loss: 1.04789031\n",
      "Step: [3255] d_loss: 0.42966101, g_loss: 1.09503329\n",
      "Step: [3256] d_loss: 0.42985985, g_loss: 1.09221792\n",
      "Step: [3257] d_loss: 0.42952621, g_loss: 1.12102461\n",
      "Step: [3258] d_loss: 0.42993608, g_loss: 1.09709156\n",
      "Step: [3259] d_loss: 0.43260956, g_loss: 1.08766735\n",
      "Step: [3260] d_loss: 0.43254548, g_loss: 1.10913444\n",
      "Step: [3261] d_loss: 0.43229836, g_loss: 1.12031615\n",
      "Step: [3262] d_loss: 0.43076217, g_loss: 1.06453800\n",
      "Step: [3263] d_loss: 0.44065353, g_loss: 1.07443035\n",
      "Step: [3264] d_loss: 0.46544039, g_loss: 1.12082756\n",
      "Step: [3265] d_loss: 0.44128752, g_loss: 1.11908066\n",
      "Step: [3266] d_loss: 0.44204864, g_loss: 1.07587934\n",
      "Step: [3267] d_loss: 0.43788871, g_loss: 1.16604018\n",
      "Step: [3268] d_loss: 0.43199715, g_loss: 1.11494911\n",
      "Step: [3269] d_loss: 0.43587738, g_loss: 1.10235214\n",
      "Step: [3270] d_loss: 0.43322757, g_loss: 1.10394967\n",
      "Step: [3271] d_loss: 0.43518603, g_loss: 1.14267719\n",
      "Step: [3272] d_loss: 0.43523157, g_loss: 1.08920133\n",
      "Step: [3273] d_loss: 0.42807025, g_loss: 1.06523693\n",
      "Step: [3274] d_loss: 0.42888272, g_loss: 1.07919443\n",
      "Step: [3275] d_loss: 0.43333802, g_loss: 1.09114122\n",
      "Step: [3276] d_loss: 0.43308631, g_loss: 1.08095527\n",
      "Step: [3277] d_loss: 0.43050149, g_loss: 1.10179830\n",
      "Step: [3278] d_loss: 0.43310070, g_loss: 1.10280859\n",
      "Step: [3279] d_loss: 0.42883617, g_loss: 1.08300960\n",
      "Step: [3280] d_loss: 0.42902297, g_loss: 1.08035493\n",
      "Step: [3281] d_loss: 0.42765823, g_loss: 1.07012796\n",
      "Step: [3282] d_loss: 0.43241185, g_loss: 1.07438910\n",
      "Step: [3283] d_loss: 0.42672449, g_loss: 1.06951821\n",
      "Step: [3284] d_loss: 0.42518365, g_loss: 1.05340302\n",
      "Step: [3285] d_loss: 0.43026903, g_loss: 1.06515741\n",
      "Step: [3286] d_loss: 0.42868423, g_loss: 1.06026065\n",
      "Step: [3287] d_loss: 0.43174726, g_loss: 1.05558372\n",
      "Step: [3288] d_loss: 0.43642768, g_loss: 1.08776188\n",
      "Step: [3289] d_loss: 0.43372762, g_loss: 1.06419611\n",
      "Step: [3290] d_loss: 0.43289763, g_loss: 1.07364178\n",
      "Step: [3291] d_loss: 0.42936471, g_loss: 1.05904448\n",
      "Step: [3292] d_loss: 0.43483576, g_loss: 1.13182688\n",
      "Step: [3293] d_loss: 0.43229902, g_loss: 1.11078155\n",
      "Step: [3294] d_loss: 0.43411592, g_loss: 1.09694636\n",
      "Step: [3295] d_loss: 0.43036115, g_loss: 1.07907605\n",
      "Step: [3296] d_loss: 0.42860079, g_loss: 1.11261880\n",
      "Step: [3297] d_loss: 0.43048441, g_loss: 1.09064388\n",
      "Step: [3298] d_loss: 0.43161505, g_loss: 1.08818388\n",
      "Step: [3299] d_loss: 0.44236171, g_loss: 1.13263643\n",
      "Step: [3300] d_loss: 0.43393129, g_loss: 1.06991470\n",
      "Step: [3301] d_loss: 0.43817818, g_loss: 1.06555343\n",
      "Step: [3302] d_loss: 0.43515104, g_loss: 1.09820974\n",
      "Step: [3303] d_loss: 0.43823841, g_loss: 1.12038469\n",
      "Step: [3304] d_loss: 0.43564406, g_loss: 1.10276937\n",
      "Step: [3305] d_loss: 0.43337435, g_loss: 1.09880543\n",
      "Step: [3306] d_loss: 0.43498623, g_loss: 1.05467498\n",
      "Step: [3307] d_loss: 0.43422562, g_loss: 1.07144022\n",
      "Step: [3308] d_loss: 0.44235703, g_loss: 1.10214376\n",
      "Step: [3309] d_loss: 0.43453065, g_loss: 1.08780289\n",
      "Step: [3310] d_loss: 0.43202060, g_loss: 1.11974943\n",
      "Step: [3311] d_loss: 0.43333554, g_loss: 1.09606361\n",
      "Step: [3312] d_loss: 0.43234757, g_loss: 1.11951923\n",
      "Step: [3313] d_loss: 0.43191630, g_loss: 1.08590031\n",
      "Step: [3314] d_loss: 0.43561789, g_loss: 1.12410653\n",
      "Step: [3315] d_loss: 0.43130097, g_loss: 1.12330341\n",
      "Step: [3316] d_loss: 0.43423927, g_loss: 1.11266625\n",
      "Step: [3317] d_loss: 0.43610021, g_loss: 1.08463264\n",
      "Step: [3318] d_loss: 0.42928362, g_loss: 1.12159729\n",
      "Step: [3319] d_loss: 0.43019959, g_loss: 1.08847427\n",
      "Step: [3320] d_loss: 0.42992568, g_loss: 1.09148908\n",
      "Step: [3321] d_loss: 0.43190423, g_loss: 1.13314021\n",
      "Step: [3322] d_loss: 0.42837557, g_loss: 1.06821084\n",
      "Step: [3323] d_loss: 0.43123913, g_loss: 1.11807096\n",
      "Step: [3324] d_loss: 0.43128780, g_loss: 1.05636156\n",
      "Step: [3325] d_loss: 0.43702111, g_loss: 1.12477541\n",
      "Step: [3326] d_loss: 0.43842772, g_loss: 1.16356921\n",
      "Step: [3327] d_loss: 0.43943474, g_loss: 1.09235132\n",
      "Step: [3328] d_loss: 0.43117517, g_loss: 1.12941074\n",
      "Step: [3329] d_loss: 0.43323153, g_loss: 1.11036563\n",
      "Step: [3330] d_loss: 0.43278897, g_loss: 1.04479408\n",
      "Step: [3331] d_loss: 0.42997596, g_loss: 1.11941254\n",
      "Step: [3332] d_loss: 0.43106097, g_loss: 1.10925198\n",
      "Step: [3333] d_loss: 0.43181136, g_loss: 1.11810756\n",
      "Step: [3334] d_loss: 0.43139261, g_loss: 1.14033604\n",
      "Step: [3335] d_loss: 0.44395038, g_loss: 1.07871950\n",
      "Step: [3336] d_loss: 0.43809915, g_loss: 1.10755277\n",
      "Step: [3337] d_loss: 0.43573895, g_loss: 1.08320594\n",
      "Step: [3338] d_loss: 0.43218970, g_loss: 1.08746970\n",
      "Step: [3339] d_loss: 0.43251726, g_loss: 1.09498155\n",
      "Step: [3340] d_loss: 0.43054584, g_loss: 1.07958889\n",
      "Step: [3341] d_loss: 0.42945236, g_loss: 1.07778108\n",
      "Step: [3342] d_loss: 0.43230933, g_loss: 1.09266174\n",
      "Step: [3343] d_loss: 0.43164229, g_loss: 1.08982623\n",
      "Step: [3344] d_loss: 0.43278521, g_loss: 1.07368195\n",
      "Step: [3345] d_loss: 0.43118149, g_loss: 1.11028433\n",
      "Step: [3346] d_loss: 0.43018663, g_loss: 1.08184814\n",
      "Step: [3347] d_loss: 0.43614826, g_loss: 1.09247255\n",
      "Step: [3348] d_loss: 0.43552223, g_loss: 1.06928170\n",
      "Step: [3349] d_loss: 0.43592554, g_loss: 1.06509066\n",
      "Step: [3350] d_loss: 0.43248919, g_loss: 1.06521785\n",
      "Step: [3351] d_loss: 0.44025290, g_loss: 1.07002783\n",
      "Step: [3352] d_loss: 0.43025285, g_loss: 1.13628042\n",
      "Step: [3353] d_loss: 0.43168280, g_loss: 1.08619356\n",
      "Step: [3354] d_loss: 0.43744019, g_loss: 1.09798360\n",
      "Step: [3355] d_loss: 0.43199912, g_loss: 1.06475091\n",
      "Step: [3356] d_loss: 0.43231589, g_loss: 1.10405540\n",
      "Step: [3357] d_loss: 0.42979637, g_loss: 1.12014472\n",
      "Step: [3358] d_loss: 0.43026060, g_loss: 1.06297481\n",
      "Step: [3359] d_loss: 0.42794135, g_loss: 1.09617174\n",
      "Step: [3360] d_loss: 0.43661347, g_loss: 1.10974348\n",
      "Step: [3361] d_loss: 0.43344525, g_loss: 1.08624542\n",
      "Step: [3362] d_loss: 0.43635932, g_loss: 1.14739013\n",
      "Step: [3363] d_loss: 0.43574882, g_loss: 1.08371091\n",
      "Step: [3364] d_loss: 0.43798387, g_loss: 1.12060130\n",
      "Step: [3365] d_loss: 0.43407720, g_loss: 1.10828638\n",
      "Step: [3366] d_loss: 0.43868122, g_loss: 1.09085608\n",
      "Step: [3367] d_loss: 0.43531439, g_loss: 1.09406710\n",
      "Step: [3368] d_loss: 0.43161130, g_loss: 1.13018346\n",
      "Step: [3369] d_loss: 0.44416007, g_loss: 1.11536002\n",
      "Step: [3370] d_loss: 0.43088150, g_loss: 1.10240936\n",
      "Step: [3371] d_loss: 0.43363228, g_loss: 1.11961091\n",
      "Step: [3372] d_loss: 0.42854324, g_loss: 1.10324812\n",
      "Step: [3373] d_loss: 0.43160453, g_loss: 1.11192727\n",
      "Step: [3374] d_loss: 0.43252748, g_loss: 1.11269259\n",
      "Step: [3375] d_loss: 0.43452257, g_loss: 1.09466529\n",
      "Step: [3376] d_loss: 0.44247895, g_loss: 1.11962020\n",
      "Step: [3377] d_loss: 0.44944724, g_loss: 1.10122764\n",
      "Step: [3378] d_loss: 0.44500884, g_loss: 1.13343060\n",
      "Step: [3379] d_loss: 0.43638572, g_loss: 1.09718132\n",
      "Step: [3380] d_loss: 0.44127926, g_loss: 1.09240007\n",
      "Step: [3381] d_loss: 0.43509820, g_loss: 1.10809612\n",
      "Step: [3382] d_loss: 0.43029800, g_loss: 1.08842242\n",
      "Step: [3383] d_loss: 0.43867010, g_loss: 1.14490128\n",
      "Step: [3384] d_loss: 0.43210924, g_loss: 1.11431360\n",
      "Step: [3385] d_loss: 0.43056494, g_loss: 1.09068012\n",
      "Step: [3386] d_loss: 0.43328568, g_loss: 1.13117826\n",
      "Step: [3387] d_loss: 0.43473318, g_loss: 1.06653750\n",
      "Step: [3388] d_loss: 0.43540227, g_loss: 1.11505663\n",
      "Step: [3389] d_loss: 0.43143007, g_loss: 1.08698165\n",
      "Step: [3390] d_loss: 0.43386731, g_loss: 1.08755219\n",
      "Step: [3391] d_loss: 0.42943257, g_loss: 1.07092869\n",
      "Step: [3392] d_loss: 0.43081942, g_loss: 1.09949052\n",
      "Step: [3393] d_loss: 0.42883724, g_loss: 1.07465219\n",
      "Step: [3394] d_loss: 0.43199369, g_loss: 1.10182488\n",
      "Step: [3395] d_loss: 0.44348192, g_loss: 1.08839560\n",
      "Step: [3396] d_loss: 0.43107715, g_loss: 1.07478416\n",
      "Step: [3397] d_loss: 0.43965217, g_loss: 1.09551811\n",
      "Step: [3398] d_loss: 0.43597838, g_loss: 1.10348618\n",
      "Step: [3399] d_loss: 0.43511754, g_loss: 1.11361170\n",
      "Step: [3400] d_loss: 0.43194255, g_loss: 1.07631469\n",
      "Step: [3401] d_loss: 0.43688890, g_loss: 1.10934067\n",
      "Step: [3402] d_loss: 0.44387963, g_loss: 1.10222983\n",
      "Step: [3403] d_loss: 0.43443227, g_loss: 1.09699309\n",
      "Step: [3404] d_loss: 0.43492341, g_loss: 1.12340438\n",
      "Step: [3405] d_loss: 0.43483379, g_loss: 1.10558927\n",
      "Step: [3406] d_loss: 0.43155670, g_loss: 1.09712112\n",
      "Step: [3407] d_loss: 0.43238965, g_loss: 1.08606136\n",
      "Step: [3408] d_loss: 0.42890966, g_loss: 1.08782029\n",
      "Step: [3409] d_loss: 0.43204880, g_loss: 1.08187222\n",
      "Step: [3410] d_loss: 0.43295476, g_loss: 1.09288633\n",
      "Step: [3411] d_loss: 0.43315488, g_loss: 1.10252833\n",
      "Step: [3412] d_loss: 0.43164071, g_loss: 1.08740807\n",
      "Step: [3413] d_loss: 0.43439224, g_loss: 1.09857678\n",
      "Step: [3414] d_loss: 0.44064620, g_loss: 1.12763667\n",
      "Step: [3415] d_loss: 0.43712795, g_loss: 1.10502303\n",
      "Step: [3416] d_loss: 0.43600833, g_loss: 1.10412288\n",
      "Step: [3417] d_loss: 0.43220747, g_loss: 1.10666215\n",
      "Step: [3418] d_loss: 0.43443096, g_loss: 1.12574410\n",
      "Step: [3419] d_loss: 0.43114001, g_loss: 1.11724043\n",
      "Step: [3420] d_loss: 0.43298396, g_loss: 1.09559011\n",
      "Step: [3421] d_loss: 0.42941856, g_loss: 1.10102355\n",
      "Step: [3422] d_loss: 0.43359461, g_loss: 1.13987470\n",
      "Step: [3423] d_loss: 0.42998284, g_loss: 1.10168004\n",
      "Step: [3424] d_loss: 0.42984065, g_loss: 1.11307693\n",
      "Step: [3425] d_loss: 0.43235409, g_loss: 1.10909629\n",
      "Step: [3426] d_loss: 0.43108442, g_loss: 1.07184160\n",
      "Step: [3427] d_loss: 0.43433419, g_loss: 1.11500382\n",
      "Step: [3428] d_loss: 0.42998597, g_loss: 1.10105515\n",
      "Step: [3429] d_loss: 0.43239349, g_loss: 1.07328928\n",
      "Step: [3430] d_loss: 0.42895189, g_loss: 1.12554288\n",
      "Step: [3431] d_loss: 0.42904174, g_loss: 1.05674899\n",
      "Step: [3432] d_loss: 0.43124324, g_loss: 1.11192322\n",
      "Step: [3433] d_loss: 0.42715034, g_loss: 1.11372733\n",
      "Step: [3434] d_loss: 0.42572668, g_loss: 1.06479657\n",
      "Step: [3435] d_loss: 0.43104905, g_loss: 1.07040119\n",
      "Step: [3436] d_loss: 0.42882431, g_loss: 1.09371877\n",
      "Step: [3437] d_loss: 0.43012401, g_loss: 1.05215442\n",
      "Step: [3438] d_loss: 0.43039265, g_loss: 1.08198011\n",
      "Step: [3439] d_loss: 0.42886004, g_loss: 1.10787487\n",
      "Step: [3440] d_loss: 0.42812291, g_loss: 1.07215154\n",
      "Step: [3441] d_loss: 0.42748898, g_loss: 1.10853434\n",
      "Step: [3442] d_loss: 0.43071747, g_loss: 1.08869469\n",
      "Step: [3443] d_loss: 0.43191373, g_loss: 1.09307313\n",
      "Step: [3444] d_loss: 0.43184581, g_loss: 1.05682743\n",
      "Step: [3445] d_loss: 0.43103048, g_loss: 1.13090622\n",
      "Step: [3446] d_loss: 0.42924827, g_loss: 1.11087441\n",
      "Step: [3447] d_loss: 0.43009830, g_loss: 1.11465681\n",
      "Step: [3448] d_loss: 0.43119019, g_loss: 1.08059359\n",
      "Step: [3449] d_loss: 0.42521620, g_loss: 1.09539247\n",
      "Step: [3450] d_loss: 0.43088210, g_loss: 1.11809659\n",
      "Step: [3451] d_loss: 0.42728117, g_loss: 1.11408103\n",
      "Step: [3452] d_loss: 0.43363413, g_loss: 1.11469543\n",
      "Step: [3453] d_loss: 0.42722449, g_loss: 1.08152735\n",
      "Step: [3454] d_loss: 0.42702833, g_loss: 1.08926702\n",
      "Step: [3455] d_loss: 0.42672092, g_loss: 1.10786712\n",
      "Step: [3456] d_loss: 0.42862719, g_loss: 1.09253943\n",
      "Step: [3457] d_loss: 0.42694324, g_loss: 1.14214277\n",
      "Step: [3458] d_loss: 0.43148842, g_loss: 1.09978986\n",
      "Step: [3459] d_loss: 0.42602366, g_loss: 1.09750724\n",
      "Step: [3460] d_loss: 0.42841437, g_loss: 1.13310671\n",
      "Step: [3461] d_loss: 0.43060604, g_loss: 1.12313330\n",
      "Step: [3462] d_loss: 0.43758515, g_loss: 1.10519934\n",
      "Step: [3463] d_loss: 0.43552154, g_loss: 1.10650897\n",
      "Step: [3464] d_loss: 0.42699009, g_loss: 1.05805147\n",
      "Step: [3465] d_loss: 0.42954874, g_loss: 1.09997666\n",
      "Step: [3466] d_loss: 0.43048635, g_loss: 1.13128543\n",
      "Step: [3467] d_loss: 0.43253911, g_loss: 1.06351125\n",
      "Step: [3468] d_loss: 0.43352863, g_loss: 1.08115721\n",
      "Step: [3469] d_loss: 0.43007523, g_loss: 1.11138952\n",
      "Step: [3470] d_loss: 0.42974290, g_loss: 1.12510467\n",
      "Step: [3471] d_loss: 0.43026116, g_loss: 1.14449275\n",
      "Step: [3472] d_loss: 0.43040586, g_loss: 1.12813067\n",
      "Step: [3473] d_loss: 0.42574984, g_loss: 1.12736487\n",
      "Step: [3474] d_loss: 0.43089834, g_loss: 1.15203893\n",
      "Step: [3475] d_loss: 0.42974404, g_loss: 1.11275947\n",
      "Step: [3476] d_loss: 0.43116423, g_loss: 1.10222173\n",
      "Step: [3477] d_loss: 0.43246669, g_loss: 1.10767734\n",
      "Step: [3478] d_loss: 0.42692438, g_loss: 1.10217261\n",
      "Step: [3479] d_loss: 0.42770594, g_loss: 1.09438181\n",
      "Step: [3480] d_loss: 0.43307665, g_loss: 1.11321449\n",
      "Step: [3481] d_loss: 0.43015119, g_loss: 1.12471867\n",
      "Step: [3482] d_loss: 0.45902559, g_loss: 1.14877379\n",
      "Step: [3483] d_loss: 0.43744105, g_loss: 1.13382268\n",
      "Step: [3484] d_loss: 0.43135932, g_loss: 1.14786971\n",
      "Step: [3485] d_loss: 0.42934257, g_loss: 1.10331249\n",
      "Step: [3486] d_loss: 0.46288532, g_loss: 1.12458885\n",
      "Step: [3487] d_loss: 0.43860415, g_loss: 1.14313090\n",
      "Step: [3488] d_loss: 0.43230191, g_loss: 1.14533913\n",
      "Step: [3489] d_loss: 0.43099663, g_loss: 1.09459364\n",
      "Step: [3490] d_loss: 0.43205228, g_loss: 1.12590468\n",
      "Step: [3491] d_loss: 0.43544230, g_loss: 1.14222801\n",
      "Step: [3492] d_loss: 0.43132880, g_loss: 1.14053571\n",
      "Step: [3493] d_loss: 0.42947471, g_loss: 1.14498329\n",
      "Step: [3494] d_loss: 0.43222272, g_loss: 1.10938728\n",
      "Step: [3495] d_loss: 0.42671576, g_loss: 1.10538208\n",
      "Step: [3496] d_loss: 0.42877963, g_loss: 1.12052274\n",
      "Step: [3497] d_loss: 0.43048152, g_loss: 1.10274744\n",
      "Step: [3498] d_loss: 0.42661580, g_loss: 1.09315872\n",
      "Step: [3499] d_loss: 0.42742446, g_loss: 1.08740389\n",
      "Step: [3500] d_loss: 0.43113855, g_loss: 1.13946879\n",
      "Step: [3501] d_loss: 0.42960650, g_loss: 1.08457482\n",
      "Step: [3502] d_loss: 0.43427491, g_loss: 1.16872764\n",
      "Step: [3503] d_loss: 0.43315926, g_loss: 1.11428535\n",
      "Step: [3504] d_loss: 0.43254372, g_loss: 1.13773632\n",
      "Step: [3505] d_loss: 0.43021315, g_loss: 1.12555718\n",
      "Step: [3506] d_loss: 0.42892575, g_loss: 1.10396266\n",
      "Step: [3507] d_loss: 0.42724118, g_loss: 1.08035767\n",
      "Step: [3508] d_loss: 0.43046570, g_loss: 1.07486546\n",
      "Step: [3509] d_loss: 0.42610294, g_loss: 1.03886247\n",
      "Step: [3510] d_loss: 0.42858472, g_loss: 1.04928041\n",
      "Step: [3511] d_loss: 0.42899752, g_loss: 1.07513249\n",
      "Step: [3512] d_loss: 0.43129045, g_loss: 1.10981321\n",
      "Step: [3513] d_loss: 0.42744443, g_loss: 1.09050858\n",
      "Step: [3514] d_loss: 0.42826900, g_loss: 1.08518457\n",
      "Step: [3515] d_loss: 0.42764866, g_loss: 1.06239974\n",
      "Step: [3516] d_loss: 0.42986128, g_loss: 1.10368025\n",
      "Step: [3517] d_loss: 0.43047699, g_loss: 1.04977274\n",
      "Step: [3518] d_loss: 0.42620385, g_loss: 1.05893624\n",
      "Step: [3519] d_loss: 0.43347278, g_loss: 1.09317243\n",
      "Step: [3520] d_loss: 0.43067244, g_loss: 1.09911454\n",
      "Step: [3521] d_loss: 0.42944437, g_loss: 1.08635855\n",
      "Step: [3522] d_loss: 0.43464053, g_loss: 1.08986759\n",
      "Step: [3523] d_loss: 0.43001989, g_loss: 1.09210515\n",
      "Step: [3524] d_loss: 0.42750868, g_loss: 1.08469117\n",
      "Step: [3525] d_loss: 0.42529508, g_loss: 1.07071257\n",
      "Step: [3526] d_loss: 0.42990223, g_loss: 1.05639315\n",
      "Step: [3527] d_loss: 0.42942831, g_loss: 1.09195495\n",
      "Step: [3528] d_loss: 0.42330158, g_loss: 1.06136715\n",
      "Step: [3529] d_loss: 0.42518154, g_loss: 1.08490634\n",
      "Step: [3530] d_loss: 0.42687249, g_loss: 1.07657254\n",
      "Step: [3531] d_loss: 0.42816120, g_loss: 1.07886314\n",
      "Step: [3532] d_loss: 0.42753553, g_loss: 1.11396265\n",
      "Step: [3533] d_loss: 0.43700781, g_loss: 1.09255755\n",
      "Step: [3534] d_loss: 0.43070388, g_loss: 1.08930671\n",
      "Step: [3535] d_loss: 0.42857862, g_loss: 1.07003427\n",
      "Step: [3536] d_loss: 0.42574733, g_loss: 1.09408271\n",
      "Step: [3537] d_loss: 0.42654327, g_loss: 1.06043744\n",
      "Step: [3538] d_loss: 0.42781460, g_loss: 1.09457636\n",
      "Step: [3539] d_loss: 0.42756626, g_loss: 1.08540845\n",
      "Step: [3540] d_loss: 0.43071726, g_loss: 1.05734515\n",
      "Step: [3541] d_loss: 0.43218961, g_loss: 1.13857520\n",
      "Step: [3542] d_loss: 0.42744350, g_loss: 1.07434714\n",
      "Step: [3543] d_loss: 0.42945892, g_loss: 1.06539631\n",
      "Step: [3544] d_loss: 0.43520901, g_loss: 1.07079911\n",
      "Step: [3545] d_loss: 0.42961743, g_loss: 1.07712638\n",
      "Step: [3546] d_loss: 0.42864710, g_loss: 1.09257281\n",
      "Step: [3547] d_loss: 0.42824110, g_loss: 1.05445266\n",
      "Step: [3548] d_loss: 0.42763481, g_loss: 1.10207522\n",
      "Step: [3549] d_loss: 0.42332244, g_loss: 1.06826782\n",
      "Step: [3550] d_loss: 0.42748928, g_loss: 1.07081437\n",
      "Step: [3551] d_loss: 0.42909509, g_loss: 1.10532045\n",
      "Step: [3552] d_loss: 0.42460740, g_loss: 1.11596191\n",
      "Step: [3553] d_loss: 0.42561394, g_loss: 1.11923647\n",
      "Step: [3554] d_loss: 0.42514893, g_loss: 1.08354640\n",
      "Step: [3555] d_loss: 0.42675817, g_loss: 1.07876742\n",
      "Step: [3556] d_loss: 0.42491600, g_loss: 1.07982802\n",
      "Step: [3557] d_loss: 0.42786184, g_loss: 1.05715370\n",
      "Step: [3558] d_loss: 0.43016973, g_loss: 1.11305737\n",
      "Step: [3559] d_loss: 0.43115544, g_loss: 1.09219885\n",
      "Step: [3560] d_loss: 0.42623645, g_loss: 1.06223464\n",
      "Step: [3561] d_loss: 0.42790657, g_loss: 1.09302723\n",
      "Step: [3562] d_loss: 0.43049464, g_loss: 1.04253113\n",
      "Step: [3563] d_loss: 0.43273652, g_loss: 1.09646237\n",
      "Step: [3564] d_loss: 0.43753850, g_loss: 1.07902682\n",
      "Step: [3565] d_loss: 0.42575115, g_loss: 1.05382454\n",
      "Step: [3566] d_loss: 0.43127650, g_loss: 1.08977222\n",
      "Step: [3567] d_loss: 0.43280408, g_loss: 1.10963035\n",
      "Step: [3568] d_loss: 0.43116453, g_loss: 1.07890832\n",
      "Step: [3569] d_loss: 0.43315065, g_loss: 1.10488379\n",
      "Step: [3570] d_loss: 0.42997849, g_loss: 1.06018400\n",
      "Step: [3571] d_loss: 0.42896295, g_loss: 1.08597612\n",
      "Step: [3572] d_loss: 0.42956185, g_loss: 1.08231175\n",
      "Step: [3573] d_loss: 0.42650366, g_loss: 1.08965480\n",
      "Step: [3574] d_loss: 0.42370111, g_loss: 1.10348237\n",
      "Step: [3575] d_loss: 0.42474365, g_loss: 1.11510301\n",
      "Step: [3576] d_loss: 0.42154995, g_loss: 1.11513531\n",
      "Step: [3577] d_loss: 0.42785496, g_loss: 1.09109795\n",
      "Step: [3578] d_loss: 0.42739812, g_loss: 1.08959198\n",
      "Step: [3579] d_loss: 0.42801616, g_loss: 1.08334351\n",
      "Step: [3580] d_loss: 0.42444044, g_loss: 1.07529807\n",
      "Step: [3581] d_loss: 0.42943773, g_loss: 1.07568681\n",
      "Step: [3582] d_loss: 0.42812505, g_loss: 1.09640598\n",
      "Step: [3583] d_loss: 0.43328038, g_loss: 1.07972503\n",
      "Step: [3584] d_loss: 0.43311217, g_loss: 1.11732984\n",
      "Step: [3585] d_loss: 0.43227202, g_loss: 1.09430587\n",
      "Step: [3586] d_loss: 0.43225288, g_loss: 1.09423018\n",
      "Step: [3587] d_loss: 0.43321866, g_loss: 1.10157108\n",
      "Step: [3588] d_loss: 0.43137366, g_loss: 1.11269975\n",
      "Step: [3589] d_loss: 0.42742547, g_loss: 1.07140839\n",
      "Step: [3590] d_loss: 0.43045154, g_loss: 1.08567846\n",
      "Step: [3591] d_loss: 0.42858416, g_loss: 1.06943047\n",
      "Step: [3592] d_loss: 0.42909959, g_loss: 1.08030653\n",
      "Step: [3593] d_loss: 0.42658001, g_loss: 1.12577677\n",
      "Step: [3594] d_loss: 0.43107268, g_loss: 1.13241613\n",
      "Step: [3595] d_loss: 0.43672803, g_loss: 1.10419059\n",
      "Step: [3596] d_loss: 0.43218592, g_loss: 1.11653304\n",
      "Step: [3597] d_loss: 0.42564708, g_loss: 1.07243299\n",
      "Step: [3598] d_loss: 0.42883331, g_loss: 1.09587157\n",
      "Step: [3599] d_loss: 0.43173763, g_loss: 1.07150817\n",
      "Step: [3600] d_loss: 0.43313459, g_loss: 1.11062634\n",
      "Step: [3601] d_loss: 0.42971265, g_loss: 1.08324826\n",
      "Step: [3602] d_loss: 0.44260952, g_loss: 1.09483755\n",
      "Step: [3603] d_loss: 0.43255338, g_loss: 1.09530950\n",
      "Step: [3604] d_loss: 0.43857700, g_loss: 1.08941376\n",
      "Step: [3605] d_loss: 0.43558913, g_loss: 1.12202060\n",
      "Step: [3606] d_loss: 0.42777747, g_loss: 1.11017156\n",
      "Step: [3607] d_loss: 0.42670265, g_loss: 1.10659444\n",
      "Step: [3608] d_loss: 0.42707735, g_loss: 1.13220978\n",
      "Step: [3609] d_loss: 0.43038777, g_loss: 1.11929429\n",
      "Step: [3610] d_loss: 0.42705795, g_loss: 1.11412275\n",
      "Step: [3611] d_loss: 0.42882016, g_loss: 1.10838592\n",
      "Step: [3612] d_loss: 0.42825922, g_loss: 1.11924624\n",
      "Step: [3613] d_loss: 0.42799610, g_loss: 1.10271740\n",
      "Step: [3614] d_loss: 0.42720976, g_loss: 1.11127687\n",
      "Step: [3615] d_loss: 0.42894036, g_loss: 1.08096743\n",
      "Step: [3616] d_loss: 0.42965281, g_loss: 1.11871159\n",
      "Step: [3617] d_loss: 0.42924547, g_loss: 1.10571086\n",
      "Step: [3618] d_loss: 0.42891425, g_loss: 1.11462355\n",
      "Step: [3619] d_loss: 0.42706257, g_loss: 1.11214995\n",
      "Step: [3620] d_loss: 0.42602211, g_loss: 1.11774933\n",
      "Step: [3621] d_loss: 0.43242708, g_loss: 1.10512972\n",
      "Step: [3622] d_loss: 0.43056223, g_loss: 1.10156596\n",
      "Step: [3623] d_loss: 0.42643398, g_loss: 1.11899483\n",
      "Step: [3624] d_loss: 0.42493105, g_loss: 1.07535112\n",
      "Step: [3625] d_loss: 0.42558751, g_loss: 1.09219527\n",
      "Step: [3626] d_loss: 0.42919001, g_loss: 1.07693267\n",
      "Step: [3627] d_loss: 0.43119246, g_loss: 1.09996808\n",
      "Step: [3628] d_loss: 0.42970833, g_loss: 1.07558787\n",
      "Step: [3629] d_loss: 0.45524189, g_loss: 1.09402025\n",
      "Step: [3630] d_loss: 0.44293290, g_loss: 1.09169114\n",
      "Step: [3631] d_loss: 0.42877689, g_loss: 1.09699368\n",
      "Step: [3632] d_loss: 0.42654389, g_loss: 1.13661194\n",
      "Step: [3633] d_loss: 0.42689139, g_loss: 1.11716175\n",
      "Step: [3634] d_loss: 0.42987651, g_loss: 1.13834262\n",
      "Step: [3635] d_loss: 0.42894733, g_loss: 1.10373938\n",
      "Step: [3636] d_loss: 0.42755127, g_loss: 1.11730218\n",
      "Step: [3637] d_loss: 0.42943835, g_loss: 1.09036875\n",
      "Step: [3638] d_loss: 0.42885476, g_loss: 1.05086231\n",
      "Step: [3639] d_loss: 0.43304092, g_loss: 1.08477104\n",
      "Step: [3640] d_loss: 0.43031538, g_loss: 1.08256197\n",
      "Step: [3641] d_loss: 0.42867526, g_loss: 1.05964410\n",
      "Step: [3642] d_loss: 0.43051335, g_loss: 1.08604789\n",
      "Step: [3643] d_loss: 0.42896515, g_loss: 1.11064863\n",
      "Step: [3644] d_loss: 0.42966536, g_loss: 1.13612330\n",
      "Step: [3645] d_loss: 0.42925897, g_loss: 1.11976182\n",
      "Step: [3646] d_loss: 0.42888334, g_loss: 1.10468054\n",
      "Step: [3647] d_loss: 0.42504889, g_loss: 1.13027561\n",
      "Step: [3648] d_loss: 0.42830062, g_loss: 1.11575484\n",
      "Step: [3649] d_loss: 0.42410284, g_loss: 1.09266269\n",
      "Step: [3650] d_loss: 0.42807555, g_loss: 1.08599639\n",
      "Step: [3651] d_loss: 0.43075538, g_loss: 1.09857535\n",
      "Step: [3652] d_loss: 0.43843549, g_loss: 1.12171686\n",
      "Step: [3653] d_loss: 0.42978096, g_loss: 1.12637222\n",
      "Step: [3654] d_loss: 0.42889079, g_loss: 1.12635577\n",
      "Step: [3655] d_loss: 0.42637900, g_loss: 1.08863080\n",
      "Step: [3656] d_loss: 0.43036720, g_loss: 1.15708256\n",
      "Step: [3657] d_loss: 0.43073142, g_loss: 1.10744309\n",
      "Step: [3658] d_loss: 0.42517719, g_loss: 1.13163280\n",
      "Step: [3659] d_loss: 0.42912728, g_loss: 1.13440335\n",
      "Step: [3660] d_loss: 0.42604578, g_loss: 1.12819064\n",
      "Step: [3661] d_loss: 0.42711806, g_loss: 1.11441123\n",
      "Step: [3662] d_loss: 0.42648479, g_loss: 1.11116970\n",
      "Step: [3663] d_loss: 0.42739677, g_loss: 1.12686265\n",
      "Step: [3664] d_loss: 0.42828047, g_loss: 1.07708871\n",
      "Step: [3665] d_loss: 0.43502113, g_loss: 1.10672748\n",
      "Step: [3666] d_loss: 0.44731638, g_loss: 1.10891318\n",
      "Step: [3667] d_loss: 0.43430752, g_loss: 1.14206505\n",
      "Step: [3668] d_loss: 0.43056369, g_loss: 1.11656237\n",
      "Step: [3669] d_loss: 0.42808253, g_loss: 1.10002756\n",
      "Step: [3670] d_loss: 0.43033049, g_loss: 1.13706672\n",
      "Step: [3671] d_loss: 0.43281800, g_loss: 1.15890229\n",
      "Step: [3672] d_loss: 0.43142390, g_loss: 1.10423315\n",
      "Step: [3673] d_loss: 0.42848682, g_loss: 1.17387354\n",
      "Step: [3674] d_loss: 0.43037167, g_loss: 1.11133456\n",
      "Step: [3675] d_loss: 0.42545596, g_loss: 1.12100303\n",
      "Step: [3676] d_loss: 0.42840201, g_loss: 1.11524999\n",
      "Step: [3677] d_loss: 0.43169001, g_loss: 1.13341713\n",
      "Step: [3678] d_loss: 0.42962018, g_loss: 1.13727927\n",
      "Step: [3679] d_loss: 0.42981610, g_loss: 1.13505995\n",
      "Step: [3680] d_loss: 0.42613137, g_loss: 1.12575424\n",
      "Step: [3681] d_loss: 0.42786151, g_loss: 1.07966530\n",
      "Step: [3682] d_loss: 0.42581660, g_loss: 1.12025476\n",
      "Step: [3683] d_loss: 0.43065047, g_loss: 1.14747322\n",
      "Step: [3684] d_loss: 0.42905068, g_loss: 1.10662162\n",
      "Step: [3685] d_loss: 0.42757449, g_loss: 1.10488725\n",
      "Step: [3686] d_loss: 0.42868587, g_loss: 1.12454391\n",
      "Step: [3687] d_loss: 0.43720308, g_loss: 1.10896015\n",
      "Step: [3688] d_loss: 0.43337291, g_loss: 1.11499584\n",
      "Step: [3689] d_loss: 0.43056357, g_loss: 1.09624958\n",
      "Step: [3690] d_loss: 0.43028113, g_loss: 1.14161837\n",
      "Step: [3691] d_loss: 0.43098867, g_loss: 1.19298482\n",
      "Step: [3692] d_loss: 0.43134972, g_loss: 1.13982093\n",
      "Step: [3693] d_loss: 0.43077928, g_loss: 1.14240193\n",
      "Step: [3694] d_loss: 0.42819616, g_loss: 1.15425670\n",
      "Step: [3695] d_loss: 0.42875031, g_loss: 1.10742390\n",
      "Step: [3696] d_loss: 0.43122455, g_loss: 1.13620627\n",
      "Step: [3697] d_loss: 0.42987564, g_loss: 1.15910542\n",
      "Step: [3698] d_loss: 0.42668724, g_loss: 1.13071895\n",
      "Step: [3699] d_loss: 0.42764881, g_loss: 1.10036290\n",
      "Step: [3700] d_loss: 0.42837414, g_loss: 1.09141099\n",
      "Step: [3701] d_loss: 0.42774177, g_loss: 1.12867951\n",
      "Step: [3702] d_loss: 0.42551526, g_loss: 1.13039398\n",
      "Step: [3703] d_loss: 0.44405589, g_loss: 1.09723139\n",
      "Step: [3704] d_loss: 0.43718922, g_loss: 1.13951123\n",
      "Step: [3705] d_loss: 0.43756834, g_loss: 1.09455633\n",
      "Step: [3706] d_loss: 0.43607149, g_loss: 1.11530733\n",
      "Step: [3707] d_loss: 0.42870939, g_loss: 1.13309574\n",
      "Step: [3708] d_loss: 0.42648399, g_loss: 1.10510039\n",
      "Step: [3709] d_loss: 0.42978007, g_loss: 1.10570550\n",
      "Step: [3710] d_loss: 0.42544505, g_loss: 1.11243963\n",
      "Step: [3711] d_loss: 0.42587948, g_loss: 1.13539779\n",
      "Step: [3712] d_loss: 0.42732921, g_loss: 1.11038816\n",
      "Step: [3713] d_loss: 0.42907977, g_loss: 1.09797823\n",
      "Step: [3714] d_loss: 0.42626172, g_loss: 1.12175107\n",
      "Step: [3715] d_loss: 0.42801201, g_loss: 1.09017849\n",
      "Step: [3716] d_loss: 0.42723569, g_loss: 1.14489365\n",
      "Step: [3717] d_loss: 0.42570648, g_loss: 1.17235339\n",
      "Step: [3718] d_loss: 0.42851326, g_loss: 1.07269156\n",
      "Step: [3719] d_loss: 0.42884406, g_loss: 1.12318778\n",
      "Step: [3720] d_loss: 0.43138120, g_loss: 1.10864115\n",
      "Step: [3721] d_loss: 0.42538458, g_loss: 1.11039460\n",
      "Step: [3722] d_loss: 0.42570081, g_loss: 1.08908761\n",
      "Step: [3723] d_loss: 0.43164292, g_loss: 1.10816336\n",
      "Step: [3724] d_loss: 0.43226019, g_loss: 1.11037219\n",
      "Step: [3725] d_loss: 0.42809474, g_loss: 1.12184644\n",
      "Step: [3726] d_loss: 0.42970020, g_loss: 1.10866940\n",
      "Step: [3727] d_loss: 0.42966259, g_loss: 1.12620354\n",
      "Step: [3728] d_loss: 0.42942697, g_loss: 1.11621702\n",
      "Step: [3729] d_loss: 0.42560542, g_loss: 1.10666335\n",
      "Step: [3730] d_loss: 0.43007860, g_loss: 1.06746233\n",
      "Step: [3731] d_loss: 0.42642453, g_loss: 1.11713028\n",
      "Step: [3732] d_loss: 0.42692038, g_loss: 1.11647701\n",
      "Step: [3733] d_loss: 0.43204501, g_loss: 1.11910880\n",
      "Step: [3734] d_loss: 0.42873564, g_loss: 1.10020173\n",
      "Step: [3735] d_loss: 0.43023905, g_loss: 1.13345480\n",
      "Step: [3736] d_loss: 0.42771891, g_loss: 1.10029650\n",
      "Step: [3737] d_loss: 0.43596202, g_loss: 1.12084246\n",
      "Step: [3738] d_loss: 0.43251926, g_loss: 1.14306116\n",
      "Step: [3739] d_loss: 0.43105721, g_loss: 1.14511752\n",
      "Step: [3740] d_loss: 0.43090096, g_loss: 1.11486733\n",
      "Step: [3741] d_loss: 0.43205917, g_loss: 1.11007607\n",
      "Step: [3742] d_loss: 0.43046597, g_loss: 1.16293275\n",
      "Step: [3743] d_loss: 0.44948649, g_loss: 1.12649322\n",
      "Step: [3744] d_loss: 0.42936417, g_loss: 1.09979999\n",
      "Step: [3745] d_loss: 0.42985910, g_loss: 1.12170267\n",
      "Step: [3746] d_loss: 0.42878273, g_loss: 1.10544312\n",
      "Step: [3747] d_loss: 0.42980331, g_loss: 1.05805171\n",
      "Step: [3748] d_loss: 0.42772433, g_loss: 1.10077274\n",
      "Step: [3749] d_loss: 0.42979106, g_loss: 1.12199461\n",
      "Step: [3750] d_loss: 0.42770159, g_loss: 1.10218382\n",
      "Step: [3751] d_loss: 0.42714331, g_loss: 1.10195589\n",
      "Step: [3752] d_loss: 0.42655727, g_loss: 1.11435759\n",
      "Step: [3753] d_loss: 0.42491460, g_loss: 1.09429502\n",
      "Step: [3754] d_loss: 0.42430803, g_loss: 1.10543394\n",
      "Step: [3755] d_loss: 0.42687833, g_loss: 1.09064043\n",
      "Step: [3756] d_loss: 0.42688724, g_loss: 1.10058820\n",
      "Step: [3757] d_loss: 0.42673454, g_loss: 1.10058057\n",
      "Step: [3758] d_loss: 0.43071106, g_loss: 1.09307051\n",
      "Step: [3759] d_loss: 0.42791069, g_loss: 1.06255531\n",
      "Step: [3760] d_loss: 0.42801005, g_loss: 1.07523072\n",
      "Step: [3761] d_loss: 0.42538792, g_loss: 1.07330537\n",
      "Step: [3762] d_loss: 0.43211871, g_loss: 1.09237087\n",
      "Step: [3763] d_loss: 0.43436563, g_loss: 1.07903695\n",
      "Step: [3764] d_loss: 0.42799780, g_loss: 1.11302054\n",
      "Step: [3765] d_loss: 0.43056592, g_loss: 1.06700659\n",
      "Step: [3766] d_loss: 0.42979988, g_loss: 1.11296272\n",
      "Step: [3767] d_loss: 0.43279845, g_loss: 1.11593294\n",
      "Step: [3768] d_loss: 0.43089390, g_loss: 1.06740475\n",
      "Step: [3769] d_loss: 0.42967543, g_loss: 1.08822238\n",
      "Step: [3770] d_loss: 0.42529169, g_loss: 1.07253134\n",
      "Step: [3771] d_loss: 0.42759117, g_loss: 1.11113167\n",
      "Step: [3772] d_loss: 0.42741308, g_loss: 1.10650563\n",
      "Step: [3773] d_loss: 0.42652377, g_loss: 1.13034594\n",
      "Step: [3774] d_loss: 0.43086660, g_loss: 1.10483885\n",
      "Step: [3775] d_loss: 0.43004492, g_loss: 1.09163904\n",
      "Step: [3776] d_loss: 0.42813563, g_loss: 1.08826995\n",
      "Step: [3777] d_loss: 0.42564654, g_loss: 1.07758784\n",
      "Step: [3778] d_loss: 0.42795002, g_loss: 1.09414470\n",
      "Step: [3779] d_loss: 0.43122989, g_loss: 1.10019839\n",
      "Step: [3780] d_loss: 0.42993835, g_loss: 1.07602668\n",
      "Step: [3781] d_loss: 0.42697510, g_loss: 1.10447693\n",
      "Step: [3782] d_loss: 0.43013376, g_loss: 1.09461999\n",
      "Step: [3783] d_loss: 0.43210441, g_loss: 1.10685587\n",
      "Step: [3784] d_loss: 0.42672434, g_loss: 1.11169028\n",
      "Step: [3785] d_loss: 0.42951199, g_loss: 1.07752240\n",
      "Step: [3786] d_loss: 0.43471444, g_loss: 1.05403268\n",
      "Step: [3787] d_loss: 0.42877519, g_loss: 1.14090490\n",
      "Step: [3788] d_loss: 0.42981935, g_loss: 1.11731410\n",
      "Step: [3789] d_loss: 0.43273169, g_loss: 1.07889271\n",
      "Step: [3790] d_loss: 0.43241784, g_loss: 1.07246983\n",
      "Step: [3791] d_loss: 0.42910033, g_loss: 1.08264220\n",
      "Step: [3792] d_loss: 0.42628109, g_loss: 1.07387841\n",
      "Step: [3793] d_loss: 0.42882687, g_loss: 1.08105063\n",
      "Step: [3794] d_loss: 0.42974064, g_loss: 1.08791947\n",
      "Step: [3795] d_loss: 0.42670679, g_loss: 1.09531260\n",
      "Step: [3796] d_loss: 0.42569011, g_loss: 1.05742717\n",
      "Step: [3797] d_loss: 0.43150252, g_loss: 1.07911861\n",
      "Step: [3798] d_loss: 0.43689084, g_loss: 1.08067489\n",
      "Step: [3799] d_loss: 0.48662397, g_loss: 1.12740910\n",
      "Step: [3800] d_loss: 0.47288170, g_loss: 1.15059066\n",
      "Step: [3801] d_loss: 0.46761581, g_loss: 1.14208829\n",
      "Step: [3802] d_loss: 0.44411677, g_loss: 1.17961979\n",
      "Step: [3803] d_loss: 0.43880031, g_loss: 1.10662210\n",
      "Step: [3804] d_loss: 0.43456277, g_loss: 1.11082256\n",
      "Step: [3805] d_loss: 0.43067503, g_loss: 1.15434778\n",
      "Step: [3806] d_loss: 0.43145704, g_loss: 1.11518359\n",
      "Step: [3807] d_loss: 0.42931569, g_loss: 1.10846233\n",
      "Step: [3808] d_loss: 0.42676052, g_loss: 1.10984313\n",
      "Step: [3809] d_loss: 0.42774507, g_loss: 1.09193873\n",
      "Step: [3810] d_loss: 0.42761329, g_loss: 1.07235742\n",
      "Step: [3811] d_loss: 0.43059847, g_loss: 1.10070968\n",
      "Step: [3812] d_loss: 0.42677745, g_loss: 1.05169642\n",
      "Step: [3813] d_loss: 0.42914656, g_loss: 1.10438550\n",
      "Step: [3814] d_loss: 0.42658448, g_loss: 1.06654358\n",
      "Step: [3815] d_loss: 0.42914900, g_loss: 1.05019307\n",
      "Step: [3816] d_loss: 0.42778477, g_loss: 1.14037776\n",
      "Step: [3817] d_loss: 0.42448771, g_loss: 1.09429646\n",
      "Step: [3818] d_loss: 0.43948889, g_loss: 1.08407354\n",
      "Step: [3819] d_loss: 0.43087757, g_loss: 1.12190628\n",
      "Step: [3820] d_loss: 0.43024296, g_loss: 1.09825504\n",
      "Step: [3821] d_loss: 0.42973942, g_loss: 1.06872630\n",
      "Step: [3822] d_loss: 0.42650190, g_loss: 1.06818223\n",
      "Step: [3823] d_loss: 0.42683843, g_loss: 1.07274055\n",
      "Step: [3824] d_loss: 0.42588973, g_loss: 1.10909474\n",
      "Step: [3825] d_loss: 0.42878500, g_loss: 1.06769919\n",
      "Step: [3826] d_loss: 0.43027315, g_loss: 1.12836576\n",
      "Step: [3827] d_loss: 0.42980045, g_loss: 1.05375135\n",
      "Step: [3828] d_loss: 0.42793423, g_loss: 1.11205900\n",
      "Step: [3829] d_loss: 0.42806900, g_loss: 1.05715561\n",
      "Step: [3830] d_loss: 0.42460856, g_loss: 1.07249808\n",
      "Step: [3831] d_loss: 0.42364272, g_loss: 1.07258618\n",
      "Step: [3832] d_loss: 0.42374295, g_loss: 1.10370398\n",
      "Step: [3833] d_loss: 0.42697790, g_loss: 1.09318960\n",
      "Step: [3834] d_loss: 0.42676902, g_loss: 1.04923654\n",
      "Step: [3835] d_loss: 0.43617201, g_loss: 1.08292913\n",
      "Step: [3836] d_loss: 0.43910587, g_loss: 1.09766638\n",
      "Step: [3837] d_loss: 0.42882448, g_loss: 1.13950133\n",
      "Step: [3838] d_loss: 0.42403078, g_loss: 1.12069929\n",
      "Step: [3839] d_loss: 0.42809409, g_loss: 1.08084452\n",
      "Step: [3840] d_loss: 0.42559844, g_loss: 1.13294172\n",
      "Step: [3841] d_loss: 0.42753968, g_loss: 1.07892239\n",
      "Step: [3842] d_loss: 0.43195632, g_loss: 1.09809577\n",
      "Step: [3843] d_loss: 0.42798671, g_loss: 1.08809316\n",
      "Step: [3844] d_loss: 0.43795103, g_loss: 1.10063851\n",
      "Step: [3845] d_loss: 0.43806276, g_loss: 1.11058033\n",
      "Step: [3846] d_loss: 0.43601078, g_loss: 1.10391796\n",
      "Step: [3847] d_loss: 0.43589526, g_loss: 1.10228014\n",
      "Step: [3848] d_loss: 0.43788373, g_loss: 1.09271801\n",
      "Step: [3849] d_loss: 0.43617848, g_loss: 1.08282888\n",
      "Step: [3850] d_loss: 0.43066254, g_loss: 1.06071997\n",
      "Step: [3851] d_loss: 0.42642221, g_loss: 1.10535049\n",
      "Step: [3852] d_loss: 0.42989492, g_loss: 1.08348513\n",
      "Step: [3853] d_loss: 0.43028873, g_loss: 1.08421004\n",
      "Step: [3854] d_loss: 0.42876235, g_loss: 1.10562313\n",
      "Step: [3855] d_loss: 0.42695385, g_loss: 1.11814952\n",
      "Step: [3856] d_loss: 0.42681730, g_loss: 1.06722176\n",
      "Step: [3857] d_loss: 0.42884994, g_loss: 1.07743454\n",
      "Step: [3858] d_loss: 0.43142709, g_loss: 1.10511601\n",
      "Step: [3859] d_loss: 0.42469648, g_loss: 1.07682312\n",
      "Step: [3860] d_loss: 0.43349594, g_loss: 1.11981881\n",
      "Step: [3861] d_loss: 0.43177453, g_loss: 1.09177542\n",
      "Step: [3862] d_loss: 0.43241701, g_loss: 1.08653009\n",
      "Step: [3863] d_loss: 0.42968181, g_loss: 1.04876959\n",
      "Step: [3864] d_loss: 0.42812967, g_loss: 1.06339931\n",
      "Step: [3865] d_loss: 0.43063337, g_loss: 1.11767554\n",
      "Step: [3866] d_loss: 0.42942792, g_loss: 1.08000767\n",
      "Step: [3867] d_loss: 0.42756248, g_loss: 1.07559180\n",
      "Step: [3868] d_loss: 0.42601392, g_loss: 1.09247983\n",
      "Step: [3869] d_loss: 0.42600700, g_loss: 1.06861126\n",
      "Step: [3870] d_loss: 0.42779678, g_loss: 1.07384694\n",
      "Step: [3871] d_loss: 0.42962420, g_loss: 1.09115362\n",
      "Step: [3872] d_loss: 0.42971694, g_loss: 1.06449878\n",
      "Step: [3873] d_loss: 0.43062586, g_loss: 1.07367015\n",
      "Step: [3874] d_loss: 0.42615995, g_loss: 1.05409408\n",
      "Step: [3875] d_loss: 0.43167594, g_loss: 1.02732384\n",
      "Step: [3876] d_loss: 0.42730081, g_loss: 1.07744014\n",
      "Step: [3877] d_loss: 0.42632881, g_loss: 1.11689401\n",
      "Step: [3878] d_loss: 0.43244416, g_loss: 1.05356622\n",
      "Step: [3879] d_loss: 0.42754379, g_loss: 1.08242321\n",
      "Step: [3880] d_loss: 0.42614105, g_loss: 1.11057889\n",
      "Step: [3881] d_loss: 0.42644262, g_loss: 1.14329863\n",
      "Step: [3882] d_loss: 0.42662561, g_loss: 1.10348308\n",
      "Step: [3883] d_loss: 0.43074635, g_loss: 1.11226869\n",
      "Step: [3884] d_loss: 0.42510319, g_loss: 1.09425294\n",
      "Step: [3885] d_loss: 0.42613059, g_loss: 1.08697402\n",
      "Step: [3886] d_loss: 0.43279457, g_loss: 1.10121059\n",
      "Step: [3887] d_loss: 0.43400437, g_loss: 1.11626542\n",
      "Step: [3888] d_loss: 0.43186620, g_loss: 1.11266840\n",
      "Step: [3889] d_loss: 0.43423086, g_loss: 1.07453203\n",
      "Step: [3890] d_loss: 0.42521167, g_loss: 1.11246395\n",
      "Step: [3891] d_loss: 0.43232748, g_loss: 1.12569189\n",
      "Step: [3892] d_loss: 0.42706156, g_loss: 1.07925880\n",
      "Step: [3893] d_loss: 0.42603827, g_loss: 1.10737240\n",
      "Step: [3894] d_loss: 0.42345783, g_loss: 1.11922562\n",
      "Step: [3895] d_loss: 0.42629820, g_loss: 1.10700107\n",
      "Step: [3896] d_loss: 0.42317426, g_loss: 1.12514877\n",
      "Step: [3897] d_loss: 0.42600143, g_loss: 1.09410918\n",
      "Step: [3898] d_loss: 0.42887637, g_loss: 1.08187246\n",
      "Step: [3899] d_loss: 0.42619890, g_loss: 1.11718786\n",
      "Step: [3900] d_loss: 0.42636114, g_loss: 1.09192073\n",
      "Step: [3901] d_loss: 0.42739782, g_loss: 1.11116576\n",
      "Step: [3902] d_loss: 0.45054427, g_loss: 1.08451641\n",
      "Step: [3903] d_loss: 0.44374168, g_loss: 1.12765217\n",
      "Step: [3904] d_loss: 0.43512541, g_loss: 1.12053454\n",
      "Step: [3905] d_loss: 0.43370456, g_loss: 1.09573650\n",
      "Step: [3906] d_loss: 0.42766023, g_loss: 1.13752115\n",
      "Step: [3907] d_loss: 0.42665321, g_loss: 1.07203639\n",
      "Step: [3908] d_loss: 0.42742187, g_loss: 1.08191597\n",
      "Step: [3909] d_loss: 0.44566733, g_loss: 1.10723627\n",
      "Step: [3910] d_loss: 0.43219703, g_loss: 1.08795285\n",
      "Step: [3911] d_loss: 0.42556119, g_loss: 1.09782374\n",
      "Step: [3912] d_loss: 0.42723817, g_loss: 1.13527894\n",
      "Step: [3913] d_loss: 0.43584645, g_loss: 1.08516502\n",
      "Step: [3914] d_loss: 0.43785059, g_loss: 1.08282435\n",
      "Step: [3915] d_loss: 0.43085545, g_loss: 1.06350183\n",
      "Step: [3916] d_loss: 0.43101588, g_loss: 1.11274147\n",
      "Step: [3917] d_loss: 0.42530698, g_loss: 1.09684932\n",
      "Step: [3918] d_loss: 0.43043545, g_loss: 1.07219315\n",
      "Step: [3919] d_loss: 0.42837214, g_loss: 1.09953117\n",
      "Step: [3920] d_loss: 0.42675820, g_loss: 1.06713402\n",
      "Step: [3921] d_loss: 0.42885059, g_loss: 1.10877872\n",
      "Step: [3922] d_loss: 0.42749178, g_loss: 1.08685756\n",
      "Step: [3923] d_loss: 0.43169892, g_loss: 1.07620490\n",
      "Step: [3924] d_loss: 0.42611703, g_loss: 1.09037936\n",
      "Step: [3925] d_loss: 0.42615309, g_loss: 1.09942341\n",
      "Step: [3926] d_loss: 0.42549932, g_loss: 1.05878031\n",
      "Step: [3927] d_loss: 0.42564788, g_loss: 1.11212957\n",
      "Step: [3928] d_loss: 0.42451501, g_loss: 1.06586063\n",
      "Step: [3929] d_loss: 0.42722180, g_loss: 1.11141348\n",
      "Step: [3930] d_loss: 0.42524722, g_loss: 1.09517896\n",
      "Step: [3931] d_loss: 0.42540833, g_loss: 1.08029282\n",
      "Step: [3932] d_loss: 0.42355648, g_loss: 1.09977353\n",
      "Step: [3933] d_loss: 0.42602736, g_loss: 1.09008002\n",
      "Step: [3934] d_loss: 0.42948440, g_loss: 1.09447682\n",
      "Step: [3935] d_loss: 0.42605281, g_loss: 1.04177904\n",
      "Step: [3936] d_loss: 0.43001842, g_loss: 1.07641602\n",
      "Step: [3937] d_loss: 0.42521045, g_loss: 1.09634399\n",
      "Step: [3938] d_loss: 0.43259317, g_loss: 1.09847867\n",
      "Step: [3939] d_loss: 0.42454916, g_loss: 1.07774246\n",
      "Step: [3940] d_loss: 0.42319134, g_loss: 1.09082389\n",
      "Step: [3941] d_loss: 0.42809266, g_loss: 1.05986464\n",
      "Step: [3942] d_loss: 0.42760605, g_loss: 1.05678427\n",
      "Step: [3943] d_loss: 0.42666978, g_loss: 1.08642578\n",
      "Step: [3944] d_loss: 0.42522922, g_loss: 1.11140871\n",
      "Step: [3945] d_loss: 0.42360905, g_loss: 1.08049977\n",
      "Step: [3946] d_loss: 0.42888933, g_loss: 1.07974148\n",
      "Step: [3947] d_loss: 0.42513368, g_loss: 1.12612402\n",
      "Step: [3948] d_loss: 0.42802775, g_loss: 1.07894874\n",
      "Step: [3949] d_loss: 0.44377410, g_loss: 1.05272806\n",
      "Step: [3950] d_loss: 0.43262112, g_loss: 1.07827687\n",
      "Step: [3951] d_loss: 0.43350908, g_loss: 1.06579113\n",
      "Step: [3952] d_loss: 0.43164229, g_loss: 1.06960452\n",
      "Step: [3953] d_loss: 0.43324280, g_loss: 1.13887525\n",
      "Step: [3954] d_loss: 0.43055606, g_loss: 1.08524358\n",
      "Step: [3955] d_loss: 0.42706481, g_loss: 1.08116055\n",
      "Step: [3956] d_loss: 0.42778227, g_loss: 1.11604452\n",
      "Step: [3957] d_loss: 0.43085393, g_loss: 1.08748806\n",
      "Step: [3958] d_loss: 0.43270472, g_loss: 1.10194325\n",
      "Step: [3959] d_loss: 0.42917302, g_loss: 1.09290123\n",
      "Step: [3960] d_loss: 0.43494967, g_loss: 1.09873223\n",
      "Step: [3961] d_loss: 0.43141723, g_loss: 1.11933231\n",
      "Step: [3962] d_loss: 0.43286803, g_loss: 1.04128957\n",
      "Step: [3963] d_loss: 0.43186602, g_loss: 1.07199121\n",
      "Step: [3964] d_loss: 0.42918891, g_loss: 1.09034097\n",
      "Step: [3965] d_loss: 0.42678192, g_loss: 1.10995698\n",
      "Step: [3966] d_loss: 0.42828238, g_loss: 1.04895580\n",
      "Step: [3967] d_loss: 0.42919639, g_loss: 1.10304153\n",
      "Step: [3968] d_loss: 0.43111286, g_loss: 1.12176514\n",
      "Step: [3969] d_loss: 0.43247551, g_loss: 1.10639799\n",
      "Step: [3970] d_loss: 0.43750349, g_loss: 1.12070322\n",
      "Step: [3971] d_loss: 0.42787704, g_loss: 1.10202909\n",
      "Step: [3972] d_loss: 0.43092406, g_loss: 1.09859681\n",
      "Step: [3973] d_loss: 0.43119764, g_loss: 1.12421823\n",
      "Step: [3974] d_loss: 0.43016565, g_loss: 1.08502650\n",
      "Step: [3975] d_loss: 0.43242544, g_loss: 1.09865093\n",
      "Step: [3976] d_loss: 0.42619416, g_loss: 1.11108088\n",
      "Step: [3977] d_loss: 0.42818639, g_loss: 1.11161709\n",
      "Step: [3978] d_loss: 0.43239501, g_loss: 1.07205701\n",
      "Step: [3979] d_loss: 0.43248463, g_loss: 1.10855031\n",
      "Step: [3980] d_loss: 0.43450671, g_loss: 1.10362983\n",
      "Step: [3981] d_loss: 0.43334770, g_loss: 1.07964170\n",
      "Step: [3982] d_loss: 0.43135616, g_loss: 1.09905171\n",
      "Step: [3983] d_loss: 0.43039486, g_loss: 1.08641386\n",
      "Step: [3984] d_loss: 0.43040147, g_loss: 1.09607148\n",
      "Step: [3985] d_loss: 0.43143663, g_loss: 1.11376941\n",
      "Step: [3986] d_loss: 0.42857838, g_loss: 1.11160362\n",
      "Step: [3987] d_loss: 0.42732972, g_loss: 1.10422873\n",
      "Step: [3988] d_loss: 0.42798129, g_loss: 1.12840569\n",
      "Step: [3989] d_loss: 0.42861718, g_loss: 1.09064722\n",
      "Step: [3990] d_loss: 0.42511773, g_loss: 1.06186688\n",
      "Step: [3991] d_loss: 0.42559651, g_loss: 1.07470322\n",
      "Step: [3992] d_loss: 0.42774057, g_loss: 1.04941690\n",
      "Step: [3993] d_loss: 0.43150601, g_loss: 1.06561363\n",
      "Step: [3994] d_loss: 0.42875317, g_loss: 1.03005588\n",
      "Step: [3995] d_loss: 0.43674564, g_loss: 1.08848214\n",
      "Step: [3996] d_loss: 0.43711591, g_loss: 1.04755008\n",
      "Step: [3997] d_loss: 0.99039525, g_loss: 3.28170371\n",
      "Step: [3998] d_loss: 0.92988276, g_loss: 1.26845121\n",
      "Step: [3999] d_loss: 0.56029129, g_loss: 1.23403323\n",
      "Step: [4000] d_loss: 0.51406658, g_loss: 1.24120712\n",
      "Step: [4001] d_loss: 0.48013547, g_loss: 1.25349426\n",
      "Step: [4002] d_loss: 0.50891238, g_loss: 1.23512983\n",
      "Step: [4003] d_loss: 0.47638404, g_loss: 1.20081115\n",
      "Step: [4004] d_loss: 0.49399930, g_loss: 1.23400307\n",
      "Step: [4005] d_loss: 0.46133396, g_loss: 1.20273685\n",
      "Step: [4006] d_loss: 0.46616146, g_loss: 1.19266284\n",
      "Step: [4007] d_loss: 0.45974043, g_loss: 1.20343482\n",
      "Step: [4008] d_loss: 0.46435988, g_loss: 1.13953805\n",
      "Step: [4009] d_loss: 0.46793067, g_loss: 1.15888834\n",
      "Step: [4010] d_loss: 0.46592629, g_loss: 1.22663844\n",
      "Step: [4011] d_loss: 0.45526484, g_loss: 1.14611256\n",
      "Step: [4012] d_loss: 0.44991001, g_loss: 1.10351157\n",
      "Step: [4013] d_loss: 0.47603515, g_loss: 1.16362751\n",
      "Step: [4014] d_loss: 0.45880786, g_loss: 1.08987987\n",
      "Step: [4015] d_loss: 0.46574900, g_loss: 1.15005958\n",
      "Step: [4016] d_loss: 0.45885339, g_loss: 1.12158167\n",
      "Step: [4017] d_loss: 0.44705516, g_loss: 1.09046888\n",
      "Step: [4018] d_loss: 0.45804960, g_loss: 1.11112046\n",
      "Step: [4019] d_loss: 0.45427784, g_loss: 1.09143174\n",
      "Step: [4020] d_loss: 0.45317614, g_loss: 1.10944569\n",
      "Step: [4021] d_loss: 0.45027769, g_loss: 1.14386380\n",
      "Step: [4022] d_loss: 0.46042424, g_loss: 1.09654891\n",
      "Step: [4023] d_loss: 0.44545078, g_loss: 1.13857007\n",
      "Step: [4024] d_loss: 0.45413560, g_loss: 1.08504272\n",
      "Step: [4025] d_loss: 0.45015833, g_loss: 1.11543381\n",
      "Step: [4026] d_loss: 0.44352859, g_loss: 1.10247517\n",
      "Step: [4027] d_loss: 0.44343814, g_loss: 1.07294202\n",
      "Step: [4028] d_loss: 0.45179710, g_loss: 1.07160819\n",
      "Step: [4029] d_loss: 0.44122034, g_loss: 1.14548671\n",
      "Step: [4030] d_loss: 0.44235322, g_loss: 1.08931124\n",
      "Step: [4031] d_loss: 0.43998724, g_loss: 1.13045025\n",
      "Step: [4032] d_loss: 0.44266340, g_loss: 1.12852395\n",
      "Step: [4033] d_loss: 0.43787858, g_loss: 1.13330042\n",
      "Step: [4034] d_loss: 0.43904662, g_loss: 1.13460624\n",
      "Step: [4035] d_loss: 0.44693524, g_loss: 1.13394606\n",
      "Step: [4036] d_loss: 0.45081657, g_loss: 1.16084754\n",
      "Step: [4037] d_loss: 0.44364792, g_loss: 1.12219048\n",
      "Step: [4038] d_loss: 0.44705880, g_loss: 1.17330277\n",
      "Step: [4039] d_loss: 0.44351417, g_loss: 1.15900850\n",
      "Step: [4040] d_loss: 0.44179189, g_loss: 1.11978340\n",
      "Step: [4041] d_loss: 0.43938598, g_loss: 1.12672830\n",
      "Step: [4042] d_loss: 0.45704499, g_loss: 1.13810623\n",
      "Step: [4043] d_loss: 0.45804906, g_loss: 1.08842158\n",
      "Step: [4044] d_loss: 0.45887709, g_loss: 1.18892717\n",
      "Step: [4045] d_loss: 0.44494599, g_loss: 1.14226055\n",
      "Step: [4046] d_loss: 0.43914467, g_loss: 1.13873625\n",
      "Step: [4047] d_loss: 0.44455844, g_loss: 1.13911641\n",
      "Step: [4048] d_loss: 0.43651041, g_loss: 1.11939096\n",
      "Step: [4049] d_loss: 0.43405029, g_loss: 1.10396266\n",
      "Step: [4050] d_loss: 0.44139397, g_loss: 1.11640906\n",
      "Step: [4051] d_loss: 0.43389317, g_loss: 1.10867441\n",
      "Step: [4052] d_loss: 0.43521193, g_loss: 1.11990750\n",
      "Step: [4053] d_loss: 0.43973580, g_loss: 1.08893287\n",
      "Step: [4054] d_loss: 0.43904579, g_loss: 1.06802678\n",
      "Step: [4055] d_loss: 0.43840531, g_loss: 1.07666063\n",
      "Step: [4056] d_loss: 0.44843161, g_loss: 1.07622850\n",
      "Step: [4057] d_loss: 0.44398287, g_loss: 1.09763372\n",
      "Step: [4058] d_loss: 0.43357092, g_loss: 1.11674118\n",
      "Step: [4059] d_loss: 0.43497977, g_loss: 1.09710443\n",
      "Step: [4060] d_loss: 0.43524024, g_loss: 1.05048120\n",
      "Step: [4061] d_loss: 0.44508931, g_loss: 1.05717719\n",
      "Step: [4062] d_loss: 0.44297427, g_loss: 1.10536623\n",
      "Step: [4063] d_loss: 0.43941516, g_loss: 1.09988058\n",
      "Step: [4064] d_loss: 0.44260669, g_loss: 1.05866504\n",
      "Step: [4065] d_loss: 0.43759710, g_loss: 1.09544885\n",
      "Step: [4066] d_loss: 0.44143477, g_loss: 1.11105096\n",
      "Step: [4067] d_loss: 0.43748534, g_loss: 1.11752057\n",
      "Step: [4068] d_loss: 0.43573391, g_loss: 1.08757246\n",
      "Step: [4069] d_loss: 0.44037935, g_loss: 1.07330847\n",
      "Step: [4070] d_loss: 0.43372771, g_loss: 1.08958578\n",
      "Step: [4071] d_loss: 0.43795615, g_loss: 1.12288368\n",
      "Step: [4072] d_loss: 0.43587264, g_loss: 1.08842945\n",
      "Step: [4073] d_loss: 0.43977037, g_loss: 1.11654127\n",
      "Step: [4074] d_loss: 0.44185394, g_loss: 1.09030306\n",
      "Step: [4075] d_loss: 0.44067976, g_loss: 1.09905064\n",
      "Step: [4076] d_loss: 0.43702239, g_loss: 1.07548344\n",
      "Step: [4077] d_loss: 0.43702117, g_loss: 1.09054983\n",
      "Step: [4078] d_loss: 0.43101496, g_loss: 1.12659657\n",
      "Step: [4079] d_loss: 0.43346703, g_loss: 1.08828771\n",
      "Step: [4080] d_loss: 0.43715265, g_loss: 1.08961236\n",
      "Step: [4081] d_loss: 0.44679421, g_loss: 1.08430541\n",
      "Step: [4082] d_loss: 0.44315368, g_loss: 1.11231637\n",
      "Step: [4083] d_loss: 0.43882304, g_loss: 1.15386081\n",
      "Step: [4084] d_loss: 0.43542016, g_loss: 1.11039746\n",
      "Step: [4085] d_loss: 0.44156790, g_loss: 1.07918656\n",
      "Step: [4086] d_loss: 0.43696517, g_loss: 1.13007081\n",
      "Step: [4087] d_loss: 0.43348435, g_loss: 1.14944148\n",
      "Step: [4088] d_loss: 0.44397387, g_loss: 1.11556149\n",
      "Step: [4089] d_loss: 0.43343687, g_loss: 1.12215972\n",
      "Step: [4090] d_loss: 0.43903854, g_loss: 1.10025096\n",
      "Step: [4091] d_loss: 0.43338686, g_loss: 1.10125136\n",
      "Step: [4092] d_loss: 0.43130186, g_loss: 1.11314511\n",
      "Step: [4093] d_loss: 0.43842345, g_loss: 1.07964623\n",
      "Step: [4094] d_loss: 0.43497327, g_loss: 1.13559294\n",
      "Step: [4095] d_loss: 0.43294069, g_loss: 1.09183347\n",
      "Step: [4096] d_loss: 0.43115878, g_loss: 1.09690213\n",
      "Step: [4097] d_loss: 0.43379402, g_loss: 1.08610988\n",
      "Step: [4098] d_loss: 0.43850607, g_loss: 1.09515142\n",
      "Step: [4099] d_loss: 0.46364331, g_loss: 1.13725162\n",
      "Step: [4100] d_loss: 0.44701961, g_loss: 1.11291552\n",
      "Step: [4101] d_loss: 0.43661869, g_loss: 1.08859229\n",
      "Step: [4102] d_loss: 0.44839609, g_loss: 1.08514893\n",
      "Step: [4103] d_loss: 0.44086757, g_loss: 1.14948237\n",
      "Step: [4104] d_loss: 0.43447644, g_loss: 1.11139739\n",
      "Step: [4105] d_loss: 0.43539104, g_loss: 1.07272732\n",
      "Step: [4106] d_loss: 0.43269530, g_loss: 1.14025378\n",
      "Step: [4107] d_loss: 0.43688387, g_loss: 1.07513750\n",
      "Step: [4108] d_loss: 0.44682157, g_loss: 1.09941292\n",
      "Step: [4109] d_loss: 0.44365567, g_loss: 1.08607960\n",
      "Step: [4110] d_loss: 0.43658006, g_loss: 1.09455144\n",
      "Step: [4111] d_loss: 0.44262564, g_loss: 1.11222231\n",
      "Step: [4112] d_loss: 0.43737465, g_loss: 1.14626300\n",
      "Step: [4113] d_loss: 0.44068384, g_loss: 1.10942435\n",
      "Step: [4114] d_loss: 0.44225326, g_loss: 1.10061049\n",
      "Step: [4115] d_loss: 0.43358138, g_loss: 1.10250092\n",
      "Step: [4116] d_loss: 0.43881389, g_loss: 1.11551595\n",
      "Step: [4117] d_loss: 0.44342074, g_loss: 1.09866822\n",
      "Step: [4118] d_loss: 0.43146923, g_loss: 1.09597254\n",
      "Step: [4119] d_loss: 0.43580809, g_loss: 1.10335708\n",
      "Step: [4120] d_loss: 0.43524992, g_loss: 1.14693964\n",
      "Step: [4121] d_loss: 0.43669006, g_loss: 1.11409640\n",
      "Step: [4122] d_loss: 0.43578124, g_loss: 1.11807108\n",
      "Step: [4123] d_loss: 0.43895680, g_loss: 1.10894549\n",
      "Step: [4124] d_loss: 0.43438047, g_loss: 1.14978206\n",
      "Step: [4125] d_loss: 0.42958656, g_loss: 1.12494183\n",
      "Step: [4126] d_loss: 0.44399184, g_loss: 1.09256387\n",
      "Step: [4127] d_loss: 0.43523350, g_loss: 1.12428904\n",
      "Step: [4128] d_loss: 0.43840742, g_loss: 1.08971977\n",
      "Step: [4129] d_loss: 0.43544951, g_loss: 1.10262656\n",
      "Step: [4130] d_loss: 0.43660146, g_loss: 1.09566963\n",
      "Step: [4131] d_loss: 0.45029175, g_loss: 1.08685005\n",
      "Step: [4132] d_loss: 0.44444823, g_loss: 1.05439794\n",
      "Step: [4133] d_loss: 0.43864864, g_loss: 1.12927759\n",
      "Step: [4134] d_loss: 0.44119620, g_loss: 1.13042188\n",
      "Step: [4135] d_loss: 0.44347471, g_loss: 1.09530735\n",
      "Step: [4136] d_loss: 0.44034746, g_loss: 1.11711335\n",
      "Step: [4137] d_loss: 0.43477505, g_loss: 1.10032392\n",
      "Step: [4138] d_loss: 0.43810499, g_loss: 1.13501370\n",
      "Step: [4139] d_loss: 0.43654099, g_loss: 1.10448694\n",
      "Step: [4140] d_loss: 0.43809274, g_loss: 1.09225845\n",
      "Step: [4141] d_loss: 0.43273562, g_loss: 1.12294340\n",
      "Step: [4142] d_loss: 0.43249899, g_loss: 1.07161307\n",
      "Step: [4143] d_loss: 0.43506438, g_loss: 1.07217956\n",
      "Step: [4144] d_loss: 0.43506289, g_loss: 1.10621440\n",
      "Step: [4145] d_loss: 0.43502426, g_loss: 1.09651101\n",
      "Step: [4146] d_loss: 0.43254760, g_loss: 1.06393993\n",
      "Step: [4147] d_loss: 0.43210709, g_loss: 1.07283378\n",
      "Step: [4148] d_loss: 0.43412584, g_loss: 1.07925200\n",
      "Step: [4149] d_loss: 0.43310487, g_loss: 1.08127987\n",
      "Step: [4150] d_loss: 0.43462476, g_loss: 1.05246794\n",
      "Step: [4151] d_loss: 0.45354927, g_loss: 1.13116705\n",
      "Step: [4152] d_loss: 0.43240499, g_loss: 1.08711994\n",
      "Step: [4153] d_loss: 0.44156650, g_loss: 1.08249867\n",
      "Step: [4154] d_loss: 0.43685561, g_loss: 1.11482155\n",
      "Step: [4155] d_loss: 0.43341461, g_loss: 1.07793820\n",
      "Step: [4156] d_loss: 0.44213942, g_loss: 1.11301756\n",
      "Step: [4157] d_loss: 0.44158772, g_loss: 1.07116961\n",
      "Step: [4158] d_loss: 0.43655422, g_loss: 1.10258210\n",
      "Step: [4159] d_loss: 0.44253904, g_loss: 1.07828081\n",
      "Step: [4160] d_loss: 0.43871841, g_loss: 1.10986221\n",
      "Step: [4161] d_loss: 0.43156701, g_loss: 1.13405919\n",
      "Step: [4162] d_loss: 0.43632698, g_loss: 1.07146585\n",
      "Step: [4163] d_loss: 0.43402839, g_loss: 1.06582355\n",
      "Step: [4164] d_loss: 0.43499130, g_loss: 1.08945072\n",
      "Step: [4165] d_loss: 0.43238923, g_loss: 1.12028408\n",
      "Step: [4166] d_loss: 0.45076609, g_loss: 1.07794309\n",
      "Step: [4167] d_loss: 0.43521941, g_loss: 1.11432087\n",
      "Step: [4168] d_loss: 0.43735909, g_loss: 1.12567234\n",
      "Step: [4169] d_loss: 0.43758327, g_loss: 1.09871936\n",
      "Step: [4170] d_loss: 0.43661234, g_loss: 1.09871650\n",
      "Step: [4171] d_loss: 0.43811154, g_loss: 1.10021341\n",
      "Step: [4172] d_loss: 0.43648028, g_loss: 1.06636822\n",
      "Step: [4173] d_loss: 0.43778750, g_loss: 1.10812593\n",
      "Step: [4174] d_loss: 0.43701008, g_loss: 1.10017240\n",
      "Step: [4175] d_loss: 0.43385804, g_loss: 1.07073426\n",
      "Step: [4176] d_loss: 0.43167198, g_loss: 1.07616782\n",
      "Step: [4177] d_loss: 0.43819651, g_loss: 1.03854632\n",
      "Step: [4178] d_loss: 0.42750061, g_loss: 1.06857836\n",
      "Step: [4179] d_loss: 0.42926016, g_loss: 1.07501996\n",
      "Step: [4180] d_loss: 0.43276796, g_loss: 1.08173561\n",
      "Step: [4181] d_loss: 0.43134600, g_loss: 1.07832980\n",
      "Step: [4182] d_loss: 0.43374103, g_loss: 1.05261159\n",
      "Step: [4183] d_loss: 0.43789637, g_loss: 1.09646440\n",
      "Step: [4184] d_loss: 0.43589315, g_loss: 1.10956168\n",
      "Step: [4185] d_loss: 0.42763755, g_loss: 1.12349582\n",
      "Step: [4186] d_loss: 0.43086302, g_loss: 1.08017886\n",
      "Step: [4187] d_loss: 0.43488914, g_loss: 1.04340470\n",
      "Step: [4188] d_loss: 0.43513477, g_loss: 1.06314552\n",
      "Step: [4189] d_loss: 0.43076745, g_loss: 1.14240026\n",
      "Step: [4190] d_loss: 0.43118298, g_loss: 1.07678485\n",
      "Step: [4191] d_loss: 0.44085884, g_loss: 1.08558321\n",
      "Step: [4192] d_loss: 0.44438776, g_loss: 1.13102603\n",
      "Step: [4193] d_loss: 0.43596765, g_loss: 1.08238351\n",
      "Step: [4194] d_loss: 0.43205374, g_loss: 1.15407240\n",
      "Step: [4195] d_loss: 0.43622807, g_loss: 1.07912195\n",
      "Step: [4196] d_loss: 0.43698001, g_loss: 1.09239519\n",
      "Step: [4197] d_loss: 0.43507835, g_loss: 1.06775749\n",
      "Step: [4198] d_loss: 0.43484390, g_loss: 1.12051618\n",
      "Step: [4199] d_loss: 0.43076366, g_loss: 1.12422192\n",
      "Step: [4200] d_loss: 0.43456039, g_loss: 1.11538672\n",
      "Step: [4201] d_loss: 0.43186092, g_loss: 1.06813133\n",
      "Step: [4202] d_loss: 0.43225849, g_loss: 1.04213953\n",
      "Step: [4203] d_loss: 0.44631824, g_loss: 1.14991307\n",
      "Step: [4204] d_loss: 0.43827695, g_loss: 1.08896065\n",
      "Step: [4205] d_loss: 0.43729690, g_loss: 1.07451129\n",
      "Step: [4206] d_loss: 0.43446726, g_loss: 1.08358622\n",
      "Step: [4207] d_loss: 0.43022096, g_loss: 1.09649289\n",
      "Step: [4208] d_loss: 0.43314385, g_loss: 1.11827505\n",
      "Step: [4209] d_loss: 0.43735573, g_loss: 1.08889067\n",
      "Step: [4210] d_loss: 0.43118632, g_loss: 1.11218870\n",
      "Step: [4211] d_loss: 0.43482804, g_loss: 1.03703225\n",
      "Step: [4212] d_loss: 0.42952514, g_loss: 1.05694914\n",
      "Step: [4213] d_loss: 0.43257970, g_loss: 1.09401774\n",
      "Step: [4214] d_loss: 0.42969075, g_loss: 1.06998610\n",
      "Step: [4215] d_loss: 0.42722878, g_loss: 1.09429204\n",
      "Step: [4216] d_loss: 0.43872911, g_loss: 1.03899705\n",
      "Step: [4217] d_loss: 0.44620591, g_loss: 1.07977378\n",
      "Step: [4218] d_loss: 0.44642004, g_loss: 1.08605647\n",
      "Step: [4219] d_loss: 0.62349117, g_loss: 1.14010930\n",
      "Step: [4220] d_loss: 0.47332522, g_loss: 1.12195492\n",
      "Step: [4221] d_loss: 0.45064533, g_loss: 1.11159325\n",
      "Step: [4222] d_loss: 0.43543151, g_loss: 1.12735832\n",
      "Step: [4223] d_loss: 0.44654661, g_loss: 1.10279715\n",
      "Step: [4224] d_loss: 0.44105741, g_loss: 1.08054292\n",
      "Step: [4225] d_loss: 0.43604982, g_loss: 1.12098968\n",
      "Step: [4226] d_loss: 0.43675691, g_loss: 1.07415795\n",
      "Step: [4227] d_loss: 0.43212903, g_loss: 1.08490336\n",
      "Step: [4228] d_loss: 0.43405849, g_loss: 1.06078577\n",
      "Step: [4229] d_loss: 0.43166578, g_loss: 1.10563922\n",
      "Step: [4230] d_loss: 0.43645412, g_loss: 1.09478283\n",
      "Step: [4231] d_loss: 0.43402657, g_loss: 1.08914757\n",
      "Step: [4232] d_loss: 0.44139341, g_loss: 1.11202300\n",
      "Step: [4233] d_loss: 0.43990195, g_loss: 1.10206831\n",
      "Step: [4234] d_loss: 0.43449578, g_loss: 1.13022745\n",
      "Step: [4235] d_loss: 0.44029143, g_loss: 1.14222753\n",
      "Step: [4236] d_loss: 0.43730798, g_loss: 1.12265575\n",
      "Step: [4237] d_loss: 0.43280336, g_loss: 1.08834350\n",
      "Step: [4238] d_loss: 0.44088566, g_loss: 1.12300706\n",
      "Step: [4239] d_loss: 0.43222314, g_loss: 1.12314820\n",
      "Step: [4240] d_loss: 0.43727013, g_loss: 1.07597387\n",
      "Step: [4241] d_loss: 0.44225556, g_loss: 1.06514728\n",
      "Step: [4242] d_loss: 0.43740422, g_loss: 1.11076891\n",
      "Step: [4243] d_loss: 0.43568468, g_loss: 1.05222344\n",
      "Step: [4244] d_loss: 0.43100321, g_loss: 1.09221458\n",
      "Step: [4245] d_loss: 0.43529436, g_loss: 1.08352911\n",
      "Step: [4246] d_loss: 0.43822622, g_loss: 1.12319255\n",
      "Step: [4247] d_loss: 0.43438864, g_loss: 1.10910702\n",
      "Step: [4248] d_loss: 0.43415087, g_loss: 1.09850836\n",
      "Step: [4249] d_loss: 0.43153754, g_loss: 1.07066846\n",
      "Step: [4250] d_loss: 0.43329710, g_loss: 1.10292292\n",
      "Step: [4251] d_loss: 0.43376029, g_loss: 1.07268870\n",
      "Step: [4252] d_loss: 0.43566087, g_loss: 1.08914924\n",
      "Step: [4253] d_loss: 0.43580490, g_loss: 1.08842599\n",
      "Step: [4254] d_loss: 0.43026614, g_loss: 1.05133641\n",
      "Step: [4255] d_loss: 0.43147433, g_loss: 1.09927917\n",
      "Step: [4256] d_loss: 0.42988327, g_loss: 1.07529259\n",
      "Step: [4257] d_loss: 0.43406713, g_loss: 1.06933713\n",
      "Step: [4258] d_loss: 0.43151283, g_loss: 1.09599209\n",
      "Step: [4259] d_loss: 0.43549064, g_loss: 1.12646890\n",
      "Step: [4260] d_loss: 0.44924578, g_loss: 1.05684376\n",
      "Step: [4261] d_loss: 0.43141809, g_loss: 1.08885932\n",
      "Step: [4262] d_loss: 0.42975509, g_loss: 1.03044939\n",
      "Step: [4263] d_loss: 0.43350095, g_loss: 1.11367750\n",
      "Step: [4264] d_loss: 0.43442869, g_loss: 1.03310382\n",
      "Step: [4265] d_loss: 0.43224311, g_loss: 1.12259066\n",
      "Step: [4266] d_loss: 0.43409926, g_loss: 1.08819544\n",
      "Step: [4267] d_loss: 0.43467844, g_loss: 1.11586738\n",
      "Step: [4268] d_loss: 0.44408801, g_loss: 1.06262445\n",
      "Step: [4269] d_loss: 0.43290743, g_loss: 1.09643471\n",
      "Step: [4270] d_loss: 0.43321845, g_loss: 1.08058941\n",
      "Step: [4271] d_loss: 0.43801561, g_loss: 1.10027516\n",
      "Step: [4272] d_loss: 0.44100541, g_loss: 1.09400654\n",
      "Step: [4273] d_loss: 0.47394371, g_loss: 1.07100010\n",
      "Step: [4274] d_loss: 0.44244587, g_loss: 1.10810733\n",
      "Step: [4275] d_loss: 0.44038412, g_loss: 1.10873485\n",
      "Step: [4276] d_loss: 0.44148332, g_loss: 1.07836151\n",
      "Step: [4277] d_loss: 0.43320206, g_loss: 1.04357302\n",
      "Step: [4278] d_loss: 0.43318388, g_loss: 1.10567570\n",
      "Step: [4279] d_loss: 0.43223333, g_loss: 1.09458363\n",
      "Step: [4280] d_loss: 0.43089989, g_loss: 1.06085861\n",
      "Step: [4281] d_loss: 0.43614835, g_loss: 1.07831562\n",
      "Step: [4282] d_loss: 0.43225166, g_loss: 1.11656249\n",
      "Step: [4283] d_loss: 0.43185908, g_loss: 1.06792486\n",
      "Step: [4284] d_loss: 0.43243355, g_loss: 1.09402847\n",
      "Step: [4285] d_loss: 0.43072274, g_loss: 1.11117268\n",
      "Step: [4286] d_loss: 0.43009633, g_loss: 1.11423504\n",
      "Step: [4287] d_loss: 0.42820477, g_loss: 1.07696688\n",
      "Step: [4288] d_loss: 0.43606925, g_loss: 1.05486798\n",
      "Step: [4289] d_loss: 0.42757607, g_loss: 1.07729650\n",
      "Step: [4290] d_loss: 0.43001285, g_loss: 1.07106507\n",
      "Step: [4291] d_loss: 0.43401670, g_loss: 1.07181048\n",
      "Step: [4292] d_loss: 0.43745622, g_loss: 1.09106469\n",
      "Step: [4293] d_loss: 0.43089873, g_loss: 1.10611367\n",
      "Step: [4294] d_loss: 0.43709117, g_loss: 1.13275230\n",
      "Step: [4295] d_loss: 0.44271114, g_loss: 1.07882071\n",
      "Step: [4296] d_loss: 0.44026130, g_loss: 1.05297518\n",
      "Step: [4297] d_loss: 0.43220568, g_loss: 1.09284472\n",
      "Step: [4298] d_loss: 0.43745312, g_loss: 1.11257052\n",
      "Step: [4299] d_loss: 0.43679485, g_loss: 1.07319188\n",
      "Step: [4300] d_loss: 0.43513897, g_loss: 1.05731285\n",
      "Step: [4301] d_loss: 0.42615235, g_loss: 1.07204878\n",
      "Step: [4302] d_loss: 0.43533209, g_loss: 1.08616245\n",
      "Step: [4303] d_loss: 0.42789260, g_loss: 1.11699665\n",
      "Step: [4304] d_loss: 0.42976218, g_loss: 1.14298964\n",
      "Step: [4305] d_loss: 0.43842849, g_loss: 1.09268999\n",
      "Step: [4306] d_loss: 0.43447039, g_loss: 1.08383775\n",
      "Step: [4307] d_loss: 0.43473151, g_loss: 1.13960302\n",
      "Step: [4308] d_loss: 0.43336818, g_loss: 1.06892812\n",
      "Step: [4309] d_loss: 0.43269360, g_loss: 1.11063612\n",
      "Step: [4310] d_loss: 0.45329788, g_loss: 1.13520241\n",
      "Step: [4311] d_loss: 0.43159142, g_loss: 1.06820011\n",
      "Step: [4312] d_loss: 0.43472824, g_loss: 1.11886215\n",
      "Step: [4313] d_loss: 0.42815951, g_loss: 1.10804844\n",
      "Step: [4314] d_loss: 0.43085590, g_loss: 1.11094189\n",
      "Step: [4315] d_loss: 0.43536800, g_loss: 1.13683033\n",
      "Step: [4316] d_loss: 0.42939448, g_loss: 1.10856557\n",
      "Step: [4317] d_loss: 0.42861173, g_loss: 1.13087261\n",
      "Step: [4318] d_loss: 0.43071425, g_loss: 1.09942830\n",
      "Step: [4319] d_loss: 0.42889935, g_loss: 1.14037895\n",
      "Step: [4320] d_loss: 0.43126678, g_loss: 1.13280046\n",
      "Step: [4321] d_loss: 0.43064713, g_loss: 1.10657036\n",
      "Step: [4322] d_loss: 0.43887815, g_loss: 1.09320569\n",
      "Step: [4323] d_loss: 0.43118024, g_loss: 1.11267602\n",
      "Step: [4324] d_loss: 0.43318427, g_loss: 1.10021031\n",
      "Step: [4325] d_loss: 0.44109356, g_loss: 1.07347262\n",
      "Step: [4326] d_loss: 0.43268645, g_loss: 1.12623560\n",
      "Step: [4327] d_loss: 0.42962798, g_loss: 1.14100802\n",
      "Step: [4328] d_loss: 0.42999676, g_loss: 1.10481834\n",
      "Step: [4329] d_loss: 0.42845958, g_loss: 1.12878621\n",
      "Step: [4330] d_loss: 0.42614836, g_loss: 1.08567524\n",
      "Step: [4331] d_loss: 0.43192792, g_loss: 1.11319947\n",
      "Step: [4332] d_loss: 0.42865428, g_loss: 1.06829882\n",
      "Step: [4333] d_loss: 0.42824164, g_loss: 1.09891272\n",
      "Step: [4334] d_loss: 0.43462846, g_loss: 1.08108592\n",
      "Step: [4335] d_loss: 0.43133825, g_loss: 1.08060133\n",
      "Step: [4336] d_loss: 0.43378055, g_loss: 1.08643878\n",
      "Step: [4337] d_loss: 0.43474159, g_loss: 1.10634935\n",
      "Step: [4338] d_loss: 0.43077186, g_loss: 1.05653644\n",
      "Step: [4339] d_loss: 0.43415305, g_loss: 1.11953008\n",
      "Step: [4340] d_loss: 0.43343470, g_loss: 1.08797824\n",
      "Step: [4341] d_loss: 0.42929545, g_loss: 1.11029041\n",
      "Step: [4342] d_loss: 0.42943558, g_loss: 1.06993580\n",
      "Step: [4343] d_loss: 0.42792696, g_loss: 1.12004316\n",
      "Step: [4344] d_loss: 0.42553538, g_loss: 1.06260049\n",
      "Step: [4345] d_loss: 0.42765838, g_loss: 1.10257185\n",
      "Step: [4346] d_loss: 0.43118918, g_loss: 1.08824813\n",
      "Step: [4347] d_loss: 0.42382756, g_loss: 1.10068107\n",
      "Step: [4348] d_loss: 0.42689049, g_loss: 1.10555089\n",
      "Step: [4349] d_loss: 0.42577922, g_loss: 1.12032616\n",
      "Step: [4350] d_loss: 0.43064031, g_loss: 1.07435143\n",
      "Step: [4351] d_loss: 0.43100175, g_loss: 1.04522645\n",
      "Step: [4352] d_loss: 0.42804897, g_loss: 1.06490731\n",
      "Step: [4353] d_loss: 0.42519078, g_loss: 1.06328797\n",
      "Step: [4354] d_loss: 0.42642325, g_loss: 1.11753201\n",
      "Step: [4355] d_loss: 0.42599279, g_loss: 1.08917308\n",
      "Step: [4356] d_loss: 0.42452449, g_loss: 1.08513689\n",
      "Step: [4357] d_loss: 0.43029302, g_loss: 1.05172014\n",
      "Step: [4358] d_loss: 0.43843088, g_loss: 1.10452092\n",
      "Step: [4359] d_loss: 0.43137574, g_loss: 1.09008014\n",
      "Step: [4360] d_loss: 0.43358785, g_loss: 1.10513330\n",
      "Step: [4361] d_loss: 0.42571110, g_loss: 1.05311882\n",
      "Step: [4362] d_loss: 0.42528385, g_loss: 1.09960020\n",
      "Step: [4363] d_loss: 0.42546296, g_loss: 1.10435545\n",
      "Step: [4364] d_loss: 0.42437991, g_loss: 1.11833251\n",
      "Step: [4365] d_loss: 0.42891362, g_loss: 1.08226025\n",
      "Step: [4366] d_loss: 0.43022266, g_loss: 1.07523739\n",
      "Step: [4367] d_loss: 0.42549717, g_loss: 1.07813585\n",
      "Step: [4368] d_loss: 0.42908683, g_loss: 1.10829937\n",
      "Step: [4369] d_loss: 0.43380085, g_loss: 1.05745721\n",
      "Step: [4370] d_loss: 0.43786621, g_loss: 1.07306564\n",
      "Step: [4371] d_loss: 0.43272251, g_loss: 1.11078191\n",
      "Step: [4372] d_loss: 0.43123138, g_loss: 1.12359035\n",
      "Step: [4373] d_loss: 0.42840135, g_loss: 1.11923993\n",
      "Step: [4374] d_loss: 0.43501008, g_loss: 1.05254710\n",
      "Step: [4375] d_loss: 0.42915839, g_loss: 1.08019805\n",
      "Step: [4376] d_loss: 0.43116847, g_loss: 1.08144951\n",
      "Step: [4377] d_loss: 0.43194890, g_loss: 1.09085023\n",
      "Step: [4378] d_loss: 0.43243963, g_loss: 1.09069097\n",
      "Step: [4379] d_loss: 0.42868039, g_loss: 1.10356677\n",
      "Step: [4380] d_loss: 0.42527491, g_loss: 1.07139897\n",
      "Step: [4381] d_loss: 0.42892292, g_loss: 1.05371952\n",
      "Step: [4382] d_loss: 0.42530900, g_loss: 1.05627143\n",
      "Step: [4383] d_loss: 0.43074986, g_loss: 1.09971249\n",
      "Step: [4384] d_loss: 0.43554306, g_loss: 1.11635220\n",
      "Step: [4385] d_loss: 0.42453426, g_loss: 1.08521295\n",
      "Step: [4386] d_loss: 0.42402938, g_loss: 1.09514952\n",
      "Step: [4387] d_loss: 0.42776203, g_loss: 1.08480680\n",
      "Step: [4388] d_loss: 0.42896506, g_loss: 1.09917355\n",
      "Step: [4389] d_loss: 0.42544097, g_loss: 1.07566667\n",
      "Step: [4390] d_loss: 0.43177703, g_loss: 1.11231923\n",
      "Step: [4391] d_loss: 0.42869842, g_loss: 1.07192409\n",
      "Step: [4392] d_loss: 0.42884746, g_loss: 1.09978867\n",
      "Step: [4393] d_loss: 0.43156207, g_loss: 1.13573861\n",
      "Step: [4394] d_loss: 0.43741652, g_loss: 1.09642363\n",
      "Step: [4395] d_loss: 0.42952025, g_loss: 1.08641267\n",
      "Step: [4396] d_loss: 0.43264008, g_loss: 1.07703531\n",
      "Step: [4397] d_loss: 0.43410149, g_loss: 1.10777140\n",
      "Step: [4398] d_loss: 0.43399057, g_loss: 1.11519408\n",
      "Step: [4399] d_loss: 0.42577532, g_loss: 1.09158897\n",
      "Step: [4400] d_loss: 0.43400732, g_loss: 1.08396375\n",
      "Step: [4401] d_loss: 0.42757556, g_loss: 1.08728647\n",
      "Step: [4402] d_loss: 0.43055874, g_loss: 1.12394845\n",
      "Step: [4403] d_loss: 0.42469883, g_loss: 1.07717681\n",
      "Step: [4404] d_loss: 0.42738938, g_loss: 1.11825418\n",
      "Step: [4405] d_loss: 0.42671832, g_loss: 1.06006575\n",
      "Step: [4406] d_loss: 0.42992589, g_loss: 1.07818413\n",
      "Step: [4407] d_loss: 0.43075305, g_loss: 1.10207498\n",
      "Step: [4408] d_loss: 0.42799968, g_loss: 1.07051885\n",
      "Step: [4409] d_loss: 0.42319536, g_loss: 1.08316267\n",
      "Step: [4410] d_loss: 0.42495108, g_loss: 1.05421352\n",
      "Step: [4411] d_loss: 0.42545396, g_loss: 1.07730711\n",
      "Step: [4412] d_loss: 0.42682958, g_loss: 1.06135142\n",
      "Step: [4413] d_loss: 0.42308253, g_loss: 1.10358417\n",
      "Step: [4414] d_loss: 0.42461658, g_loss: 1.09525073\n",
      "Step: [4415] d_loss: 0.42801121, g_loss: 1.09929466\n",
      "Step: [4416] d_loss: 0.43027529, g_loss: 1.15408194\n",
      "Step: [4417] d_loss: 0.42400235, g_loss: 1.11183369\n",
      "Step: [4418] d_loss: 0.42491049, g_loss: 1.07409716\n",
      "Step: [4419] d_loss: 0.42472738, g_loss: 1.10090899\n",
      "Step: [4420] d_loss: 0.42538774, g_loss: 1.06890953\n",
      "Step: [4421] d_loss: 0.42744350, g_loss: 1.05175829\n",
      "Step: [4422] d_loss: 0.42682898, g_loss: 1.10622156\n",
      "Step: [4423] d_loss: 0.42961097, g_loss: 1.11073387\n",
      "Step: [4424] d_loss: 0.43214855, g_loss: 1.07332957\n",
      "Step: [4425] d_loss: 0.42685997, g_loss: 1.09138644\n",
      "Step: [4426] d_loss: 0.42870608, g_loss: 1.06951547\n",
      "Step: [4427] d_loss: 0.43202215, g_loss: 1.08749449\n",
      "Step: [4428] d_loss: 0.42380998, g_loss: 1.09045768\n",
      "Step: [4429] d_loss: 0.43361011, g_loss: 1.04963875\n",
      "Step: [4430] d_loss: 0.42434201, g_loss: 1.06833851\n",
      "Step: [4431] d_loss: 0.43410566, g_loss: 1.10522962\n",
      "Step: [4432] d_loss: 0.42919433, g_loss: 1.12124407\n",
      "Step: [4433] d_loss: 0.43876204, g_loss: 1.11348081\n",
      "Step: [4434] d_loss: 0.43241271, g_loss: 1.12085855\n",
      "Step: [4435] d_loss: 0.42910069, g_loss: 1.08971310\n",
      "Step: [4436] d_loss: 0.42819232, g_loss: 1.08947599\n",
      "Step: [4437] d_loss: 0.43131235, g_loss: 1.09922123\n",
      "Step: [4438] d_loss: 0.43036643, g_loss: 1.05776775\n",
      "Step: [4439] d_loss: 0.42912406, g_loss: 1.06926608\n",
      "Step: [4440] d_loss: 0.42991632, g_loss: 1.06161547\n",
      "Step: [4441] d_loss: 0.42745104, g_loss: 1.08034718\n",
      "Step: [4442] d_loss: 0.42607942, g_loss: 1.13812280\n",
      "Step: [4443] d_loss: 0.42731887, g_loss: 1.07201922\n",
      "Step: [4444] d_loss: 0.42503813, g_loss: 1.12380803\n",
      "Step: [4445] d_loss: 0.42850235, g_loss: 1.04211116\n",
      "Step: [4446] d_loss: 0.42650902, g_loss: 1.04646611\n",
      "Step: [4447] d_loss: 0.42832640, g_loss: 1.03692424\n",
      "Step: [4448] d_loss: 0.42739660, g_loss: 1.14256310\n",
      "Step: [4449] d_loss: 0.42880005, g_loss: 1.06346333\n",
      "Step: [4450] d_loss: 0.42836058, g_loss: 1.08399701\n",
      "Step: [4451] d_loss: 0.42947209, g_loss: 1.08029628\n",
      "Step: [4452] d_loss: 0.43482715, g_loss: 1.10206628\n",
      "Step: [4453] d_loss: 0.42494032, g_loss: 1.08350837\n",
      "Step: [4454] d_loss: 0.42993268, g_loss: 1.07945538\n",
      "Step: [4455] d_loss: 0.42410553, g_loss: 1.08498085\n",
      "Step: [4456] d_loss: 0.42793542, g_loss: 1.10235357\n",
      "Step: [4457] d_loss: 0.42635834, g_loss: 1.04835129\n",
      "Step: [4458] d_loss: 0.42594400, g_loss: 1.10337532\n",
      "Step: [4459] d_loss: 0.42598820, g_loss: 1.06711531\n",
      "Step: [4460] d_loss: 0.42851457, g_loss: 1.12317622\n",
      "Step: [4461] d_loss: 0.42574513, g_loss: 1.06596982\n",
      "Step: [4462] d_loss: 0.43200356, g_loss: 1.10545754\n",
      "Step: [4463] d_loss: 0.42600816, g_loss: 1.04823780\n",
      "Step: [4464] d_loss: 0.42947739, g_loss: 1.07785082\n",
      "Step: [4465] d_loss: 0.42605984, g_loss: 1.04369009\n",
      "Step: [4466] d_loss: 0.42629054, g_loss: 1.09908986\n",
      "Step: [4467] d_loss: 0.42586893, g_loss: 1.07708180\n",
      "Step: [4468] d_loss: 0.42522174, g_loss: 1.08545792\n",
      "Step: [4469] d_loss: 0.45262045, g_loss: 1.09593642\n",
      "Step: [4470] d_loss: 0.43421325, g_loss: 1.08299518\n",
      "Step: [4471] d_loss: 0.42969134, g_loss: 1.09107399\n",
      "Step: [4472] d_loss: 0.42808440, g_loss: 1.07395506\n",
      "Step: [4473] d_loss: 0.43285888, g_loss: 1.05775440\n",
      "Step: [4474] d_loss: 0.42786440, g_loss: 1.11908448\n",
      "Step: [4475] d_loss: 0.43124089, g_loss: 1.06727111\n",
      "Step: [4476] d_loss: 0.42714190, g_loss: 1.04588151\n",
      "Step: [4477] d_loss: 0.42615333, g_loss: 1.07143819\n",
      "Step: [4478] d_loss: 0.42485934, g_loss: 1.06792057\n",
      "Step: [4479] d_loss: 0.43398595, g_loss: 1.11104167\n",
      "Step: [4480] d_loss: 0.42938802, g_loss: 1.08770299\n",
      "Step: [4481] d_loss: 0.43107080, g_loss: 1.03171265\n",
      "Step: [4482] d_loss: 0.43005624, g_loss: 1.07736313\n",
      "Step: [4483] d_loss: 0.42710024, g_loss: 1.09717679\n",
      "Step: [4484] d_loss: 0.42563328, g_loss: 1.11855674\n",
      "Step: [4485] d_loss: 0.42370558, g_loss: 1.08099127\n",
      "Step: [4486] d_loss: 0.42549801, g_loss: 1.06625772\n",
      "Step: [4487] d_loss: 0.43250439, g_loss: 1.09167469\n",
      "Step: [4488] d_loss: 0.42810565, g_loss: 1.01902032\n",
      "Step: [4489] d_loss: 0.42599565, g_loss: 1.08268535\n",
      "Step: [4490] d_loss: 0.42585459, g_loss: 1.06776965\n",
      "Step: [4491] d_loss: 0.42757148, g_loss: 1.06731021\n",
      "Step: [4492] d_loss: 0.42841297, g_loss: 1.15412664\n",
      "Step: [4493] d_loss: 0.43059400, g_loss: 1.04045999\n",
      "Step: [4494] d_loss: 0.42795119, g_loss: 1.11585784\n",
      "Step: [4495] d_loss: 0.42906514, g_loss: 1.05461490\n",
      "Step: [4496] d_loss: 0.43079603, g_loss: 1.09375417\n",
      "Step: [4497] d_loss: 0.42868796, g_loss: 1.08494413\n",
      "Step: [4498] d_loss: 0.43062666, g_loss: 1.08773899\n",
      "Step: [4499] d_loss: 0.43188521, g_loss: 1.09807706\n",
      "Step: [4500] d_loss: 0.42802137, g_loss: 1.10604954\n",
      "Step: [4501] d_loss: 0.42793804, g_loss: 1.08283210\n",
      "Step: [4502] d_loss: 0.42758596, g_loss: 1.09875548\n",
      "Step: [4503] d_loss: 0.43136907, g_loss: 1.12048018\n",
      "Step: [4504] d_loss: 0.42857268, g_loss: 1.11334348\n",
      "Step: [4505] d_loss: 0.42711815, g_loss: 1.07662249\n",
      "Step: [4506] d_loss: 0.42556721, g_loss: 1.09023523\n",
      "Step: [4507] d_loss: 0.42443547, g_loss: 1.07594585\n",
      "Step: [4508] d_loss: 0.42971700, g_loss: 1.09264505\n",
      "Step: [4509] d_loss: 0.42464820, g_loss: 1.10083747\n",
      "Step: [4510] d_loss: 0.42511117, g_loss: 1.10787427\n",
      "Step: [4511] d_loss: 0.42640036, g_loss: 1.13312352\n",
      "Step: [4512] d_loss: 0.42340985, g_loss: 1.09315050\n",
      "Step: [4513] d_loss: 0.42430034, g_loss: 1.06909156\n",
      "Step: [4514] d_loss: 0.42677656, g_loss: 1.09685850\n",
      "Step: [4515] d_loss: 0.42574406, g_loss: 1.08694541\n",
      "Step: [4516] d_loss: 0.42672500, g_loss: 1.07030571\n",
      "Step: [4517] d_loss: 0.42652610, g_loss: 1.09455347\n",
      "Step: [4518] d_loss: 0.42462590, g_loss: 1.07807720\n",
      "Step: [4519] d_loss: 0.43461594, g_loss: 1.14655566\n",
      "Step: [4520] d_loss: 0.42990190, g_loss: 1.06549573\n",
      "Step: [4521] d_loss: 0.42974755, g_loss: 1.07253790\n",
      "Step: [4522] d_loss: 0.42637298, g_loss: 1.08513761\n",
      "Step: [4523] d_loss: 0.42660347, g_loss: 1.09126318\n",
      "Step: [4524] d_loss: 0.42904609, g_loss: 1.09402847\n",
      "Step: [4525] d_loss: 0.42738724, g_loss: 1.09598756\n",
      "Step: [4526] d_loss: 0.42267895, g_loss: 1.10083079\n",
      "Step: [4527] d_loss: 0.42738006, g_loss: 1.12256777\n",
      "Step: [4528] d_loss: 0.42411393, g_loss: 1.07585311\n",
      "Step: [4529] d_loss: 0.43065143, g_loss: 1.09743750\n",
      "Step: [4530] d_loss: 0.42656040, g_loss: 1.12064242\n",
      "Step: [4531] d_loss: 0.42454615, g_loss: 1.08972681\n",
      "Step: [4532] d_loss: 0.42711571, g_loss: 1.10606766\n",
      "Step: [4533] d_loss: 0.42623764, g_loss: 1.07511771\n",
      "Step: [4534] d_loss: 0.42630845, g_loss: 1.07696390\n",
      "Step: [4535] d_loss: 0.42643800, g_loss: 1.08327401\n",
      "Step: [4536] d_loss: 0.42255497, g_loss: 1.06331992\n",
      "Step: [4537] d_loss: 0.42560154, g_loss: 1.10293746\n",
      "Step: [4538] d_loss: 0.42420125, g_loss: 1.05573416\n",
      "Step: [4539] d_loss: 0.42481339, g_loss: 1.07180333\n",
      "Step: [4540] d_loss: 0.42554909, g_loss: 1.09766817\n",
      "Step: [4541] d_loss: 0.42836437, g_loss: 1.06379390\n",
      "Step: [4542] d_loss: 0.42683896, g_loss: 1.08368480\n",
      "Step: [4543] d_loss: 0.42608231, g_loss: 1.07638276\n",
      "Step: [4544] d_loss: 0.42701298, g_loss: 1.09922683\n",
      "Step: [4545] d_loss: 0.42748815, g_loss: 1.05593860\n",
      "Step: [4546] d_loss: 0.42618585, g_loss: 1.07638144\n",
      "Step: [4547] d_loss: 0.42502120, g_loss: 1.09156990\n",
      "Step: [4548] d_loss: 0.42679435, g_loss: 1.09582531\n",
      "Step: [4549] d_loss: 0.43954292, g_loss: 1.01175952\n",
      "Step: [4550] d_loss: 0.43052050, g_loss: 1.12794399\n",
      "Step: [4551] d_loss: 0.42870313, g_loss: 1.08874118\n",
      "Step: [4552] d_loss: 0.42679986, g_loss: 1.06837153\n",
      "Step: [4553] d_loss: 0.43108171, g_loss: 1.02278996\n",
      "Step: [4554] d_loss: 0.42962882, g_loss: 1.11725116\n",
      "Step: [4555] d_loss: 0.42468601, g_loss: 1.10256720\n",
      "Step: [4556] d_loss: 0.42784041, g_loss: 1.08277833\n",
      "Step: [4557] d_loss: 0.42511368, g_loss: 1.06958425\n",
      "Step: [4558] d_loss: 0.42664102, g_loss: 1.03683877\n",
      "Step: [4559] d_loss: 0.42640144, g_loss: 1.09808445\n",
      "Step: [4560] d_loss: 0.43182403, g_loss: 1.05388200\n",
      "Step: [4561] d_loss: 0.42575356, g_loss: 1.03560281\n",
      "Step: [4562] d_loss: 0.42784324, g_loss: 1.11742008\n",
      "Step: [4563] d_loss: 0.42479852, g_loss: 1.04447782\n",
      "Step: [4564] d_loss: 0.42648649, g_loss: 1.06765103\n",
      "Step: [4565] d_loss: 0.42591041, g_loss: 1.12247956\n",
      "Step: [4566] d_loss: 0.42501122, g_loss: 1.05636680\n",
      "Step: [4567] d_loss: 0.43036592, g_loss: 1.08856702\n",
      "Step: [4568] d_loss: 0.42763922, g_loss: 1.04929864\n",
      "Step: [4569] d_loss: 0.42657435, g_loss: 1.09182537\n",
      "Step: [4570] d_loss: 0.43101242, g_loss: 1.05736589\n",
      "Step: [4571] d_loss: 0.42944366, g_loss: 1.02602768\n",
      "Step: [4572] d_loss: 0.42809945, g_loss: 1.10324490\n",
      "Step: [4573] d_loss: 0.42864215, g_loss: 1.10634601\n",
      "Step: [4574] d_loss: 0.42724311, g_loss: 1.08202374\n",
      "Step: [4575] d_loss: 0.42710197, g_loss: 1.06805491\n",
      "Step: [4576] d_loss: 0.42845154, g_loss: 1.08723867\n",
      "Step: [4577] d_loss: 0.42597419, g_loss: 1.08494806\n",
      "Step: [4578] d_loss: 0.42682868, g_loss: 1.05135691\n",
      "Step: [4579] d_loss: 0.43171099, g_loss: 1.01963282\n",
      "Step: [4580] d_loss: 0.43131319, g_loss: 1.10195935\n",
      "Step: [4581] d_loss: 0.42632145, g_loss: 1.06732786\n",
      "Step: [4582] d_loss: 0.42555976, g_loss: 1.07080662\n",
      "Step: [4583] d_loss: 0.42366230, g_loss: 1.08026433\n",
      "Step: [4584] d_loss: 0.42563251, g_loss: 1.05315149\n",
      "Step: [4585] d_loss: 0.42574075, g_loss: 1.10760558\n",
      "Step: [4586] d_loss: 0.42500561, g_loss: 1.05121493\n",
      "Step: [4587] d_loss: 0.42507324, g_loss: 1.09513891\n",
      "Step: [4588] d_loss: 0.42292774, g_loss: 1.07165718\n",
      "Step: [4589] d_loss: 0.42257464, g_loss: 1.10015202\n",
      "Step: [4590] d_loss: 0.42304716, g_loss: 1.11330914\n",
      "Step: [4591] d_loss: 0.42485142, g_loss: 1.09423089\n",
      "Step: [4592] d_loss: 0.42663568, g_loss: 1.04788160\n",
      "Step: [4593] d_loss: 0.42257661, g_loss: 1.05518830\n",
      "Step: [4594] d_loss: 0.42386615, g_loss: 1.03998923\n",
      "Step: [4595] d_loss: 0.42242569, g_loss: 1.05207074\n",
      "Step: [4596] d_loss: 0.42828792, g_loss: 1.04791546\n",
      "Step: [4597] d_loss: 0.43305302, g_loss: 1.06507778\n",
      "Step: [4598] d_loss: 0.42928761, g_loss: 1.06011486\n",
      "Step: [4599] d_loss: 0.42541158, g_loss: 1.07562172\n",
      "Step: [4600] d_loss: 0.42602745, g_loss: 1.08580220\n",
      "Step: [4601] d_loss: 0.42555261, g_loss: 1.07466173\n",
      "Step: [4602] d_loss: 0.42585325, g_loss: 1.08175099\n",
      "Step: [4603] d_loss: 0.42581421, g_loss: 1.06056821\n",
      "Step: [4604] d_loss: 0.43286327, g_loss: 1.05467105\n",
      "Step: [4605] d_loss: 0.42488900, g_loss: 1.05580914\n",
      "Step: [4606] d_loss: 0.43528497, g_loss: 1.06465209\n",
      "Step: [4607] d_loss: 0.42684788, g_loss: 1.01507163\n",
      "Step: [4608] d_loss: 0.42652529, g_loss: 1.01317298\n",
      "Step: [4609] d_loss: 0.42704943, g_loss: 1.03654706\n",
      "Step: [4610] d_loss: 0.42582977, g_loss: 1.06904995\n",
      "Step: [4611] d_loss: 0.42524824, g_loss: 1.02977908\n",
      "Step: [4612] d_loss: 0.42336822, g_loss: 1.04675174\n",
      "Step: [4613] d_loss: 0.42318079, g_loss: 1.06229353\n",
      "Step: [4614] d_loss: 0.42663056, g_loss: 1.04397893\n",
      "Step: [4615] d_loss: 0.42136291, g_loss: 1.05903924\n",
      "Step: [4616] d_loss: 0.42545873, g_loss: 1.05398965\n",
      "Step: [4617] d_loss: 0.42574814, g_loss: 1.04203856\n",
      "Step: [4618] d_loss: 0.42855909, g_loss: 1.03365386\n",
      "Step: [4619] d_loss: 0.43037266, g_loss: 1.07409883\n",
      "Step: [4620] d_loss: 0.42080897, g_loss: 1.06190646\n",
      "Step: [4621] d_loss: 0.42284256, g_loss: 1.06798518\n",
      "Step: [4622] d_loss: 0.42547289, g_loss: 1.04372919\n",
      "Step: [4623] d_loss: 0.42456409, g_loss: 1.05160487\n",
      "Step: [4624] d_loss: 0.43188307, g_loss: 1.03200710\n",
      "Step: [4625] d_loss: 0.43048230, g_loss: 1.04324460\n",
      "Step: [4626] d_loss: 0.42625239, g_loss: 1.06370771\n",
      "Step: [4627] d_loss: 0.42567790, g_loss: 1.10269475\n",
      "Step: [4628] d_loss: 0.42652944, g_loss: 1.09393752\n",
      "Step: [4629] d_loss: 0.42478687, g_loss: 1.06197691\n",
      "Step: [4630] d_loss: 0.42611933, g_loss: 1.07662356\n",
      "Step: [4631] d_loss: 0.42583084, g_loss: 1.08758116\n",
      "Step: [4632] d_loss: 0.42175075, g_loss: 1.07853365\n",
      "Step: [4633] d_loss: 0.42268935, g_loss: 1.09886742\n",
      "Step: [4634] d_loss: 0.42317098, g_loss: 1.02645934\n",
      "Step: [4635] d_loss: 0.43277729, g_loss: 1.09409726\n",
      "Step: [4636] d_loss: 0.42261815, g_loss: 1.07184207\n",
      "Step: [4637] d_loss: 0.42002508, g_loss: 1.07298648\n",
      "Step: [4638] d_loss: 0.42593673, g_loss: 1.03736746\n",
      "Step: [4639] d_loss: 0.42380396, g_loss: 1.07405365\n",
      "Step: [4640] d_loss: 0.42723516, g_loss: 1.09251618\n",
      "Step: [4641] d_loss: 0.42624435, g_loss: 1.09215546\n",
      "Step: [4642] d_loss: 0.42482120, g_loss: 1.04113913\n",
      "Step: [4643] d_loss: 0.42608318, g_loss: 1.06349039\n",
      "Step: [4644] d_loss: 0.42194444, g_loss: 1.06875503\n",
      "Step: [4645] d_loss: 0.42665622, g_loss: 1.09354162\n",
      "Step: [4646] d_loss: 0.42408133, g_loss: 1.07653582\n",
      "Step: [4647] d_loss: 0.42483819, g_loss: 1.09088123\n",
      "Step: [4648] d_loss: 0.43193012, g_loss: 1.06760228\n",
      "Step: [4649] d_loss: 0.42700005, g_loss: 1.05274057\n",
      "Step: [4650] d_loss: 0.42318973, g_loss: 1.07229996\n",
      "Step: [4651] d_loss: 0.42576301, g_loss: 1.08739877\n",
      "Step: [4652] d_loss: 0.42377552, g_loss: 1.09413552\n",
      "Step: [4653] d_loss: 0.42251813, g_loss: 1.07265210\n",
      "Step: [4654] d_loss: 0.42303544, g_loss: 1.08702958\n",
      "Step: [4655] d_loss: 0.42522970, g_loss: 1.09713078\n",
      "Step: [4656] d_loss: 0.42638123, g_loss: 1.07632303\n",
      "Step: [4657] d_loss: 0.42502344, g_loss: 1.08639657\n",
      "Step: [4658] d_loss: 0.43344417, g_loss: 1.08429074\n",
      "Step: [4659] d_loss: 0.42918712, g_loss: 1.07559907\n",
      "Step: [4660] d_loss: 0.43040398, g_loss: 1.06648767\n",
      "Step: [4661] d_loss: 0.42969796, g_loss: 1.07856894\n",
      "Step: [4662] d_loss: 0.42940632, g_loss: 1.08994865\n",
      "Step: [4663] d_loss: 0.42724362, g_loss: 1.07674599\n",
      "Step: [4664] d_loss: 0.42614108, g_loss: 1.09844971\n",
      "Step: [4665] d_loss: 0.42640349, g_loss: 1.09439778\n",
      "Step: [4666] d_loss: 0.42389619, g_loss: 1.08420551\n",
      "Step: [4667] d_loss: 0.42406830, g_loss: 1.09623706\n",
      "Step: [4668] d_loss: 0.42268407, g_loss: 1.06292832\n",
      "Step: [4669] d_loss: 0.42413932, g_loss: 1.08104026\n",
      "Step: [4670] d_loss: 0.42267606, g_loss: 1.08023489\n",
      "Step: [4671] d_loss: 0.42819959, g_loss: 1.09568274\n",
      "Step: [4672] d_loss: 0.42471758, g_loss: 1.07102609\n",
      "Step: [4673] d_loss: 0.42270476, g_loss: 1.11524236\n",
      "Step: [4674] d_loss: 0.42434859, g_loss: 1.08042336\n",
      "Step: [4675] d_loss: 0.42340469, g_loss: 1.07517600\n",
      "Step: [4676] d_loss: 0.42605019, g_loss: 1.06306708\n",
      "Step: [4677] d_loss: 0.42809045, g_loss: 1.09243679\n",
      "Step: [4678] d_loss: 0.42768803, g_loss: 1.07911682\n",
      "Step: [4679] d_loss: 0.42547736, g_loss: 1.08321440\n",
      "Step: [4680] d_loss: 0.42624834, g_loss: 1.08577192\n",
      "Step: [4681] d_loss: 0.42609578, g_loss: 1.12382317\n",
      "Step: [4682] d_loss: 0.42458573, g_loss: 1.11745465\n",
      "Step: [4683] d_loss: 0.42699775, g_loss: 1.07280397\n",
      "Step: [4684] d_loss: 0.43221703, g_loss: 1.10353279\n",
      "Step: [4685] d_loss: 0.42913932, g_loss: 1.10762358\n",
      "Step: [4686] d_loss: 0.42817628, g_loss: 1.09327769\n",
      "Step: [4687] d_loss: 0.42514697, g_loss: 1.05747294\n",
      "Step: [4688] d_loss: 0.42681873, g_loss: 1.06082129\n",
      "Step: [4689] d_loss: 0.42582619, g_loss: 1.05105138\n",
      "Step: [4690] d_loss: 0.42659885, g_loss: 1.05718100\n",
      "Step: [4691] d_loss: 0.42440820, g_loss: 1.08040929\n",
      "Step: [4692] d_loss: 0.42248121, g_loss: 1.07191825\n",
      "Step: [4693] d_loss: 0.42034632, g_loss: 1.05100036\n",
      "Step: [4694] d_loss: 0.42350873, g_loss: 1.05913210\n",
      "Step: [4695] d_loss: 0.42868221, g_loss: 1.07733071\n",
      "Step: [4696] d_loss: 0.42448929, g_loss: 1.05908096\n",
      "Step: [4697] d_loss: 0.42365837, g_loss: 1.05205405\n",
      "Step: [4698] d_loss: 0.42504621, g_loss: 1.06188488\n",
      "Step: [4699] d_loss: 0.42721558, g_loss: 1.08501065\n",
      "Step: [4700] d_loss: 0.42858136, g_loss: 1.09166443\n",
      "Step: [4701] d_loss: 0.42203727, g_loss: 1.06386483\n",
      "Step: [4702] d_loss: 0.42318189, g_loss: 1.05172813\n",
      "Step: [4703] d_loss: 0.43769816, g_loss: 1.03977835\n",
      "Step: [4704] d_loss: 0.42670822, g_loss: 1.05324662\n",
      "Step: [4705] d_loss: 0.42457917, g_loss: 1.04836488\n",
      "Step: [4706] d_loss: 0.42559755, g_loss: 1.04604709\n",
      "Step: [4707] d_loss: 0.42072546, g_loss: 1.04995251\n",
      "Step: [4708] d_loss: 0.42650148, g_loss: 1.04215610\n",
      "Step: [4709] d_loss: 0.42337990, g_loss: 1.03564239\n",
      "Step: [4710] d_loss: 0.42017230, g_loss: 1.04359818\n",
      "Step: [4711] d_loss: 0.42568111, g_loss: 1.03368342\n",
      "Step: [4712] d_loss: 0.42406529, g_loss: 1.03895271\n",
      "Step: [4713] d_loss: 0.42418319, g_loss: 1.05120730\n",
      "Step: [4714] d_loss: 0.42323369, g_loss: 1.06017506\n",
      "Step: [4715] d_loss: 0.42488116, g_loss: 1.04557109\n",
      "Step: [4716] d_loss: 0.42845461, g_loss: 1.04686451\n",
      "Step: [4717] d_loss: 0.42622277, g_loss: 1.03992879\n",
      "Step: [4718] d_loss: 0.42233959, g_loss: 1.06457829\n",
      "Step: [4719] d_loss: 0.42698827, g_loss: 1.05480862\n",
      "Step: [4720] d_loss: 0.42731625, g_loss: 1.03271139\n",
      "Step: [4721] d_loss: 0.42509109, g_loss: 1.05151451\n",
      "Step: [4722] d_loss: 0.42690766, g_loss: 1.07512546\n",
      "Step: [4723] d_loss: 0.42212862, g_loss: 1.06393945\n",
      "Step: [4724] d_loss: 0.42319882, g_loss: 1.05776954\n",
      "Step: [4725] d_loss: 0.42422497, g_loss: 1.04906106\n",
      "Step: [4726] d_loss: 0.42518362, g_loss: 1.06428301\n",
      "Step: [4727] d_loss: 0.43035764, g_loss: 1.04375315\n",
      "Step: [4728] d_loss: 0.42211735, g_loss: 1.05020952\n",
      "Step: [4729] d_loss: 0.42563468, g_loss: 1.02862966\n",
      "Step: [4730] d_loss: 0.42020592, g_loss: 1.04392493\n",
      "Step: [4731] d_loss: 0.42400464, g_loss: 1.04939628\n",
      "Step: [4732] d_loss: 0.42444104, g_loss: 1.02517867\n",
      "Step: [4733] d_loss: 0.42402849, g_loss: 1.07350516\n",
      "Step: [4734] d_loss: 0.42224300, g_loss: 1.05482256\n",
      "Step: [4735] d_loss: 0.42473394, g_loss: 1.02956939\n",
      "Step: [4736] d_loss: 0.42513338, g_loss: 1.05090284\n",
      "Step: [4737] d_loss: 0.42761481, g_loss: 1.04966402\n",
      "Step: [4738] d_loss: 0.42650890, g_loss: 1.06609011\n",
      "Step: [4739] d_loss: 0.42350432, g_loss: 1.03388441\n",
      "Step: [4740] d_loss: 0.42416036, g_loss: 1.06881702\n",
      "Step: [4741] d_loss: 0.42629579, g_loss: 1.03860998\n",
      "Step: [4742] d_loss: 0.42660540, g_loss: 1.03932393\n",
      "Step: [4743] d_loss: 0.42464125, g_loss: 1.03310394\n",
      "Step: [4744] d_loss: 0.42209113, g_loss: 1.05436587\n",
      "Step: [4745] d_loss: 0.42262727, g_loss: 1.03033805\n",
      "Step: [4746] d_loss: 0.42427155, g_loss: 1.03480947\n",
      "Step: [4747] d_loss: 0.42158991, g_loss: 1.03418970\n",
      "Step: [4748] d_loss: 0.42256954, g_loss: 1.04634154\n",
      "Step: [4749] d_loss: 0.42493871, g_loss: 1.04153049\n",
      "Step: [4750] d_loss: 0.42508835, g_loss: 1.05068457\n",
      "Step: [4751] d_loss: 0.42270923, g_loss: 1.03066325\n",
      "Step: [4752] d_loss: 0.42363241, g_loss: 1.04786205\n",
      "Step: [4753] d_loss: 0.42350847, g_loss: 1.04870999\n",
      "Step: [4754] d_loss: 0.42648423, g_loss: 1.07133842\n",
      "Step: [4755] d_loss: 0.42467484, g_loss: 1.07073402\n",
      "Step: [4756] d_loss: 0.42174339, g_loss: 1.09870362\n",
      "Step: [4757] d_loss: 0.42474380, g_loss: 1.04355681\n",
      "Step: [4758] d_loss: 0.42532194, g_loss: 1.05017197\n",
      "Step: [4759] d_loss: 0.42381778, g_loss: 1.01583874\n",
      "Step: [4760] d_loss: 0.42352194, g_loss: 1.07943046\n",
      "Step: [4761] d_loss: 0.42731485, g_loss: 1.05079460\n",
      "Step: [4762] d_loss: 0.42239049, g_loss: 1.03429961\n",
      "Step: [4763] d_loss: 0.42439106, g_loss: 1.05156291\n",
      "Step: [4764] d_loss: 0.43113270, g_loss: 1.09463191\n",
      "Step: [4765] d_loss: 0.42323875, g_loss: 1.05512130\n",
      "Step: [4766] d_loss: 0.42397651, g_loss: 1.07970428\n",
      "Step: [4767] d_loss: 0.42307979, g_loss: 1.05054438\n",
      "Step: [4768] d_loss: 0.42179888, g_loss: 1.08032608\n",
      "Step: [4769] d_loss: 0.41979188, g_loss: 1.06234431\n",
      "Step: [4770] d_loss: 0.42494148, g_loss: 1.04110932\n",
      "Step: [4771] d_loss: 0.42653421, g_loss: 1.08900964\n",
      "Step: [4772] d_loss: 0.42073232, g_loss: 1.04170585\n",
      "Step: [4773] d_loss: 0.42663315, g_loss: 1.05840194\n",
      "Step: [4774] d_loss: 0.42446637, g_loss: 1.04847848\n",
      "Step: [4775] d_loss: 0.42635816, g_loss: 1.08628905\n",
      "Step: [4776] d_loss: 0.42471436, g_loss: 1.05039787\n",
      "Step: [4777] d_loss: 0.42373183, g_loss: 1.06222749\n",
      "Step: [4778] d_loss: 0.42149907, g_loss: 1.04018426\n",
      "Step: [4779] d_loss: 0.42388105, g_loss: 1.03145599\n",
      "Step: [4780] d_loss: 0.42123875, g_loss: 1.07018745\n",
      "Step: [4781] d_loss: 0.42136863, g_loss: 1.05219960\n",
      "Step: [4782] d_loss: 0.43293488, g_loss: 1.04676628\n",
      "Step: [4783] d_loss: 0.42805210, g_loss: 1.05520284\n",
      "Step: [4784] d_loss: 0.42484257, g_loss: 1.05423224\n",
      "Step: [4785] d_loss: 0.42340171, g_loss: 1.06745565\n",
      "Step: [4786] d_loss: 0.43083590, g_loss: 1.04064715\n",
      "Step: [4787] d_loss: 0.42594710, g_loss: 1.04696929\n",
      "Step: [4788] d_loss: 0.42544970, g_loss: 1.03222847\n",
      "Step: [4789] d_loss: 0.42861685, g_loss: 1.05455577\n",
      "Step: [4790] d_loss: 0.42889020, g_loss: 1.03507519\n",
      "Step: [4791] d_loss: 0.42569590, g_loss: 1.05159557\n",
      "Step: [4792] d_loss: 0.42312890, g_loss: 1.04306221\n",
      "Step: [4793] d_loss: 0.43014672, g_loss: 1.04727578\n",
      "Step: [4794] d_loss: 0.42340642, g_loss: 1.02296734\n",
      "Step: [4795] d_loss: 0.42370489, g_loss: 1.05254030\n",
      "Step: [4796] d_loss: 0.42278212, g_loss: 1.05942965\n",
      "Step: [4797] d_loss: 0.42078775, g_loss: 1.03956783\n",
      "Step: [4798] d_loss: 0.42541611, g_loss: 1.04731381\n",
      "Step: [4799] d_loss: 0.42346787, g_loss: 1.05587626\n",
      "Step: [4800] d_loss: 0.42183688, g_loss: 1.05458009\n",
      "Step: [4801] d_loss: 0.43001825, g_loss: 1.05344677\n",
      "Step: [4802] d_loss: 0.42578766, g_loss: 1.04399633\n",
      "Step: [4803] d_loss: 0.42280149, g_loss: 1.05101681\n",
      "Step: [4804] d_loss: 0.42264524, g_loss: 1.01151037\n",
      "Step: [4805] d_loss: 0.42267862, g_loss: 1.05803263\n",
      "Step: [4806] d_loss: 0.42971545, g_loss: 1.04357374\n",
      "Step: [4807] d_loss: 0.42613727, g_loss: 1.01514542\n",
      "Step: [4808] d_loss: 0.42663431, g_loss: 1.05386674\n",
      "Step: [4809] d_loss: 0.42255712, g_loss: 1.06801462\n",
      "Step: [4810] d_loss: 0.42295316, g_loss: 1.05645680\n",
      "Step: [4811] d_loss: 0.42404026, g_loss: 1.02384782\n",
      "Step: [4812] d_loss: 0.43230662, g_loss: 1.07174194\n",
      "Step: [4813] d_loss: 0.42794004, g_loss: 1.06016517\n",
      "Step: [4814] d_loss: 0.42861027, g_loss: 1.05670261\n",
      "Step: [4815] d_loss: 0.42676798, g_loss: 1.08849669\n",
      "Step: [4816] d_loss: 0.42131260, g_loss: 1.04350889\n",
      "Step: [4817] d_loss: 0.42543292, g_loss: 1.05189610\n",
      "Step: [4818] d_loss: 0.42214665, g_loss: 1.04506981\n",
      "Step: [4819] d_loss: 0.42483482, g_loss: 1.05709922\n",
      "Step: [4820] d_loss: 0.42453790, g_loss: 1.06620502\n",
      "Step: [4821] d_loss: 0.42490813, g_loss: 1.06394827\n",
      "Step: [4822] d_loss: 0.42705959, g_loss: 1.09138465\n",
      "Step: [4823] d_loss: 0.42449358, g_loss: 1.03588879\n",
      "Step: [4824] d_loss: 0.42428544, g_loss: 1.09860992\n",
      "Step: [4825] d_loss: 0.42445689, g_loss: 1.08773351\n",
      "Step: [4826] d_loss: 0.42314306, g_loss: 1.10445952\n",
      "Step: [4827] d_loss: 0.42356497, g_loss: 1.08154953\n",
      "Step: [4828] d_loss: 0.42178407, g_loss: 1.07113349\n",
      "Step: [4829] d_loss: 0.42803892, g_loss: 1.07668936\n",
      "Step: [4830] d_loss: 0.42862210, g_loss: 1.03436565\n",
      "Step: [4831] d_loss: 0.42605063, g_loss: 1.09265959\n",
      "Step: [4832] d_loss: 0.42863134, g_loss: 1.09441042\n",
      "Step: [4833] d_loss: 0.42247224, g_loss: 1.12554407\n",
      "Step: [4834] d_loss: 0.43336833, g_loss: 1.06271636\n",
      "Step: [4835] d_loss: 0.42762110, g_loss: 1.05238092\n",
      "Step: [4836] d_loss: 0.42698416, g_loss: 1.06321645\n",
      "Step: [4837] d_loss: 0.42753673, g_loss: 1.05232131\n",
      "Step: [4838] d_loss: 0.43439394, g_loss: 1.08282578\n",
      "Step: [4839] d_loss: 0.42971757, g_loss: 1.08515990\n",
      "Step: [4840] d_loss: 0.43098032, g_loss: 1.06574440\n",
      "Step: [4841] d_loss: 0.42528963, g_loss: 1.07740927\n",
      "Step: [4842] d_loss: 0.42710602, g_loss: 1.04885817\n",
      "Step: [4843] d_loss: 0.42521042, g_loss: 1.07873654\n",
      "Step: [4844] d_loss: 0.42277279, g_loss: 1.05604088\n",
      "Step: [4845] d_loss: 0.42638546, g_loss: 1.08018315\n",
      "Step: [4846] d_loss: 0.42482495, g_loss: 1.01488364\n",
      "Step: [4847] d_loss: 0.42626667, g_loss: 1.07264400\n",
      "Step: [4848] d_loss: 0.42798838, g_loss: 1.07356894\n",
      "Step: [4849] d_loss: 0.42524439, g_loss: 1.08105588\n",
      "Step: [4850] d_loss: 0.43175671, g_loss: 1.12249684\n",
      "Step: [4851] d_loss: 0.42456287, g_loss: 1.06219602\n",
      "Step: [4852] d_loss: 0.42329654, g_loss: 1.06546760\n",
      "Step: [4853] d_loss: 0.42594954, g_loss: 1.07129347\n",
      "Step: [4854] d_loss: 0.42405695, g_loss: 1.07151461\n",
      "Step: [4855] d_loss: 0.42140162, g_loss: 1.05973160\n",
      "Step: [4856] d_loss: 0.42127514, g_loss: 1.06366134\n",
      "Step: [4857] d_loss: 0.42749611, g_loss: 1.04448974\n",
      "Step: [4858] d_loss: 0.42222914, g_loss: 1.06031644\n",
      "Step: [4859] d_loss: 0.42399129, g_loss: 1.07107544\n",
      "Step: [4860] d_loss: 0.42596245, g_loss: 1.05040908\n",
      "Step: [4861] d_loss: 0.42871350, g_loss: 1.05354965\n",
      "Step: [4862] d_loss: 0.42596927, g_loss: 1.06294143\n",
      "Step: [4863] d_loss: 0.42427510, g_loss: 1.08021271\n",
      "Step: [4864] d_loss: 0.42357698, g_loss: 1.07658601\n",
      "Step: [4865] d_loss: 0.42575821, g_loss: 1.04098141\n",
      "Step: [4866] d_loss: 0.42412645, g_loss: 1.04646850\n",
      "Step: [4867] d_loss: 0.42568052, g_loss: 1.06283283\n",
      "Step: [4868] d_loss: 0.42267787, g_loss: 1.06257915\n",
      "Step: [4869] d_loss: 0.42389756, g_loss: 1.05574310\n",
      "Step: [4870] d_loss: 0.42501718, g_loss: 1.05634975\n",
      "Step: [4871] d_loss: 0.42667782, g_loss: 1.10219300\n",
      "Step: [4872] d_loss: 0.42612207, g_loss: 1.08700466\n",
      "Step: [4873] d_loss: 0.42703786, g_loss: 1.05735493\n",
      "Step: [4874] d_loss: 0.42944026, g_loss: 1.08255923\n",
      "Step: [4875] d_loss: 0.42610520, g_loss: 1.09856176\n",
      "Step: [4876] d_loss: 0.42407230, g_loss: 1.05814385\n",
      "Step: [4877] d_loss: 0.42340896, g_loss: 1.06040061\n",
      "Step: [4878] d_loss: 0.42232639, g_loss: 1.06169224\n",
      "Step: [4879] d_loss: 0.42399985, g_loss: 1.05060208\n",
      "Step: [4880] d_loss: 0.42213449, g_loss: 1.10270846\n",
      "Step: [4881] d_loss: 0.42109197, g_loss: 1.05150139\n",
      "Step: [4882] d_loss: 0.42534828, g_loss: 1.03445005\n",
      "Step: [4883] d_loss: 0.43136215, g_loss: 1.03794789\n",
      "Step: [4884] d_loss: 0.42920032, g_loss: 1.10004485\n",
      "Step: [4885] d_loss: 0.42631564, g_loss: 1.08380246\n",
      "Step: [4886] d_loss: 0.42487651, g_loss: 1.03863311\n",
      "Step: [4887] d_loss: 0.42746067, g_loss: 1.11236501\n",
      "Step: [4888] d_loss: 0.42841238, g_loss: 1.10050988\n",
      "Step: [4889] d_loss: 0.42945087, g_loss: 1.09440649\n",
      "Step: [4890] d_loss: 0.42739147, g_loss: 1.08029687\n",
      "Step: [4891] d_loss: 0.42664769, g_loss: 1.06631982\n",
      "Step: [4892] d_loss: 0.42823869, g_loss: 1.10013914\n",
      "Step: [4893] d_loss: 0.42607591, g_loss: 1.09282482\n",
      "Step: [4894] d_loss: 0.42908669, g_loss: 1.08647501\n",
      "Step: [4895] d_loss: 0.42890418, g_loss: 1.05750012\n",
      "Step: [4896] d_loss: 0.43458578, g_loss: 1.09738052\n",
      "Step: [4897] d_loss: 0.42714599, g_loss: 1.05755711\n",
      "Step: [4898] d_loss: 0.42564172, g_loss: 1.08668220\n",
      "Step: [4899] d_loss: 0.42022771, g_loss: 1.04369092\n",
      "Step: [4900] d_loss: 0.42117345, g_loss: 1.10349524\n",
      "Step: [4901] d_loss: 0.42208010, g_loss: 1.07345164\n",
      "Step: [4902] d_loss: 0.42384845, g_loss: 1.12187755\n",
      "Step: [4903] d_loss: 0.42377821, g_loss: 1.09052372\n",
      "Step: [4904] d_loss: 0.42541838, g_loss: 1.08053362\n",
      "Step: [4905] d_loss: 0.42363703, g_loss: 1.10949457\n",
      "Step: [4906] d_loss: 0.42299423, g_loss: 1.09839880\n",
      "Step: [4907] d_loss: 0.42166382, g_loss: 1.11752117\n",
      "Step: [4908] d_loss: 0.42160946, g_loss: 1.08737946\n",
      "Step: [4909] d_loss: 0.42173278, g_loss: 1.09555984\n",
      "Step: [4910] d_loss: 0.42140141, g_loss: 1.09578633\n",
      "Step: [4911] d_loss: 0.42040008, g_loss: 1.04845464\n",
      "Step: [4912] d_loss: 0.42236924, g_loss: 1.08229470\n",
      "Step: [4913] d_loss: 0.42438972, g_loss: 1.08319473\n",
      "Step: [4914] d_loss: 0.42376789, g_loss: 1.07670057\n",
      "Step: [4915] d_loss: 0.42429271, g_loss: 1.06287003\n",
      "Step: [4916] d_loss: 0.42518318, g_loss: 1.05546308\n",
      "Step: [4917] d_loss: 0.42706004, g_loss: 1.10631549\n",
      "Step: [4918] d_loss: 0.42341810, g_loss: 1.07578242\n",
      "Step: [4919] d_loss: 0.42792881, g_loss: 1.04331815\n",
      "Step: [4920] d_loss: 0.42489165, g_loss: 1.07435346\n",
      "Step: [4921] d_loss: 0.42511106, g_loss: 1.09872544\n",
      "Step: [4922] d_loss: 0.42666265, g_loss: 1.12554693\n",
      "Step: [4923] d_loss: 0.42276341, g_loss: 1.08959758\n",
      "Step: [4924] d_loss: 0.42077446, g_loss: 1.12580812\n",
      "Step: [4925] d_loss: 0.42008570, g_loss: 1.07447064\n",
      "Step: [4926] d_loss: 0.42218816, g_loss: 1.06399083\n",
      "Step: [4927] d_loss: 0.42399192, g_loss: 1.09137583\n",
      "Step: [4928] d_loss: 0.42377219, g_loss: 1.06655729\n",
      "Step: [4929] d_loss: 0.42284709, g_loss: 1.10821533\n",
      "Step: [4930] d_loss: 0.42543253, g_loss: 1.07137573\n",
      "Step: [4931] d_loss: 0.42420277, g_loss: 1.07191849\n",
      "Step: [4932] d_loss: 0.42432541, g_loss: 1.05867004\n",
      "Step: [4933] d_loss: 0.42292473, g_loss: 1.10649192\n",
      "Step: [4934] d_loss: 0.42473084, g_loss: 1.05417752\n",
      "Step: [4935] d_loss: 0.42711318, g_loss: 1.05050981\n",
      "Step: [4936] d_loss: 0.42444339, g_loss: 1.09625208\n",
      "Step: [4937] d_loss: 0.42833737, g_loss: 1.09002483\n",
      "Step: [4938] d_loss: 0.42847916, g_loss: 1.11044621\n",
      "Step: [4939] d_loss: 0.42498773, g_loss: 1.07636571\n",
      "Step: [4940] d_loss: 0.42401412, g_loss: 1.08644307\n",
      "Step: [4941] d_loss: 0.42565161, g_loss: 1.06327760\n",
      "Step: [4942] d_loss: 0.42146093, g_loss: 1.10092509\n",
      "Step: [4943] d_loss: 0.42506003, g_loss: 1.06766796\n",
      "Step: [4944] d_loss: 0.42310220, g_loss: 1.07688951\n",
      "Step: [4945] d_loss: 0.42674205, g_loss: 1.10155785\n",
      "Step: [4946] d_loss: 0.42180392, g_loss: 1.06506836\n",
      "Step: [4947] d_loss: 0.42342201, g_loss: 1.10861146\n",
      "Step: [4948] d_loss: 0.42593226, g_loss: 1.08772552\n",
      "Step: [4949] d_loss: 0.42379838, g_loss: 1.10349357\n",
      "Step: [4950] d_loss: 0.42463925, g_loss: 1.10634184\n",
      "Step: [4951] d_loss: 0.42438233, g_loss: 1.10019052\n",
      "Step: [4952] d_loss: 0.42395878, g_loss: 1.08210373\n",
      "Step: [4953] d_loss: 0.42039320, g_loss: 1.09723055\n",
      "Step: [4954] d_loss: 0.41970170, g_loss: 1.07673848\n",
      "Step: [4955] d_loss: 0.42203939, g_loss: 1.07476783\n",
      "Step: [4956] d_loss: 0.42262098, g_loss: 1.08710349\n",
      "Step: [4957] d_loss: 0.42014888, g_loss: 1.05922306\n",
      "Step: [4958] d_loss: 0.42096567, g_loss: 1.07654822\n",
      "Step: [4959] d_loss: 0.42040348, g_loss: 1.06510031\n",
      "Step: [4960] d_loss: 0.42377251, g_loss: 1.09371972\n",
      "Step: [4961] d_loss: 0.42601812, g_loss: 1.06140399\n",
      "Step: [4962] d_loss: 0.42272687, g_loss: 1.09824884\n",
      "Step: [4963] d_loss: 0.42327148, g_loss: 1.04877722\n",
      "Step: [4964] d_loss: 0.42200583, g_loss: 1.06237960\n",
      "Step: [4965] d_loss: 0.42396089, g_loss: 1.04265213\n",
      "Step: [4966] d_loss: 0.42382854, g_loss: 1.09617889\n",
      "Step: [4967] d_loss: 0.42630953, g_loss: 1.02867079\n",
      "Step: [4968] d_loss: 0.42370617, g_loss: 1.07012033\n",
      "Step: [4969] d_loss: 0.42107233, g_loss: 1.10704505\n",
      "Step: [4970] d_loss: 0.42404544, g_loss: 1.05489814\n",
      "Step: [4971] d_loss: 0.42417455, g_loss: 1.11450434\n",
      "Step: [4972] d_loss: 0.42341086, g_loss: 1.08743370\n",
      "Step: [4973] d_loss: 0.42250907, g_loss: 1.08428895\n",
      "Step: [4974] d_loss: 0.42650527, g_loss: 1.05749464\n",
      "Step: [4975] d_loss: 0.42985070, g_loss: 1.08061194\n",
      "Step: [4976] d_loss: 0.43073449, g_loss: 1.11331069\n",
      "Step: [4977] d_loss: 0.42611620, g_loss: 1.05089200\n",
      "Step: [4978] d_loss: 0.42896509, g_loss: 1.04713607\n",
      "Step: [4979] d_loss: 0.42652029, g_loss: 1.06260800\n",
      "Step: [4980] d_loss: 0.42587012, g_loss: 1.14231491\n",
      "Step: [4981] d_loss: 0.42550182, g_loss: 1.07210243\n",
      "Step: [4982] d_loss: 0.42453939, g_loss: 1.06755984\n",
      "Step: [4983] d_loss: 0.42334080, g_loss: 1.06983554\n",
      "Step: [4984] d_loss: 0.42317268, g_loss: 1.09539056\n",
      "Step: [4985] d_loss: 0.42218027, g_loss: 1.10405314\n",
      "Step: [4986] d_loss: 0.42260808, g_loss: 1.07764697\n",
      "Step: [4987] d_loss: 0.43796110, g_loss: 1.09420884\n",
      "Step: [4988] d_loss: 0.42824495, g_loss: 1.08910465\n",
      "Step: [4989] d_loss: 0.42822987, g_loss: 1.09452534\n",
      "Step: [4990] d_loss: 0.42603037, g_loss: 1.09815931\n",
      "Step: [4991] d_loss: 0.42525008, g_loss: 1.08575511\n",
      "Step: [4992] d_loss: 0.42670882, g_loss: 1.10349202\n",
      "Step: [4993] d_loss: 0.42246878, g_loss: 1.06694698\n",
      "Step: [4994] d_loss: 0.42938986, g_loss: 1.08831489\n",
      "Step: [4995] d_loss: 0.43860003, g_loss: 1.08868682\n",
      "Step: [4996] d_loss: 0.42398402, g_loss: 1.09386265\n",
      "Step: [4997] d_loss: 0.42418337, g_loss: 1.07398641\n",
      "Step: [4998] d_loss: 0.42382216, g_loss: 1.06655860\n",
      "Step: [4999] d_loss: 0.42247698, g_loss: 1.08415902\n",
      "Step: [5000] d_loss: 0.43187562, g_loss: 1.09116614\n",
      "Step: [5001] d_loss: 0.42597795, g_loss: 1.07546854\n",
      "Step: [5002] d_loss: 0.42354473, g_loss: 1.10671520\n",
      "Step: [5003] d_loss: 0.42333600, g_loss: 1.10552871\n",
      "Step: [5004] d_loss: 0.42306277, g_loss: 1.09009969\n",
      "Step: [5005] d_loss: 0.42607254, g_loss: 1.05292964\n",
      "Step: [5006] d_loss: 0.42648712, g_loss: 1.07100570\n",
      "Step: [5007] d_loss: 0.42381263, g_loss: 1.12204993\n",
      "Step: [5008] d_loss: 0.42978069, g_loss: 1.08686149\n",
      "Step: [5009] d_loss: 0.42297086, g_loss: 1.07726312\n",
      "Step: [5010] d_loss: 0.42405108, g_loss: 1.10234964\n",
      "Step: [5011] d_loss: 0.42195255, g_loss: 1.05057251\n",
      "Step: [5012] d_loss: 0.42187333, g_loss: 1.09845233\n",
      "Step: [5013] d_loss: 0.42454711, g_loss: 1.07659864\n",
      "Step: [5014] d_loss: 0.42373541, g_loss: 1.06730235\n",
      "Step: [5015] d_loss: 0.42352024, g_loss: 1.07920802\n",
      "Step: [5016] d_loss: 0.42892253, g_loss: 1.06861115\n",
      "Step: [5017] d_loss: 0.42516819, g_loss: 1.08594513\n",
      "Step: [5018] d_loss: 0.42493886, g_loss: 1.09320736\n",
      "Step: [5019] d_loss: 0.42859438, g_loss: 1.08016646\n",
      "Step: [5020] d_loss: 0.42881474, g_loss: 1.09102106\n",
      "Step: [5021] d_loss: 0.42748913, g_loss: 1.09946978\n",
      "Step: [5022] d_loss: 0.42534754, g_loss: 1.06831717\n",
      "Step: [5023] d_loss: 0.42109361, g_loss: 1.06350195\n",
      "Step: [5024] d_loss: 0.42389831, g_loss: 1.04672587\n",
      "Step: [5025] d_loss: 0.41964769, g_loss: 1.08663487\n",
      "Step: [5026] d_loss: 0.42588878, g_loss: 1.06235993\n",
      "Step: [5027] d_loss: 0.42298982, g_loss: 1.08394110\n",
      "Step: [5028] d_loss: 0.42542133, g_loss: 1.08738029\n",
      "Step: [5029] d_loss: 0.42237321, g_loss: 1.09135199\n",
      "Step: [5030] d_loss: 0.42165306, g_loss: 1.09074759\n",
      "Step: [5031] d_loss: 0.42620838, g_loss: 1.07009518\n",
      "Step: [5032] d_loss: 0.42762634, g_loss: 1.07253969\n",
      "Step: [5033] d_loss: 0.42245486, g_loss: 1.07750988\n",
      "Step: [5034] d_loss: 0.42580521, g_loss: 1.12751377\n",
      "Step: [5035] d_loss: 0.42680305, g_loss: 1.09521163\n",
      "Step: [5036] d_loss: 0.42881620, g_loss: 1.12361336\n",
      "Step: [5037] d_loss: 0.42947763, g_loss: 1.11206889\n",
      "Step: [5038] d_loss: 0.43130183, g_loss: 1.12180531\n",
      "Step: [5039] d_loss: 0.42671728, g_loss: 1.12436378\n",
      "Step: [5040] d_loss: 0.42614031, g_loss: 1.08863652\n",
      "Step: [5041] d_loss: 0.42583495, g_loss: 1.08820581\n",
      "Step: [5042] d_loss: 0.43139815, g_loss: 1.11116755\n",
      "Step: [5043] d_loss: 0.42865011, g_loss: 1.06458557\n",
      "Step: [5044] d_loss: 0.42826620, g_loss: 1.06436265\n",
      "Step: [5045] d_loss: 0.42325845, g_loss: 1.12090230\n",
      "Step: [5046] d_loss: 0.42677066, g_loss: 1.10299420\n",
      "Step: [5047] d_loss: 0.42394906, g_loss: 1.07114398\n",
      "Step: [5048] d_loss: 0.42432034, g_loss: 1.08055830\n",
      "Step: [5049] d_loss: 0.42528763, g_loss: 1.09338498\n",
      "Step: [5050] d_loss: 0.42400289, g_loss: 1.05928683\n",
      "Step: [5051] d_loss: 0.42217830, g_loss: 1.06301141\n",
      "Step: [5052] d_loss: 0.43052167, g_loss: 1.07589424\n",
      "Step: [5053] d_loss: 0.43922549, g_loss: 1.06253541\n",
      "Step: [5054] d_loss: 0.43750641, g_loss: 1.06007218\n",
      "Step: [5055] d_loss: 0.42694798, g_loss: 1.08460045\n",
      "Step: [5056] d_loss: 0.42255303, g_loss: 1.08731103\n",
      "Step: [5057] d_loss: 0.41982505, g_loss: 1.07701921\n",
      "Step: [5058] d_loss: 0.43939996, g_loss: 1.05050862\n",
      "Step: [5059] d_loss: 0.42888030, g_loss: 1.08638680\n",
      "Step: [5060] d_loss: 0.42631724, g_loss: 1.08751345\n",
      "Step: [5061] d_loss: 0.42603013, g_loss: 1.05848086\n",
      "Step: [5062] d_loss: 0.42478266, g_loss: 1.07755780\n",
      "Step: [5063] d_loss: 0.42041534, g_loss: 1.04707563\n",
      "Step: [5064] d_loss: 0.43128568, g_loss: 1.07768607\n",
      "Step: [5065] d_loss: 0.42991740, g_loss: 1.10221934\n",
      "Step: [5066] d_loss: 0.42453530, g_loss: 1.09075713\n",
      "Step: [5067] d_loss: 0.42800930, g_loss: 1.05478537\n",
      "Step: [5068] d_loss: 0.42493281, g_loss: 1.06933784\n",
      "Step: [5069] d_loss: 0.42681053, g_loss: 1.06030285\n",
      "Step: [5070] d_loss: 0.42989877, g_loss: 1.05004716\n",
      "Step: [5071] d_loss: 0.42723781, g_loss: 1.05686128\n",
      "Step: [5072] d_loss: 0.42253676, g_loss: 1.05795956\n",
      "Step: [5073] d_loss: 0.43475345, g_loss: 1.06646550\n",
      "Step: [5074] d_loss: 0.42966801, g_loss: 1.06551933\n",
      "Step: [5075] d_loss: 0.42813781, g_loss: 1.03185797\n",
      "Step: [5076] d_loss: 0.42783901, g_loss: 1.06078935\n",
      "Step: [5077] d_loss: 0.42619061, g_loss: 1.05917001\n",
      "Step: [5078] d_loss: 0.42535275, g_loss: 1.06634164\n",
      "Step: [5079] d_loss: 0.42525318, g_loss: 1.03480399\n",
      "Step: [5080] d_loss: 0.42277250, g_loss: 1.04074264\n",
      "Step: [5081] d_loss: 0.42472437, g_loss: 1.06276476\n",
      "Step: [5082] d_loss: 0.42294562, g_loss: 1.05891812\n",
      "Step: [5083] d_loss: 0.42153299, g_loss: 1.04330981\n",
      "Step: [5084] d_loss: 0.42570603, g_loss: 1.04829538\n",
      "Step: [5085] d_loss: 0.42464238, g_loss: 1.05069268\n",
      "Step: [5086] d_loss: 0.42390269, g_loss: 1.05953753\n",
      "Step: [5087] d_loss: 0.42565632, g_loss: 1.07363343\n",
      "Step: [5088] d_loss: 0.42321798, g_loss: 1.09299123\n",
      "Step: [5089] d_loss: 0.42479470, g_loss: 1.05966985\n",
      "Step: [5090] d_loss: 0.42002496, g_loss: 1.08306789\n",
      "Step: [5091] d_loss: 0.42470002, g_loss: 1.03709340\n",
      "Step: [5092] d_loss: 0.42412591, g_loss: 1.05523777\n",
      "Step: [5093] d_loss: 0.42371139, g_loss: 1.06060755\n",
      "Step: [5094] d_loss: 0.42192262, g_loss: 1.06422675\n",
      "Step: [5095] d_loss: 0.42205107, g_loss: 1.09831142\n",
      "Step: [5096] d_loss: 0.42420661, g_loss: 1.03788924\n",
      "Step: [5097] d_loss: 0.42186230, g_loss: 1.06567025\n",
      "Step: [5098] d_loss: 0.42246783, g_loss: 1.06143212\n",
      "Step: [5099] d_loss: 0.42155176, g_loss: 1.07743967\n",
      "Step: [5100] d_loss: 0.42013505, g_loss: 1.06629157\n",
      "Step: [5101] d_loss: 0.42356053, g_loss: 1.04212713\n",
      "Step: [5102] d_loss: 0.42144713, g_loss: 1.05372548\n",
      "Step: [5103] d_loss: 0.42752910, g_loss: 1.08804357\n",
      "Step: [5104] d_loss: 0.42501804, g_loss: 1.05778694\n",
      "Step: [5105] d_loss: 0.42820624, g_loss: 1.04642081\n",
      "Step: [5106] d_loss: 0.42265320, g_loss: 1.01330233\n",
      "Step: [5107] d_loss: 0.42408672, g_loss: 1.05158281\n",
      "Step: [5108] d_loss: 0.42076650, g_loss: 1.05901444\n",
      "Step: [5109] d_loss: 0.42322081, g_loss: 1.08506763\n",
      "Step: [5110] d_loss: 0.42129022, g_loss: 1.05888057\n",
      "Step: [5111] d_loss: 0.42347559, g_loss: 1.11405301\n",
      "Step: [5112] d_loss: 0.42259306, g_loss: 1.05345964\n",
      "Step: [5113] d_loss: 0.42344534, g_loss: 1.07169938\n",
      "Step: [5114] d_loss: 0.42274448, g_loss: 1.07471013\n",
      "Step: [5115] d_loss: 0.42370445, g_loss: 1.09571636\n",
      "Step: [5116] d_loss: 0.42085671, g_loss: 1.06294632\n",
      "Step: [5117] d_loss: 0.41833368, g_loss: 1.08001935\n",
      "Step: [5118] d_loss: 0.41864374, g_loss: 1.06905270\n",
      "Step: [5119] d_loss: 0.42050567, g_loss: 1.06999898\n",
      "Step: [5120] d_loss: 0.41832510, g_loss: 1.04710662\n",
      "Step: [5121] d_loss: 0.41843915, g_loss: 1.06639850\n",
      "Step: [5122] d_loss: 0.42228383, g_loss: 1.06076062\n",
      "Step: [5123] d_loss: 0.42507708, g_loss: 1.12715626\n",
      "Step: [5124] d_loss: 0.42289931, g_loss: 1.07431757\n",
      "Step: [5125] d_loss: 0.42136997, g_loss: 1.11034584\n",
      "Step: [5126] d_loss: 0.42119506, g_loss: 1.10046220\n",
      "Step: [5127] d_loss: 0.42597124, g_loss: 1.08507085\n",
      "Step: [5128] d_loss: 0.42397311, g_loss: 1.06546962\n",
      "Step: [5129] d_loss: 0.42363402, g_loss: 1.06700015\n",
      "Step: [5130] d_loss: 0.42472494, g_loss: 1.04751861\n",
      "Step: [5131] d_loss: 0.42752019, g_loss: 1.10361230\n",
      "Step: [5132] d_loss: 0.42361775, g_loss: 1.05734384\n",
      "Step: [5133] d_loss: 0.42065245, g_loss: 1.06098080\n",
      "Step: [5134] d_loss: 0.42246586, g_loss: 1.09126401\n",
      "Step: [5135] d_loss: 0.42401946, g_loss: 1.06673670\n",
      "Step: [5136] d_loss: 0.42380327, g_loss: 1.05533850\n",
      "Step: [5137] d_loss: 0.42649570, g_loss: 1.10176861\n",
      "Step: [5138] d_loss: 0.42421407, g_loss: 1.03743625\n",
      "Step: [5139] d_loss: 0.42253137, g_loss: 1.05759680\n",
      "Step: [5140] d_loss: 0.42332989, g_loss: 1.05604756\n",
      "Step: [5141] d_loss: 0.42774266, g_loss: 1.10096824\n",
      "Step: [5142] d_loss: 0.42167062, g_loss: 1.08995390\n",
      "Step: [5143] d_loss: 0.42495590, g_loss: 1.09369433\n",
      "Step: [5144] d_loss: 0.41953978, g_loss: 1.05631077\n",
      "Step: [5145] d_loss: 0.42813891, g_loss: 1.06975317\n",
      "Step: [5146] d_loss: 0.42450958, g_loss: 1.09776294\n",
      "Step: [5147] d_loss: 0.42093572, g_loss: 1.07989967\n",
      "Step: [5148] d_loss: 0.42451355, g_loss: 1.08475280\n",
      "Step: [5149] d_loss: 0.42649686, g_loss: 1.05404913\n",
      "Step: [5150] d_loss: 0.42207593, g_loss: 1.06571615\n",
      "Step: [5151] d_loss: 0.42203513, g_loss: 1.07742047\n",
      "Step: [5152] d_loss: 0.43741465, g_loss: 1.05063188\n",
      "Step: [5153] d_loss: 0.42041832, g_loss: 1.06535625\n",
      "Step: [5154] d_loss: 0.42283633, g_loss: 1.06641304\n",
      "Step: [5155] d_loss: 0.42510548, g_loss: 1.08617258\n",
      "Step: [5156] d_loss: 0.42235997, g_loss: 1.04103994\n",
      "Step: [5157] d_loss: 0.42114145, g_loss: 1.09698117\n",
      "Step: [5158] d_loss: 0.42379442, g_loss: 1.07785630\n",
      "Step: [5159] d_loss: 0.42541796, g_loss: 1.05650556\n",
      "Step: [5160] d_loss: 0.42153090, g_loss: 1.08894563\n",
      "Step: [5161] d_loss: 0.42208028, g_loss: 1.07516694\n",
      "Step: [5162] d_loss: 0.42357910, g_loss: 1.10349143\n",
      "Step: [5163] d_loss: 0.42381704, g_loss: 1.04997623\n",
      "Step: [5164] d_loss: 0.42740366, g_loss: 1.08649397\n",
      "Step: [5165] d_loss: 0.42285591, g_loss: 1.08583605\n",
      "Step: [5166] d_loss: 0.42353389, g_loss: 1.07478356\n",
      "Step: [5167] d_loss: 0.42166346, g_loss: 1.10750210\n",
      "Step: [5168] d_loss: 0.42055956, g_loss: 1.06479824\n",
      "Step: [5169] d_loss: 0.42266637, g_loss: 1.08392811\n",
      "Step: [5170] d_loss: 0.42390588, g_loss: 1.11546242\n",
      "Step: [5171] d_loss: 0.42082244, g_loss: 1.10148406\n",
      "Step: [5172] d_loss: 0.41975179, g_loss: 1.08223104\n",
      "Step: [5173] d_loss: 0.42344296, g_loss: 1.09048402\n",
      "Step: [5174] d_loss: 0.42346144, g_loss: 1.07903731\n",
      "Step: [5175] d_loss: 0.42186791, g_loss: 1.08585453\n",
      "Step: [5176] d_loss: 0.42097500, g_loss: 1.09627473\n",
      "Step: [5177] d_loss: 0.42160547, g_loss: 1.07293808\n",
      "Step: [5178] d_loss: 0.42290097, g_loss: 1.07654512\n",
      "Step: [5179] d_loss: 0.42092964, g_loss: 1.06674230\n",
      "Step: [5180] d_loss: 0.42423534, g_loss: 1.13043344\n",
      "Step: [5181] d_loss: 0.42166281, g_loss: 1.05265963\n",
      "Step: [5182] d_loss: 0.42176646, g_loss: 1.09018993\n",
      "Step: [5183] d_loss: 0.42157218, g_loss: 1.10822487\n",
      "Step: [5184] d_loss: 0.41789347, g_loss: 1.03704941\n",
      "Step: [5185] d_loss: 0.41977623, g_loss: 1.04425287\n",
      "Step: [5186] d_loss: 0.41861749, g_loss: 1.10202742\n",
      "Step: [5187] d_loss: 0.41945097, g_loss: 1.06753242\n",
      "Step: [5188] d_loss: 0.42221308, g_loss: 1.10375118\n",
      "Step: [5189] d_loss: 0.41669348, g_loss: 1.05626965\n",
      "Step: [5190] d_loss: 0.42011657, g_loss: 1.10861683\n",
      "Step: [5191] d_loss: 0.41630524, g_loss: 1.09882498\n",
      "Step: [5192] d_loss: 0.42245665, g_loss: 1.07942402\n",
      "Step: [5193] d_loss: 0.42187724, g_loss: 1.08638692\n",
      "Step: [5194] d_loss: 0.42002583, g_loss: 1.10692143\n",
      "Step: [5195] d_loss: 0.42346668, g_loss: 1.06324601\n",
      "Step: [5196] d_loss: 0.42159992, g_loss: 1.07604241\n",
      "Step: [5197] d_loss: 0.42107588, g_loss: 1.13931727\n",
      "Step: [5198] d_loss: 0.42252642, g_loss: 1.08067846\n",
      "Step: [5199] d_loss: 0.42645332, g_loss: 1.09183991\n",
      "Step: [5200] d_loss: 0.42029703, g_loss: 1.09007406\n",
      "Step: [5201] d_loss: 0.42036808, g_loss: 1.07180512\n",
      "Step: [5202] d_loss: 0.42145824, g_loss: 1.08939791\n",
      "Step: [5203] d_loss: 0.42099372, g_loss: 1.10314333\n",
      "Step: [5204] d_loss: 0.42359626, g_loss: 1.09391332\n",
      "Step: [5205] d_loss: 0.42167062, g_loss: 1.09490097\n",
      "Step: [5206] d_loss: 0.42409602, g_loss: 1.10694635\n",
      "Step: [5207] d_loss: 0.42466787, g_loss: 1.11612844\n",
      "Step: [5208] d_loss: 0.42137161, g_loss: 1.11446917\n",
      "Step: [5209] d_loss: 0.42300138, g_loss: 1.10985482\n",
      "Step: [5210] d_loss: 0.42499596, g_loss: 1.10671163\n",
      "Step: [5211] d_loss: 0.42124966, g_loss: 1.11351752\n",
      "Step: [5212] d_loss: 0.41940039, g_loss: 1.09195435\n",
      "Step: [5213] d_loss: 0.41870898, g_loss: 1.07543015\n",
      "Step: [5214] d_loss: 0.41916069, g_loss: 1.11176944\n",
      "Step: [5215] d_loss: 0.41943696, g_loss: 1.10628450\n",
      "Step: [5216] d_loss: 0.41977328, g_loss: 1.09539974\n",
      "Step: [5217] d_loss: 0.42391437, g_loss: 1.06334376\n",
      "Step: [5218] d_loss: 0.42211330, g_loss: 1.06291878\n",
      "Step: [5219] d_loss: 0.42621571, g_loss: 1.10384297\n",
      "Step: [5220] d_loss: 0.42011184, g_loss: 1.12229037\n",
      "Step: [5221] d_loss: 0.41950753, g_loss: 1.10266089\n",
      "Step: [5222] d_loss: 0.42266962, g_loss: 1.07101786\n",
      "Step: [5223] d_loss: 0.42070663, g_loss: 1.07490492\n",
      "Step: [5224] d_loss: 0.42622024, g_loss: 1.09619570\n",
      "Step: [5225] d_loss: 0.41937557, g_loss: 1.05933785\n",
      "Step: [5226] d_loss: 0.42182007, g_loss: 1.01737761\n",
      "Step: [5227] d_loss: 0.42302066, g_loss: 1.09398627\n",
      "Step: [5228] d_loss: 0.42104214, g_loss: 1.11200440\n",
      "Step: [5229] d_loss: 0.41972244, g_loss: 1.11583579\n",
      "Step: [5230] d_loss: 0.41939262, g_loss: 1.09404194\n",
      "Step: [5231] d_loss: 0.41862172, g_loss: 1.10295618\n",
      "Step: [5232] d_loss: 0.42059156, g_loss: 1.11554217\n",
      "Step: [5233] d_loss: 0.41970849, g_loss: 1.06775141\n",
      "Step: [5234] d_loss: 0.42065039, g_loss: 1.11143363\n",
      "Step: [5235] d_loss: 0.42154136, g_loss: 1.08376360\n",
      "Step: [5236] d_loss: 0.41753370, g_loss: 1.08008277\n",
      "Step: [5237] d_loss: 0.41989756, g_loss: 1.08375466\n",
      "Step: [5238] d_loss: 0.41934711, g_loss: 1.11747777\n",
      "Step: [5239] d_loss: 0.42010096, g_loss: 1.09698963\n",
      "Step: [5240] d_loss: 0.41995880, g_loss: 1.08735025\n",
      "Step: [5241] d_loss: 0.42147642, g_loss: 1.09141088\n",
      "Step: [5242] d_loss: 0.41814816, g_loss: 1.10004890\n",
      "Step: [5243] d_loss: 0.41728663, g_loss: 1.08200097\n",
      "Step: [5244] d_loss: 0.42117670, g_loss: 1.07811046\n",
      "Step: [5245] d_loss: 0.42046639, g_loss: 1.08546865\n",
      "Step: [5246] d_loss: 0.41938835, g_loss: 1.08182502\n",
      "Step: [5247] d_loss: 0.42015287, g_loss: 1.06260657\n",
      "Step: [5248] d_loss: 0.41730246, g_loss: 1.09289753\n",
      "Step: [5249] d_loss: 0.41831002, g_loss: 1.09504318\n",
      "Step: [5250] d_loss: 0.42152721, g_loss: 1.06521583\n",
      "Step: [5251] d_loss: 0.42263868, g_loss: 1.09898841\n",
      "Step: [5252] d_loss: 0.41717413, g_loss: 1.09139919\n",
      "Step: [5253] d_loss: 0.41838047, g_loss: 1.06883800\n",
      "Step: [5254] d_loss: 0.42177981, g_loss: 1.05651832\n",
      "Step: [5255] d_loss: 0.42164329, g_loss: 1.07791650\n",
      "Step: [5256] d_loss: 0.42246571, g_loss: 1.04301858\n",
      "Step: [5257] d_loss: 0.42041630, g_loss: 1.09985828\n",
      "Step: [5258] d_loss: 0.42185459, g_loss: 1.11316705\n",
      "Step: [5259] d_loss: 0.41757438, g_loss: 1.02281272\n",
      "Step: [5260] d_loss: 0.41924369, g_loss: 1.06748021\n",
      "Step: [5261] d_loss: 0.42317271, g_loss: 1.06412923\n",
      "Step: [5262] d_loss: 0.42171901, g_loss: 1.06714320\n",
      "Step: [5263] d_loss: 0.42096236, g_loss: 1.05935705\n",
      "Step: [5264] d_loss: 0.42343053, g_loss: 1.08048677\n",
      "Step: [5265] d_loss: 0.42221528, g_loss: 1.09912002\n",
      "Step: [5266] d_loss: 0.42241102, g_loss: 1.05048764\n",
      "Step: [5267] d_loss: 0.42105898, g_loss: 1.09423077\n",
      "Step: [5268] d_loss: 0.41983801, g_loss: 1.08750749\n",
      "Step: [5269] d_loss: 0.42271981, g_loss: 1.10693848\n",
      "Step: [5270] d_loss: 0.41980782, g_loss: 1.07988691\n",
      "Step: [5271] d_loss: 0.42190447, g_loss: 1.09479558\n",
      "Step: [5272] d_loss: 0.41745749, g_loss: 1.09780526\n",
      "Step: [5273] d_loss: 0.41778359, g_loss: 1.09976935\n",
      "Step: [5274] d_loss: 0.43531471, g_loss: 1.06977630\n",
      "Step: [5275] d_loss: 0.42103106, g_loss: 1.13196635\n",
      "Step: [5276] d_loss: 0.42154217, g_loss: 1.05983567\n",
      "Step: [5277] d_loss: 0.42004308, g_loss: 1.07589710\n",
      "Step: [5278] d_loss: 0.42091733, g_loss: 1.10166419\n",
      "Step: [5279] d_loss: 0.41977012, g_loss: 1.11060739\n",
      "Step: [5280] d_loss: 0.42342594, g_loss: 1.11023283\n",
      "Step: [5281] d_loss: 0.42324170, g_loss: 1.08285582\n",
      "Step: [5282] d_loss: 0.42010298, g_loss: 1.10366547\n",
      "Step: [5283] d_loss: 0.42310876, g_loss: 1.08825731\n",
      "Step: [5284] d_loss: 0.42008123, g_loss: 1.11102915\n",
      "Step: [5285] d_loss: 0.42238247, g_loss: 1.06701767\n",
      "Step: [5286] d_loss: 0.41797480, g_loss: 1.09516990\n",
      "Step: [5287] d_loss: 0.41594079, g_loss: 1.05867779\n",
      "Step: [5288] d_loss: 0.41869563, g_loss: 1.05961359\n",
      "Step: [5289] d_loss: 0.42220223, g_loss: 1.07649386\n",
      "Step: [5290] d_loss: 0.41765815, g_loss: 1.05621684\n",
      "Step: [5291] d_loss: 0.41921356, g_loss: 1.05505478\n",
      "Step: [5292] d_loss: 0.41888142, g_loss: 1.04200613\n",
      "Step: [5293] d_loss: 0.41975859, g_loss: 1.05010295\n",
      "Step: [5294] d_loss: 0.42130885, g_loss: 1.06739759\n",
      "Step: [5295] d_loss: 0.42093608, g_loss: 1.06808996\n",
      "Step: [5296] d_loss: 0.41857705, g_loss: 1.05634880\n",
      "Step: [5297] d_loss: 0.42004803, g_loss: 1.07979774\n",
      "Step: [5298] d_loss: 0.42612779, g_loss: 1.07625270\n",
      "Step: [5299] d_loss: 0.42658079, g_loss: 1.07614303\n",
      "Step: [5300] d_loss: 0.42339149, g_loss: 1.04298294\n",
      "Step: [5301] d_loss: 0.42066240, g_loss: 1.07622182\n",
      "Step: [5302] d_loss: 0.42085734, g_loss: 1.06998420\n",
      "Step: [5303] d_loss: 0.42187464, g_loss: 1.05799901\n",
      "Step: [5304] d_loss: 0.42851311, g_loss: 1.10321903\n",
      "Step: [5305] d_loss: 0.41934180, g_loss: 1.07884037\n",
      "Step: [5306] d_loss: 0.42115417, g_loss: 1.05851233\n",
      "Step: [5307] d_loss: 0.42174205, g_loss: 1.05305171\n",
      "Step: [5308] d_loss: 0.41907105, g_loss: 1.06880629\n",
      "Step: [5309] d_loss: 0.42376572, g_loss: 1.01045609\n",
      "Step: [5310] d_loss: 0.42184040, g_loss: 1.05208242\n",
      "Step: [5311] d_loss: 0.42201868, g_loss: 1.06573737\n",
      "Step: [5312] d_loss: 0.42080763, g_loss: 1.06040442\n",
      "Step: [5313] d_loss: 0.42050919, g_loss: 1.07767916\n",
      "Step: [5314] d_loss: 0.41761860, g_loss: 1.06380272\n",
      "Step: [5315] d_loss: 0.41989377, g_loss: 1.11170161\n",
      "Step: [5316] d_loss: 0.42180163, g_loss: 1.07652354\n",
      "Step: [5317] d_loss: 0.42211902, g_loss: 1.06313074\n",
      "Step: [5318] d_loss: 0.42064509, g_loss: 1.08594000\n",
      "Step: [5319] d_loss: 0.41925687, g_loss: 1.07121122\n",
      "Step: [5320] d_loss: 0.42170361, g_loss: 1.06551921\n",
      "Step: [5321] d_loss: 0.41936061, g_loss: 1.06998384\n",
      "Step: [5322] d_loss: 0.41848114, g_loss: 1.07946110\n",
      "Step: [5323] d_loss: 0.42504162, g_loss: 1.06289148\n",
      "Step: [5324] d_loss: 0.42106077, g_loss: 1.10129666\n",
      "Step: [5325] d_loss: 0.41842300, g_loss: 1.06239092\n",
      "Step: [5326] d_loss: 0.42090851, g_loss: 1.06471801\n",
      "Step: [5327] d_loss: 0.42058370, g_loss: 1.10070622\n",
      "Step: [5328] d_loss: 0.42012203, g_loss: 1.05701375\n",
      "Step: [5329] d_loss: 0.42033172, g_loss: 1.06580138\n",
      "Step: [5330] d_loss: 0.41928399, g_loss: 1.05672717\n",
      "Step: [5331] d_loss: 0.41835904, g_loss: 1.09234166\n",
      "Step: [5332] d_loss: 0.41901958, g_loss: 1.09138691\n",
      "Step: [5333] d_loss: 0.41780460, g_loss: 1.07928956\n",
      "Step: [5334] d_loss: 0.41864023, g_loss: 1.07395339\n",
      "Step: [5335] d_loss: 0.41875559, g_loss: 1.09543264\n",
      "Step: [5336] d_loss: 0.41728812, g_loss: 1.09087777\n",
      "Step: [5337] d_loss: 0.41832337, g_loss: 1.11090100\n",
      "Step: [5338] d_loss: 0.41730279, g_loss: 1.10198927\n",
      "Step: [5339] d_loss: 0.41857353, g_loss: 1.06986797\n",
      "Step: [5340] d_loss: 0.41926339, g_loss: 1.06331837\n",
      "Step: [5341] d_loss: 0.41961727, g_loss: 1.09547389\n",
      "Step: [5342] d_loss: 0.41803899, g_loss: 1.10654247\n",
      "Step: [5343] d_loss: 0.41797101, g_loss: 1.07279742\n",
      "Step: [5344] d_loss: 0.41820076, g_loss: 1.05960655\n",
      "Step: [5345] d_loss: 0.42104766, g_loss: 1.09172499\n",
      "Step: [5346] d_loss: 0.41919374, g_loss: 1.08507848\n",
      "Step: [5347] d_loss: 0.41652724, g_loss: 1.13586318\n",
      "Step: [5348] d_loss: 0.41701087, g_loss: 1.08926630\n",
      "Step: [5349] d_loss: 0.41643450, g_loss: 1.07043552\n",
      "Step: [5350] d_loss: 0.42065731, g_loss: 1.09738684\n",
      "Step: [5351] d_loss: 0.42212278, g_loss: 1.08695710\n",
      "Step: [5352] d_loss: 0.42358088, g_loss: 1.08473647\n",
      "Step: [5353] d_loss: 0.41941053, g_loss: 1.11057329\n",
      "Step: [5354] d_loss: 0.42213726, g_loss: 1.10267067\n",
      "Step: [5355] d_loss: 0.41878960, g_loss: 1.07996213\n",
      "Step: [5356] d_loss: 0.41676193, g_loss: 1.06771874\n",
      "Step: [5357] d_loss: 0.41609249, g_loss: 1.09743285\n",
      "Step: [5358] d_loss: 0.42149276, g_loss: 1.08723378\n",
      "Step: [5359] d_loss: 0.41847217, g_loss: 1.12648249\n",
      "Step: [5360] d_loss: 0.41914043, g_loss: 1.10211980\n",
      "Step: [5361] d_loss: 0.41817361, g_loss: 1.08643281\n",
      "Step: [5362] d_loss: 0.41986415, g_loss: 1.07412994\n",
      "Step: [5363] d_loss: 0.41685697, g_loss: 1.04352105\n",
      "Step: [5364] d_loss: 0.41720989, g_loss: 1.05303383\n",
      "Step: [5365] d_loss: 0.42003658, g_loss: 1.04898405\n",
      "Step: [5366] d_loss: 0.42188913, g_loss: 1.10494411\n",
      "Step: [5367] d_loss: 0.42081392, g_loss: 1.07103384\n",
      "Step: [5368] d_loss: 0.42263389, g_loss: 1.07244265\n",
      "Step: [5369] d_loss: 0.41982210, g_loss: 1.07378876\n",
      "Step: [5370] d_loss: 0.42065674, g_loss: 1.09803152\n",
      "Step: [5371] d_loss: 0.42041868, g_loss: 1.11278749\n",
      "Step: [5372] d_loss: 0.42034191, g_loss: 1.11848104\n",
      "Step: [5373] d_loss: 0.41719234, g_loss: 1.11369336\n",
      "Step: [5374] d_loss: 0.41806147, g_loss: 1.07416177\n",
      "Step: [5375] d_loss: 0.41752908, g_loss: 1.06706870\n",
      "Step: [5376] d_loss: 0.41558486, g_loss: 1.04676056\n",
      "Step: [5377] d_loss: 0.41618079, g_loss: 1.08365130\n",
      "Step: [5378] d_loss: 0.41764370, g_loss: 1.08563089\n",
      "Step: [5379] d_loss: 0.41844827, g_loss: 1.04720235\n",
      "Step: [5380] d_loss: 0.42107385, g_loss: 1.08713222\n",
      "Step: [5381] d_loss: 0.41961700, g_loss: 1.07461929\n",
      "Step: [5382] d_loss: 0.42064744, g_loss: 1.08330822\n",
      "Step: [5383] d_loss: 0.42238915, g_loss: 1.04768968\n",
      "Step: [5384] d_loss: 0.42209107, g_loss: 1.04079187\n",
      "Step: [5385] d_loss: 0.41945854, g_loss: 1.09580588\n",
      "Step: [5386] d_loss: 0.41864413, g_loss: 1.10692918\n",
      "Step: [5387] d_loss: 0.41799375, g_loss: 1.01635337\n",
      "Step: [5388] d_loss: 0.42055371, g_loss: 1.08938015\n",
      "Step: [5389] d_loss: 0.41708475, g_loss: 1.05710053\n",
      "Step: [5390] d_loss: 0.41877204, g_loss: 1.07082903\n",
      "Step: [5391] d_loss: 0.41637015, g_loss: 1.05589163\n",
      "Step: [5392] d_loss: 0.41856152, g_loss: 1.02478921\n",
      "Step: [5393] d_loss: 0.41877085, g_loss: 1.04131293\n",
      "Step: [5394] d_loss: 0.41809160, g_loss: 1.08069956\n",
      "Step: [5395] d_loss: 0.41771698, g_loss: 1.02933443\n",
      "Step: [5396] d_loss: 0.42107907, g_loss: 1.02565646\n",
      "Step: [5397] d_loss: 0.42335367, g_loss: 1.03684998\n",
      "Step: [5398] d_loss: 0.42043275, g_loss: 1.07071400\n",
      "Step: [5399] d_loss: 0.41542253, g_loss: 1.08072555\n",
      "Step: [5400] d_loss: 0.41634122, g_loss: 1.04562187\n",
      "Step: [5401] d_loss: 0.41827923, g_loss: 1.04518914\n",
      "Step: [5402] d_loss: 0.41694272, g_loss: 1.06857502\n",
      "Step: [5403] d_loss: 0.41447356, g_loss: 1.02686071\n",
      "Step: [5404] d_loss: 0.41848466, g_loss: 1.01276553\n",
      "Step: [5405] d_loss: 0.41748771, g_loss: 1.04903913\n",
      "Step: [5406] d_loss: 0.42138895, g_loss: 1.07858706\n",
      "Step: [5407] d_loss: 0.42005268, g_loss: 1.07561040\n",
      "Step: [5408] d_loss: 0.41690826, g_loss: 1.06569839\n",
      "Step: [5409] d_loss: 0.41749433, g_loss: 1.07306659\n",
      "Step: [5410] d_loss: 0.43540236, g_loss: 1.03118110\n",
      "Step: [5411] d_loss: 0.42324096, g_loss: 1.08816385\n",
      "Step: [5412] d_loss: 0.42350599, g_loss: 1.10186100\n",
      "Step: [5413] d_loss: 0.41999787, g_loss: 1.08174562\n",
      "Step: [5414] d_loss: 0.42129344, g_loss: 1.04102790\n",
      "Step: [5415] d_loss: 0.42033747, g_loss: 1.08070076\n",
      "Step: [5416] d_loss: 0.41846037, g_loss: 1.08358395\n",
      "Step: [5417] d_loss: 0.42098528, g_loss: 1.08679867\n",
      "Step: [5418] d_loss: 0.42066252, g_loss: 1.08757126\n",
      "Step: [5419] d_loss: 0.41781440, g_loss: 1.03355920\n",
      "Step: [5420] d_loss: 0.41761398, g_loss: 1.05367196\n",
      "Step: [5421] d_loss: 0.41623369, g_loss: 1.09239638\n",
      "Step: [5422] d_loss: 0.41686082, g_loss: 1.04011464\n",
      "Step: [5423] d_loss: 0.41953242, g_loss: 1.04806018\n",
      "Step: [5424] d_loss: 0.41802126, g_loss: 1.09641230\n",
      "Step: [5425] d_loss: 0.42143980, g_loss: 1.11029446\n",
      "Step: [5426] d_loss: 0.41801512, g_loss: 1.09181142\n",
      "Step: [5427] d_loss: 0.42440635, g_loss: 1.04119074\n",
      "Step: [5428] d_loss: 0.41965303, g_loss: 1.08548522\n",
      "Step: [5429] d_loss: 0.42177612, g_loss: 1.08852184\n",
      "Step: [5430] d_loss: 0.42552748, g_loss: 1.05643344\n",
      "Step: [5431] d_loss: 0.42772806, g_loss: 1.05806684\n",
      "Step: [5432] d_loss: 0.42465648, g_loss: 1.08524358\n",
      "Step: [5433] d_loss: 0.42236957, g_loss: 1.07873690\n",
      "Step: [5434] d_loss: 0.42079386, g_loss: 1.06816745\n",
      "Step: [5435] d_loss: 0.42514300, g_loss: 1.09982276\n",
      "Step: [5436] d_loss: 0.42011562, g_loss: 1.12526071\n",
      "Step: [5437] d_loss: 0.41954097, g_loss: 1.05597651\n",
      "Step: [5438] d_loss: 0.42694446, g_loss: 1.06799161\n",
      "Step: [5439] d_loss: 0.42080164, g_loss: 1.08497095\n",
      "Step: [5440] d_loss: 0.42328054, g_loss: 1.05193067\n",
      "Step: [5441] d_loss: 0.42341217, g_loss: 1.13509607\n",
      "Step: [5442] d_loss: 0.42020777, g_loss: 1.08697748\n",
      "Step: [5443] d_loss: 0.41820142, g_loss: 1.04509902\n",
      "Step: [5444] d_loss: 0.41869915, g_loss: 1.06796861\n",
      "Step: [5445] d_loss: 0.41961056, g_loss: 1.10039854\n",
      "Step: [5446] d_loss: 0.42245150, g_loss: 1.08446062\n",
      "Step: [5447] d_loss: 0.41924310, g_loss: 1.10931802\n",
      "Step: [5448] d_loss: 0.42347437, g_loss: 1.11155188\n",
      "Step: [5449] d_loss: 0.41800302, g_loss: 1.07601213\n",
      "Step: [5450] d_loss: 0.41780952, g_loss: 1.04392374\n",
      "Step: [5451] d_loss: 0.41808566, g_loss: 1.08616626\n",
      "Step: [5452] d_loss: 0.42210308, g_loss: 1.03747869\n",
      "Step: [5453] d_loss: 0.42146495, g_loss: 1.09520650\n",
      "Step: [5454] d_loss: 0.41897637, g_loss: 1.09721243\n",
      "Step: [5455] d_loss: 0.41644955, g_loss: 1.06745315\n",
      "Step: [5456] d_loss: 0.42117372, g_loss: 1.08540392\n",
      "Step: [5457] d_loss: 0.42189106, g_loss: 1.10926569\n",
      "Step: [5458] d_loss: 0.42100480, g_loss: 1.06982160\n",
      "Step: [5459] d_loss: 0.42077953, g_loss: 1.12374282\n",
      "Step: [5460] d_loss: 0.41925004, g_loss: 1.06033480\n",
      "Step: [5461] d_loss: 0.42112517, g_loss: 1.04340482\n",
      "Step: [5462] d_loss: 0.41874281, g_loss: 1.09041917\n",
      "Step: [5463] d_loss: 0.41851479, g_loss: 1.05983090\n",
      "Step: [5464] d_loss: 0.41989535, g_loss: 1.09243369\n",
      "Step: [5465] d_loss: 0.42462575, g_loss: 1.04395390\n",
      "Step: [5466] d_loss: 0.42048287, g_loss: 1.08075655\n",
      "Step: [5467] d_loss: 0.42308572, g_loss: 1.08551466\n",
      "Step: [5468] d_loss: 0.41853714, g_loss: 1.10579121\n",
      "Step: [5469] d_loss: 0.41713995, g_loss: 1.08701599\n",
      "Step: [5470] d_loss: 0.42287153, g_loss: 1.04786539\n",
      "Step: [5471] d_loss: 0.42315453, g_loss: 1.11788952\n",
      "Step: [5472] d_loss: 0.42433280, g_loss: 1.04962373\n",
      "Step: [5473] d_loss: 0.42141864, g_loss: 1.00736070\n",
      "Step: [5474] d_loss: 0.42502362, g_loss: 1.09935343\n",
      "Step: [5475] d_loss: 0.42196441, g_loss: 1.05635798\n",
      "Step: [5476] d_loss: 0.42408419, g_loss: 1.12356997\n",
      "Step: [5477] d_loss: 0.42107606, g_loss: 1.09457695\n",
      "Step: [5478] d_loss: 0.42193758, g_loss: 1.15237963\n",
      "Step: [5479] d_loss: 0.41854075, g_loss: 1.04028225\n",
      "Step: [5480] d_loss: 0.42131993, g_loss: 1.08170986\n",
      "Step: [5481] d_loss: 0.42267931, g_loss: 1.08051813\n",
      "Step: [5482] d_loss: 0.41988182, g_loss: 1.08262730\n",
      "Step: [5483] d_loss: 0.42178929, g_loss: 1.06887937\n",
      "Step: [5484] d_loss: 0.41921011, g_loss: 1.10276151\n",
      "Step: [5485] d_loss: 0.41921002, g_loss: 1.06521642\n",
      "Step: [5486] d_loss: 0.41946277, g_loss: 1.08419502\n",
      "Step: [5487] d_loss: 0.42668405, g_loss: 1.09624732\n",
      "Step: [5488] d_loss: 0.41958281, g_loss: 1.08605611\n",
      "Step: [5489] d_loss: 0.42148554, g_loss: 1.07213068\n",
      "Step: [5490] d_loss: 0.42341956, g_loss: 1.08983064\n",
      "Step: [5491] d_loss: 0.42071643, g_loss: 1.08481216\n",
      "Step: [5492] d_loss: 0.42224759, g_loss: 1.12367201\n",
      "Step: [5493] d_loss: 0.42569828, g_loss: 1.09330344\n",
      "Step: [5494] d_loss: 0.42192698, g_loss: 1.07880247\n",
      "Step: [5495] d_loss: 0.42293993, g_loss: 1.11828601\n",
      "Step: [5496] d_loss: 0.41918722, g_loss: 1.09533584\n",
      "Step: [5497] d_loss: 0.41933143, g_loss: 1.09023285\n",
      "Step: [5498] d_loss: 0.42110854, g_loss: 1.08899128\n",
      "Step: [5499] d_loss: 0.41911310, g_loss: 1.13302994\n",
      "Step: [5500] d_loss: 0.42001486, g_loss: 1.06478250\n",
      "Step: [5501] d_loss: 0.43109754, g_loss: 1.11617136\n",
      "Step: [5502] d_loss: 0.42036283, g_loss: 1.12149155\n",
      "Step: [5503] d_loss: 0.42301878, g_loss: 1.10747087\n",
      "Step: [5504] d_loss: 0.42363238, g_loss: 1.06400597\n",
      "Step: [5505] d_loss: 0.42378834, g_loss: 1.11748755\n",
      "Step: [5506] d_loss: 0.42244235, g_loss: 1.10385668\n",
      "Step: [5507] d_loss: 0.42265591, g_loss: 1.05386209\n",
      "Step: [5508] d_loss: 0.42392790, g_loss: 1.10186708\n",
      "Step: [5509] d_loss: 0.42252377, g_loss: 1.05145621\n",
      "Step: [5510] d_loss: 0.42129135, g_loss: 1.09844446\n",
      "Step: [5511] d_loss: 0.41965142, g_loss: 1.07545435\n",
      "Step: [5512] d_loss: 0.41851354, g_loss: 1.08931077\n",
      "Step: [5513] d_loss: 0.42039627, g_loss: 1.09838736\n",
      "Step: [5514] d_loss: 0.41986421, g_loss: 1.05026853\n",
      "Step: [5515] d_loss: 0.42141867, g_loss: 1.09651780\n",
      "Step: [5516] d_loss: 0.41842145, g_loss: 1.11598778\n",
      "Step: [5517] d_loss: 0.42422763, g_loss: 1.11009467\n",
      "Step: [5518] d_loss: 0.42789835, g_loss: 1.10519791\n",
      "Step: [5519] d_loss: 0.42317194, g_loss: 1.11706495\n",
      "Step: [5520] d_loss: 0.42404562, g_loss: 1.11205888\n",
      "Step: [5521] d_loss: 0.42542037, g_loss: 1.13062501\n",
      "Step: [5522] d_loss: 0.41825452, g_loss: 1.11169326\n",
      "Step: [5523] d_loss: 0.42179793, g_loss: 1.10313678\n",
      "Step: [5524] d_loss: 0.41772133, g_loss: 1.07827067\n",
      "Step: [5525] d_loss: 0.41759002, g_loss: 1.09579825\n",
      "Step: [5526] d_loss: 0.41848320, g_loss: 1.08235455\n",
      "Step: [5527] d_loss: 0.41850603, g_loss: 1.11340868\n",
      "Step: [5528] d_loss: 0.41829413, g_loss: 1.13627017\n",
      "Step: [5529] d_loss: 0.42164600, g_loss: 1.07720733\n",
      "Step: [5530] d_loss: 0.42081285, g_loss: 1.08242989\n",
      "Step: [5531] d_loss: 0.41616070, g_loss: 1.11806571\n",
      "Step: [5532] d_loss: 0.41873983, g_loss: 1.10524106\n",
      "Step: [5533] d_loss: 0.41826349, g_loss: 1.13712931\n",
      "Step: [5534] d_loss: 0.41769966, g_loss: 1.07961226\n",
      "Step: [5535] d_loss: 0.42182919, g_loss: 1.07388651\n",
      "Step: [5536] d_loss: 0.42162025, g_loss: 1.10115361\n",
      "Step: [5537] d_loss: 0.42311305, g_loss: 1.11044884\n",
      "Step: [5538] d_loss: 0.42115203, g_loss: 1.08852327\n",
      "Step: [5539] d_loss: 0.42188010, g_loss: 1.07654953\n",
      "Step: [5540] d_loss: 0.42200193, g_loss: 1.14437509\n",
      "Step: [5541] d_loss: 0.42372996, g_loss: 1.06704974\n",
      "Step: [5542] d_loss: 0.42671153, g_loss: 1.14539349\n",
      "Step: [5543] d_loss: 0.42673910, g_loss: 1.08220470\n",
      "Step: [5544] d_loss: 0.42107910, g_loss: 1.12539160\n",
      "Step: [5545] d_loss: 0.42016894, g_loss: 1.11351526\n",
      "Step: [5546] d_loss: 0.42898476, g_loss: 1.05782866\n",
      "Step: [5547] d_loss: 0.42247996, g_loss: 1.06466937\n",
      "Step: [5548] d_loss: 0.44432235, g_loss: 1.08805871\n",
      "Step: [5549] d_loss: 0.42093828, g_loss: 1.08904564\n",
      "Step: [5550] d_loss: 0.42700014, g_loss: 1.08832169\n",
      "Step: [5551] d_loss: 0.41846243, g_loss: 1.07183754\n",
      "Step: [5552] d_loss: 0.42112449, g_loss: 1.07077932\n",
      "Step: [5553] d_loss: 0.42216676, g_loss: 1.10760081\n",
      "Step: [5554] d_loss: 0.41911167, g_loss: 1.13075995\n",
      "Step: [5555] d_loss: 0.41968235, g_loss: 1.08440888\n",
      "Step: [5556] d_loss: 0.42527226, g_loss: 1.10177612\n",
      "Step: [5557] d_loss: 0.42319468, g_loss: 1.10560989\n",
      "Step: [5558] d_loss: 0.42049131, g_loss: 1.05706239\n",
      "Step: [5559] d_loss: 0.43049929, g_loss: 1.13897777\n",
      "Step: [5560] d_loss: 0.42194277, g_loss: 1.09408832\n",
      "Step: [5561] d_loss: 0.41908139, g_loss: 1.10656154\n",
      "Step: [5562] d_loss: 0.42067975, g_loss: 1.05878758\n",
      "Step: [5563] d_loss: 0.42206165, g_loss: 1.12419248\n",
      "Step: [5564] d_loss: 0.42574209, g_loss: 1.07008183\n",
      "Step: [5565] d_loss: 0.42664477, g_loss: 1.09832478\n",
      "Step: [5566] d_loss: 0.42264232, g_loss: 1.05082858\n",
      "Step: [5567] d_loss: 0.42208862, g_loss: 1.06067836\n",
      "Step: [5568] d_loss: 0.42156085, g_loss: 1.08831131\n",
      "Step: [5569] d_loss: 0.42122397, g_loss: 1.10605967\n",
      "Step: [5570] d_loss: 0.41947156, g_loss: 1.08784080\n",
      "Step: [5571] d_loss: 0.42099208, g_loss: 1.10243762\n",
      "Step: [5572] d_loss: 0.42349404, g_loss: 1.06825256\n",
      "Step: [5573] d_loss: 0.42096490, g_loss: 1.04931378\n",
      "Step: [5574] d_loss: 0.41982499, g_loss: 1.07214677\n",
      "Step: [5575] d_loss: 0.42250916, g_loss: 1.07816982\n",
      "Step: [5576] d_loss: 0.42174307, g_loss: 1.06413496\n",
      "Step: [5577] d_loss: 0.42241535, g_loss: 1.06877542\n",
      "Step: [5578] d_loss: 0.41952410, g_loss: 1.07142675\n",
      "Step: [5579] d_loss: 0.41926315, g_loss: 1.08465624\n",
      "Step: [5580] d_loss: 0.42117932, g_loss: 1.04732120\n",
      "Step: [5581] d_loss: 0.41830933, g_loss: 1.09317076\n",
      "Step: [5582] d_loss: 0.42021057, g_loss: 1.05319726\n",
      "Step: [5583] d_loss: 0.41771197, g_loss: 1.07196987\n",
      "Step: [5584] d_loss: 0.41676936, g_loss: 1.07188773\n",
      "Step: [5585] d_loss: 0.42600745, g_loss: 1.06331575\n",
      "Step: [5586] d_loss: 0.43899655, g_loss: 1.10415053\n",
      "Step: [5587] d_loss: 0.42569309, g_loss: 1.06332016\n",
      "Step: [5588] d_loss: 0.42512810, g_loss: 1.02152956\n",
      "Step: [5589] d_loss: 0.42023778, g_loss: 1.07749939\n",
      "Step: [5590] d_loss: 0.42205873, g_loss: 1.06188035\n",
      "Step: [5591] d_loss: 0.42458367, g_loss: 1.06278372\n",
      "Step: [5592] d_loss: 0.42148060, g_loss: 1.06071997\n",
      "Step: [5593] d_loss: 0.42307511, g_loss: 1.07509482\n",
      "Step: [5594] d_loss: 0.42799947, g_loss: 1.09639204\n",
      "Step: [5595] d_loss: 0.42682198, g_loss: 1.08645797\n",
      "Step: [5596] d_loss: 0.42744023, g_loss: 1.10798371\n",
      "Step: [5597] d_loss: 0.42784792, g_loss: 1.04606330\n",
      "Step: [5598] d_loss: 0.42692977, g_loss: 1.07635331\n",
      "Step: [5599] d_loss: 0.42254642, g_loss: 1.08777666\n",
      "Step: [5600] d_loss: 0.42497274, g_loss: 1.06191683\n",
      "Step: [5601] d_loss: 0.42526433, g_loss: 1.08682954\n",
      "Step: [5602] d_loss: 0.42019281, g_loss: 1.11559474\n",
      "Step: [5603] d_loss: 0.42048708, g_loss: 1.08742929\n",
      "Step: [5604] d_loss: 0.42177695, g_loss: 1.08067286\n",
      "Step: [5605] d_loss: 0.42074078, g_loss: 1.07124507\n",
      "Step: [5606] d_loss: 0.42113391, g_loss: 1.10463607\n",
      "Step: [5607] d_loss: 0.42276305, g_loss: 1.10369837\n",
      "Step: [5608] d_loss: 0.42045417, g_loss: 1.08200288\n",
      "Step: [5609] d_loss: 0.41870028, g_loss: 1.08327782\n",
      "Step: [5610] d_loss: 0.42528135, g_loss: 1.11663878\n",
      "Step: [5611] d_loss: 0.42141029, g_loss: 1.13608420\n",
      "Step: [5612] d_loss: 0.42066950, g_loss: 1.10288370\n",
      "Step: [5613] d_loss: 0.42088994, g_loss: 1.11000395\n",
      "Step: [5614] d_loss: 0.42170316, g_loss: 1.10899615\n",
      "Step: [5615] d_loss: 0.41989639, g_loss: 1.10150063\n",
      "Step: [5616] d_loss: 0.41763836, g_loss: 1.10672021\n",
      "Step: [5617] d_loss: 0.41716149, g_loss: 1.09469736\n",
      "Step: [5618] d_loss: 0.42082679, g_loss: 1.10473096\n",
      "Step: [5619] d_loss: 0.41774264, g_loss: 1.08638430\n",
      "Step: [5620] d_loss: 0.41960779, g_loss: 1.14058745\n",
      "Step: [5621] d_loss: 0.41624552, g_loss: 1.08349979\n",
      "Step: [5622] d_loss: 0.41878948, g_loss: 1.09667015\n",
      "Step: [5623] d_loss: 0.41822237, g_loss: 1.05337107\n",
      "Step: [5624] d_loss: 0.42314178, g_loss: 1.08920527\n",
      "Step: [5625] d_loss: 0.41913205, g_loss: 1.09299111\n",
      "Step: [5626] d_loss: 0.42197829, g_loss: 1.13344443\n",
      "Step: [5627] d_loss: 0.41745237, g_loss: 1.07526350\n",
      "Step: [5628] d_loss: 0.42001793, g_loss: 1.01609755\n",
      "Step: [5629] d_loss: 0.42017281, g_loss: 1.06134045\n",
      "Step: [5630] d_loss: 0.42003524, g_loss: 1.06973052\n",
      "Step: [5631] d_loss: 0.42189109, g_loss: 1.04709625\n",
      "Step: [5632] d_loss: 0.41874501, g_loss: 1.07497489\n",
      "Step: [5633] d_loss: 0.41905341, g_loss: 1.05777025\n",
      "Step: [5634] d_loss: 0.41774404, g_loss: 1.06820142\n",
      "Step: [5635] d_loss: 0.42181799, g_loss: 1.07477117\n",
      "Step: [5636] d_loss: 0.42095339, g_loss: 1.11625886\n",
      "Step: [5637] d_loss: 0.42515287, g_loss: 1.06701815\n",
      "Step: [5638] d_loss: 0.42191711, g_loss: 1.05412126\n",
      "Step: [5639] d_loss: 0.42251033, g_loss: 1.08533072\n",
      "Step: [5640] d_loss: 0.42475393, g_loss: 1.03941953\n",
      "Step: [5641] d_loss: 0.42407128, g_loss: 1.05316699\n",
      "Step: [5642] d_loss: 0.42095202, g_loss: 1.07006848\n",
      "Step: [5643] d_loss: 0.42155278, g_loss: 1.07134998\n",
      "Step: [5644] d_loss: 0.42246032, g_loss: 1.10435939\n",
      "Step: [5645] d_loss: 0.42130923, g_loss: 1.07367158\n",
      "Step: [5646] d_loss: 0.41879895, g_loss: 1.05937243\n",
      "Step: [5647] d_loss: 0.41853005, g_loss: 1.07656860\n",
      "Step: [5648] d_loss: 0.42115727, g_loss: 1.06409359\n",
      "Step: [5649] d_loss: 0.41886279, g_loss: 1.08162248\n",
      "Step: [5650] d_loss: 0.42093959, g_loss: 1.06393921\n",
      "Step: [5651] d_loss: 0.41883105, g_loss: 1.07680047\n",
      "Step: [5652] d_loss: 0.42057627, g_loss: 1.10368216\n",
      "Step: [5653] d_loss: 0.42370272, g_loss: 1.04871416\n",
      "Step: [5654] d_loss: 0.42219993, g_loss: 1.05937326\n",
      "Step: [5655] d_loss: 0.42123878, g_loss: 1.10435522\n",
      "Step: [5656] d_loss: 0.41924438, g_loss: 1.06816030\n",
      "Step: [5657] d_loss: 0.42737207, g_loss: 1.08010423\n",
      "Step: [5658] d_loss: 0.42382330, g_loss: 1.05111694\n",
      "Step: [5659] d_loss: 0.42676395, g_loss: 1.04100919\n",
      "Step: [5660] d_loss: 0.42179486, g_loss: 1.09080660\n",
      "Step: [5661] d_loss: 0.42306796, g_loss: 1.05717373\n",
      "Step: [5662] d_loss: 0.41929704, g_loss: 1.08970010\n",
      "Step: [5663] d_loss: 0.41944093, g_loss: 1.07769275\n",
      "Step: [5664] d_loss: 0.41807705, g_loss: 1.08731377\n",
      "Step: [5665] d_loss: 0.43078393, g_loss: 1.04861963\n",
      "Step: [5666] d_loss: 0.42575642, g_loss: 1.07213473\n",
      "Step: [5667] d_loss: 0.42051333, g_loss: 1.07832217\n",
      "Step: [5668] d_loss: 0.42009345, g_loss: 1.07391644\n",
      "Step: [5669] d_loss: 0.42047542, g_loss: 1.07030916\n",
      "Step: [5670] d_loss: 0.42210278, g_loss: 1.06583643\n",
      "Step: [5671] d_loss: 0.42269152, g_loss: 1.07449353\n",
      "Step: [5672] d_loss: 0.42065525, g_loss: 1.01690984\n",
      "Step: [5673] d_loss: 0.42515373, g_loss: 1.09875989\n",
      "Step: [5674] d_loss: 0.42125282, g_loss: 1.02496397\n",
      "Step: [5675] d_loss: 0.42207843, g_loss: 1.05747378\n",
      "Step: [5676] d_loss: 0.42009419, g_loss: 1.06671011\n",
      "Step: [5677] d_loss: 0.42233330, g_loss: 1.07019114\n",
      "Step: [5678] d_loss: 0.42508060, g_loss: 1.03854978\n",
      "Step: [5679] d_loss: 0.42129123, g_loss: 1.09073913\n",
      "Step: [5680] d_loss: 0.42139798, g_loss: 1.04096341\n",
      "Step: [5681] d_loss: 0.42111167, g_loss: 1.07484639\n",
      "Step: [5682] d_loss: 0.42165250, g_loss: 1.08796465\n",
      "Step: [5683] d_loss: 0.41959485, g_loss: 1.05194318\n",
      "Step: [5684] d_loss: 0.43125662, g_loss: 1.03379226\n",
      "Step: [5685] d_loss: 0.41992530, g_loss: 1.07955670\n",
      "Step: [5686] d_loss: 0.42220104, g_loss: 1.05341852\n",
      "Step: [5687] d_loss: 0.42541313, g_loss: 1.05343807\n",
      "Step: [5688] d_loss: 0.42655960, g_loss: 1.09778666\n",
      "Step: [5689] d_loss: 0.42153192, g_loss: 1.09973872\n",
      "Step: [5690] d_loss: 0.41986024, g_loss: 1.09567153\n",
      "Step: [5691] d_loss: 0.42132467, g_loss: 1.06907785\n",
      "Step: [5692] d_loss: 0.41833699, g_loss: 1.05624247\n",
      "Step: [5693] d_loss: 0.41929021, g_loss: 1.08409595\n",
      "Step: [5694] d_loss: 0.42019889, g_loss: 1.08260059\n",
      "Step: [5695] d_loss: 0.42139250, g_loss: 1.03659093\n",
      "Step: [5696] d_loss: 0.41918018, g_loss: 1.06065941\n",
      "Step: [5697] d_loss: 0.41779914, g_loss: 1.07390761\n",
      "Step: [5698] d_loss: 0.42155394, g_loss: 1.10155010\n",
      "Step: [5699] d_loss: 0.42206734, g_loss: 1.08487463\n",
      "Step: [5700] d_loss: 0.41901210, g_loss: 1.08170891\n",
      "Step: [5701] d_loss: 0.42230418, g_loss: 1.07456493\n",
      "Step: [5702] d_loss: 0.42173094, g_loss: 1.09052777\n",
      "Step: [5703] d_loss: 0.42072800, g_loss: 1.04488707\n",
      "Step: [5704] d_loss: 0.42375562, g_loss: 1.05084217\n",
      "Step: [5705] d_loss: 0.42087018, g_loss: 1.07479882\n",
      "Step: [5706] d_loss: 0.41887677, g_loss: 1.05187225\n",
      "Step: [5707] d_loss: 0.42263198, g_loss: 1.08112001\n",
      "Step: [5708] d_loss: 0.42233065, g_loss: 1.03311598\n",
      "Step: [5709] d_loss: 0.42163962, g_loss: 1.02194893\n",
      "Step: [5710] d_loss: 0.42037258, g_loss: 1.08545315\n",
      "Step: [5711] d_loss: 0.42346475, g_loss: 1.07213926\n",
      "Step: [5712] d_loss: 0.41868410, g_loss: 1.04069912\n",
      "Step: [5713] d_loss: 0.41859430, g_loss: 1.03806841\n",
      "Step: [5714] d_loss: 0.42201796, g_loss: 1.04900146\n",
      "Step: [5715] d_loss: 0.42205101, g_loss: 1.03458059\n",
      "Step: [5716] d_loss: 0.41854665, g_loss: 1.00392556\n",
      "Step: [5717] d_loss: 0.41830593, g_loss: 1.06450522\n",
      "Step: [5718] d_loss: 0.41920117, g_loss: 1.06943750\n",
      "Step: [5719] d_loss: 0.41817600, g_loss: 1.06536114\n",
      "Step: [5720] d_loss: 0.41985694, g_loss: 1.04147446\n",
      "Step: [5721] d_loss: 0.42582193, g_loss: 1.02765846\n",
      "Step: [5722] d_loss: 0.41917101, g_loss: 1.03460050\n",
      "Step: [5723] d_loss: 0.41889247, g_loss: 1.02235651\n",
      "Step: [5724] d_loss: 0.42657027, g_loss: 1.04503083\n",
      "Step: [5725] d_loss: 0.42018011, g_loss: 1.09598982\n",
      "Step: [5726] d_loss: 0.42025414, g_loss: 1.03048384\n",
      "Step: [5727] d_loss: 0.42090943, g_loss: 1.08612490\n",
      "Step: [5728] d_loss: 0.42262360, g_loss: 1.04763567\n",
      "Step: [5729] d_loss: 0.41967234, g_loss: 1.02615821\n",
      "Step: [5730] d_loss: 0.41990033, g_loss: 1.04062390\n",
      "Step: [5731] d_loss: 0.42144561, g_loss: 1.08879161\n",
      "Step: [5732] d_loss: 0.41972283, g_loss: 1.05122304\n",
      "Step: [5733] d_loss: 0.41961822, g_loss: 1.03757930\n",
      "Step: [5734] d_loss: 0.42434350, g_loss: 1.09178972\n",
      "Step: [5735] d_loss: 0.41905576, g_loss: 1.07302904\n",
      "Step: [5736] d_loss: 0.42168319, g_loss: 1.10492122\n",
      "Step: [5737] d_loss: 0.42078856, g_loss: 1.08328223\n",
      "Step: [5738] d_loss: 0.41950577, g_loss: 1.07924676\n",
      "Step: [5739] d_loss: 0.42044383, g_loss: 1.08176970\n",
      "Step: [5740] d_loss: 0.41931671, g_loss: 1.04632449\n",
      "Step: [5741] d_loss: 0.42378220, g_loss: 1.05017018\n",
      "Step: [5742] d_loss: 0.41727266, g_loss: 1.06241429\n",
      "Step: [5743] d_loss: 0.42085311, g_loss: 1.06063139\n",
      "Step: [5744] d_loss: 0.42165357, g_loss: 1.07583392\n",
      "Step: [5745] d_loss: 0.41761813, g_loss: 1.08313549\n",
      "Step: [5746] d_loss: 0.42244059, g_loss: 1.05364323\n",
      "Step: [5747] d_loss: 0.42174232, g_loss: 1.07489014\n",
      "Step: [5748] d_loss: 0.42312047, g_loss: 1.05729532\n",
      "Step: [5749] d_loss: 0.42458996, g_loss: 1.06639433\n",
      "Step: [5750] d_loss: 0.41890508, g_loss: 1.03477561\n",
      "Step: [5751] d_loss: 0.41965330, g_loss: 1.03041959\n",
      "Step: [5752] d_loss: 0.41922906, g_loss: 1.05786395\n",
      "Step: [5753] d_loss: 0.41723663, g_loss: 1.06146157\n",
      "Step: [5754] d_loss: 0.41729194, g_loss: 1.04686105\n",
      "Step: [5755] d_loss: 0.41657600, g_loss: 1.06090438\n",
      "Step: [5756] d_loss: 0.42065981, g_loss: 1.01372325\n",
      "Step: [5757] d_loss: 0.41852295, g_loss: 1.07519507\n",
      "Step: [5758] d_loss: 0.41676199, g_loss: 1.04484487\n",
      "Step: [5759] d_loss: 0.41710338, g_loss: 1.07183933\n",
      "Step: [5760] d_loss: 0.42078084, g_loss: 1.05597806\n",
      "Step: [5761] d_loss: 0.41905493, g_loss: 1.08112478\n",
      "Step: [5762] d_loss: 0.42901880, g_loss: 1.05036759\n",
      "Step: [5763] d_loss: 0.42249030, g_loss: 1.06677115\n",
      "Step: [5764] d_loss: 0.41744268, g_loss: 1.07776058\n",
      "Step: [5765] d_loss: 0.41718480, g_loss: 1.07155812\n",
      "Step: [5766] d_loss: 0.41742519, g_loss: 1.02522469\n",
      "Step: [5767] d_loss: 0.41916952, g_loss: 1.05537987\n",
      "Step: [5768] d_loss: 0.41882688, g_loss: 1.07706738\n",
      "Step: [5769] d_loss: 0.41765827, g_loss: 1.06309903\n",
      "Step: [5770] d_loss: 0.41833550, g_loss: 1.02440405\n",
      "Step: [5771] d_loss: 0.41984129, g_loss: 1.06514132\n",
      "Step: [5772] d_loss: 0.41496658, g_loss: 1.11148953\n",
      "Step: [5773] d_loss: 0.41819611, g_loss: 1.04804575\n",
      "Step: [5774] d_loss: 0.41893506, g_loss: 1.04938996\n",
      "Step: [5775] d_loss: 0.41586828, g_loss: 1.05247951\n",
      "Step: [5776] d_loss: 0.42010581, g_loss: 1.07073319\n",
      "Step: [5777] d_loss: 0.41875127, g_loss: 1.06630325\n",
      "Step: [5778] d_loss: 0.42197761, g_loss: 1.06761622\n",
      "Step: [5779] d_loss: 0.41827938, g_loss: 1.06521583\n",
      "Step: [5780] d_loss: 0.42120099, g_loss: 1.04676151\n",
      "Step: [5781] d_loss: 0.42153692, g_loss: 1.10195303\n",
      "Step: [5782] d_loss: 0.41868848, g_loss: 1.06176126\n",
      "Step: [5783] d_loss: 0.41728297, g_loss: 1.04811299\n",
      "Step: [5784] d_loss: 0.41685274, g_loss: 1.05873692\n",
      "Step: [5785] d_loss: 0.41629547, g_loss: 1.05222142\n",
      "Step: [5786] d_loss: 0.41620463, g_loss: 1.03818631\n",
      "Step: [5787] d_loss: 0.41765019, g_loss: 1.05718458\n",
      "Step: [5788] d_loss: 0.41893807, g_loss: 1.04193628\n",
      "Step: [5789] d_loss: 0.42682362, g_loss: 1.06244242\n",
      "Step: [5790] d_loss: 0.42162883, g_loss: 1.05554712\n",
      "Step: [5791] d_loss: 0.41673377, g_loss: 1.05374289\n",
      "Step: [5792] d_loss: 0.41686758, g_loss: 1.05499053\n",
      "Step: [5793] d_loss: 0.41738993, g_loss: 1.03953826\n",
      "Step: [5794] d_loss: 0.41803586, g_loss: 1.04890621\n",
      "Step: [5795] d_loss: 0.42084050, g_loss: 1.04726398\n",
      "Step: [5796] d_loss: 0.41641057, g_loss: 1.05681705\n",
      "Step: [5797] d_loss: 0.42071122, g_loss: 1.06254089\n",
      "Step: [5798] d_loss: 0.42076802, g_loss: 1.05134010\n",
      "Step: [5799] d_loss: 0.42017165, g_loss: 1.04219210\n",
      "Step: [5800] d_loss: 0.42054415, g_loss: 1.04628658\n",
      "Step: [5801] d_loss: 0.41835600, g_loss: 1.04802847\n",
      "Step: [5802] d_loss: 0.42176190, g_loss: 1.02910256\n",
      "Step: [5803] d_loss: 0.42128706, g_loss: 1.04291320\n",
      "Step: [5804] d_loss: 0.42190802, g_loss: 1.02162087\n",
      "Step: [5805] d_loss: 0.42047539, g_loss: 1.07494676\n",
      "Step: [5806] d_loss: 0.41970885, g_loss: 1.07327616\n",
      "Step: [5807] d_loss: 0.42080012, g_loss: 1.06139219\n",
      "Step: [5808] d_loss: 0.41817862, g_loss: 1.06179488\n",
      "Step: [5809] d_loss: 0.41843909, g_loss: 1.03137875\n",
      "Step: [5810] d_loss: 0.41751596, g_loss: 1.06398594\n",
      "Step: [5811] d_loss: 0.41816437, g_loss: 1.06439364\n",
      "Step: [5812] d_loss: 0.41793546, g_loss: 1.03812611\n",
      "Step: [5813] d_loss: 0.41672745, g_loss: 1.03926110\n",
      "Step: [5814] d_loss: 0.41612488, g_loss: 1.01188850\n",
      "Step: [5815] d_loss: 0.41648179, g_loss: 1.03588545\n",
      "Step: [5816] d_loss: 0.41788274, g_loss: 1.03114951\n",
      "Step: [5817] d_loss: 0.41719878, g_loss: 1.04438984\n",
      "Step: [5818] d_loss: 0.41788131, g_loss: 1.02039587\n",
      "Step: [5819] d_loss: 0.41955563, g_loss: 1.05289853\n",
      "Step: [5820] d_loss: 0.41497153, g_loss: 1.06390166\n",
      "Step: [5821] d_loss: 0.42667797, g_loss: 1.04715598\n",
      "Step: [5822] d_loss: 0.41452190, g_loss: 1.06735575\n",
      "Step: [5823] d_loss: 0.42979532, g_loss: 1.05057395\n",
      "Step: [5824] d_loss: 0.42685860, g_loss: 1.04724681\n",
      "Step: [5825] d_loss: 0.42133367, g_loss: 1.04420817\n",
      "Step: [5826] d_loss: 0.42561257, g_loss: 1.03349543\n",
      "Step: [5827] d_loss: 0.42295048, g_loss: 1.03121364\n",
      "Step: [5828] d_loss: 0.42457238, g_loss: 1.01279986\n",
      "Step: [5829] d_loss: 0.42267525, g_loss: 1.04530191\n",
      "Step: [5830] d_loss: 0.42318690, g_loss: 1.06811714\n",
      "Step: [5831] d_loss: 0.42293948, g_loss: 1.04800093\n",
      "Step: [5832] d_loss: 0.42172846, g_loss: 1.06907821\n",
      "Step: [5833] d_loss: 0.42251182, g_loss: 1.08003330\n",
      "Step: [5834] d_loss: 0.41981474, g_loss: 1.08128512\n",
      "Step: [5835] d_loss: 0.42019263, g_loss: 1.06241107\n",
      "Step: [5836] d_loss: 0.42252243, g_loss: 1.08620334\n",
      "Step: [5837] d_loss: 0.42166409, g_loss: 1.03024316\n",
      "Step: [5838] d_loss: 0.42064720, g_loss: 1.04556692\n",
      "Step: [5839] d_loss: 0.42020512, g_loss: 1.06768072\n",
      "Step: [5840] d_loss: 0.42108953, g_loss: 1.09372759\n",
      "Step: [5841] d_loss: 0.42098179, g_loss: 1.08673584\n",
      "Step: [5842] d_loss: 0.41824305, g_loss: 1.05823636\n",
      "Step: [5843] d_loss: 0.42066360, g_loss: 1.10619903\n",
      "Step: [5844] d_loss: 0.42126486, g_loss: 1.09631836\n",
      "Step: [5845] d_loss: 0.42681593, g_loss: 1.07442963\n",
      "Step: [5846] d_loss: 0.42181265, g_loss: 1.05900824\n",
      "Step: [5847] d_loss: 0.41851544, g_loss: 1.07043159\n",
      "Step: [5848] d_loss: 0.41818827, g_loss: 1.08835793\n",
      "Step: [5849] d_loss: 0.41664582, g_loss: 1.04091322\n",
      "Step: [5850] d_loss: 0.41751796, g_loss: 1.08555484\n",
      "Step: [5851] d_loss: 0.41798320, g_loss: 1.06438255\n",
      "Step: [5852] d_loss: 0.42068070, g_loss: 1.08238673\n",
      "Step: [5853] d_loss: 0.41962406, g_loss: 1.04329097\n",
      "Step: [5854] d_loss: 0.42273113, g_loss: 1.08474028\n",
      "Step: [5855] d_loss: 0.41867754, g_loss: 1.09307921\n",
      "Step: [5856] d_loss: 0.41740036, g_loss: 1.06381631\n",
      "Step: [5857] d_loss: 0.41534531, g_loss: 1.05564618\n",
      "Step: [5858] d_loss: 0.41458049, g_loss: 1.07744920\n",
      "Step: [5859] d_loss: 0.42024601, g_loss: 1.06280255\n",
      "Step: [5860] d_loss: 0.41781789, g_loss: 1.06754172\n",
      "Step: [5861] d_loss: 0.41589633, g_loss: 1.05580068\n",
      "Step: [5862] d_loss: 0.41631275, g_loss: 1.06198895\n",
      "Step: [5863] d_loss: 0.41989872, g_loss: 1.06030357\n",
      "Step: [5864] d_loss: 0.41598502, g_loss: 1.03496909\n",
      "Step: [5865] d_loss: 0.42775622, g_loss: 1.05960619\n",
      "Step: [5866] d_loss: 0.41858906, g_loss: 1.04757214\n",
      "Step: [5867] d_loss: 0.41645086, g_loss: 1.05877566\n",
      "Step: [5868] d_loss: 0.41778693, g_loss: 1.08273125\n",
      "Step: [5869] d_loss: 0.41950735, g_loss: 1.05584836\n",
      "Step: [5870] d_loss: 0.41913030, g_loss: 1.06619322\n",
      "Step: [5871] d_loss: 0.41696605, g_loss: 1.02295911\n",
      "Step: [5872] d_loss: 0.41721609, g_loss: 1.07891858\n",
      "Step: [5873] d_loss: 0.41864169, g_loss: 1.08805823\n",
      "Step: [5874] d_loss: 0.41667187, g_loss: 1.06671154\n",
      "Step: [5875] d_loss: 0.41736850, g_loss: 1.09777462\n",
      "Step: [5876] d_loss: 0.41856733, g_loss: 1.03533471\n",
      "Step: [5877] d_loss: 0.41866910, g_loss: 1.12230945\n",
      "Step: [5878] d_loss: 0.41763005, g_loss: 1.05442977\n",
      "Step: [5879] d_loss: 0.41658968, g_loss: 1.05547249\n",
      "Step: [5880] d_loss: 0.41792899, g_loss: 1.10805750\n",
      "Step: [5881] d_loss: 0.41647503, g_loss: 1.08756649\n",
      "Step: [5882] d_loss: 0.41381502, g_loss: 1.08811462\n",
      "Step: [5883] d_loss: 0.41755259, g_loss: 1.06134403\n",
      "Step: [5884] d_loss: 0.41911435, g_loss: 1.06258655\n",
      "Step: [5885] d_loss: 0.41888070, g_loss: 1.03023851\n",
      "Step: [5886] d_loss: 0.41823351, g_loss: 1.07616079\n",
      "Step: [5887] d_loss: 0.41858873, g_loss: 1.06422579\n",
      "Step: [5888] d_loss: 0.41800970, g_loss: 1.07774746\n",
      "Step: [5889] d_loss: 0.42080367, g_loss: 1.05961192\n",
      "Step: [5890] d_loss: 0.42065242, g_loss: 1.06984210\n",
      "Step: [5891] d_loss: 0.42291820, g_loss: 1.02502024\n",
      "Step: [5892] d_loss: 0.41873312, g_loss: 1.09352982\n",
      "Step: [5893] d_loss: 0.42112902, g_loss: 1.05010867\n",
      "Step: [5894] d_loss: 0.42177531, g_loss: 1.06118453\n",
      "Step: [5895] d_loss: 0.41969982, g_loss: 1.08763504\n",
      "Step: [5896] d_loss: 0.42646927, g_loss: 1.07872021\n",
      "Step: [5897] d_loss: 0.42179319, g_loss: 1.10105336\n",
      "Step: [5898] d_loss: 0.42134932, g_loss: 1.06895280\n",
      "Step: [5899] d_loss: 0.41800967, g_loss: 1.06888592\n",
      "Step: [5900] d_loss: 0.41753212, g_loss: 1.01749384\n",
      "Step: [5901] d_loss: 0.41822761, g_loss: 1.06953430\n",
      "Step: [5902] d_loss: 0.42084274, g_loss: 1.04597545\n",
      "Step: [5903] d_loss: 0.41753137, g_loss: 1.06951237\n",
      "Step: [5904] d_loss: 0.41908392, g_loss: 1.05783927\n",
      "Step: [5905] d_loss: 0.41811872, g_loss: 1.10284698\n",
      "Step: [5906] d_loss: 0.41763705, g_loss: 1.07109845\n",
      "Step: [5907] d_loss: 0.41984102, g_loss: 1.07267261\n",
      "Step: [5908] d_loss: 0.42083457, g_loss: 1.06425893\n",
      "Step: [5909] d_loss: 0.42071533, g_loss: 1.09145653\n",
      "Step: [5910] d_loss: 0.42446339, g_loss: 1.02057242\n",
      "Step: [5911] d_loss: 0.42003569, g_loss: 1.05544579\n",
      "Step: [5912] d_loss: 0.42086512, g_loss: 1.08190942\n",
      "Step: [5913] d_loss: 0.41911611, g_loss: 1.07806516\n",
      "Step: [5914] d_loss: 0.41917580, g_loss: 1.09195447\n",
      "Step: [5915] d_loss: 0.41743612, g_loss: 1.09390867\n",
      "Step: [5916] d_loss: 0.42129061, g_loss: 1.03945100\n",
      "Step: [5917] d_loss: 0.42272505, g_loss: 1.06985939\n",
      "Step: [5918] d_loss: 0.42024159, g_loss: 1.02215087\n",
      "Step: [5919] d_loss: 0.41939205, g_loss: 1.07535470\n",
      "Step: [5920] d_loss: 0.41666898, g_loss: 1.08704615\n",
      "Step: [5921] d_loss: 0.41764939, g_loss: 1.05687737\n",
      "Step: [5922] d_loss: 0.41985402, g_loss: 1.08271432\n",
      "Step: [5923] d_loss: 0.41846725, g_loss: 1.08096135\n",
      "Step: [5924] d_loss: 0.42128965, g_loss: 1.04950058\n",
      "Step: [5925] d_loss: 0.41965684, g_loss: 1.08924401\n",
      "Step: [5926] d_loss: 0.41975692, g_loss: 1.05593383\n",
      "Step: [5927] d_loss: 0.41798449, g_loss: 1.08632660\n",
      "Step: [5928] d_loss: 0.41941470, g_loss: 1.04705727\n",
      "Step: [5929] d_loss: 0.41708115, g_loss: 1.06141424\n",
      "Step: [5930] d_loss: 0.41688746, g_loss: 1.07549715\n",
      "Step: [5931] d_loss: 0.42111307, g_loss: 1.12512827\n",
      "Step: [5932] d_loss: 0.42045045, g_loss: 1.08715928\n",
      "Step: [5933] d_loss: 0.42091170, g_loss: 1.07305717\n",
      "Step: [5934] d_loss: 0.41883069, g_loss: 1.12472856\n",
      "Step: [5935] d_loss: 0.41658548, g_loss: 1.10539496\n",
      "Step: [5936] d_loss: 0.41857797, g_loss: 1.04733777\n",
      "Step: [5937] d_loss: 0.41770998, g_loss: 1.07664239\n",
      "Step: [5938] d_loss: 0.41890070, g_loss: 1.08772266\n",
      "Step: [5939] d_loss: 0.42142388, g_loss: 1.06596136\n",
      "Step: [5940] d_loss: 0.41908506, g_loss: 1.07566953\n",
      "Step: [5941] d_loss: 0.41821569, g_loss: 1.05625212\n",
      "Step: [5942] d_loss: 0.41740853, g_loss: 1.06529295\n",
      "Step: [5943] d_loss: 0.41579387, g_loss: 1.06146371\n",
      "Step: [5944] d_loss: 0.41804644, g_loss: 1.06632519\n",
      "Step: [5945] d_loss: 0.41859642, g_loss: 1.07456255\n",
      "Step: [5946] d_loss: 0.41650173, g_loss: 1.04438698\n",
      "Step: [5947] d_loss: 0.41945860, g_loss: 1.07617843\n",
      "Step: [5948] d_loss: 0.42082649, g_loss: 1.05704653\n",
      "Step: [5949] d_loss: 0.41592741, g_loss: 1.09080875\n",
      "Step: [5950] d_loss: 0.41793883, g_loss: 1.05379617\n",
      "Step: [5951] d_loss: 0.41663355, g_loss: 1.00863409\n",
      "Step: [5952] d_loss: 0.41457596, g_loss: 1.02566147\n",
      "Step: [5953] d_loss: 0.41884053, g_loss: 1.03611112\n",
      "Step: [5954] d_loss: 0.41730523, g_loss: 1.05316818\n",
      "Step: [5955] d_loss: 0.42158014, g_loss: 1.03050029\n",
      "Step: [5956] d_loss: 0.41754305, g_loss: 1.04541600\n",
      "Step: [5957] d_loss: 0.41750598, g_loss: 1.03289187\n",
      "Step: [5958] d_loss: 0.41809928, g_loss: 1.03651309\n",
      "Step: [5959] d_loss: 0.41645828, g_loss: 1.07834589\n",
      "Step: [5960] d_loss: 0.41785932, g_loss: 1.03986323\n",
      "Step: [5961] d_loss: 0.42010951, g_loss: 1.06128478\n",
      "Step: [5962] d_loss: 0.41739237, g_loss: 1.06718159\n",
      "Step: [5963] d_loss: 0.41775471, g_loss: 1.05066741\n",
      "Step: [5964] d_loss: 0.41749761, g_loss: 1.03228116\n",
      "Step: [5965] d_loss: 0.42334458, g_loss: 1.06738806\n",
      "Step: [5966] d_loss: 0.42028344, g_loss: 1.02084911\n",
      "Step: [5967] d_loss: 0.41750610, g_loss: 1.06508124\n",
      "Step: [5968] d_loss: 0.41951534, g_loss: 1.02788329\n",
      "Step: [5969] d_loss: 0.42031214, g_loss: 1.02738595\n",
      "Step: [5970] d_loss: 0.42252183, g_loss: 1.08247364\n",
      "Step: [5971] d_loss: 0.42443985, g_loss: 1.01759839\n",
      "Step: [5972] d_loss: 0.41711068, g_loss: 1.04393578\n",
      "Step: [5973] d_loss: 0.41957030, g_loss: 1.10346711\n",
      "Step: [5974] d_loss: 0.41966537, g_loss: 1.05003297\n",
      "Step: [5975] d_loss: 0.41939989, g_loss: 1.06679320\n",
      "Step: [5976] d_loss: 0.42103767, g_loss: 1.06302714\n",
      "Step: [5977] d_loss: 0.41940290, g_loss: 1.03257763\n",
      "Step: [5978] d_loss: 0.41435933, g_loss: 1.04443526\n",
      "Step: [5979] d_loss: 0.41665491, g_loss: 1.04943419\n",
      "Step: [5980] d_loss: 0.41754273, g_loss: 1.10278094\n",
      "Step: [5981] d_loss: 0.41804671, g_loss: 1.06343913\n",
      "Step: [5982] d_loss: 0.41573051, g_loss: 1.01342344\n",
      "Step: [5983] d_loss: 0.41842619, g_loss: 1.03773069\n",
      "Step: [5984] d_loss: 0.41955206, g_loss: 1.04546273\n",
      "Step: [5985] d_loss: 0.41892791, g_loss: 1.02304196\n",
      "Step: [5986] d_loss: 0.42021507, g_loss: 1.07220125\n",
      "Step: [5987] d_loss: 0.41850242, g_loss: 1.08105755\n",
      "Step: [5988] d_loss: 0.41776493, g_loss: 1.04896641\n",
      "Step: [5989] d_loss: 0.41542277, g_loss: 1.03302765\n",
      "Step: [5990] d_loss: 0.42026633, g_loss: 1.08847427\n",
      "Step: [5991] d_loss: 0.41747612, g_loss: 1.09427345\n",
      "Step: [5992] d_loss: 0.41864580, g_loss: 1.08661842\n",
      "Step: [5993] d_loss: 0.41882846, g_loss: 1.08221829\n",
      "Step: [5994] d_loss: 0.42190775, g_loss: 1.06237721\n",
      "Step: [5995] d_loss: 0.42005280, g_loss: 1.08070052\n",
      "Step: [5996] d_loss: 0.42030248, g_loss: 1.07950878\n",
      "Step: [5997] d_loss: 0.41823509, g_loss: 1.07131839\n",
      "Step: [5998] d_loss: 0.41810471, g_loss: 1.05212867\n",
      "Step: [5999] d_loss: 0.41686156, g_loss: 1.07683158\n",
      "testing................\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sbatch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e7323faea0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing................'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtestx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs_supervised\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtextl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sbatch_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 6000\n",
    "# loop for epoch\n",
    "start_time = time.time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step_ind in range(num_steps):\n",
    "    \n",
    "    '''get the real data'''\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    batch_images,batch_labels = real_image_batch\n",
    "    batch_images = batch_images.reshape([batch_size]+image_dims).astype(np.float32)\n",
    "    batch_labels = real_image_batch[1].astype(np.float32)\n",
    "    '''get the noise data'''\n",
    "    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "    \n",
    "    flag_input = 1 if step_ind < 400 else 0 \n",
    "\n",
    "    feed_dict_input = {inputs_supervised:batch_images,inputs_unsupervised:batch_images, z:batch_z, y:batch_labels, flag:1}\n",
    "    # update D network\n",
    "    _ , D_loss, d1, d2 = sess.run([d_optim, d_loss, d_loss_1, d_loss_2], feed_dict=feed_dict_input)\n",
    "    _ , D_loss, d1, d2 = sess.run([d_optim, d_loss, d_loss_1, d_loss_2], feed_dict=feed_dict_input)\n",
    "\n",
    "    # update G network\n",
    "    _, G_loss = sess.run([g_optim,g_loss], feed_dict=feed_dict_input)\n",
    "   # _, G_loss = sess.run([g_optim,g_loss], feed_dict=feed_dict_input)\n",
    "\n",
    "    # display training status\n",
    "    print(\"Step: [%d] d_loss: %.8f, g_loss: %.8f\" % (step_ind, D_loss, G_loss) )\n",
    "\n",
    "    # save training results for every 300 steps\n",
    "    if np.mod(step_ind, 300) == 0:\n",
    "        samples = sess.run(fake_images, feed_dict={z: sample_z})\n",
    "        display(samples, batch_size, img_h, img_w, image_dims, step_ind)\n",
    "        \n",
    "''' testing the di'''       \n",
    "print('testing................')\n",
    "count = 0\n",
    "for i in range(10000//batch_size):\n",
    "    testx, textl = mnist.test.next_batch(batch_size)\n",
    "    prediction1 = sess.run(prediction, feed_dict={inputs_supervised:testx, y:textl})\n",
    "    count += np.sum(prediction1)\n",
    "print(f'acc: {count/10000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
